<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Paper Feed</title>
    <style>
        :root {
            /* Controls panel variables */
            --panel-bg: #ffffff;
            --panel-shadow: rgba(0, 0, 0, 0.1);
            --panel-border: #e2e8f0;
            --button-hover: #f8fafc;
            
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --text-color: #1e293b;
            --secondary-text: #64748b;
            --border-color: #e2e8f0;
            --accent-bg: #f1f5f9;
            --link-color: #2563eb;
        }

        body {
            font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 2rem 1rem;
            margin: 0;
        }

        .container {
            max-width: 860px;
            margin: 0 auto;
        }

        header {
            margin-bottom: 3rem;
            text-align: center;
        }

        h1 {
            font-size: 2.25rem;
            font-weight: 800;
            margin-bottom: 1rem;
        }

        .header-desc {
            color: var(--secondary-text);
            font-size: 1.125rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .day-group {
            margin-bottom: 1.5rem;
        }

        .day-header {
            background: var(--accent-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1rem 1.5rem;
            margin-bottom: 1rem;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .day-header:hover {
            background: #e9eef5;
        }

        .papers-container {
            display: grid;
            grid-template-rows: 1fr;
            transition: grid-template-rows 0.3s ease-out;
        }

        .papers-container-inner {
            overflow: hidden;
        }

        .collapsed .papers-container {
            grid-template-rows: 0fr;
        }

        /* Controls Panel Styles */
        .controls-button {
            position: fixed;
            top: 1rem;
            right: 1rem;
            z-index: 1000;
            padding: 0.5rem;
            background: var(--panel-bg);
            border: 1px solid var(--panel-border);
            border-radius: 0.5rem;
            cursor: pointer;
            box-shadow: 0 2px 4px var(--panel-shadow);
            transition: all 0.2s ease;
        }

        .controls-button:hover {
            background: var(--button-hover);
        }

        .controls-panel {
            position: fixed;
            top: 1rem;
            right: 1rem;
            width: 300px;
            background: var(--panel-bg);
            border: 1px solid var(--panel-border);
            border-radius: 0.5rem;
            padding: 1rem;
            box-shadow: 0 4px 6px var(--panel-shadow);
            z-index: 999;
            transform: translateX(120%);
            transition: transform 0.3s ease;
        }

        .controls-panel.expanded {
            transform: translateX(0);
        }

        .controls-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--panel-border);
        }

        .controls-title {
            font-weight: 600;
            color: var(--text-color);
        }

        .close-controls {
            cursor: pointer;
            color: var(--secondary-text);
        }

        .control-group {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .radio-group {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-bottom: 0.5rem;
        }

        .radio-label {
            font-weight: 500;
            color: var(--text-color);
            white-space: nowrap;
            min-width: 4rem;
        }

        .radio-options {
            display: flex;
            gap: 1rem;
        }

        .radio-option {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--text-color);
            cursor: pointer;
            white-space: nowrap;
        }

        .radio-option input {
            cursor: pointer;
        }

        .control-button {
            width: 100%;
            padding: 0.5rem;
            background: var(--accent-bg);
            border: 1px solid var(--border-color);
            border-radius: 0.25rem;
            color: var(--text-color);
            font-weight: 500;
            cursor: pointer;
            transition: background 0.2s ease;
        }

        .control-button:hover {
            background: var(--button-hover);
        }

        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 0.75rem 1.5rem;
            margin-bottom: 0.5rem;
            cursor: pointer;
        }

        .paper-header {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .expand-icon {
            color: var(--secondary-text);
            transition: transform 0.3s ease;
            font-size: 0.75rem;
            min-width: 12px;
        }

        .expanded .expand-icon {
            transform: rotate(90deg);
        }

        .arxiv-id {
            font-family: ui-monospace, SFMono-Regular, Menlo, monospace;
            color: var(--link-color);
            font-size: 0.9rem;
            min-width: 90px;
        }

        .paper-title {
            font-size: 1rem;
            font-weight: 500;
            margin: 0;
            color: var(--text-color);
        }

        .paper-content {
            overflow: hidden;
            display: grid;
            grid-template-rows: 0fr;
            transition: grid-template-rows 0.3s ease-out;
        }

        .paper-content-inner {
            overflow: hidden;
            padding-top: 1rem;
            margin-top: 1rem;
            border-top: 1px solid var(--border-color);
        }

        .expanded .paper-content {
            grid-template-rows: 1fr;
        }

        .paper-meta {
            margin-bottom: 1rem;
            color: var(--secondary-text);
        }

        .meta-divider {
            margin: 0 0.5rem;
        }

        /* Tag filtering styles */
        .filter-container {
            margin-bottom: 2rem;
            padding: 1rem;
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
        }

        .filter-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
        }

        .filter-mode {
            display: flex;
            gap: 0.5rem;
        }

        .mode-button {
            padding: 0.25rem 0.75rem;
            border: 1px solid var(--border-color);
            border-radius: 1rem;
            background: var(--bg-color);
            cursor: pointer;
            font-size: 0.875rem;
        }

        .mode-button.active {
            background: var(--link-color);
            color: white;
            border-color: var(--link-color);
        }

        .filter-stats {
            font-size: 0.875rem;
            color: var(--secondary-text);
        }

        .tag-cloud {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-bottom: 1rem;
        }

        .filter-actions {
            display: flex;
            gap: 0.5rem;
            justify-content: flex-end;
        }

        .filter-action {
            padding: 0.25rem 0.75rem;
            border: 1px solid var(--border-color);
            border-radius: 0.25rem;
            background: var(--bg-color);
            cursor: pointer;
            font-size: 0.875rem;
        }

        .filter-action:hover {
            background: var(--button-hover);
        }

        .tooltip {
            position: absolute;
            bottom: 100%;
            left: 50%;
            transform: translateX(-50%);
            padding: 0.5rem;
            background: var(--text-color);
            color: white;
            border-radius: 0.25rem;
            font-size: 0.75rem;
            white-space: nowrap;
            opacity: 0;
            pointer-events: none;
            transition: opacity 0.2s ease;
            margin-bottom: 0.5rem;
            z-index: 1000;
        }

        .tag-pill:hover .tooltip {
            opacity: 1;
        }

        /* Hide papers that don't match the filter */
        .paper-card.filtered {
            display: none;
        }

        @media (max-width: 640px) {
            body {
                padding: 1rem;
            }

            .paper-card {
                padding: 0.75rem 1rem;
            }

            .paper-header {
                gap: 0.75rem;
            }
        }

        .coloring-controls {
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid var(--panel-border);
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .toggle-switch {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .coloring-type {
            margin-left: 0.5rem;
        }

        .toggle-label {
            font-size: 0.875rem;
            color: var(--text-color);
        }

        .switch {
            position: relative;
            display: inline-block;
            width: 36px;
            height: 20px;
        }

        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: var(--secondary-text);
            transition: .4s;
            border-radius: 20px;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 16px;
            width: 16px;
            left: 2px;
            bottom: 2px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }

        input:checked + .slider {
            background-color: var(--link-color);
        }

        input:checked + .slider:before {
            transform: translateX(16px);
        }
    </style>
</head>
<body>
    <button class="controls-button" id="showControls" aria-label="Show controls">⚙️</button>
    
    <div class="controls-panel" id="controlsPanel">
        <div class="controls-header">
            <span class="controls-title">Display Controls</span>
            <span class="close-controls" id="closeControls">×</span>
        </div>
        <div class="controls-content">
            <div class="controls-section">
                <div class="controls-subtitle">Display Options</div>
                <div class="control-group">
                    <div class="radio-group">
                        <div class="radio-label">Target:</div>
                        <div class="radio-options">
                            <label class="radio-option">
                                <input type="radio" name="target" value="days" checked>
                                Days
                            </label>
                            <label class="radio-option">
                                <input type="radio" name="target" value="papers">
                                Papers
                            </label>
                        </div>
                    </div>
                    <div class="radio-group">
                        <div class="radio-label">Action:</div>
                        <div class="radio-options">
                            <label class="radio-option">
                                <input type="radio" name="action" value="collapse" checked>
                                Collapse
                            </label>
                            <label class="radio-option">
                                <input type="radio" name="action" value="expand">
                                Expand
                            </label>
                        </div>
                    </div>
                    <div class="coloring-controls">
                        <div class="toggle-switch">
                            <span class="toggle-label">Enable ID coloring</span>
                            <label class="switch">
                                <input type="checkbox" id="coloringToggle" checked>
                                <span class="slider"></span>
                            </label>
                        </div>
                        <div class="radio-group coloring-type">
                            <div class="radio-label">Color by:</div>
                            <div class="radio-options">
                                <label class="radio-option">
                                    <input type="radio" name="colorBy" value="freshness" checked>
                                    Freshness
                                </label>
                                <label class="radio-option">
                                    <input type="radio" name="colorBy" value="readingTime">
                                    Reading Time
                                </label>
                            </div>
                        </div>
                    </div>
                    <button id="executeAction" class="control-button">Apply</button>
                </div>
            </div>
            <div class="controls-section">
                <div class="controls-subtitle">Category Filters</div>
                    <div class="filter-mode">
                        <button class="mode-button active" data-mode="any">ANY</button>
                        <button class="mode-button" data-mode="all">ALL</button>
                        <button class="mode-button" data-mode="none">NONE</button>
                    </div>
                    <div class="filter-stats">
                        Showing <span id="filtered-count">0</span> of <span id="total-count">0</span> papers
                    </div>
                    <div class="tag-cloud" id="tag-cloud">
                        <!-- Tags will be inserted here -->
                    </div>
                    <div class="filter-actions">
                        <button class="filter-action" id="clear-filters">Clear</button>
                        <button class="filter-action" id="select-all">Select All</button>
                    </div>
                </div>
            </div>
    </div>

    <div class="container">
        <header>
            <h1>ArXiv Paper Feed</h1>
            <p class="header-desc">Papers recently visited by <a href="https://bsky.app/profile/digthatdata.bsky.social">@DigThatData</a></p>
        </header>

        <main id="papers-container">
            <!-- Papers will be inserted here -->
        </main>
    </div>

    <script>
        // Styles for tag pills
        const styles = `
            .tag-pill {
                padding: 0.25rem 0.75rem;
                border-radius: 1rem;
                border: 1px solid transparent;
                cursor: pointer;
                font-size: 0.875rem;
                display: flex;
                align-items: center;
                gap: 0.5rem;
                transition: all 0.2s ease;
                position: relative;
            }

            .tag-pill:hover {
                filter: brightness(0.95);
            }

            .tag-pill.active {
                border: 1px solid rgba(0, 0, 0, 0.2);
                filter: brightness(0.9);
            }

            .tag-count {
                background: rgba(0, 0, 0, 0.1);
                padding: 0.125rem 0.375rem;
                border-radius: 1rem;
                font-size: 0.75rem;
            }
        `;

        function formatDate(dateString, format = 'full') {
            const date = new Date(dateString);
            if (format === 'full') {
                return date.toLocaleDateString('en-US', {
                    year: 'numeric',
                    month: 'short',
                    day: 'numeric'
                });
            } else if (format === 'group') {
                return date.toLocaleDateString('en-US', {
                    weekday: 'long',
                    year: 'numeric',
                    month: 'long',
                    day: 'numeric'
                });
            }
        }

        function toggleDayGroup(element) {
            const group = element.closest('.day-group');
            group.classList.toggle('collapsed');
            const date = group.dataset.date;
            const collapsedDays = JSON.parse(localStorage.getItem('collapsedDays') || '{}');
            collapsedDays[date] = group.classList.contains('collapsed');
            localStorage.setItem('collapsedDays', JSON.stringify(collapsedDays));
        }

        function togglePaperCard(element, event) {
            event.stopPropagation();
            const card = element.closest('.paper-card');
            card.classList.toggle('expanded');
            const paperId = card.dataset.paperId;
            const expandedCards = JSON.parse(localStorage.getItem('expandedCards') || '{}');
            expandedCards[paperId] = card.classList.contains('expanded');
            localStorage.setItem('expandedCards', JSON.stringify(expandedCards));
        }

        function getCategoryInfo(tag) {
            // Get the parent category (everything before the dot)
            const parentCategory = tag.split('.')[0];
            
            // Using ColorBrewer Set3 qualitative palette, optimized for colorblind accessibility
            const parentCategoryMap = {
                'cs': { color: '#8dd3c7', category: 'Computer Science' },
                'stat': { color: '#ffffb3', category: 'Statistics' },
                'math': { color: '#bebada', category: 'Mathematics' },
                'physics': { color: '#fb8072', category: 'Physics' },
                'q-bio': { color: '#80b1d3', category: 'Quantitative Biology' },
                'q-fin': { color: '#fdb462', category: 'Quantitative Finance' }
            };
            
            // Map of specific subcategory names
            const subcategoryMap = {
                // Computer Science
                'cs.AI': 'Artificial Intelligence',
                'cs.LG': 'Machine Learning',
                'cs.CL': 'Computation and Language',
                'cs.CV': 'Computer Vision and Pattern Recognition',
                'cs.RO': 'Robotics',
                'cs.NE': 'Neural and Evolutionary Computing',
                'cs.IR': 'Information Retrieval',
                'cs.HC': 'Human-Computer Interaction',
                'cs.SI': 'Social and Information Networks',
                'cs.DB': 'Databases',
                
                // Statistics
                'stat.ML': 'Machine Learning (Statistics)',
                'stat.ME': 'Methodology',
                'stat.TH': 'Statistics Theory',
                
                // Mathematics
                'math.ST': 'Statistics Theory',
                'math.PR': 'Probability',
                'math.OC': 'Optimization',
                
                // Physics
                'physics.data-an': 'Data Analysis',
                'physics.soc-ph': 'Social Physics',
                
                // Quantitative Biology
                'q-bio.NC': 'Neurons and Cognition',
                'q-bio.QM': 'Quantitative Methods',
                
                // Quantitative Finance
                'q-fin.ST': 'Statistical Finance',
                'q-fin.PM': 'Portfolio Management'
            };
            
            const parentInfo = parentCategoryMap[parentCategory] || { color: '#f5f5f5', category: 'Other' };
            const name = subcategoryMap[tag] || tag;
            
            return {
                name: name,
                color: parentInfo.color
            };
        }

        function renderTagCloud() {
            const tags = new Map();
            
            // Collect tags and counts
            Object.values(window.yamlData).forEach(paper => {
                if (paper.arxiv_tags) {
                    paper.arxiv_tags.forEach(tag => {
                        const count = tags.get(tag) || 0;
                        tags.set(tag, count + 1);
                    });
                }
            });

            // Sort tags by count
            const sortedTags = Array.from(tags.entries())
                .sort(([, a], [, b]) => b - a);

            // Render tag cloud
            const tagCloud = document.getElementById('tag-cloud');
            tagCloud.innerHTML = sortedTags.map(([tag, count]) => {
                const { name, color } = getCategoryInfo(tag);
                return `
                    <button class="tag-pill" data-tag="${tag}" style="background-color: ${color}">
                        <span class="tag-name">${tag}</span>
                        <span class="tag-count">${count}</span>
                        <span class="tooltip">${name}</span>
                    </button>
                `;
            }).join('');

            // Re-add click handlers
            document.querySelectorAll('.tag-pill').forEach(pill => {
                pill.addEventListener('click', () => {
                    const tag = pill.dataset.tag;
                    if (window.filterState.activeTags.has(tag)) {
                        window.filterState.activeTags.delete(tag);
                        pill.classList.remove('active');
                    } else {
                        window.filterState.activeTags.add(tag);
                        pill.classList.add('active');
                    }
                    applyFilters();
                });
            });
        }
        
        function calculateColor(paper, coloringEnabled = true) {
            if (!coloringEnabled) return 'rgb(255, 255, 255)';  // White when coloring is disabled
            
            const colorBy = document.querySelector('input[name="colorBy"]:checked').value;
            
            if (colorBy === 'freshness') {
                if (!paper.last_visited || !paper.published_date) return 'rgb(255, 255, 255)';
                
                const visitDate = new Date(paper.last_visited);
                const pubDate = new Date(paper.published_date);
                const diffDays = Math.floor((visitDate - pubDate) / (1000 * 60 * 60 * 24));
                
                const maxAge = 365;
                const freshness = Math.max(0, Math.min(1, 1 - (diffDays / maxAge)));
                const value = Math.round(255 - (freshness * 55));
                return `rgb(${value}, 255, ${value})`; // Green gradient
            } else {
                // Reading time coloring
                const readingTime = paper.total_reading_time_seconds || 0;
                const maxReadingTime = 300; // 5 minutes
                const intensity = Math.max(0, Math.min(1, readingTime / maxReadingTime));
                const value = Math.round(255 - (intensity * 55));
                return `rgb(255, ${value}, ${value})`; // Red gradient
            }
        }
        
        function renderPaperCard(paper, expanded) {
            const readingTime = paper.total_reading_time_seconds 
                ? `${Math.round(paper.total_reading_time_seconds / 60)} min read`
                : '';
        
            // Get freshness toggle state
            const coloringEnabled = document.getElementById('coloringToggle')?.checked ?? true;
            const bgColor = calculateColor(paper, coloringEnabled);
            
            const metaParts = [];
            
            // Add authors
            metaParts.push(`<span>${paper.authors}</span>`);
            
            // Add reading time if available
            if (readingTime) {
                metaParts.push(`<span class="meta-divider">•</span><span>${readingTime}</span>`);
            }
            
            // Add publication date if available
            if (paper.published_date) {
                const pubDate = new Date(paper.published_date).toLocaleDateString();
                metaParts.push(`<span class="meta-divider">•</span><span>Published: ${pubDate}</span>`);
            }
            
            // Add arXiv tags if available
            if (paper.arxiv_tags && paper.arxiv_tags.length > 0) {
                const tags = paper.arxiv_tags.join(', ');
                metaParts.push(`<span class="meta-divider">•</span><span>${tags}</span>`);
            }
            
            return `
                <article class="paper-card ${expanded ? 'expanded' : ''}" data-paper-id="${paper.id}">
                    <div class="paper-header">
                        <span class="expand-icon">▶</span>
                        <a href="${paper.url}" class="arxiv-id" onclick="event.stopPropagation()" 
                           style="background-color: ${bgColor}; padding: 4px 8px; border-radius: 4px;">
                            ${paper.arxivId}
                        </a>
                        <span class="paper-title">${paper.title}</span>
                    </div>
                    <div class="paper-content">
                        <div class="paper-content-inner">
                            <div class="paper-meta">
                                ${metaParts.join('')}
                            </div>
                            <div class="paper-abstract">${paper.abstract}</div>
                        </div>
                    </div>
                </article>
            `;
        }
        
        function initializeFilters() {
            // Add the updated styles to the document
            const styleSheet = document.createElement("style");
            styleSheet.textContent = styles;
            document.head.appendChild(styleSheet);

            // Initialize filter state
            window.filterState = {
                mode: 'any',
                activeTags: new Set()
            };

            // Render initial tag cloud
            renderTagCloud();

            // Update total count
            document.getElementById('total-count').textContent = 
                Object.keys(window.yamlData).length;

            // Mode buttons already handled in initializeEventListeners()
        }
        
        function renderPapers() {
            const container = document.getElementById('papers-container');
            container.innerHTML = '';
            const expandedCards = JSON.parse(localStorage.getItem('expandedCards') || '{}');
            const collapsedDays = JSON.parse(localStorage.getItem('collapsedDays') || '{}');
            
            const papersByDay = {};
            Object.entries(window.yamlData)
                .sort(([_, a], [__, b]) => new Date(b.last_visited) - new Date(a.last_visited))
                .forEach(([id, paper]) => {
                    const date = paper.last_visited.split('T')[0];
                    if (!papersByDay[date]) papersByDay[date] = [];
                    papersByDay[date].push({ ...paper, id });
                });
        
            Object.entries(papersByDay).forEach(([date, papers]) => {
                const dayGroup = document.createElement('section');
                dayGroup.className = `day-group ${collapsedDays[date] ? 'collapsed' : ''}`;
                dayGroup.dataset.date = date;
        
                const dayHeader = document.createElement('div');
                dayHeader.className = 'day-header';
                dayHeader.onclick = () => toggleDayGroup(dayHeader);
                dayHeader.innerHTML = `
                    <span class="day-title">${formatDate(date, 'group')}</span>
                    <span class="paper-count">${papers.length} paper${papers.length !== 1 ? 's' : ''}</span>
                `;
        
                const papersContainer = document.createElement('div');
                papersContainer.className = 'papers-container';
        
                const papersContainerInner = document.createElement('div');
                papersContainerInner.className = 'papers-container-inner';
                papersContainerInner.innerHTML = papers
                    .map(paper => renderPaperCard(paper, expandedCards[paper.id]))
                    .join('');
        
                papersContainer.appendChild(papersContainerInner);
                dayGroup.appendChild(dayHeader);
                dayGroup.appendChild(papersContainer);
                container.appendChild(dayGroup);
            });

            // Add click handlers for paper cards
            document.querySelectorAll('.paper-card').forEach(card => {
                card.onclick = (e) => togglePaperCard(card, e);
            });

            // Initialize filters after papers are rendered
            applyFilters();
        }

        function initializeEventListeners() {
            // Add coloring controls listeners
            const coloringToggle = document.getElementById('coloringToggle');
            if (coloringToggle) {
                // Load saved preferences
                const savedColoring = localStorage.getItem('coloringEnabled');
                if (savedColoring !== null) {
                    coloringToggle.checked = savedColoring === 'true';
                }
                
                const savedColorBy = localStorage.getItem('colorBy');
                if (savedColorBy) {
                    const radio = document.querySelector(`input[name="colorBy"][value="${savedColorBy}"]`);
                    if (radio) radio.checked = true;
                }
                
                // Add listeners
                coloringToggle.addEventListener('change', () => {
                    localStorage.setItem('coloringEnabled', coloringToggle.checked);
                    renderPapers();
                });
                
                document.querySelectorAll('input[name="colorBy"]').forEach(radio => {
                    radio.addEventListener('change', () => {
                        localStorage.setItem('colorBy', radio.value);
                        renderPapers();
                    });
                });
            }

            // Mode buttons
            document.querySelectorAll('.mode-button').forEach(button => {
                button.addEventListener('click', () => {
                    document.querySelectorAll('.mode-button').forEach(b => 
                        b.classList.remove('active'));
                    button.classList.add('active');
                    window.filterState.mode = button.dataset.mode;
                    applyFilters();
                });
            });

            // Clear filters
            document.getElementById('clear-filters').addEventListener('click', () => {
                window.filterState.activeTags.clear();
                document.querySelectorAll('.tag-pill').forEach(pill => 
                    pill.classList.remove('active'));
                applyFilters();
            });

            // Select all
            document.getElementById('select-all').addEventListener('click', () => {
                document.querySelectorAll('.tag-pill').forEach(pill => {
                    const tag = pill.dataset.tag;
                    window.filterState.activeTags.add(tag);
                    pill.classList.add('active');
                });
                applyFilters();
            });

            // Controls panel functionality
            const showControls = document.getElementById('showControls');
            const controlsPanel = document.getElementById('controlsPanel');
            const closeControls = document.getElementById('closeControls');

            showControls.addEventListener('click', () => {
                controlsPanel.classList.add('expanded');
                showControls.style.visibility = 'hidden';
            });

            closeControls.addEventListener('click', () => {
                controlsPanel.classList.remove('expanded');
                showControls.style.visibility = 'visible';
            });

            // Close panel when clicking outside
            document.addEventListener('click', (event) => {
                if (!controlsPanel.contains(event.target) && 
                    event.target !== showControls && 
                    controlsPanel.classList.contains('expanded')) {
                    controlsPanel.classList.remove('expanded');
                    showControls.style.visibility = 'visible';
                }
            });

            // Bulk collapse/expand functionality
            document.getElementById('executeAction').addEventListener('click', () => {
                const target = document.querySelector('input[name="target"]:checked').value;
                const action = document.querySelector('input[name="action"]:checked').value;
                const shouldCollapse = action === 'collapse';
                
                if (target === 'days') {
                    toggleAllDays(shouldCollapse);
                } else {
                    toggleAllPapers(shouldCollapse);
                }
            });
        }

        function toggleAllDays(shouldCollapse) {
            const dayGroups = document.querySelectorAll('.day-group');
            const collapsedDays = JSON.parse(localStorage.getItem('collapsedDays') || '{}');
            
            dayGroups.forEach(group => {
                if (shouldCollapse) {
                    group.classList.add('collapsed');
                    collapsedDays[group.dataset.date] = true;
                } else {
                    group.classList.remove('collapsed');
                    delete collapsedDays[group.dataset.date];
                }
            });
            
            localStorage.setItem('collapsedDays', JSON.stringify(collapsedDays));
        }

        function toggleAllPapers(shouldCollapse) {
            const visiblePapers = document.querySelectorAll('.day-group:not(.collapsed) .paper-card');
            const expandedCards = JSON.parse(localStorage.getItem('expandedCards') || '{}');
            
            visiblePapers.forEach(card => {
                if (shouldCollapse) {
                    card.classList.remove('expanded');
                    delete expandedCards[card.dataset.paperId];
                } else {
                    card.classList.add('expanded');
                    expandedCards[card.dataset.paperId] = true;
                }
            });
            
            localStorage.setItem('expandedCards', JSON.stringify(expandedCards));
        }

        function applyFilters() {
            const { mode, activeTags } = window.filterState;
            let visibleCount = 0;

            document.querySelectorAll('.paper-card').forEach(card => {
                const paperId = card.dataset.paperId;
                const paper = window.yamlData[paperId];
                const paperTags = new Set(paper.arxiv_tags || []);

                let visible = true;
                if (activeTags.size > 0) {
                    if (mode === 'any') {
                        visible = Array.from(activeTags).some(tag => 
                            paperTags.has(tag));
                    } else if (mode === 'all') {
                        visible = Array.from(activeTags).every(tag => 
                            paperTags.has(tag));
                    } else if (mode === 'none') {
                        visible = Array.from(activeTags).every(tag => 
                            !paperTags.has(tag));
                    }
                }

                card.classList.toggle('filtered', !visible);
                if (visible) visibleCount++;
            });

            document.getElementById('filtered-count').textContent = visibleCount;
        }

        window.yamlData = {
  "1201.1717": {
    "id": "1201.1717",
    "title": "On the Hyperbolicity of Small-World and Tree-Like Random Graphs",
    "authors": "Wei Chen, Wenjie Fang, Guangda Hu, Michael W. Mahoney",
    "abstract": "Hyperbolicity is a property of a graph that may be viewed as being a \"soft\" version of a tree, and recent empirical and theoretical work has suggested that many graphs arising in Internet and related data applications have hyperbolic properties. We consider Gromov's notion of \\delta-hyperbolicity, and establish several results for small-world and tree-like random graph models. First, we study the hyperbolicity of Kleinberg small-world random graphs and show that the hyperbolicity of these random graphs is not significantly improved comparing to graph diameter even when it greatly improves decentralized navigation. Next we study a class of tree-like graphs called ringed trees that have constant hyperbolicity. We show that adding random links among the leaves similar to the small-world graph constructions may easily destroy the hyperbolicity of the graphs, except for a class of random edges added using an exponentially decaying probability function based on the ring distance among the leaves.   Our study provides one of the first significant analytical results on the hyperbolicity of a rich class of random graphs, which shed light on the relationship between hyperbolicity and navigability of random graphs, as well as on the sensitivity of hyperbolic {\\delta} to noises in random graphs.",
    "url": "https://arxiv.org/abs/1201.1717",
    "arxivId": "1201.1717",
    "last_visited": "2024-12-29T01:41:46.909000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2012-01-09T09:30:38Z",
    "arxiv_tags": [
      "cs.SI",
      "cs.DM",
      "physics.soc-ph"
    ]
  },
  "1411.1792": {
    "id": "1411.1792",
    "title": "How transferable are features in deep neural networks?",
    "authors": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson",
    "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
    "url": "https://arxiv.org/abs/1411.1792",
    "arxivId": "1411.1792",
    "last_visited": "2024-12-24T02:43:09.950000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2014-11-06T23:09:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE"
    ]
  },
  "1503.02531": {
    "id": "1503.02531",
    "title": "Distilling the Knowledge in a Neural Network",
    "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
    "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
    "url": "https://arxiv.org/abs/1503.02531",
    "arxivId": "1503.02531",
    "last_visited": "2024-12-21T16:05:50.608000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2015-03-09T15:44:49Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "cs.NE"
    ]
  },
  "1503.03585": {
    "id": "1503.03585",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
    "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
    "url": "https://arxiv.org/abs/1503.03585",
    "arxivId": "1503.03585",
    "last_visited": "2024-12-30T14:44:36.204142",
    "last_read": "2024-12-30T14:44:36.204142",
    "total_reading_time_seconds": 15,
    "published_date": "2015-03-12T04:51:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "q-bio.NC",
      "stat.ML"
    ]
  },
  "1506.06579": {
    "id": "1506.06579",
    "title": "Understanding Neural Networks Through Deep Visualization",
    "authors": "Jason Yosinski, Jeff Clune, Anh Nguyen and 2 others",
    "abstract": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.",
    "url": "https://arxiv.org/abs/1506.06579",
    "arxivId": "1506.06579",
    "last_visited": "2024-12-24T02:47:02.227000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2015-06-22T12:57:15Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ]
  },
  "1602.05897": {
    "id": "1602.05897",
    "title": "Toward Deeper Understanding of Neural Networks: The Power of   Initialization and a Dual View on Expressivity",
    "authors": "Amit Daniely, Roy Frostig, Yoram Singer",
    "abstract": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.",
    "url": "https://arxiv.org/abs/1602.05897",
    "arxivId": "1602.05897",
    "last_visited": "2024-12-29T11:06:20.722000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2016-02-18T18:14:19Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "cs.DS",
      "stat.ML"
    ]
  },
  "1705.08039": {
    "id": "1705.08039",
    "title": "Poincaré Embeddings for Learning Hierarchical Representations",
    "authors": "Maximilian Nickel, Douwe Kiela",
    "abstract": "Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\\'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.",
    "url": "https://arxiv.org/abs/1705.08039",
    "arxivId": "1705.08039",
    "last_visited": "2024-12-30T08:28:07.585743",
    "last_read": "2024-12-30T08:28:07.585743",
    "total_reading_time_seconds": 20,
    "published_date": "2017-05-22T23:14:36Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  "1705.10359": {
    "id": "1705.10359",
    "title": "Neural Embeddings of Graphs in Hyperbolic Space",
    "authors": "Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth",
    "abstract": "Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.",
    "url": "https://arxiv.org/abs/1705.10359",
    "arxivId": "1705.10359",
    "last_visited": "2024-12-30T08:28:07.586810",
    "last_read": "2024-12-30T08:28:07.586810",
    "total_reading_time_seconds": 14,
    "published_date": "2017-05-29T18:47:30Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "1706.05806": {
    "id": "1706.05806",
    "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning   Dynamics and Interpretability",
    "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
    "abstract": "We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/",
    "url": "https://arxiv.org/abs/1706.05806",
    "arxivId": "1706.05806",
    "last_visited": "2024-12-30T14:44:48.199297",
    "last_read": "2024-12-30T14:44:48.199297",
    "total_reading_time_seconds": 20,
    "published_date": "2017-06-19T07:09:20Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "1710.09412": {
    "id": "1710.09412",
    "title": "mixup: Beyond Empirical Risk Minimization",
    "authors": "Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
    "url": "https://arxiv.org/abs/1710.09412",
    "arxivId": "1710.09412",
    "last_visited": "2024-12-22T17:42:57.473000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2017-10-25T18:30:49Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "1711.06077": {
    "id": "1711.06077",
    "title": "The Perception-Distortion Tradeoff",
    "authors": "Yochai Blau, Tomer Michaeli",
    "abstract": "Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.",
    "url": "https://arxiv.org/abs/1711.06077",
    "arxivId": "1711.06077",
    "last_visited": "2024-12-22T08:19:51.755000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2017-11-16T13:22:30Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "1711.11586": {
    "id": "1711.11586",
    "title": "Toward Multimodal Image-to-Image Translation",
    "authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak and 4 others",
    "abstract": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \\emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
    "url": "https://arxiv.org/abs/1711.11586",
    "arxivId": "1711.11586",
    "last_visited": "2024-12-30T14:50:13.148849",
    "last_read": "2024-12-30T14:50:13.148849",
    "total_reading_time_seconds": 60,
    "published_date": "2017-11-30T18:59:01Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR",
      "stat.ML"
    ]
  },
  "1802.04956": {
    "id": "1802.04956",
    "title": "D2KE: From Distance to Kernel and Embedding",
    "authors": "Lingfei Wu, Ian En-Hsu Yen, Fangli Xu and 2 others",
    "abstract": "For many machine learning problem settings, particularly with structured inputs such as sequences or sets of objects, a distance measure between inputs can be specified more naturally than a feature representation. However, most standard machine models are designed for inputs with a vector feature representation. In this work, we consider the estimation of a function $f:\\mathcal{X} \\rightarrow \\R$ based solely on a dissimilarity measure $d:\\mathcal{X}\\times\\mathcal{X} \\rightarrow \\R$ between inputs. In particular, we propose a general framework to derive a family of \\emph{positive definite kernels} from a given dissimilarity measure, which subsumes the widely-used \\emph{representative-set method} as a special case, and relates to the well-known \\emph{distance substitution kernel} in a limiting case. We show that functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are Lipschitz-continuous w.r.t. the given distance metric. We provide a tractable algorithm to estimate a function from this RKHS, and show that it enjoys better generalizability than Nearest-Neighbor estimates. Our approach draws from the literature of Random Features, but instead of deriving feature maps from an existing kernel, we construct novel kernels from a random feature map, that we specify given the distance measure. We conduct classification experiments with such disparate domains as strings, time series, and sets of vectors, where our proposed framework compares favorably to existing distance-based learning methods such as $k$-nearest-neighbors, distance-substitution kernels, pseudo-Euclidean embedding, and the representative-set method.",
    "url": "https://arxiv.org/abs/1802.04956",
    "arxivId": "1802.04956",
    "last_visited": "2024-12-29T10:54:09.487000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2018-02-14T04:58:13Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "1803.00567": {
    "id": "1803.00567",
    "title": "Computational Optimal Transport",
    "authors": "Gabriel Peyré, Marco Cuturi",
    "abstract": "Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a \"global\" cost to every such transport, using the \"local\" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.",
    "url": "https://arxiv.org/abs/1803.00567",
    "arxivId": "1803.00567",
    "last_visited": "2024-12-22T08:08:46.564000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2018-03-01T18:28:43Z",
    "arxiv_tags": [
      "stat.ML"
    ]
  },
  "1804.03329": {
    "id": "1804.03329",
    "title": "Representation Tradeoffs for Hyperbolic Embeddings",
    "authors": "Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala",
    "abstract": "Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.'s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.",
    "url": "https://arxiv.org/abs/1804.03329",
    "arxivId": "1804.03329",
    "last_visited": "2024-12-29T02:34:35.575000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2018-04-10T03:39:16Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "1804.08838": {
    "id": "1804.08838",
    "title": "Measuring the Intrinsic Dimension of Objective Landscapes",
    "authors": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski",
    "abstract": "Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.",
    "url": "https://arxiv.org/abs/1804.08838",
    "arxivId": "1804.08838",
    "last_visited": "2024-12-30T14:48:52.141320",
    "last_read": "2024-12-30T14:48:52.141320",
    "total_reading_time_seconds": 26,
    "published_date": "2018-04-24T04:29:10Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ]
  },
  "1806.00468": {
    "id": "1806.00468",
    "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
    "authors": "Suriya Gunasekar, Jason Lee, Daniel Soudry, Nathan Srebro",
    "abstract": "We show that gradient descent on full-width linear convolutional networks of depth $L$ converges to a linear predictor related to the $\\ell_{2/L}$ bridge penalty in the frequency domain. This is in contrast to linearly fully connected networks, where gradient descent converges to the hard margin linear support vector machine solution, regardless of depth.",
    "url": "https://arxiv.org/abs/1806.00468",
    "arxivId": "1806.00468",
    "last_visited": "2024-12-30T08:27:34.533947",
    "last_read": "2024-12-30T08:27:34.533947",
    "total_reading_time_seconds": 23,
    "published_date": "2018-06-01T17:58:58Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "1906.01563": {
    "id": "1906.01563",
    "title": "Hamiltonian Neural Networks",
    "authors": "Sam Greydanus, Misko Dzamba, Jason Yosinski",
    "abstract": "Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.",
    "url": "https://arxiv.org/abs/1906.01563",
    "arxivId": "1906.01563",
    "last_visited": "2024-12-24T02:49:04.570000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2019-06-04T16:27:55Z",
    "arxiv_tags": [
      "cs.NE"
    ]
  },
  "1912.02757": {
    "id": "1912.02757",
    "title": "Deep Ensembles: A Loss Landscape Perspective",
    "authors": "Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan",
    "abstract": "Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.",
    "url": "https://arxiv.org/abs/1912.02757",
    "arxivId": "1912.02757",
    "last_visited": "2024-12-30T14:44:42.201667",
    "last_read": "2024-12-30T14:44:42.201667",
    "total_reading_time_seconds": 6,
    "published_date": "2019-12-05T17:48:18Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "2001.04063": {
    "id": "2001.04063",
    "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence   Pre-training",
    "authors": "Weizhen Qi, Yu Yan, Yeyun Gong and 5 others",
    "abstract": "This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.",
    "url": "https://arxiv.org/abs/2001.04063",
    "arxivId": "2001.04063",
    "last_visited": "2024-12-26T17:17:59.219000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-01-13T05:12:38Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2006.11120": {
    "id": "2006.11120",
    "title": "From Discrete to Continuous Convolution Layers",
    "authors": "Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani",
    "abstract": "A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.",
    "url": "https://arxiv.org/abs/2006.11120",
    "arxivId": "2006.11120",
    "last_visited": "2024-12-30T14:44:33.195947",
    "last_read": "2024-12-30T14:44:33.195947",
    "total_reading_time_seconds": 24,
    "published_date": "2020-06-19T13:16:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ]
  },
  "2006.14769": {
    "id": "2006.14769",
    "title": "Supermasks in Superposition",
    "authors": "Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu and 4 others",
    "abstract": "We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.",
    "url": "https://arxiv.org/abs/2006.14769",
    "arxivId": "2006.14769",
    "last_visited": "2024-12-24T02:37:38.778000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-06-26T03:16:44Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  "2009.10195": {
    "id": "2009.10195",
    "title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving   Out-of-Domain Robustness",
    "authors": "Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi",
    "abstract": "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
    "url": "https://arxiv.org/abs/2009.10195",
    "arxivId": "2009.10195",
    "last_visited": "2024-12-21T18:19:38.104000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-09-21T22:02:33Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ]
  },
  "2012.13255": {
    "id": "2012.13255",
    "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model   Fine-Tuning",
    "authors": "Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta",
    "abstract": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
    "url": "https://arxiv.org/abs/2012.13255",
    "arxivId": "2012.13255",
    "last_visited": "2024-12-30T14:48:45.896767",
    "last_read": "2024-12-30T14:48:45.896767",
    "total_reading_time_seconds": 7,
    "published_date": "2020-12-22T07:42:30Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2105.05720": {
    "id": "2105.05720",
    "title": "Breaking the Computation and Communication Abstraction Barrier in   Distributed Machine Learning Workloads",
    "authors": "Abhinav Jangda, Jun Huang, Guodong Liu and 6 others",
    "abstract": "Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone.   Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.",
    "url": "https://arxiv.org/abs/2105.05720",
    "arxivId": "2105.05720",
    "last_visited": "2024-12-30T08:26:40.761102",
    "last_read": "2024-12-30T08:26:40.761102",
    "total_reading_time_seconds": 38,
    "published_date": "2021-05-12T15:13:43Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.LG",
      "cs.PL"
    ]
  },
  "2106.04647": {
    "id": "2106.04647",
    "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
    "authors": "Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder",
    "abstract": "Adapting large-scale pretrained language models to downstream tasks via fine-tuning is the standard method for achieving state-of-the-art performance on NLP benchmarks. However, fine-tuning all weights of models with millions or billions of parameters is sample-inefficient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developed parameter-efficient fine-tuning methods, but these approaches either still require a relatively large number of parameters or underperform standard fine-tuning. In this work, we propose Compacter, a method for fine-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work. Compacter accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers. Specifically, Compacter inserts task-specific weight matrices into a pretrained model's weights, which are computed efficiently as a sum of Kronecker products between shared \"slow\" weights and \"fast\" rank-one matrices defined per Compacter layer. By only training 0.047% of a pretrained model's parameters, Compacter performs on par with standard fine-tuning on GLUE and outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our code is publicly available at~\\url{https://github.com/rabeehk/compacter}.",
    "url": "https://arxiv.org/abs/2106.04647",
    "arxivId": "2106.04647",
    "last_visited": "2024-12-30T14:48:49.363744",
    "last_read": "2024-12-30T14:48:49.363744",
    "total_reading_time_seconds": 3,
    "published_date": "2021-06-08T19:17:04Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2106.10165": {
    "id": "2106.10165",
    "title": "The Principles of Deep Learning Theory",
    "authors": "Daniel A. Roberts, Sho Yaida, Boris Hanin",
    "abstract": "This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",
    "url": "https://arxiv.org/abs/2106.10165",
    "arxivId": "2106.10165",
    "last_visited": "2024-12-29T22:46:13.679000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2021-06-18T15:00:00Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "hep-th",
      "stat.ML"
    ]
  },
  "2204.00595": {
    "id": "2204.00595",
    "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate   Training",
    "authors": "Tri Dao, Beidi Chen, Nimit Sohoni and 7 others",
    "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called \"reverse sparsification,\" Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
    "url": "https://arxiv.org/abs/2204.00595",
    "arxivId": "2204.00595",
    "last_visited": "2024-12-30T14:44:03.223396",
    "last_read": "2024-12-30T14:44:03.223396",
    "total_reading_time_seconds": 35,
    "published_date": "2022-04-01T17:37:29Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2207.10342": {
    "id": "2207.10342",
    "title": "Language Model Cascades",
    "authors": "David Dohan, Winnie Xu, Aitor Lewkowycz and 9 others",
    "abstract": "Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.",
    "url": "https://arxiv.org/abs/2207.10342",
    "arxivId": "2207.10342",
    "last_visited": "2024-12-29T08:48:42.548000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-07-21T07:35:18Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2208.11665": {
    "id": "2208.11665",
    "title": "Statistical exploration of the Manifold Hypothesis",
    "authors": "Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy",
    "abstract": "The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism. These procedures operate under minimal assumptions and make use of well known, scaleable graph-analytic algorithms.",
    "url": "https://arxiv.org/abs/2208.11665",
    "arxivId": "2208.11665",
    "last_visited": "2024-12-29T02:26:31.276000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-08-24T17:00:16Z",
    "arxiv_tags": [
      "stat.ME",
      "cs.LG",
      "stat.ML",
      "62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30"
    ]
  },
  "2209.02740": {
    "id": "2209.02740",
    "title": "Emergent hypernetworks in weakly coupled oscillators",
    "authors": "Eddie Nijholt, Jorge Luis Ocampo-Espindola, Deniz Eroglu and 2 others",
    "abstract": "Networks of weakly coupled oscillators had a profound impact on our understanding of complex systems. Studies on model reconstruction from data have shown prevalent contributions from hypernetworks with triplet and higher interactions among oscillators, in spite that such models were originally defined as oscillator networks with pairwise interactions. Here, we show that hypernetworks can spontaneously emerge even in the presence of pairwise albeit nonlinear coupling given certain triplet frequency resonance conditions. The results are demonstrated in experiments with electrochemical oscillators and in simulations with integrate-and-fire neurons. By developing a comprehensive theory, we uncover the mechanism for emergent hypernetworks by identifying appearing and forbidden frequency resonant conditions. Furthermore, it is shown that microscopic linear (difference) coupling among units results in coupled mean fields, which have sufficient nonlinearity to facilitate hypernetworks. Our findings shed light on the apparent abundance of hypernetworks and provide a constructive way to predict and engineer their emergence.",
    "url": "https://arxiv.org/abs/2209.02740",
    "arxivId": "2209.02740",
    "last_visited": "2024-12-22T05:30:04.148000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-09-06T18:02:12Z",
    "arxiv_tags": [
      "math.DS"
    ]
  },
  "2210.14891": {
    "id": "2210.14891",
    "title": "Broken Neural Scaling Laws",
    "authors": "Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger",
    "abstract": "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models & extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures & for each of various tasks within a large & diverse set of upstream & downstream tasks, in zero-shot, prompted, & finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems, \"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning, & reinforcement learning (single agent & multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models & extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent & the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",
    "url": "https://arxiv.org/abs/2210.14891",
    "arxivId": "2210.14891",
    "last_visited": "2024-12-21T05:51:08.748000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-10-26T17:45:01Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2212.07677": {
    "id": "2212.07677",
    "title": "Transformers learn in-context by gradient descent",
    "authors": "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo and 4 others",
    "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "url": "https://arxiv.org/abs/2212.07677",
    "arxivId": "2212.07677",
    "last_visited": "2024-12-21T08:19:06.235000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-12-15T09:21:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  "2302.04222": {
    "id": "2302.04222",
    "title": "Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models",
    "authors": "Shawn Shan, Jenna Cryan, Emily Wenger and 3 others",
    "abstract": "Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after \"fine-tuning\" on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply \"style cloaks\" to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%).",
    "url": "https://arxiv.org/abs/2302.04222",
    "arxivId": "2302.04222",
    "last_visited": "2024-12-22T16:18:52.405000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-02-08T17:45:23Z",
    "arxiv_tags": [
      "cs.CR"
    ]
  },
  "2302.05543": {
    "id": "2302.05543",
    "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
    "authors": "Lvmin Zhang, Anyi Rao, Maneesh Agrawala",
    "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
    "url": "https://arxiv.org/abs/2302.05543",
    "arxivId": "2302.05543",
    "last_visited": "2024-12-30T14:48:46.296877",
    "last_read": "2024-12-30T14:48:46.296877",
    "total_reading_time_seconds": 23,
    "published_date": "2023-02-10T23:12:37Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.MM"
    ]
  },
  "2302.10866": {
    "id": "2302.10866",
    "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
    "authors": "Michael Poli, Stefano Massaroli, Eric Nguyen and 6 others",
    "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
    "url": "https://arxiv.org/abs/2302.10866",
    "arxivId": "2302.10866",
    "last_visited": "2024-12-28T07:17:27.699000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-02-21T18:29:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2302.13714": {
    "id": "2302.13714",
    "title": "On the Design of Codes for DNA Computing: Secondary Structure Avoidance   Codes",
    "authors": "Tuan Thanh Nguyen, Kui Cai, Han Mao Kiah and 2 others",
    "abstract": "In this work, we investigate a challenging problem, which has been considered to be an important criterion in designing codewords for DNA computing purposes, namely secondary structure avoidance in single-stranded DNA molecules. In short, secondary structure refers to the tendency of a single-stranded DNA sequence to fold back upon itself, thus becoming inactive in the computation process. While some design criteria that reduces the possibility of secondary structure formation has been proposed by Milenkovic and Kashyap (2006), the main contribution of this work is to provide an explicit construction of DNA codes that completely avoid secondary structure of arbitrary stem length. Formally, given codeword length n and arbitrary integer m>=2, we provide efficient methods to construct DNA codes of length n that avoid secondary structure of any stem length more than or equal to m. Particularly, when m = 3, our constructions yield a family of DNA codes of rate 1.3031 bits/nt, while the highest rate found in the prior art was 1.1609 bits/nt. In addition, for m>=3log n + 4, we provide an efficient encoder that incurs only one redundant symbol.",
    "url": "https://arxiv.org/abs/2302.13714",
    "arxivId": "2302.13714",
    "last_visited": "2024-12-30T08:28:25.574539",
    "last_read": "2024-12-30T08:28:25.574539",
    "total_reading_time_seconds": 6,
    "published_date": "2023-02-27T12:22:07Z",
    "arxiv_tags": [
      "cs.IT",
      "math.CO",
      "math.IT"
    ]
  },
  "2303.08500": {
    "id": "2303.08500",
    "title": "The Devil's Advocate: Shattering the Illusion of Unexploitable Data   using Diffusion Models",
    "authors": "Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie",
    "abstract": "Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data \"unexploitable.\" This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: https://github.com/hmdolatabadi/AVATAR.",
    "url": "https://arxiv.org/abs/2303.08500",
    "arxivId": "2303.08500",
    "last_visited": "2024-12-22T16:41:41.995000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-03-15T10:20:49Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  "2303.09489": {
    "id": "2303.09489",
    "title": "Effectively Modeling Time Series with Simple Discrete State Spaces",
    "authors": "Michael Zhang, Khaled K. Saab, Michael Poli and 3 others",
    "abstract": "Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -- a canonical representation for discrete-time processes -- which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a \"closed-loop\" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layer-wise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length $\\ell$ and state-space size $d$, we go from $\\tilde{O}(d \\ell)$ na\\\"ively to $\\tilde{O}(d + \\ell)$. In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs.",
    "url": "https://arxiv.org/abs/2303.09489",
    "arxivId": "2303.09489",
    "last_visited": "2024-12-28T07:09:58.237000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-03-16T17:08:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2303.11435": {
    "id": "2303.11435",
    "title": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for   Image Restoration",
    "authors": "Mauricio Delbracio, Peyman Milanfar",
    "abstract": "Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called \"regression to the mean\" effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models. Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality. While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising.",
    "url": "https://arxiv.org/abs/2303.11435",
    "arxivId": "2303.11435",
    "last_visited": "2024-12-22T08:12:43.437000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-03-20T20:28:17Z",
    "arxiv_tags": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ]
  },
  "2304.02234": {
    "id": "2304.02234",
    "title": "JPEG Compressed Images Can Bypass Protections Against AI Editing",
    "authors": "Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein",
    "abstract": "Recently developed text-to-image diffusion models make it easy to edit or create high-quality images. Their ease of use has raised concerns about the potential for malicious editing or deepfake creation. Imperceptible perturbations have been proposed as a means of protecting images from malicious editing by preventing diffusion models from generating realistic images. However, we find that the aforementioned perturbations are not robust to JPEG compression, which poses a major weakness because of the common usage and availability of JPEG. We discuss the importance of robustness for additive imperceptible perturbations and encourage alternative approaches to protect images against editing.",
    "url": "https://arxiv.org/abs/2304.02234",
    "arxivId": "2304.02234",
    "last_visited": "2024-12-22T16:44:11.039000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-05T05:30:09Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  "2304.15004": {
    "id": "2304.15004",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "authors": "Rylan Schaeffer, Brando Miranda, Sanmi Koyejo",
    "abstract": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.",
    "url": "https://arxiv.org/abs/2304.15004",
    "arxivId": "2304.15004",
    "last_visited": "2024-12-21T06:06:11.226000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-28T17:52:11Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.LG"
    ]
  },
  "2305.06161": {
    "id": "2305.06161",
    "title": "StarCoder: may the source be with you!",
    "authors": "Raymond Li, Loubna Ben Allal, Yangtian Zi and 64 others",
    "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
    "url": "https://arxiv.org/abs/2305.06161",
    "arxivId": "2305.06161",
    "last_visited": "2024-12-30T14:43:30.206539",
    "last_read": "2024-12-30T14:43:30.206539",
    "total_reading_time_seconds": 12,
    "published_date": "2023-05-09T08:16:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.PL",
      "cs.SE"
    ]
  },
  "2305.13169": {
    "id": "2305.13169",
    "title": "A Pretrainer's Guide to Training Data: Measuring the Effects of Data   Age, Domain Coverage, Quality, & Toxicity",
    "authors": "Shayne Longpre, Gregory Yauney, Emily Reif and 8 others",
    "abstract": "Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.",
    "url": "https://arxiv.org/abs/2305.13169",
    "arxivId": "2305.13169",
    "last_visited": "2024-12-30T20:20:50.556000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-05-22T15:57:53Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2305.19693": {
    "id": "2305.19693",
    "title": "Spontaneous Symmetry Breaking in Generative Diffusion Models",
    "authors": "Gabriel Raya, Luca Ambrogioni",
    "abstract": "Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two \"phases\" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, while also increasing sample diversity (e.g., racial composition of generated CelebA images). Our work offers a new way to understand the generative dynamics of diffusion models that has the potential to bring about higher performance and less biased fast-samplers.",
    "url": "https://arxiv.org/abs/2305.19693",
    "arxivId": "2305.19693",
    "last_visited": "2024-12-15T09:58:39.593000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-05-31T09:36:34Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ]
  },
  "2307.08691": {
    "id": "2307.08691",
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work   Partitioning",
    "authors": "Tri Dao",
    "abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
    "url": "https://arxiv.org/abs/2307.08691",
    "arxivId": "2307.08691",
    "last_visited": "2024-12-28T07:08:37.022000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-07-17T17:50:36Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2307.09288": {
    "id": "2307.09288",
    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "authors": "Hugo Touvron, Louis Martin, Kevin Stone and 65 others",
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
    "url": "https://arxiv.org/abs/2307.09288",
    "arxivId": "2307.09288",
    "last_visited": "2024-12-30T08:28:19.537524",
    "last_read": "2024-12-30T08:28:19.537524",
    "total_reading_time_seconds": 5,
    "published_date": "2023-07-18T14:31:57Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2308.06259": {
    "id": "2308.06259",
    "title": "Self-Alignment with Instruction Backtranslation",
    "authors": "Xian Li, Ping Yu, Chunting Zhou and 5 others",
    "abstract": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
    "url": "https://arxiv.org/abs/2308.06259",
    "arxivId": "2308.06259",
    "last_visited": "2024-12-30T04:57:10.471000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-08-11T17:47:54Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2308.10718": {
    "id": "2308.10718",
    "title": "Backdooring Textual Inversion for Concept Censorship",
    "authors": "Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang",
    "abstract": "Recent years have witnessed success in AIGC (AI Generated Content). People can make use of a pre-trained diffusion model to generate images of high quality or freely modify existing pictures with only prompts in nature language. More excitingly, the emerging personalization techniques make it feasible to create specific-desired images with only a few images as references. However, this induces severe threats if such advanced techniques are misused by malicious users, such as spreading fake news or defaming individual reputations. Thus, it is necessary to regulate personalization models (i.e., concept censorship) for their development and advancement.   In this paper, we focus on the personalization technique dubbed Textual Inversion (TI), which is becoming prevailing for its lightweight nature and excellent performance. TI crafts the word embedding that contains detailed information about a specific object. Users can easily download the word embedding from public websites like Civitai and add it to their own stable diffusion model without fine-tuning for personalization. To achieve the concept censorship of a TI model, we propose leveraging the backdoor technique for good by injecting backdoors into the Textual Inversion embeddings. Briefly, we select some sensitive words as triggers during the training of TI, which will be censored for normal use. In the subsequent generation stage, if the triggers are combined with personalized embeddings as final prompts, the model will output a pre-defined target image rather than images including the desired malicious concept.   To demonstrate the effectiveness of our approach, we conduct extensive experiments on Stable Diffusion, a prevailing open-sourced text-to-image model. Our code, data, and results are available at https://concept-censorship.github.io.",
    "url": "https://arxiv.org/abs/2308.10718",
    "arxivId": "2308.10718",
    "last_visited": "2024-12-22T16:47:15.076000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-08-21T13:39:04Z",
    "arxiv_tags": [
      "cs.CR",
      "cs.CV"
    ]
  },
  "2309.07965": {
    "id": "2309.07965",
    "title": "A survey on relative Lipschitz saturation of algebras and its relation   with radicial algebras",
    "authors": "Thiago da Silva, Guilherme Schultz Netto",
    "abstract": "In this work, we introduce the concept of relative Lipschitz saturation, along with its key categorical and algebraic properties, and demonstrate how such a structure always gives rise to a radicial algebra.",
    "url": "https://arxiv.org/abs/2309.07965",
    "arxivId": "2309.07965",
    "last_visited": "2024-12-30T08:28:28.532202",
    "last_read": "2024-12-30T08:28:28.532202",
    "total_reading_time_seconds": 6,
    "published_date": "2023-09-14T18:02:12Z",
    "arxiv_tags": [
      "math.AC",
      "13B22"
    ]
  },
  "2309.12032": {
    "id": "2309.12032",
    "title": "Human-in-the-Loop Causal Discovery under Latent Confounding using   Ancestral GFlowNets",
    "authors": "Tiago da Silva, Eliezer Silva, António Góis and 4 others",
    "abstract": "Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid.",
    "url": "https://arxiv.org/abs/2309.12032",
    "arxivId": "2309.12032",
    "last_visited": "2024-12-30T14:43:36.200393",
    "last_read": "2024-12-30T14:43:36.200393",
    "total_reading_time_seconds": 12,
    "published_date": "2023-09-21T12:53:45Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "2309.14556": {
    "id": "2309.14556",
    "title": "Art or Artifice? Large Language Models and the False Promise of   Creativity",
    "authors": "Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal and 2 others",
    "abstract": "Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.",
    "url": "https://arxiv.org/abs/2309.14556",
    "arxivId": "2309.14556",
    "last_visited": "2024-12-30T14:42:55.576000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-09-25T22:02:46Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ]
  },
  "2310.01889": {
    "id": "2310.01889",
    "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
    "authors": "Hao Liu, Matei Zaharia, Pieter Abbeel",
    "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
    "url": "https://arxiv.org/abs/2310.01889",
    "arxivId": "2310.01889",
    "last_visited": "2025-01-02T07:41:48.644101",
    "last_read": "2025-01-02T07:41:48.644101",
    "total_reading_time_seconds": 40,
    "published_date": "2023-10-03T08:44:50Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2310.17157": {
    "id": "2310.17157",
    "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
    "authors": "Zichang Liu, Jue Wang, Tri Dao and 8 others",
    "abstract": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.",
    "url": "https://arxiv.org/abs/2310.17157",
    "arxivId": "2310.17157",
    "last_visited": "2024-12-30T14:43:39.229852",
    "last_read": "2024-12-30T14:43:39.229852",
    "total_reading_time_seconds": 36,
    "published_date": "2023-10-26T05:01:09Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2311.17910": {
    "id": "2311.17910",
    "title": "HUGS: Human Gaussian Splats",
    "authors": "Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel and 2 others",
    "abstract": "Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs",
    "url": "https://arxiv.org/abs/2311.17910",
    "arxivId": "2311.17910",
    "last_visited": "2024-12-15T08:36:54.412000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-11-29T18:56:32Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR"
    ]
  },
  "2312.00330": {
    "id": "2312.00330",
    "title": "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style   Adapter",
    "authors": "Gongye Liu, Menghan Xia, Yong Zhang and 6 others",
    "abstract": "Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired stylized videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image. Considering the scarcity of stylized video datasets, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm. To promote content-style disentanglement, we remove style descriptions from the text prompt and extract style information solely from the reference image using a decoupling learning strategy. Additionally, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. StyleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Experiments demonstrate that our approach is more flexible and efficient than existing competitors.",
    "url": "https://arxiv.org/abs/2312.00330",
    "arxivId": "2312.00330",
    "last_visited": "2024-12-15T20:33:49.865000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-01T03:53:21Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI"
    ]
  },
  "2312.00752": {
    "id": "2312.00752",
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "authors": "Albert Gu, Tri Dao",
    "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
    "url": "https://arxiv.org/abs/2312.00752",
    "arxivId": "2312.00752",
    "last_visited": "2024-12-28T06:55:41.314000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-01T18:01:34Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2312.07731": {
    "id": "2312.07731",
    "title": "A Response to Glaze Purification via IMPRESS",
    "authors": "Shawn Shan, Stanley Wu, Haitao Zheng, Ben Y. Zhao",
    "abstract": "Recent work proposed a new mechanism to remove protective perturbation added by Glaze in order to again enable mimicry of art styles from images protected by Glaze. Despite promising results shown in the original paper, our own tests with the authors' code demonstrated several limitations of the proposed purification approach. The main limitations are 1) purification has a limited effect when tested on artists that are not well-known historical artists already embedded in original training data, 2) problems in evaluation metrics, and 3) collateral damage on mimicry result for clean images. We believe these limitations should be carefully considered in order to understand real world usability of the purification attack.",
    "url": "https://arxiv.org/abs/2312.07731",
    "arxivId": "2312.07731",
    "last_visited": "2024-12-22T16:49:35.138000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-12T20:52:27Z",
    "arxiv_tags": [
      "cs.CR"
    ]
  },
  "2312.17127": {
    "id": "2312.17127",
    "title": "Probabilistic programming interfaces for random graphs: Markov   categories, graphons, and nominal sets",
    "authors": "Nathanael L. Ackerman, Cameron E. Freer, Younesse Kaddar and 5 others",
    "abstract": "We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics. We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.   We provide three constructions for showing that every graphon arises from an equational theory. The first is an abstract construction, using Markov categories and monoidal indeterminates. The second and third are more concrete. The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons. The third is in terms of probability monads on the nominal sets of Gabbay and Pitts. Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\'enyi graphons. In this way, we build new models of graph probabilistic programming from graphons.",
    "url": "https://arxiv.org/abs/2312.17127",
    "arxivId": "2312.17127",
    "last_visited": "2024-12-30T08:27:28.538601",
    "last_read": "2024-12-30T08:27:28.538601",
    "total_reading_time_seconds": 10,
    "published_date": "2023-12-28T17:04:50Z",
    "arxiv_tags": [
      "cs.PL",
      "cs.LO",
      "math.PR"
    ]
  },
  "2401.10774": {
    "id": "2401.10774",
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple   Decoding Heads",
    "authors": "Tianle Cai, Yuhong Li, Zhengyang Geng and 4 others",
    "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.",
    "url": "https://arxiv.org/abs/2401.10774",
    "arxivId": "2401.10774",
    "last_visited": "2024-12-30T08:28:31.553017",
    "last_read": "2024-12-30T08:28:31.553017",
    "total_reading_time_seconds": 20,
    "published_date": "2024-01-19T15:48:40Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2402.03239": {
    "id": "2402.03239",
    "title": "Bluesky and the AT Protocol: Usable Decentralized Social Media",
    "authors": "Martin Kleppmann, Paul Frazee, Jake Gold and 6 others",
    "abstract": "Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media. It was launched in private beta in February 2023, and has grown to over 10 million registered users by October 2024. In this paper we introduce the architecture of Bluesky and the AT Protocol, and explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature. The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation.",
    "url": "https://arxiv.org/abs/2402.03239",
    "arxivId": "2402.03239",
    "last_visited": "2024-12-22T05:41:38.653000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-05T17:55:51Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.SI"
    ]
  },
  "2402.10193": {
    "id": "2402.10193",
    "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
    "authors": "James Liu, Guangxuan Xiao, Kai Li and 4 others",
    "abstract": "Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings.",
    "url": "https://arxiv.org/abs/2402.10193",
    "arxivId": "2402.10193",
    "last_visited": "2024-12-30T14:43:45.227170",
    "last_read": "2024-12-30T14:43:45.227170",
    "total_reading_time_seconds": 73,
    "published_date": "2024-02-15T18:50:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2402.14903": {
    "id": "2402.14903",
    "title": "Tokenization counts: the impact of tokenization on arithmetic in   frontier LLMs",
    "authors": "Aaditya K. Singh, DJ Strouse",
    "abstract": "Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.",
    "url": "https://arxiv.org/abs/2402.14903",
    "arxivId": "2402.14903",
    "last_visited": "2024-12-19T22:43:04.367000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-22T18:14:09Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2402.16670": {
    "id": "2402.16670",
    "title": "Pay Attention: a Call to Regulate the Attention Market and Prevent   Algorithmic Emotional Governance",
    "authors": "Franck Michel, Fabien Gandon",
    "abstract": "Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.",
    "url": "https://arxiv.org/abs/2402.16670",
    "arxivId": "2402.16670",
    "last_visited": "2024-12-27T22:07:23.174000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-26T15:46:43Z",
    "arxiv_tags": [
      "cs.SI"
    ]
  },
  "2402.19173": {
    "id": "2402.19173",
    "title": "StarCoder 2 and The Stack v2: The Next Generation",
    "authors": "Anton Lozhkov, Raymond Li, Loubna Ben Allal and 63 others",
    "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
    "url": "https://arxiv.org/abs/2402.19173",
    "arxivId": "2402.19173",
    "last_visited": "2024-12-30T14:43:48.214522",
    "last_read": "2024-12-30T14:43:48.214522",
    "total_reading_time_seconds": 9,
    "published_date": "2024-02-29T13:53:35Z",
    "arxiv_tags": [
      "cs.SE",
      "cs.AI"
    ]
  },
  "2403.00231": {
    "id": "2403.00231",
    "title": "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of   Large Vision-Language Models",
    "authors": "Lei Li, Yuqi Wang, Runxin Xu and 4 others",
    "abstract": "Large vision-language models (LVLMs) excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances open-sourced LVLMs' mathematical reasoning capabilities, achieving a 10.4\\% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs. Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, while domain-specific training yields substantial performance gains. Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.",
    "url": "https://arxiv.org/abs/2403.00231",
    "arxivId": "2403.00231",
    "last_visited": "2024-12-30T14:44:18.199164",
    "last_read": "2024-12-30T14:44:18.199164",
    "total_reading_time_seconds": 8,
    "published_date": "2024-03-01T02:21:30Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.CL"
    ]
  },
  "2403.03353": {
    "id": "2403.03353",
    "title": "Hypothesis Spaces for Deep Learning",
    "authors": "Rui Wang, Yuesheng Xu, Mingsong Yan",
    "abstract": "This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel.",
    "url": "https://arxiv.org/abs/2403.03353",
    "arxivId": "2403.03353",
    "last_visited": "2024-12-30T08:27:46.543045",
    "last_read": "2024-12-30T08:27:46.543045",
    "total_reading_time_seconds": 13,
    "published_date": "2024-03-05T22:42:29Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "math.FA"
    ]
  },
  "2403.08540": {
    "id": "2403.08540",
    "title": "Language models scale reliably with over-training and on downstream   tasks",
    "authors": "Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar and 22 others",
    "abstract": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
    "url": "https://arxiv.org/abs/2403.08540",
    "arxivId": "2403.08540",
    "last_visited": "2024-12-30T20:09:59.182254",
    "last_read": "2024-12-30T20:09:59.182254",
    "total_reading_time_seconds": 46,
    "published_date": "2024-03-13T13:54:00Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2403.10304": {
    "id": "2403.10304",
    "title": "KIF: A Wikidata-Based Framework for Integrating Heterogeneous Knowledge   Sources",
    "authors": "Guilherme Lima, João M. B. Rodrigues, Marcelo Machado and 6 others",
    "abstract": "We present a Wikidata-based framework, called KIF, for virtually integrating heterogeneous knowledge sources. KIF is written in Python and is released as open-source. It leverages Wikidata's data model and vocabulary plus user-defined mappings to construct a unified view of the underlying sources while keeping track of the context and provenance of their statements. The underlying sources can be triplestores, relational databases, CSV files, etc., which may or may not use the vocabulary and RDF encoding of Wikidata. The end result is a virtual knowledge base which behaves like an \"extended Wikidata\" and which can be queried using a simple but expressive pattern language, defined in terms of Wikidata's data model. In this paper, we present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF",
    "url": "https://arxiv.org/abs/2403.10304",
    "arxivId": "2403.10304",
    "last_visited": "2024-12-28T06:48:13.707000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-03-15T13:46:36Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.DB"
    ]
  },
  "2403.12187": {
    "id": "2403.12187",
    "title": "Approximation of RKHS Functionals by Neural Networks",
    "authors": "Tian-Yi Zhou, Namjoon Suh, Guang Cheng, Xiaoming Huo",
    "abstract": "Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is much simpler in that we use point evaluations to replace basis function expansions.",
    "url": "https://arxiv.org/abs/2403.12187",
    "arxivId": "2403.12187",
    "last_visited": "2024-12-30T08:27:40.565450",
    "last_read": "2024-12-30T08:27:40.565450",
    "total_reading_time_seconds": 10,
    "published_date": "2024-03-18T18:58:23Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ]
  },
  "2403.14554": {
    "id": "2403.14554",
    "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering",
    "authors": "Antoine Guédon, Vincent Lepetit",
    "abstract": "We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: https://anttwo.github.io/frosting/",
    "url": "https://arxiv.org/abs/2403.14554",
    "arxivId": "2403.14554",
    "last_visited": "2025-01-03T02:52:11.974000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-03-21T16:53:03Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR"
    ]
  },
  "2403.14781": {
    "id": "2403.14781",
    "title": "Champ: Controllable and Consistent Human Image Animation with 3D   Parametric Guidance",
    "authors": "Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai and 6 others",
    "abstract": "In this study, we introduce a methodology for human image animation by leveraging a 3D human parametric model within a latent diffusion framework to enhance shape alignment and motion guidance in curernt human generative techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear) model as the 3D human parametric model to establish a unified representation of body shape and pose. This facilitates the accurate capture of intricate human geometry and motion characteristics from source videos. Specifically, we incorporate rendered depth images, normal maps, and semantic maps obtained from SMPL sequences, alongside skeleton-based motion guidance, to enrich the conditions to the latent diffusion model with comprehensive 3D shape and detailed pose attributes. A multi-layer motion fusion module, integrating self-attention mechanisms, is employed to fuse the shape and motion latent representations in the spatial domain. By representing the 3D human parametric model as the motion guidance, we can perform parametric shape alignment of the human body between the reference image and the source video motion. Experimental evaluations conducted on benchmark datasets demonstrate the methodology's superior ability to generate high-quality human animations that accurately capture both pose and shape variations. Furthermore, our approach also exhibits superior generalization capabilities on the proposed in-the-wild dataset. Project page: https://fudan-generative-vision.github.io/champ.",
    "url": "https://arxiv.org/abs/2403.14781",
    "arxivId": "2403.14781",
    "last_visited": "2024-12-23T21:24:01.786000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-03-21T18:52:58Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2404.00399": {
    "id": "2404.00399",
    "title": "Aurora-M: Open Source Continual Pre-training for Multilingual Language   and Code",
    "authors": "Taishi Nakamura, Mayank Mishra, Simone Tedeschi and 42 others",
    "abstract": "Pretrained language models are an integral part of AI applications, but their high computational cost for training limits accessibility. Initiatives such as Bloom and StarCoder aim to democratize access to pretrained models for collaborative community development. Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks.   This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.   We evaluate Aurora-M across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source Aurora-M and its variants to encourage responsible open-source development of large language models at https://huggingface.co/aurora-m.",
    "url": "https://arxiv.org/abs/2404.00399",
    "arxivId": "2404.00399",
    "last_visited": "2024-12-30T21:17:55.181058",
    "last_read": "2024-12-30T21:17:55.181058",
    "total_reading_time_seconds": 28,
    "published_date": "2024-03-30T15:38:54Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2404.13320": {
    "id": "2404.13320",
    "title": "Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than   We Think",
    "authors": "Haotian Xue, Yongxin Chen",
    "abstract": "Adversarial examples for diffusion models are widely used as solutions for safety concerns. By adding adversarial perturbations to personal images, attackers can not edit or imitate them easily. However, it is essential to note that all these protections target the latent diffusion model (LDMs), the adversarial examples for diffusion models in the pixel space (PDMs) are largely overlooked. This may mislead us to think that the diffusion models are vulnerable to adversarial attacks like most deep models. In this paper, we show novel findings that: even though gradient-based white-box attacks can be used to attack the LDMs, they fail to attack PDMs. This finding is supported by extensive experiments of almost a wide range of attacking methods on various PDMs and LDMs with different model structures, which means diffusion models are indeed much more robust against adversarial attacks. We also find that PDMs can be used as an off-the-shelf purifier to effectively remove the adversarial patterns that were generated on LDMs to protect the images, which means that most protection methods nowadays, to some extent, cannot protect our images from malicious attacks. We hope that our insights will inspire the community to rethink the adversarial samples for diffusion models as protection methods and move forward to more effective protection. Codes are available in https://github.com/xavihart/PDM-Pure.",
    "url": "https://arxiv.org/abs/2404.13320",
    "arxivId": "2404.13320",
    "last_visited": "2024-12-22T16:33:17.796000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-04-20T08:28:43Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI"
    ]
  },
  "2405.04434": {
    "id": "2405.04434",
    "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts   Language Model",
    "authors": "DeepSeek-AI, Aixin Liu, Bei Feng and 154 others",
    "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
    "url": "https://arxiv.org/abs/2405.04434",
    "arxivId": "2405.04434",
    "last_visited": "2024-12-30T14:44:24.211461",
    "last_read": "2024-12-30T14:44:24.211461",
    "total_reading_time_seconds": 46,
    "published_date": "2024-05-07T15:56:43Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2405.04517": {
    "id": "2405.04517",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "authors": "Maximilian Beck, Korbinian Pöppel, Markus Spanring and 6 others",
    "abstract": "In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.",
    "url": "https://arxiv.org/abs/2405.04517",
    "arxivId": "2405.04517",
    "last_visited": "2024-12-30T20:16:05.678195",
    "last_read": "2024-12-30T20:16:05.678195",
    "total_reading_time_seconds": 34,
    "published_date": "2024-05-07T17:50:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  "2405.06865": {
    "id": "2405.06865",
    "title": "Disrupting Style Mimicry Attacks on Video Imagery",
    "authors": "Josephine Passananti, Stanley Wu, Shawn Shan and 2 others",
    "abstract": "Generative AI models are often used to perform mimicry attacks, where a pretrained model is fine-tuned on a small sample of images to learn to mimic a specific artist of interest. While researchers have introduced multiple anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence points to a growing trend of mimicry models using videos as sources of training data. This paper presents our experiences exploring techniques to disrupt style mimicry on video imagery. We first validate that mimicry attacks can succeed by training on individual frames extracted from videos. We show that while anti-mimicry tools can offer protection when applied to individual frames, this approach is vulnerable to an adaptive countermeasure that removes protection by exploiting randomness in optimization results of consecutive (nearly-identical) frames. We develop a new, tool-agnostic framework that segments videos into short scenes based on frame-level similarity, and use a per-scene optimization baseline to remove inter-frame randomization while reducing computational cost. We show via both image level metrics and an end-to-end user study that the resulting protection restores protection against mimicry (including the countermeasure). Finally, we develop another adaptive countermeasure and find that it falls short against our framework.",
    "url": "https://arxiv.org/abs/2405.06865",
    "arxivId": "2405.06865",
    "last_visited": "2024-12-22T16:37:12.908000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-11T01:40:19Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.CR"
    ]
  },
  "2405.07987": {
    "id": "2405.07987",
    "title": "The Platonic Representation Hypothesis",
    "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",
    "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.",
    "url": "https://arxiv.org/abs/2405.07987",
    "arxivId": "2405.07987",
    "last_visited": "2024-12-29T01:39:54.598000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-13T17:58:30Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ]
  },
  "2405.13954": {
    "id": "2405.13954",
    "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence   Functions",
    "authors": "Sang Keun Choe, Hwijeen Ahn, Juhan Bae and 11 others",
    "abstract": "Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited. In response to this issue, data valuation (or data attribution), which quantifies the contribution or value of each data to the model output, has been discussed as a potential solution. Nevertheless, applying existing data valuation methods to recent LLMs and their vast training datasets has been largely limited by prohibitive compute and memory costs. In this work, we focus on influence functions, a popular gradient-based data valuation method, and significantly improve its scalability with an efficient gradient projection strategy called LoGra that leverages the gradient structure in backpropagation. We then provide a theoretical motivation of gradient projection approaches to influence functions to promote trust in the data valuation process. Lastly, we lower the barrier to implementing data valuation systems by introducing LogIX, a software package that can transform existing training code into data valuation code with minimal effort. In our data valuation experiments, LoGra achieves competitive accuracy against more expensive baselines while showing up to 6,500x improvement in throughput and 5x reduction in GPU memory usage when applied to Llama3-8B-Instruct and the 1B-token dataset.",
    "url": "https://arxiv.org/abs/2405.13954",
    "arxivId": "2405.13954",
    "last_visited": "2024-12-30T21:11:50.214396",
    "last_read": "2024-12-30T21:11:50.214396",
    "total_reading_time_seconds": 30,
    "published_date": "2024-05-22T19:39:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  "2405.15523": {
    "id": "2405.15523",
    "title": "Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language   Models",
    "authors": "Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye",
    "abstract": "The immense datasets used to develop Large Language Models (LLMs) often include copyright-protected content, typically without the content creator's consent. Copyright traps have been proposed to be injected into the original content, improving content detectability in newly released LLMs. Traps, however, rely on the exact duplication of a unique text sequence, leaving them vulnerable to commonly deployed data deduplication techniques. We here propose the generation of fuzzy copyright traps, featuring slight modifications across duplication. When injected in the fine-tuning data of a 1.3B LLM, we show fuzzy trap sequences to be memorized nearly as well as exact duplicates. Specifically, the Membership Inference Attack (MIA) ROC AUC only drops from 0.90 to 0.87 when 4 tokens are replaced across the fuzzy duplicates. We also find that selecting replacement positions to minimize the exact overlap between fuzzy duplicates leads to similar memorization, while making fuzzy duplicates highly unlikely to be removed by any deduplication process. Lastly, we argue that the fact that LLMs memorize across fuzzy duplicates challenges the study of LLM memorization relying on naturally occurring duplicates. Indeed, we find that the commonly used training dataset, The Pile, contains significant amounts of fuzzy duplicates. This introduces a previously unexplored confounding factor in post-hoc studies of LLM memorization, and questions the effectiveness of (exact) data deduplication as a privacy protection technique.",
    "url": "https://arxiv.org/abs/2405.15523",
    "arxivId": "2405.15523",
    "last_visited": "2024-12-30T20:02:03.116000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-24T13:05:05Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2405.16567": {
    "id": "2405.16567",
    "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
    "authors": "Minseon Kim, Hyomin Lee, Boqing Gong and 2 others",
    "abstract": "Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs). At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking. However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked. In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts. From this empirical study, we find that Copilot and Gemini block only 12% and 17% of the attacks with naive prompts, respectively, while ChatGPT blocks 84% of them. Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards. Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation. Surprisingly, our simple yet effective approach successfully jailbreaks the ChatGPT with 11.0% block rate, making it generate copyrighted contents in 76% of the time. Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms.",
    "url": "https://arxiv.org/abs/2405.16567",
    "arxivId": "2405.16567",
    "last_visited": "2024-12-22T16:31:57.733000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-26T13:32:24Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CR"
    ]
  },
  "2405.17399": {
    "id": "2405.17399",
    "title": "Transformers Can Do Arithmetic with the Right Embeddings",
    "authors": "Sean McLeish, Arpit Bansal, Alex Stein and 8 others",
    "abstract": "The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.   With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.",
    "url": "https://arxiv.org/abs/2405.17399",
    "arxivId": "2405.17399",
    "last_visited": "2024-12-19T22:43:44.883000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-27T17:49:18Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2405.20053": {
    "id": "2405.20053",
    "title": "Would I Lie To You? Inference Time Alignment of Language Models using   Direct Preference Heads",
    "authors": "Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic",
    "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.",
    "url": "https://arxiv.org/abs/2405.20053",
    "arxivId": "2405.20053",
    "last_visited": "2024-12-15T17:25:17.781000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-30T13:38:52Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2405.21060": {
    "id": "2405.21060",
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms   Through Structured State Space Duality",
    "authors": "Tri Dao, Albert Gu",
    "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "url": "https://arxiv.org/abs/2405.21060",
    "arxivId": "2405.21060",
    "last_visited": "2024-12-30T14:43:54.222879",
    "last_read": "2024-12-30T14:43:54.222879",
    "total_reading_time_seconds": 7,
    "published_date": "2024-05-31T17:50:01Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2406.01506": {
    "id": "2406.01506",
    "title": "The Geometry of Categorical and Hierarchical Concepts in Large Language   Models",
    "authors": "Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch",
    "abstract": "The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) as directions in representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as vectors. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet.",
    "url": "https://arxiv.org/abs/2406.01506",
    "arxivId": "2406.01506",
    "last_visited": "2024-12-29T01:39:53.364000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-06-03T16:34:01Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  "2406.01981": {
    "id": "2406.01981",
    "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
    "authors": "Yury Tokpanov, Beren Millidge, Paolo Glorioso and 4 others",
    "abstract": "The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.",
    "url": "https://arxiv.org/abs/2406.01981",
    "arxivId": "2406.01981",
    "last_visited": "2024-12-30T21:42:20.891633",
    "last_read": "2024-12-30T21:42:20.891633",
    "total_reading_time_seconds": 8,
    "published_date": "2024-06-04T05:47:17Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2406.10670": {
    "id": "2406.10670",
    "title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language   Model Pre-training",
    "authors": "David Brandfonbrener, Hanlin Zhang, Andreas Kirsch and 2 others",
    "abstract": "Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.   In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.   Code: https://github.com/davidbrandfonbrener/color-filter-olmo   Filtered data: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4",
    "url": "https://arxiv.org/abs/2406.10670",
    "arxivId": "2406.10670",
    "last_visited": "2024-12-30T20:04:09.709497",
    "last_read": "2024-12-30T20:04:09.709497",
    "total_reading_time_seconds": 7,
    "published_date": "2024-06-15T15:28:02Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  "2406.12027": {
    "id": "2406.12027",
    "title": "Adversarial Perturbations Cannot Reliably Protect Artists From   Generative AI",
    "authors": "Robert Hönig, Javier Rando, Nicholas Carlini, Florian Tramèr",
    "abstract": "Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections -- with millions of downloads -- and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that all existing protections can be easily bypassed, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative non-technological solutions.",
    "url": "https://arxiv.org/abs/2406.12027",
    "arxivId": "2406.12027",
    "last_visited": "2024-12-22T16:38:53.342000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-06-17T18:51:45Z",
    "arxiv_tags": [
      "cs.CR"
    ]
  },
  "2406.16746": {
    "id": "2406.16746",
    "title": "The Responsible Foundation Model Development Cheatsheet: A Review of   Tools & Resources",
    "authors": "Shayne Longpre, Stella Biderman, Alon Albalak and 20 others",
    "abstract": "Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.",
    "url": "https://arxiv.org/abs/2406.16746",
    "arxivId": "2406.16746",
    "last_visited": "2024-12-30T21:04:12.761011",
    "last_read": "2024-12-30T21:04:12.761011",
    "total_reading_time_seconds": 22,
    "published_date": "2024-06-24T15:55:49Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  "2407.01392": {
    "id": "2407.01392",
    "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
    "authors": "Boyuan Chen, Diego Marti Monso, Yilun Du and 3 others",
    "abstract": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing",
    "url": "https://arxiv.org/abs/2407.01392",
    "arxivId": "2407.01392",
    "last_visited": "2024-12-30T14:44:15.206543",
    "last_read": "2024-12-30T14:44:15.206543",
    "total_reading_time_seconds": 34,
    "published_date": "2024-07-01T15:43:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "cs.RO"
    ]
  },
  "2407.01492": {
    "id": "2407.01492",
    "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
    "authors": "Qian Liu, Xiaosen Zheng, Niklas Muennighoff and 5 others",
    "abstract": "The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at https://github.com/sail-sg/regmix.",
    "url": "https://arxiv.org/abs/2407.01492",
    "arxivId": "2407.01492",
    "last_visited": "2024-12-30T20:19:02.128885",
    "last_read": "2024-12-30T20:19:02.128885",
    "total_reading_time_seconds": 56,
    "published_date": "2024-07-01T17:31:03Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2407.05872": {
    "id": "2407.05872",
    "title": "Scaling Exponents Across Parameterizations and Optimizers",
    "authors": "Katie Everett, Lechao Xiao, Mitchell Wortsman and 8 others",
    "abstract": "Robust and effective scaling of models from small to large width typically requires the precise adjustment of many algorithmic and architectural details, such as parameterization and optimizer choices. In this work, we propose a new perspective on parameterization by investigating a key assumption in prior work about the alignment between parameters and data and derive new theoretical results under weaker assumptions and a broader set of optimizers. Our extensive empirical investigation includes tens of thousands of models trained with all combinations of three optimizers, four parameterizations, several alignment assumptions, more than a dozen learning rates, and fourteen model sizes up to 26.8B parameters. We find that the best learning rate scaling prescription would often have been excluded by the assumptions in prior work. Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer; moreover, our novel per-layer learning rate prescription for standard parameterization outperforms muP. Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow and propose Adam-atan2, a new numerically stable, scale-invariant version of Adam that eliminates the epsilon hyperparameter entirely.",
    "url": "https://arxiv.org/abs/2407.05872",
    "arxivId": "2407.05872",
    "last_visited": "2024-12-30T14:44:30.236437",
    "last_read": "2024-12-30T14:44:30.236437",
    "total_reading_time_seconds": 4,
    "published_date": "2024-07-08T12:32:51Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2407.08608": {
    "id": "2407.08608",
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and   Low-precision",
    "authors": "Jay Shah, Ganesh Bikshandi, Ying Zhang and 3 others",
    "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention.",
    "url": "https://arxiv.org/abs/2407.08608",
    "arxivId": "2407.08608",
    "last_visited": "2024-12-30T14:43:57.200892",
    "last_read": "2024-12-30T14:43:57.200892",
    "total_reading_time_seconds": 3,
    "published_date": "2024-07-11T15:44:48Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2407.17465": {
    "id": "2407.17465",
    "title": "u-$μ$P: The Unit-Scaled Maximal Update Parametrization",
    "authors": "Charlie Blake, Constantin Eichenberg, Josef Dean and 7 others",
    "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: $\\mu$P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-$\\mu$P models reaching a loss that is equal to or lower than comparable $\\mu$P models and working out-of-the-box in FP8.",
    "url": "https://arxiv.org/abs/2407.17465",
    "arxivId": "2407.17465",
    "last_visited": "2024-12-19T21:12:13.293000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-24T17:58:42Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2407.20177": {
    "id": "2407.20177",
    "title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for   Training LLMs",
    "authors": "Feiyang Kang, Yifan Sun, Bingbing Wen and 4 others",
    "abstract": "Domain reweighting is an emerging research area aimed at adjusting the relative weights of different data sources to improve the effectiveness and efficiency of language model pre-training. This paper demonstrates that the optimal composition of training data from different domains is scale-dependent, challenging the existing practice of determining optimal mixtures through small-scale experiments and directly applying them at larger scales. We derive an analytical model for the dependence of optimal weights on data scale and introduce *AutoScale*, a novel, practical approach for optimizing data compositions at potentially large training data scales. *AutoScale* first uses a principled optimization framework to find optimal compositions at smaller, feasible scales, then predicts optimal compositions at larger scales using our derived model. Our evaluation on GPT-2 Large and BERT pre-training demonstrates *AutoScale*'s effectiveness in improving training convergence and downstream performance. Particularly, for GPT-2 Large on RedPajama, *AutoScale* decreases validation perplexity 28% faster than baselines, with up to 38% speed-up over unweighted training, achieving the best performance across downstream tasks. This work provides insights into the varying benefits of data sources across training scales for language models, contributing to the burgeoning research on scale-dependent data curation. Code is open-sourced.",
    "url": "https://arxiv.org/abs/2407.20177",
    "arxivId": "2407.20177",
    "last_visited": "2024-12-30T21:14:30.006913",
    "last_read": "2024-12-30T21:14:30.006913",
    "total_reading_time_seconds": 41,
    "published_date": "2024-07-29T17:06:30Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ]
  },
  "2407.21783": {
    "id": "2407.21783",
    "title": "The Llama 3 Herd of Models",
    "authors": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri and 558 others",
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "url": "https://arxiv.org/abs/2407.21783",
    "arxivId": "2407.21783",
    "last_visited": "2024-12-30T21:58:38.440000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-31T17:54:27Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  "2407.21787": {
    "id": "2407.21787",
    "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
    "authors": "Bradley Brown, Jordan Juravsky, Ryan Ehrlich and 4 others",
    "abstract": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.",
    "url": "https://arxiv.org/abs/2407.21787",
    "arxivId": "2407.21787",
    "last_visited": "2024-12-30T08:27:25.560109",
    "last_read": "2024-12-30T08:27:25.560109",
    "total_reading_time_seconds": 17,
    "published_date": "2024-07-31T17:57:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2408.03314": {
    "id": "2408.03314",
    "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than   Scaling Model Parameters",
    "authors": "Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
    "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
    "url": "https://arxiv.org/abs/2408.03314",
    "arxivId": "2408.03314",
    "last_visited": "2024-12-30T08:27:25.558565",
    "last_read": "2024-12-30T08:27:25.558565",
    "total_reading_time_seconds": 39,
    "published_date": "2024-08-06T17:35:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2408.04809": {
    "id": "2408.04809",
    "title": "On the Geometry of Deep Learning",
    "authors": "Randall Balestriero, Ahmed Imtiaz Humayun, Richard Baraniuk",
    "abstract": "In this paper, we overview one promising avenue of progress at the mathematical foundation of deep learning: the connection between deep networks and function approximation by affine splines (continuous piecewise linear functions in multiple dimensions). In particular, we will overview work over the past decade on understanding certain geometrical properties of a deep network's affine spline mapping, in particular how it tessellates its input space. As we will see, the affine spline connection and geometrical viewpoint provide a powerful portal through which to view, analyze, and improve the inner workings of a deep network.",
    "url": "https://arxiv.org/abs/2408.04809",
    "arxivId": "2408.04809",
    "last_visited": "2024-12-30T08:27:31.804366",
    "last_read": "2024-12-30T08:27:31.804366",
    "total_reading_time_seconds": 29,
    "published_date": "2024-08-09T01:40:12Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  "2408.06072": {
    "id": "2408.06072",
    "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
    "authors": "Zhuoyi Yang, Jiayan Teng, Wendi Zheng and 16 others",
    "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.",
    "url": "https://arxiv.org/abs/2408.06072",
    "arxivId": "2408.06072",
    "last_visited": "2024-12-30T15:19:26.618045",
    "last_read": "2024-12-30T15:19:26.618045",
    "total_reading_time_seconds": 26,
    "published_date": "2024-08-12T11:47:11Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2408.06663": {
    "id": "2408.06663",
    "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and   Fine-Tuning of Large Language Models",
    "authors": "Kaiser Sun, Mark Dredze",
    "abstract": "The development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the model resembles high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated by more pre-training.",
    "url": "https://arxiv.org/abs/2408.06663",
    "arxivId": "2408.06663",
    "last_visited": "2024-12-30T21:09:12.920713",
    "last_read": "2024-12-30T21:09:12.920713",
    "total_reading_time_seconds": 29,
    "published_date": "2024-08-13T06:28:43Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2408.08252": {
    "id": "2408.08252",
    "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models   with Soft Value-Based Decoding",
    "authors": "Xiner Li, Yulai Zhao, Chenyu Wang and 8 others",
    "abstract": "Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require ``differentiable'' proxy models (\\textit{e.g.}, classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at \\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.",
    "url": "https://arxiv.org/abs/2408.08252",
    "arxivId": "2408.08252",
    "last_visited": "2025-01-02T19:16:07.604355",
    "last_read": "2025-01-02T19:16:07.604355",
    "total_reading_time_seconds": 30,
    "published_date": "2024-08-15T16:47:59Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN",
      "stat.ML"
    ]
  },
  "2408.11810": {
    "id": "2408.11810",
    "title": "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain   Diffusion Models",
    "authors": "Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao and 3 others",
    "abstract": "Diffusion Models have emerged as powerful generative models for high-quality image synthesis, with many subsequent image editing techniques based on them. However, the ease of text-based image editing introduces significant risks, such as malicious editing for scams or intellectual property infringement. Previous works have attempted to safeguard images from diffusion-based editing by adding imperceptible perturbations. These methods are costly and specifically target prevalent Latent Diffusion Models (LDMs), while Pixel-domain Diffusion Models (PDMs) remain largely unexplored and robust against such attacks. Our work addresses this gap by proposing a novel attacking framework with a feature representation attack loss that exploits vulnerabilities in denoising UNets and a latent optimization strategy to enhance the naturalness of protected images. Extensive experiments demonstrate the effectiveness of our approach in attacking dominant PDM-based editing methods (e.g., SDEdit) while maintaining reasonable protection fidelity and robustness against common defense methods. Additionally, our framework is extensible to LDMs, achieving comparable performance to existing approaches.",
    "url": "https://arxiv.org/abs/2408.11810",
    "arxivId": "2408.11810",
    "last_visited": "2024-12-22T16:45:08.960000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-08-21T17:56:34Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2408.13402": {
    "id": "2408.13402",
    "title": "LLaVaOLMoBitnet1B: Ternary LLM goes Multimodal!",
    "authors": "Jainaveen Sundaram, Ravi Iyer",
    "abstract": "Multimodal Large Language Models (MM-LLMs) have seen significant advancements in the last year, demonstrating impressive performance across tasks. However, to truly democratize AI, models must exhibit strong capabilities and be able to run efficiently on small compute footprints accessible by most. Part of this quest, we introduce LLaVaOLMoBitnet1B - the first Ternary Multimodal LLM capable of accepting Image(s)+Text inputs to produce coherent textual responses. The model is fully open-sourced along with training scripts to encourage further research in this space. This accompanying technical report highlights the training process, evaluation details, challenges associated with ternary models and future opportunities. Link to the model: https://huggingface.co/IntelLabs/LlavaOLMoBitnet1B",
    "url": "https://arxiv.org/abs/2408.13402",
    "arxivId": "2408.13402",
    "last_visited": "2024-12-30T21:20:33.349696",
    "last_read": "2024-12-30T21:20:33.349696",
    "total_reading_time_seconds": 16,
    "published_date": "2024-08-23T23:00:19Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2408.14837": {
    "id": "2408.14837",
    "title": "Diffusion Models Are Real-Time Game Engines",
    "authors": "Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter",
    "abstract": "We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.",
    "url": "https://arxiv.org/abs/2408.14837",
    "arxivId": "2408.14837",
    "last_visited": "2024-12-27T07:47:16.058000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-08-27T07:46:07Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  "2409.04431": {
    "id": "2409.04431",
    "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
    "authors": "Jason Ramapuram, Federico Danieli, Eeshan Dhekane and 8 others",
    "abstract": "Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.",
    "url": "https://arxiv.org/abs/2409.04431",
    "arxivId": "2409.04431",
    "last_visited": "2025-01-02T19:41:19.761886",
    "last_read": "2025-01-02T19:41:19.761886",
    "total_reading_time_seconds": 24,
    "published_date": "2024-09-06T17:53:26Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2409.05816": {
    "id": "2409.05816",
    "title": "Improving Pretraining Data Using Perplexity Correlations",
    "authors": "Tristan Thrush, Christopher Potts, Tatsunori Hashimoto",
    "abstract": "Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.",
    "url": "https://arxiv.org/abs/2409.05816",
    "arxivId": "2409.05816",
    "last_visited": "2024-12-30T21:00:49.577030",
    "last_read": "2024-12-30T21:00:49.577030",
    "total_reading_time_seconds": 18,
    "published_date": "2024-09-09T17:23:29Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ]
  },
  "2409.08514": {
    "id": "2409.08514",
    "title": "Apollo: Band-sequence Modeling for High-Quality Audio Restoration",
    "authors": "Kai Li, Yi Luo",
    "abstract": "Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at https://github.com/JusperLee/Apollo.",
    "url": "https://arxiv.org/abs/2409.08514",
    "arxivId": "2409.08514",
    "last_visited": "2024-12-30T08:26:43.563812",
    "last_read": "2024-12-30T08:26:43.563812",
    "total_reading_time_seconds": 4,
    "published_date": "2024-09-13T03:25:34Z",
    "arxiv_tags": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ]
  },
  "2409.09347": {
    "id": "2409.09347",
    "title": "Schrödinger Bridge Flow for Unpaired Data Translation",
    "authors": "Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, Arnaud Doucet",
    "abstract": "Mass transport problems arise in many areas of machine learning whereby one wants to compute a map transporting one distribution to another. Generative modeling techniques like Generative Adversarial Networks (GANs) and Denoising Diffusion Models (DDMs) have been successfully adapted to solve such transport problems, resulting in CycleGAN and Bridge Matching respectively. However, these methods do not approximate Optimal Transport (OT) maps, which are known to have desirable properties. Existing techniques approximating OT maps for high-dimensional data-rich problems, such as DDM-based Rectified Flow and Schr\\\"odinger Bridge procedures, require fully training a DDM-type model at each iteration, or use mini-batch techniques which can introduce significant errors. We propose a novel algorithm to compute the Schr\\\"odinger Bridge, a dynamic entropy-regularised version of OT, that eliminates the need to train multiple DDM-like models. This algorithm corresponds to a discretisation of a flow of path measures, which we call the Schr\\\"odinger Bridge Flow, whose only stationary point is the Schr\\\"odinger Bridge. We demonstrate the performance of our algorithm on a variety of unpaired data translation tasks.",
    "url": "https://arxiv.org/abs/2409.09347",
    "arxivId": "2409.09347",
    "last_visited": "2024-12-22T08:06:20.666000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-14T07:34:30Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "2409.13731": {
    "id": "2409.13731",
    "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented   Generation",
    "authors": "Lei Liang, Mengshu Sun, Zhengke Gui and 16 others",
    "abstract": "The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.",
    "url": "https://arxiv.org/abs/2409.13731",
    "arxivId": "2409.13731",
    "last_visited": "2025-01-02T15:09:51.002229",
    "last_read": "2025-01-02T15:09:51.002229",
    "total_reading_time_seconds": 3,
    "published_date": "2024-09-10T02:00:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2409.16986": {
    "id": "2409.16986",
    "title": "Harnessing Diversity for Important Data Selection in Pretraining Large   Language Models",
    "authors": "Chi Zhang, Huaping Zhong, Kuan Zhang and 10 others",
    "abstract": "Data selection is of great significance in pre-training large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores. However, this approach has several limitations. (1) Computing the influence of all available data is time-consuming. (2) The selected data instances are not diverse enough, which may hinder the pre-trained model's ability to generalize effectively to various downstream tasks. In this paper, we introduce \\texttt{Quad}, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pre-training results. In particular, noting that attention layers capture extensive semantic details, we have adapted the accelerated $iHVP$ computation methods for attention layers, enhancing our ability to evaluate the influence of data, $i.e.,$ its quality. For the diversity, \\texttt{Quad} clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. To determine which clusters to select, we utilize the classic Multi-Armed Bandit method, treating each cluster as an arm. This approach favors clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.",
    "url": "https://arxiv.org/abs/2409.16986",
    "arxivId": "2409.16986",
    "last_visited": "2024-12-30T21:40:07.563000",
    "last_read": "2024-12-30T20:07:21.331051",
    "total_reading_time_seconds": 20,
    "published_date": "2024-09-25T14:49:29Z",
    "arxiv_tags": [
      "cs.AI"
    ]
  },
  "2409.19606": {
    "id": "2409.19606",
    "title": "Hyper-Connections",
    "authors": "Defa Zhu, Hongzhi Huang, Zihao Huang and 5 others",
    "abstract": "We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.",
    "url": "https://arxiv.org/abs/2409.19606",
    "arxivId": "2409.19606",
    "last_visited": "2025-01-02T18:13:30.652000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-29T07:57:07Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "cs.NE"
    ]
  },
  "2409.20325": {
    "id": "2409.20325",
    "title": "Old Optimizer, New Norm: An Anthology",
    "authors": "Jeremy Bernstein, Laker Newhouse",
    "abstract": "Deep learning optimizers are often motivated through a mix of convex and approximate second-order theory. We select three such methods -- Adam, Shampoo and Prodigy -- and argue that each method can instead be understood as a squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm. By generalizing this observation, we chart a new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of $\\mathbb{R}^{m\\times n}$, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training.",
    "url": "https://arxiv.org/abs/2409.20325",
    "arxivId": "2409.20325",
    "last_visited": "2025-01-02T19:29:00.437000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-30T14:26:12Z",
    "arxiv_tags": [
      "cs.LG",
      "math.OC"
    ]
  },
  "2410.01131": {
    "id": "2410.01131",
    "title": "nGPT: Normalized Transformer with Representation Learning on the   Hypersphere",
    "authors": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg",
    "abstract": "We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.",
    "url": "https://arxiv.org/abs/2410.01131",
    "arxivId": "2410.01131",
    "last_visited": "2024-12-30T14:50:31.140367",
    "last_read": "2024-12-30T14:50:31.140367",
    "total_reading_time_seconds": 73,
    "published_date": "2024-10-01T23:50:09Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2410.01560": {
    "id": "2410.01560",
    "title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source   Instruction Data",
    "authors": "Shubham Toshniwal, Wei Du, Ivan Moshkov and 3 others",
    "abstract": "Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become \\emph{closed-source} due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released \\texttt{Llama3.1} family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms equally-sized data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset, which consists of 14M question-solution pairs ($\\approx$ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\% $\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license.",
    "url": "https://arxiv.org/abs/2410.01560",
    "arxivId": "2410.01560",
    "last_visited": "2024-12-30T21:06:41.155472",
    "last_read": "2024-12-30T21:06:41.155472",
    "total_reading_time_seconds": 14,
    "published_date": "2024-10-02T14:00:09Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2410.04715": {
    "id": "2410.04715",
    "title": "Rule-based Data Selection for Large Language Models",
    "authors": "Xiaomin Li, Mingye Gao, Zhiwei Zhang and 2 others",
    "abstract": "The quality of training data significantly impacts the performance of large language models (LLMs). There are increasing studies using LLMs to rate and select data based on several human-crafted metrics (rules). However, these conventional rule-based approaches often depend too heavily on human heuristics, lack effective metrics for assessing rules, and exhibit limited adaptability to new tasks. In our study, we introduce an innovative rule-based framework that utilizes the orthogonality of score vectors associated with rules as a novel metric for rule evaluations. Our approach includes an automated pipeline that first uses LLMs to generate a diverse set of rules, encompassing various rating dimensions to evaluate data quality. Then it rates a batch of data based on these rules and uses the determinantal point process (DPP) from random matrix theory to select the most orthogonal score vectors, thereby identifying a set of independent rules. These rules are subsequently used to evaluate all data, selecting samples with the highest average scores for downstream tasks such as LLM training. We verify the effectiveness of our method through two experimental setups: 1) comparisons with ground truth ratings and 2) benchmarking LLMs trained with the chosen data. Our comprehensive experiments cover a range of scenarios, including general pre-training and domain-specific fine-tuning in areas such as IMDB, Medical, Math, and Code. The outcomes demonstrate that our DPP-based rule rating method consistently outperforms other approaches, including rule-free rating, uniform sampling, importance resampling, and QuRating, in terms of both rating precision and model performance.",
    "url": "https://arxiv.org/abs/2410.04715",
    "arxivId": "2410.04715",
    "last_visited": "2024-12-30T21:18:12.128000",
    "last_read": "2024-12-30T20:04:21.781140",
    "total_reading_time_seconds": 21,
    "published_date": "2024-10-07T03:13:06Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2410.05639": {
    "id": "2410.05639",
    "title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing   with Language Models",
    "authors": "Ranchi Zhao, Zhen Leng Thai, Yifan Zhang and 6 others",
    "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce DecorateLM, a data engineering method designed to refine the pretraining corpus through data rating, tagging and editing. Specifically, DecorateLM rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for DecorateLM using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply DecorateLM to enhance 100 billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus.",
    "url": "https://arxiv.org/abs/2410.05639",
    "arxivId": "2410.05639",
    "last_visited": "2024-12-30T21:19:21.741000",
    "last_read": "2024-12-30T20:04:18.842488",
    "total_reading_time_seconds": 16,
    "published_date": "2024-10-08T02:42:56Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2410.08800": {
    "id": "2410.08800",
    "title": "Data Processing for the OpenGPT-X Model Family",
    "authors": "Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick and 19 others",
    "abstract": "This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.",
    "url": "https://arxiv.org/abs/2410.08800",
    "arxivId": "2410.08800",
    "last_visited": "2024-12-30T21:53:15.465877",
    "last_read": "2024-12-30T21:53:15.465877",
    "total_reading_time_seconds": 47,
    "published_date": "2024-10-11T13:34:24Z",
    "arxiv_tags": [
      "cs.CL",
      "H.3.1; I.2.7"
    ]
  },
  "2410.10792": {
    "id": "2410.10792",
    "title": "Semantic Image Inversion and Editing using Rectified Stochastic   Differential Equations",
    "authors": "Litu Rout, Yujia Chen, Nataniel Ruiz and 3 others",
    "abstract": "Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.",
    "url": "https://arxiv.org/abs/2410.10792",
    "arxivId": "2410.10792",
    "last_visited": "2024-12-30T15:14:34.707000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-14T17:56:24Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ]
  },
  "2410.13835": {
    "id": "2410.13835",
    "title": "Active-Dormant Attention Heads: Mechanistically Demystifying   Extreme-Token Phenomena in LLMs",
    "authors": "Tianyu Guo, Druv Pai, Yu Bai and 3 others",
    "abstract": "Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): attention sinks, value-state drains, and residual-state peaks, collectively referred to as extreme-token phenomena. These phenomena are characterized by certain so-called \"sink tokens\" receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability.   We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures -- transformers with one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar active-dormant mechanism as in the BB task, and that the mutual reinforcement mechanism also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs.",
    "url": "https://arxiv.org/abs/2410.13835",
    "arxivId": "2410.13835",
    "last_visited": "2024-12-30T22:18:12.909562",
    "last_read": "2024-12-30T22:18:12.909562",
    "total_reading_time_seconds": 44,
    "published_date": "2024-10-17T17:54:06Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2410.21265": {
    "id": "2410.21265",
    "title": "Modular Duality in Deep Learning",
    "authors": "Jeremy Bernstein, Laker Newhouse",
    "abstract": "An old idea in optimization theory says that since the gradient is a dual vector it may not be subtracted from the weights without first being mapped to the primal space where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call modular dualization, forms a unifying theoretical basis for training algorithms that are a) fast and b) scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers -- the latter two methods are based on a rectangular Newton-Schulz iteration (Kovarik, 1970; Bj\\\"orck & Bowie, 1971). A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.",
    "url": "https://arxiv.org/abs/2410.21265",
    "arxivId": "2410.21265",
    "last_visited": "2025-01-02T19:43:53.760000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-28T17:57:31Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ]
  },
  "2411.02335": {
    "id": "2411.02335",
    "title": "Sparsing Law: Towards Large Language Models with Greater Activation   Sparsity",
    "authors": "Yuqi Luo, Chenyang Song, Xu Han and 4 others",
    "abstract": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.",
    "url": "https://arxiv.org/abs/2411.02335",
    "arxivId": "2411.02335",
    "last_visited": "2024-12-30T21:20:33.348459",
    "last_read": "2024-12-30T21:20:33.348459",
    "total_reading_time_seconds": 4,
    "published_date": "2024-11-04T17:59:04Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "stat.ML",
      "I.2.7"
    ]
  },
  "2411.04330": {
    "id": "2411.04330",
    "title": "Scaling Laws for Precision",
    "authors": "Tanishq Kumar, Zachary Ankner, Benjamin F. Spector and 6 others",
    "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
    "url": "https://arxiv.org/abs/2411.04330",
    "arxivId": "2411.04330",
    "last_visited": "2024-12-30T20:19:02.129673",
    "last_read": "2024-12-30T20:19:02.129673",
    "total_reading_time_seconds": 11,
    "published_date": "2024-11-07T00:10:10Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2411.05899": {
    "id": "2411.05899",
    "title": "Streaming Bayes GFlowNets",
    "authors": "Tiago da Silva, Daniel Augusto de Souza, Diego Mesquita",
    "abstract": "Bayes' rule naturally allows for inference refinement in a streaming fashion, without the need to recompute posteriors from scratch whenever new data arrives. In principle, Bayesian streaming is straightforward: we update our prior with the available data and use the resulting posterior as a prior when processing the next data chunk. In practice, however, this recipe entails i) approximating an intractable posterior at each time step; and ii) encapsulating results appropriately to allow for posterior propagation. For continuous state spaces, variational inference (VI) is particularly convenient due to its scalability and the tractability of variational posteriors. For discrete state spaces, however, state-of-the-art VI results in analytically intractable approximations that are ill-suited for streaming settings. To enable streaming Bayesian inference over discrete parameter spaces, we propose streaming Bayes GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed GFlowNets -- a powerful class of amortized samplers for discrete compositional objects. Notably, SB-GFlowNet approximates the initial posterior using a standard GFlowNet and subsequently updates it using a tailored procedure that requires only the newly observed data. Our case studies in linear preference learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets in sampling from an unnormalized posterior in a streaming setting. As expected, we also observe that SB-GFlowNets is significantly faster than repeatedly training a GFlowNet from scratch to sample from the full posterior.",
    "url": "https://arxiv.org/abs/2411.05899",
    "arxivId": "2411.05899",
    "last_visited": "2024-12-30T14:44:00.196230",
    "last_read": "2024-12-30T14:44:00.196230",
    "total_reading_time_seconds": 9,
    "published_date": "2024-11-08T15:53:56Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2411.06068": {
    "id": "2411.06068",
    "title": "Zyda-2: a 5 Trillion Token High-Quality Dataset",
    "authors": "Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge",
    "abstract": "In this technical report, we present Zyda-2: a five trillion token dataset for language model pretraining. Zyda-2 was used to train our Zamba2 series of models which are state-of-the-art for their weight class. We build Zyda-2 by collating high-quality open-source tokens such as FineWeb and DCLM, then distilling them to the highest-quality subset via cross-deduplication and model-based quality filtering. Zyda-2 is released under a permissive open license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2",
    "url": "https://arxiv.org/abs/2411.06068",
    "arxivId": "2411.06068",
    "last_visited": "2024-12-30T22:20:47.473236",
    "last_read": "2024-12-30T22:20:47.473236",
    "total_reading_time_seconds": 39,
    "published_date": "2024-11-09T04:57:41Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2411.12372": {
    "id": "2411.12372",
    "title": "RedPajama: an Open Dataset for Training Large Language Models",
    "authors": "Maurice Weber, Daniel Fu, Quentin Anthony and 16 others",
    "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",
    "url": "https://arxiv.org/abs/2411.12372",
    "arxivId": "2411.12372",
    "last_visited": "2024-12-30T20:18:27.856000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-19T09:35:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2411.18933": {
    "id": "2411.18933",
    "title": "Efficient Track Anything",
    "authors": "Yunyang Xiong, Chong Zhou, Xiaoyu Xiang and 10 others",
    "abstract": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.",
    "url": "https://arxiv.org/abs/2411.18933",
    "arxivId": "2411.18933",
    "last_visited": "2024-12-15T20:13:52.905000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-28T05:52:10Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2411.19108": {
    "id": "2411.19108",
    "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
    "authors": "Feng Liu, Shiwei Zhang, Xiaofeng Wang and 6 others",
    "abstract": "As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.",
    "url": "https://arxiv.org/abs/2411.19108",
    "arxivId": "2411.19108",
    "last_visited": "2024-12-30T15:12:03.734623",
    "last_read": "2024-12-30T15:12:03.734623",
    "total_reading_time_seconds": 40,
    "published_date": "2024-11-28T12:50:05Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2411.19722": {
    "id": "2411.19722",
    "title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text",
    "authors": "Michael Tschannen, André Susano Pinto, Alexander Kolesnikov",
    "abstract": "Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we further streamline joint generative modeling of images and text. We propose an autoregressive decoder-only transformer - JetFormer - which is trained to directly maximize the likelihood of raw data, without relying on any separately pretrained components, and can understand and generate both text and images. Specifically, we leverage a normalizing flow model to obtain a soft-token image representation that is jointly trained with an autoregressive multimodal transformer. The normalizing flow model serves as both an image encoder for perception tasks and an image decoder for image generation tasks during inference. JetFormer achieves text-to-image generation quality competitive with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained image autoencoders, which are trained with a complex mixture of losses, including perceptual ones. At the same time, JetFormer demonstrates robust image understanding capabilities. To the best of our knowledge, JetFormer is the first model that is capable of generating high-fidelity images and producing strong log-likelihood bounds.",
    "url": "https://arxiv.org/abs/2411.19722",
    "arxivId": "2411.19722",
    "last_visited": "2024-12-30T14:50:25.157252",
    "last_read": "2024-12-30T14:50:25.157252",
    "total_reading_time_seconds": 300,
    "published_date": "2024-11-29T14:14:59Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  "2412.01023": {
    "id": "2412.01023",
    "title": "Learning Structured Representations with Hyperbolic Embeddings",
    "authors": "Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao",
    "abstract": "Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work [Zeng et al., 2022] proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context [Chen et al., 2013]. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss, and can be combined with any standard task loss to learn hierarchy-informed features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance especially under low dimensional scenarios. For a better understanding of structured representation, we perform eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at \\url{https://github.com/uiuctml/HypStructure}.",
    "url": "https://arxiv.org/abs/2412.01023",
    "arxivId": "2412.01023",
    "last_visited": "2024-12-30T08:28:10.548268",
    "last_read": "2024-12-30T08:28:10.548268",
    "total_reading_time_seconds": 81,
    "published_date": "2024-12-02T00:56:44Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ]
  },
  "2412.02595": {
    "id": "2412.02595",
    "title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset",
    "authors": "Dan Su, Kezhi Kong, Ying Lin and 6 others",
    "abstract": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html",
    "url": "https://arxiv.org/abs/2412.02595",
    "arxivId": "2412.02595",
    "last_visited": "2024-12-30T21:28:37.112341",
    "last_read": "2024-12-30T21:28:37.112341",
    "total_reading_time_seconds": 4,
    "published_date": "2024-12-03T17:28:50Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.03603": {
    "id": "2412.03603",
    "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
    "authors": "Weijie Kong, Qi Tian, Zijian Zhang and 49 others",
    "abstract": "Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.",
    "url": "https://arxiv.org/abs/2412.03603",
    "arxivId": "2412.03603",
    "last_visited": "2024-12-30T15:18:09.192840",
    "last_read": "2024-12-30T15:18:09.192840",
    "total_reading_time_seconds": 74,
    "published_date": "2024-12-03T23:52:37Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2412.04619": {
    "id": "2412.04619",
    "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
    "authors": "Tian Qin, Naomi Saphra, David Alvarez-Melis",
    "abstract": "Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.",
    "url": "https://arxiv.org/abs/2412.04619",
    "arxivId": "2412.04619",
    "last_visited": "2024-12-30T14:48:58.180775",
    "last_read": "2024-12-30T14:48:58.180775",
    "total_reading_time_seconds": 60,
    "published_date": "2024-12-05T21:12:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2412.06769": {
    "id": "2412.06769",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "authors": "Shibo Hao, Sainbayar Sukhbaatar, DiJia Su and 4 others",
    "abstract": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
    "url": "https://arxiv.org/abs/2412.06769",
    "arxivId": "2412.06769",
    "last_visited": "2025-01-02T19:41:19.760530",
    "last_read": "2025-01-02T19:41:19.760530",
    "total_reading_time_seconds": 32,
    "published_date": "2024-12-09T18:55:56Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.06845": {
    "id": "2412.06845",
    "title": "Fully Open Source Moxin-7B Technical Report",
    "authors": "Pu Zhao, Xuan Shen, Zhenglun Kong and 13 others",
    "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be \"open-source,\" which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of \"open science\" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.",
    "url": "https://arxiv.org/abs/2412.06845",
    "arxivId": "2412.06845",
    "last_visited": "2024-12-30T21:42:20.892376",
    "last_read": "2024-12-30T21:42:20.892376",
    "total_reading_time_seconds": 53,
    "published_date": "2024-12-08T02:01:46Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2412.10271": {
    "id": "2412.10271",
    "title": "Benchmarking Linguistic Diversity of Large Language Models",
    "authors": "Yanzhu Guo, Guokan Shang, Chloé Clavel",
    "abstract": "The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an in-depth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.",
    "url": "https://arxiv.org/abs/2412.10271",
    "arxivId": "2412.10271",
    "last_visited": "2024-12-30T21:39:52.561339",
    "last_read": "2024-12-30T21:39:52.561339",
    "total_reading_time_seconds": 52,
    "published_date": "2024-12-13T16:46:03Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.11766": {
    "id": "2412.11766",
    "title": "Interplay of epidemic spreading and vaccine uptake under complex social   contagion",
    "authors": "Alfonso de Miguel-Arribas, Alberto Aleta, Yamir Moreno",
    "abstract": "Modeling human behavior is essential to accurately predict epidemic spread, with behaviors like vaccine hesitancy complicating control efforts. While epidemic spread is often treated as a simple contagion, vaccine uptake may follow complex contagion dynamics, where individuals' decisions depend on multiple social contacts. Recently, the concept of complex contagion has received strong theoretical underpinnings thanks to the generalization of spreading phenomena from pairwise to higher-order interactions. Although several potential applications have been suggested, examples of complex contagions motivated by real data remain scarce. Surveys on COVID-19 vaccine hesitancy in the US suggest that vaccination attitudes may indeed depend on the vaccination status of social peers, aligning with complex contagion principles. In this work, we examine the interactions between epidemic spread, vaccination, and vaccine uptake attitudes under complex contagion. Using the SIR model with a dynamic, threshold-based vaccination campaign, we simulate scenarios on an age-structured multilayer network informed by US contact data. Our results offer insights into the role of social dynamics in shaping vaccination behavior and epidemic outcomes.",
    "url": "https://arxiv.org/abs/2412.11766",
    "arxivId": "2412.11766",
    "last_visited": "2024-12-30T14:44:06.211749",
    "last_read": "2024-12-30T14:44:06.211749",
    "total_reading_time_seconds": 6,
    "published_date": "2024-12-16T13:37:27Z",
    "arxiv_tags": [
      "physics.soc-ph",
      "cs.SI"
    ]
  },
  "2412.11768": {
    "id": "2412.11768",
    "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
    "authors": "Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen",
    "abstract": "In this work, we question the necessity of adaptive gradient methods for training deep neural networks. SGD-SaI is a simple yet effective enhancement to stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning rate Scaling at Initialization (SaI) to distinct parameter groups, guided by their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning rates without relying on adaptive second-order momentum, SGD-SaI helps prevent training imbalances from the very first iteration and cuts the optimizer's memory usage by half compared to AdamW. Despite its simplicity and efficiency, SGD-SaI consistently matches or outperforms AdamW in training a variety of Transformer-based tasks, effectively overcoming a long-standing challenge of using SGD for training Transformers. SGD-SaI excels in ImageNet-1K classification with Vision Transformers(ViT) and GPT-2 pretraining for large language models (LLMs, transformer decoder-only), demonstrating robustness to hyperparameter variations and practicality for diverse applications. We further tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion models, where it consistently outperforms state-of-the-art optimizers. From a memory efficiency perspective, SGD-SaI achieves substantial memory savings for optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters) and 25.15 GB for Llama2-7B compared to AdamW in full-precision training settings.",
    "url": "https://arxiv.org/abs/2412.11768",
    "arxivId": "2412.11768",
    "last_visited": "2024-12-30T08:28:19.539482",
    "last_read": "2024-12-30T08:28:19.539482",
    "total_reading_time_seconds": 11,
    "published_date": "2024-12-16T13:41:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2412.11965": {
    "id": "2412.11965",
    "title": "Inferring Functionality of Attention Heads from their Parameters",
    "authors": "Amit Elhelo, Mor Geva",
    "abstract": "Attention heads are one of the building blocks of large language models (LLMs). Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks. In this work, we seek a comprehensive mapping of the operations they implement in a model. We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference. We showcase the utility of MAPS for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality. Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head's outputs during inference and are causally linked to the model's predictions. Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS to characterize the salient operations of a given head. Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations.",
    "url": "https://arxiv.org/abs/2412.11965",
    "arxivId": "2412.11965",
    "last_visited": "2024-12-30T14:49:34.121572",
    "last_read": "2024-12-30T14:49:34.121572",
    "total_reading_time_seconds": 60,
    "published_date": "2024-12-16T16:45:33Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.13145": {
    "id": "2412.13145",
    "title": "Agnosticism About Artificial Consciousness",
    "authors": "Tom McClelland",
    "abstract": "Could an AI have conscious experiences? Any answer to this question should conform to Evidentialism - that is, it should be based not on intuition, dogma or speculation but on solid scientific evidence. I argue that such evidence is hard to come by and that the only justifiable stance on the prospects of artificial consciousness is agnosticism. In the current debate, the main division is between biological views that are sceptical of artificial consciousness and functional views that are sympathetic to it. I argue that both camps make the same mistake of over-estimating what the evidence tells us. Scientific insights into consciousness have been achieved through the study of conscious organisms. Although this has enabled cautious assessments of consciousness in various creatures, extending this to AI faces serious obstacles. AI thus presents consciousness researchers with a dilemma: either reach a verdict on artificial consciousness but violate Evidentialism; or respect Evidentialism but offer no verdict on the prospects of artificial consciousness. The dominant trend in the literature has been to take the first option while purporting to follow the scientific evidence. I argue that if we truly follow the evidence, we must take the second option and adopt agnosticism.",
    "url": "https://arxiv.org/abs/2412.13145",
    "arxivId": "2412.13145",
    "last_visited": "2024-12-22T05:22:42.487000",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-17T18:11:12Z",
    "arxiv_tags": [
      "cs.AI"
    ]
  },
  "2412.15285": {
    "id": "2412.15285",
    "title": "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase   Pretraining",
    "authors": "Steven Feng, Shrimai Prabhumoye, Kezhi Kong and 4 others",
    "abstract": "Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, we formalize the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends.",
    "url": "https://arxiv.org/abs/2412.15285",
    "arxivId": "2412.15285",
    "last_visited": "2024-12-30T23:20:45.108596",
    "last_read": "2024-12-30T23:20:45.108596",
    "total_reading_time_seconds": 49,
    "published_date": "2024-12-18T18:41:18Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2412.17558": {
    "id": "2412.17558",
    "title": "A Survey of Query Optimization in Large Language Models",
    "authors": "Mingyang Song, Mao Zheng",
    "abstract": "\\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the efficiency and quality of Large Language Models (LLMs) in understanding and answering queries, especially complex ones in scenarios like Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the limitations of LLMs by dynamically retrieving and leveraging up-to-date relevant information, which provides a cost-effective solution to the challenge of LLMs producing plausible but potentially inaccurate responses. Recently, as RAG evolves and incorporates multiple components that influence its performance, QO has emerged as a critical element, playing a pivotal role in determining the effectiveness of RAG's retrieval stage in accurately sourcing the necessary multiple pieces of evidence to answer queries correctly. In this paper, we trace the evolution of QO techniques by summarizing and analyzing significant studies. Through an organized framework and categorization, we aim to consolidate existing QO techniques in RAG, elucidate their technological foundations, and highlight their potential to enhance the versatility and applications of LLMs.",
    "url": "https://arxiv.org/abs/2412.17558",
    "arxivId": "2412.17558",
    "last_visited": "2024-12-30T08:27:13.538442",
    "last_read": "2024-12-30T08:27:13.538442",
    "total_reading_time_seconds": 46,
    "published_date": "2024-12-23T13:26:04Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.17758": {
    "id": "2412.17758",
    "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
    "authors": "Łukasz Borchmann",
    "abstract": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.",
    "url": "https://arxiv.org/abs/2412.17758",
    "arxivId": "2412.17758",
    "last_visited": "2024-12-30T08:27:52.551795",
    "last_read": "2024-12-30T08:27:52.551795",
    "total_reading_time_seconds": 6,
    "published_date": "2024-12-23T18:14:36Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2412.17759": {
    "id": "2412.17759",
    "title": "Survey of Large Multimodal Model Datasets, Application Categories and   Taxonomy",
    "authors": "Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar and 4 others",
    "abstract": "Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video. Inspired by the human ability to assimilate information through many senses, this method enables applications such as text-to-video conversion, visual question answering, and image captioning. Recent developments in datasets that support multimodal language models (MLLMs) are highlighted in this overview. Large-scale multimodal datasets are essential because they allow for thorough testing and training of these models. With an emphasis on their contributions to the discipline, the study examines a variety of datasets, including those for training, domain-specific tasks, and real-world applications. It also emphasizes how crucial benchmark datasets are for assessing models' performance in a range of scenarios, scalability, and applicability. Since multimodal learning is always changing, overcoming these obstacles will help AI research and applications reach new heights.",
    "url": "https://arxiv.org/abs/2412.17759",
    "arxivId": "2412.17759",
    "last_visited": "2024-12-30T08:27:13.536863",
    "last_read": "2024-12-30T08:27:13.536863",
    "total_reading_time_seconds": 5,
    "published_date": "2024-12-23T18:15:19Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  "2412.17799": {
    "id": "2412.17799",
    "title": "Automating the Search for Artificial Life with Foundation Models",
    "authors": "Akarsh Kumar, Chris Lu, Louis Kirsch and 4 others",
    "abstract": "With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.",
    "url": "https://arxiv.org/abs/2412.17799",
    "arxivId": "2412.17799",
    "last_visited": "2024-12-30T14:44:39.236746",
    "last_read": "2024-12-30T14:44:39.236746",
    "total_reading_time_seconds": 3,
    "published_date": "2024-12-23T18:57:00Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.NE"
    ]
  },
  "2412.17805": {
    "id": "2412.17805",
    "title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
    "authors": "Yazhou Xing, Yang Fei, Yingqing He and 4 others",
    "abstract": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~\\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.",
    "url": "https://arxiv.org/abs/2412.17805",
    "arxivId": "2412.17805",
    "last_visited": "2025-01-02T19:37:57.495826",
    "last_read": "2025-01-02T19:37:57.495826",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-23T18:58:24Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2412.17847": {
    "id": "2412.17847",
    "title": "Bridging the Data Provenance Gap Across Text, Speech and Video",
    "authors": "Shayne Longpre, Nikhil Singh, Manuel Cherep and 40 others",
    "abstract": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.",
    "url": "https://arxiv.org/abs/2412.17847",
    "arxivId": "2412.17847",
    "last_visited": "2024-12-30T21:36:00.113000",
    "last_read": "2024-12-30T20:04:03.800797",
    "total_reading_time_seconds": 5,
    "published_date": "2024-12-19T01:30:19Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "cs.MM"
    ]
  },
  "2412.18052": {
    "id": "2412.18052",
    "title": "Beyond Gradient Averaging in Parallel Optimization: Improved Robustness   through Gradient Agreement Filtering",
    "authors": "Francois Chaubard, Duncan Eddy, Mykel J. Kochenderfer",
    "abstract": "We introduce Gradient Agreement Filtering (GAF) to improve on gradient averaging in distributed deep learning optimization. Traditional distributed data-parallel stochastic gradient descent involves averaging gradients of microbatches to calculate a macrobatch gradient that is then used to update model parameters. We find that gradients across microbatches are often orthogonal or negatively correlated, especially in late stages of training, which leads to memorization of the training set, reducing generalization. In this paper, we introduce a simple, computationally effective way to reduce gradient variance by computing the cosine distance between micro-gradients during training and filtering out conflicting updates prior to averaging. We improve validation accuracy with significantly smaller microbatch sizes. We also show this reduces memorizing noisy labels. We demonstrate the effectiveness of this technique on standard image classification benchmarks including CIFAR-100 and CIFAR-100N-Fine. We show this technique consistently outperforms validation accuracy, in some cases by up to 18.2\\% compared to traditional training approaches while reducing the computation required nearly an order of magnitude because we can now rely on smaller microbatch sizes without destabilizing training.",
    "url": "https://arxiv.org/abs/2412.18052",
    "arxivId": "2412.18052",
    "last_visited": "2025-01-02T19:27:21.959879",
    "last_read": "2025-01-02T19:27:21.959879",
    "total_reading_time_seconds": 17,
    "published_date": "2024-12-24T00:00:11Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2412.18082": {
    "id": "2412.18082",
    "title": "Prompt Tuning for Item Cold-start Recommendation",
    "authors": "Yuezihan Jiang, Gaode Chen, Wenhan Zhang and 6 others",
    "abstract": "The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of the positive feedback to the model, which is the core of the cold-start problem that hinders the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that compared to the content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios",
    "url": "https://arxiv.org/abs/2412.18082",
    "arxivId": "2412.18082",
    "last_visited": "2024-12-30T08:26:55.552642",
    "last_read": "2024-12-30T08:26:55.552642",
    "total_reading_time_seconds": 19,
    "published_date": "2024-12-24T01:38:19Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.AI"
    ]
  },
  "2412.18168": {
    "id": "2412.18168",
    "title": "From Pairwise to Ranking: Climbing the Ladder to Ideal Collaborative   Filtering with Pseudo-Ranking",
    "authors": "Yuhan Zhao, Rui Chen, Li Chen and 3 others",
    "abstract": "Intuitively, an ideal collaborative filtering (CF) model should learn from users' full rankings over all items to make optimal top-K recommendations. Due to the absence of such full rankings in practice, most CF models rely on pairwise loss functions to approximate full rankings, resulting in an immense performance gap. In this paper, we provide a novel analysis using the multiple ordinal classification concept to reveal the inevitable gap between a pairwise approximation and the ideal case. However, bridging the gap in practice encounters two formidable challenges: (1) none of the real-world datasets contains full ranking information; (2) there does not exist a loss function that is capable of consuming ranking information. To overcome these challenges, we propose a pseudo-ranking paradigm (PRP) that addresses the lack of ranking information by introducing pseudo-rankings supervised by an original noise injection mechanism. Additionally, we put forward a new ranking loss function designed to handle ranking information effectively. To ensure our method's robustness against potential inaccuracies in pseudo-rankings, we equip the ranking loss function with a gradient-based confidence mechanism to detect and mitigate abnormal gradients. Extensive experiments on four real-world datasets demonstrate that PRP significantly outperforms state-of-the-art methods.",
    "url": "https://arxiv.org/abs/2412.18168",
    "arxivId": "2412.18168",
    "last_visited": "2024-12-30T08:27:02.068052",
    "last_read": "2024-12-30T08:27:02.068052",
    "total_reading_time_seconds": 6,
    "published_date": "2024-12-24T05:01:16Z",
    "arxiv_tags": [
      "cs.IR"
    ]
  },
  "2412.18431": {
    "id": "2412.18431",
    "title": "GeAR: Graph-enhanced Agent for Retrieval-augmented Generation",
    "authors": "Zhili Shen, Chenxin Diao, Pavlos Vougiouklis and 8 others",
    "abstract": "Retrieval-augmented generation systems rely on effective document retrieval capabilities. By design, conventional sparse or dense retrievers face challenges in multi-hop retrieval scenarios. In this paper, we present GeAR, which advances RAG performance through two key innovations: (i) graph expansion, which enhances any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates graph expansion. Our evaluation demonstrates GeAR's superior retrieval performance on three multi-hop question answering datasets. Additionally, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while requiring fewer tokens and iterations compared to other multi-step retrieval systems.",
    "url": "https://arxiv.org/abs/2412.18431",
    "arxivId": "2412.18431",
    "last_visited": "2024-12-30T08:27:04.571191",
    "last_read": "2024-12-30T08:27:04.571191",
    "total_reading_time_seconds": 24,
    "published_date": "2024-12-24T13:45:22Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ]
  },
  "2412.18537": {
    "id": "2412.18537",
    "title": "Harnessing Large Language Models for Knowledge Graph Question Answering   via Adaptive Multi-Aspect Retrieval-Augmentation",
    "authors": "Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin and 6 others",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs' output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy over its best competitor and a 6.6\\% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.",
    "url": "https://arxiv.org/abs/2412.18537",
    "arxivId": "2412.18537",
    "last_visited": "2024-12-30T08:27:07.549197",
    "last_read": "2024-12-30T08:27:07.549197",
    "total_reading_time_seconds": 4,
    "published_date": "2024-12-24T16:38:04Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.18860": {
    "id": "2412.18860",
    "title": "Bootstrap Your Own Context Length",
    "authors": "Liang Wang, Nan Yang, Xingxing Zhang and 2 others",
    "abstract": "We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.",
    "url": "https://arxiv.org/abs/2412.18860",
    "arxivId": "2412.18860",
    "last_visited": "2024-12-30T08:26:49.556588",
    "last_read": "2024-12-30T08:26:49.556588",
    "total_reading_time_seconds": 29,
    "published_date": "2024-12-25T10:08:54Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.IR"
    ]
  },
  "2412.18956": {
    "id": "2412.18956",
    "title": "Musings About the Future of Search: A Return to the Past?",
    "authors": "Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne",
    "abstract": "When you have a question, the most effective way to have the question answered is to directly connect with experts on the topic and have a conversation with them. Prior to the invention of writing, this was the only way. Although effective, this solution exhibits scalability challenges. Writing allowed knowledge to be materialized, preserved, and replicated, enabling the development of different technologies over the centuries to connect information seekers with relevant information. This progression ultimately culminated in the ten-blue-links web search paradigm we're familiar with, just before the recent emergence of generative AI. However, we often forget that consuming static content is an imperfect solution. With the advent of large language models, it has become possible to develop a superior experience by allowing users to directly engage with experts. These interactions can of course satisfy information needs, but expert models can do so much more. This coming future requires reimagining search.",
    "url": "https://arxiv.org/abs/2412.18956",
    "arxivId": "2412.18956",
    "last_visited": "2024-12-30T08:26:52.832952",
    "last_read": "2024-12-30T08:26:52.832952",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-25T18:09:34Z",
    "arxiv_tags": [
      "cs.IR"
    ]
  },
  "2412.19442": {
    "id": "2412.19442",
    "title": "A Survey on Large Language Model Acceleration based on KV Cache   Management",
    "authors": "Haoyang Li, Yiming Li, Anxin Tian and 7 others",
    "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
    "url": "https://arxiv.org/abs/2412.19442",
    "arxivId": "2412.19442",
    "last_visited": "2024-12-30T08:27:02.067361",
    "last_read": "2024-12-30T08:27:02.067361",
    "total_reading_time_seconds": 5,
    "published_date": "2024-12-27T04:17:57Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.DC"
    ]
  },
  "2412.20292": {
    "id": "2412.20292",
    "title": "An analytic theory of creativity in convolutional diffusion models",
    "authors": "Mason Kamb, Surya Ganguli",
    "abstract": "We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without any training can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our ELS machine reveals a locally consistent patch mosaic model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \\sim 0.75$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.",
    "url": "https://arxiv.org/abs/2412.20292",
    "arxivId": "2412.20292",
    "last_visited": "2025-01-01T16:01:30.151380",
    "last_read": "2025-01-01T16:01:30.151380",
    "total_reading_time_seconds": 3,
    "published_date": "2024-12-28T22:33:29Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "q-bio.NC",
      "stat.ML",
      "I.2.10"
    ]
  },
  "2412.20800": {
    "id": "2412.20800",
    "title": "VMix: Improving Text-to-Image Diffusion Model with Cross-Attention   Mixing Control",
    "authors": "Shaojin Wu, Fei Ding, Mengqi Huang and 2 others",
    "abstract": "While diffusion models show extraordinary talents in text-to-image generation, they may still fail to generate highly aesthetic images. More specifically, there is still a gap between the generated images and the real-world aesthetic images in finer-grained dimensions including color, lighting, composition, etc. In this paper, we propose Cross-Attention Value Mixing Control (VMix) Adapter, a plug-and-play aesthetics adapter, to upgrade the quality of generated images while maintaining generality across visual concepts by (1) disentangling the input text prompt into the content description and aesthetic description by the initialization of aesthetic embedding, and (2) integrating aesthetic conditions into the denoising process through value-mixed cross-attention, with the network connected by zero-initialized linear layers. Our key insight is to enhance the aesthetic presentation of existing diffusion models by designing a superior condition control method, all while preserving the image-text alignment. Through our meticulous design, VMix is flexible enough to be applied to community models for better visual performance without retraining. To validate the effectiveness of our method, we conducted extensive experiments, showing that VMix outperforms other state-of-the-art methods and is compatible with other community modules (e.g., LoRA, ControlNet, and IPAdapter) for image generation. The project page is https://vmix-diffusion.github.io/VMix/.",
    "url": "https://arxiv.org/abs/2412.20800",
    "arxivId": "2412.20800",
    "last_visited": "2025-01-02T15:27:19.572474",
    "last_read": "2025-01-02T15:27:19.572474",
    "total_reading_time_seconds": 4,
    "published_date": "2024-12-30T08:47:25Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2412.20977": {
    "id": "2412.20977",
    "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI",
    "authors": "Fangwei Zhong, Kui Wu, Churan Wang and 4 others",
    "abstract": "We introduce UnrealZoo, a rich collection of photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of the open worlds. Additionally, we offer a variety of playable entities for embodied AI agents. Based on UnrealCV, we provide a suite of easy-to-use Python APIs and tools for various potential applications, such as data collection, environment augmentation, distributed training, and benchmarking. We optimize the rendering and communication efficiency of UnrealCV to support advanced applications, such as multi-agent interaction. Our experiments benchmark agents in various complex scenes, focusing on visual navigation and tracking, which are fundamental capabilities for embodied visual intelligence. The results yield valuable insights into the advantages of diverse training environments for reinforcement learning (RL) agents and the challenges faced by current embodied vision agents, including those based on RL and large vision-language models (VLMs), in open worlds. These challenges involve latency in closed-loop control in dynamic scenes and reasoning about 3D spatial structures in unstructured terrain.",
    "url": "https://arxiv.org/abs/2412.20977",
    "arxivId": "2412.20977",
    "last_visited": "2025-01-02T19:12:48.163001",
    "last_read": "2025-01-02T19:12:48.163001",
    "total_reading_time_seconds": 47,
    "published_date": "2024-12-30T14:31:01Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ]
  }
};
        initializeEventListeners();
        initializeFilters();
        renderPapers();
        
    </script>
</body>
</html>
