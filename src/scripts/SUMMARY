---
File: src/scripts/arxiv_client.py
---
# src/scripts/arxiv_client.py
"""Client for interacting with arXiv API and downloading papers."""

import os
import time
import shutil
import tarfile
import tempfile
import requests
from pathlib import Path
from datetime import datetime
from typing import Optional
from loguru import logger

from .models import Paper

class ArxivClient:
    """Client for interacting with arXiv API and downloading papers."""
    
    def __init__(self, papers_dir: str | Path):
        """
        Initialize ArxivClient.
        
        Args:
            papers_dir: Base directory for paper storage
        """
        self.papers_dir = Path(papers_dir)
        self.papers_dir.mkdir(parents=True, exist_ok=True)
        
        # Rate limiting controls
        self.last_request = 0
        self.min_delay = 3  # Seconds between requests
        self.headers = {'User-Agent': 'ArxivPaperTracker/1.0'}
        self.api_base = "http://export.arxiv.org/api/query"
    
    def _wait_for_rate_limit(self):
        """Enforce rate limiting between requests."""
        now = time.time()
        time_since_last = now - self.last_request
        if time_since_last < self.min_delay:
            time.sleep(self.min_delay - time_since_last)
        self.last_request = time.time()
    
    def get_paper_dir(self, arxiv_id: str) -> Path:
        """Get paper's directory, creating if needed."""
        paper_dir = self.papers_dir / arxiv_id
        paper_dir.mkdir(parents=True, exist_ok=True)
        return paper_dir
    
    def get_paper_status(self, arxiv_id: str) -> dict:
        """
        Get current status of paper downloads.
        
        Returns:
            dict with keys:
                - has_pdf: Whether PDF exists
                - has_source: Whether source exists
                - pdf_size: Size of PDF if it exists
                - source_size: Size of source directory if it exists
        """
        paper_dir = self.papers_dir / arxiv_id
        pdf_file = paper_dir / f"{arxiv_id}.pdf"
        source_dir = paper_dir / "source"
        
        return {
            "has_pdf": pdf_file.exists(),
            "has_source": source_dir.exists(),
            "pdf_size": pdf_file.stat().st_size if pdf_file.exists() else 0,
            "source_size": sum(
                f.stat().st_size for f in source_dir.rglob('*') if f.is_file()
            ) if source_dir.exists() else 0
        }
    
    def fetch_metadata(self, arxiv_id: str) -> Paper:
        """
        Fetch paper metadata from arXiv API.
        
        Args:
            arxiv_id: The arXiv identifier
            
        Returns:
            Paper: Constructed Paper object
            
        Raises:
            ValueError: If API response is invalid
            Exception: For network or parsing errors
        """
        self._wait_for_rate_limit()
        
        try:
            url = f"{self.api_base}?id_list={arxiv_id}"
            logger.debug(f"Fetching arXiv metadata: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=30)
            if response.status_code != 200:
                raise ValueError(f"ArXiv API error: {response.status_code}")
            
            return self._parse_arxiv_response(response.text, arxiv_id)
                
        except Exception as e:
            logger.error(f"Error fetching arXiv metadata for {arxiv_id}: {e}")
            raise
        
    def _parse_arxiv_response(self, xml_text: str, arxiv_id: str) -> Paper:
        """Parse ArXiv API response XML into Paper object."""
        import xml.etree.ElementTree as ET
        
        try:
            # Parse XML
            root = ET.fromstring(xml_text)
            
            # ArXiv API uses Atom namespace
            ns = {'atom': 'http://www.w3.org/2005/Atom',
                  'arxiv': 'http://arxiv.org/schemas/atom'}
            
            # Find the entry element
            entry = root.find('.//atom:entry', ns)
            if entry is None:
                raise ValueError(f"No entry found for {arxiv_id}")
    
            # Extract metadata
            title = entry.find('atom:title', ns).text.strip()
            abstract = entry.find('atom:summary', ns).text.strip()
            authors = ", ".join(
                author.text.strip() 
                for author in entry.findall('.//atom:author/atom:name', ns)
            )
    
            # Extract URLs
            urls = {
                link.get('title', ''): link.get('href', '')
                for link in entry.findall('atom:link', ns)
            }
            html_url = urls.get('abs', f"https://arxiv.org/abs/{arxiv_id}")
    
            # Extract published date (for v1)
            published = entry.find('atom:published', ns)
            published_date = published.text if published is not None else None
    
            # Extract arXiv categories/tags
            primary_category = entry.find('arxiv:primary_category', ns)
            categories = [
                term.get('term') 
                for term in entry.findall('atom:category', ns)
            ]
            if primary_category is not None:
                primary = primary_category.get('term')
                if primary and primary not in categories:
                    categories.insert(0, primary)
    
            # Construct Paper object
            return Paper(
                arxivId=arxiv_id,
                title=title,
                authors=authors,
                abstract=abstract,
                url=html_url,
                issue_number=0,
                issue_url="",
                created_at=datetime.utcnow().isoformat(),
                state="open",
                labels=["paper"],
                total_reading_time_seconds=0,
                last_read=None,
                published_date=published_date,
                arxiv_tags=categories
            )
    
        except ET.ParseError as e:
            logger.error(f"XML parsing error for {arxiv_id}: {e}")
            raise ValueError(f"Invalid XML response from arXiv API: {e}")
        except Exception as e:
            logger.error(f"Error parsing arXiv response: {e}")
            raise
    
    def get_pdf_url(self, arxiv_id: str) -> str:
        """Get PDF URL from arXiv ID."""
        return f"https://arxiv.org/pdf/{arxiv_id}.pdf"
    
    def get_source_url(self, arxiv_id: str) -> str:
        """Get source URL from arXiv ID."""
        return f"https://arxiv.org/e-print/{arxiv_id}"

    def download_pdf(self, arxiv_id: str) -> bool:
        """
        Download PDF for a paper.
        
        Args:
            arxiv_id: Paper ID to download
            
        Returns:
            bool: True if successful
        """
        try:
            pdf_url = self.get_pdf_url(arxiv_id)
            paper_dir = self.get_paper_dir(arxiv_id)
            pdf_path = paper_dir / f"{arxiv_id}.pdf"
            
            if pdf_path.exists():
                logger.info(f"PDF already exists for {arxiv_id}")
                return True
            
            self._wait_for_rate_limit()
            logger.info(f"Downloading PDF: {pdf_path}")
            
            response = requests.get(pdf_url, headers=self.headers, timeout=30)
            if response.status_code != 200:
                raise ValueError(f"Failed to download PDF: {response.status_code}")
            
            pdf_path.write_bytes(response.content)
            return True
            
        except Exception as e:
            logger.error(f"Error downloading PDF for {arxiv_id}: {e}")
            return False

    def download_source(self, arxiv_id: str) -> bool:
        """
        Download and extract source files for a paper.
        
        Args:
            arxiv_id: Paper ID to download
            
        Returns:
            bool: True if successful
        """
        try:
            source_url = self.get_source_url(arxiv_id)
            paper_dir = self.get_paper_dir(arxiv_id)
            source_dir = paper_dir / "source"
            
            if source_dir.exists():
                logger.info(f"Source already exists for {arxiv_id}")
                return True
            
            self._wait_for_rate_limit()
            logger.info(f"Downloading source: {source_dir}")
            
            response = requests.get(source_url, headers=self.headers, timeout=30)
            if response.status_code != 200:
                raise ValueError(f"Failed to download source: {response.status_code}")
            
            # Create temporary file for the tar content
            with tempfile.NamedTemporaryFile(suffix='.tar', delete=False) as tmp_file:
                tmp_file.write(response.content)
                tmp_file_path = tmp_file.name
            
            try:
                source_dir.mkdir(exist_ok=True)
                
                # Extract tar file
                try:
                    with tarfile.open(tmp_file_path) as tar:
                        def is_within_directory(directory, target):
                            abs_directory = os.path.abspath(directory)
                            abs_target = os.path.abspath(target)
                            prefix = os.path.commonprefix([abs_directory, abs_target])
                            return prefix == abs_directory

                        def safe_extract(tar, path=".", members=None):
                            for member in tar.getmembers():
                                member_path = os.path.join(path, member.name)
                                if not is_within_directory(path, member_path):
                                    raise Exception("Attempted path traversal in tar file")
                            tar.extractall(path=path, members=members)

                        safe_extract(tar, path=source_dir)
                        
                except tarfile.ReadError:
                    # If not a tar file, just copy it as a single file
                    main_tex = source_dir / "main.tex"
                    main_tex.write_bytes(response.content)
            finally:
                # Clean up temporary file
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
            
            return True
            
        except Exception as e:
            logger.error(f"Error downloading source for {arxiv_id}: {e}")
            if source_dir.exists():
                shutil.rmtree(source_dir)  # Clean up on failure
            return False

    def download_paper(self, arxiv_id: str, skip_existing: bool = True) -> bool:
        """
        Download both PDF and source files for a paper.
        
        Args:
            arxiv_id: Paper ID to download
            skip_existing: Skip downloads if files exist
            
        Returns:
            bool: True if all downloads successful
        """
        status = self.get_paper_status(arxiv_id)
        
        if skip_existing and status["has_pdf"] and status["has_source"]:
            logger.info(f"All files already exist for {arxiv_id}")
            return True
        
        if not status["has_pdf"]:
            if not self.download_pdf(arxiv_id):
                return False
        
        if not status["has_source"]:
            if not self.download_source(arxiv_id):
                return False
        
        return True



---
File: src/scripts/asset_manager.py
---
# src/scripts/asset_manager.py
"""Manage paper assets including downloads, source files, and markdown conversions."""

import time
from pathlib import Path
from loguru import logger
from typing import Optional
import fire

from .arxiv_client import ArxivClient
from .markdown_service import MarkdownService

class PaperAssetManager:
    """Manages paper assets including PDFs, source files, and markdown conversions."""
    
    def __init__(self, papers_dir: str | Path, 
                 arxiv_client: Optional[ArxivClient] = None,
                 markdown_service: Optional[MarkdownService] = None):
        self.papers_dir = Path(papers_dir)
        self.papers_dir.mkdir(parents=True, exist_ok=True)
        self.arxiv = arxiv_client or ArxivClient(papers_dir)
        self.markdown = markdown_service or MarkdownService(papers_dir)
    
    def find_missing_pdfs(self) -> list[str]:
        """Find papers missing PDF downloads."""
        missing = []
        for paper_dir in self.papers_dir.iterdir():
            if not paper_dir.is_dir():
                continue
            arxiv_id = paper_dir.name
            status = self.arxiv.get_paper_status(arxiv_id)
            if not status["has_pdf"]:
                missing.append(arxiv_id)
        return missing
    
    def find_missing_source(self) -> list[str]:
        """Find papers missing source files."""
        missing = []
        for paper_dir in self.papers_dir.iterdir():
            if not paper_dir.is_dir():
                continue
            arxiv_id = paper_dir.name
            status = self.arxiv.get_paper_status(arxiv_id)
            if not status["has_source"]:
                missing.append(arxiv_id)
        return missing
    
    def find_pending_markdown(self) -> list[str]:
        """Find papers with source but no markdown."""
        pending = []
        for paper_dir in self.papers_dir.iterdir():
            if not paper_dir.is_dir():
                continue
            arxiv_id = paper_dir.name
            download_status = self.arxiv.get_paper_status(arxiv_id)
            markdown_status = self.markdown.get_conversion_status(arxiv_id)
            if (download_status["has_source"] and 
                not markdown_status["has_markdown"] and 
                not markdown_status["failed"]):
                pending.append(arxiv_id)
        return pending
    
    def download_pdfs(self, force: bool = False) -> dict[str, bool]:
        """Download PDFs for papers missing them."""
        papers = self.find_missing_pdfs() if not force else [
            p.name for p in self.papers_dir.iterdir() if p.is_dir()
        ]
        results = {}
        for arxiv_id in papers:
            logger.info(f"Downloading PDF for {arxiv_id}")
            success = self.arxiv.download_pdf(arxiv_id)
            results[arxiv_id] = success
        return results
    
    def download_source(self, force: bool = False) -> dict[str, bool]:
        """Download source files for papers missing them."""
        papers = self.find_missing_source() if not force else [
            p.name for p in self.papers_dir.iterdir() if p.is_dir()
        ]
        results = {}
        for arxiv_id in papers:
            logger.info(f"Downloading source for {arxiv_id}")
            success = self.arxiv.download_source(arxiv_id)
            results[arxiv_id] = success
        return results
        
    def convert_markdown(self, force: bool = False) -> dict[str, bool]:
        """Convert papers with source to markdown."""
        # Get candidate papers
        if force:
            # On force, attempt all papers that have source files
            candidates = [
                p.name for p in self.papers_dir.iterdir() 
                if p.is_dir() and self.arxiv.get_paper_status(p.name)["has_source"]
            ]
        else:
            # Get papers with source but no markdown
            candidates = []
            for paper_dir in self.papers_dir.iterdir():
                if not paper_dir.is_dir():
                    continue
                arxiv_id = paper_dir.name
                download_status = self.arxiv.get_paper_status(arxiv_id)
                markdown_status = self.markdown.get_conversion_status(arxiv_id)
                
                if (download_status["has_source"] and not markdown_status["has_markdown"]):
                    candidates.append(arxiv_id)
        
        # Process candidates
        results = {}
        for arxiv_id in candidates:
            logger.info(f"Converting {arxiv_id} to markdown")
            try:
                success = self.markdown.convert_paper(arxiv_id, force=force)
                results[arxiv_id] = success
            except Exception as e:
                logger.error(f"Error converting {arxiv_id}: {e}")
                results[arxiv_id] = False
        
        return results
    
    def ensure_all_assets(self, force: bool = False, retry_failed: bool = True):
        """Ensure all papers have complete assets."""
        if retry_failed:
            self.markdown.retry_failed_conversions(force=force)
        
        download_results = self.download_pdfs(force)
        source_results = self.download_source(force)
        markdown_results = self.convert_markdown(force)
        
        total = len(download_results) + len(source_results) + len(markdown_results)
        success = (
            sum(download_results.values()) + 
            sum(source_results.values()) + 
            sum(markdown_results.values())
        )
        
        if total == 0:
            logger.info("All paper assets are complete")
        else:
            logger.info(f"Successfully processed {success}/{total} items")

def main():
    """Command-line interface."""
    manager = PaperAssetManager(papers_dir="data/papers")
    fire.Fire({
        'ensure': manager.ensure_all_assets,
        'download-pdfs': manager.download_pdfs,
        'download-source': manager.download_source,
        'convert-markdown': manager.convert_markdown,
        'retry-failures': lambda: manager.markdown.retry_failed_conversions(force=True),
        'status': lambda: {
            'missing_pdfs': manager.find_missing_pdfs(),
            'missing_source': manager.find_missing_source(),
            'pending_markdown': manager.find_pending_markdown(),
            'failed_markdown': list(manager.markdown.failed_conversions.keys())
        }
    })

if __name__ == "__main__":
    main()



---
File: src/scripts/frontend/__init__.py
---




---
File: src/scripts/frontend/generate_html.py
---
# src/scripts/frontend/generate_html.py
import yaml
import json
from pathlib import Path
import fire
from typing import Dict, Any
from datetime import datetime

def format_authors(authors: str | list[str]) -> str:
    """Format author list consistently."""
    if isinstance(authors, str):
        author_list = [a.strip() for a in authors.split(',')]
    elif isinstance(authors, list):
        author_list = authors
    else:
        return 'Unknown authors'
    
    if len(author_list) > 4:
        return f"{', '.join(author_list[:3])} and {len(author_list) - 3} others"
    return ', '.join(author_list)

def normalize_datetime(date_str: str | None) -> datetime | None:
    """Parse datetime string to UTC datetime and strip timezone info."""
    if not date_str:
        return None
    try:
        # Replace Z with +00:00 for consistent timezone handling
        dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        # Convert to UTC if timezone aware
        if dt.tzinfo is not None:
            dt = dt.astimezone().replace(tzinfo=None)
        return dt
    except (ValueError, AttributeError):
        return None

def get_last_visited(paper: Dict[str, Any]) -> str:
    """Compute the most recent interaction time for a paper."""
    last_read = normalize_datetime(paper.get('last_read'))
    last_visited = normalize_datetime(paper.get('last_visited'))
    
    # Compare only if both exist
    if last_read and last_visited:
        latest = max(last_read, last_visited)
    elif last_read:
        latest = last_read
    elif last_visited:
        latest = last_visited
    else:
        return ''
    
    return latest.isoformat()

def preprocess_paper(paper: Dict[str, Any]) -> Dict[str, Any]:
    """Process a single paper entry."""
    return {
        'id': paper.get('arxivId', ''),
        'title': paper.get('title', '').replace('\n', ' '),
        'authors': format_authors(paper.get('authors', [])),
        'abstract': paper.get('abstract', '').replace('\n', ' '),
        'url': paper.get('url', ''),
        'arxivId': paper.get('arxivId', ''),
        'last_visited': get_last_visited(paper),
        'last_read': paper.get('last_read', ''),  # Keep for "Read on" display
        'total_reading_time_seconds': paper.get('total_reading_time_seconds', 0),
        'published_date': paper.get('published_date'),
        'arxiv_tags': paper.get('arxiv_tags', []),
    }

def preprocess_papers(papers: Dict[str, Any]) -> Dict[str, Any]:
    """Process all papers and prepare them for display."""
    # Process all papers that have either last_read or last_visited
    processed_papers = {
        id_: preprocess_paper(paper)
        for id_, paper in papers.items()
        if paper.get('last_read') or paper.get('last_visited')
    }
    
    return processed_papers

def generate_html(
    data_path: str,
    template_path: str,
    output_path: str,
) -> None:
    """Generate HTML page from papers data and template.
    
    Args:
        data_path: Path to papers YAML file
        template_path: Path to HTML template file
        output_path: Path where generated HTML should be written
    """
    # Convert all paths to Path objects
    data_path = Path(data_path)
    template_path = Path(template_path)
    output_path = Path(output_path)

    # Create output directory if it doesn't exist
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Read the papers YAML
    with open(data_path, 'r', encoding='utf-8') as f:
        papers = yaml.safe_load(f)
    
    # Preprocess the papers data
    processed_papers = preprocess_papers(papers)
    
    # Read the template
    with open(template_path, 'r', encoding='utf-8') as f:
        template = f.read()
    
    # Convert processed papers to JSON
    papers_json = json.dumps(
        processed_papers,
        indent=2,
        ensure_ascii=False
    )
    
    # Replace the placeholder in template
    html = template.replace(
        'window.yamlData = {};',
        f'window.yamlData = {papers_json};'
    )
    
    # Write the final HTML
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html)

if __name__ == '__main__':
    fire.Fire(generate_html)



---
File: src/scripts/frontend/index.template.html
---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Paper Feed</title>
    <style>
        :root {
            /* Controls panel variables */
            --panel-bg: #ffffff;
            --panel-shadow: rgba(0, 0, 0, 0.1);
            --panel-border: #e2e8f0;
            --button-hover: #f8fafc;
            
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --text-color: #1e293b;
            --secondary-text: #64748b;
            --border-color: #e2e8f0;
            --accent-bg: #f1f5f9;
            --link-color: #2563eb;
        }

        body {
            font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 2rem 1rem;
            margin: 0;
        }

        .container {
            max-width: 860px;
            margin: 0 auto;
        }

        header {
            margin-bottom: 3rem;
            text-align: center;
        }

        h1 {
            font-size: 2.25rem;
            font-weight: 800;
            margin-bottom: 1rem;
        }

        .header-desc {
            color: var(--secondary-text);
            font-size: 1.125rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .day-group {
            margin-bottom: 1.5rem;
        }

        .day-header {
            background: var(--accent-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1rem 1.5rem;
            margin-bottom: 1rem;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .day-header:hover {
            background: #e9eef5;
        }

        .papers-container {
            display: grid;
            grid-template-rows: 1fr;
            transition: grid-template-rows 0.3s ease-out;
        }

        .papers-container-inner {
            overflow: hidden;
        }

        .collapsed .papers-container {
            grid-template-rows: 0fr;
        }

        /* Controls Panel Styles */
        .controls-button {
            position: fixed;
            top: 1rem;
            right: 1rem;
            z-index: 1000;
            padding: 0.5rem;
            background: var(--panel-bg);
            border: 1px solid var(--panel-border);
            border-radius: 0.5rem;
            cursor: pointer;
            box-shadow: 0 2px 4px var(--panel-shadow);
            transition: all 0.2s ease;
        }

        .controls-button:hover {
            background: var(--button-hover);
        }

        .controls-panel {
            position: fixed;
            top: 1rem;
            right: 1rem;
            width: 300px;
            background: var(--panel-bg);
            border: 1px solid var(--panel-border);
            border-radius: 0.5rem;
            padding: 1rem;
            box-shadow: 0 4px 6px var(--panel-shadow);
            z-index: 999;
            transform: translateX(120%);
            transition: transform 0.3s ease;
        }

        .controls-panel.expanded {
            transform: translateX(0);
        }

        .controls-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--panel-border);
        }

        .controls-title {
            font-weight: 600;
            color: var(--text-color);
        }

        .close-controls {
            cursor: pointer;
            color: var(--secondary-text);
        }

        .control-group {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .radio-group {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-bottom: 0.5rem;
        }

        .radio-label {
            font-weight: 500;
            color: var(--text-color);
            white-space: nowrap;
            min-width: 4rem;
        }

        .radio-options {
            display: flex;
            gap: 1rem;
        }

        .radio-option {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--text-color);
            cursor: pointer;
            white-space: nowrap;
        }

        .radio-option input {
            cursor: pointer;
        }

        .control-button {
            width: 100%;
            padding: 0.5rem;
            background: var(--accent-bg);
            border: 1px solid var(--border-color);
            border-radius: 0.25rem;
            color: var(--text-color);
            font-weight: 500;
            cursor: pointer;
            transition: background 0.2s ease;
        }

        .control-button:hover {
            background: var(--button-hover);
        }

        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 0.75rem 1.5rem;
            margin-bottom: 0.5rem;
            cursor: pointer;
        }

        .paper-header {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .expand-icon {
            color: var(--secondary-text);
            transition: transform 0.3s ease;
            font-size: 0.75rem;
            min-width: 12px;
        }

        .expanded .expand-icon {
            transform: rotate(90deg);
        }

        .arxiv-id {
            font-family: ui-monospace, SFMono-Regular, Menlo, monospace;
            color: var(--link-color);
            font-size: 0.9rem;
            min-width: 90px;
        }

        .paper-title {
            font-size: 1rem;
            font-weight: 500;
            margin: 0;
            color: var(--text-color);
        }

        .paper-content {
            overflow: hidden;
            display: grid;
            grid-template-rows: 0fr;
            transition: grid-template-rows 0.3s ease-out;
        }

        .paper-content-inner {
            overflow: hidden;
            padding-top: 1rem;
            margin-top: 1rem;
            border-top: 1px solid var(--border-color);
        }

        .expanded .paper-content {
            grid-template-rows: 1fr;
        }

        .paper-meta {
            margin-bottom: 1rem;
            color: var(--secondary-text);
        }

        .meta-divider {
            margin: 0 0.5rem;
        }

        /* Tag filtering styles */
        .filter-container {
            margin-bottom: 2rem;
            padding: 1rem;
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
        }

        .filter-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
        }

        .filter-mode {
            display: flex;
            gap: 0.5rem;
        }

        .mode-button {
            padding: 0.25rem 0.75rem;
            border: 1px solid var(--border-color);
            border-radius: 1rem;
            background: var(--bg-color);
            cursor: pointer;
            font-size: 0.875rem;
        }

        .mode-button.active {
            background: var(--link-color);
            color: white;
            border-color: var(--link-color);
        }

        .filter-stats {
            font-size: 0.875rem;
            color: var(--secondary-text);
        }

        .tag-cloud {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-bottom: 1rem;
        }

        .filter-actions {
            display: flex;
            gap: 0.5rem;
            justify-content: flex-end;
        }

        .filter-action {
            padding: 0.25rem 0.75rem;
            border: 1px solid var(--border-color);
            border-radius: 0.25rem;
            background: var(--bg-color);
            cursor: pointer;
            font-size: 0.875rem;
        }

        .filter-action:hover {
            background: var(--button-hover);
        }

        .tooltip {
            position: absolute;
            bottom: 100%;
            left: 50%;
            transform: translateX(-50%);
            padding: 0.5rem;
            background: var(--text-color);
            color: white;
            border-radius: 0.25rem;
            font-size: 0.75rem;
            white-space: nowrap;
            opacity: 0;
            pointer-events: none;
            transition: opacity 0.2s ease;
            margin-bottom: 0.5rem;
            z-index: 1000;
        }

        .tag-pill:hover .tooltip {
            opacity: 1;
        }

        /* Hide papers that don't match the filter */
        .paper-card.filtered {
            display: none;
        }

        @media (max-width: 640px) {
            body {
                padding: 1rem;
            }

            .paper-card {
                padding: 0.75rem 1rem;
            }

            .paper-header {
                gap: 0.75rem;
            }
        }

        .coloring-controls {
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid var(--panel-border);
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .toggle-switch {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .coloring-type {
            margin-left: 0.5rem;
        }

        .toggle-label {
            font-size: 0.875rem;
            color: var(--text-color);
        }

        .switch {
            position: relative;
            display: inline-block;
            width: 36px;
            height: 20px;
        }

        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: var(--secondary-text);
            transition: .4s;
            border-radius: 20px;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 16px;
            width: 16px;
            left: 2px;
            bottom: 2px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }

        input:checked + .slider {
            background-color: var(--link-color);
        }

        input:checked + .slider:before {
            transform: translateX(16px);
        }
    </style>
</head>
<body>
    <button class="controls-button" id="showControls" aria-label="Show controls">⚙️</button>
    
    <div class="controls-panel" id="controlsPanel">
        <div class="controls-header">
            <span class="controls-title">Display Controls</span>
            <span class="close-controls" id="closeControls">×</span>
        </div>
        <div class="controls-content">
            <div class="controls-section">
                <div class="controls-subtitle">Display Options</div>
                <div class="control-group">
                    <div class="radio-group">
                        <div class="radio-label">Target:</div>
                        <div class="radio-options">
                            <label class="radio-option">
                                <input type="radio" name="target" value="days" checked>
                                Days
                            </label>
                            <label class="radio-option">
                                <input type="radio" name="target" value="papers">
                                Papers
                            </label>
                        </div>
                    </div>
                    <div class="radio-group">
                        <div class="radio-label">Action:</div>
                        <div class="radio-options">
                            <label class="radio-option">
                                <input type="radio" name="action" value="collapse" checked>
                                Collapse
                            </label>
                            <label class="radio-option">
                                <input type="radio" name="action" value="expand">
                                Expand
                            </label>
                        </div>
                    </div>
                    <div class="coloring-controls">
                        <div class="toggle-switch">
                            <span class="toggle-label">Enable ID coloring</span>
                            <label class="switch">
                                <input type="checkbox" id="coloringToggle" checked>
                                <span class="slider"></span>
                            </label>
                        </div>
                        <div class="radio-group coloring-type">
                            <div class="radio-label">Color by:</div>
                            <div class="radio-options">
                                <label class="radio-option">
                                    <input type="radio" name="colorBy" value="freshness" checked>
                                    Freshness
                                </label>
                                <label class="radio-option">
                                    <input type="radio" name="colorBy" value="readingTime">
                                    Reading Time
                                </label>
                            </div>
                        </div>
                    </div>
                    <button id="executeAction" class="control-button">Apply</button>
                </div>
            </div>
            <div class="controls-section">
                <div class="controls-subtitle">Category Filters</div>
                    <div class="filter-mode">
                        <button class="mode-button active" data-mode="any">ANY</button>
                        <button class="mode-button" data-mode="all">ALL</button>
                        <button class="mode-button" data-mode="none">NONE</button>
                    </div>
                    <div class="filter-stats">
                        Showing <span id="filtered-count">0</span> of <span id="total-count">0</span> papers
                    </div>
                    <div class="tag-cloud" id="tag-cloud">
                        <!-- Tags will be inserted here -->
                    </div>
                    <div class="filter-actions">
                        <button class="filter-action" id="clear-filters">Clear</button>
                        <button class="filter-action" id="select-all">Select All</button>
                    </div>
                </div>
            </div>
    </div>

    <div class="container">
        <header>
            <h1>ArXiv Paper Feed</h1>
            <p class="header-desc">Papers recently visited by <a href="https://bsky.app/profile/digthatdata.bsky.social">@DigThatData</a></p>
        </header>

        <main id="papers-container">
            <!-- Papers will be inserted here -->
        </main>
    </div>

    <script>
        // Styles for tag pills
        const styles = `
            .tag-pill {
                padding: 0.25rem 0.75rem;
                border-radius: 1rem;
                border: 1px solid transparent;
                cursor: pointer;
                font-size: 0.875rem;
                display: flex;
                align-items: center;
                gap: 0.5rem;
                transition: all 0.2s ease;
                position: relative;
            }

            .tag-pill:hover {
                filter: brightness(0.95);
            }

            .tag-pill.active {
                border: 1px solid rgba(0, 0, 0, 0.2);
                filter: brightness(0.9);
            }

            .tag-count {
                background: rgba(0, 0, 0, 0.1);
                padding: 0.125rem 0.375rem;
                border-radius: 1rem;
                font-size: 0.75rem;
            }
        `;

        function formatDate(dateString, format = 'full') {
            const date = new Date(dateString);
            if (format === 'full') {
                return date.toLocaleDateString('en-US', {
                    year: 'numeric',
                    month: 'short',
                    day: 'numeric'
                });
            } else if (format === 'group') {
                return date.toLocaleDateString('en-US', {
                    weekday: 'long',
                    year: 'numeric',
                    month: 'long',
                    day: 'numeric'
                });
            }
        }

        function toggleDayGroup(element) {
            const group = element.closest('.day-group');
            group.classList.toggle('collapsed');
            const date = group.dataset.date;
            const collapsedDays = JSON.parse(localStorage.getItem('collapsedDays') || '{}');
            collapsedDays[date] = group.classList.contains('collapsed');
            localStorage.setItem('collapsedDays', JSON.stringify(collapsedDays));
        }

        function togglePaperCard(element, event) {
            event.stopPropagation();
            const card = element.closest('.paper-card');
            card.classList.toggle('expanded');
            const paperId = card.dataset.paperId;
            const expandedCards = JSON.parse(localStorage.getItem('expandedCards') || '{}');
            expandedCards[paperId] = card.classList.contains('expanded');
            localStorage.setItem('expandedCards', JSON.stringify(expandedCards));
        }

        function getCategoryInfo(tag) {
            // Get the parent category (everything before the dot)
            const parentCategory = tag.split('.')[0];
            
            // Using ColorBrewer Set3 qualitative palette, optimized for colorblind accessibility
            const parentCategoryMap = {
                'cs': { color: '#8dd3c7', category: 'Computer Science' },
                'stat': { color: '#ffffb3', category: 'Statistics' },
                'math': { color: '#bebada', category: 'Mathematics' },
                'physics': { color: '#fb8072', category: 'Physics' },
                'q-bio': { color: '#80b1d3', category: 'Quantitative Biology' },
                'q-fin': { color: '#fdb462', category: 'Quantitative Finance' }
            };
            
            // Map of specific subcategory names
            const subcategoryMap = {
                // Computer Science
                'cs.AI': 'Artificial Intelligence',
                'cs.LG': 'Machine Learning',
                'cs.CL': 'Computation and Language',
                'cs.CV': 'Computer Vision and Pattern Recognition',
                'cs.RO': 'Robotics',
                'cs.NE': 'Neural and Evolutionary Computing',
                'cs.IR': 'Information Retrieval',
                'cs.HC': 'Human-Computer Interaction',
                'cs.SI': 'Social and Information Networks',
                'cs.DB': 'Databases',
                
                // Statistics
                'stat.ML': 'Machine Learning (Statistics)',
                'stat.ME': 'Methodology',
                'stat.TH': 'Statistics Theory',
                
                // Mathematics
                'math.ST': 'Statistics Theory',
                'math.PR': 'Probability',
                'math.OC': 'Optimization',
                
                // Physics
                'physics.data-an': 'Data Analysis',
                'physics.soc-ph': 'Social Physics',
                
                // Quantitative Biology
                'q-bio.NC': 'Neurons and Cognition',
                'q-bio.QM': 'Quantitative Methods',
                
                // Quantitative Finance
                'q-fin.ST': 'Statistical Finance',
                'q-fin.PM': 'Portfolio Management'
            };
            
            const parentInfo = parentCategoryMap[parentCategory] || { color: '#f5f5f5', category: 'Other' };
            const name = subcategoryMap[tag] || tag;
            
            return {
                name: name,
                color: parentInfo.color
            };
        }

        function renderTagCloud() {
            const tags = new Map();
            
            // Collect tags and counts
            Object.values(window.yamlData).forEach(paper => {
                if (paper.arxiv_tags) {
                    paper.arxiv_tags.forEach(tag => {
                        const count = tags.get(tag) || 0;
                        tags.set(tag, count + 1);
                    });
                }
            });

            // Sort tags by count
            const sortedTags = Array.from(tags.entries())
                .sort(([, a], [, b]) => b - a);

            // Render tag cloud
            const tagCloud = document.getElementById('tag-cloud');
            tagCloud.innerHTML = sortedTags.map(([tag, count]) => {
                const { name, color } = getCategoryInfo(tag);
                return `
                    <button class="tag-pill" data-tag="${tag}" style="background-color: ${color}">
                        <span class="tag-name">${tag}</span>
                        <span class="tag-count">${count}</span>
                        <span class="tooltip">${name}</span>
                    </button>
                `;
            }).join('');

            // Re-add click handlers
            document.querySelectorAll('.tag-pill').forEach(pill => {
                pill.addEventListener('click', () => {
                    const tag = pill.dataset.tag;
                    if (window.filterState.activeTags.has(tag)) {
                        window.filterState.activeTags.delete(tag);
                        pill.classList.remove('active');
                    } else {
                        window.filterState.activeTags.add(tag);
                        pill.classList.add('active');
                    }
                    applyFilters();
                });
            });
        }
        
        function calculateColor(paper, coloringEnabled = true) {
            if (!coloringEnabled) return 'rgb(255, 255, 255)';  // White when coloring is disabled
            
            const colorBy = document.querySelector('input[name="colorBy"]:checked').value;
            
            if (colorBy === 'freshness') {
                if (!paper.last_visited || !paper.published_date) return 'rgb(255, 255, 255)';
                
                const visitDate = new Date(paper.last_visited);
                const pubDate = new Date(paper.published_date);
                const diffDays = Math.floor((visitDate - pubDate) / (1000 * 60 * 60 * 24));
                
                const maxAge = 365;
                const freshness = Math.max(0, Math.min(1, 1 - (diffDays / maxAge)));
                const value = Math.round(255 - (freshness * 55));
                return `rgb(${value}, 255, ${value})`; // Green gradient
            } else {
                // Reading time coloring
                const readingTime = paper.total_reading_time_seconds || 0;
                const maxReadingTime = 300; // 5 minutes
                const intensity = Math.max(0, Math.min(1, readingTime / maxReadingTime));
                const value = Math.round(255 - (intensity * 55));
                return `rgb(255, ${value}, ${value})`; // Red gradient
            }
        }
        
        function renderPaperCard(paper, expanded) {
            const readingTime = paper.total_reading_time_seconds 
                ? `${Math.round(paper.total_reading_time_seconds / 60)} min read`
                : '';
        
            // Get freshness toggle state
            const coloringEnabled = document.getElementById('coloringToggle')?.checked ?? true;
            const bgColor = calculateColor(paper, coloringEnabled);
            
            const metaParts = [];
            
            // Add authors
            metaParts.push(`<span>${paper.authors}</span>`);
            
            // Add reading time if available
            if (readingTime) {
                metaParts.push(`<span class="meta-divider">•</span><span>${readingTime}</span>`);
            }
            
            // Add publication date if available
            if (paper.published_date) {
                const pubDate = new Date(paper.published_date).toLocaleDateString();
                metaParts.push(`<span class="meta-divider">•</span><span>Published: ${pubDate}</span>`);
            }
            
            // Add arXiv tags if available
            if (paper.arxiv_tags && paper.arxiv_tags.length > 0) {
                const tags = paper.arxiv_tags.join(', ');
                metaParts.push(`<span class="meta-divider">•</span><span>${tags}</span>`);
            }
            
            return `
                <article class="paper-card ${expanded ? 'expanded' : ''}" data-paper-id="${paper.id}">
                    <div class="paper-header">
                        <span class="expand-icon">▶</span>
                        <a href="${paper.url}" class="arxiv-id" onclick="event.stopPropagation()" 
                           style="background-color: ${bgColor}; padding: 4px 8px; border-radius: 4px;">
                            ${paper.arxivId}
                        </a>
                        <span class="paper-title">${paper.title}</span>
                    </div>
                    <div class="paper-content">
                        <div class="paper-content-inner">
                            <div class="paper-meta">
                                ${metaParts.join('')}
                            </div>
                            <div class="paper-abstract">${paper.abstract}</div>
                        </div>
                    </div>
                </article>
            `;
        }
        
        function initializeFilters() {
            // Add the updated styles to the document
            const styleSheet = document.createElement("style");
            styleSheet.textContent = styles;
            document.head.appendChild(styleSheet);

            // Initialize filter state
            window.filterState = {
                mode: 'any',
                activeTags: new Set()
            };

            // Render initial tag cloud
            renderTagCloud();

            // Update total count
            document.getElementById('total-count').textContent = 
                Object.keys(window.yamlData).length;

            // Mode buttons already handled in initializeEventListeners()
        }
        
        function renderPapers() {
            const container = document.getElementById('papers-container');
            container.innerHTML = '';
            const expandedCards = JSON.parse(localStorage.getItem('expandedCards') || '{}');
            const collapsedDays = JSON.parse(localStorage.getItem('collapsedDays') || '{}');
            
            const papersByDay = {};
            Object.entries(window.yamlData)
                .sort(([_, a], [__, b]) => new Date(b.last_visited) - new Date(a.last_visited))
                .forEach(([id, paper]) => {
                    const date = paper.last_visited.split('T')[0];
                    if (!papersByDay[date]) papersByDay[date] = [];
                    papersByDay[date].push({ ...paper, id });
                });
        
            Object.entries(papersByDay).forEach(([date, papers]) => {
                const dayGroup = document.createElement('section');
                dayGroup.className = `day-group ${collapsedDays[date] ? 'collapsed' : ''}`;
                dayGroup.dataset.date = date;
        
                const dayHeader = document.createElement('div');
                dayHeader.className = 'day-header';
                dayHeader.onclick = () => toggleDayGroup(dayHeader);
                dayHeader.innerHTML = `
                    <span class="day-title">${formatDate(date, 'group')}</span>
                    <span class="paper-count">${papers.length} paper${papers.length !== 1 ? 's' : ''}</span>
                `;
        
                const papersContainer = document.createElement('div');
                papersContainer.className = 'papers-container';
        
                const papersContainerInner = document.createElement('div');
                papersContainerInner.className = 'papers-container-inner';
                papersContainerInner.innerHTML = papers
                    .map(paper => renderPaperCard(paper, expandedCards[paper.id]))
                    .join('');
        
                papersContainer.appendChild(papersContainerInner);
                dayGroup.appendChild(dayHeader);
                dayGroup.appendChild(papersContainer);
                container.appendChild(dayGroup);
            });

            // Add click handlers for paper cards
            document.querySelectorAll('.paper-card').forEach(card => {
                card.onclick = (e) => togglePaperCard(card, e);
            });

            // Initialize filters after papers are rendered
            applyFilters();
        }

        function initializeEventListeners() {
            // Add coloring controls listeners
            const coloringToggle = document.getElementById('coloringToggle');
            if (coloringToggle) {
                // Load saved preferences
                const savedColoring = localStorage.getItem('coloringEnabled');
                if (savedColoring !== null) {
                    coloringToggle.checked = savedColoring === 'true';
                }
                
                const savedColorBy = localStorage.getItem('colorBy');
                if (savedColorBy) {
                    const radio = document.querySelector(`input[name="colorBy"][value="${savedColorBy}"]`);
                    if (radio) radio.checked = true;
                }
                
                // Add listeners
                coloringToggle.addEventListener('change', () => {
                    localStorage.setItem('coloringEnabled', coloringToggle.checked);
                    renderPapers();
                });
                
                document.querySelectorAll('input[name="colorBy"]').forEach(radio => {
                    radio.addEventListener('change', () => {
                        localStorage.setItem('colorBy', radio.value);
                        renderPapers();
                    });
                });
            }

            // Mode buttons
            document.querySelectorAll('.mode-button').forEach(button => {
                button.addEventListener('click', () => {
                    document.querySelectorAll('.mode-button').forEach(b => 
                        b.classList.remove('active'));
                    button.classList.add('active');
                    window.filterState.mode = button.dataset.mode;
                    applyFilters();
                });
            });

            // Clear filters
            document.getElementById('clear-filters').addEventListener('click', () => {
                window.filterState.activeTags.clear();
                document.querySelectorAll('.tag-pill').forEach(pill => 
                    pill.classList.remove('active'));
                applyFilters();
            });

            // Select all
            document.getElementById('select-all').addEventListener('click', () => {
                document.querySelectorAll('.tag-pill').forEach(pill => {
                    const tag = pill.dataset.tag;
                    window.filterState.activeTags.add(tag);
                    pill.classList.add('active');
                });
                applyFilters();
            });

            // Controls panel functionality
            const showControls = document.getElementById('showControls');
            const controlsPanel = document.getElementById('controlsPanel');
            const closeControls = document.getElementById('closeControls');

            showControls.addEventListener('click', () => {
                controlsPanel.classList.add('expanded');
                showControls.style.visibility = 'hidden';
            });

            closeControls.addEventListener('click', () => {
                controlsPanel.classList.remove('expanded');
                showControls.style.visibility = 'visible';
            });

            // Close panel when clicking outside
            document.addEventListener('click', (event) => {
                if (!controlsPanel.contains(event.target) && 
                    event.target !== showControls && 
                    controlsPanel.classList.contains('expanded')) {
                    controlsPanel.classList.remove('expanded');
                    showControls.style.visibility = 'visible';
                }
            });

            // Bulk collapse/expand functionality
            document.getElementById('executeAction').addEventListener('click', () => {
                const target = document.querySelector('input[name="target"]:checked').value;
                const action = document.querySelector('input[name="action"]:checked').value;
                const shouldCollapse = action === 'collapse';
                
                if (target === 'days') {
                    toggleAllDays(shouldCollapse);
                } else {
                    toggleAllPapers(shouldCollapse);
                }
            });
        }

        function toggleAllDays(shouldCollapse) {
            const dayGroups = document.querySelectorAll('.day-group');
            const collapsedDays = JSON.parse(localStorage.getItem('collapsedDays') || '{}');
            
            dayGroups.forEach(group => {
                if (shouldCollapse) {
                    group.classList.add('collapsed');
                    collapsedDays[group.dataset.date] = true;
                } else {
                    group.classList.remove('collapsed');
                    delete collapsedDays[group.dataset.date];
                }
            });
            
            localStorage.setItem('collapsedDays', JSON.stringify(collapsedDays));
        }

        function toggleAllPapers(shouldCollapse) {
            const visiblePapers = document.querySelectorAll('.day-group:not(.collapsed) .paper-card');
            const expandedCards = JSON.parse(localStorage.getItem('expandedCards') || '{}');
            
            visiblePapers.forEach(card => {
                if (shouldCollapse) {
                    card.classList.remove('expanded');
                    delete expandedCards[card.dataset.paperId];
                } else {
                    card.classList.add('expanded');
                    expandedCards[card.dataset.paperId] = true;
                }
            });
            
            localStorage.setItem('expandedCards', JSON.stringify(expandedCards));
        }

        function applyFilters() {
            const { mode, activeTags } = window.filterState;
            let visibleCount = 0;

            document.querySelectorAll('.paper-card').forEach(card => {
                const paperId = card.dataset.paperId;
                const paper = window.yamlData[paperId];
                const paperTags = new Set(paper.arxiv_tags || []);

                let visible = true;
                if (activeTags.size > 0) {
                    if (mode === 'any') {
                        visible = Array.from(activeTags).some(tag => 
                            paperTags.has(tag));
                    } else if (mode === 'all') {
                        visible = Array.from(activeTags).every(tag => 
                            paperTags.has(tag));
                    } else if (mode === 'none') {
                        visible = Array.from(activeTags).every(tag => 
                            !paperTags.has(tag));
                    }
                }

                card.classList.toggle('filtered', !visible);
                if (visible) visibleCount++;
            });

            document.getElementById('filtered-count').textContent = visibleCount;
        }

        window.yamlData = {};
        initializeEventListeners();
        initializeFilters();
        renderPapers();
        
    </script>
</body>
</html>



---
File: src/scripts/github_client.py
---
# src/scripts/github_client.py
from typing import List, Dict, Any
import requests
from loguru import logger

def patch_schema_change(issue):
    if 'duration_minutes' in issue:
        issue['duration_seconds'] = issue.pop('duraction_minutes') * 60
    return issue

class GithubClient:
    """Handles GitHub API interactions."""
    def __init__(self, token: str, repo: str):
        self.token = token
        self.repo = repo
        self.headers = {
            "Authorization": f"token {token}",
            "Accept": "application/vnd.github.v3+json"
        }

    def get_open_issues(self) -> List[Dict[str, Any]]:
        """Fetch open issues with paper or reading-session labels."""
        url = f"https://api.github.com/repos/{self.repo}/issues"
        params = {"state": "open", "per_page": 100}
        outv=[]
        response = requests.get(url, headers=self.headers, params=params, timeout=30)
        if response.status_code == 200:
            all_issues = response.json()
            outv = [
                patch_schema_change(issue) for issue in all_issues
                if any(label['name'] in ['paper', 'reading-session'] 
                      for label in issue['labels'])
            ]
        return outv

    def close_issue(self, issue_number: int) -> bool:
        """Close an issue with comment."""
        base_url = f"https://api.github.com/repos/{self.repo}/issues/{issue_number}"
        
        # Add comment
        comment_data = {"body": "✅ Event processed and recorded. Closing this issue."}
        comment_response = requests.post(
            f"{base_url}/comments", 
            headers=self.headers, 
            json=comment_data,
            timeout=30
        )
        if comment_response.status_code != 201:
            logger.error(f"Failed to add comment to issue {issue_number}")
            return False

        # Close issue
        close_data = {"state": "closed"}
        close_response = requests.patch(
            base_url, 
            headers=self.headers, 
            json=close_data,
            timeout=30
        )
        if close_response.status_code != 200:
            logger.error(f"Failed to close issue {issue_number}")
            return False

        return True



---
File: src/scripts/markdown_service.py
---
# src/scripts/markdown_service.py
"""Service for managing markdown conversions of arXiv papers."""

from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import Optional
from loguru import logger

from .pandoc_utils import PandocConverter, create_default_config
from .tex_utils import find_main_tex_file

class MarkdownService:
    """Manages the conversion of LaTeX papers to Markdown format."""
    
    def __init__(self, papers_dir: str | Path):
        """
        Initialize MarkdownService.
        
        Args:
            papers_dir: Base directory for paper storage
        """
        self.papers_dir = Path(papers_dir)
        self.failed_conversions_file = self.papers_dir / "failed_markdown.json"
        self._load_failed_conversions()
        
    def _load_failed_conversions(self):
        """Load record of failed conversions with timestamps."""
        self.failed_conversions = {}
        if self.failed_conversions_file.exists():
            import json
            try:
                self.failed_conversions = json.loads(
                    self.failed_conversions_file.read_text()
                )
            except Exception as e:
                logger.error(f"Error loading failed conversions: {e}")
    
    def _save_failed_conversions(self):
        """Save record of failed conversions."""
        import json
        try:
            self.failed_conversions_file.write_text(
                json.dumps(self.failed_conversions, indent=2)
            )
        except Exception as e:
            logger.error(f"Error saving failed conversions: {e}")
    
    def _record_failure(self, arxiv_id: str, error: str):
        """Record a conversion failure with timestamp."""
        self.failed_conversions[arxiv_id] = {
            "last_attempt": datetime.now(timezone.utc).isoformat(),
            "error": str(error)
        }
        self._save_failed_conversions()
    
    def _clear_failure(self, arxiv_id: str):
        """Clear a failure record after successful conversion."""
        if arxiv_id in self.failed_conversions:
            del self.failed_conversions[arxiv_id]
            self._save_failed_conversions()
    
    def should_retry_conversion(self, arxiv_id: str, retry_after_hours: int = 24) -> bool:
        """
        Check if we should retry a failed conversion.
        
        Args:
            arxiv_id: Paper ID to check
            retry_after_hours: Hours to wait before retrying
            
        Returns:
            bool: True if enough time has passed to retry
        """
        if arxiv_id not in self.failed_conversions:
            return True
            
        last_attempt = datetime.fromisoformat(
            self.failed_conversions[arxiv_id]["last_attempt"]
        )
        retry_threshold = datetime.now(timezone.utc) - timedelta(hours=retry_after_hours)
        return last_attempt < retry_threshold
    
    def convert_paper(self, arxiv_id: str, force: bool = False, tex_file: Optional[Path] = None) -> bool:
        """
        Convert a paper's LaTeX source to Markdown.
        
        Args:
            arxiv_id: Paper ID to convert
            force: Force conversion even if previously failed
            tex_file: Optional specific tex file to use for conversion
        """
        try:
            # Check if we should skip conversion
            if not force:
                if not self.should_retry_conversion(arxiv_id):
                    logger.info(f"Skipping recent failed conversion for {arxiv_id}")
                    return False
                    
                paper_dir = self.papers_dir / arxiv_id
                markdown_file = paper_dir / f"{arxiv_id}.md"
                if markdown_file.exists() and markdown_file.stat().st_size > 0:
                    logger.info(f"Markdown already exists for {arxiv_id}")
                    self._clear_failure(arxiv_id)
                    return True
            
            paper_dir = self.papers_dir / arxiv_id
            source_dir = paper_dir / "source"
            markdown_file = paper_dir / f"{arxiv_id}.md"
            
            # Verify source exists
            if not source_dir.exists():
                raise FileNotFoundError(f"No source directory for {arxiv_id}")
            
            # Check metadata for main_tex_file first
            metadata_file = paper_dir / "metadata.json"
            if metadata_file.exists():
                import json
                try:
                    metadata = json.loads(metadata_file.read_text())
                    if metadata.get('main_tex_file'):
                        specified_tex = paper_dir / metadata['main_tex_file']
                        if specified_tex.exists():
                            main_tex = specified_tex
                            logger.info(f"Using main_tex_file from metadata: {main_tex}")
                        else:
                            logger.warning(f"Specified main_tex_file does not exist: {specified_tex}")
                except Exception as e:
                    logger.warning(f"Error reading metadata.json: {e}")

            # Fall back to provided tex_file or inference if needed
            if not locals().get('main_tex'):
                if tex_file is not None:
                    if not tex_file.exists():
                        raise FileNotFoundError(f"Specified tex file does not exist: {tex_file}")
                    main_tex = tex_file
                else:
                    # Find main tex file
                    tex_files = list(source_dir.rglob("*.tex"))
                    if not tex_files:
                        raise FileNotFoundError(f"No .tex files found for {arxiv_id}")
                    
                    main_tex = find_main_tex_file(tex_files, arxiv_id)
                    if not main_tex:
                        raise ValueError(f"Could not identify main tex file for {arxiv_id}")
            
            # Set up Pandoc conversion
            config = create_default_config(paper_dir)
            converter = PandocConverter(config)
            
            # Attempt conversion - will raise exception on failure
            converter.convert_tex_to_markdown(main_tex, markdown_file)
            logger.success(f"Successfully converted {arxiv_id} to Markdown")
            self._clear_failure(arxiv_id)
            return True
                
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Error converting {arxiv_id} to Markdown: {error_msg}")
            self._record_failure(arxiv_id, error_msg)
            return False
    
    def retry_failed_conversions(self, force: bool = False):
        """
        Retry converting papers that previously failed.
        
        Args:
            force: Force retry all failed conversions regardless of timing
        """
        for arxiv_id in list(self.failed_conversions.keys()):
            if force or self.should_retry_conversion(arxiv_id):
                logger.info(f"Retrying conversion for {arxiv_id}")
                self.convert_paper(arxiv_id, force=force)

    def get_conversion_status(self, arxiv_id: str) -> dict:
        """
        Get the current conversion status for a paper.
        
        Args:
            arxiv_id: Paper ID to check
            
        Returns:
            dict: Status information including:
                - has_markdown: Whether markdown exists
                - has_source: Whether source exists
                - failed: Whether conversion previously failed
                - last_attempt: Timestamp of last attempt if failed
                - error: Error message if failed
        """
        paper_dir = self.papers_dir / arxiv_id
        return {
            "has_markdown": (paper_dir / f"{arxiv_id}.md").exists(),
            "has_source": (paper_dir / "source").exists(),
            "failed": arxiv_id in self.failed_conversions,
            "last_attempt": self.failed_conversions.get(arxiv_id, {}).get("last_attempt"),
            "error": self.failed_conversions.get(arxiv_id, {}).get("error")
        }



---
File: src/scripts/models.py
---
from pydantic import BaseModel, Field
from datetime import datetime, timezone

class Paper(BaseModel):
    """Schema for paper metadata"""
    arxiv_id: str = Field(..., alias="arxivId")
    title: str
    authors: str
    abstract: str
    url: str
    issue_number: int
    issue_url: str
    created_at: str
    state: str
    labels: list[str]
    total_reading_time_seconds: int = 0
    last_read: str | None = None
    last_visited: str | None = None
    main_tex_file: str | None = None  # Path to main TeX file used for conversion
    
    published_date: str | None = None  # v1 publication date on arXiv
    arxiv_tags: list[str] | None = None 
    
    class Config:
        populate_by_name = True

class ReadingSession(BaseModel):
    """Schema for reading session events"""
    type: str = "reading_session"
    arxiv_id: str = Field(..., alias="arxivId") 
    timestamp: str = Field(..., description="Original timestamp when reading occurred")
    duration_seconds: int
    issue_url: str
    processed_at: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

class PaperVisitEvent(BaseModel):
    """Schema for paper visit events"""
    type: str = "paper_visit" 
    timestamp: str = Field(..., description="Original timestamp when visit occurred")
    issue_url: str
    arxiv_id: str



---
File: src/scripts/pandoc_utils.py
---
# src/scripts/pandoc_utils.py
"""Utilities for converting LaTeX papers to Markdown using Pandoc."""

import os
import shutil
import subprocess
import tempfile
from pathlib import Path
from typing import Optional
from loguru import logger
from dataclasses import dataclass

@dataclass
class PandocConfig:
    """Configuration for Pandoc conversion."""
    extract_media_dir: Path
    metadata_file: Optional[Path] = None
    css_file: Optional[Path] = None
    bib_file: Optional[Path] = None
    lua_filter: Optional[Path] = None

class PandocConverter:
    """Convert LaTeX papers to Markdown using enhanced Pandoc settings."""
    
    def __init__(self, config: PandocConfig):
        """Initialize converter with configuration."""
        self.config = config
        self._ensure_directories()
        self._create_default_files()
    
    def _ensure_directories(self):
        """Ensure all required directories exist."""
        # Create main media directory
        try:
            self.config.extract_media_dir.mkdir(parents=True, exist_ok=True)
            logger.debug(f"Created media directory: {self.config.extract_media_dir}")
            
            # Create parent directories for all configured paths
            paths_to_check = [
                self.config.metadata_file,
                self.config.css_file,
                self.config.bib_file,
                self.config.lua_filter
            ]
            
            for path in paths_to_check:
                if path is not None:
                    path.parent.mkdir(parents=True, exist_ok=True)
                    logger.debug(f"Created directory for: {path}")
                    
        except Exception as e:
            logger.error(f"Error creating directories: {e}")
            raise
    
    def _write_file(self, path: Path, content: str) -> bool:
        """Write content to file and verify it exists."""
        try:
            path.write_text(content)
            # Verify file was written
            if not path.exists():
                logger.error(f"Failed to create file: {path}")
                return False
            logger.debug(f"Successfully wrote file: {path}")
            return True
        except Exception as e:
            logger.error(f"Error writing file {path}: {e}")
            return False
    
    def _create_default_files(self):
        """Create default supporting files if not provided."""
        # Create and assign paths relative to media directory if not provided
        if not self.config.lua_filter:
            self.config.lua_filter = self.config.extract_media_dir / 'crossref.lua'
        
        if not self.config.metadata_file:
            self.config.metadata_file = self.config.extract_media_dir / 'metadata.yaml'
        
        # Ensure parent directories exist again (in case paths were just assigned)
        self._ensure_directories()
        
        # Create Lua filter
        lua_content = '''
function Math(elem)
    -- Preserve math content
    return elem
end

function Link(elem)
    -- Handle cross-references
    return elem
end

function Image(elem)
    -- Handle figure references
    return elem
end

function Table(elem)
    -- Handle table formatting
    return elem
end
'''
        if not self._write_file(self.config.lua_filter, lua_content):
            raise RuntimeError(f"Failed to create Lua filter: {self.config.lua_filter}")
        
        # Create metadata file
        metadata_content = '''---
reference-section-title: "References"
link-citations: true
citation-style: ieee
header-includes:
  - \\usepackage{amsmath}
  - \\usepackage{amsthm}
---'''
        if not self._write_file(self.config.metadata_file, metadata_content):
            raise RuntimeError(f"Failed to create metadata file: {self.config.metadata_file}")
        
        logger.debug("Successfully created all supporting files")
    
    def _verify_files_exist(self) -> bool:
        """Verify that all required files exist before running pandoc."""
        files_to_check = []
        
        if self.config.metadata_file:
            files_to_check.append(self.config.metadata_file)
        if self.config.lua_filter:
            files_to_check.append(self.config.lua_filter)
        if self.config.css_file:
            files_to_check.append(self.config.css_file)
        if self.config.bib_file:
            files_to_check.append(self.config.bib_file)
            
        for file_path in files_to_check:
            if not file_path.exists():
                logger.error(f"Required file does not exist: {file_path}")
                return False
            logger.debug(f"Verified file exists: {file_path}")
        
        return True
        
    def build_pandoc_command(self, input_file: Path, output_file: Path) -> list[str]:
        """Build Pandoc command with all necessary arguments."""
        cmd = [
            'pandoc',
            # Input/output formats
            '-f', 'latex+raw_tex',
            '-t', 'gfm',
            
            # Math handling
            '--mathjax',
            
            # Table and formatting
            '--columns=1000',
            '--wrap=none',
            
            # Figure handling
            f'--extract-media={self.config.extract_media_dir.resolve()}',
            '--standalone',
            
            # Debug info
            '--verbose',
        ]
        
        # Add optional components with absolute paths
        if self.config.metadata_file and self.config.metadata_file.exists():
            cmd.extend(['--metadata-file', str(self.config.metadata_file.resolve())])
            logger.debug(f"Adding metadata file: {self.config.metadata_file.resolve()}")
        
        if self.config.css_file and self.config.css_file.exists():
            cmd.extend(['--css', str(self.config.css_file.resolve())])
            logger.debug(f"Adding CSS file: {self.config.css_file.resolve()}")
            
        if self.config.bib_file and self.config.bib_file.exists():
            cmd.extend([
                '--citeproc',
                '--bibliography', str(self.config.bib_file.resolve())
            ])
            logger.debug(f"Adding bibliography file: {self.config.bib_file.resolve()}")
            
        if self.config.lua_filter and self.config.lua_filter.exists():
            cmd.extend(['--lua-filter', str(self.config.lua_filter.resolve())])
            logger.debug(f"Adding Lua filter: {self.config.lua_filter.resolve()}")
            
        # Add input/output files with absolute paths
        cmd.extend([
            str(input_file.resolve()),
            '-o', str(output_file.resolve())
        ])
        
        return cmd
        
    def convert_tex_to_markdown(self, tex_file: Path, output_file: Optional[Path] = None) -> bool:
        """
        Convert a LaTeX file to Markdown using Pandoc.
        
        Args:
            tex_file: Path to LaTeX file
            output_file: Optional output path, defaults to same name with .md extension
            
        Returns:
            bool: True if conversion successful
        """
        try:
            if not tex_file.exists():
                raise FileNotFoundError(f"LaTeX file not found: {tex_file}")
                
            if not output_file:
                output_file = tex_file.with_suffix('.md')
                    
            # Verify all required files exist
            if not self._verify_files_exist():
                raise FileNotFoundError("Missing required pandoc configuration files")
                    
            # Create temporary directory for conversion
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_dir = Path(temp_dir)
                
                # Copy LaTeX file to temp directory
                temp_tex = temp_dir / tex_file.name
                shutil.copy2(tex_file, temp_tex)
                if not temp_tex.exists():
                    raise RuntimeError(f"Failed to copy LaTeX file to temp directory: {temp_tex}")
                
                # Build and run Pandoc command
                cmd = self.build_pandoc_command(temp_tex, output_file)
                logger.debug(f"Running Pandoc command: {' '.join(cmd)}")
                
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    cwd=str(temp_dir)
                )
                
                if result.returncode != 0:
                    error_msg = result.stderr.strip() or "Unknown pandoc error"
                    raise RuntimeError(f"Pandoc conversion failed: {error_msg}")
                
                # Verify output file was created and not empty
                if not output_file.exists():
                    raise RuntimeError(f"Output file not created: {output_file}")
                if output_file.stat().st_size == 0:
                    raise RuntimeError(f"Output file is empty: {output_file}")
                    
                logger.success(f"Successfully converted {tex_file} to {output_file}")
                return True
                
        except Exception as e:
            logger.error(f"Error converting {tex_file} to Markdown: {e}")
            raise

def create_default_config(paper_dir: Path) -> PandocConfig:
    """Create default Pandoc configuration for a paper directory."""
    media_dir = paper_dir / "media"
    return PandocConfig(extract_media_dir=media_dir)



---
File: src/scripts/paper_manager.py
---
# src/scripts/paper_manager.py
"""Paper metadata management with automatic hydration of missing fields."""

import json
from pathlib import Path
from loguru import logger
from datetime import datetime, timezone
from typing import Optional

from .models import Paper, ReadingSession, PaperVisitEvent
from .arxiv_client import ArxivClient

class PaperManager:
    """Manages paper metadata and event storage."""
    _event_log_fname = "interactions.log"

    def __init__(self, data_dir: Path, arxiv_client: Optional[ArxivClient] = None):
        """Initialize PaperManager with data directory and optional ArxivClient."""
        self.data_dir = data_dir
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.arxiv_client = arxiv_client or ArxivClient(data_dir)
        self.modified_files: set[str] = set()

    def _needs_hydration(self, paper: Paper) -> bool:
        """Check if paper needs metadata hydration."""
        return (
            paper.published_date is None or 
            paper.arxiv_tags is None or
            not paper.arxiv_tags  # Also hydrate if tags list is empty
        )
    
    def _hydrate_metadata(self, paper: Paper) -> Paper:
        """Fetch missing metadata from arXiv API."""
        try:
            # Get fresh metadata from arXiv
            updated_paper = self.arxiv_client.fetch_metadata(paper.arxiv_id)
            
            # Keep track of fields we want to preserve from the existing paper
            preserve_fields = [
                "issue_number", "issue_url", "state", "labels",
                "total_reading_time_seconds", "last_read", "last_visited",
                "main_tex_file"
            ]
            
            # Update our paper with new metadata while preserving existing fields
            updated_dict = updated_paper.model_dump()
            paper_dict = paper.model_dump()
            
            for field in preserve_fields:
                if paper_dict.get(field) is not None:
                    updated_dict[field] = paper_dict[field]
            
            # Create new paper instance with combined data
            hydrated_paper = Paper.model_validate(updated_dict)
            logger.info(f"Hydrated metadata for {paper.arxiv_id}")
            return hydrated_paper
            
        except Exception as e:
            logger.error(f"Failed to hydrate metadata for {paper.arxiv_id}: {e}")
            return paper  # Return original paper if hydration fails
    
    def get_paper(self, arxiv_id: str) -> Paper:
        """Get paper metadata, hydrating if necessary."""
        paper = self.load_metadata(arxiv_id)
        
        if self._needs_hydration(paper):
            logger.info(f"Missing metadata fields for {arxiv_id}, hydrating...")
            paper = self._hydrate_metadata(paper)
            self.save_metadata(paper)
        
        return paper

    def fetch_new_paper(self, arxiv_id: str) -> Paper:
        """Fetch paper metadata from ArXiv."""
        paper = self.arxiv_client.fetch_metadata(arxiv_id)
        self.create_paper(paper)
        return paper

    def get_or_create_paper(self, arxiv_id: str) -> Paper:
        """Get existing paper or create new one."""
        try:
            return self.get_paper(arxiv_id)
        except FileNotFoundError:
            return self.fetch_new_paper(arxiv_id)

    def create_paper(self, paper: Paper) -> None:
        """Create new paper directory and initialize metadata."""
        paper_dir = self.data_dir / paper.arxiv_id
        if paper_dir.exists():
            raise ValueError(f"Paper directory already exists: {paper.arxiv_id}")

        try:
            # Create directory and save metadata
            paper_dir.mkdir(parents=True)
            
            # Check if we need to hydrate metadata before saving
            if self._needs_hydration(paper):
                paper = self._hydrate_metadata(paper)
            
            self.save_metadata(paper)

            # Record visit event with paper's timestamp
            event = PaperVisitEvent(
                timestamp=paper.created_at,  # Use paper's creation timestamp
                issue_url=paper.issue_url,
                arxiv_id=paper.arxiv_id
            )
            self.append_event(paper.arxiv_id, event)

        except Exception as e:
            logger.error(f"Failed to create paper {paper.arxiv_id}: {e}")
            if paper_dir.exists():
                paper_dir.rmdir()  # Cleanup on failure
            raise

    def save_metadata(self, paper: Paper) -> None:
        """Save paper metadata to file."""
        if self._needs_hydration(paper):
            logger.warning(f"Saving paper {paper.arxiv_id} with missing metadata fields")
            
        paper_dir = self.data_dir / paper.arxiv_id
        metadata_file = paper_dir / "metadata.json"
        paper_dir.mkdir(parents=True, exist_ok=True)
        
        # Convert to dict and store
        data = paper.model_dump(by_alias=True)
        # Ensure relative paths for main_tex_file
        if data.get('main_tex_file'):
            try:
                # Convert to relative path from paper directory
                full_path = Path(data['main_tex_file'])
                rel_path = full_path.relative_to(paper_dir)
                data['main_tex_file'] = str(rel_path)
            except ValueError:
                # If path is already relative or invalid, store as-is
                pass
                
        with metadata_file.open('w') as f:
            json.dump(data, f, indent=2)
        self.modified_files.add(str(metadata_file))

    def load_metadata(self, arxiv_id: str) -> Paper:
        """Load paper metadata from file."""
        paper_dir = self.data_dir / arxiv_id
        metadata_file = paper_dir / "metadata.json"
        if not metadata_file.exists():
            raise FileNotFoundError(f"No metadata found for paper {arxiv_id}")
        
        with metadata_file.open('r') as f:
            data = json.load(f)
            # Convert relative main_tex_file path to absolute if it exists
            if data.get('main_tex_file'):
                data['main_tex_file'] = str(paper_dir / data['main_tex_file'])
            return Paper.model_validate(data)

    def append_event(self, arxiv_id: str, event: PaperVisitEvent | ReadingSession) -> None:
        """Append event to paper's event log."""
        paper_dir = self.data_dir / arxiv_id
        paper_dir.mkdir(parents=True, exist_ok=True)
        
        # Create and write to events file
        events_file = paper_dir / self._event_log_fname
        with events_file.open('a+', encoding='utf-8') as f:
            f.write(f"{event.model_dump_json()}\n")
        self.modified_files.add(str(events_file))

    def update_reading_time(self, arxiv_id: str, duration_seconds: int) -> None:
        """Update paper's total reading time."""
        paper = self.get_or_create_paper(arxiv_id)
        paper.total_reading_time_seconds += duration_seconds
        paper.last_read = datetime.utcnow().isoformat()
        self.save_metadata(paper)

    def get_modified_files(self) -> set[str]:
        """Get set of modified file paths."""
        return self.modified_files.copy()

    def clear_modified_files(self) -> None:
        """Clear set of modified files."""
        self.modified_files.clear()
        
    def update_main_tex_file(self, arxiv_id: str, tex_file: Path) -> None:
        """Update paper's main TeX file path."""
        paper = self.get_paper(arxiv_id)
        paper.main_tex_file = str(tex_file)
        self.save_metadata(paper)
        
        # Check if markdown exists
        paper_dir = self.data_dir / arxiv_id
        markdown_file = paper_dir / f"{arxiv_id}.md"
        
        if not markdown_file.exists() or markdown_file.stat().st_size == 0:
            # Attempt conversion with specified tex file
            from .markdown_service import MarkdownService
            service = MarkdownService(self.data_dir)
            service.convert_paper(arxiv_id, force=True, tex_file=tex_file)



---
File: src/scripts/process_events.py
---
# src/scripts/process_events.py
"""
Event Processing System
======================

This module handles the processing of paper-related events through GitHub issues.

Flow:
1. GitHub issues are fetched
2. Issues are categorized (paper/reading)
3. Events are processed and stored
4. Registry is updated
5. Issues are closed

For new events, see models.py for available event types.
"""

import os
import json
import yaml
from pathlib import Path
from datetime import datetime, timedelta, timezone
from loguru import logger
from typing import Optional, List, Dict, Any

from .models import Paper, ReadingSession, PaperVisitEvent
from .paper_manager import PaperManager
from .github_client import GithubClient
from llamero.utils import commit_and_push

class EventProcessor:
    """Processes GitHub issues into paper events."""

    def __init__(self, papers_dir: str|Path = "data/papers"):
        """Initialize EventProcessor with GitHub credentials and paths."""
        self.github = GithubClient(
            token=os.environ["GITHUB_TOKEN"],
            repo=os.environ["GITHUB_REPOSITORY"]
        )
        self.papers_dir = Path(papers_dir)
        self.papers_dir.mkdir(parents=True, exist_ok=True)
        self.paper_manager = PaperManager(self.papers_dir)
        self.processed_issues: list[int] = []

    def process_paper_issue(self, issue_data: Dict[str, Any]) -> bool:
        """Process paper registration issue."""
        try:
            paper_data = json.loads(issue_data["body"])
            arxiv_id = paper_data.get("arxivId")
            if not arxiv_id:
                raise ValueError("No arXiv ID found in metadata")

            # Create visit event using original timestamp
            timestamp = paper_data.get("timestamp", datetime.now(timezone.utc).isoformat())
            event = PaperVisitEvent(
                arxiv_id=arxiv_id,
                timestamp=timestamp,
                issue_url=issue_data["html_url"]
            )
            
            # Update paper metadata
            paper = self.paper_manager.get_or_create_paper(arxiv_id)
            paper.issue_number = issue_data["number"]
            paper.issue_url = issue_data["html_url"]
            paper.labels = [label["name"] for label in issue_data["labels"]]
            paper.last_visited = timestamp
            
            # Save both metadata and event
            self.paper_manager.save_metadata(paper)
            self.paper_manager.append_event(arxiv_id, event)
            self.processed_issues.append(issue_data["number"])
            return True

        except Exception as e:
            logger.error(f"Error processing paper issue: {e}")
            return False

    def process_reading_issue(self, issue_data: Dict[str, Any]) -> bool:
        """Process reading session issue."""
        try:
            session_data = json.loads(issue_data["body"])
            arxiv_id = session_data.get("arxivId")
            duration_seconds = session_data.get("duration_seconds")
            timestamp = session_data.get("timestamp")
            
            if not all([arxiv_id, duration_seconds, timestamp]):
                raise ValueError("Missing required fields in session data")

            event = ReadingSession(
                arxivId=arxiv_id,
                timestamp=timestamp,  # Use original timestamp from the event
                duration_seconds=duration_seconds,
                issue_url=issue_data["html_url"]
            )
            
            # Calculate visit end time by adding duration to timestamp
            visit_time = datetime.fromisoformat(timestamp)
            visit_end = visit_time + timedelta(seconds=duration_seconds)
            
            paper = self.paper_manager.get_or_create_paper(arxiv_id)
            paper.last_visited = visit_end.isoformat()
            self.paper_manager.save_metadata(paper)
            
            self.paper_manager.update_reading_time(arxiv_id, duration_seconds)
            self.paper_manager.append_event(arxiv_id, event)
            self.processed_issues.append(issue_data["number"])
            return True

        except Exception as e:
            logger.error(f"Error processing reading session: {e}")
            return False

    def update_registry(self) -> None:
        """Update central registry with modified papers."""
        registry_file = self.papers_dir / "papers.yaml"
        registry = {}
        
        if registry_file.exists():
            with registry_file.open('r') as f:
                registry = yaml.safe_load(f) or {}
        
        modified_papers = {
            path.parent.name 
            for path in map(Path, self.paper_manager.get_modified_files())
            if "metadata.json" in str(path)
        }
        
        for arxiv_id in modified_papers:
            try:
                paper = self.paper_manager.load_metadata(arxiv_id)
                registry[arxiv_id] = paper.model_dump(by_alias=True)
            except Exception as e:
                logger.error(f"Error adding {arxiv_id} to registry: {e}")
        
        if modified_papers:
            with registry_file.open('w') as f:
                yaml.safe_dump(registry, f, sort_keys=True, indent=2, allow_unicode=True)
            self.paper_manager.modified_files.add(str(registry_file))

    def process_all_issues(self) -> None:
        """Process all open issues."""
        # Get and process issues
        issues = self.github.get_open_issues()
        for issue in issues:
            labels = [label["name"] for label in issue["labels"]]
            if "reading-session" in labels:
                self.process_reading_issue(issue)
            elif "paper" in labels:
                self.process_paper_issue(issue)

        # Update registry and close issues
        if self.paper_manager.get_modified_files():
            self.update_registry()
            try:
                commit_and_push(list(self.paper_manager.get_modified_files()))
                for issue_number in self.processed_issues:
                    self.github.close_issue(issue_number)
                logger.info("Git operations successful and processed issues closed.")
                # logger.info("Setting EVENTS_PROCESSED variable to trigger deploy-and-publish workflow.")
                # os.environ["EVENTS_PROCESSED"]="true"
                # # with open(os.environ['GITHUB_ENV'], 'a') as f:
                # #     f.write('EVENTS_PROCESSED=true\n')
                print("Events processed.")
            except Exception as e:
                logger.error(f"Failed to commit changes: {e}")
            finally:
                self.paper_manager.clear_modified_files()

def main():
    """Main entry point for processing paper events."""
    processor = EventProcessor()
    processor.process_all_issues()

if __name__ == "__main__":
    main()



---
File: src/scripts/tex_utils.py
---
# scripts/tex_utils.py
"""TeX file utilities for arXiv paper processing."""

import re
from pathlib import Path
from loguru import logger
from dataclasses import dataclass
from typing import Sequence

@dataclass
class TeXFileScore:
    """Score details for a TeX file candidate."""
    path: Path
    score: int
    reasons: list[str]



# Common ML conference and file patterns
ML_MAIN_FILE_NAMES = {
    # Standard names
    'main.tex', 'paper.tex', 'article.tex', 'manuscript.tex',
    'submission.tex', 'arxiv.tex', 'draft.tex', 'final.tex',
    # ML/AI Conference specific
    'neurips.tex', 'neurips_main.tex', 'neurips_camera_ready.tex',
    'iclr.tex', 'iclr_main.tex', 'iclr_conference.tex', 
    'icml.tex', 'icml_final.tex', 'icml_conference.tex',
    'aaai.tex', 'aaai_submission.tex', 'aaai_camera_ready.tex',
    'acl.tex', 'acl_main.tex', 'acl_camera_ready.tex',
    'emnlp.tex', 'emnlp_main.tex', 'emnlp_final.tex',
    'cvpr.tex', 'cvpr_main.tex', 'cvpr_final.tex',
    'iccv.tex', 'iccv_main.tex', 'iccv_camera_ready.tex',
    'eccv.tex', 'eccv_main.tex', 'eccv_submission.tex',
    'mlsys.tex', 'ml4ps.tex', 'ml4md.tex', 'aistats.tex',
}

ML_CONFERENCE_PATTERNS = [
    # Major ML conferences
    r'neurips.*(?:conference|final|main)',
    r'iclr.*(?:conference|final|main)',
    r'icml.*(?:conference|final|main)',
    # NLP conferences
    r'acl.*(?:conference|final|main)',
    r'emnlp.*(?:conference|final|main)',
    r'naacl.*(?:conference|final|main)',
    # Vision conferences
    r'cvpr.*(?:conference|final|main)',
    r'iccv.*(?:conference|final|main)',
    r'eccv.*(?:conference|final|main)',
    # AI conferences
    r'aaai.*(?:conference|final|main)',
    r'ijcai.*(?:conference|final|main)',
    # Systems and specialized
    r'mlsys.*(?:conference|final|main)',
    r'kdd.*(?:conference|final|main)',
    r'aistats.*(?:conference|final|main)',
]

def score_tex_file(tex_file: Path) -> TeXFileScore:
    """Score a single TeX file based on ML-focused heuristics."""
    score = 0
    reasons: list[str] = []
    
    try:
        content = tex_file.read_text(encoding='utf-8', errors='ignore')
        
        # File name scoring
        if tex_file.name.lower() in ML_MAIN_FILE_NAMES:
            score += 3
            reasons.append(f"Main filename match (+3): {tex_file.name}")
        
        # Conference pattern scoring
        for pattern in ML_CONFERENCE_PATTERNS:
            if re.search(pattern, tex_file.name.lower()):
                score += 3
                reasons.append(f"Conference pattern match (+3): {pattern}")
        
        # Document structure
        if r'\documentclass' in content:
            score += 5
            reasons.append("Has documentclass (+5)")
        if r'\begin{document}' in content:
            score += 4
            reasons.append("Has begin{document} (+4)")
        if r'\end{document}' in content:
            score += 4
            reasons.append("Has end{document} (+4)")
        
        # Negative indicators
        input_count = len(re.findall(r'\\input{', content))
        include_count = len(re.findall(r'\\include{', content))
        if input_count + include_count > 0:
            penalty = -2 if input_count + include_count > 2 else -1
            score += penalty
            reasons.append(f"Input/include commands ({penalty})")
        
        # File size scoring
        file_size = len(content)
        size_score = 0
        if file_size > 50000:
            size_score = 4
        elif file_size > 20000:
            size_score = 3
        elif file_size > 10000:
            size_score = 2
        elif file_size > 5000:
            size_score = 1
        if size_score > 0:
            score += size_score
            reasons.append(f"File size {file_size/1000:.1f}KB (+{size_score})")
            
    except Exception as e:
        logger.debug(f"Error processing {tex_file}: {e}")
        return TeXFileScore(tex_file, 0, [f"Error: {str(e)}"])
    
    return TeXFileScore(tex_file, score, reasons)

def find_main_tex_file(tex_files: Sequence[Path], arxiv_id: str = "unknown") -> Path | None:
    """
    Find the most likely main TeX file from a list of candidates.
    
    Args:
        tex_files: List of TeX file paths to evaluate
        arxiv_id: ArXiv ID for logging context
    
    Returns:
        Path to the most likely main TeX file, or None if no valid candidates
    """
    if not tex_files:
        return None
    
    # Score all files
    scored_files = [score_tex_file(f) for f in tex_files]
    
    # Log detailed scoring
    logger.debug(f"\nTeX file scoring for {arxiv_id}:")
    for result in scored_files:
        logger.debug(f"\n{result.path.name}: Total Score = {result.score}")
        for reason in result.reasons:
            logger.debug(f"  {reason}")
    
    # Filter and sort files
    valid_files = [f for f in scored_files if f.score >= 10]  # Minimum score threshold
    if not valid_files:
        return None
        
    valid_files.sort(key=lambda x: x.score, reverse=True)
    return valid_files[0].path


