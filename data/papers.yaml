'1711.11586':
  abstract: 'Many image-to-image translation problems are ambiguous, as a single input

    image may correspond to multiple possible outputs. In this work, we aim to

    model a \emph{distribution} of possible outputs in a conditional generative

    modeling setting. The ambiguity of the mapping is distilled in a

    low-dimensional latent vector, which can be randomly sampled at test time. A

    generator learns to map the given input, combined with this latent code, to the

    output. We explicitly encourage the connection between output and the latent

    code to be invertible. This helps prevent a many-to-one mapping from the latent

    code to the output during training, also known as the problem of mode collapse,

    and produces more diverse results. We explore several variants of this approach

    by employing different training objectives, network architectures, and methods

    of injecting the latent code. Our proposed method encourages bijective

    consistency between the latent encoding and output modes. We present a

    systematic comparison of our method and other variants on both perceptual

    realism and diversity.'
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-15T09:47:47Z'
  issue_number: 23
  issue_url: https://github.com/dmarx/arxiv-archive/issues/23
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T09:47:47.572Z'
  title: Toward Multimodal Image-to-Image Translation
  url: https://arxiv.org/pdf/1711.11586
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    forgenerating high-dimensional data. In this paper, we show that the dynamics
    ofthese models exhibit a spontaneous symmetry breaking that divides thegenerative
    dynamics into two distinct phases: 1) A linear steady-state dynamicsaround a central
    fixed-point and 2) an attractor dynamics directed towards thedata manifold. These
    two "phases" are separated by the change in stability ofthe central fixed-point,
    with the resulting window of instability beingresponsible for the diversity of
    the generated samples. Using both theoreticaland empirical evidence, we show that
    an accurate simulation of the earlydynamics does not significantly contribute
    to the final generation, since earlyfluctuations are reverted to the central fixed
    point. To leverage this insight,we propose a Gaussian late initialization scheme,
    which significantly improvesmodel performance, achieving up to 3x FID improvements
    on fast samplers, whilealso increasing sample diversity (e.g., racial composition
    of generated CelebAimages). Our work offers a new way to understand the generative
    dynamics ofdiffusion models that has the potential to bring about higher performance
    andless biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-15T10:27:24Z'
  issue_number: 27
  issue_url: https://github.com/dmarx/arxiv-archive/issues/27
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T10:27:24.289Z'
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  url: https://arxiv.org/pdf/2305.19693
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering

    times by orders of magnitude. While these methods demonstrate state-of-the-art

    quality and speed, they are designed for photogrammetry of static scenes and do

    not generalize well to freely moving humans in the environment. In this work,

    we introduce Human Gaussian Splats (HUGS) that represents an animatable human

    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes

    only a monocular video with a small number of (50-100) frames, and it

    automatically learns to disentangle the static scene and a fully animatable

    human avatar within 30 minutes. We utilize the SMPL body model to initialize

    the human Gaussians. To capture details that are not modeled by SMPL (e.g.

    cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.

    Utilizing 3D Gaussians for animated humans brings new challenges, including the

    artifacts created when articulating the Gaussians. We propose to jointly

    optimize the linear blend skinning weights to coordinate the movements of

    individual Gaussians during animation. Our approach enables novel-pose

    synthesis of human and novel view synthesis of both the human and the scene. We

    achieve state-of-the-art rendering quality with a rendering speed of 60 FPS

    while being ~100x faster to train over previous work. Our code will be

    announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-15T08:43:32Z'
  issue_number: 21
  issue_url: https://github.com/dmarx/arxiv-archive/issues/21
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T08:43:32.125Z'
  title: 'HUGS: Human Gaussian Splats'
  url: https://arxiv.org/abs/2311.17910
