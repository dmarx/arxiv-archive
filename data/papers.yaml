'1711.11586':
  abstract: Many image-to-image translation problems are ambiguous, as a single inputimage
    may correspond to multiple possible outputs. In this work, we aim tomodel a \emph{distribution}
    of possible outputs in a conditional generativemodeling setting. The ambiguity
    of the mapping is distilled in alow-dimensional latent vector, which can be randomly
    sampled at test time. Agenerator learns to map the given input, combined with
    this latent code, to theoutput. We explicitly encourage the connection between
    output and the latentcode to be invertible. This helps prevent a many-to-one mapping
    from the latentcode to the output during training, also known as the problem of
    mode collapse,and produces more diverse results. We explore several variants of
    this approachby employing different training objectives, network architectures,
    and methodsof injecting the latent code. Our proposed method encourages bijectiveconsistency
    between the latent encoding and output modes. We present asystematic comparison
    of our method and other variants on both perceptualrealism and diversity.
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-15T17:02:34Z'
  issue_number: 28
  issue_url: https://github.com/dmarx/arxiv-archive/issues/28
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T17:02:34.023Z'
  title: Toward Multimodal Image-to-Image Translation
  url: https://arxiv.org/pdf/1711.11586
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    forgenerating high-dimensional data. In this paper, we show that the dynamics
    ofthese models exhibit a spontaneous symmetry breaking that divides thegenerative
    dynamics into two distinct phases: 1) A linear steady-state dynamicsaround a central
    fixed-point and 2) an attractor dynamics directed towards thedata manifold. These
    two "phases" are separated by the change in stability ofthe central fixed-point,
    with the resulting window of instability beingresponsible for the diversity of
    the generated samples. Using both theoreticaland empirical evidence, we show that
    an accurate simulation of the earlydynamics does not significantly contribute
    to the final generation, since earlyfluctuations are reverted to the central fixed
    point. To leverage this insight,we propose a Gaussian late initialization scheme,
    which significantly improvesmodel performance, achieving up to 3x FID improvements
    on fast samplers, whilealso increasing sample diversity (e.g., racial composition
    of generated CelebAimages). Our work offers a new way to understand the generative
    dynamics ofdiffusion models that has the potential to bring about higher performance
    andless biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-15T10:27:24Z'
  issue_number: 27
  issue_url: https://github.com/dmarx/arxiv-archive/issues/27
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T10:27:24.289Z'
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  url: https://arxiv.org/pdf/2305.19693
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and renderingtimes
    by orders of magnitude. While these methods demonstrate state-of-the-artquality
    and speed, they are designed for photogrammetry of static scenes and donot generalize
    well to freely moving humans in the environment. In this work,we introduce Human
    Gaussian Splats (HUGS) that represents an animatable humantogether with the scene
    using 3D Gaussian Splatting (3DGS). Our method takesonly a monocular video with
    a small number of (50-100) frames, and itautomatically learns to disentangle the
    static scene and a fully animatablehuman avatar within 30 minutes. We utilize
    the SMPL body model to initializethe human Gaussians. To capture details that
    are not modeled by SMPL (e.g.cloth, hairs), we allow the 3D Gaussians to deviate
    from the human body model.Utilizing 3D Gaussians for animated humans brings new
    challenges, including theartifacts created when articulating the Gaussians. We
    propose to jointlyoptimize the linear blend skinning weights to coordinate the
    movements ofindividual Gaussians during animation. Our approach enables novel-posesynthesis
    of human and novel view synthesis of both the human and the scene. Weachieve state-of-the-art
    rendering quality with a rendering speed of 60 FPSwhile being ~100x faster to
    train over previous work. Our code will beannounced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-15T17:03:00Z'
  issue_number: 29
  issue_url: https://github.com/dmarx/arxiv-archive/issues/29
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T17:02:59.876Z'
  title: 'HUGS: Human Gaussian Splats'
  url: https://arxiv.org/abs/2311.17910
'2405.20053':
  abstract: Pre-trained Language Models (LMs) exhibit strong zero-shot and in-contextlearning
    capabilities; however, their behaviors are often difficult to control.By utilizing
    Reinforcement Learning from Human Feedback (RLHF), it is possibleto fine-tune
    unsupervised LMs to follow instructions and produce outputs thatreflect human
    preferences. Despite its benefits, RLHF has been shown topotentially harm a language
    model's reasoning capabilities and introduceartifacts such as hallucinations where
    the model may fabricate facts. Toaddress this issue we introduce Direct Preference
    Heads (DPH), a fine-tuningframework that enables LMs to learn human preference
    signals through anauxiliary reward head without directly affecting the output
    distribution of thelanguage modeling head. We perform a theoretical analysis of
    our objectivefunction and find strong ties to Conservative Direct Preference Optimization(cDPO).
    Finally we evaluate our models on GLUE, RACE, and the GPT4Allevaluation suite
    and demonstrate that our method produces models which achievehigher scores than
    those fine-tuned with Supervised Fine-Tuning (SFT) or DirectPreference Optimization
    (DPO) alone.
  arxivId: '2405.20053'
  authors: Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic
  created_at: '2024-12-15T17:25:18Z'
  issue_number: 31
  issue_url: https://github.com/dmarx/arxiv-archive/issues/31
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T17:25:17.781Z'
  title: "Would I Lie To You? Inference Time Alignment of Language Models using\n\
    \  Direct Preference Heads"
  url: https://arxiv.org/abs/2405.20053
'2410.01131':
  abstract: We propose a novel neural network architecture, the normalized Transformer(nGPT)
    with representation learning on the hypersphere. In nGPT, all vectorsforming the
    embeddings, MLP, attention matrices and hidden states are unit normnormalized.
    The input stream of tokens travels on the surface of a hypersphere,with each layer
    contributing a displacement towards the target outputpredictions. These displacements
    are defined by the MLP and attention blocks,whose vector components also reside
    on the same hypersphere. Experiments showthat nGPT learns much faster, reducing
    the number of training steps required toachieve the same accuracy by a factor
    of 4 to 20, depending on the sequencelength.
  arxivId: '2410.01131'
  authors: Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg
  created_at: '2024-12-15T17:43:04Z'
  issue_number: 33
  issue_url: https://github.com/dmarx/arxiv-archive/issues/33
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T17:43:04.035Z'
  title: "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere"
  url: https://arxiv.org/abs/2410.01131
