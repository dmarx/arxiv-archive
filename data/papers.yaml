'1503.02531':
  abstract: A very simple way to improve the performance of almost any machine learning
    algorithm is to train many different models on the same data and then to average
    their predictions. Unfortunately, making predictions using a whole ensemble of
    models is cumbersome and may be too computationally expensive to allow deployment
    to a large number of users, especially if the individual models are large neural
    nets. Caruana and his collaborators have shown that it is possible to compress
    the knowledge in an ensemble into a single model which is much easier to deploy
    and we develop this approach further using a different compression technique.
    We achieve some surprising results on MNIST and we show that we can significantly
    improve the acoustic model of a heavily used commercial system by distilling the
    knowledge in an ensemble of models into a single model. We also introduce a new
    type of ensemble composed of one or more full models and many specialist models
    which learn to distinguish fine-grained classes that the full models confuse.
    Unlike a mixture of experts, these specialist models can be trained rapidly and
    in parallel.
  arxivId: '1503.02531'
  authors: Geoffrey Hinton, Oriol Vinyals, Jeff Dean
  created_at: '2024-12-21T16:05:51Z'
  issue_number: 93
  issue_url: https://github.com/dmarx/arxiv-archive/issues/93
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Distilling the Knowledge in a Neural Network
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1503.02531
'1503.03585':
  abstract: A central problem in machine learning involves modeling complex data-sets
    using highly flexible families of probability distributions in which learning,
    sampling, inference, and evaluation are still analytically or computationally
    tractable. Here, we develop an approach that simultaneously achieves both flexibility
    and tractability. The essential idea, inspired by non-equilibrium statistical
    physics, is to systematically and slowly destroy structure in a data distribution
    through an iterative forward diffusion process. We then learn a reverse diffusion
    process that restores structure in data, yielding a highly flexible and tractable
    generative model of the data. This approach allows us to rapidly learn, sample
    from, and evaluate probabilities in deep generative models with thousands of layers
    or time steps, as well as to compute conditional and posterior probabilities under
    the learned model. We additionally release an open source reference implementation
    of the algorithm.
  arxivId: '1503.03585'
  authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
  created_at: '2024-12-22T07:09:21Z'
  issue_number: 118
  issue_url: https://github.com/dmarx/arxiv-archive/issues/118
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1503.03585
'1705.08926':
  abstract: Cooperative multi-agent systems can be naturally used to model many real
    world problems, such as network packet routing and the coordination of autonomous
    vehicles. There is a great need for new reinforcement learning methods that can
    efficiently learn decentralised policies for such systems. To this end, we propose
    a new multi-agent actor-critic method called counterfactual multi-agent (COMA)
    policy gradients. COMA uses a centralised critic to estimate the Q-function and
    decentralised actors to optimise the agents' policies. In addition, to address
    the challenges of multi-agent credit assignment, it uses a counterfactual baseline
    that marginalises out a single agent's action, while keeping the other agents'
    actions fixed. COMA also uses a critic representation that allows the counterfactual
    baseline to be computed efficiently in a single forward pass. We evaluate COMA
    in the testbed of StarCraft unit micromanagement, using a decentralised variant
    with significant partial observability. COMA significantly improves average performance
    over other multi-agent actor-critic methods in this setting, and the best performing
    agents are competitive with state-of-the-art centralised controllers that get
    access to the full state.
  arxivId: '1705.08926'
  authors: Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli,
    Shimon Whiteson
  created_at: '2024-12-22T07:25:41Z'
  issue_number: 127
  issue_url: https://github.com/dmarx/arxiv-archive/issues/127
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Counterfactual Multi-Agent Policy Gradients
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1705.08926
'1711.11586':
  abstract: Many image-to-image translation problems are ambiguous, as a single input
    image may correspond to multiple possible outputs. In this work, we aim to model
    a \emph{distribution} of possible outputs in a conditional generative modeling
    setting. The ambiguity of the mapping is distilled in a low-dimensional latent
    vector, which can be randomly sampled at test time. A generator learns to map
    the given input, combined with this latent code, to the output. We explicitly
    encourage the connection between output and the latent code to be invertible.
    This helps prevent a many-to-one mapping from the latent code to the output during
    training, also known as the problem of mode collapse, and produces more diverse
    results. We explore several variants of this approach by employing different training
    objectives, network architectures, and methods of injecting the latent code. Our
    proposed method encourages bijective consistency between the latent encoding and
    output modes. We present a systematic comparison of our method and other variants
    on both perceptual realism and diversity.
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-15T07:21:47Z'
  issue_number: 14
  issue_url: https://github.com/dmarx/arxiv-archive/issues/14
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1711.11586
'1803.00567':
  abstract: 'Optimal transport (OT) theory can be informally described using the words
    of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel
    in hand has to move a large pile of sand lying on a construction site. The goal
    of the worker is to erect with all that sand a target pile with a prescribed shape
    (for example, that of a giant sand castle). Naturally, the worker wishes to minimize
    her total effort, quantified for instance as the total distance or time spent
    carrying shovelfuls of sand. Mathematicians interested in OT cast that problem
    as that of comparing two probability distributions, two different piles of sand
    of the same volume. They consider all of the many possible ways to morph, transport
    or reshape the first pile into the second, and associate a "global" cost to every
    such transport, using the "local" consideration of how much it costs to move a
    grain of sand from one place to another. Recent years have witnessed the spread
    of OT in several fields, thanks to the emergence of approximate solvers that can
    scale to sizes and dimensions that are relevant to data sciences. Thanks to this
    newfound scalability, OT is being increasingly used to unlock various problems
    in imaging sciences (such as color or texture processing), computer vision and
    graphics (for shape manipulation) or machine learning (for regression, classification
    and density fitting). This short book reviews OT with a bias toward numerical
    methods and their applications in data sciences, and sheds lights on the theoretical
    properties of OT that make it particularly useful for some of these applications.'
  arxivId: '1803.00567'
  authors: Gabriel Peyr√©, Marco Cuturi
  created_at: '2024-12-22T08:08:47Z'
  issue_number: 138
  issue_url: https://github.com/dmarx/arxiv-archive/issues/138
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Computational Optimal Transport
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1803.00567
'1904.08779':
  abstract: We present SpecAugment, a simple data augmentation method for speech recognition.
    SpecAugment is applied directly to the feature inputs of a neural network (i.e.,
    filter bank coefficients). The augmentation policy consists of warping the features,
    masking blocks of frequency channels, and masking blocks of time steps. We apply
    SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition
    tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard
    300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER
    on test-other without the use of a language model, and 5.8% WER with shallow fusion
    with a language model. This compares to the previous state-of-the-art hybrid system
    of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome
    portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1%
    with shallow fusion, which compares to the previous state-of-the-art hybrid system
    at 8.3%/17.3% WER.
  arxivId: '1904.08779'
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2024-12-17T14:41:06Z'
  issue_number: 60
  issue_url: https://github.com/dmarx/arxiv-archive/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1904.08779
'1906.05433':
  abstract: Climate change is one of the greatest challenges facing humanity, and
    we, as machine learning experts, may wonder how we can help. Here we describe
    how machine learning can be a powerful tool in reducing greenhouse gas emissions
    and helping society adapt to a changing climate. From smart grids to disaster
    management, we identify high impact problems where existing gaps can be filled
    by machine learning, in collaboration with other fields. Our recommendations encompass
    exciting research questions as well as promising business opportunities. We call
    on the machine learning community to join the global effort against climate change.
  arxivId: '1906.05433'
  authors: David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre
    Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques,
    Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik
    Mukkavilli, Konrad P. Kording, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John
    C. Platt, Felix Creutzig, Jennifer Chayes, Yoshua Bengio
  created_at: '2024-12-22T06:23:13Z'
  issue_number: 111
  issue_url: https://github.com/dmarx/arxiv-archive/issues/111
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Tackling Climate Change with Machine Learning
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1906.05433
'2009.10195':
  abstract: Models that perform well on a training domain often fail to generalize
    to out-of-domain (OOD) examples. Data augmentation is a common method used to
    prevent overfitting and improve OOD generalization. However, in natural language,
    it is difficult to generate new examples that stay on the underlying data manifold.
    We introduce SSMBA, a data augmentation method for generating synthetic training
    examples by using a pair of corruption and reconstruction functions to move randomly
    on a data manifold. We investigate the use of SSMBA in the natural language domain,
    leveraging the manifold assumption to reconstruct corrupted text with masked language
    models. In experiments on robustness benchmarks across 3 tasks and 9 datasets,
    SSMBA consistently outperforms existing data augmentation methods and baseline
    models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD
    Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.
  arxivId: '2009.10195'
  authors: Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi
  created_at: '2024-12-21T18:19:38Z'
  issue_number: 97
  issue_url: https://github.com/dmarx/arxiv-archive/issues/97
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving\n\
    \  Out-of-Domain Robustness"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2009.10195
'2203.15556':
  abstract: 'We investigate the optimal model size and number of tokens for training
    a transformer language model under a given compute budget. We find that current
    large language models are significantly undertrained, a consequence of the recent
    focus on scaling language models whilst keeping the amount of training data constant.
    By training over 400 language models ranging from 70 million to over 16 billion
    parameters on 5 to 500 billion tokens, we find that for compute-optimal training,
    the model size and the number of training tokens should be scaled equally: for
    every doubling of model size the number of training tokens should also be doubled.
    We test this hypothesis by training a predicted compute-optimal model, Chinchilla,
    that uses the same compute budget as Gopher but with 70B parameters and 4$\times$
    more more data. Chinchilla uniformly and significantly outperforms Gopher (280B),
    GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range
    of downstream evaluation tasks. This also means that Chinchilla uses substantially
    less compute for fine-tuning and inference, greatly facilitating downstream usage.
    As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
    on the MMLU benchmark, greater than a 7% improvement over Gopher.'
  arxivId: '2203.15556'
  authors: Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den
    Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen,
    Jack W. Rae, Oriol Vinyals, Laurent Sifre
  created_at: '2024-12-18T22:03:26Z'
  issue_number: 67
  issue_url: https://github.com/dmarx/arxiv-archive/issues/67
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Training Compute-Optimal Large Language Models
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2203.15556
'2209.02740':
  abstract: Networks of weakly coupled oscillators had a profound impact on our understanding
    of complex systems. Studies on model reconstruction from data have shown prevalent
    contributions from hypernetworks with triplet and higher interactions among oscillators,
    in spite that such models were originally defined as oscillator networks with
    pairwise interactions. Here, we show that hypernetworks can spontaneously emerge
    even in the presence of pairwise albeit nonlinear coupling given certain triplet
    frequency resonance conditions. The results are demonstrated in experiments with
    electrochemical oscillators and in simulations with integrate-and-fire neurons.
    By developing a comprehensive theory, we uncover the mechanism for emergent hypernetworks
    by identifying appearing and forbidden frequency resonant conditions. Furthermore,
    it is shown that microscopic linear (difference) coupling among units results
    in coupled mean fields, which have sufficient nonlinearity to facilitate hypernetworks.
    Our findings shed light on the apparent abundance of hypernetworks and provide
    a constructive way to predict and engineer their emergence.
  arxivId: '2209.02740'
  authors: Eddie Nijholt, Jorge Luis Ocampo-Espindola, Deniz Eroglu, Istv√°n Z. Kiss,
    Tiago Pereira
  created_at: '2024-12-22T05:30:04Z'
  issue_number: 103
  issue_url: https://github.com/dmarx/arxiv-archive/issues/103
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Emergent hypernetworks in weakly coupled oscillators
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2209.02740
'2210.14891':
  abstract: We present a smoothly broken power law functional form (that we refer
    to as a Broken Neural Scaling Law (BNSL)) that accurately models &amp; extrapolates
    the scaling behaviors of deep neural networks (i.e. how the evaluation metric
    of interest varies as amount of compute used for training (or inference), number
    of model parameters, training dataset size, model input size, number of training
    steps, or upstream performance varies) for various architectures &amp; for each
    of various tasks within a large &amp; diverse set of upstream &amp; downstream
    tasks, in zero-shot, prompted, &amp; finetuned settings. This set includes large-scale
    vision, language, audio, video, diffusion, generative modeling, multimodal learning,
    contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution
    (OOD) generalization, continual learning, transfer learning, uncertainty estimation
    / calibration, OOD detection, adversarial robustness, distillation, sparsity,
    retrieval, quantization, pruning, fairness, molecules, computer programming/coding,
    math word problems, "emergent phase transitions", arithmetic, supervised learning,
    unsupervised/self-supervised learning, &amp; reinforcement learning (single agent
    &amp; multi-agent). When compared to other functional forms for neural scaling,
    this functional form yields extrapolations of scaling behavior that are considerably
    more accurate on this set. Moreover, this functional form accurately models &amp;
    extrapolates scaling behavior that other functional forms are incapable of expressing
    such as the nonmonotonic transitions present in the scaling behavior of phenomena
    such as double descent &amp; the delayed, sharp inflection points present in the
    scaling behavior of tasks such as arithmetic. Lastly, we use this functional form
    to glean insights about the limit of the predictability of scaling behavior. Code
    is available at https://github.com/ethancaballero/broken_neural_scaling_laws
  arxivId: '2210.14891'
  authors: Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger
  created_at: '2024-12-21T05:51:09Z'
  issue_number: 85
  issue_url: https://github.com/dmarx/arxiv-archive/issues/85
  labels:
  - paper
  - rating:downvote
  last_read: null
  state: open
  title: Broken Neural Scaling Laws
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2210.14891
'2212.07677':
  abstract: At present, the mechanisms of in-context learning in Transformers are
    not well understood and remain mostly an intuition. In this paper, we suggest
    that training Transformers on auto-regressive objectives is closely related to
    gradient-based meta-learning formulations. We start by providing a simple weight
    construction that shows the equivalence of data transformations induced by 1)
    a single linear self-attention layer and by 2) gradient-descent (GD) on a regression
    loss. Motivated by that construction, we show empirically that when training self-attention-only
    Transformers on simple regression tasks either the models learned by GD and Transformers
    show great similarity or, remarkably, the weights found by optimization match
    the construction. Thus we show how trained Transformers become mesa-optimizers
    i.e. learn models by gradient descent in their forward pass. This allows us, at
    least in the domain of regression problems, to mechanistically understand the
    inner workings of in-context learning in optimized Transformers. Building on this
    insight, we furthermore identify how Transformers surpass the performance of plain
    gradient descent by learning an iterative curvature correction and learn linear
    models on deep data representations to solve non-linear regression tasks. Finally,
    we discuss intriguing parallels to a mechanism identified to be crucial for in-context
    learning termed induction-head (Olsson et al., 2022) and show how it could be
    understood as a specific case of in-context learning by gradient descent learning
    within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
    .
  arxivId: '2212.07677'
  authors: Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo√£o Sacramento,
    Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov
  created_at: '2024-12-21T08:19:06Z'
  issue_number: 91
  issue_url: https://github.com/dmarx/arxiv-archive/issues/91
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Transformers learn in-context by gradient descent
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2212.07677
'2303.11435':
  abstract: Inversion by Direct Iteration (InDI) is a new formulation for supervised
    image restoration that avoids the so-called "regression to the mean" effect and
    produces more realistic and detailed images than existing regression-based methods.
    It does this by gradually improving image quality in small steps, similar to generative
    denoising diffusion models. Image restoration is an ill-posed problem where multiple
    high-quality images are plausible reconstructions of a given low-quality input.
    Therefore, the outcome of a single step regression model is typically an aggregate
    of all possible explanations, therefore lacking details and realism. The main
    advantage of InDI is that it does not try to predict the clean target image in
    a single step but instead gradually improves the image in small steps, resulting
    in better perceptual quality. While generative denoising diffusion models also
    work in small steps, our formulation is distinct in that it does not require knowledge
    of any analytic form of the degradation process. Instead, we directly learn an
    iterative restoration process from low-quality and high-quality paired examples.
    InDI can be applied to virtually any image degradation, given paired training
    data. In conditional denoising diffusion image restoration the denoising network
    generates the restored image by repeatedly denoising an initial image of pure
    noise, conditioned on the degraded input. Contrary to conditional denoising formulations,
    InDI directly proceeds by iteratively restoring the input low-quality image, producing
    high-quality results on a variety of image restoration tasks, including motion
    and out-of-focus deblurring, super-resolution, compression artifact removal, and
    denoising.
  arxivId: '2303.11435'
  authors: Mauricio Delbracio, Peyman Milanfar
  created_at: '2024-12-22T08:12:43Z'
  issue_number: 140
  issue_url: https://github.com/dmarx/arxiv-archive/issues/140
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for\n\
    \  Image Restoration"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2303.11435
'2304.15004':
  abstract: 'Recent work claims that large language models display emergent abilities,
    abilities not present in smaller-scale models that are present in larger-scale
    models. What makes emergent abilities intriguing is two-fold: their sharpness,
    transitioning seemingly instantaneously from not present to present, and their
    unpredictability, appearing at seemingly unforeseeable model scales. Here, we
    present an alternative explanation for emergent abilities: that for a particular
    task and model family, when analyzing fixed model outputs, emergent abilities
    appear due to the researcher''s choice of metric rather than due to fundamental
    changes in model behavior with scale. Specifically, nonlinear or discontinuous
    metrics produce apparent emergent abilities, whereas linear or continuous metrics
    produce smooth, continuous predictable changes in model performance. We present
    our alternative explanation in a simple mathematical model, then test it in three
    complementary ways: we (1) make, test and confirm three predictions on the effect
    of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent
    abilities; (2) make, test and confirm two predictions about metric choices in
    a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics
    to produce never-before-seen seemingly emergent abilities in multiple vision tasks
    across diverse deep networks. Via all three analyses, we provide evidence that
    alleged emergent abilities evaporate with different metrics or with better statistics,
    and may not be a fundamental property of scaling AI models.'
  arxivId: '2304.15004'
  authors: Rylan Schaeffer, Brando Miranda, Sanmi Koyejo
  created_at: '2024-12-21T06:06:11Z'
  issue_number: 89
  issue_url: https://github.com/dmarx/arxiv-archive/issues/89
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Are Emergent Abilities of Large Language Models a Mirage?
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2304.15004
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    for generating high-dimensional data. In this paper, we show that the dynamics
    of these models exhibit a spontaneous symmetry breaking that divides the generative
    dynamics into two distinct phases: 1) A linear steady-state dynamics around a
    central fixed-point and 2) an attractor dynamics directed towards the data manifold.
    These two "phases" are separated by the change in stability of the central fixed-point,
    with the resulting window of instability being responsible for the diversity of
    the generated samples. Using both theoretical and empirical evidence, we show
    that an accurate simulation of the early dynamics does not significantly contribute
    to the final generation, since early fluctuations are reverted to the central
    fixed point. To leverage this insight, we propose a Gaussian late initialization
    scheme, which significantly improves model performance, achieving up to 3x FID
    improvements on fast samplers, while also increasing sample diversity (e.g., racial
    composition of generated CelebA images). Our work offers a new way to understand
    the generative dynamics of diffusion models that has the potential to bring about
    higher performance and less biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-15T09:58:39Z'
  issue_number: 24
  issue_url: https://github.com/dmarx/arxiv-archive/issues/24
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2305.19693
'2309.03060':
  abstract: Many areas of machine learning and science involve large linear algebra
    problems, such as eigendecompositions, solving linear systems, computing matrix
    exponentials, and trace estimation. The matrices involved often have Kronecker,
    convolutional, block diagonal, sum, or product structure. In this paper, we propose
    a simple but general framework for large-scale linear algebra problems in machine
    learning, named CoLA (Compositional Linear Algebra). By combining a linear operator
    abstraction with compositional dispatch rules, CoLA automatically constructs memory
    and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient
    automatic differentiation, low precision computation, and GPU acceleration in
    both JAX and PyTorch, while also accommodating new objects, operations, and rules
    in downstream packages via multiple dispatch. CoLA can accelerate many algebraic
    operations, while making it easy to prototype matrix structures and algorithms,
    providing an appealing drop-in tool for virtually any computational effort that
    requires linear algebra. We showcase its efficacy across a broad range of applications,
    including partial differential equations, Gaussian processes, equivariant model
    construction, and unsupervised learning.
  arxivId: '2309.03060'
  authors: Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson
  created_at: '2024-12-22T07:34:16Z'
  issue_number: 132
  issue_url: https://github.com/dmarx/arxiv-archive/issues/132
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "CoLA: Exploiting Compositional Structure for Automatic and Efficient\n \
    \ Numerical Linear Algebra"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2309.03060
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering
    times by orders of magnitude. While these methods demonstrate state-of-the-art
    quality and speed, they are designed for photogrammetry of static scenes and do
    not generalize well to freely moving humans in the environment. In this work,
    we introduce Human Gaussian Splats (HUGS) that represents an animatable human
    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only
    a monocular video with a small number of (50-100) frames, and it automatically
    learns to disentangle the static scene and a fully animatable human avatar within
    30 minutes. We utilize the SMPL body model to initialize the human Gaussians.
    To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow
    the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians
    for animated humans brings new challenges, including the artifacts created when
    articulating the Gaussians. We propose to jointly optimize the linear blend skinning
    weights to coordinate the movements of individual Gaussians during animation.
    Our approach enables novel-pose synthesis of human and novel view synthesis of
    both the human and the scene. We achieve state-of-the-art rendering quality with
    a rendering speed of 60 FPS while being ~100x faster to train over previous work.
    Our code will be announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-15T08:20:13Z'
  issue_number: 18
  issue_url: https://github.com/dmarx/arxiv-archive/issues/18
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'HUGS: Human Gaussian Splats'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2311.17910
'2401.17671':
  abstract: Recent advancements in artificial intelligence have sparked interest in
    the parallels between large language models (LLMs) and human neural processing,
    particularly in language comprehension. While prior research has established similarities
    in the representation of LLMs and the brain, the underlying computational principles
    that cause this convergence, especially in the context of evolving LLMs, remain
    elusive. Here, we examined a diverse selection of high-performance LLMs with similar
    parameter sizes to investigate the factors contributing to their alignment with
    the brain's language processing mechanisms. We find that as LLMs achieve higher
    performance on benchmark tasks, they not only become more brain-like as measured
    by higher performance when predicting neural responses from LLM embeddings, but
    also their hierarchical feature extraction pathways map more closely onto the
    brain's while using fewer layers to do the same encoding. We also compare the
    feature extraction pathways of the LLMs to each other and identify new ways in
    which high-performing models have converged toward similar hierarchical processing
    mechanisms. Finally, we show the importance of contextual information in improving
    model performance and brain similarity. Our findings reveal the converging aspects
    of language processing in the brain and LLMs and offer new directions for developing
    models that align more closely with human cognitive processing.
  arxivId: '2401.17671'
  authors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima
    Mesgarani
  created_at: '2024-12-19T11:12:50Z'
  issue_number: 70
  issue_url: https://github.com/dmarx/arxiv-archive/issues/70
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Contextual Feature Extraction Hierarchies Converge in Large Language\n \
    \ Models and the Brain"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2401.17671
'2402.03239':
  abstract: 'Bluesky is a new social network built upon the AT Protocol, a decentralized
    foundation for public social media. It was launched in private beta in February
    2023, and has grown to over 10 million registered users by October 2024. In this
    paper we introduce the architecture of Bluesky and the AT Protocol, and explain
    how the technical design of Bluesky is informed by our goals: to enable decentralization
    by having multiple interoperable providers for every part of the system; to make
    it easy for users to switch providers; to give users agency over the content they
    see; and to provide a simple user experience that does not burden users with complexity
    arising from the system''s decentralized nature. The system''s openness allows
    anybody to contribute to content moderation and community management, and we invite
    the research community to use Bluesky as a dataset and testing ground for new
    approaches in social media moderation.'
  arxivId: '2402.03239'
  authors: Martin Kleppmann, Paul Frazee, Jake Gold, Jay Graber, Daniel Holmgren,
    Devin Ivy, Jeromy Johnson, Bryan Newbold, Jaz Volpert
  created_at: '2024-12-22T05:41:39Z'
  issue_number: 105
  issue_url: https://github.com/dmarx/arxiv-archive/issues/105
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Bluesky and the AT Protocol: Usable Decentralized Social Media'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2402.03239
'2402.14903':
  abstract: Tokenization, the division of input text into input tokens, is an often
    overlooked aspect of the large language model (LLM) pipeline and could be the
    source of useful or harmful inductive biases. Historically, LLMs have relied on
    byte pair encoding, without care to specific input domains. With the increased
    use of LLMs for reasoning, various number-specific tokenization schemes have been
    adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization
    while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers.
    In this work, we study the effect this choice has on numerical reasoning through
    the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization
    for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma
    separating numbers at inference time) leads to largely improved performance. Furthermore,
    we find that model errors when using standard left-to-right tokenization follow
    stereotyped error patterns, suggesting that model computations are systematic
    rather than approximate. We show that the model is able to convert between tokenizations
    easily, thus allowing chain-of-thought-inspired approaches to recover performance
    on left-to-right tokenized inputs. We also find the gap between tokenization directions
    decreases when models are scaled, possibly indicating that larger models are better
    able to override this tokenization-dependent inductive bias. In summary, our work
    performs the first study of how number tokenization choices lead to differences
    in model performance on arithmetic tasks, accompanied by a thorough analysis of
    error patterns. We hope this work inspires practitioners to more carefully ablate
    number tokenization-related choices when working towards general models of numerical
    reasoning.
  arxivId: '2402.14903'
  authors: Aaditya K. Singh, DJ Strouse
  created_at: '2024-12-19T22:43:04Z'
  issue_number: 78
  issue_url: https://github.com/dmarx/arxiv-archive/issues/78
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier\
    \ LLMs"
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2402.14903
'2405.17399':
  abstract: The poor performance of transformers on arithmetic tasks seems to stem
    in large part from their inability to keep track of the exact position of each
    digit inside of a large span of digits. We mend this problem by adding an embedding
    to each digit that encodes its position relative to the start of the number. In
    addition to the boost these embeddings provide on their own, we show that this
    fix enables architectural modifications such as input injection and recurrent
    layers to improve performance even further.   With positions resolved, we can
    study the logical extrapolation ability of transformers. Can they solve arithmetic
    problems that are larger and more complex than those in their training data? We
    find that training on only 20 digit numbers with a single GPU for one day, we
    can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit
    addition problems. Finally, we show that these gains in numeracy also unlock improvements
    on other multi-step reasoning tasks including sorting and multiplication.
  arxivId: '2405.17399'
  authors: Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian
    R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild,
    Tom Goldstein
  created_at: '2024-12-19T22:43:45Z'
  issue_number: 79
  issue_url: https://github.com/dmarx/arxiv-archive/issues/79
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Transformers Can Do Arithmetic with the Right Embeddings
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2405.17399v1
'2406.06158':
  abstract: While the impressive performance of modern neural networks is often attributed
    to their capacity to efficiently extract task-relevant features from data, the
    mechanisms underlying this rich feature learning regime remain elusive, with much
    of our theoretical understanding stemming from the opposing lazy regime. In this
    work, we derive exact solutions to a minimal model that transitions between lazy
    and rich learning, precisely elucidating how unbalanced layer-specific initialization
    variances and learning rates determine the degree of feature learning. Our analysis
    reveals that they conspire to influence the learning regime through a set of conserved
    quantities that constrain and modify the geometry of learning trajectories in
    parameter and function space. We extend our analysis to more complex linear models
    with multiple neurons, outputs, and layers and to shallow nonlinear networks with
    piecewise linear activation functions. In linear networks, rapid feature learning
    only occurs from balanced initializations, where all layers learn at similar speeds.
    While in nonlinear networks, unbalanced initializations that promote faster learning
    in earlier layers can accelerate rich learning. Through a series of experiments,
    we provide evidence that this unbalanced rich regime drives feature learning in
    deep finite-width networks, promotes interpretability of early layers in CNNs,
    reduces the sample complexity of learning hierarchical data, and decreases the
    time to grokking in modular arithmetic. Our theory motivates further exploration
    of unbalanced initializations to enhance efficient feature learning.
  arxivId: '2406.06158'
  authors: Daniel Kunin, Allan Ravent√≥s, Cl√©mentine Domin√©, Feng Chen, David Klindt,
    Andrew Saxe, Surya Ganguli
  created_at: '2024-12-22T07:10:42Z'
  issue_number: 120
  issue_url: https://github.com/dmarx/arxiv-archive/issues/120
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Get rich quick: exact solutions reveal how unbalanced initializations\n\
    \  promote rapid feature learning"
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2406.06158
'2406.06248':
  abstract: Dense linear layers are the dominant computational bottleneck in foundation
    models. Identifying more efficient alternatives to dense matrices has enormous
    potential for building more compute-efficient models, as exemplified by the success
    of convolutional networks in the image domain. In this work, we systematically
    explore structured matrices as replacements for dense matrices. We show that different
    structures often require drastically different initialization scales and learning
    rates, which are crucial to performance, especially as models scale. Using insights
    from the Maximal Update Parameterization, we determine the optimal scaling for
    initialization and learning rates of these unconventional layers. Finally, we
    measure the scaling laws of different structures to compare how quickly their
    performance improves with compute. We propose a novel matrix family containing
    Monarch matrices, the Block Tensor-Train (BTT), which we show performs better
    than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with
    augmentation, BTT achieves exponentially lower training loss than dense when training
    MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8
    times less compute and is more efficient than dense for training small GPT-2 language
    models.
  arxivId: '2406.06248'
  authors: Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon
    Wilson
  created_at: '2024-12-22T07:34:10Z'
  issue_number: 130
  issue_url: https://github.com/dmarx/arxiv-archive/issues/130
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Compute Better Spent: Replacing Dense Layers with Structured Matrices'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2406.06248
'2406.19354':
  abstract: 'The model editing problem concerns how language models should learn new
    facts about the world over time. While empirical research on model editing has
    drawn widespread attention, the conceptual foundations of model editing remain
    shaky -- perhaps unsurprisingly, since model editing is essentially belief revision,
    a storied problem in philosophy that has eluded succinct solutions for decades.
    Model editing nonetheless demands a solution, since we need to be able to control
    the knowledge within language models. With this goal in mind, this paper critiques
    the standard formulation of the model editing problem and proposes a formal testbed
    for model editing research. We first describe 12 open problems with model editing,
    based on challenges with (1) defining the problem, (2) developing benchmarks,
    and (3) assuming LLMs have editable beliefs in the first place. Many of these
    challenges are extremely difficult to address, e.g. determining far-reaching consequences
    of edits, labeling probabilistic entailments between facts, and updating beliefs
    of agent simulators. Next, we introduce a semi-synthetic dataset for model editing
    based on Wikidata, where we can evaluate edits against labels given by an idealized
    Bayesian agent. This enables us to say exactly how belief revision in language
    models falls short of a desirable epistemic standard. We encourage further research
    exploring settings where such a gold standard can be compared against. Our code
    is publicly available at: https://github.com/peterbhase/LLM-belief-revision'
  arxivId: '2406.19354'
  authors: Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal
  created_at: '2024-12-22T07:28:19Z'
  issue_number: 129
  issue_url: https://github.com/dmarx/arxiv-archive/issues/129
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision\
    \ Work in LLMs?"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2406.19354
'2407.17465':
  abstract: 'The Maximal Update Parametrization ($\mu$P) aims to make the optimal
    hyperparameters (HPs) of a model independent of its size, allowing them to be
    swept using a cheap proxy model rather than the full-size target model. We present
    a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with Unit Scaling,
    a method for designing models that makes them easy to train in low-precision.
    The two techniques have a natural affinity: $\mu$P ensures that the scale of activations
    is independent of model size, and Unit Scaling ensures that activations, weights
    and gradients begin training with a scale of one. This synthesis opens the door
    to a simpler scheme, whose default values are near-optimal. This in turn facilitates
    a more efficient sweeping strategy, with u-$\mu$P models reaching a loss that
    is equal to or lower than comparable $\mu$P models and working out-of-the-box
    in FP8.'
  arxivId: '2407.17465'
  authors: Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y.
    Prince, Bj√∂rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach,
    Douglas Orr
  created_at: '2024-12-19T18:35:47Z'
  issue_number: 72
  issue_url: https://github.com/dmarx/arxiv-archive/issues/72
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'u-$Œº$P: The Unit-Scaled Maximal Update Parametrization'
  total_reading_time_minutes: 0
  url: https://arxiv.org/html/2407.17465v2
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is a critical step towards building generally self-improving agents that can operate
    on open-ended natural language. In this paper, we study the scaling of inference-time
    computation in LLMs, with a focus on answering the question: if an LLM is allowed
    to use a fixed but non-trivial amount of inference-time compute, how much can
    it improve its performance on a challenging prompt? Answering this question has
    implications not only on the achievable performance of LLMs, but also on the future
    of LLM pretraining and how one should tradeoff inference-time and pre-training
    compute. Despite its importance, little research attempted to understand the scaling
    behaviors of various test-time inference methods. Moreover, current work largely
    provides negative results for a number of these strategies. In this work, we analyze
    two primary mechanisms to scale test-time computation: (1) searching against dense,
    process-based verifier reward models; and (2) updating the model''s distribution
    over a response adaptively, given the prompt at test time. We find that in both
    cases, the effectiveness of different approaches to scaling test-time compute
    critically varies depending on the difficulty of the prompt. This observation
    motivates applying a "compute-optimal" scaling strategy, which acts to most effectively
    allocate test-time compute adaptively per prompt. Using this compute-optimal strategy,
    we can improve the efficiency of test-time compute scaling by more than 4x compared
    to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find
    that on problems where a smaller base model attains somewhat non-trivial success
    rates, test-time compute can be used to outperform a 14x larger model.'
  arxivId: '2408.03314'
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-17T13:48:03Z'
  issue_number: 57
  issue_url: https://github.com/dmarx/arxiv-archive/issues/57
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2408.03314
'2409.09347':
  abstract: Mass transport problems arise in many areas of machine learning whereby
    one wants to compute a map transporting one distribution to another. Generative
    modeling techniques like Generative Adversarial Networks (GANs) and Denoising
    Diffusion Models (DDMs) have been successfully adapted to solve such transport
    problems, resulting in CycleGAN and Bridge Matching respectively. However, these
    methods do not approximate Optimal Transport (OT) maps, which are known to have
    desirable properties. Existing techniques approximating OT maps for high-dimensional
    data-rich problems, such as DDM-based Rectified Flow and Schr\"odinger Bridge
    procedures, require fully training a DDM-type model at each iteration, or use
    mini-batch techniques which can introduce significant errors. We propose a novel
    algorithm to compute the Schr\"odinger Bridge, a dynamic entropy-regularised version
    of OT, that eliminates the need to train multiple DDM-like models. This algorithm
    corresponds to a discretisation of a flow of path measures, which we call the
    Schr\"odinger Bridge Flow, whose only stationary point is the Schr\"odinger Bridge.
    We demonstrate the performance of our algorithm on a variety of unpaired data
    translation tasks.
  arxivId: '2409.09347'
  authors: Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, Arnaud Doucet
  created_at: '2024-12-22T08:05:51Z'
  issue_number: 136
  issue_url: https://github.com/dmarx/arxiv-archive/issues/136
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Schr√∂dinger Bridge Flow for Unpaired Data Translation
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2409.09347
'2410.02117':
  abstract: Dense linear layers are the dominant computational bottleneck in large
    neural networks, presenting a critical need for more efficient alternatives. Previous
    efforts focused on a small number of hand-crafted structured matrices and neglected
    to investigate whether these structures can surpass dense layers in terms of compute-optimal
    scaling laws when both the model size and training examples are optimally allocated.
    In this work, we present a unifying framework that enables searching among all
    linear operators expressible via an Einstein summation. This framework encompasses
    many previously proposed structures, such as low-rank, Kronecker, Tensor-Train,
    Block Tensor-Train (BTT), and Monarch, along with many novel structures. To analyze
    the framework, we develop a taxonomy of all such operators based on their computational
    and algebraic properties and show that differences in the compute-optimal scaling
    laws are mostly governed by a small number of variables that we introduce. Namely,
    a small $\omega$ (which measures parameter sharing) and large $\psi$ (which measures
    the rank) reliably led to better scaling laws. Guided by the insight that full-rank
    structures that maximize parameters per unit of compute perform the best, we propose
    BTT-MoE, a novel Mixture-of-Experts (MoE) architecture obtained by sparsifying
    computation in the BTT structure. In contrast to the standard sparse MoE for each
    entire feed-forward network, BTT-MoE learns an MoE in every single linear layer
    of the model, including the projection matrices in the attention blocks. We find
    BTT-MoE provides a substantial compute-efficiency gain over dense layers and standard
    MoE.
  arxivId: '2410.02117'
  authors: Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen,
    Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson
  created_at: '2024-12-22T07:34:12Z'
  issue_number: 131
  issue_url: https://github.com/dmarx/arxiv-archive/issues/131
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Searching for Efficient Linear Layers over a Continuous Space of\n  Structured\
    \ Matrices"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.02117
'2410.02423':
  abstract: In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm
    for solving imaging inverse problems. PnP methods leverage the strength of pre-trained
    denoisers, often deep neural networks, by integrating them in optimization schemes.
    While they achieve state-of-the-art performance on various inverse problems in
    imaging, PnP approaches face inherent limitations on more generative tasks like
    inpainting. On the other hand, generative models such as Flow Matching pushed
    the boundary in image sampling yet lack a clear method for efficient use in image
    restoration. We propose to combine the PnP framework with Flow Matching (FM) by
    defining a time-dependent denoiser using a pre-trained FM model. Our algorithm
    alternates between gradient descent steps on the data-fidelity term, reprojections
    onto the learned FM path, and denoising. Notably, our method is computationally
    efficient and memory-friendly, as it avoids backpropagation through ODEs and trace
    computations. We evaluate its performance on denoising, super-resolution, deblurring,
    and inpainting tasks, demonstrating superior results compared to existing PnP
    algorithms and Flow Matching based state-of-the-art methods.
  arxivId: '2410.02423'
  authors: S√©gol√®ne Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl
  created_at: '2024-12-22T06:43:50Z'
  issue_number: 115
  issue_url: https://github.com/dmarx/arxiv-archive/issues/115
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'PnP-Flow: Plug-and-Play Image Restoration with Flow Matching'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.02423
'2410.11900':
  abstract: Modern Question Answering (QA) and Reasoning approaches based on Large
    Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought
    (CoT), assuming the resulting generation will have a more granular exploration
    and reasoning over the question space and scope. However, such methods struggle
    with generating outputs that are faithful to the intermediate chain of reasoning
    produced by the model. On the other end of the spectrum, neuro-symbolic methods
    such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers.
    While such approaches boast a high degree of faithfulness, they usually require
    a model trained for code generation and struggle with tasks that are ambiguous
    or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided
    $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable
    approach for traversing the problem space using task decompositions. We use the
    LLM to plan a solution, soft-formalise the query into facts and predicates using
    a logic programming code and simulate that code execution using an exhaustive
    multi-hop search over the defined space. Our method allows us to compute the faithfulness
    of the reasoning process w.r.t. the generated code and analyse the steps of the
    multi-hop search without relying on external solvers. Our methods achieve SOTA
    results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also
    show that model faithfulness positively correlates with overall performance and
    further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors
    sufficient for and leading to the correct answer with optimal reasoning during
    the multi-hop search.
  arxivId: '2410.11900'
  authors: Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle
    Augenstein
  created_at: '2024-12-22T07:17:20Z'
  issue_number: 121
  issue_url: https://github.com/dmarx/arxiv-archive/issues/121
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'FLARE: Faithful Logic-Aided Reasoning and Exploration'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.11900
'2410.15468':
  abstract: 'We consider emergence from the perspective of dynamics: states of a system
    evolving with time. We focus on the role of a decomposition of wholes into parts,
    and attempt to characterize relationships between levels without reference to
    whether higher-level properties are "novel" or "unexpected." We offer a classification
    of different varieties of emergence, with and without new ontological elements
    at higher levels.'
  arxivId: '2410.15468'
  authors: Sean M. Carroll, Achyuth Parola
  created_at: '2024-12-22T05:48:56Z'
  issue_number: 107
  issue_url: https://github.com/dmarx/arxiv-archive/issues/107
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: What Emergence Can Possibly Mean
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.15468
'2410.15815':
  abstract: 'We present a method for computing free-energy differences using thermodynamic
    integration with a neural network potential that interpolates between two target
    Hamiltonians. The interpolation is defined at the sample distribution level, and
    the neural network potential is optimized to match the corresponding equilibrium
    potential at every intermediate time-step. Once the interpolating potentials and
    samples are well-aligned, the free-energy difference can be estimated using (neural)
    thermodynamic integration. To target molecular systems, we simultaneously couple
    Lennard-Jones and electrostatic interactions and model the rigid-body rotation
    of molecules. We report accurate results for several benchmark systems: a Lennard-Jones
    particle in a Lennard-Jones fluid, as well as the insertion of both water and
    methane solutes in a water solvent at atomistic resolution using a simple three-body
    neural-network potential.'
  arxivId: '2410.15815'
  authors: B√°lint M√°t√©, Fran√ßois Fleuret, Tristan Bereau
  created_at: '2024-12-22T07:19:55Z'
  issue_number: 123
  issue_url: https://github.com/dmarx/arxiv-archive/issues/123
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Solvation Free Energies from Neural Thermodynamic Integration
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.15815
'2410.24054':
  abstract: We develop EigenVI, an eigenvalue-based approach for black-box variational
    inference (BBVI). EigenVI constructs its variational approximations from orthogonal
    function expansions. For distributions over $\mathbb{R}^D$, the lowest order term
    in these expansions provides a Gaussian variational approximation, while higher-order
    terms provide a systematic way to model non-Gaussianity. These approximations
    are flexible enough to model complex distributions (multimodal, asymmetric), but
    they are simple enough that one can calculate their low-order moments and draw
    samples from them. EigenVI can also model other types of random variables (e.g.,
    nonnegative, bounded) by constructing variational approximations from different
    families of orthogonal functions. Within these families, EigenVI computes the
    variational approximation that best matches the score function of the target distribution
    by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization
    reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps
    the iterative gradient-based optimizations that are required for many other BBVI
    algorithms. (Gradient-based methods can be sensitive to learning rates, termination
    criteria, and other tunable hyperparameters.) We use EigenVI to approximate a
    variety of target distributions, including a benchmark suite of Bayesian models
    from posteriordb. On these distributions, we find that EigenVI is more accurate
    than existing methods for Gaussian BBVI.
  arxivId: '2410.24054'
  authors: Diana Cai, Chirag Modi, Charles C. Margossian, Robert M. Gower, David M.
    Blei, Lawrence K. Saul
  created_at: '2024-12-22T05:52:47Z'
  issue_number: 109
  issue_url: https://github.com/dmarx/arxiv-archive/issues/109
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "EigenVI: score-based variational inference with orthogonal function\n  expansions"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.24054
'2411.19722':
  abstract: Removing modeling constraints and unifying architectures across domains
    has been a key driver of the recent progress in training large multimodal models.
    However, most of these models still rely on many separately trained components
    such as modality-specific encoders and decoders. In this work, we further streamline
    joint generative modeling of images and text. We propose an autoregressive decoder-only
    transformer - JetFormer - which is trained to directly maximize the likelihood
    of raw data, without relying on any separately pretrained components, and can
    understand and generate both text and images. Specifically, we leverage a normalizing
    flow model to obtain a soft-token image representation that is jointly trained
    with an autoregressive multimodal transformer. The normalizing flow model serves
    as both an image encoder for perception tasks and an image decoder for image generation
    tasks during inference. JetFormer achieves text-to-image generation quality competitive
    with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
    image autoencoders, which are trained with a complex mixture of losses, including
    perceptual ones. At the same time, JetFormer demonstrates robust image understanding
    capabilities. To the best of our knowledge, JetFormer is the first model that
    is capable of generating high-fidelity images and producing strong log-likelihood
    bounds.
  arxivId: '2411.19722'
  authors: Michael Tschannen, Andr√© Susano Pinto, Alexander Kolesnikov
  created_at: '2024-12-16T17:41:00Z'
  issue_number: 54
  issue_url: https://github.com/dmarx/arxiv-archive/issues/54
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2411.19722
'2412.05265':
  abstract: This manuscript gives a big-picture, up-to-date overview of the field
    of (deep) reinforcement learning and sequential decision making, covering value-based
    RL, policy-gradient methods, model-based methods, and various other topics (including
    a very brief discussion of RL+LLMs).
  arxivId: '2412.05265'
  authors: Kevin Murphy
  created_at: '2024-12-22T07:23:45Z'
  issue_number: 125
  issue_url: https://github.com/dmarx/arxiv-archive/issues/125
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Reinforcement Learning: An Overview'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2412.05265
'2412.06264':
  abstract: Flow Matching (FM) is a recent framework for generative modeling that
    has achieved state-of-the-art performance across various domains, including image,
    video, audio, speech, and biological structures. This guide offers a comprehensive
    and self-contained review of FM, covering its mathematical foundations, design
    choices, and extensions. By also providing a PyTorch package featuring relevant
    examples (e.g., image and text generation), this work aims to serve as a resource
    for both novice and experienced researchers interested in understanding, applying
    and further developing FM.
  arxivId: '2412.06264'
  authors: Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian
    Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat
  created_at: '2024-12-22T06:52:21Z'
  issue_number: 117
  issue_url: https://github.com/dmarx/arxiv-archive/issues/117
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Flow Matching Guide and Code
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2412.06264
'2412.09621':
  abstract: 'Learning to understand dynamic 3D scenes from imagery is crucial for
    applications ranging from robotics to scene reconstruction. Yet, unlike other
    problems where large-scale supervised training has enabled rapid progress, directly
    supervising methods for recovering 3D motion remains challenging due to the fundamental
    difficulty of obtaining ground truth annotations. We present a system for mining
    high-quality 4D reconstructions from internet stereoscopic, wide-angle videos.
    Our system fuses and filters the outputs of camera pose estimation, stereo depth
    estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions.
    We use this method to generate large-scale data in the form of world-consistent,
    pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate
    the utility of this data by training a variant of DUSt3R to predict structure
    and 3D motion from real-world image pairs, showing that training on our reconstructed
    data enables generalization to diverse real-world scenes. Project page: https://stereo4d.github.io'
  arxivId: '2412.09621'
  authors: Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander
    Holynski
  created_at: '2024-12-16T08:59:30Z'
  issue_number: 53
  issue_url: https://github.com/dmarx/arxiv-archive/issues/53
  labels:
  - reading-session
  last_read: '2024-12-16T08:59:30.170Z'
  state: open
  title: 'Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos'
  total_reading_time_minutes: 1
  url: https://arxiv.org/abs/2412.09621v1
'2412.13145':
  abstract: 'Could an AI have conscious experiences? Any answer to this question should
    conform to Evidentialism - that is, it should be based not on intuition, dogma
    or speculation but on solid scientific evidence. I argue that such evidence is
    hard to come by and that the only justifiable stance on the prospects of artificial
    consciousness is agnosticism. In the current debate, the main division is between
    biological views that are sceptical of artificial consciousness and functional
    views that are sympathetic to it. I argue that both camps make the same mistake
    of over-estimating what the evidence tells us. Scientific insights into consciousness
    have been achieved through the study of conscious organisms. Although this has
    enabled cautious assessments of consciousness in various creatures, extending
    this to AI faces serious obstacles. AI thus presents consciousness researchers
    with a dilemma: either reach a verdict on artificial consciousness but violate
    Evidentialism; or respect Evidentialism but offer no verdict on the prospects
    of artificial consciousness. The dominant trend in the literature has been to
    take the first option while purporting to follow the scientific evidence. I argue
    that if we truly follow the evidence, we must take the second option and adopt
    agnosticism.'
  arxivId: '2412.13145'
  authors: Tom McClelland
  created_at: '2024-12-22T05:22:42Z'
  issue_number: 102
  issue_url: https://github.com/dmarx/arxiv-archive/issues/102
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Agnosticism About Artificial Consciousness
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2412.13145
'2412.13663':
  abstract: Encoder-only transformer models such as BERT offer a great performance-size
    tradeoff for retrieval and classification tasks with respect to larger decoder-only
    models. Despite being the workhorse of numerous production pipelines, there have
    been limited Pareto improvements to BERT since its release. In this paper, we
    introduce ModernBERT, bringing modern model optimizations to encoder-only models
    and representing a major Pareto improvement over older encoders. Trained on 2
    trillion tokens with a native 8192 sequence length, ModernBERT models exhibit
    state-of-the-art results on a large pool of evaluations encompassing diverse classification
    tasks and both single and multi-vector retrieval on different domains (including
    code). In addition to strong downstream performance, ModernBERT is also the most
    speed and memory efficient encoder and is designed for inference on common GPUs.
  arxivId: '2412.13663'
  authors: Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar
    Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom
    Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli
  created_at: '2024-12-22T06:30:57Z'
  issue_number: 113
  issue_url: https://github.com/dmarx/arxiv-archive/issues/113
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast,\
    \ Memory Efficient, and Long Context Finetuning and Inference"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2412.13663
