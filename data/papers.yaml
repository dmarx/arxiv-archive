'1711.11586':
  abstract: Many image-to-image translation problems are ambiguous, as a single input
    image may correspond to multiple possible outputs. In this work, we aim to model
    a \emph{distribution} of possible outputs in a conditional generative modeling
    setting. The ambiguity of the mapping is distilled in a low-dimensional latent
    vector, which can be randomly sampled at test time. A generator learns to map
    the given input, combined with this latent code, to the output. We explicitly
    encourage the connection between output and the latent code to be invertible.
    This helps prevent a many-to-one mapping from the latent code to the output during
    training, also known as the problem of mode collapse, and produces more diverse
    results. We explore several variants of this approach by employing different training
    objectives, network architectures, and methods of injecting the latent code. Our
    proposed method encourages bijective consistency between the latent encoding and
    output modes. We present a systematic comparison of our method and other variants
    on both perceptual realism and diversity.
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-15T07:21:47Z'
  issue_number: 14
  issue_url: https://github.com/dmarx/arxiv-archive/issues/14
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1711.11586
'1904.08779':
  abstract: We present SpecAugment, a simple data augmentation method for speech recognition.
    SpecAugment is applied directly to the feature inputs of a neural network (i.e.,
    filter bank coefficients). The augmentation policy consists of warping the features,
    masking blocks of frequency channels, and masking blocks of time steps. We apply
    SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition
    tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard
    300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER
    on test-other without the use of a language model, and 5.8% WER with shallow fusion
    with a language model. This compares to the previous state-of-the-art hybrid system
    of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome
    portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1%
    with shallow fusion, which compares to the previous state-of-the-art hybrid system
    at 8.3%/17.3% WER.
  arxivId: '1904.08779'
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2024-12-17T14:41:06Z'
  issue_number: 60
  issue_url: https://github.com/dmarx/arxiv-archive/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1904.08779
'2203.15556':
  abstract: 'We investigate the optimal model size and number of tokens for training
    a transformer language model under a given compute budget. We find that current
    large language models are significantly undertrained, a consequence of the recent
    focus on scaling language models whilst keeping the amount of training data constant.
    By training over 400 language models ranging from 70 million to over 16 billion
    parameters on 5 to 500 billion tokens, we find that for compute-optimal training,
    the model size and the number of training tokens should be scaled equally: for
    every doubling of model size the number of training tokens should also be doubled.
    We test this hypothesis by training a predicted compute-optimal model, Chinchilla,
    that uses the same compute budget as Gopher but with 70B parameters and 4$\times$
    more more data. Chinchilla uniformly and significantly outperforms Gopher (280B),
    GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range
    of downstream evaluation tasks. This also means that Chinchilla uses substantially
    less compute for fine-tuning and inference, greatly facilitating downstream usage.
    As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
    on the MMLU benchmark, greater than a 7% improvement over Gopher.'
  arxivId: '2203.15556'
  authors: Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den
    Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen,
    Jack W. Rae, Oriol Vinyals, Laurent Sifre
  created_at: '2024-12-18T22:03:26Z'
  issue_number: 67
  issue_url: https://github.com/dmarx/arxiv-archive/issues/67
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Training Compute-Optimal Large Language Models
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2203.15556
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    for generating high-dimensional data. In this paper, we show that the dynamics
    of these models exhibit a spontaneous symmetry breaking that divides the generative
    dynamics into two distinct phases: 1) A linear steady-state dynamics around a
    central fixed-point and 2) an attractor dynamics directed towards the data manifold.
    These two "phases" are separated by the change in stability of the central fixed-point,
    with the resulting window of instability being responsible for the diversity of
    the generated samples. Using both theoretical and empirical evidence, we show
    that an accurate simulation of the early dynamics does not significantly contribute
    to the final generation, since early fluctuations are reverted to the central
    fixed point. To leverage this insight, we propose a Gaussian late initialization
    scheme, which significantly improves model performance, achieving up to 3x FID
    improvements on fast samplers, while also increasing sample diversity (e.g., racial
    composition of generated CelebA images). Our work offers a new way to understand
    the generative dynamics of diffusion models that has the potential to bring about
    higher performance and less biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-15T09:58:39Z'
  issue_number: 24
  issue_url: https://github.com/dmarx/arxiv-archive/issues/24
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2305.19693
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering
    times by orders of magnitude. While these methods demonstrate state-of-the-art
    quality and speed, they are designed for photogrammetry of static scenes and do
    not generalize well to freely moving humans in the environment. In this work,
    we introduce Human Gaussian Splats (HUGS) that represents an animatable human
    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only
    a monocular video with a small number of (50-100) frames, and it automatically
    learns to disentangle the static scene and a fully animatable human avatar within
    30 minutes. We utilize the SMPL body model to initialize the human Gaussians.
    To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow
    the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians
    for animated humans brings new challenges, including the artifacts created when
    articulating the Gaussians. We propose to jointly optimize the linear blend skinning
    weights to coordinate the movements of individual Gaussians during animation.
    Our approach enables novel-pose synthesis of human and novel view synthesis of
    both the human and the scene. We achieve state-of-the-art rendering quality with
    a rendering speed of 60 FPS while being ~100x faster to train over previous work.
    Our code will be announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-15T08:20:13Z'
  issue_number: 18
  issue_url: https://github.com/dmarx/arxiv-archive/issues/18
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'HUGS: Human Gaussian Splats'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2311.17910
'2401.17671':
  abstract: Recent advancements in artificial intelligence have sparked interest in
    the parallels between large language models (LLMs) and human neural processing,
    particularly in language comprehension. While prior research has established similarities
    in the representation of LLMs and the brain, the underlying computational principles
    that cause this convergence, especially in the context of evolving LLMs, remain
    elusive. Here, we examined a diverse selection of high-performance LLMs with similar
    parameter sizes to investigate the factors contributing to their alignment with
    the brain's language processing mechanisms. We find that as LLMs achieve higher
    performance on benchmark tasks, they not only become more brain-like as measured
    by higher performance when predicting neural responses from LLM embeddings, but
    also their hierarchical feature extraction pathways map more closely onto the
    brain's while using fewer layers to do the same encoding. We also compare the
    feature extraction pathways of the LLMs to each other and identify new ways in
    which high-performing models have converged toward similar hierarchical processing
    mechanisms. Finally, we show the importance of contextual information in improving
    model performance and brain similarity. Our findings reveal the converging aspects
    of language processing in the brain and LLMs and offer new directions for developing
    models that align more closely with human cognitive processing.
  arxivId: '2401.17671'
  authors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima
    Mesgarani
  created_at: '2024-12-19T11:12:50Z'
  issue_number: 70
  issue_url: https://github.com/dmarx/arxiv-archive/issues/70
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Contextual Feature Extraction Hierarchies Converge in Large Language\n \
    \ Models and the Brain"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2401.17671
'2402.14903':
  abstract: Tokenization, the division of input text into input tokens, is an often
    overlooked aspect of the large language model (LLM) pipeline and could be the
    source of useful or harmful inductive biases. Historically, LLMs have relied on
    byte pair encoding, without care to specific input domains. With the increased
    use of LLMs for reasoning, various number-specific tokenization schemes have been
    adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization
    while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers.
    In this work, we study the effect this choice has on numerical reasoning through
    the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization
    for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma
    separating numbers at inference time) leads to largely improved performance. Furthermore,
    we find that model errors when using standard left-to-right tokenization follow
    stereotyped error patterns, suggesting that model computations are systematic
    rather than approximate. We show that the model is able to convert between tokenizations
    easily, thus allowing chain-of-thought-inspired approaches to recover performance
    on left-to-right tokenized inputs. We also find the gap between tokenization directions
    decreases when models are scaled, possibly indicating that larger models are better
    able to override this tokenization-dependent inductive bias. In summary, our work
    performs the first study of how number tokenization choices lead to differences
    in model performance on arithmetic tasks, accompanied by a thorough analysis of
    error patterns. We hope this work inspires practitioners to more carefully ablate
    number tokenization-related choices when working towards general models of numerical
    reasoning.
  arxivId: '2402.14903'
  authors: Aaditya K. Singh, DJ Strouse
  created_at: '2024-12-19T22:43:04Z'
  issue_number: 78
  issue_url: https://github.com/dmarx/arxiv-archive/issues/78
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier\
    \ LLMs"
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2402.14903
'2405.17399':
  abstract: The poor performance of transformers on arithmetic tasks seems to stem
    in large part from their inability to keep track of the exact position of each
    digit inside of a large span of digits. We mend this problem by adding an embedding
    to each digit that encodes its position relative to the start of the number. In
    addition to the boost these embeddings provide on their own, we show that this
    fix enables architectural modifications such as input injection and recurrent
    layers to improve performance even further.   With positions resolved, we can
    study the logical extrapolation ability of transformers. Can they solve arithmetic
    problems that are larger and more complex than those in their training data? We
    find that training on only 20 digit numbers with a single GPU for one day, we
    can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit
    addition problems. Finally, we show that these gains in numeracy also unlock improvements
    on other multi-step reasoning tasks including sorting and multiplication.
  arxivId: '2405.17399'
  authors: Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian
    R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild,
    Tom Goldstein
  created_at: '2024-12-19T22:43:45Z'
  issue_number: 79
  issue_url: https://github.com/dmarx/arxiv-archive/issues/79
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Transformers Can Do Arithmetic with the Right Embeddings
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2405.17399v1
'2407.17465':
  abstract: 'The Maximal Update Parametrization ($\mu$P) aims to make the optimal
    hyperparameters (HPs) of a model independent of its size, allowing them to be
    swept using a cheap proxy model rather than the full-size target model. We present
    a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with Unit Scaling,
    a method for designing models that makes them easy to train in low-precision.
    The two techniques have a natural affinity: $\mu$P ensures that the scale of activations
    is independent of model size, and Unit Scaling ensures that activations, weights
    and gradients begin training with a scale of one. This synthesis opens the door
    to a simpler scheme, whose default values are near-optimal. This in turn facilitates
    a more efficient sweeping strategy, with u-$\mu$P models reaching a loss that
    is equal to or lower than comparable $\mu$P models and working out-of-the-box
    in FP8.'
  arxivId: '2407.17465'
  authors: Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y.
    Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach,
    Douglas Orr
  created_at: '2024-12-19T18:35:47Z'
  issue_number: 72
  issue_url: https://github.com/dmarx/arxiv-archive/issues/72
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'u-$μ$P: The Unit-Scaled Maximal Update Parametrization'
  total_reading_time_minutes: 0
  url: https://arxiv.org/html/2407.17465v2
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is a critical step towards building generally self-improving agents that can operate
    on open-ended natural language. In this paper, we study the scaling of inference-time
    computation in LLMs, with a focus on answering the question: if an LLM is allowed
    to use a fixed but non-trivial amount of inference-time compute, how much can
    it improve its performance on a challenging prompt? Answering this question has
    implications not only on the achievable performance of LLMs, but also on the future
    of LLM pretraining and how one should tradeoff inference-time and pre-training
    compute. Despite its importance, little research attempted to understand the scaling
    behaviors of various test-time inference methods. Moreover, current work largely
    provides negative results for a number of these strategies. In this work, we analyze
    two primary mechanisms to scale test-time computation: (1) searching against dense,
    process-based verifier reward models; and (2) updating the model''s distribution
    over a response adaptively, given the prompt at test time. We find that in both
    cases, the effectiveness of different approaches to scaling test-time compute
    critically varies depending on the difficulty of the prompt. This observation
    motivates applying a "compute-optimal" scaling strategy, which acts to most effectively
    allocate test-time compute adaptively per prompt. Using this compute-optimal strategy,
    we can improve the efficiency of test-time compute scaling by more than 4x compared
    to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find
    that on problems where a smaller base model attains somewhat non-trivial success
    rates, test-time compute can be used to outperform a 14x larger model.'
  arxivId: '2408.03314'
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-17T13:48:03Z'
  issue_number: 57
  issue_url: https://github.com/dmarx/arxiv-archive/issues/57
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2408.03314
'2411.19722':
  abstract: Removing modeling constraints and unifying architectures across domains
    has been a key driver of the recent progress in training large multimodal models.
    However, most of these models still rely on many separately trained components
    such as modality-specific encoders and decoders. In this work, we further streamline
    joint generative modeling of images and text. We propose an autoregressive decoder-only
    transformer - JetFormer - which is trained to directly maximize the likelihood
    of raw data, without relying on any separately pretrained components, and can
    understand and generate both text and images. Specifically, we leverage a normalizing
    flow model to obtain a soft-token image representation that is jointly trained
    with an autoregressive multimodal transformer. The normalizing flow model serves
    as both an image encoder for perception tasks and an image decoder for image generation
    tasks during inference. JetFormer achieves text-to-image generation quality competitive
    with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
    image autoencoders, which are trained with a complex mixture of losses, including
    perceptual ones. At the same time, JetFormer demonstrates robust image understanding
    capabilities. To the best of our knowledge, JetFormer is the first model that
    is capable of generating high-fidelity images and producing strong log-likelihood
    bounds.
  arxivId: '2411.19722'
  authors: Michael Tschannen, André Susano Pinto, Alexander Kolesnikov
  created_at: '2024-12-16T17:41:00Z'
  issue_number: 54
  issue_url: https://github.com/dmarx/arxiv-archive/issues/54
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2411.19722
'2412.09621':
  abstract: 'Learning to understand dynamic 3D scenes from imagery is crucial for
    applications ranging from robotics to scene reconstruction. Yet, unlike other
    problems where large-scale supervised training has enabled rapid progress, directly
    supervising methods for recovering 3D motion remains challenging due to the fundamental
    difficulty of obtaining ground truth annotations. We present a system for mining
    high-quality 4D reconstructions from internet stereoscopic, wide-angle videos.
    Our system fuses and filters the outputs of camera pose estimation, stereo depth
    estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions.
    We use this method to generate large-scale data in the form of world-consistent,
    pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate
    the utility of this data by training a variant of DUSt3R to predict structure
    and 3D motion from real-world image pairs, showing that training on our reconstructed
    data enables generalization to diverse real-world scenes. Project page: https://stereo4d.github.io'
  arxivId: '2412.09621'
  authors: Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander
    Holynski
  created_at: '2024-12-16T08:59:30Z'
  issue_number: 53
  issue_url: https://github.com/dmarx/arxiv-archive/issues/53
  labels:
  - reading-session
  last_read: '2024-12-16T08:59:30.170Z'
  state: open
  title: 'Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos'
  total_reading_time_minutes: 1
  url: https://arxiv.org/abs/2412.09621v1
