'1711.11586':
  abstract: Many image-to-image translation problems are ambiguous, as a single input
    image may correspond to multiple possible outputs. In this work, we aim to model
    a \emph{distribution} of possible outputs in a conditional generative modeling
    setting. The ambiguity of the mapping is distilled in a low-dimensional latent
    vector, which can be randomly sampled at test time. A generator learns to map
    the given input, combined with this latent code, to the output. We explicitly
    encourage the connection between output and the latent code to be invertible.
    This helps prevent a many-to-one mapping from the latent code to the output during
    training, also known as the problem of mode collapse, and produces more diverse
    results. We explore several variants of this approach by employing different training
    objectives, network architectures, and methods of injecting the latent code. Our
    proposed method encourages bijective consistency between the latent encoding and
    output modes. We present a systematic comparison of our method and other variants
    on both perceptual realism and diversity.
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-15T07:21:47Z'
  issue_number: 14
  issue_url: https://github.com/dmarx/arxiv-archive/issues/14
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1711.11586
'1904.08779':
  abstract: We present SpecAugment, a simple data augmentation method for speech recognition.
    SpecAugment is applied directly to the feature inputs of a neural network (i.e.,
    filter bank coefficients). The augmentation policy consists of warping the features,
    masking blocks of frequency channels, and masking blocks of time steps. We apply
    SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition
    tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard
    300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER
    on test-other without the use of a language model, and 5.8% WER with shallow fusion
    with a language model. This compares to the previous state-of-the-art hybrid system
    of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome
    portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1%
    with shallow fusion, which compares to the previous state-of-the-art hybrid system
    at 8.3%/17.3% WER.
  arxivId: '1904.08779'
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2024-12-17T14:41:06Z'
  issue_number: 60
  issue_url: https://github.com/dmarx/arxiv-archive/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1904.08779
'2105.05720':
  abstract: Recent trend towards increasing large machine learning models require
    both training and inference tasks to be distributed. Considering the huge cost
    of training these models, it is imperative to unlock optimizations in computation
    and communication to obtain best performance. However, current logical separation
    between computation and communication kernels in deep learning frameworks misses
    the optimization opportunities across such barrier. Breaking this abstraction
    with a holistic consideration can provide many optimizations to provide performance
    improvements in distributed workloads. Manually applying these optimizations needs
    modifications in underlying computation and communication libraries for each scenario,
    which is time consuming and error-prone.   Therefore, we present CoCoNeT, with
    a DSL to express a program with both computation and communication. CoCoNeT contains
    several machine learning aware transformations to optimize a program and a compiler
    to generate high performance kernels. Providing both computation and communication
    as first class constructs allows users to work on a high-level abstraction and
    apply powerful optimizations, such as fusion or overlapping of communication and
    computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel
    workloads in large language models with only a few lines of code. Experiments
    show CoCoNeT significantly outperforms state-of-the-art distributed machine learning
    implementations.
  arxivId: '2105.05720'
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2024-12-16T05:53:22Z'
  issue_number: 49
  issue_url: https://github.com/dmarx/arxiv-archive/issues/49
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2105.05720
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    for generating high-dimensional data. In this paper, we show that the dynamics
    of these models exhibit a spontaneous symmetry breaking that divides the generative
    dynamics into two distinct phases: 1) A linear steady-state dynamics around a
    central fixed-point and 2) an attractor dynamics directed towards the data manifold.
    These two "phases" are separated by the change in stability of the central fixed-point,
    with the resulting window of instability being responsible for the diversity of
    the generated samples. Using both theoretical and empirical evidence, we show
    that an accurate simulation of the early dynamics does not significantly contribute
    to the final generation, since early fluctuations are reverted to the central
    fixed point. To leverage this insight, we propose a Gaussian late initialization
    scheme, which significantly improves model performance, achieving up to 3x FID
    improvements on fast samplers, while also increasing sample diversity (e.g., racial
    composition of generated CelebA images). Our work offers a new way to understand
    the generative dynamics of diffusion models that has the potential to bring about
    higher performance and less biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-15T09:58:39Z'
  issue_number: 24
  issue_url: https://github.com/dmarx/arxiv-archive/issues/24
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2305.19693
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering
    times by orders of magnitude. While these methods demonstrate state-of-the-art
    quality and speed, they are designed for photogrammetry of static scenes and do
    not generalize well to freely moving humans in the environment. In this work,
    we introduce Human Gaussian Splats (HUGS) that represents an animatable human
    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only
    a monocular video with a small number of (50-100) frames, and it automatically
    learns to disentangle the static scene and a fully animatable human avatar within
    30 minutes. We utilize the SMPL body model to initialize the human Gaussians.
    To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow
    the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians
    for animated humans brings new challenges, including the artifacts created when
    articulating the Gaussians. We propose to jointly optimize the linear blend skinning
    weights to coordinate the movements of individual Gaussians during animation.
    Our approach enables novel-pose synthesis of human and novel view synthesis of
    both the human and the scene. We achieve state-of-the-art rendering quality with
    a rendering speed of 60 FPS while being ~100x faster to train over previous work.
    Our code will be announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-15T08:20:13Z'
  issue_number: 18
  issue_url: https://github.com/dmarx/arxiv-archive/issues/18
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'HUGS: Human Gaussian Splats'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2311.17910
'2312.00330':
  abstract: Text-to-video (T2V) models have shown remarkable capabilities in generating
    diverse videos. However, they struggle to produce user-desired stylized videos
    due to (i) text's inherent clumsiness in expressing specific styles and (ii) the
    generally degraded style fidelity. To address these challenges, we introduce StyleCrafter,
    a generic method that enhances pre-trained T2V models with a style control adapter,
    enabling video generation in any style by providing a reference image. Considering
    the scarcity of stylized video datasets, we propose to first train a style control
    adapter using style-rich image datasets, then transfer the learned stylization
    ability to video generation through a tailor-made finetuning paradigm. To promote
    content-style disentanglement, we remove style descriptions from the text prompt
    and extract style information solely from the reference image using a decoupling
    learning strategy. Additionally, we design a scale-adaptive fusion module to balance
    the influences of text-based content features and image-based style features,
    which helps generalization across various text and style combinations. StyleCrafter
    efficiently generates high-quality stylized videos that align with the content
    of the texts and resemble the style of the reference images. Experiments demonstrate
    that our approach is more flexible and efficient than existing competitors.
  arxivId: '2312.00330'
  authors: Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang,
    Xintao Wang, Yujiu Yang, Ying Shan
  created_at: '2024-12-15T20:33:50Z'
  issue_number: 38
  issue_url: https://github.com/dmarx/arxiv-archive/issues/38
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style\n \
    \ Adapter"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2312.00330
'2405.20053':
  abstract: Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context
    learning capabilities; however, their behaviors are often difficult to control.
    By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible
    to fine-tune unsupervised LMs to follow instructions and produce outputs that
    reflect human preferences. Despite its benefits, RLHF has been shown to potentially
    harm a language model's reasoning capabilities and introduce artifacts such as
    hallucinations where the model may fabricate facts. To address this issue we introduce
    Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn
    human preference signals through an auxiliary reward head without directly affecting
    the output distribution of the language modeling head. We perform a theoretical
    analysis of our objective function and find strong ties to Conservative Direct
    Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE,
    and the GPT4All evaluation suite and demonstrate that our method produces models
    which achieve higher scores than those fine-tuned with Supervised Fine-Tuning
    (SFT) or Direct Preference Optimization (DPO) alone.
  arxivId: '2405.20053'
  authors: Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic
  created_at: '2024-12-15T17:25:18Z'
  issue_number: 31
  issue_url: https://github.com/dmarx/arxiv-archive/issues/31
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Would I Lie To You? Inference Time Alignment of Language Models using\n\
    \  Direct Preference Heads"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2405.20053
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is a critical step towards building generally self-improving agents that can operate
    on open-ended natural language. In this paper, we study the scaling of inference-time
    computation in LLMs, with a focus on answering the question: if an LLM is allowed
    to use a fixed but non-trivial amount of inference-time compute, how much can
    it improve its performance on a challenging prompt? Answering this question has
    implications not only on the achievable performance of LLMs, but also on the future
    of LLM pretraining and how one should tradeoff inference-time and pre-training
    compute. Despite its importance, little research attempted to understand the scaling
    behaviors of various test-time inference methods. Moreover, current work largely
    provides negative results for a number of these strategies. In this work, we analyze
    two primary mechanisms to scale test-time computation: (1) searching against dense,
    process-based verifier reward models; and (2) updating the model''s distribution
    over a response adaptively, given the prompt at test time. We find that in both
    cases, the effectiveness of different approaches to scaling test-time compute
    critically varies depending on the difficulty of the prompt. This observation
    motivates applying a "compute-optimal" scaling strategy, which acts to most effectively
    allocate test-time compute adaptively per prompt. Using this compute-optimal strategy,
    we can improve the efficiency of test-time compute scaling by more than 4x compared
    to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find
    that on problems where a smaller base model attains somewhat non-trivial success
    rates, test-time compute can be used to outperform a 14x larger model.'
  arxivId: '2408.03314'
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-17T13:48:03Z'
  issue_number: 57
  issue_url: https://github.com/dmarx/arxiv-archive/issues/57
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2408.03314
'2410.01131':
  abstract: We propose a novel neural network architecture, the normalized Transformer
    (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming
    the embeddings, MLP, attention matrices and hidden states are unit norm normalized.
    The input stream of tokens travels on the surface of a hypersphere, with each
    layer contributing a displacement towards the target output predictions. These
    displacements are defined by the MLP and attention blocks, whose vector components
    also reside on the same hypersphere. Experiments show that nGPT learns much faster,
    reducing the number of training steps required to achieve the same accuracy by
    a factor of 4 to 20, depending on the sequence length.
  arxivId: '2410.01131'
  authors: Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg
  created_at: '2024-12-15T17:43:04Z'
  issue_number: 33
  issue_url: https://github.com/dmarx/arxiv-archive/issues/33
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.01131
'2411.18933':
  abstract: Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video
    object segmentation and tracking anything. Key components of SAM 2 that drive
    the impressive video object segmentation performance include a large multistage
    image encoder for frame feature extraction and a memory mechanism that stores
    memory contexts from past frames to help current frame segmentation. The high
    computation complexity of multistage image encoder and memory module has limited
    its applications in real-world tasks, e.g., video object segmentation on mobile
    devices. To address this limitation, we propose EfficientTAMs, lightweight track
    anything models that produce high-quality results with low latency and model size.
    Our idea is based on revisiting the plain, nonhierarchical Vision Transformer
    (ViT) as an image encoder for video object segmentation, and introducing an efficient
    memory module, which reduces the complexity for both frame feature extraction
    and memory computation for current frame segmentation. We take vanilla lightweight
    ViTs and efficient memory module to build EfficientTAMs, and train the models
    on SA-1B and SA-V datasets for video object segmentation and track anything tasks.
    We evaluate on multiple video segmentation benchmarks including semi-supervised
    VOS and promptable video segmentation, and find that our proposed EfficientTAM
    with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup
    on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs
    also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter
    reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can
    run at ~10 FPS for performing video object segmentation with reasonable quality,
    highlighting the capability of small models for on-device video object segmentation
    applications.
  arxivId: '2411.18933'
  authors: Yunyang Xiong, Chong Zhou, Xiaoyu Xiang, Lemeng Wu, Chenchen Zhu, Zechun
    Liu, Saksham Suri, Balakrishnan Varadarajan, Ramya Akula, Forrest Iandola, Raghuraman
    Krishnamoorthi, Bilge Soran, Vikas Chandra
  created_at: '2024-12-15T20:13:55Z'
  issue_number: 37
  issue_url: https://github.com/dmarx/arxiv-archive/issues/37
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Efficient Track Anything
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2411.18933
'2411.19722':
  abstract: Removing modeling constraints and unifying architectures across domains
    has been a key driver of the recent progress in training large multimodal models.
    However, most of these models still rely on many separately trained components
    such as modality-specific encoders and decoders. In this work, we further streamline
    joint generative modeling of images and text. We propose an autoregressive decoder-only
    transformer - JetFormer - which is trained to directly maximize the likelihood
    of raw data, without relying on any separately pretrained components, and can
    understand and generate both text and images. Specifically, we leverage a normalizing
    flow model to obtain a soft-token image representation that is jointly trained
    with an autoregressive multimodal transformer. The normalizing flow model serves
    as both an image encoder for perception tasks and an image decoder for image generation
    tasks during inference. JetFormer achieves text-to-image generation quality competitive
    with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
    image autoencoders, which are trained with a complex mixture of losses, including
    perceptual ones. At the same time, JetFormer demonstrates robust image understanding
    capabilities. To the best of our knowledge, JetFormer is the first model that
    is capable of generating high-fidelity images and producing strong log-likelihood
    bounds.
  arxivId: '2411.19722'
  authors: Michael Tschannen, André Susano Pinto, Alexander Kolesnikov
  created_at: '2024-12-15T22:16:44Z'
  issue_number: 40
  issue_url: https://github.com/dmarx/arxiv-archive/issues/40
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2411.19722
'2412.09621':
  abstract: 'Learning to understand dynamic 3D scenes from imagery is crucial for
    applications ranging from robotics to scene reconstruction. Yet, unlike other
    problems where large-scale supervised training has enabled rapid progress, directly
    supervising methods for recovering 3D motion remains challenging due to the fundamental
    difficulty of obtaining ground truth annotations. We present a system for mining
    high-quality 4D reconstructions from internet stereoscopic, wide-angle videos.
    Our system fuses and filters the outputs of camera pose estimation, stereo depth
    estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions.
    We use this method to generate large-scale data in the form of world-consistent,
    pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate
    the utility of this data by training a variant of DUSt3R to predict structure
    and 3D motion from real-world image pairs, showing that training on our reconstructed
    data enables generalization to diverse real-world scenes. Project page: https://stereo4d.github.io'
  arxivId: '2412.09621'
  authors: Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander
    Holynski
  created_at: '2024-12-16T08:58:27Z'
  issue_number: 52
  issue_url: https://github.com/dmarx/arxiv-archive/issues/52
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2412.09621v1
