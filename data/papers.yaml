'1711.11586':
  abstract: Many image-to-image translation problems are ambiguous, as a single inputimage
    may correspond to multiple possible outputs. In this work, we aim tomodel a \emph{distribution}
    of possible outputs in a conditional generativemodeling setting. The ambiguity
    of the mapping is distilled in alow-dimensional latent vector, which can be randomly
    sampled at test time. Agenerator learns to map the given input, combined with
    this latent code, to theoutput. We explicitly encourage the connection between
    output and the latentcode to be invertible. This helps prevent a many-to-one mapping
    from the latentcode to the output during training, also known as the problem of
    mode collapse,and produces more diverse results. We explore several variants of
    this approachby employing different training objectives, network architectures,
    and methodsof injecting the latent code. Our proposed method encourages bijectiveconsistency
    between the latent encoding and output modes. We present asystematic comparison
    of our method and other variants on both perceptualrealism and diversity.
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-15T17:02:34Z'
  issue_number: 28
  issue_url: https://github.com/dmarx/arxiv-archive/issues/28
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T17:02:34.023Z'
  title: Toward Multimodal Image-to-Image Translation
  url: https://arxiv.org/pdf/1711.11586
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    forgenerating high-dimensional data. In this paper, we show that the dynamics
    ofthese models exhibit a spontaneous symmetry breaking that divides thegenerative
    dynamics into two distinct phases: 1) A linear steady-state dynamicsaround a central
    fixed-point and 2) an attractor dynamics directed towards thedata manifold. These
    two "phases" are separated by the change in stability ofthe central fixed-point,
    with the resulting window of instability beingresponsible for the diversity of
    the generated samples. Using both theoreticaland empirical evidence, we show that
    an accurate simulation of the earlydynamics does not significantly contribute
    to the final generation, since earlyfluctuations are reverted to the central fixed
    point. To leverage this insight,we propose a Gaussian late initialization scheme,
    which significantly improvesmodel performance, achieving up to 3x FID improvements
    on fast samplers, whilealso increasing sample diversity (e.g., racial composition
    of generated CelebAimages). Our work offers a new way to understand the generative
    dynamics ofdiffusion models that has the potential to bring about higher performance
    andless biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-15T10:27:24Z'
  issue_number: 27
  issue_url: https://github.com/dmarx/arxiv-archive/issues/27
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T10:27:24.289Z'
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  url: https://arxiv.org/pdf/2305.19693
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and renderingtimes
    by orders of magnitude. While these methods demonstrate state-of-the-artquality
    and speed, they are designed for photogrammetry of static scenes and donot generalize
    well to freely moving humans in the environment. In this work,we introduce Human
    Gaussian Splats (HUGS) that represents an animatable humantogether with the scene
    using 3D Gaussian Splatting (3DGS). Our method takesonly a monocular video with
    a small number of (50-100) frames, and itautomatically learns to disentangle the
    static scene and a fully animatablehuman avatar within 30 minutes. We utilize
    the SMPL body model to initializethe human Gaussians. To capture details that
    are not modeled by SMPL (e.g.cloth, hairs), we allow the 3D Gaussians to deviate
    from the human body model.Utilizing 3D Gaussians for animated humans brings new
    challenges, including theartifacts created when articulating the Gaussians. We
    propose to jointlyoptimize the linear blend skinning weights to coordinate the
    movements ofindividual Gaussians during animation. Our approach enables novel-posesynthesis
    of human and novel view synthesis of both the human and the scene. Weachieve state-of-the-art
    rendering quality with a rendering speed of 60 FPSwhile being ~100x faster to
    train over previous work. Our code will beannounced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-15T17:03:00Z'
  issue_number: 29
  issue_url: https://github.com/dmarx/arxiv-archive/issues/29
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T17:02:59.876Z'
  title: 'HUGS: Human Gaussian Splats'
  url: https://arxiv.org/abs/2311.17910
'2312.00330':
  abstract: 'Text-to-video (T2V) models have shown remarkable capabilities in generating

    diverse videos. However, they struggle to produce user-desired stylized videos

    due to (i) text''s inherent clumsiness in expressing specific styles and (ii)

    the generally degraded style fidelity. To address these challenges, we

    introduce StyleCrafter, a generic method that enhances pre-trained T2V models

    with a style control adapter, enabling video generation in any style by

    providing a reference image. Considering the scarcity of stylized video

    datasets, we propose to first train a style control adapter using style-rich

    image datasets, then transfer the learned stylization ability to video

    generation through a tailor-made finetuning paradigm. To promote content-style

    disentanglement, we remove style descriptions from the text prompt and extract

    style information solely from the reference image using a decoupling learning

    strategy. Additionally, we design a scale-adaptive fusion module to balance the

    influences of text-based content features and image-based style features, which

    helps generalization across various text and style combinations. StyleCrafter

    efficiently generates high-quality stylized videos that align with the content

    of the texts and resemble the style of the reference images. Experiments

    demonstrate that our approach is more flexible and efficient than existing

    competitors.'
  arxivId: '2312.00330'
  authors: Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang,
    Xintao Wang, Yujiu Yang, Ying Shan
  created_at: '2024-12-15T21:53:26Z'
  issue_number: 39
  issue_url: https://github.com/dmarx/arxiv-archive/issues/39
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style\n \
    \ Adapter"
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2312.00330
'2405.20053':
  abstract: Pre-trained Language Models (LMs) exhibit strong zero-shot and in-contextlearning
    capabilities; however, their behaviors are often difficult to control.By utilizing
    Reinforcement Learning from Human Feedback (RLHF), it is possibleto fine-tune
    unsupervised LMs to follow instructions and produce outputs thatreflect human
    preferences. Despite its benefits, RLHF has been shown topotentially harm a language
    model's reasoning capabilities and introduceartifacts such as hallucinations where
    the model may fabricate facts. Toaddress this issue we introduce Direct Preference
    Heads (DPH), a fine-tuningframework that enables LMs to learn human preference
    signals through anauxiliary reward head without directly affecting the output
    distribution of thelanguage modeling head. We perform a theoretical analysis of
    our objectivefunction and find strong ties to Conservative Direct Preference Optimization(cDPO).
    Finally we evaluate our models on GLUE, RACE, and the GPT4Allevaluation suite
    and demonstrate that our method produces models which achievehigher scores than
    those fine-tuned with Supervised Fine-Tuning (SFT) or DirectPreference Optimization
    (DPO) alone.
  arxivId: '2405.20053'
  authors: Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic
  created_at: '2024-12-15T17:25:18Z'
  issue_number: 31
  issue_url: https://github.com/dmarx/arxiv-archive/issues/31
  labels:
  - paper
  - rating:novote
  rating: novote
  state: open
  timestamp: '2024-12-15T17:25:17.781Z'
  title: "Would I Lie To You? Inference Time Alignment of Language Models using\n\
    \  Direct Preference Heads"
  url: https://arxiv.org/abs/2405.20053
'2410.01131':
  arxivId: '2410.01131'
  created_at: '2024-12-15T17:43:45Z'
  duration_minutes: 1
  duration_ms: 40005
  issue_number: 35
  issue_url: https://github.com/dmarx/arxiv-archive/issues/35
  labels:
  - reading-session
  - paper:2410.01131
  last_read: '2024-12-15T17:43:44.983Z'
  paper_title: "nGPT: Normalized Transformer with Representation Learning on the\n\
    \  Hypersphere"
  paper_url: https://arxiv.org/pdf/2410.01131
  state: open
  timestamp: '2024-12-15T17:43:44.983Z'
  total_reading_time_minutes: 1
  type: reading_session
'2411.19722':
  abstract: 'Removing modeling constraints and unifying architectures across domains
    has

    been a key driver of the recent progress in training large multimodal models.

    However, most of these models still rely on many separately trained components

    such as modality-specific encoders and decoders. In this work, we further

    streamline joint generative modeling of images and text. We propose an

    autoregressive decoder-only transformer - JetFormer - which is trained to

    directly maximize the likelihood of raw data, without relying on any separately

    pretrained components, and can understand and generate both text and images.

    Specifically, we leverage a normalizing flow model to obtain a soft-token image

    representation that is jointly trained with an autoregressive multimodal

    transformer. The normalizing flow model serves as both an image encoder for

    perception tasks and an image decoder for image generation tasks during

    inference. JetFormer achieves text-to-image generation quality competitive with

    recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained

    image autoencoders, which are trained with a complex mixture of losses,

    including perceptual ones. At the same time, JetFormer demonstrates robust

    image understanding capabilities. To the best of our knowledge, JetFormer is

    the first model that is capable of generating high-fidelity images and

    producing strong log-likelihood bounds.'
  arxivId: '2411.19722'
  authors: Michael Tschannen, André Susano Pinto, Alexander Kolesnikov
  created_at: '2024-12-15T22:16:44Z'
  issue_number: 40
  issue_url: https://github.com/dmarx/arxiv-archive/issues/40
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2411.19722
