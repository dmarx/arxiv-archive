---
abstract: |
  The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce *DecorateLM*, a <u>d</u>ata <u>e</u>ngineering method designed to refine the pretraining <u>co</u>rpus through data <u>ra</u>ting, <u>t</u>agging and <u>e</u>diting. Specifically, *DecorateLM* rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for *DecorateLM* using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply *DecorateLM* to enhance 100 billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus.
author:
- |
  **Ranchi Zhao<sup>1</sup>[^1]**, **Zhen Leng Thai<sup>2</sup>**, **Yifan Zhang<sup>1</sup>**, **Shengding Hu<sup>2</sup>**,  
  **Yunqi Ba<sup>1</sup>**, **Jie Zhou<sup>1</sup>**, **Jie Cai<sup>1</sup>**, **Zhiyuan Liu<sup>2</sup>[^2]**, **Maosong Sun<sup>2</sup>**,  
  <sup>1</sup>Modelbest Inc, <sup>2</sup>Department of Computer Science and Technology, Tsinghua University  
  {ranchizhao,thaizhenleng123,yifanzhang634,shengdinghu}@gmail.com
bibliography:
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models"
---





# Introduction

The advent of Large Language Models (LLMs) has ushered in transformative changes across various domains of artificial intelligence , from natural language processing to complex task execution . The backbone of these models’ effectiveness lies in their training processes, specifically in the quality and composition of their pre-training corpora . Traditionally, LLMs are pre-trained on vast datasets composed of billions of tokens harvested from diverse text sources.

Data quality is of vital importance for training LLM . However, acquiring high-quality data is a formidable challenge due to the sheer volume and unstructured nature of it.

The reliance on large-scale unsupervised data leads to the inclusion of numerous low-quality texts within the training data. This infusion of poor-quality data can adversely affect the models’ learning processes, resulting in performance deficiencies and limitations in their applicability. However, the existing methods for curating and enhancing the quality of such datasets are often inadequate. They typically lack the capacity to scale to the size required while maintaining or improving data quality, primarily due to the absence of fine-grained annotations and the impracticality of manual oversight.

Addressing these challenges requires innovative approaches that can scale with the data requirements of LLMs while ensuring enhancements in data quality. This paper introduces *DecorateLM*, a comprehensive <u>d</u>ata <u>e</u>ngineering methodology designed to refine the pretraining <u>co</u>rpus through a systematic "decorating" process. The term "decorating" in this context refers to a series of processes aimed at enriching the data with additional metadata, improving its structure, and ensuring its relevance and quality.

*DecorateLM* employs a three-phase strategy to accomplish these goals. The first phase, <u>ra</u>ting, involves evaluating texts against a predefined set of quality criteria. These criteria are designed to assess the educational value, expertise, fact and trivia, reasoning level, scarcity, structural format, story-likeness and subjectivity of texts. The second phase, <u>t</u>agging, categorizes the texts using a hierarchical label system that reflects the content of the data. This labeling enhances data management and retrieval efficiency, a key aspect of iterative training processes. The final phase, <u>e</u>diting, involves revising and standardizing texts to meet higher linguistic standards of formality and clarity.

To implement this methodology effectively, we curate a specialized training corpus using pre-trained LLMs to preprocess and initially rate potential data samples. This approach leverages the model’s capabilities to perform initial assessments at scale. We then distill our data engineering expertise into a small language model (SLM)—which is optimized for more detailed and nuanced data processing tasks. We name this SLM as the *DecorateLM*. Using *DecorateLM*, we enhance 100 billion tokens from our initial datasets, selecting 45 billion tokens that exhibit optimal quality and diversity. These tokens are subsequently used to train LM to demonstrate DecorateLM’s effectiveness.

The results from our study underscore the substantial benefits of using high-quality, well-curated data in training LLMs. Not only do these results demonstrate improved model performance, but they also suggest that *DecorateLM* offers a scalable and effective solution to one of the most pressing issues in modern AI—enhancing the quality of training datasets amid expanding data requirements.

# Related Work

In recent years, the quality and selection of data for training language models receive considerable attention. Researchers propose various methodologies to assess, select, and improve high-quality data, with the goal of enhancing both the performance and efficiency of models .

**Data Annotation and Rating**. QuRating, DEITA, and ALPAGASUS are employed for data annotation, each utilizing distinct methodologies to enhance training via refined rating scores . Phi-1 and MoDS use GPT-4 and DeBERTa to improve educational data and precise data selection, accelerating learning and fine-tuning .

**Domain Diversity in Data**. INSTAG introduces a detailed tagging system for diverse SFT data, improving MT-Bench scores with less data . Phi-1.5 extends Phi-1 by adding synthetic data across multiple domains in a textbook style .

**Data Optimization for Model Training**. Studies show that models can perform well with smaller datasets and less computing. WRAP maintains performance with fewer resources on the C4 dataset, and TinyStories uses simple vocabulary for quicker learning . Additionally, Phi-3 uses a two-stage training with web and synthetic data to improve reasoning and specialized skills .

# Method

<figure id="fig:spearmanscore">
<span class="image placeholder" data-original-image-src="figs/spearmanscore.pdf" data-original-image-title="" width="\columnwidth"></span>
<figcaption>Spearman correlation coefficients between various rating criteria. The correlations align with intuitive expectations. For instance, data with higher educational value often exhibits enhanced reasoning levels, which, in turn, enhances their comprehensibility. </figcaption>
</figure>

## Framework

In this section, we detail the methodology of DecorateLM, which is designed for sample-level annotation and enhancement. The framework of DecorateLM consists of three distinct phases: rating, tagging, and editing. During the rating phase, DecorateLM assigns numeric scores to a text based on predefined quality dimensions. In the tagging phase, DecorateLM predicts hierarchical tags at three levels for the text. In the editing phase, DecorateLM rephrases the text to present alternative narratives, thereby facilitating the model’s acquisition of core knowledge from varied perspectives.

The training pipeline of DecorateLM incorporates both a teacher model and a student model. The teacher model, which is larger, excels in processing detailed instructions related to text quality. However, its slower processing speed limits its practicality for annotating or editing extensive pretraining corpora. To address this, knowledge from the teacher model is distilled into a more compact student model to enhance efficiency. Distinct distillation strategies are employed for each of the three phases. The rating and tagging phases, which involve processing the entire raw corpus and generating concise annotations, exhibit similar input-output dynamics. Consequently, DecorateLM is configured to manage these two phases concurrently to optimize efficiency, instead of leveraging two separate models. For the editing phase, a separate distillation process is implemented to distill the knowledge required for effective rephrasing into another model of DecorateLM.

## Rating

<figure id="fig:wordcloud">
<span class="image placeholder" data-original-image-src="figs/cloud.pdf" data-original-image-title="" width="\columnwidth"></span>
<figcaption>Word cloud of tags. The size of each tag is proportional to its frequency in the annotated dataset. Tags are color-coded based on their levels: first-level tags in dark blue, second-level tags in medium blue, and third-level tags in light blue.</figcaption>
</figure>

High-quality training data is crucial for developing powerful language models. However, the ideal properties that constitute an optimal training corpus remain challenging to characterize comprehensively. To achieve robust language understanding and generation capabilities, language models should be trained on high-quality data meticulously curated based on diverse criteria that capture the essential and abstract qualities of natural language texts.

#### Criteria.

| **Model**  | **First** | **Second** | **Third** |
|:-----------|:---------:|:----------:|:---------:|
| DecorateLM |   92.1    |    75.6    |   62.3    |
| GPT-4      |   93.6    |    77.3    |   68.5    |

Comparison of tagging accuracy between DecorateLM and GPT-4 across three hierarchical levels on the validation set. GPT-4, lacking prior knowledge of the designed tagging hierarchy, is provided with the relevant labels for each level through prompts in successive rounds of interaction.

To assess the quality of texts, we define eight evaluative criteria that quantitatively measure the contributions of a text to model training from multiple perspectives. For each criterion, data samples are assigned a quantitative score, enabling an objective evaluation across the various criteria.

1.  *Educational Value* evaluates whether the content is suitable for educational purposes, specifically its utility in textbooks. It assesses the clarity, detail, and comprehensibility of explanations and guiding principles.

2.  *Expertise* measures the depth of knowledge that content reflects, typically possessed by subject matter experts.

3.  *Fact&Trivia* focuses on the accuracy of factual information presented in the content, which does not necessarily require specialized expertise to understand.

4.  *Reasoning Level* assesses the necessity for high-level reasoning, sequential thought processes, or chain of thought  capabilities in the content.

5.  *Scarcity* targets accurate yet relatively unknown information that is typically familiar only to a select few due to its specialized, niche, or obscure nature.

6.  *Structural Format* evaluates the organization and structure of data, such as the use of numbered lists, bulleted lists, and markdown formatting.

7.  *Story-likeness* assesses whether the content narrates a story or describes a scenario.

8.  *Subjectivity* focuses on content with personal opinions and conversations.

#### Annotated Dataset Construction.

In alignment with the established criteria, we annotate a set of carefully selected samples using GPT-4 to form the annotated dataset. Considering the inaccuracy of LLMs in assigning precise quality scores , we adopt a pairwise comparison method. Inspired by QuRating , this work employs the Bradley-Terry (B-T) model  to derive preference probabilities from pairwise comparisons. All prompts used in the rating phase are displayed in Appendix . Subsequently, we normalize these probabilities by sorting them and applying a linear transformation to map them onto a uniform rating scale from 0 to 100, thereby establishing the final scores for each criterion.

#### Analysis.

Upon acquiring the meticulously curated annotated dataset, we proceed to train DecorateLM, with the training details provided in Appendix . A validation set is segregated prior to training. DecorateLM is employed to assign scores to each data sample. For a fair comparison, we also use GPT-4 to assign numeric scores to these samples. Then we compute the Spearman correlation coefficient between the model-provided scores and the ground truth annotation from the B-T model. As depicted in Figure , GPT-4, untrained for the rating task, demonstrates *inferior* scoring performance compared to DecorateLM.

In the analysis presented in Figure , we compute the Spearman correlation coefficients between various rating criteria. The results reveal a modest positive correlation across most pairs of criteria, indicating both the independence between different criteria and the commonality present among high-quality texts.

## Tagging

The quality of the pretraining corpus is initially assessed through rating criteria. However, these criteria alone are insufficient for ensuring diversity in the pretraining samples and for the fine-grained selection of data. Tagging pretraining data into a broad spectrum of topics and fields can ensure diversity within the training corpus. Furthermore, a structured tagging system facilitates the targeted enhancement of the model by incorporating data that address specific areas, consequently improving the model’s performance in particular domains. Next, we introduce our hierarchical tagging system.

#### Tags Design.

To systematically categorize the pretraining dataset, we first clearly define 21 primary categories that cover a wide range of human knowledge, from Natural Sciences to Social Events. We then expand this framework by engaging GPT-4, which serves as a human expert, in a two-step iterative dialogue process. The first dialogue iteration yields 255 second-level tags. For the third-level tags, we inform GPT-4 of each first-level category along with its corresponding second-level tags, prompting the model to generate a total of 793 specific third-level tags under the second-level categories. The details and prompts are in Appendix .

<figure id="fig:enter-label">
<span class="image placeholder" data-original-image-src="figs/dataset_1tag_dis.pdf" data-original-image-title="" width="\columnwidth"></span>
<figcaption>Distribution of first-level tags across different datasets, arranged in descending order by frequency in the decorated corpus.</figcaption>
</figure>

<figure id="fig:ppl">
<span class="image placeholder" data-original-image-src="figs/ppl.pdf" data-original-image-title="" width="0.8\columnwidth"></span>
<figcaption>Perplexity distribution of the corpus.</figcaption>
</figure>

#### Analysis.

We present the result of the tag tree in Figure  and the word cloud of the tag tree in Figure . To access the tag prediction performance, we manually re-annotated the existing validation split set with tags at the first, second, and third levels. We then compare the accuracy of DecorateLM and GPT-4 using this newly re-annotated validation set. As shown in Table , DecorateLM achieves performance comparable to that of GPT-4.

## Editing

The process of rating and tagging extracts valuable data from the pretraining corpus. Despite undergoing a rigorous cleaning pipeline, even high-quality data sourced from the internet may still retain some noise. Inspired by the work of , we propose to enhance the utilization of this high-quality data by rephrasing it based on the intrinsic attributes of the samples. By transforming the data into different verbal forms, we aim to preserve the core information diversity of the pertaining stage while being as clean as the SFT-stage dataset.

#### Annotated Dataset Construction.

We begin by selecting 10,000 data samples, each containing between 50 and 2048 tokens, to create a noisy dataset. We observe that this noisy dataset continues to exhibit issues such as unclear expressions, lack of natural language fluency, and mixed topics that are not fully resolved by standard cleaning methods. This noisy dataset is rephrased using GPT-4 based on prompts in Appendix .

#### Analysis.

Due to the absence of a comprehensive metric for evaluating rephrased text against the original text, we design several custom metrics and use human evaluation to quality-check the rephrased texts. For each evaluation metric, we compare the rephrased outputs of DecorateLM and GPT-4, with human annotators rating each output as a win, lose, or tie. The evaluation metrics are as follows: *Enhanced Clarity*, which determines the text’s increased conciseness and clearer expression; *Text Fluency*, which assesses the smoothness and readability of the text; *Term Precision*, which checks the retention of specialized terminology; *Logical Coherence*, which examines the consistency of causal and logical relationships within the text; *Information Precision*, which verifies that the original meaning, core information, and arguments are accurately preserved; *Information Completeness*, which ensures that no crucial information is missing from the text. The validation set size is 500. As presented in Figure , the editing model of DecorateLM, demonstrates satisfactory performance in this task.

<figure id="fig:editing human">
<span class="image placeholder" data-original-image-src="figs/winrate.pdf" data-original-image-title="" width="0.8\columnwidth"></span>
<figcaption>Human Preference for Edited Texts on Validation Set: DecorateLM vs. GPT-4.</figcaption>
</figure>

## The Final Decorated Corpus

After we train the DecorateLM on the curated annotated dataset, we proceed to decorate the pre-training corpus. Specifically, we select five large pre-training datasets including Common Crawl Chn (CC-CN), Dolma, C4, The Pile, and Baidu Wiki (BD-Wiki). Due to limited resources, we only sample a volume of 100 billion tokens from these datasets.

For the rated and tagged corpus, as shown in Figure , the English datasets, Dolma and The Pile, exhibit relatively high ratings and low cross-entropy, making them relatively ideal training corpora that are high-quality and well-balanced across domains. In contrast, the Chinese datasets, BD-wiki and CC-CN, exhibit lower ratings and higher cross-entropy, indicating shortcomings in overall quality and data distribution. This also underscores the necessity of using DecorateLM to improve the quality of the non-English corpus. For the tagging result alone, the analysis of the distribution of these datasets across the first-level labels is illustrated in Figure  . Regarding the effectiveness of editing on the Decorated Corpus, the original and edited texts are assessed using the perplexity metric with the CCNet model . The results, shown in Figure  , indicate a significant reduction in perplexity following the editing process. This improvement suggests that the editing effectively organizes the data in a manner that is more conducive to learning by models, ensuring enhanced comprehensibility and learnability.

<figure id="fig:baseline vs taggingv1 vs tagging mmlu">
<span class="image placeholder" data-original-image-src="figs/mmlu.pdf" data-original-image-title="" width="0.9\columnwidth"></span>
<figcaption>The performance of the MMLU-Tag. Model across the various subtasks of MMLU. The tasks where the sampling weights are increased on the corresponding tags based on the Tag. Methods are highlighted in red.</figcaption>
</figure>

# Experiments

In this section, we conduct data experiments to demonstrate the effectiveness of decorated corpus.

## Experiment Setup

We train the same SLM, MiniCPM-1.2B, used as the backbone for DecorateLM, aiming to improve its performance. MiniCPM-1.2B follows the multi-stage training pipeline . The stable training stage utilizes a constant learning rate until the decay stage, where the learning rate decreases rapidly. During the decay stage, the loss reduction accelerates significantly. This stage is deemed suitable for ablation studies on different data due to its substantial loss reduction and short training duration. We leverage the last checkpoint before the decay stage to reprocess the decay with both the raw and decorated corpora. Performance is evaluated against a wide range of publicly available benchmarks.

## Experiments on Rating

Given the rating of each test sample, we can select each sample with a probability determined by these ratings . We explore two sampling methods.

The first method, referred to as “Separate Criterion Sampling”, follows the approach proposed by . Specifically, each criterion is given a weight that represents its relative importance. The sampling method begins from the criterion with the highest weight to the lowest one. The transition between criteria happens when the sampled data from the dimension satisfies its predetermined corpus proportion. Within each criterion, data is sampled according to the following weight . The ratings for the $i$-th data point in $t$-th criterion are calculated using the following equation: $$\label{single dimension}
W_{i,t} = e^{\frac{\text{score}_{i,t} - \lambda}{\tau}},$$ where $i$ is the data point index and $t$ is the criterion index, both $\lambda$ and $\tau$ are set to 50.

The second method, called “Aggregate Criterion Sampling”, calculates the sampling weight $W_i$ for the $i$-th data as follows: $$W_i = \sum_{t=1}^{8} k_t \cdot e^{\frac{\text{score}_{t,i} - \mu_t}{\sigma_t}},$$ where the parameter $k_t$ represents the relative significance of each rating dimension.

For both Rat. (Sep.) with weights and Rat. (Agg.) with $k_t$, the main method assigns a weight of 0.2 to the dimensions of Educational Value, Expertise, Fact and Trivia, and Reasoning Level, while the four remaining dimensions are each assigned a weight of 0.05 according to the authors’ prior knowledge of the data quality.

In practice, we sample 58.5B tokens but only use 45B tokens among them as the high-quality data. This has a similar effect as increasing the temperature of sampling in .

## Experiments on Tagging

We enhance the diversity and balance of different domains by incorporating a sampling strategy among tags. Intuitively, a large domain should be undersampled and a rare domain should be upsampled. Specifically, we sample an instance with a hierarchical tag of $a \rightarrow b\rightarrow c$ with the weight of $$\begin{split}
% W_{\mathrm{I}=a, \mathrm{II}=b, \mathrm{III}=c} = 
W_{a, b, c} = 
& \frac{N_{\mathrm{I}=a}^\alpha}{\sum_{i=1}^{N_{\mathrm{I}}} N_{\mathrm{I}=i}^\alpha} \cdot \frac{N_{\mathrm{I}=a, \mathrm{II}=b}^\beta}{\sum_{i=1}^{N_{\mathrm{I}=a, \mathrm{II}}} N_{\mathrm{I}=a, \mathrm{II}=i}^\beta} \cdot \\
& \frac{N_{\mathrm{I}=a, \mathrm{II}=b, \mathrm{III}=c}^\gamma}{\sum_{i=1}^{N_{\mathrm{I}=a, \mathrm{II}=b, \mathrm{III}}} N_{\mathrm{I}=a, \mathrm{II}=b, \mathrm{III}=i}^\gamma} ,
% & \frac{1}{N_{\mathrm{I}=a, \mathrm{II}=b, \mathrm{III}=c}}
\end{split}
\label{eq: tag foumula}$$ where $N_{{X}={x}}$ represents the number of instance whose belong to tag $x$ at tag level $X$. The exponents $\alpha, \beta, \gamma$ are similar to what is suggested by  to tune the distribution to be smooth or concentrated.

For the combined method of Rat. (Agg) & Tag. , we calculate the sampling weights by multiplying the weights of Rat. (Sep.) and Tag..

**Domain Coverage Criterion (Avg. (DC))**. To demonstrate the improvements brought by making the domain more balanced through tagging, we construct a domain coverage criterion by averaging the accuracy scores of 6 tasks within the following 5 domains. *Sports* domain is represented by SportQA  dataset. *Medicine* domain is represented by MedMCQA  and MedQA-USMLE  datasets. *Law* domain is represented by JECQA  dataset. *Natural sciences* domain is represented by SciQ  dataset. *Finance* domain is represented by OpenFinData dataset[^3].

## Experiments on Editing

Building upon the existing methods (Baseline, Rat. (Agg.), and Rat. (Agg.)&Tag.), we introduce the Editing approach. We randomly select one-third of the training data to be replaced with edited data.

## Results

In this section, we present the results of data experiments. Details and specific settings of the evaluation experiments can be found in Appendix .

As shown in Table , the integration of various methods yields several significant insights:

- **Rating:** Both rating sampling methods exhibit superior overall performance compared to the baseline. Rat. (Agg.) improves almost all tasks and achieves an overall average score increase of 2.4 points, which is greater than Rat. (Sep.).

- **Tagging:** The Tag. method shows a slight improvement over the baseline in overall benchmarks and achieves a significant 4.3-point increase on the Domain Coverage benchmark. The Rat. (Agg) & Tag. method has comparable overall performance to Rat. (Agg), with an additional 2-point improvement on Avg.(DC). Moreover, to validate the effectiveness of domain filtering, we evaluate an MMLU-oriented tagging model, as depicted in Figure . The model targets 20 specific MMLU subtasks, enhancing their sampling probability. It demonstrates improvement in 15 of these 20 tasks compared to the Tag. method, thereby affirming the efficacy of the tagging system in modifying domain composition for targeted reinforcement.

- **Editing:** Integration of the Editing method significantly enhances model performance on downstream tasks. Edit. increases the average score by 2.3 percentage points compared to the baseline, demonstrating its effectiveness in rephrasing training data.

- **Rating and Editing:** Rat. (Agg.)&Edit. emerges as the best-performing method, enhancing the average score by 4.1 points relative to the baseline and demonstrating improvements across all tasks. Rat. (Agg.)&Tag.&Edit. attains the highest score on Avg. (DC) and maintains excellent performance in other tasks, suggesting that the integration of tagging with rating and editing expands the models’ knowledge base without substantially compromising depth.

# Conclusion

In this paper, we present *DecorateLM*, a <u>d</u>ata <u>e</u>ngineering method designed to refine the pretraining <u>co</u>rpus through data <u>ra</u>ting, <u>t</u>agging and <u>e</u>diting. *DecorateLM* employs a dual-training strategy, wherein two student models with 1.2 B parameters are trained: one designed for rating and tagging, and the other focused on editing. Our experiments show that introducing rating and editing in data corpus significantly enhances data quality by improving the overall performance of SLM on various existing benchmarks. Furthermore, our empirical study verifies that the implemented tagging strategy achieves a more balanced distribution of categories within the training dataset. This equilibrium in categorization enables a more thorough comprehension of SLM proficiency across diverse domains. These encouraging results underscore the importance of training data quality in fully exploiting the capabilities of Large Language Models, thereby suggesting several compelling avenues for future research.

# Limitations

Our study, while enhancing the quality of data effectively, is subject to several limitations. Firstly, the biases present in GPT-4 may be reflected in the fine-tuning data used for DecorateLM, potentially causing DecorateLM to inherit these biases Additionally, due to computational and time constraints, we limit our model training to 1.2 billion parameter models using high-quality data. The generalizability of our findings would benefit from replication with larger language models and a wider range of datasets. Thirdly, our investigation is confined to training models during the decay stage using the Decorated Corpus. An additional dimension to our work would involve creating a dataset of 1.1 trillion tokens with DecorateLM, followed by training a model from scratch on this enlarged dataset, which we believe represents an important direction for future research.

Moreover, although DecorateLM performs well in filtering data from large-scale web data, its ability to handle more specialized domains still requires improvement. The classification and labeling of the diverse content of the real world by humans are challenging to fully capture with a three-layer labeling system. Future research could explore a more granular labeling system to enhance the model’s precision and breadth in professional fields. Lastly, while DecorateLM considered both English and Chinese, it did not take other languages such as French and Russian into account, which may limit its generalizability to other languages.

An additional limitation lies in the current approach to sampling, which may not adequately capture the nuanced relationships between ratings and taggings across various tasks. Therefore, future research should explore a wider array of sampling strategies for rating and tagging to assess their impact on task performance more comprehensively.

# Ethical Considerations

As we develop DecorateLM, we recognize the inherent risk of introducing or magnifying biases within our datasets. The training process, while intended to refine and improve data accuracy, could inadvertently perpetuate biases present in the original data. This raises significant ethical concerns, as biased data can lead to unfair outcomes in decision-making processes that rely on our enhanced training data.

# Full Prompts

## Prompts of Rating

## Prompts of Tagging

## Prompts of Editing

# DecorateLM Training

## Details of rating and tagging model

We employ MiniCPM-1.2B  as our base model. Utilizing the previously proposed rating and tagging methodologies, we collect rating and three-level tagging of 30,000 training data samples and subsequently apply supervised fine-tuning to the MiniCPM-1.2B with a learning rate of 0.00125 and total batch size of 480 every iteration. The fine-tuning process is conducted on three machines, each equipped with eight Nvidia A100 GPUs. We implement an decay step every 120 iterations and a warm-up phase of 3 iterations, yielding distilled rating and tagging models. We observe that only 200 steps are needed to fine-tune the model to its optimal performance in rating and tagging.

## Details of editing model

Similar to the rating and tagging model, we utilize the previously proposed editing method and collect 10,000 data samples with rephrased content by GPT-4. Subsequently, we apply supervised fine-tuning to MiniCPM-1.2B with the same method and hyperparameters as the rating and tagging model, yielding an editing model. We observe that fine-tuning the model for optimal performance in editing tasks requires 600 steps, a notably higher number compared to the steps needed for the rating and tagging model. This increased demand for training iterations likely reflects the greater complexity and difficulty associated with editing tasks.

# Further Analysis of DecorateLM

## Cost Analysis

Utilizing the vLLM framework  and Ray , we facilitate the generation of synthetic data across distinct phases with varying processing efficiencies on a single Nvidia A100 GPU. In the rating and tagging phase, the MiniCPM-1.2B model processes 16 million tokens per hour, requiring approximately 6,250 GPU hours to generate 100 billion tokens. Conversely, in the editing phase, the same model configuration processes 12.5 million tokens per hour, necessitating around 8,000 GPU hours for the production of an equivalent volume of tokens.

## Details of Decorated Corpus

The Decorated Corpus is constructed from a variety of datasets, each contributing to the total composition according to the proportions specified in Table .

#### Dolma.

Dolma dataset  encompasses a comprehensive corpus designed for advancing the field of language model pretraining.

#### CC-CN.

CC-CN dataset is composed of a combination of sources from , , and 

#### C4.

C4 dataset  represents a significant milestone in the field of natural language processing, particularly within the domain of transfer learning.

#### The Pile.

The Pile dataset  is a substantial contribution to large-scale language model training, featuring an extensive corpus of 825 GiB of English text.

#### BD Wiki.

The BD Wiki dataset, derived from the Baidu Baike[^4], is a semi-open Chinese online encyclopedia operated by Baidu Inc.

# Training With Decorated Corpus

## Experimental Details

We employ the pre-decay version of MiniCPM-1.2B, pre-trained on a corpus comprising 800 billion tokens, as our base model. For training, the Decorated Corpus and additional high-quality datasets are utilized. The base model undergoes a decay process over 20,000 steps with a learning rate of 0.01 and a batch size of 1200 tokens per iteration, distributed across 10 machines, each equipped with eight A100-80GB GPUs. A decay step is implemented every 5000 iterations.

## Evaluation Details

The overall evaluation utilizes the open-source tool UltraEval[^5]. The underlying inference and acceleration use the open-source framework vLLM , and the dataset includes commonly used datasets: C-Eval  and CMMLU  for Chinese knowledge, AGI-Eval  for World Knowledge, MMLU  for English knowledge, HumanEval  and MBPP  for coding, GSM8K  and MATH  for mathematics, and BBH  for logic reasoning, and ARC-E , ARC-C for commonsense reasoning, and TriviaQA  for Reading Comprehension. Additionally, we conduct the Domain Coverage (DC) benchmark to evaluate the model’s capability across various domain-specific knowledge bases. The DC Benchmark includes datasets such as SportQA  for sports, MedMCQA  and MedQA-USMLE  for medicine, JECQA  for law, SciQ  for natural sciences, and OpenFinData[^6] for finance.

# Inspecting cases of DecorateLM

[^1]: Equal contribution.

[^2]: Corresponding author.

[^3]: <https://github.com/open-compass/OpenFinData>

[^4]: <https://baike.baidu.com/>

[^5]: <https://ultraeval.openbmb.cn/home>

[^6]: <https://github.com/open-compass/OpenFinData>
