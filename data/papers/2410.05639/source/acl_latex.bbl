\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{arXiv preprint arXiv:2404.14219}.

\bibitem[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}.

\bibitem[{Bradley and Terry(1952)}]{bradley1952rank}
Ralph~Allan Bradley and Milton~E Terry. 1952.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39(3/4):324--345.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:1877--1901.

\bibitem[{Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang et~al.}]{chen2023alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al. 2023.
\newblock Alpagasus: Training a better alpaca with fewer data.
\newblock \emph{arXiv preprint arXiv:2307.08701}.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}.

\bibitem[{Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2023.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24(240):1--113.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{Du et~al.(2023)Du, Zong, and Zhang}]{du2023mods}
Qianlong Du, Chengqing Zong, and Jiajun Zhang. 2023.
\newblock Mods: Model-oriented data selection for instruction tuning.
\newblock \emph{arXiv preprint arXiv:2311.15653}.

\bibitem[{Elazar et~al.(2023)Elazar, Bhagia, Magnusson, Ravichander, Schwenk, Suhr, Walsh, Groeneveld, Soldaini, Singh et~al.}]{elazar2023s}
Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et~al. 2023.
\newblock What's in my big data?
\newblock \emph{arXiv preprint arXiv:2310.20707}.

\bibitem[{Eldan and Li(2023)}]{eldan2023tinystories}
Ronen Eldan and Yuanzhi Li. 2023.
\newblock Tinystories: How small can language models be and still speak coherent english?
\newblock \emph{arXiv preprint arXiv:2305.07759}.

\bibitem[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima et~al.}]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}.

\bibitem[{Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi et~al.}]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, et~al. 2023.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}.

\bibitem[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{arXiv preprint arXiv:2103.03874}.

\bibitem[{Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao et~al.}]{hu2024minicpm}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al. 2024.
\newblock Minicpm: Unveiling the potential of small language models with scalable training strategies.
\newblock \emph{arXiv preprint arXiv:2404.06395}.

\bibitem[{Huang et~al.(2024)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Fu et~al.}]{huang2024c}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et~al. 2024.
\newblock C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Jin et~al.(2021)Jin, Pan, Oufattole, Weng, Fang, and Szolovits}]{jin2021disease}
Di~Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021.
\newblock What disease does this patient have? a large-scale open domain question answering dataset from medical exams.
\newblock \emph{Applied Sciences}, 11(14):6421.

\bibitem[{Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer}]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer. 2017.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock \emph{arXiv preprint arXiv:1705.03551}.

\bibitem[{Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica}]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems Principles}, pages 611--626.

\bibitem[{Lample and Conneau(2019)}]{lample2019cross}
Guillaume Lample and Alexis Conneau. 2019.
\newblock Cross-lingual language model pretraining.
\newblock \emph{arXiv preprint arXiv:1901.07291}.

\bibitem[{Le~Scao et~al.(2023)Le~Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}]{le2023bloom}
Teven Le~Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al. 2023.
\newblock Bloom: A 176b-parameter open-access multilingual language model.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin}]{li2023cmmlu}
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023{\natexlab{a}}.
\newblock Cmmlu: Measuring massive multitask language understanding in chinese.
\newblock \emph{arXiv preprint arXiv:2306.09212}.

\bibitem[{Li et~al.(2024)Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora et~al.}]{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al. 2024.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock \emph{arXiv preprint arXiv:2406.11794}.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023{\natexlab{b}}.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock \emph{arXiv preprint arXiv:2309.05463}.

\bibitem[{Liu et~al.(2023)Liu, Zeng, He, Jiang, and He}]{liu2023makes}
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023.
\newblock What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
\newblock \emph{arXiv preprint arXiv:2312.15685}.

\bibitem[{Longpre et~al.(2023)Longpre, Yauney, Reif, Lee, Roberts, Zoph, Zhou, Wei, Robinson, Mimno et~al.}]{longpre2023pretrainer}
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et~al. 2023.
\newblock A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity.
\newblock \emph{arXiv preprint arXiv:2305.13169}.

\bibitem[{Lu et~al.(2023)Lu, Yuan, Yuan, Lin, Lin, Tan, Zhou, and Zhou}]{lu2023instag}
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023.
\newblock \# instag: Instruction tagging for analyzing supervised fine-tuning of large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Maini et~al.(2024)Maini, Seto, Bai, Grangier, Zhang, and Jaitly}]{maini2024rephrasing}
Pratyush Maini, Skyler Seto, He~Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. 2024.
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock \emph{arXiv preprint arXiv:2401.16380}.

\bibitem[{Moritz et~al.(2018)Moritz, Nishihara, Wang, Tumanov, Liaw, Liang, Elibol, Yang, Paul, Jordan et~al.}]{moritz2018ray}
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael~I Jordan, et~al. 2018.
\newblock Ray: A distributed framework for emerging $\{$AI$\}$ applications.
\newblock In \emph{13th USENIX symposium on operating systems design and implementation (OSDI 18)}, pages 561--577.

\bibitem[{Pal et~al.(2022)Pal, Umapathi, and Sankarasubbu}]{pal2022medmcqa}
Ankit Pal, Logesh~Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.
\newblock Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering.
\newblock In \emph{Conference on health, inference, and learning}, pages 248--260. PMLR.

\bibitem[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay}]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock \emph{arXiv preprint arXiv:2306.01116}.

\bibitem[{Qian et~al.(2023)Qian, Cong, Yang, Chen, Su, Xu, Liu, and Sun}]{qian2023communicative}
Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023.
\newblock Communicative agents for software development.
\newblock \emph{arXiv preprint arXiv:2307.07924}.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21(140):1--67.

\bibitem[{Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar et~al.}]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al. 2024.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}.

\bibitem[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou et~al.}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al. 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:24824--24837.

\bibitem[{Wei et~al.(2023)Wei, Zhao, Zhang, Zhu, Wang, Yang, Li, Cheng, Lü, Hu, Li, Yang, Luo, Wu, Liu, Cheng, Cheng, Zhang, Zhang, Lin, Wang, Ma, Dong, Sun, Chen, Peng, Liang, Yan, Fang, and Zhou}]{wei2023skywork}
Tianwen Wei, Liang Zhao, Lichang Zhang, Bo~Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou. 2023.
\newblock \href {https://arxiv.org/abs/2310.19341} {Skywork: A more open bilingual foundation model}.
\newblock \emph{Preprint}, arXiv:2310.19341.

\bibitem[{Welbl et~al.(2017)Welbl, Liu, and Gardner}]{welbl2017crowdsourcing}
Johannes Welbl, Nelson~F Liu, and Matt Gardner. 2017.
\newblock Crowdsourcing multiple choice science questions.
\newblock \emph{arXiv preprint arXiv:1707.06209}.

\bibitem[{Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave}]{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave. 2019.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock \emph{arXiv preprint arXiv:1911.00359}.

\bibitem[{Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen}]{wettig2024qurating}
Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024.
\newblock Qurating: Selecting high-quality data for training language models.
\newblock \emph{arXiv preprint arXiv:2402.09739}.

\bibitem[{Wu et~al.(2021)Wu, Zhao, Yu, Zhang, Shen, Liu, Li, Zhu, Luo, Xu et~al.}]{wu2021yuan}
Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, et~al. 2021.
\newblock Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning.
\newblock \emph{arXiv preprint arXiv:2110.04725}.

\bibitem[{Xia et~al.(2024)Xia, Yang, Wang, Tracy, Zhao, Huang, Chen, Zhu, Wang, and Shen}]{xia2024sportqa}
Haotian Xia, Zhengbang Yang, Yuqing Wang, Rhys Tracy, Yun Zhao, Dongdong Huang, Zezhi Chen, Yan Zhu, Yuan-fang Wang, and Weining Shen. 2024.
\newblock Sportqa: A benchmark for sports understanding in large language models.
\newblock \emph{arXiv preprint arXiv:2402.15862}.

\bibitem[{Xie et~al.(2023)Xie, Santurkar, Ma, and Liang}]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang. 2023.
\newblock Data selection for language models via importance resampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:34201--34227.

\bibitem[{Xu et~al.(2020)Xu, Zhang, and Dong}]{CLUECorpus2020}
Liang Xu, Xuanwei Zhang, and Qianqian Dong. 2020.
\newblock Cluecorpus2020: A large-scale chinese corpus for pre-training language model.
\newblock \emph{ArXiv}, abs/2003.01355.

\bibitem[{Zheng et~al.(2024)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing et~al.}]{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2024.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Zhong et~al.(2020)Zhong, Xiao, Tu, Zhang, Liu, and Sun}]{zhong2020jec}
Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020.
\newblock Jec-qa: a legal-domain question answering dataset.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 9701--9708.

\bibitem[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan}]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023.
\newblock Agieval: A human-centric benchmark for evaluating foundation models.
\newblock \emph{arXiv preprint arXiv:2304.06364}.

\bibitem[{Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu et~al.}]{zhou2024lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al. 2024.
\newblock Lima: Less is more for alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\end{thebibliography}
