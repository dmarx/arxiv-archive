\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos]{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication.
\newblock \emph{arXiv preprint arXiv:2303.09540}, 2023.

\bibitem[Allen-Zhu and Li(2024)]{allenzhu2024physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 3.3, knowledge capacity scaling laws, 2024.

\bibitem[Ash et~al.(2021)Ash, Goel, Krishnamurthy, and Kakade]{ash2021gone}
Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade.
\newblock Gone fishing: Neural active learning with fisher embeddings.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 8927--8939, 2021.

\bibitem[Ash et~al.(2019)Ash, Zhang, Krishnamurthy, Langford, and Agarwal]{ash2019deep}
Jordan~T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal.
\newblock Deep batch active learning by diverse, uncertain gradient lower bounds.
\newblock \emph{arXiv preprint arXiv:1906.03671}, 2019.

\bibitem[{Bickford Smith} et~al.(2023){Bickford Smith}, Kirsch, Farquhar, Gal, Foster, and Rainforth]{bickfordsmith2023prediction}
Freddie {Bickford Smith}, Andreas Kirsch, Sebastian Farquhar, Yarin Gal, Adam Foster, and Tom Rainforth.
\newblock Prediction-oriented {Bayesian} active learning.
\newblock \emph{International Conference on Artificial Intelligence and Statistics}, 2023.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Casson(2023)]{casson2023transformerflops}
Adam Casson.
\newblock Transformer flops, 2023.
\newblock URL \url{https://adamcasson.com/posts/transformer-flops}.

\bibitem[Chang et~al.(2024)Chang, Wang, Wang, Wu, Yang, Zhu, Chen, Yi, Wang, Wang, et~al.]{chang2024survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology}, 15\penalty0 (3):\penalty0 1--45, 2024.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Computer(2023)]{together2023redpajama}
Together Computer.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Das and Kempe(2018)]{das2018}
Abhimanyu Das and David Kempe.
\newblock Approximate submodularity and its applications: Subset selection, sparse approximation and dictionary selection.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0 (3):\penalty0 1--34, 2018.
\newblock URL \url{http://jmlr.org/papers/v19/16-534.html}.

\bibitem[Elazar et~al.(2023)Elazar, Bhagia, Magnusson, Ravichander, Schwenk, Suhr, Walsh, Groeneveld, Soldaini, Singh, et~al.]{elazar2023s}
Yanai Elazar, Akshita Bhagia, Ian~Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan~Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et~al.
\newblock What's in my big data?
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Engstrom et~al.(2024)Engstrom, Feldmann, and Madry]{engstrom2024dsdm}
Logan Engstrom, Axel Feldmann, and Aleksander Madry.
\newblock Dsdm: Model-aware dataset selection with datamodels, 2024.

\bibitem[Evans et~al.(2023)Evans, Pathak, Merzic, Schwarz, Tanno, and Henaff]{evans2023bad}
Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier~J Henaff.
\newblock Bad students make great teachers: Active learning accelerates large-scale visual understanding.
\newblock \emph{arXiv preprint arXiv:2312.05328}, 2023.

\bibitem[Fang et~al.(2023)Fang, Jose, Jain, Schmidt, Toshev, and Shankar]{fang2023data}
Alex Fang, Albin~Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar.
\newblock Data filtering networks, 2023.

\bibitem[Fleuret(2023)]{fleuret2023little}
Fran{\c{c}}ois Fleuret.
\newblock The little book of deep learning.
\newblock \emph{A lovely concise introduction}, page 297, 2023.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Graves et~al.(2017)Graves, Bellemare, Menick, Munos, and Kavukcuoglu]{graves2017automated}
Alex Graves, Marc~G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu.
\newblock Automated curriculum learning for neural networks.
\newblock In \emph{international conference on machine learning}, pages 1311--1320. Pmlr, 2017.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu, Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik, Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Strubell, Subramani, Wortsman, Dasigi, Lambert, Richardson, Zettlemoyer, Dodge, Lo, Soldaini, Smith, and Hajishirzi]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi~Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah~A. Smith, and Hannaneh Hajishirzi.
\newblock Olmo: Accelerating the science of language models, 2024.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, Salim, Shah, Behl, Wang, Bubeck, Eldan, Kalai, Lee, and Li]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio César~Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat~Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need, 2023.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models, 2022.

\bibitem[Houlsby et~al.(2011)Houlsby, Husz{\'a}r, Ghahramani, and Lengyel]{houlsby2011bayesian}
Neil Houlsby, Ferenc Husz{\'a}r, Zoubin Ghahramani, and M{\'a}t{\'e} Lengyel.
\newblock Bayesian active learning for classification and preference learning.
\newblock \emph{stat}, 1050:\penalty0 24, 2011.

\bibitem[Ilyas et~al.(2022)Ilyas, Park, Engstrom, Leclerc, and Madry]{ilyas2022datamodels}
Andrew Ilyas, Sung~Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry.
\newblock Datamodels: Predicting predictions from training data.
\newblock \emph{arXiv preprint arXiv:2202.00622}, 2022.

\bibitem[Joulin et~al.(2017)Joulin, Grave, Bojanowski, and Mikolov]{joulin2017bag}
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.
\newblock Bag of tricks for efficient text classification.
\newblock In \emph{Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers}, pages 427--431. Association for Computational Linguistics, April 2017.

\bibitem[Kirsch(2023)]{kirsch2023a}
A~Kirsch.
\newblock \emph{Advanced deep active learning and data subset selection: unifying principles with information-theory intuitions}.
\newblock PhD thesis, University of Oxford, 2023.

\bibitem[Lee et~al.(2022)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch, and Carlini]{lee2022deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8424--8445, 2022.

\bibitem[Li et~al.(2024)Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora, et~al.]{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock \emph{arXiv preprint arXiv:2406.11794}, 2024.

\bibitem[Lin et~al.(2024)Lin, Gou, Gong, Liu, Shen, Xu, Lin, Yang, Jiao, Duan, and Chen]{lin2024rho1}
Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen.
\newblock Rho-1: Not all tokens are what you need, 2024.

\bibitem[Lindley(1956)]{lindley1956measure}
Dennis~V Lindley.
\newblock On a measure of the information provided by an experiment.
\newblock \emph{The Annals of Mathematical Statistics}, 27\penalty0 (4):\penalty0 986--1005, 1956.

\bibitem[Longpre et~al.(2023)Longpre, Yauney, Reif, Lee, Roberts, Zoph, Zhou, Wei, Robinson, Mimno, et~al.]{longpre2023pretrainer}
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et~al.
\newblock A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity.
\newblock \emph{arXiv preprint arXiv:2305.13169}, 2023.

\bibitem[Magnusson et~al.(2023)Magnusson, Bhagia, Hofmann, Soldaini, Jha, Tafjord, Schwenk, Walsh, Elazar, Lo, Groeneveld, Beltagy, Hajishirzi, Smith, Richardson, and Dodge]{magnusson2023paloma}
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya~Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan~Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz~Beltagy, Hannaneh Hajishirzi, Noah~A. Smith, Kyle Richardson, and Jesse Dodge.
\newblock Paloma: A benchmark for evaluating language model fit, 2023.

\bibitem[Meta(2024)]{meta2023llama3}
Meta.
\newblock Llama 3, 2024.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch, Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, et~al.]{mindermann2022prioritized}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot, Sebastian Farquhar, et~al.
\newblock Prioritized training on points that are learnable, worth learning, and not yet learnt.
\newblock In \emph{International Conference on Machine Learning}, pages 15630--15649. PMLR, 2022.

\bibitem[Moore and Lewis(2010)]{moore2010intelligent}
Robert~C Moore and William Lewis.
\newblock Intelligent selection of language model training data.
\newblock In \emph{Proceedings of the ACL 2010 conference short papers}, pages 220--224, 2010.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and Fisher]{Nemhauser1978AnAO}
George~L. Nemhauser, Laurence~A. Wolsey, and Marshall~L. Fisher.
\newblock An analysis of approximations for maximizing submodular set functions—i.
\newblock \emph{Mathematical Programming}, 14:\penalty0 265--294, 1978.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:206800425}.

\bibitem[Park et~al.(2023)Park, Georgiev, Ilyas, Leclerc, and Madry]{park2023trak}
Sung~Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry.
\newblock Trak: Attributing model behavior at scale.
\newblock \emph{arXiv preprint arXiv:2303.14186}, 2023.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock \emph{arXiv preprint arXiv:2306.01116}, 2023.

\bibitem[Penedo et~al.(2024)Penedo, Kydl{\'\i}{\v{c}}ek, Lozhkov, Mitchell, Raffel, Von~Werra, Wolf, et~al.]{penedo2024fineweb}
Guilherme Penedo, Hynek Kydl{\'\i}{\v{c}}ek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von~Werra, Thomas Wolf, et~al.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.
\newblock \emph{arXiv preprint arXiv:2406.17557}, 2024.

\bibitem[Pukelsheim(2006)]{pukelsheim2006optimal}
Friedrich Pukelsheim.
\newblock \emph{Optimal design of experiments}.
\newblock SIAM, 2006.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rainforth et~al.(2024)Rainforth, Foster, Ivanova, and Bickford~Smith]{rainforth2024modern}
Tom Rainforth, Adam Foster, Desi~R Ivanova, and Freddie Bickford~Smith.
\newblock Modern bayesian experimental design.
\newblock \emph{Statistical Science}, 39\penalty0 (1):\penalty0 100--114, 2024.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock \emph{arXiv preprint arXiv:1904.09728}, 2019.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25278--25294, 2022.

\bibitem[Sener and Savarese(2018)]{sener2018active}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: A core-set approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{cerebras2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}.
\newblock \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, 2023.
\newblock URL \url{https://huggingface.co/datasets/cerebras/SlimPajama-627B}.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning, Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, pages 1631--1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/D13-1170}.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and Morcos]{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data pruning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 19523--19536, 2022.

\bibitem[Tack et~al.(2024)Tack, Kim, Yu, Lee, Shin, and Schwarz]{tack2024learning}
Jihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, and Jonathan~Richard Schwarz.
\newblock Learning large-scale neural fields via context pruned meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant]{talmor2018commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge.
\newblock \emph{arXiv preprint arXiv:1811.00937}, 2018.

\bibitem[Tirumala et~al.(2024)Tirumala, Simig, Aghajanyan, and Morcos]{tirumala2024d4}
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
\newblock D4: Improving llm pretraining via document de-duplication and diversification.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, and Bowman]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language understanding systems.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Welbl et~al.(2017)Welbl, Liu, and Gardner]{welbl2017crowdsourcing}
Johannes Welbl, Nelson~F Liu, and Matt Gardner.
\newblock Crowdsourcing multiple choice science questions.
\newblock \emph{arXiv preprint arXiv:1707.06209}, 2017.

\bibitem[Wenzek et~al.(2020)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave]{wenzek2020ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and {\'E}douard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock In \emph{Proceedings of the Twelfth Language Resources and Evaluation Conference}, pages 4003--4012, 2020.

\bibitem[Wortsman et~al.(2024)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, Pennington, Sohl-Dickstein, Xu, Lee, Gilmer, and Kornblith]{wortsman2024smallscale}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie~E Everett, Alexander~A Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Xie et~al.(2023)Xie, Santurkar, Ma, and Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 34201--34227, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\end{thebibliography}
