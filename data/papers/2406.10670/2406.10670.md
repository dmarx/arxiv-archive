---
abstract: |
  Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.

  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.

  Code: <https://github.com/davidbrandfonbrener/color-filter-olmo>

  Filtered data: <https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4>
author:
- |
  David Brandfonbrener  
  Kempner Institute at Harvard University Hanlin Zhang  
  Harvard University Andreas Kirsch  
  University of Oxford Jonathan Richard Schwarz  
  Harvard University Sham Kakade  
  Kempner Institute at Harvard University
bibliography:
- references.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  CoLoR-Filter: Conditional Loss Reduction Filtering  
  for Targeted Language Model Pre-training
---





# Introduction

<figure id="fig:1b">
<p><span class="image placeholder" data-original-image-src="images/1b.pdf" data-original-image-title="" width="\textwidth">image</span> </p>
<figcaption>Learning curves for 1.2 billion parameter language models trained on data selected by CoLoR-Filter using smaller 150 million parameter auxiliary models for two different target distributions. (Left) We target and evaluate loss on Books, lower is better. (Right) We target and evaluate accuracy on a suite of 8 downstream tasks from <span class="citation" data-cites="groeneveld2024olmo"></span>, higher is better. In both cases, test data is held out from the data used by CoLoR-Filter to guide selection. <span class="math inline">\(\tau\)</span> is the subset size multiplier denoting the number of examples considered for each selected data point. The CoLoR-Filter line terminates when we run out of data in C4 (<span class="math inline">\(\approx\)</span>175b possible tokens).</figcaption>
</figure>

The content of the data that a language model is trained on can have profound effects on its performance and the efficiency of the training process . But it remains an open research question how to decide which data to include in the training set. In this paper, we analyze a family of loss-based approaches for targeted selection of pre-training data, propose a simple approach that outperforms existing methods, and provide some preliminary evidence of favorable scaling properties.

To formulate the data selection problem, we first need to specify an objective that quantifies whether the selected data is good. Defining this objective requires evaluating a pre-trained language model, which is an area of active research . For this paper, we will take the goal to be to maximize performance on a set of downstream tasks. Since the preferred metrics on a given set of tasks are not necessarily the same nor amenable to direct optimization, we consider the likelihood of sequences sampled from the downstream tasks as a proxy objective. With this objective, we now have a straightforward goal: given a very large corpus of sequences and a small amount of high-quality data from a set of downstream tasks, we want to select a subset from the corpus so that training on the selected data maximizes likelihood on the downstream tasks. Then we can also test performance on the tasks under their preferred metrics.

From this objective, we derive an algorithm dubbed CoLoR-Filter (Conditional Loss Reduction Filtering). In we derive this method by applying Bayes’ rule and approximate empirical Bayes to the downstream likelihood objective. The resulting method is simple and intuitive: each sequence is scored by the difference in likelihood between a “prior” model and a “conditional” model that results from fine-tuning the prior model on the downstream data. Sequences that are more likely under the fine-tuned model are good. We also compare this algorithm to prior work (e.g., ) and discuss computational costs.

To evaluate our method, we consider two tasks. First, in , we consider a semi-synthetic task where the downstream task is language modeling on Books. Given access to C4 as potential pre-training data and a small (25 million tokens) sample of data from Books, we use CoLoR-Filter and a variety of baselines to select 3 billion tokens. We find that data selected by CoLoR-Filter can substantially outperform models trained on 8x as much randomly chosen data. Second, in , we consider a suite of 8 downstream multiple-choice tasks from . As downstream data we take the training sets of the tasks, but we evaluate accuracy on the held-out test sets. We again find that selecting with CoLoR-Filter outperforms training on 8x as much randomly selected data. Moreover, in both tasks, performance scales smoothly with the hyperparameter $\tau$ that governs how aggressively we select the data, suggesting that further scaling would yield further improvements.

In addition to finding that CoLoR-Filter can select good subsets of data, we also consider the computational cost of the selection procedure itself. CoLoR-Filter only requires running inference of the two auxiliary models to select data. This is computationally beneficial compared to online methods like RHOLoss since inference is cheaper than training and is entirely parallelizable. To maximize the computational benefits we also show that data selected with a small (150 million parameter) model can be transferred to a larger (1.2 billion parameter) model. Results are shown in , showing substantial efficiency improvements.

# Setting and Derivations

Assume that we are given a large pre-training dataset $D_{\text{train}}$, a small downstream dataset $D_{\text{down}}$ from the downstream task(s) of interest, and a “prior” dataset $D_{\text{prior}}$ we can use as prior knowledge (in practice we often just sample from $D_{\text{train}}$). We will assume for all practical purposes that $D_{\text{train}}$ is infinite and training proceeds in the “online” or “single pass” setting where we do not repeat data points. Our goal is to choose a subset $S \subset D_{\text{train}}$ of a fixed size $|S| = n$ that minimizes the downstream loss (maximizes the downstream likelihood).

This section introduces our CoLoR-Filter algorithm, inspired by and building upon the RHOLoss approach from prior work  . We also discuss related algorithms applicable to this setting such as DSIR and DSDM . Additional related work is discussed further in .

## Bayesian Data Selection

Our objective can be formulated as a Bayesian optimization problem, where the goal is to select a set $S$ so as to maximize the posterior probability of $D_{\text{down}}$, i.e. $$\begin{aligned}
    \min_{S \subset D_{\text{train}}, |S| = n} -\log \Pr (D_{\text{down}}| S),
\end{aligned}$$ where $\Pr (D_{\text{down}}| S)$ is the posterior probability. Applying Bayes rule we get: $$\begin{aligned}
    \min_{S \subset D_{\text{train}}, |S| = n} -\log \Pr(S| D_{\text{down}}) + \log \Pr(S) - \log \Pr(D_{\text{down}})
\end{aligned}$$ Note that the last term does not depend on $S$, so it can be ignored when optimizing over $S$. Introducing a prior over model parameters $\theta$, we get: $$\begin{aligned}
\label{eq:bayes}
     \min_{S \subset D_{\text{train}}, |S| = n} \underbrace{-\log \int_\theta\Pr(S| \theta)\Pr(\theta | D_{\text{down}})}_{\text{``conditional''}} + \underbrace{\log \int_\theta \Pr(S | \theta)\Pr(\theta)}_{\text{``marginal''}}
\end{aligned}$$ We will refer to the two terms as the conditional and marginal terms, respectively.[^1] Note that the conditional and marginal terms together make up the negative pointwise mutual information between the selected and downstream data, which has deep connections to prior work on active learning and active sampling .

## CoLoR-Filter

Given that we have access to prior knowledge from the dataset $D_{\text{prior}}$, we can replace the uninformed prior over $\theta$ with an empirical Bayes prior that conditions on $D_{\text{prior}}$ to obtain: $$\begin{aligned}
     \min_{S \subset D_{\text{train}}, |S| = n} -\log \int_\theta\Pr(S| \theta)\Pr(\theta | D_{\text{down}}, D_{\text{prior}}) + \log \int_\theta \Pr(S | \theta)\Pr(\theta| D_{\text{prior}})
\end{aligned}$$ As this integration is still intractable, we now make our main simplifying assumption which is to replace this integration over parameters by a point estimate: $$\begin{aligned}
\label{eq:prior}
     \approx \min_{S \subset D_{\text{train}}, |S| = n}  -\log \Pr(S | \theta_{\text{prior}+ \text{down}}) + \log \Pr(S | \theta_{\text{prior}}),
\end{aligned}$$ where $\theta_{\text{prior}}$ is a model trained on $D_{\text{prior}}$ and $\theta_{\text{prior}+ \text{down}}$ is a model trained on both $D_{\text{prior}}$ and $D_{\text{down}}$ (in practice, we use a model that is pre-trained on $D_{\text{prior}}$ fine-tuned on $D_{\text{down}}$).

Moreover, this approximation leads to computational benefits by avoiding the full combinatorial optimization of subset selection. In particular, once we condition on a single model $\theta$, and assuming the distribution over points $x \in S$ is independent, i.e. $\Pr(S|\theta) = \prod_{x \in S} \Pr(x|\theta)$, we have: $$\begin{aligned}
    \min_{\{x_1,\dots, x_{n}\} \subset D_{\text{train}}}   -\log \prod_{i=1}^n \Pr(x_i| \theta_{\text{prior}+ \text{down}}) + \log \prod_{i=1}^n \Pr(x_i | \theta_{\text{prior}})
\end{aligned}$$ which simplifies to: $$\begin{aligned}
     \min_{\{x_1,\dots, x_{n}\} \subset D_{\text{train}}}  \sum_{i=1}^n -\log \Pr(x_i| \theta_{\text{prior}+ \text{down}}) -  (-\log \Pr(x_i | \theta_{\text{prior}}))
\end{aligned}$$ This gives our CoLoR-Filter criteria that we use to select data. This optimization selects the points with the largest conditional loss reduction (CoLoR), i.e. the points where the negative log-likelihood loss of the conditional model $\theta_{\text{prior}+ \text{down}}$ is lower than the marginal model $\theta_{\text{prior}}$. Intuitively, this selects data points that are more likely under the conditional model than the marginal model.

#### A note on data diversity.

While the factorization that results from our point estimate of the parameters is computationally convenient, it makes an important simplifying assumption. In particular, the CoLoR-Filter objective no longer encourages the selection of a diverse dataset, as scores are applied independently to each point. In practice, this is remedied by a few considerations: (1) we can run CoLoR-Filter on a corpus that has already been deduplicated to prevent degenerate duplications, (2) for large $n$, we must select many different data points, and (3) each datapoint is itself a sequence that may contain diverse signal across tokens. We should also note this is not a unique property of CoLoR-Filter and also happens in other methods that do offline scoring like DSDM and DSIR. We defer a detailed discussion of the nuances of this issue to .

## Related Algorithms

#### Connection to importance sampling.

Since the CoLoR-Filter objective is written as a difference of logs, it can also be written as a log of the ratio between probabilities under $\theta_{\text{prior}+ \text{down}}$ and $\theta_{\text{prior}}$. If data were actually sampled from $\theta_{\text{prior}}$, then this ratio would be the importance weight needed to reweight samples so that they are from the model defined by $\theta_{\text{prior}+ \text{down}}$. Note that DSIR directly attempts to perform importance sampling from $D_{\text{train}}$ to $D_{\text{down}}$ instead of optimizing performance on the downstream data. Thus, DSIR ends up with a somewhat related algorithm except in DSIR: (1) there is no language model, just features of a full data point (hashed n-grams), and (2) the algorithm samples rather than optimizes.

#### Connections to DSDM.

Another closely related approach is DSDM which uses a TRAK Datamodel estimator to score datapoints and then selects the top-$n$ points. The motivation and setting of DSDM are similar to CoLoR-Filter, but DSDM relies on TRAK which constructs a linear approximation of the influence that data points have on each other. Instead, CoLoR-Filter operates directly in function space by comparing the loss between models directly rather than relying on linear approximations or Datamodels .

#### Connections to RHO-down.

CoLoR-Filter is inspired by and builds on the RHOLoss approach introduced in prior work with subtle but significant differences in the setting: the original RHO paper focuses on cases where the hold-out data is sampled from the same distribution as $D_{\text{train}}$ over multiple epochs of training. In contrast, we focus on selecting data to target downstream distributions that are different from $D_{\text{train}}$ and where we only take a single pass over the data. Here, we derive a straightforward adaptation of RHOLoss to our setting, which we call RHO-down.

We now derive RHO-down in our setting, aiming to illustrate the connections between RHO-down and CoLoR-Filter. First, RHO-down approximates the full subset selection problem from by a greedy (sequential) approximation where samples are added to $S$ one (batch) at a time. Using a batch size of $1$, the $i$th-sample would be ideally added according to the following criterion: $$\begin{aligned}
     \approx \min_{x_i \in D_{\text{train}}}   -\log \int_\theta\Pr(x_i| \theta)\Pr(\theta | D_{\text{down}}, x_{<i}) + \log \int_\theta \Pr(x_i | \theta)\Pr(\theta| x_{<i}),
\end{aligned}$$ where $i$ ranges from $1$ to $n$ sequentially. RHO-down then uses a point estimate of the parameters (as we do in CoLoR-Filter): $$\begin{aligned}
     \approx \min_{x_i \in D_{\text{train}}}  -\log \Pr(x_i| \theta_{\text{down}+x_{<i}}) + \log  \Pr(x_i | \theta_{x_{<i}})
\end{aligned}$$ Finally, the RHO-down authors found that updating the conditional term to depend on $x_{<i}$ was unstable, so they instead approximate this by a fixed model $\theta_{\text{down}}$: $$\begin{aligned}
     \approx \min_{x_i \in D_{\text{train}}} -\log \Pr(x_i| \theta_{\text{down}}) + \log  \Pr(x_i | \theta_{x_{<i}}).
\end{aligned}$$

Note that while both CoLoR-Filter and RHO-down approximate the posterior over parameters with a point estimate, RHO-down makes a few additional approximations. This is largely a result of RHO-down attempting to increase data diversity by using a sequential approach to selection that conditions on the previously selected data $x_{<i}$. This is an understandable goal, but it introduces more approximations, can cause instability by creating a non-stationary data distribution, and is computationally expensive since the data selection is no longer parallelizable. A continued discussion of the pros and cons of online selection is in .

#### RHO-down + prior.

We also consider a version of the algorithm that we call “RHO-down + prior” that replaces $D_{\text{down}}, \theta_{\text{down}}$ in the RHO-down algorithm with $D_{\text{prior}}\cup D_{\text{down}}, \theta_{\text{prior}+ \text{down}}$ to incorporate the prior information. This corresponds to conditioning on both $D_{\text{prior}}$ and $D_{\text{down}}$ instead of only $D_{\text{down}}$. Intuitively, this method can better leverage stronger features learned on the larger $D_{\text{prior}}$ to integrate the information from the small $D_{\text{down}}$.

# Further Related Work

We now discuss some related work, more broadly, with regards to active learning and data curation.

**Active & Curriculum learning**. Our formulation of data selection has connections to classic and deep active learning , which are deeply rooted in optimal Bayesian experimental design , whose goal is to select a set of experiments to optimize certain information criteria such as maximally reducing the uncertainty about model parameters. Various acquisition functions are proposed in deep learning regimes and most of them focus on label-efficient image classification. Another line of recent techniques share deep methodological connections but emphasize the sub-selection of available data during training (rather than the collection of additional examples typically considered in active learning) and could thus be classified as curriculum learning . Among them, RHOLoss seeks to select data based on the hold-out reference dataset from the same distribution as the training data. It has been later implemented in continual pre-training and vision domains .

**Data curation practices in pre-training**. Though large-scale public web-crawled data are common data sources for pre-training models, low-quality, toxic, and uninformative content that can prevent successful pre-training is prevalent . Therefore, practitioners design sophisticated data pre-processing pipelines such as filtering , deduplication , and mixing to improve the data quality. Due to the immense scale, state-of-the-art pre-training datasets usually depend on simple heuristic filters (e.g., URL, length, n-gram perplexity, fastest classifiers) that can be parallelized across CPU nodes. Besides the above rule-based filtering, model-based filtering concerns using machine learning models to score and filter data, which has been proven to be effective in vision and vision-text domains . Such approaches usually leverage a given trustworthy data source like Wikipedia or Books as the reference and contrast the raw data with it. Due to computational cost, models are often designed to be small such as n-gram , single-layer neural networks , k-means clustering . There is also a growing line of work illustrating that data quality is important in shaping model training from a variety of perspectives, such as increasing data scale and using synthetic data .

# Algorithms

## From Derivations to Practical Algorithms

In our experiments, we will consider four algorithms based on the above derivations. In this section we go through each of these in turn.

#### CoLoR-Filter.

Our proposed algorithm is presented formally in . Compared to the derivation, the main difference is the introduction of $\tau$, a hyperparameter that acts as a compute-performance trade-off controlling how expensive and aggressive the data selection is. Rather than selecting data from all of $D_{\text{train}}$, we take a random subset $D_\tau$ of size $\tau n$. Thus, larger $\tau$ subselect more aggressively, but at the cost of more computation. A full discussion of this cost is in .

#### Conditional only.

As an ablation of CoLoR-Filter, we follow prior work and include a baseline that only uses the conditional model to select data. Essentially, this is CoLoR-Filter if we always assume that $\log \Pr(x|\theta^{\text{marg}}) = 0$ in Line 4 of .

#### RHO-down.

We present a practical variant of RHO-down in based on the derivation presented in . The main changes to make a practical algorithm are (1) the introduction of $\tau$ as in CoLoR-Filter, and (2) performing the algorithm batch-wise instead of using single data points.

#### RHO-down + Prior.

We can also incorporate the prior data $D_{\text{prior}}$ into by simply replacing Line 1 where $\theta^{\text{cond}}$ is trained on $D_{\text{down}}$ with a procedure where we first pre-train $\theta^{\text{cond}}$ on $D_{\text{prior}}$ and then fine-tune it on $D_{\text{down}}$.

## Computational Cost

To evaluate the computational cost of the various algorithms, we use units of “model forwards” per token where we assume that a backward pass is twice as expensive as a forward pass . Note that our 150m models take about 5e8 FLOPs per model forward of a single token . The cost of running the selection algorithms depends on $m, n, \tau$ and $L$ defined as follows: $m$ is the size of the prior data $D_{\text{prior}}$, $n$ is the size of the selected dataset $S$, $\tau$ is the hyperparameter controlling how aggressively we subselect data. Note that we assume that $|D_{\text{down}}|$ is so small that the cost of training a model on $D_{\text{down}}$ is negligible towards the total cost (and all the methods we consider just fine-tune a model once on $D_{\text{down}}$). We will also be careful to note when computation can be done in parallel before training versus computation that must happen serially during a training run. Offline algorithms like CoLoR-Filter can take advantage of parallelism to improve efficiency. In this section, we go through each method in turn and aggregate the computational costs in .

#### Scale transfer.

We also include another parameter $L$ to cover the case where we select data using small models and use it to train a larger model . Specifically, $L$ is the ratio of cost of one model forward of the *large* target model compared to the small auxiliary models used for data selection. For example, in our experiments, when we use 150 million parameter models to select data and then train a 1.2 billion parameter model on the resulting data, then $L\approx 5.5$[^2]. Training thus costs $3nL$ across all methods since we run a forward and backward for the large model on all $n$ sequences.

#### CoLoR-Filter.

The cost of selection is $2\tau n$ forward passes. But, this selection process is *entirely* parallelizable. Training the prior model costs $3m$ forwards since $|D_{\text{prior}}| = m$. And training a model on the selected data costs $3nL$ forward passes. So the total cost is $3m + 2 \tau n + 3nL$, but the $2 \tau n$ scoring computation can be done in parallel.

#### Conditional Only.

The conditional-only method is almost the same as CoLoR-Filter, except we only need $\tau n$ forward passes for selection since we only run one model over the data. The cost is thus $3m + \tau n + 3nL$, with $\tau n$ being parallelizable.

#### RHO-down.

The cost of selection is still $2 \tau n$ forward passes. Then we need an additional $2n$ to backward the output model (since the forward is already handled during scoring). Note that we need to evaluate the marginal model online, so it is not parallelizable, but the conditional model is fixed and can be computed offline. So, the cost is $2 \tau n + 2n + 3nL$, and the $\tau n$ conditional model computation can be done in parallel.

#### RHO-down + Prior.

For the version with an added prior, we just add $3m$ cost for training the prior. Thus, the cost is $2\tau n + 2n + 3nL$ with $\tau n$ parallelizable.

<div id="tab:cost">

| Method           | Prior cost | Serial cost    | Parallel cost | Training cost |
|:-----------------|:-----------|:---------------|:--------------|:--------------|
| CoLoR-Filter     | $3m$       | $0$            | $2 \tau n$    | $3n L$        |
| Conditional Only | $3m$       | $0$            | $\tau n$      | $3n L$        |
| RHO-down         | 0          | $\tau n + 2 n$ | $\tau n$      | $3n L$        |
| RHO-down + Prior | $3m$       | $\tau n + 2n$  | $\tau n$      | $3n L$        |
| Random           | 0          | 0              | 0             | $3n L$        |

Compute cost of the various algorithms measured in “model forwards”. The total cost of selection and training on the selected data is the sum of all costs across a row. The variables are $m = |D_{\text{prior}}|$, $n = |S|$, $\tau$ is a hyperparameter that controls how aggressively we subselect, and $L$ is a multiplier of the cost of model forwards between the selection model(s) and the target model (approximately the ratio of parameter counts between the models).

</div>

Overall, the methods all have comparable costs, with Conditional Only being the cheapest and RHO-down + Prior the most expensive. The main difference is that CoLoR-Filter and Conditional Only are easily parallelized while RHO-down and RHO-down + Prior are not. It should also be noted that when doing experimentation, offline methods like CoLoR-Filter also benefit from being able to re-use likelihoods multiple times, while RHO-based methods need to recompute the serial cost any time that some hyperparameter of the algorithm.

# Domain Transfer: a Simple Testbed

## Setup

#### Training.

We train language models with 150 million non-embedding parameters using the OLMo codebase and following hyper-parameter choices from . Unless otherwise noted, we use 150m models as the auxiliary models ($\theta^{\text{cond}}, \theta^{\text{marg}}$) as well as the target model $\theta$. Full hyperparameters are described in detail in .

We take $D_{\text{down}}$ to be a small dataset of 25 million tokens sampled from the Project Gutenberg Books data subset of Dolma , $D_{\text{prior}}$ to be a dataset of 3.1 billion tokens from C4 , and $D_{\text{train}}$ to be all of C4. We select a dataset $S$ of 3.1 billion tokens (which is approximately the “chinchilla optimal” amount for models of this size). To get $\theta_{\text{prior}+ \text{down}}$ or $\theta_{\text{down}}$, we fine-tune or train for one epoch on $D_{\text{down}}$.

#### Evaluation.

To evaluate the efficacy of our data selection, we report cross-entropy loss of next token prediction on a held-out dataset $\widetilde D_{\text{down}}$ from the same distribution as $D_{\text{down}}$ (Books).

#### Baselines.

The simplest baseline we consider is **Random** sampling, which has been shown to be a strong baseline for C4 pre-training . We consider all four algorithms described in : **CoLoR-Filter**, **Conditional Only**, **RHO-down**, and **RHO-down + prior**. And as one extra baseline, we also include **DSIR** which estimates n-gram importance weights between $D_{\text{train}}$ and $D_{\text{down}}$, and similarly has a parameter like $\tau$ that controls how aggressively to subselect.

Note that while it is in a similar setting to ours, we do not include DSDM as a baseline since there is no publicly available code and based on the appendix of that paper, it it much more computationally expensive than the methods we consider.

## Results

We first run the domain transfer experiments on 150m models, sweeping across $\tau$ that controls the selected subset size. In we plot how the final performance scales with $\tau$ across methods. We see that CoLoR-Filter has the best scaling performance with increased $\tau$, with no sign of saturation for $\tau = 16$. We hypothesize that by using strong models to select the data, CoLoR-Filter is able to more effectively scale to larger $\tau$ than the other methods. In in , we plot the learning curves (evaluated on the held-out validation set) for the four methods introduced in . There, we see especially clean scaling for CoLoR-Filter across the entire learning curve, substantially outperforming random selection with much less data, similar to .

#### Scale generalization.

Finally, we also conduct an experiment in scale generalization (partially shown in ) using the data selected by our 150m auxiliary models to train a 1.2b target model. In we show learning curves for a sweep over $\tau$. We still see consistent gains as we scale $\tau$ for a fixed number of training tokens. Interestingly, if we fix the total number of tokens we are *selecting from* (i.e. where the lines end when we run out of C4), then the final performance with $\tau = 32$ is better than all other values of $\tau$. This shows how a strict subset of tokens can outperform a superset (e.g. $\tau=16$). We should also point out here the computational savings when using CoLoR-Filter. As an example, consider $\tau=16$ where we match the performance of 25 billion randomly selected tokens with about 1.5 billion filtered tokens. Considering the computational costs discussed above with $L = 5.5$ and measuring $n$ in billions of tokens, the total cost for training the CoLoR-Filter model is $3 m + 2\tau n + 3nL = 3 * 3.1 + 2 * 16 * 1.5 + 3 * 1.5 * 5.5 = 82$ while the cost for training on 25 billion random tokens is $3NL = 3 * 25 * 5.5 = 412.5$, illustrating a more than 5x total compute savings to achieve the same performance on Books. A full plot visualizing the cost in FLOPs for all $\tau$ is in .

# Downstream Tasks

## Setup

#### Training.

We target the 8 tasks from the OLMo paper : Hellaswag , PIQA , ARC-challenge and ARC-easy , Openbook QA , SciQ , BoolQ , and Winogrande . Each of these datasets has a separate train split. We use these train splits to construct $D_{\text{down}}$ as follows: for each question we concatenate the question and the correct answer formatted as a grammatical continuation. Overall, this results in a small $D_{\text{down}}$ dataset of 7.4 million tokens. $D_{\text{prior}}$ and $D_{\text{train}}$ are the same as before. And we again get $\theta_{\text{prior}+ \text{down}}$ by fine-tuning $\theta_{\text{prior}}$ for one epoch on $D_{\text{down}}$.

#### Evaluation.

We evaluate on held-out data from each downstream task test or validation sets (using val if test is not publicly available). We use the evaluation procedure from OLMo which follows for evaluating these multiple-choice tasks using the rank classification approach of . We report aggregat perfromance across tasks as well as the task-specific performance.

#### Baselines.

Same as in .

## Results

<figure id="fig:heatmap">
<p><span class="image placeholder" data-original-image-src="images/heatmap.pdf" data-original-image-title="" width="85%">image</span> </p>
<figcaption>Performance improvement over training on an equivalent amount of random data broken down by task (except for Random 8x, which uses 8x more data). A table of results is in .</figcaption>
</figure>

While the curves themselves are noisier now due to the noisier nature of accuracy evaluation on small datasets compared to cross entropy on a large one, the same trends hold as we saw for domain transfer to Books. CoLoR-Filter in particular is scaling the best as we increase $\tau$. Other methods do not illustrate the same clean scaling as we increase $\tau$, which is nearly linear on a log scale for CoLoR-Filter, as seen in . Full learning curves are in .

We can also look at the performance broken down by task and illustrated relative to training on an equivalent amount (3.1 billion tokens) of randomly selected data for $\tau = 16$ illustrated in . We see especially large gains on Hellaswag, ARC easy, Openbook QA and SciQ and actually see performance decreases on BoolQ and Winogrande. However, we should note that at this scale and with all data selected from C4, we actually found BoolQ and Winogrande to be quite noisy and not even correlated with training on 8x as much random data, so it is not clear how much weight to place on those results. Across the other tasks, the gains of CoLoR-Filter over the baselines are clear. It is an interesting direction for future work to probe more deeply into how task-dependent the gains from targeted data selection can be.

#### Scale generalization.

We also consider scale generalization to a 1.2b target model and illustrate the full results of a sweep over $\tau$ in . Again we find significant benefits of CoLoR-Filter across scales. A full table of per-task results is in . Again we notice that training on a strict subset of data can outperform a larger dataset.

We can again do out the calculation of computational savings for $\tau = 16$. It now takes about 3 billion tokens for CoLoR-Filter to match the performance of training on 25 billion random tokens. This amounts to a total cost of $3 m + 2\tau n + 3nL = 3 * 3.1 + 2 * 16 * 3 + 3 * 3 * 5.5 = 154.8$, which is still an upwards of 2.5x reduction in compute to achieve the same average performance across the suite of tasks. A full plot visualizing the cost in FLOPs for all $\tau$ is in .

#### Task generalization.

We can also test task generalization beyond the 8 tasks that were used to select the data on a few more tasks that test common sense reasoning . Results are presented in compared to a random model trained on 10x as much data. The performance indicates that the data selected by CoLoR-Filter are not overfit to the particular evaluation tasks, but captures some general notion of good data for a range of tasks.

Note, we also conduct a few more experiments and ablations in the appendix: considers using CoLoR-Filter in-distribution to target C4 loss, considers applying CoLoR-Filter batchwise rather than globally, considers finetuning on $D_{\text{down}}$ after targeted pre-training, inspects some of the selected and excluded examples, and compared to FineWeb-edu .

# Discussion

While fairly simple to derive and implement, we show that CoLoR-Filter is an effective method for data selection on C4, with promising scaling behavior up to 1.2 billion models. In our experiments, CoLoR-Filter continues to improve when only using 1 out of 64 data points considered for selection and generalizes from small auxiliary models to larger target models. This opens many potential lines of research. First, while we have considered targeted pre-training, it is possible that CoLoR-Filter could be extended to fine-tuning, continual pre-training, and more general open-domain pre-training. In particular, it is an interesting open question whether the lack of an explicit consideration of data diversity hinders CoLoR-Filter in any of these settings. Second, CoLoR-Filter could be applied to more challenging domains in language like code generation or even applied beyond the language domain to other modalities. Finally, there is plenty of work to be done to make the algorithm more efficient and to test the limits of scale generalization.

# Acknowledgments

HZ is supported by an Eric and Susan Dunn Graduate Fellowship. SK acknowledges support from the Office of Naval Research under award N00014-22-1-2377 and the National Science Foundation Grant under award \#IIS 2229881. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence.

# Learning curves for 150m models

<figure id="fig:books">
<span class="image placeholder" data-original-image-src="images/books_lcs.pdf" data-original-image-title="" width="\textwidth"></span>
<figcaption>Sweeping over <span class="math inline">\(\tau\)</span> when targeting <strong>Books</strong> from C4 for 150m models.</figcaption>
</figure>

<figure id="fig:down_lcs">
<span class="image placeholder" data-original-image-src="images/down_lcs.pdf" data-original-image-title="" width="\textwidth"></span>
<figcaption>Sweeping over <span class="math inline">\(\tau\)</span> and measuring average performance on all downstream tasks for 150m models.</figcaption>
</figure>

# Tables of downstream results

# Data diversity and online vs. offline selection

Much work on active learning focuses on ensuring that we select a diverse set of data points that cover the test distribution of interest. As explained in the main text, by making a point estimate of the parameters, CoLoR-Filter is simplifying the problem and sacrificing an explicit term for diversity in the objective. In practice, this seems to be saved by the facts that (1) C4 has already been deduplicated, (2) we still select a fairly large subset without replacement, and (3) an individual sequence contains diversity across tokens.

However, the fact that CoLoR-Filter sacrifices a notion of diversity in the objective is important to consider more deeply. Here, we derive what a loss-based algorithm for data selection that prioritizes diversity would look like and why it is computationally infeasible. Then we derive an approximation (that looks somewhat like RHOLoss ) and show how it is empirically unstable, as was also observed previously by .

To derive a CoLoR-Filter-like algorithm that values diversity, we can start from by a greedy approximation where samples are added to $S$ one (batch) at a time, like in RHO: $$\begin{aligned}
     \approx \min_{x_1, \dots, x_n \subset D_{\text{train}}} \sum_{i=1}^n  -\log \int_\theta\Pr(x_i| \theta)\Pr(\theta | D_{\text{down}}, x_{<i}) + \log \int_\theta \Pr(x_i | \theta)\Pr(\theta| x_{<i})
\end{aligned}$$ Note that this sort of greedy algorithm for subset selection has a long history in active learning , is actually theoretically sound in some cases , and is used in prior work . Importantly, this algorithm still prioritizes selecting a diverse dataset. By conditioning on past data at step $i$, the objective encourages the algorithm to select data that is different from data that has already been selected.

We can also make an empirical bayes version by adding $D_{\text{prior}}$: $$\begin{aligned}
     \min_{x_1, \dots, x_n \subset D_{\text{train}}} \sum_{i=1}^n  -&\log \int_\theta\Pr(x_i| \theta)\Pr(\theta | D_{\text{prior}}, D_{\text{down}}, x_{<i}) \\ &+ \log \int_\theta \Pr(x_i | \theta)\Pr(\theta| D_{\text{prior}}, x_{<i})
\end{aligned}$$

This is, of course, still intractable since it requires integrating the parameters. But, since we have already introduced the greedy algorithm that encourages diversity, if we now make the point estimate approximation, the incentive for data diversity remains. This results in: $$\begin{aligned}
\label{eq:online}
     \approx \min_{x_1, \dots, x_n \subset D_{\text{train}}} \sum_{i=1}^n  -\log \Pr(x_i| \theta_{\text{prior}+ \text{down}+x_{<i}}) + \log  \Pr(x_i | \theta_{\text{prior}+x_{<i}})
\end{aligned}$$

The thorny issue here is how to define $\theta_{\text{prior}+ \text{down}+x_{<i}}$ and $\theta_{\text{prior}+x_{<i}}$ in practice. In theory, these parameters should be trained on an iid sample from the union of the datasets. If we add the datapoints one at a time, the dynamics of the distribution shift over time can change how well the model corresponds to conditioning on the union of the dataset. But, this would require re-training the models every time we add a new $x_i$ which is clearly impractical.

In practice, this encourages using a fine-tuning approach (as in RHO) where we continually fine-tune on the $x_i$ as they are added. But when $D_{\text{down}}$ is small and the data distribution changes over time, we can get catastrophic forgetting and unstable training dynamics. For these reasons, RHO avoids training the conditional model entirely (Appendix D of ). We also conduct an experiment on the Books task where we use this online fine-tuning algorithm that updates both the marginal and conditional models as we add data to $S$. Results in show how the training is unstable and in fact performs worse than random.

<figure id="fig:online">
<p><span class="image placeholder" data-original-image-src="images/online_perf.pdf" data-original-image-title="" height="31%">image</span>  <span class="image placeholder" data-original-image-src="images/online_lcs.pdf" data-original-image-title="" height="31%">image</span></p>
<figcaption>(Left) Performance of online selection with fine-tuning as outlined in . Online selection is worse than random. (Right) Training curves for the conditional and marginal models on the selected data <span class="math inline">\(S\)</span>. The conditional model faces training instability early on (associated with forgetting), and then eventually becomes better than the marginal on the selected data.</figcaption>
</figure>

Moreover, Note that the computational cost of even the cheapest fine-tuning algorithm is substantial compared to the algorithms in the paper. In particular, the serial cost is now $2 \tau n +  4n$ (as compared to $\tau n + 2n$ for RHO) since we need to pass the full $\tau n$ samples through both the conditional and marginal models. So this variant is clearly inferior in practice to the other approaches we consider.

# Compute cost for scale generalization

<figure id="fig:costs">
<span class="image placeholder" data-original-image-src="images/1b_costs.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Costs in FLOPs to reach equivalent performance to the final random model trained on 25b tokens (i.e. cost until we reach the dotted line in ). We split cost into the scoring cost for filtering the data using the small auxiliary models and then training cost for the large model.</figcaption>
</figure>

In the main text we computed the cost for $\tau = 16$ in terms of model forwards of 1 billion tokens. Here we can convert this to FLOPs and compute the cost for all values of $\tau$. Results are in showing the breakdown of costs into scoring FLOPs for running the small auxiliary models over the data and training FLOPs for training the large model. We measure the cost it takes to reach the final performance of the random model, i.e. until the CoLoR-filter learning curve crosses the dotted line in . The main tradeoff is that lower $\tau$ values require more scoring cost and less training cost because they are able to select better data.

We should also note that if multiple models are being trained with the same dataset, then this scoring cost can be amortized over those runs and the larger $\tau$ values will look even better.

# Can we do data selection in distribution?

<figure id="fig:in_dist">
<span class="image placeholder" data-original-image-src="images/id_lcs.pdf" data-original-image-title="" width="98%"></span>
<figcaption>Using a sample of C4 as <span class="math inline">\(D_{\text{down}}\)</span>. RHO provides marginal gains here, while CoLoR-Filter does not provide gains at all. Conditional Only is worse than random. Scaling <span class="math inline">\(\tau\)</span> does not change results as much as when we target downstream tasks.</figcaption>
</figure>

One obvious question raised by these data selection techniques is whether they can work in distribution, i.e. can we select data to make the iid loss on C4 go down faster? In we present results for running this experiment with CoLoR-Filter as well as RHO and Conditional Only. Note that there is no difference between RHO and RHO + prior now (and we drop the “down” from the name) since the prior distribution and the downstream distribution are the same. To implement CoLoR-Filter in this setting, we just take two checkpoints from pre-training the prior model and call the earlier one (at 2.5b tokens) the marginal model and the later one (at 3.1b tokens) the conditional model.

We find that in distribution selection does not work effectively with these methods. There are small gains to RHO loss, but here they are massively outweighed by the computational cost of the selection. CoLoR-Filter sees no gain at all over random and Conditional Only is worse than random. These preliminary results suggest why it is important to recognize that data selection (especially with these methods) will be most effective when we genuinely want to target a different distribution from $D_{\text{train}}$.

<figure id="fig:batch">
<span class="image placeholder" data-original-image-src="images/books_batchwise.pdf" data-original-image-title="" width="95%"></span>
<figcaption>Comparison between global and batchwise variants of CoLoR-Filter on Books. The two perform nearly identically here.</figcaption>
</figure>

# Global vs. batchwise selection

One more minor implementation aspect about CoLoR-Filter is that as presented in , we do global selection where we take the best $n$ data points across the entire train set, while in RHO-down in selection is done batchwise. Here we ablate whether the ability to do global selection is actually helpful for CoLoR-Filter. Results in suggest that there is not much difference between the two and at small $\tau$, batchwise selection maybe even beat global selection. We provide this result to illustrate that CoLoR-Filter is fairly robust to how the selection is performed.

# Finetuning after targeted pre-training

One possible question about the targeted pre-training setting we consider is: what happens if we finetune on $D_{\text{down}}$ after the targeted pre-training?

This is interesting since while the pre-trained models presented in the main text never have direct access to $D_{\text{down}}$, the selection algorithm does. In this section, we also allow access to $D_{\text{down}}$ after pre-training and then compare the final performance of the finetuned models that are pre-trained on random data vs. selected data.

First, in and we present finetuning results for the 150m models. We find that CoLoR-Filter data outperforms 8x as much random data after finetuning. Note that the conditional model that we use to guide the selection of CoLoR-Filter is equivalent to a model that has been pre-trained on 3B random tokens and then finetuned on the task. Thus, these results show that we are substantially outperforming the conditional model when both models are finetuned on the downstream data.

<div id="tab:150m-books-fine">

| Pre-training data          | Finetuned Books Val Cross Entropy |
|:---------------------------|:---------------------------------:|
| Random (3.1b tokens)       |               3.441               |
| Random (25b tokens)        |               3.357               |
| CoLoR-Filter (3.1b tokens) |             **3.258**             |

Performance after finetuning on Books for different pre-trained 150m models. Note that the Random (3.1b tokens) model is equivalent to the conditional model used to select data with CoLoR-Filter ($\tau=16$).

</div>

<div id="tab:1.2b-books-fine">

| Pre-training data          | Finetuned Books Val Cross Entropy |
|:---------------------------|:---------------------------------:|
| Random (25b tokens)        |               3.074               |
| CoLoR-Filter (2.6b tokens) |             **2.964**             |

Performance after finetuning on Books for different pre-trained 1.2b models. Note that the conditional model that selects data is only 150m parameters.

</div>

Next, we present results for the 1.2b models in and . We find that the CoLoR-Filter model outperforms or is competitive with training on about 10x as much data randomly selected data. We should also note that the CoLoR-Filter models are now dramatically outperforming the 150m conditional models that were used to filter the data, showing positive scale transfer of data selection.

# Hyperparameters

<div id="tab:150m-hyper">

| Parameter            | Value             |
|:---------------------|:------------------|
| Residual dimension   | 1024              |
| Depth                | 12                |
| MLP hidden dimension | 4096              |
| Activation           | GeLU              |
| Head dimension       | 64                |
| Context length       | 512               |
| Positional encoding  | RoPE              |
| Biases               | False             |
| Normalization        | PyTorch Layernorm |
| QK normalization     | True              |
| Precision            | Mixed, bfloat16   |
| Tokenizer            | GPTNeox           |

150m model parameters, based on

</div>

<div id="tab:1.2b-hyper">

| Parameter            | Value |
|:---------------------|:------|
| Residual dimension   | 2048  |
| Depth                | 24    |
| MLP hidden dimension | 8192  |

1.2b model, based on . Only reporting differences from 150m.

</div>

<div id="tab:train-hyper">

| Parameter          | Value                       |
|:-------------------|:----------------------------|
| Optimizer          | Adam                        |
| Batch size         | 256                         |
| Learning rate      | 1e-3                        |
| Schedule           | Linear warmup, cosine decay |
| Warmup steps       | 5% of total steps           |
| z-loss coefficient | 1e-4                        |
| Weight decay       | 0.0                         |
| $\beta_1$          | 0.9                         |
| $\beta_2$          | 0.95                        |
| $\epsilon$         | 1e-15                       |

Training parameters, based on

</div>

# Inspecting the selected data

In this section, we conduct some basic analysis of the data that is selected by CoLoR-Filter. We leave a full analysis to future work, but here we provide some high level statistics about the distributions of the scores of the conditional vs. marginal models and some representative examples from the datasets.

## Distribution of scores

First, we simply plot the CDFs of the conditional loss reduction (CoLoR) score function used to select the data. We find that there are relatively few outliers and the CoLoR scores are fairly concentrated and normally distributed. Moreover, we note that the mean CoLoR in both experiments is positive, meaning that the conditional model actually has higher losses on the datapoints in C4 than the marginal model. This makes sense because the conditional model has been finetuned on $D_{\text{down}}$ which is out of distribution relative to C4.

<figure id="fig:cdfs">
<p><span class="image placeholder" data-original-image-src="images/books_cdf.pdf" data-original-image-title="" height="31%">image</span>  <span class="image placeholder" data-original-image-src="images/down_cdf.pdf" data-original-image-title="" height="31%">image</span></p>
<figcaption>CDFs for the conditional loss reduction (CoLoR), i.e. <span class="math inline">\(-\log \Pr(x|\theta_{\text{prior}+ \text{down}}) - (-\log \Pr(x|\theta_{\text{prior}}))\)</span>. The dashed line highlights the cutoff point for <span class="math inline">\(\tau = 64\)</span>. We select the points with the lowest CoLoR.</figcaption>
</figure>

## Representative examples

Now we just list a few representative examples to give a flavor for the types of outliers that exist under our ranking of sequences and the sorts of typical sequences that are selected versus excluded. The sequences are sampled randomly from different quantiles of the distribution and we shorten all the sequences so that they fit more easily on the page.

shows outliers when targeting Books and shows more typical examples when targeting Books. Generally, we found that the documents with very high scores contain things like old English, poetry, and tables of contents that are particularly unusual in books compared to the rest of the internet. Other things like fiction and dialogue are also highly scored. Negative outliers typically have things like poorly encoded text or advertisements.

shows outliers when targeting downstream tasks and shows more typical examples when targeting downstream tasks. Here the patterns are less clear since the target tasks are more diverse, but we did observe many scientific and wiki-style documents with high scores as well as some descriptions of physical interactions that may be useful for common sense tasks. Again, the negative outliers tend to have things like poorly encoded text or advertisements.

<figure id="fig:books_outliers">
<figure>
<p><span> <code> AS now shall ye wyt, what tyme of the day ye shall angle. From the begynning of Maye vntill it be September: the byting tyme is early in the morow from four of the clocke vnto eyght of the clocke, at after none from foure to eyght also, but not so good as in the mornyng, and if it be a colde wynde and a lowryng day, it is muche better than a cleere daye. Also many poole fysshes will byte best in the morne tyde. And if ye se in any tyme of the day the Troute or greylyng lepe angle to him with a dub according to the same moneth. And where the water ebbeth and floweth: the fish wyll byte in some place at the ebbe and in some place at the flud after they haue restyng</code></span></p>
<figcaption>Good outlier, CoLoR = -0.35</figcaption>
</figure>
<figure>
<p><span> <code> ??????????????????????????????? ???????????????????????????????? ????????????????????????????????? ?????????????????????????????????? ?????????????????????????????????? ???????????????????????????????? ??????????????????????????????????? ???????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ???????????? ????????????????????????? m88 ???????????????????????? ???? m88 ?????????????????????????????????????? ??????????????????????????? ???? ?????????????????????????????????? ??????????????????????????????? ? ??????????????????? </code></span></p>
<figcaption>Bad outlier, CoLoR = 5.45</figcaption>
</figure>
<figcaption>Examples of outliers when targeting <strong>Books</strong>. Examples are sampled randomly from the top or bottom 1000 sequences. The positive outlier is written in an older dialect of English which may be related to some documents in the Project Gutenberg corpus, while the negative outlier appears to be poorly encoded.</figcaption>
</figure>

<figure id="fig:books_typical">
<figure>
<p><span> <code> C: Mrs Mackenzie, was there ever a time when you felt like you could just hop on a plane and make that flight down to the next State to be with your boys? B: Oh my dear, yes. I feel sometimes as if I’m twenty and so fit and active and I can do whatever I want to do and then I remember, good grief, I’m 86, you old fool, you can’t do that. I wish I could just fly down there and live with them all together just how it was when they were little and I was their Mum and they followed me because I was so bright and cheery and smart and active and all the things that I’m not now. Oh, I’m so sorry, listen to me. Maybe I’m just losing my marbles, what do you think, dear? C: Smiling – Imagine if I waved a magic wand and miraculously you were twenty again. What would you see yourself doing Beryl. Is it ok if I call you Beryl? </code></span></p>
<figcaption>Sequence from best 3%, CoLoR = 0.40</figcaption>
</figure>
<figure>
<p><span> <code>Chamber of Commerce and other business venues, such as the Gwinnett Civic &amp; Convention Centers and is an ideal working environment for commercial businesses and corporations in Northeast Atlanta. The prominent location is on a heavily wooded, landscaped 6.5 acre site fronting on I-85. The exterior features green-tinted thermal glass and the entrance features a curtain wall glass leading into a granite-floored lobby with vaulted ceilings. Gwinnett County is home to leading Fortune 500 companies, drawn by its reputation as a commerce and technology hub, providing businesses with a regional market of five million people. SERVPRO of Gurnee can simplify the restoration process by handling both the initial water damage mitigation and rebuilding the affected areas. Having one qualified company for the entire process can save time and keep costs low. </code></span></p>
<figcaption>Sequence from median 3%, CoLoR = 0.73</figcaption>
</figure>
<figcaption>Examples of more typical documents when targeting <strong>Books</strong>. First a document from the top 3% that would be selected with <span class="math inline">\(\tau = 32\)</span>, and then a document that scores near the median of all documents. The selected document is fictional dialogue while the median document is an advertisement.</figcaption>
</figure>

<figure id="fig:down_outliers">
<figure>
<p><span> <code> among the pinacoderm are the ostia that allow entry of water into the body of the sponge. These pores have given the sponges their phylum name Porifera—pore-bearers. In some sponges, ostia are formed by porocytes, single tube-shaped cells that act as valves to regulate the flow of water into the spongocoel. In other sponges, ostia are formed by folds in the body wall of the sponge. Between the outer layer and the feeding chambers of the sponge is a jelly-like substance called the mesohyl, which contains collagenous fibers. Various cell types reside within the mesohyl, including amoebocytes, the “stem cells” of sponges, and sclerocytes, which produce skeletal materials. The gel-like consistency of mesohyl acts like an endoskeleton and maintains the tubular morphology of sponges. The feeding chambers inside the sponge are lined by choanocytes (“collar cells”).</code></span></p>
<figcaption>Good outlier, CoLoR = -0.46</figcaption>
</figure>
<figure>
<p><span> <code> *** **********. ****** *** ***, *** ******* **** **** ** ******** ******* plates ** ****** ** ** **-** *** (*** ******* ** tested), ***** ******* ********. *** ** *** ***, *** ********* *.* ********* ******* ***** capture ****** ******** ******** ****** ** **** ** **** ******, &gt;10 ***, *** ******, **+ ***, **** ** ****** ***** or ****, *** ** ***** ****** *** *** **** **** field ** ****, ***** **’. ***** ******* ********, *** ******** ** ***** ****** ****** ****** to ******* ****** ** ****** **** ** **** ****** ** night, ****** ******* ******* *** ********. ******** ******** ** */****, *** ******** ****** ******** ******** **** front *** **** ****** ****** ** *** *** **** ******. However, **** ******* ******* *** ********** ** *** ***** ** night, ****** ** **** ****** *** ******* ************ ** *** scene. </code></span></p>
<figcaption>Bad outlier, CoLoR = 5.36</figcaption>
</figure>
<figcaption>Examples of outliers when targeting <strong>downstream</strong> tasks. Examples are sampled randomly from the top or bottom 1000 sequences. The positive outlier is a scientific document that could be relevant for tasks like SciQ, while the negative outlier appears to be poorly encoded.</figcaption>
</figure>

<figure id="fig:down_typical">
<figure>
<p><span> <code> summer plans. After thinking for a while I decided to spend my summer in Squamish, where I would work for the Admissions Team. However, due to a very large number of students interested to work on campus and a limited number of work positions, I ended up not getting a job on campus. I was very upset indeed and I began to think that there were not any job openings elsewhere, which would then result in me travelling back home. Surprisingly, there were many job opportunities in the Squamish community. Since Quest University Canada hosted a job fair on campus I, along with all the students, had the chance to meet local businesses that were looking for summer employees. It was a great opportunity to network and give my resume to the ones that interested me. </code></span></p>
<figcaption>Sequence from best 3%, CoLoR = 0.33</figcaption>
</figure>
<figure>
<p><span> <code> Can I install PDF Stacks on more than one computer? The license key is valid for only one device and is non-transferable. You can obtain additional license key(s) by placing an order. How do I use PDF Stacks? Click "File" and then "Import Folder" Once you import the PDF files, your files will be copied into PDF Stacks for easier ability to read, search, organize, take notes, print and share. Any questions, ask us! How do I create collections (virtual binders) and match/tag my documents for better organization? It’s easy. Watch the video for creating collections and tagging documents. Can multiple users access the same documents or can I access and sync my documents through multiple devices? </code></span></p>
<figcaption>Sequence from median 3%, CoLoR = 0.55</figcaption>
</figure>
<figcaption>Examples of more typical documents when targeting <strong>downstream</strong> tasks. First a document from the top 3% that would be selected with <span class="math inline">\(\tau = 32\)</span>, and then a document that scores near the median of all documents. The selected document appears to be a journal entry while the median document is software documentation</figcaption>
</figure>

# Comparison to Fineweb-Edu

Concurrent to our initial work, released FineWeb-edu, a classifier for educational content that can filter the FineWeb dataset. Here we provide a comparison between CoLoR-Filter and this classifier-based approach.

Specifically, we re-implement the CoLoR-Filter pipeline on top of the Fineweb dataset and with slightly smaller auxiliary models (125m) to make a more fair comparison to FineWeb-edu. Then we compare on the same suite of 8 downstream tasks over various settings of $\tau$ using the two scores: CoLoR-Filter or the FineWeb-edu classifier. We then train larger models (680M parameters) for 10B tokens of selected data. Results are shown in . We find that CoLoR-Filter consistently outperforms FineWeb-edu, which is not so surprising since we are doing more targeted data selection by specifically targeting the downstream NLP tasks rather than a general notion of “educational content”.

<figure id="fig:fineweb">
<span class="image placeholder" data-original-image-src="images/fineweb.png" data-original-image-title="" width="0.3\linewidth"></span>
<figcaption>A comparison of the performance of 680m models trained on 10B tokens selected with various <span class="math inline">\(\tau\)</span> between CoLoR-Filter and FineWeb-edu.</figcaption>
</figure>

# Broader Impact

The development of the CoLoR-Filter for data selection has notable broader impacts on both machine learning and society. It enhances efficiency in language model training, leading to reduced computational resources and environmental footprint, while its scalability democratizes access to high-performing models. The method’s success in diverse downstream tasks promises advancements in fields like medical text processing and legal analysis. However, it also raises concerns about dataset bias, necessitating continuous evaluation and updates. Future research should focus on ensuring models do not inherit biases from the selected training data, extending applications, improving efficiency, and implementing safeguards to maximize societal benefits while minimizing risks.

# Compute resources

All training is conducted on an internal cluster using H100 GPUs. On one GPU, each 150m training run for 3.1b tokens takes about 4 hours, running the auxiliary models offline and in parallel can be faster. Training the 1.2b model to completion takes about 2 days on 4 GPUs.

[^1]: Prior work has referred to the models that estimate these two terms as the “reference” and “learner” or “actor”, respectively. We opt for the names conditional and marginal for clarity in connections to the Bayesian viewpoint.

[^2]: Even though there are 8x as many parameters in the large model, the FLOP multiplier is less since the attention computations take the same number of FLOPs regardless of parameters.
