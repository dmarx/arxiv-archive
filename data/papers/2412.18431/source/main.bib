@InProceedings{Trivedi2023,
  author    = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {10014--10037},
  publisher = {Association for Computational Linguistics},
  abstract  = {Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \textit{what to retrieve} depends on \textit{what has already been derived}, which in turn may depend on \textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.},
  doi       = {10.18653/v1/2023.acl-long.557},
  url       = {https://aclanthology.org/2023.acl-long.557},
}

@InProceedings{Gutierrez2024,
  author    = {Bernal Jimenez Gutierrez and Yiheng Shu and Yu Gu and Michihiro Yasunaga and Yu Su},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  title     = {Hippo{RAG}: Neurobiologically Inspired Long-Term Memory for Large Language Models},
  year      = {2024},
  url       = {https://openreview.net/forum?id=hkujvAPVsg},
}

@Article{Trivedi2022,
  author    = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {{M}u{S}i{Q}ue: Multihop Questions via Single-hop Question Composition},
  year      = {2022},
  pages     = {539--554},
  volume    = {10},
  abstract  = {Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, requires proper multihop reasoning? To this end, we introduce a bottom{--}up approach that systematically selects composable pairs of single-hop questions that are connected, that is, where one reasoning step critically relies on information from another. This bottom{--}up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting k-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2{--}4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3{\mbox{$\times$}} increase in human{--}machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30-point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.1},
  address   = {Cambridge, MA},
  doi       = {10.1162/tacl_a_00475},
  editor    = {Roark, Brian and Nenkova, Ani},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2022.tacl-1.31},
}

@InProceedings{Yang2018,
  author    = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D.},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  title     = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  year      = {2018},
  address   = {Brussels, Belgium},
  editor    = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi},
  pages     = {2369--2380},
  publisher = {Association for Computational Linguistics},
  abstract  = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems{'} ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
  doi       = {10.18653/v1/D18-1259},
  url       = {https://aclanthology.org/D18-1259},
}

@Misc{Edge2024,
  author   = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  month    = {April},
  title    = {From Local to Global: A Graph {RAG} Approach to Query-Focused Summarization},
  year     = {2024},
  abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a naïve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
  url      = {https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/},
}

@Misc{Fang2024,
  author        = {Jinyuan Fang and Zaiqiao Meng and Craig Macdonald},
  title         = {{TRACE} the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2406.11460},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.11460},
}

@Misc{OpenAI2024,
  author        = {OpenAI},
  title         = {{GPT}-4 Technical Report},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2303.08774},
  primaryclass  = {cs.CL},
}

@InProceedings{Lewis2020,
  author    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  title     = {Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  year      = {2020},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  series    = {NIPS '20},
  abstract  = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  articleno = {793},
  isbn      = {9781713829546},
  location  = {Vancouver, BC, Canada},
  numpages  = {16},
}

@Misc{Ye2023,
  author        = {Junjie Ye and Xuanting Chen and Nuo Xu and Can Zu and Zekai Shao and Shichun Liu and Yuhan Cui and Zeyang Zhou and Chao Gong and Yang Shen and Jie Zhou and Siming Chen and Tao Gui and Qi Zhang and Xuanjing Huang},
  title         = {A Comprehensive Capability Analysis of {GPT}-3 and {GPT}-3.5 Series Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2303.10420},
  primaryclass  = {cs.CL},
}

@Misc{Wang2023,
  author        = {Yu Wang and Nedim Lipka and Ryan A. Rossi and Alexa Siu and Ruiyi Zhang and Tyler Derr},
  title         = {Knowledge Graph Prompting for Multi-Document Question Answering},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2308.11730},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2308.11730},
}

@InProceedings{Li2024,
  author    = {Li, Shilong and He, Yancheng and Guo, Hangyu and Bu, Xingyuan and Bai, Ge and Liu, Jie and Liu, Jiaheng and Qu, Xingwei and Li, Yangguang and Ouyang, Wanli and Su, Wenbo and Zheng, Bo},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  title     = {{G}raph{R}eader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models},
  year      = {2024},
  address   = {Miami, Florida, USA},
  editor    = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  month     = nov,
  pages     = {12758--12786},
  publisher = {Association for Computational Linguistics},
  abstract  = {Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.},
  url       = {https://aclanthology.org/2024.findings-emnlp.746},
}

@Misc{Liang2024,
  author        = {Lei Liang and Mengshu Sun and Zhengke Gui and Zhongshu Zhu and Zhouyu Jiang and Ling Zhong and Yuan Qu and Peilong Zhao and Zhongpu Bo and Jin Yang and Huaidong Xiong and Lin Yuan and Jun Xu and Zaoyang Wang and Zhiqiang Zhang and Wen Zhang and Huajun Chen and Wenguang Chen and Jun Zhou},
  title         = {{KAG}: Boosting LLMs in Professional Domains via Knowledge Augmented Generation},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2409.13731},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2409.13731},
}

@Misc{Chen2023,
  author        = {Howard Chen and Ramakanth Pasunuru and Jason Weston and Asli Celikyilmaz},
  title         = {Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2310.05029},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.05029},
}

@InProceedings{Sarthi2024,
  author    = {Parth Sarthi and Salman Abdullah and Aditi Tuli and Shubh Khanna and Anna Goldie and Christopher D Manning},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {{RAPTOR}: Recursive Abstractive Processing for Tree-Organized Retrieval},
  year      = {2024},
  url       = {https://openreview.net/forum?id=GN921JHCRw},
}

@Misc{Vijayakumar2018,
  author        = {Ashwin K Vijayakumar and Michael Cogswell and Ramprasath R. Selvaraju and Qing Sun and Stefan Lee and David Crandall and Dhruv Batra},
  title         = {Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1610.02424},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/1610.02424},
}

@InProceedings{Ippolito2019,
  author    = {Ippolito, Daphne and Kriz, Reno and Sedoc, Jo{\~a}o and Kustikova, Maria and Callison-Burch, Chris},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  title     = {Comparison of Diverse Decoding Methods from Conditional Language Models},
  year      = {2019},
  address   = {Florence, Italy},
  editor    = {Korhonen, Anna and Traum, David and M{\`a}rquez, Llu{\'\i}s},
  month     = jul,
  pages     = {3752--3762},
  publisher = {Association for Computational Linguistics},
  abstract  = {While conditional language models have greatly improved in their ability to output high quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that rerank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from a conditional language model. In addition, we present a novel method where we over-sample candidates, then use clustering to remove similar sequences, thus achieving high diversity without sacrificing quality.},
  doi       = {10.18653/v1/P19-1365},
  url       = {https://aclanthology.org/P19-1365},
}

@InProceedings{Su2024,
  author    = {Su, Weihang and Tang, Yichen and Ai, Qingyao and Wu, Zhijing and Liu, Yiqun},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {{DRAGIN}: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models},
  year      = {2024},
  address   = {Bangkok, Thailand},
  editor    = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  month     = aug,
  pages     = {12991--13013},
  publisher = {Association for Computational Linguistics},
  abstract  = {Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs).There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve).However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM{'}s most recent sentence or the last few tokens, while the LLM{'}s information needs may span across the entire context.To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM{'}s information needs during the text generation process.We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method.},
  doi       = {10.18653/v1/2024.acl-long.702},
  url       = {https://aclanthology.org/2024.acl-long.702},
}

@Article{Pan2023,
  author    = {Pan, Jeff Z. and Razniewski, Simon and Kalo, Jan-Christoph and Singhania, Sneha and Chen, Jiaoyan and Dietze, Stefan and Jabeen, Hajira and Omeliyanenko, Janna and Zhang, Wen and Lissandrini, Matteo and Biswas, Russa and de Melo, Gerard and Bonifati, Angela and Vakaj, Edlira and Dragoni, Mauro and Graux, Damien},
  journal   = {Transactions on Graph Data and Knowledge},
  title     = {{Large Language Models and Knowledge Graphs: Opportunities and Challenges}},
  year      = {2023},
  number    = {1},
  pages     = {2:1--2:38},
  volume    = {1},
  address   = {Dagstuhl, Germany},
  annote    = {Keywords: Large Language Models, Pre-trained Language Models, Knowledge Graphs, Ontology, Retrieval Augmented Language Models},
  doi       = {10.4230/TGDK.1.1.2},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  url       = {https://drops.dagstuhl.de/entities/document/10.4230/TGDK.1.1.2},
  urn       = {urn:nbn:de:0030-drops-194766},
}

@InProceedings{Cormack2009,
  author    = {Cormack, Gordon V. and Clarke, Charles L A and Buettcher, Stefan},
  booktitle = {Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
  year      = {2009},
  address   = {New York, NY, USA},
  pages     = {758–759},
  publisher = {Association for Computing Machinery},
  series    = {SIGIR '09},
  abstract  = {Reciprocal Rank Fusion (RRF), a simple method for combining the document rankings from multiple IR systems, consistently yields better results than any individual system, and better results than the standard method Condorcet Fuse. This result is demonstrated by using RRF to combine the results of several TREC experiments, and to build a meta-learner that ranks the LETOR 3 dataset better than any previously reported method},
  doi       = {10.1145/1571941.1572114},
  isbn      = {9781605584836},
  keywords  = {ranking, fusion, aggregation},
  location  = {Boston, MA, USA},
  numpages  = {2},
  url       = {https://doi.org/10.1145/1571941.1572114},
}

@Article{Drosou2010,
  author     = {Drosou, Marina and Pitoura, Evaggelia},
  journal    = {SIGMOD Rec.},
  title      = {Search result diversification},
  year       = {2010},
  issn       = {0163-5808},
  month      = sep,
  number     = {1},
  pages      = {41–47},
  volume     = {39},
  abstract   = {Result diversification has recently attracted much attention as a means of increasing user satisfaction in recommender systems and web search. Many different approaches have been proposed in the related literature for the diversification problem. In this paper, we survey, classify and comparatively study the various definitions, algorithms and metrics for result diversification.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1860702.1860709},
  issue_date = {March 2010},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/1860702.1860709},
}

@InProceedings{Dredze2010,
  author    = {Dredze, Mark and McNamee, Paul and Rao, Delip and Gerber, Adam and Finin, Tim},
  booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
  title     = {Entity Disambiguation for Knowledge Base Population},
  year      = {2010},
  address   = {Beijing, China},
  editor    = {Huang, Chu-Ren and Jurafsky, Dan},
  month     = aug,
  pages     = {277--285},
  publisher = {Coling 2010 Organizing Committee},
  url       = {https://aclanthology.org/C10-1032},
}

@Article{Lin2015,
  author       = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Learning Entity and Relation Embeddings for Knowledge Graph Completion},
  year         = {2015},
  month        = {Feb.},
  number       = {1},
  volume       = {29},
  abstractnote = {&lt;p&gt; Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. &lt;/p&gt;},
  doi          = {10.1609/aaai.v29i1.9491},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/9491},
}

@Article{Wang2024,
  author       = {Wang, Yu and Lipka, Nedim and Rossi, Ryan A. and Siu, Alexa and Zhang, Ruiyi and Derr, Tyler},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Knowledge Graph Prompting for Multi-Document Question Answering},
  year         = {2024},
  month        = {Mar.},
  number       = {17},
  pages        = {19206-19214},
  volume       = {38},
  abstractnote = {The `pre-train, prompt, predict’ paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.},
  doi          = {10.1609/aaai.v38i17.29889},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/29889},
}

@Article{Park2023,
  author   = {Jinyoung Park and Ameen Patel and Omar Zia Khan and Hyunwoo J. Kim and Joo-Kyung Kim},
  journal  = {CoRR},
  title    = {Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models},
  year     = {2023},
  volume   = {abs/2311.09762},
  cdate    = {1672531200000},
  publtype = {informal},
  url      = {https://doi.org/10.48550/arXiv.2311.09762},
}

@InProceedings{Jiang2023,
  author    = {Jiang, Zhengbao and Xu, Frank and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  title     = {Active Retrieval Augmented Generation},
  year      = {2023},
  address   = {Singapore},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  pages     = {7969--7992},
  publisher = {Association for Computational Linguistics},
  abstract  = {Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method.},
  doi       = {10.18653/v1/2023.emnlp-main.495},
  url       = {https://aclanthology.org/2023.emnlp-main.495},
}

@InProceedings{Ho2020,
  author    = {Ho, Xanh and Duong Nguyen, Anh-Khoa and Sugawara, Saku and Aizawa, Akiko},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  title     = {Constructing A Multi-hop {QA} Dataset for Comprehensive Evaluation of Reasoning Steps},
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  editor    = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
  month     = dec,
  pages     = {6609--6625},
  publisher = {International Committee on Computational Linguistics},
  abstract  = {A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required.},
  doi       = {10.18653/v1/2020.coling-main.580},
  url       = {https://aclanthology.org/2020.coling-main.580},
}

@Article{Robertson2009,
  author     = {Robertson, Stephen and Zaragoza, Hugo},
  journal    = {Found. Trends Inf. Retr.},
  title      = {The Probabilistic Relevance Framework: {BM}25 and Beyond},
  year       = {2009},
  issn       = {1554-0669},
  month      = apr,
  number     = {4},
  pages      = {333–389},
  volume     = {3},
  abstract   = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
  address    = {Hanover, MA, USA},
  doi        = {10.1561/1500000019},
  issue_date = {April 2009},
  numpages   = {57},
  publisher  = {Now Publishers Inc.},
  url        = {https://doi.org/10.1561/1500000019},
}

@InProceedings{Reimers2019,
  author    = {Reimers, Nils and Gurevych, Iryna},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks},
  year      = {2019},
  address   = {Hong Kong, China},
  editor    = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  month     = nov,
  pages     = {3982--3992},
  publisher = {Association for Computational Linguistics},
  abstract  = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  doi       = {10.18653/v1/D19-1410},
  url       = {https://aclanthology.org/D19-1410},
}

@InProceedings{Ding2024,
  author    = {Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  title     = {{L}ong{R}o{PE}: Extending {LLM} Context Window Beyond 2 Million Tokens},
  year      = {2024},
  editor    = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  month     = {21--27 Jul},
  pages     = {11091--11104},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {235},
  abstract  = {Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations. Code is available at https://github.com/microsoft/LongRoPE},
  pdf       = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/ding24i/ding24i.pdf},
  url       = {https://proceedings.mlr.press/v235/ding24i.html},
}

@InProceedings{Wang2023a,
  author    = {Wang, Liang and Yang, Nan and Wei, Furu},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  title     = {Query2doc: Query Expansion with Large Language Models},
  year      = {2023},
  address   = {Singapore},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  pages     = {9414--9423},
  publisher = {Association for Computational Linguistics},
  abstract  = {This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3{\%} to 15{\%} on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.},
  doi       = {10.18653/v1/2023.emnlp-main.585},
  url       = {https://aclanthology.org/2023.emnlp-main.585},
}

 
@Article{Hanslmayr2016,
  author    = {Hanslmayr, Simon and Staresina, Bernhard P. and Bowman, Howard},
  journal   = {Trends in Neurosciences},
  title     = {Oscillations and Episodic Memory: Addressing the Synchronization/Desynchronization Conundrum},
  year      = {2016},
  issn      = {0166-2236},
  month     = jan,
  number    = {1},
  pages     = {16--25},
  volume    = {39},
  doi       = {10.1016/j.tins.2015.11.004},
  publisher = {Elsevier BV},
}

 
@Article{Griffiths2019,
  author    = {Griffiths, Benjamin J. and Parish, George and Roux, Frederic and Michelmann, Sebastian and van der Plas, Mircea and Kolibius, Luca D. and Chelvarajah, Ramesh and Rollings, David T. and Sawlani, Vijay and Hamer, Hajo and Gollwitzer, Stephanie and Kreiselmeyer, Gernot and Staresina, Bernhard and Wimber, Maria and Hanslmayr, Simon},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Directional coupling of slow and fast hippocampal gamma with neocortical alpha/beta oscillations in human episodic memory},
  year      = {2019},
  issn      = {1091-6490},
  month     = oct,
  number    = {43},
  pages     = {21834--21842},
  volume    = {116},
  doi       = {10.1073/pnas.1914180116},
  publisher = {Proceedings of the National Academy of Sciences},
}

@InProceedings{Shen2024,
  author    = {Shen, Tao and Long, Guodong and Geng, Xiubo and Tao, Chongyang and Lei, Yibin and Zhou, Tianyi and Blumenstein, Michael and Jiang, Daxin},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
  title     = {Retrieval-Augmented Retrieval: Large Language Models are Strong Zero-Shot Retriever},
  year      = {2024},
  address   = {Bangkok, Thailand},
  editor    = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  month     = aug,
  pages     = {15933--15946},
  publisher = {Association for Computational Linguistics},
  abstract  = {We propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Large language model as Retriever (LameR), is built upon no other neural models but an LLM in a retrieval-augmented retrieval fashion, while breaking brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query{'}s in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. As a part of the prompts, they are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever, the LLM-based query augmentation becomes less effective as the retriever bottlenecks the whole pipeline. Therefore, we propose to leverage a non-parametric lexicon-based method (e.g., BM25) as the retrieval module to capture query-document overlap in a literal fashion. As such, LameR makes the retrieval procedure transparent to the LLM, thus circumventing the bottleneck.},
  doi       = {10.18653/v1/2024.findings-acl.943},
  url       = {https://aclanthology.org/2024.findings-acl.943},
}

@Misc{Wang2021,
  author        = {Sinong Wang and Han Fang and Madian Khabsa and Hanzi Mao and Hao Ma},
  title         = {Entailment as Few-Shot Learner},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2104.14690},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2104.14690},
}

@Comment{jabref-meta: databaseType:bibtex;}
