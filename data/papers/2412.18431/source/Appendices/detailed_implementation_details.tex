\section{More Implementation Details}
\label{appendix:detailed_implementation_details}



\subsection{Baselines Details}
\label{appendix:retrievers_implementation_details}
We implement all proposed approaches using Elasticsearch\footnote{\url{https://www.elastic.co}}. For SBERT, we employ the \texttt{all-mpnet-base-v2} model with approximate k-nearest neighbours and cosine similarity for vector comparisons. In IRCoT experiments, we evaluate both ColBERTv2 and BM25 retrievers â€” ColBERTv2 for alignment with HippoRAG's baselines, and BM25 for consistency with the original IRCoT implementation.

For all multi-step approaches, including ours, we follow \citeauthor{Gutierrez2024} with respect to the maximum number of retrieval iterations, which vary based on the hop requirements of each dataset. Thus, we use a maximum of 4 iterations for MuSiQue and 2 iterations for HotpotQA and 2Wiki.

\subsection{\gear Details}
\gear involves several hyperparameters, such as the beam size inside graph expansion. 
We randomly sampled $500$ questions from the MuSiQue development set, which we ensure not to overlap with the relevant test set. We select our hyperparameters based on this sample without performing a grid search across all possible configurations. Our goal is to demonstrate our method is ability to achieve state-of-the-art results without extensive parameter tuning. We acknowledge that a more thorough hyperparameter tuning may result in further improvements.


The initial retrieval phase utilises the chunks index $\mathbf{C}$ as the information source, while leaving the triple index $\mathbf{T}$ unused. Our graph expansion component implements beam search with length 2, width 10, and 100 neighbours per beam. The hyperparameter $\gamma$ employed in diverse triple beam search is set to twice the beam search width. For the scoring function, we use the cosine similarity score and the SBERT embedding model.

For the single-step configurations (i.e. any base retriever with NaiveGE or SyncGE), we set the base retriever's maximum number of returned chunks to match our evaluation recall threshold. With the multi-step setup, we maintain a consistent maximum of 10 retrieved chunks before knowledge synchronisation for the purpose of matching IRCoT's implementation. While this 10-chunk limitation applies to individual retrieval rounds, please note that the total number of accessible chunks can exceed this threshold through graph expansion and multiple \gear iterations.


\paragraph{\textrm{\texttt{passageLink}} Details\label{appendixpara:passage_link}}
We use \texttt{passageLink} to link each triple $t_j \in \mathcal{G}^{(n)}$ to its corresponding passages in $\mathbf{C}$ by running a retrieval step as follows:
\begin{align}
\mathbf{C}_{t_j} = h^k_{\text{base}}\left( t_j, {\mathbf{C} \cup \mathbf{T}} \right),
\end{align}where $j \in \left \{1, \dots, \vert\mathcal{G}^{(n)}\vert \right \}$ and $h^k_{\text{base}}\left( t_j, {\mathbf{C} \cup \mathbf{T}} \right)$ is the RRF of passages returned by both $\mathbf{T}$ and $\mathbf{C}$ when queried with $t_j$ (as defined in Eq.~\ref{eq:triple_passage_index_retrieve}).
