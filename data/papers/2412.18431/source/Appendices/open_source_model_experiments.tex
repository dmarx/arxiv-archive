\section{Compatibility with Open-weight Models}
\label{appendix_sec:open_source_model_experiments}

\paragraph{\gear Results}
As shown in Table \ref{tab:open_source_recall}, we evaluate \gear using popular 7-8B parameter open-weight models, comparing them against a closed-source alternative. On HotpotQA, Llama-3.1-7B surpasses the closed-source alternative, achieving higher recall rates at R@10 and R@15. For MuSiQue and 2Wiki, while the closed-source model maintains a slight superior edge in performance, the margin is narrow. Importantly, all tested open-weight models consistently outperform the previous state-of-the-art, HippoRAG w$/$IRCoT. This decouples \gear from the need to use closed-source models, suggesting that state-of-the-art multi-step retrieval can be achieved using more accessible models.

\begin{table*}[tbhp]
\small
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l L{2cm} ccc ccc ccc}
\toprule
& \multirow{2.5}{*}{\textbf{LLM}} & \multicolumn{3}{c}{\textbf{MuSiQue}} & \multicolumn{3}{c}{\textbf{2Wiki}} & \multicolumn{3}{c}{\textbf{HotpotQA}} \\ 
\cmidrule{3-11}
& & R@5 & R@10 & R@15 & R@5 & R@10 & R@15 & R@5 & R@10 & R@15 \\ 
\midrule
\multirow{1}{*}{\textbf{Closed-source}} 
& GPT-4o mini & $\mathbf{58.4}$ & $\mathbf{67.6}$ & $\mathbf{71.5}$ & $\mathbf{89.1}$ & $\mathbf{95.3}$ & $\mathbf{95.9}$ & $\mathbf{93.4}$ & $96.8$ & $97.3$ \\ 
\midrule
\multirow{2}{*}{\textbf{Open-weight}}
& Llama-3.1-7B & $52.4$ & $62.3$ & $66.7$ & $81.6$ & $91.0$ & $93.7$ & $92.2$ & $\textbf{97.4}$ & $\textbf{98.1}$ \\ 
& Qwen-2.5-8B & $53.7$ & $63.7$ & $66.7$ & $85.9$ & $91.6$ & $93.0$ & $91.7$ & $96.2$ & $96.9$ \\ 
\bottomrule
\end{tabular}
}
\caption{Retrieval performance of \gear across different closed-source and open-weight models on MuSiQue, 2Wiki and HotpotQA. Results are reported using Recall@$k$ (R@$k$) metrics for $k \in \left \{5, 10, 15 \right\}$, showing the percentage of questions for which the correct entries are found within the top-$k$ retrieved passages. The included open-weight models are Llama-3.1-8B-Instruct and Qwen-2.5-7B-Instruct, and the closed-source model is GPT-4o mini.}
\label{tab:open_source_recall}
\end{table*}


\begin{table*}[thbp]
\small
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l ccc ccc ccc}
\toprule
& & \multicolumn{3}{c}{\textbf{MuSiQue}} & \multicolumn{3}{c}{\textbf{2Wiki}} & \multicolumn{3}{c}{\textbf{HotpotQA}} \\ 
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
& & R@5 & R@10 & R@15 & R@5 & R@10 & R@15 & R@5 & R@10 & R@15 \\ 
\midrule
\multirow{2}{*}{GPT-4o mini}
& w$/$ diversity & \textbf{48.7} & \textbf{57.7} & \textbf{61.2} & \textbf{72.6} & \textbf{80.9} & \textbf{82.4} & \textbf{87.4} & \textbf{93.3} & \textbf{95.2} \\ 
& w$/$o diversity & 47.0 & 53.9 & 58.4 & 68.2 & 76.0 & 77.4 & 85.0 & 92.2 & 94.3 \\ 
\midrule
\multirow{2}{*}{Llama-3.1-8B-Instruct}
& w$/$ diversity & $\textbf{46.2}$ & $\textbf{54.3}$ & $\textbf{57.4}$ & $\textbf{69.1}$ & $\textbf{78.1}$ & $\textbf{81.6}$ & $\textbf{87.3}$ & $\textbf{92.8}$ & $\textbf{95.1}$ \\ 
& w$/$o diversity & $44.9$ & $52.7$ & $55.0$ & $66.9$ & $75.9$ & $78.2$ & $85.0$ & $91.7$ & $94.4$ \\ 
\bottomrule
\end{tabular}}
\caption{Retrieval performance of the Hybrid + SyncGE method with different LLMs for the \texttt{read} step (see Eq.~\ref{eq:proximal_read}) w$/$ and w$/$o diversity for triple beam search. Results are reported using Recall@$k$ (R@$k$) metrics for $k \in \left \{5, 10, 15 \right\}$, showing the percentage of questions for which the correct entries are found within the top-$k$ retrieved passages.}
\label{tab:diverse_beam_search_expanded}
\end{table*}


\paragraph{Diverse Beam Search Results}
\label{appendix_sec:diverse_beam_search_results_expanded}Expanding upon Table \ref{tab:diverse_beam_search}, Table \ref{tab:diverse_beam_search_expanded} demonstrates that diverse beam search consistently improves retrieval performance across both closed-source and open-weight models when using our proposed Hybrid + SyncGE setup. This further confirms the broader applicability of this approach.
