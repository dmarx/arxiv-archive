\section{Dataset Choices and Statistics}
\label{appendix:dataset_stats}

\begin{table}[ht]
\centering
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
& \textbf{MuSiQue} & \textbf{2Wiki} & \textbf{HotpotQA} \\ \midrule
Split Source & IRCoT & IRCoT & HippoRAG \\ \midrule
\# Hops  & $2-4$ & $2$ & $2$ \\
\# Documents & $139,416$ & $430,225$ & $9,221$ \\
\# Test Queries & $500$ & $500$ & $1,000$\\ \midrule
\# Chunks ($\mathbf{C}$) & $148,793$  & $490,454$ & $10,293$ \\
\# Triples ($\mathbf{T}$) & $1,521,136$  & $4,993,637$ & $122,492$ \\
Av. \# $\mathbf{T}/\mathbf{C}$ & $10.2$  & $10.2$ & $11.9$ \\
\bottomrule
\end{tabular}
\caption{Dataset characteristics and preprocessing statistics, where triples are extracted from chunks, and Av. \# $\mathbf{T}$$/$$\mathbf{C}$ represents the average number of triples per chunk.}
\label{tab:dataset statistics}
\end{table}

Table \ref{tab:dataset statistics} serves as a summary of various facts and statistics related to the employed datasets and the chunking and triple extraction process introduced in Section \ref{sec:preliminaries}.


\paragraph{Reasoning behind dataset split choices}
For MuSiQue and 2Wiki, we use the data provided by \citeauthor{Trivedi2023}, including the full corpus and sub-sampled test cases for each dataset. To limit the experimental cost for HotpotQA, we follow \citeauthor{Gutierrez2024} setting where both the corpus and test split are smaller than IRCoT's counterpart.

\paragraph{Reasoning behind retrieval metrics}
\label{appendix:reasoning_behind_retrieval_metrics}
Our evaluation employs recall at ranks 5, 10, and 15 (R@5, R@10, R@15). While previous work like HippoRAG evaluate R@2, we choose higher rank thresholds since many questions in MuSiQue require information from more than two documents. Additionally, given modern LLMs' expanding context length capabilities \cite{Ding2024}, examining recall beyond R@5 (HippoRAG's highest evaluated rank) provides valuable insights. Following IRCoT's approach, we measure up to R@15 and include R@10 as an intermediate point, offering a comprehensive view of model performance across retrieval depths.
