\section{Experimental Setup}

We evaluate our proposed framework on three multi-hop QA datasets in the open-domain setting: \textbf{MuSiQue} (answerable subset) \cite{Trivedi2022}, \textbf{HotpotQA} \cite{Yang2018}, and \textbf{2WikiMultiHopQA} (2Wiki) \cite{Ho2020}.
For MuSiQue and 2Wiki, we use the data splits provided in IRCoT \cite{Trivedi2023}, while for HotpotQA we follow the same data setting as in HippoRAG \cite{Gutierrez2024}. Dataset-specific statistics can be found in Appendix \ref{appendix:dataset_stats}.

We measure both retrieval and QA performance, with our primary contributions focused on the retrieval component. For retrieval evaluation, we use Recall@$k$ (R@$k$) metrics for $k \in \left \{5, 10, 15\right \}$, showing the percentage of questions where the correct entries are found within the top-$k$ retrieved passages. We include an analysis about the selected recall ranks in Appendix \ref{appendix:reasoning_behind_retrieval_metrics}. Following standard practices, QA performance is evaluated with Exact Match (EM) and F1 scores \cite{Trivedi2023}.


\begin{table*}[t]
\small
\centering
\small
\begin{tabular}{@{}l@{\hspace{2pt}}lccccccccc@{}}
\toprule
& \multirow{2.5}{*}{\textbf{Retriever}} & \multicolumn{3}{c}{\textbf{MuSiQue}} & \multicolumn{3}{c}{\textbf{2Wiki}} & \multicolumn{3}{c}{\textbf{HotpotQA}}\\ 
\cmidrule{3-11}
& & R@5 & R@10 & R@15 & R@5 & R@10 & R@15 & R@5 & R@10 & R@15 \\ \midrule
\multirow{11}{*}{\parbox{2cm}{\textbf{Single-step\\Retrieval}}}
& ColBERTv2 & $39.4$ & $44.8$ & $47.7$ & $59.1$ & $64.3$ & $66.2$ & $79.3$ & $87.1$ & $90.1$ \\
& HippoRAG & $41.0$ & $47.0$ & $51.4$ & $\mathbf{75.1}$ & $\mathbf{83.2}$ & $\mathbf{86.4}$ & $79.8$ & $89.0$ & $92.4$ \\ 
& BM25 & $33.8$ & $38.5$ & $41.3$ & $59.5$ & $62.7$ & $64.1$ & $74.2$ & $83.6$ & $86.3$ \\ 
& \hspace{2mm} + NaiveGE & $37.5$ & $45.5$ & $48.4$ & $65.0$ & $70.7$ & $71.8$ & $79.1$ & $89.1$ & $91.9$ \\ 
& \hspace{2mm} + SyncGE & $\underline{44.7}$ & $\underline{52.6}$ & $\underline{57.4}$ & $70.5$ & $76.1$ & $79.3$ & $\underline{87.4}$ & $\underline{93.0}$ & $\underline{94.0}$ \\ 
& SBERT & $31.1$ & $37.9$ & $41.6$ & $41.2$ & $48.1$ & $51.5$ & $72.1$ & $79.3$ & $84.0$ \\
& {\hspace{2mm} + NaiveGE} & $32.2$ & $41.4$ & $45.4$ & $45.1$ & $54.0$ & $57.3$ & $76.1$ & $84.7$ & $88.8$ \\
& \hspace{2mm} + SyncGE & $41.6$ & $51.3$ & $54.2$ & $54.8$ & $64.9$ & $70.7$ & $84.1$ & $89.6$ & $92.8$ \\ 
& Hybrid & $39.9$ & $46.3$ & $49.1$ & $60.0$ & $65.8$ & $66.6$ & $77.8$ & $85.8$ & $89.7$ \\
& \hspace{2mm} + NaiveGE & $41.8$ & $49.4$ & $53.0$ & $63.0$ & $70.8$ & $72.6$ & $80.6$ & $89.4$ & $92.7$ \\
& {\hspace{2mm} + SyncGE} & $\mathbf{48.7}$ & $\mathbf{57.7}$ & $\mathbf{61.2}$ & $\underline{72.6}$ & $\underline{80.9}$ & $\underline{82.4}$ & $\mathbf{87.4}$ & $\mathbf{93.3}$ & $\mathbf{95.2}$ \\ 
\midrule
\multirow{4}{*}{\parbox{2cm}{\textbf{Multi-step}\\ \textbf{Retrieval}}}
& IRCoT (BM25) & $46.1$ & $\underline{54.9}$ & $57.9$ & $67.9$ & $75.5$ & $76.1$ & $87.0$ & $92.6$ & $92.9$ \\
& IRCoT (ColBERTv2) & $47.9$ & $54.3$ & $56.4$ & $60.3$ & $86.6$ & $69.7$ & $86.9$ & $92.5$ & $92.8$ \\
& HippoRAG w$/$ IRCoT
& $\underline{48.8}$ & $54.5$ & $\underline{58.9}$ & $\underline{82.9}$ & $\underline{90.6}$ & $\underline{93.0}$ & $\underline{90.1}$ & $\underline{94.7}$ & $\underline{95.9}$ \\
& \gear & $\mathbf{58.4}$ & $\mathbf{67.6}$ & $\mathbf{71.5}$ & $\mathbf{89.1}$ & $\mathbf{95.3}$ & $\mathbf{95.9}$ & $\mathbf{93.4}$ & $\mathbf{96.8}$ & $\mathbf{97.3}$ \\ \bottomrule
\end{tabular}
 \caption{Retrieval performance for single- and multi-step retrievers on MuSiQue, 2Wiki, and HotpotQA. Results are reported using Recall@$k$ (R@$k$) metrics for $k \in \left \{5, 10, 15\right \}$, showing the percentage of questions where the correct entries are found within the top-$k$ retrieved passages.}
 \label{tab:recall_main_table}
\end{table*}

\subsection{Baselines}
We evaluate \gear against strong, multi-step baselines, including IRCoT \cite{Trivedi2023} and a combination of HippoRAG w$/$ IRCoT \cite{Gutierrez2024} which, similar to our framework, includes a graph-retrieval component and a multi-step agent. To showcase the benefits of our graph retriever (i.e. SyncGE), we evaluate it against several stand-alone, single-step retrievers: \begin{inparaenum}[(i)]\item BM25, \item Sentence-BERT (SBERT), \item a hybrid approach that combines BM25 and SBERT results through RRF and \item HippoRAG\end{inparaenum}. Throughout the experiments, we refer to the single-step setup when an approach does not support several iterations and is not equipped with an LLM agent.





\subsection{Implementation Details}
To maintain consistency and validity in comparisons with the baselines on the splits used in this study, we conducted all experiments locally using their corresponding codebases.

In addition to our proposed single-step retriever, SyncGE, we evaluate a more \textit{naive} implementation of GE (i.e. NaiveGE) in order to explore the generality of the method when in resource-constrained setting, where no LLM is involved. In NaiveGE, we use all triples that are associated with $\mathbf{C}_\mathbf{q}'$ (see Section~\ref{sec:graph_retrieval}) for diverse triple beam search.


For all models using an LLM, we employ GPT-4o mini (\texttt{gpt-4o-mini-2024-07-18}) as the backbone model with a temperature of 0, both for offline triple extraction (i.e. how the $\mathbf{T}$ index in Section~\ref{sec:preliminaries} is formed) and online retrieval operations. Our triple extraction prompt (in Appendix \ref{sec:offline_prompts}) is adapted\footnote{Our approach uses a modified version of HippoRAG's triple extraction prompt that combines entity and triple extraction into a single step, while incorporating an additional demonstration and updated in-context examples.} from the ones used by \citeauthor{Gutierrez2024}. To ensure a fair comparison against \citeauthor{Gutierrez2024}, the closest work to ours, we run experiments with HippoRAG using our prompting setup\footnote{For transparency, we also compare against HippoRAG's original triple extraction prompt in Appendix \ref{appendix_sec:hipporag_results_original_prompt}, where we observe only minor differences across the two configurations.} for triple extraction. For evaluating QA performance, we use the prompts provided in Appendix~\ref{subsec:online_qa_prompts}. Further implementation details are provided in Appendix~\ref{appendix:detailed_implementation_details}.
