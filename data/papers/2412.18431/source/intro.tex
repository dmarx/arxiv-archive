\section{Introduction}
\label{sec:intro}
Retrieval-augmented Generation (RAG) has further enhanced the remarkable success of Large Language Models (LLMs) \cite{OpenAI2024} in Question Answering (QA) tasks \cite{Lewis2020}.
Multi-hop QA usually requires reasoning capabilities across several passages or documents. A relevant example is displayed in Table~\ref{tab:qa_example} where reaching the appropriate answer requires building a $3$-hop reasoning chain starting from the main entity in the question (i.e. ``Stephen Curry'').

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{R{2.2cm}L{1.3cm}L{2.3cm}}\toprule
\multicolumn{3}{L{7.1cm}}{What year did the father of Stephen Curry join the team from which he started his college basketball career?} \\\midrule
$\text{Stephen Curry}$ & \textcolor{blue}{$\xrightarrow{\text{son of}}$} & $\text{Dell Curry}$\\
         $\text{Dell Curry}$ & \textcolor{blue}{$\xrightarrow{\text{college team}}$} & $\text{Virginia Tech}$\\
         $\text{Dell Curry}$ & \textcolor{blue}{$\xrightarrow{\text{college start}}$} & $\text{1982}$ (answer)\\\bottomrule
    \end{tabular}
    \caption{Multi-hop question (top) involving a reasoning chain (bottom) extending across several entities.}
    \label{tab:qa_example}
\end{table}




More recently, existing methods sought to leverage graph representations of the retrieved passages in order to bridge the semantic gap introduced by multi-hop questions \cite{Fang2024,Li2024,Edge2024,Gutierrez2024,Liang2024}. Most of these approaches employ an LLM to traverse a graph involving the entities appearing in the corresponding textual passages. However, within the context of RAG, this typically leads to long and interleaved prompts that require multiple LLM iterations to arrive at answers involving distant reasoning hops~\cite{Trivedi2023}. Several recent approaches build graphs associating passages with each other by extracting entities and atomic facts or semantic triples from passages in a separate offline step \cite{Li2024,Fang2024,Gutierrez2024}. Furthermore, GraphReader uses an LLM agent, with access to graph-navigating operations
% (i.e. provided as in-context inputs)
for exploring the resulting graph \cite{Li2024}. TRACE relies on an LLM to iteratively select triples to construct reasoning chains, which are then used for grounding the answer generation directly, or for filtering out irrelevant documents from an original set of retrieved results \cite{Fang2024}.

In this paper, we present \gear, a \underline{\textbf{G}}raph-\underline{\textbf{e}}nhanced \underline{\textbf{A}}gent for \underline{\textbf{R}}etrieval-augmented generation. During the offline stage, we \textit{align} an index of passages with an index of triples extracted from these passages. With such alignment, passages are intermediately connected through graphs of triples. \gear contains a graph-based passage retrieval component referred to as SyncGE. Differentiating from previous works that rely on expensive LLM calls for graph exploration, we leverage an LLM for locating initial nodes (triples) and employ a generic semantic model to expand the sub-graph of triples by exploring diverse beams of triples. Furthermore, \gear utilises multi-hop contexts retrieved by SyncGE and constructs a memory that summarises information for multi-step retrieval. 


Our work refines the neurobiology-inspired paradigm proposed by \citeauthor{Gutierrez2024}, by modelling the communication between hippocampus and neocortex when forming an episodic memory. An array of \textit{proximal triples}, in our design, functions as a gist of memory learnt through hippocampus within one or a few shots (iterations), which is projected back to neocortex for the later recall stages \cite{Hanslmayr2016,Griffiths2019}. We highlight the complementary potential of our graph retrieval approach and an LLM, which, within our system, assimilates the synergy between the hippocampus and neocortex, offering insights from a biomimetic perspective.


We evaluate the retrieval performance of \gear on three multi-hop QA benchmarks: MuSiQue, HotpotQA, and 2WikiMultihopQA. \gear pushes the state of the art, achieving significant improvements in both single- and multi-step retrieval settings, with gains exceeding $10\%$ on the most challenging MuSiQue dataset. Furthermore, we demonstrate that our framework can address multi-hop questions in fewer iterations with significantly fewer LLM tokens. Even in the case of a single iteration, \gear offers a more efficient alternative to other iterative retrieval methods, such as HippoRAG w$/$ IRCoT. Our contributions can be summarised as follows:
\begin{itemize}
\item We introduce a novel graph-based retriever, SyncGE, which leverages an LLM for locating initial nodes for graph exploration and subsequently expands them by diversifying beams of triples that link multi-hop passages.
\item We incorporate this graph retrieval method within an LLM-based agent framework, materialising \gear, achieving state-of-the-art retrieval performance across three datasets.
\item We conduct comprehensive experiments showcasing the synergetic effects between our proposed graph-based retriever and the LLM within the \gear framework.
\end{itemize}
