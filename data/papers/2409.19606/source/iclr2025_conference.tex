
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
% \DeclareMathOperator{\Tr}{Tr}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{pifont}

\usepackage{multirow}
\usepackage{makecell}

\usepackage{colortbl}
\usepackage[normalem]{ulem} % For strikethrough

% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{overpic}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{rotating}

\usepackage{xcolor}
\definecolor{customgreen}{RGB}{0,128,0}
\definecolor{customblue}{RGB}{30,144,255}
\definecolor{customyellow}{RGB}{255,215,0}

\usepackage{etoolbox}
\makeatletter
\AfterEndEnvironment{algorithm}{\let\@algcomment\relax}
\AtEndEnvironment{algorithm}{\kern2pt\hrule\relax\vskip3pt\@algcomment}
\let\@algcomment\relax
\newcommand\algcomment[1]{\def\@algcomment{\footnotesize#1}}
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
  \def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
  \def\@fs@post{}%
  \def\@fs@mid{\kern2pt\hrule\kern2pt}%
  \let\@fs@iftopcapt\iftrue}
\makeatother

\title{Hyper-Connections}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu,\\
\textbf{Qiyang Min, Xun Zhou} \\
Seed-Foundation-Model Team, ByteDance\\
\texttt{\{zhudefa,huanghongzhi.51,huangzihao.notabot,yutao.zeng,} \\
\texttt{maoyunyao.myy,wubanggu,minqiyang,zhouxun\}@bytedance.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\newtext}[1]{\textcolor{red}{#1}}
\renewcommand{\newtext}[1]{#1}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.

\end{abstract}


\section{Introduction}
\begin{figure}[h]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/train_loss_olmoe_exp_099.pdf}
        
    \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/OLMoE_1B7B_c4_en_val_loss.pdf}

     \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/OLMoE_1B7B_HellaSwag_Acc.pdf}

     \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/OLMoE_1B7B_ARC_Challenge_Acc.pdf}
    \end{minipage}
    \caption{The performance of the baseline model \texttt{OLMoE-1B-7B} and the model with hyper-connections, \texttt{OLMoE-1B-7B-DHC$\times$4}. \textbf{(1)} and \textbf{(2)} show the training loss (0.99 EMA smoothed) and the C4-en validation loss, respectively. Our method converges 1.8 times faster compared to the baseline and maintains a significant advantage at the 500B tokens. \textbf{(3)} and \textbf{(4)} show the accuracy curves on \texttt{HellaSwag} and \texttt{ARC-Challenge}, demonstrating the superior performance of the \texttt{OLMoE-1B-7B-DHC$\times$4} model.}
    \label{fig:olmoe_curves}
\end{figure}
Deep learning has achieved tremendous success across various domains, where residual connections \citep{he2016deep} have been instrumental in contemporary neural network architectures, including transformers and CNNs. Residual connections help mitigate the problem of gradient vanishing, enabling the effective training of very deep networks. However, it is important to acknowledge that residual connections are not infallible solutions and still present limitations that remain unresolved.

The two main variants of residual connections, Pre-Norm and Post-Norm, each make distinct trade-offs between gradient vanishing and representation collapse. Pre-Norm applies normalization operations to the input before each residual block, effectively addressing the problem of gradient vanishing~\citep{bengio1994learning,glorot2010understanding}. However, it can also lead to the issue of collapse in deep representations~\citep{liu2020understanding}, where hidden features in deeper layers become highly similar, diminishing the contribution of additional layers as their number increases. 
\newtext{In contrast, Post-Norm applies normalization after the output of each residual block, reducing the influence of a hidden state on subsequent layers.} This approach can alleviate the issue of representation collapse but also reintroduces the problem of vanishing gradients. The vanishing gradient and the representation collapse are like two ends of a seesaw, with these two variants making respective trade-offs between these issues. The key issue is that residual connections, including both Pre-Norm and Post-Norm variants, predefine the strength of connections between the output and input within a layer.

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=1\textwidth]{fig/hc_overview.pdf}
    \end{center}
  \caption{\newtext{{\bf Hyper-connections (HC) with an expansion rate of $n=2$.} (a) Residual connections. (b) Hyper-connections: $\beta_1$, $\beta_2$, $\alpha_{0,0}$, $\alpha_{0,1}$, $\alpha_{1,0}$, $\alpha_{1,1}$, $\alpha_{2,1}$, and $\alpha_{2,2}$ are learnable scalars or scalars predicted by the network , depending on the specific HC version. These connections enable lateral information exchange and vertical integration of features across depths. The Transformer with HC is shown in Fig.~\ref{fig:trans_with_hc}. They can be decoupled into depth-connections and width-connections. (c) Depth-connections perform a weighted sum between the layer output and the hidden vector $h_1$. (d) Width-connections allow information exchange between the hidden vectors $h_1$ and $h_2$.}}
    \label{fig:hc_overview}
\end{figure}

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    % \includegraphics[width=0.36\textwidth]{fig/compare_hidden_sim_curves.pdf}
    
    \begin{overpic}[abs,unit=1mm,scale=.25,width=0.36\textwidth]{fig/compare_hidden_sim_curves.pdf}
        \put(0,17){\small\begin{turn}{90} $\texttt{cos}(\mathbf{h}_0^i, \mathbf{h}_0^{i+1})$ \end{turn}}
        \put(20,0){\small Layer Index $i$}
    \end{overpic}
    
    \caption{Cosine similarity between the input of the current layer and the previous layers for the \texttt{OLMo-1B} Model \citep{groeneveld2024olmo}. The curve represents the median of similarity, while the shaded area indicates the range between the 5th and 95th percentiles. The red curve shows the model with Pre-Norm, and the blue curve shows that with hyper-connections. }
    \label{fig:compare_hidden_sim_curve}
\end{wrapfigure}

Driven by the limitations of residual connections, an important question arises: \textit{Can neural networks autonomously learn the optimal strength of connections to improve performance?} To address this, we propose hyper-connections (HC), which lead to significantly improved performance with a negligible increase in computation and parameters. We will show that both Post-Norm and Pre-Norm variants can be expressed as specific non-trainable forms of hyper-connections, as discussed in \S~\ref{sec:residual}.


% 简介 hyper-connections 的原理
\newtext{The core idea of hyper-connections (HC) is to propose learnable \textit{depth-connections} and \textit{width-connections}, as depicted in Fig.\ref{fig:hc_overview} (b). These connections flexibly integrate features vertically across depths, compared to the residual connections shown in Fig.\ref{fig:hc_overview} (a). Depth-connections can be considered as a generalized residual connections, assigning weights to the connections between the inputs and outputs of each layer. To enable the network to model different depth-connections simultaneously, we expand the network's input into $n$ copies, each having its own depth connection, as shown in Fig.~\ref{fig:hc_overview} (b). This design allows 
 multiple hidden vectors to reserve multiple patterns connecting preceding layers, as shown in \S~\ref{sec:visualization}. Moreover, we establish width connections between the $n$ hidden vectors, allowing information exchange between hidden vectors within the same layer, as shown in Fig.~\ref{fig:hc_overview} (b). We argue that $n$ ($>1$) hidden states are necessary. As analyzed in Appendix~\ref{app:more_visulization}, the seesaw effect persists when $n=1$, and experiments show that it does not improve performance, as shown in Fig.~\ref{fig:1b_500B_training_loss}. In contrast, when $n>1$, hyper-connections can not only learn to adjust the strength of residuals but also rearrange layers, either sequentially or in parallel, as discussed in \S~\ref{seq_par_dual}. To further enhance flexibility, we introduce dynamic hyper-connections (DHC), enabling the network to adjust connection weights according to the input.
Notably, although HC seem to increase the network's width by $n$ times, the additional parameters and computational cost are almost negligible, as analyzed in Appendix~\ref{app:para_comp_mem_analysis}. The Transformer with HC is shown in Fig.~\ref{fig:trans_with_hc}.}


Our research, primarily centered on large language models (LLMs) pre-training, also extends to visual generation and classification tasks. Using Pre-Norm as a baseline, we demonstrate the significant benefits of hyper-connections, including 1B and 7B dense models as well as 7B MoE models, as detailed in \S~\ref{sec:exp_language_modeling}. The benefits are particularly prominent for OLMoE~\citep{muennighoff2024olmoeopenmixtureofexpertslanguage} as presented in Fig.\ref{fig:olmoe_curves}. The model utilizing DHC converges \textbf{1.8} times faster and shows an improvement of \textbf{6} points on ARC-Challenge compared to the baseline trained with 500 B tokens. According to our visualization analysis, as shown in Fig.\ref{fig:compare_hidden_sim_curve}, the baseline model tends toward representation collapse, characterized by high similarity between features of adjacent layers. In contrast, models with HC exhibit significantly lower similarity between features across adjacent layers and a wider range of similarities. This suggests that HC enhance the impact of each layer. Further discussion is provided in \S\ref{sec:visualization} and in Appendix~\ref{app:more_visulization}. 
These compelling pieces of evidence demonstrate the generality of the hyper-connections principle, and we anticipate their applicability in numerous other AI challenges.






\section{Method}
\subsection{Static Hyper-Connections}
Consider the hidden vector $\mathbf h^{k-1} \in \mathbb{R}^d$ (or $\mathbf h^{k-1} \in \mathbb{R}^{d \times 1}$) as the input to the $k$-th layer, with the initial input $\mathbf h^0$ to the network. Initially, $\mathbf h^0 \in \mathbb{R}^d$ is replicated $n$ times to form the initial \textit{hyper hidden matrix}
$\mathbf{H}^0 = \begin{pmatrix} \mathbf h^0 & \mathbf h^0 & \dots & \mathbf h^0 \end{pmatrix}^\intercal \in \mathbb{R}^{n\times d}$.  Here,
$n$ is the expansion rate.
For the $k$-th layer, the input consists of the hyper hidden matrix from the previous layer $\mathbf H^{k-1} = \begin{pmatrix}\mathbf h^{k-1}_1 & \mathbf h^{k-1}_2 & \dots & \mathbf h^{k-1}_n\end{pmatrix}^\intercal \in \mathbb{R}^{n\times d}$. Finally, we sum the last hyper hidden matrix row-wise to obtain the required hidden vector, which is then passed through a final projector to produce the final output of the network (i.e., a normalization layer and an unembedding layer in transformers). To simplify the notation in subsequent analysis, we omit the layer index and simply denote the hyper-hidden matrix as $\mathbf H = \begin{pmatrix}\mathbf h_1 & \mathbf h_2 & \dots & \mathbf h_n\end{pmatrix}^\intercal$.

The hyper-connections (HC) can be represented by a matrix $\mathcal{HC}$, where each element defines the connection weight. The matrix is structured as follows:

\begin{equation}
\label{eq:shc}
\mathcal{HC}
 = \begin{pmatrix}
\mathbf{0}_{1\times1} & \mathbf{B} \\
\mathbf{A_m} & \mathbf{A_r}
\end{pmatrix}
= \begin{pmatrix}
0 & \beta_{1} & \beta_{2} & \cdots & \beta_{n} \\
\alpha_{1,0} & \alpha_{1,1} & \alpha_{1,2} & \cdots & \alpha_{1, n} \\
\alpha_{2,0} & \alpha_{2,1} & \alpha_{2,2} & \cdots & \alpha_{2, n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\alpha_{n,0} & \alpha_{n,1} & \alpha_{n,2} & \cdots & \alpha_{n, n}
\end{pmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}.
\end{equation}

Consider a network layer $\mathcal{T}$, it integrates self-attention layers and feed-forward networks within transformers. The output of the HC, denoted by $\mathbf{\hat{H}}$, can be simply formulated as follows:
\begin{equation}
\mathbf{\hat{H}} = \mathcal{HC}(\mathcal{T}, \mathbf{H}) \\
=\mathbf{B}^\intercal\mathcal{T}(\mathbf{H}^\intercal\mathbf{A_m})^\intercal + \mathbf{A_r}^\intercal\mathbf{H}. \label{eq:hc_recurrent_form}
\end{equation}


We use $\mathbf{A_m}$ as weights to perform a weighted sum on the input $\mathbf H = \begin{pmatrix}\mathbf h_1 & \mathbf h_2 & \dots & \mathbf h_n\end{pmatrix}^\intercal$ to obtain the input $\mathbf{h}_0$ of the current layer $\mathcal{T}$, which is given by:

\begin{equation}
\mathbf{h}_0^\intercal = \mathbf{A_m}^\intercal \mathbf{H},
\end{equation}

While $\mathbf{A_r}$ is used to connect $\mathbf{H}$ and map it to a hyper hidden matrix $\mathbf{H'}$, as shown below:

\begin{equation}
\mathbf{H'} = \mathbf{A_r}^\intercal \mathbf{H}.
\end{equation}


% Let $\mathbf{h}'_0$ be the output of $\mathcal{T}$, we have:
% \begin{equation} \label{eq:computation}
% \mathbf{h}'_0=\mathcal{T}\mathbf{h}_0.
% \end{equation}

Subsequently, the output is given by:
% \begin{equation} \label{eq:dc}
% \mathbf{\hat{H}}=\mathbf{B}^\intercal \mathbf{h}'^\intercal_0 + \mathbf{H'}.
% \end{equation}
\begin{equation} \label{eq:dc}
\mathbf{\hat{H}}=\mathbf{B}^\intercal (\mathcal{T}\mathbf{h}_0)^\intercal + \mathbf{H'}.
\end{equation}


The \textbf{depth-connections} can be decoupled as the following matrix, which is shown at Fig~\ref{fig:hc_overview} (a):
\begin{equation}
\mathcal{DC} = \begin{pmatrix}
\mathbf{B} \\
\text{diag}(\mathbf{A_r})
\end{pmatrix} = \begin{pmatrix}
\beta_{1} & \beta_{2} & \cdots & \beta_{n} \\
\alpha_{1,1} & \alpha_{2,2}  & \cdots & \alpha_{n, n}
\end{pmatrix} \in \mathbb{R}^{2 \times n},
\end{equation}

where the first row $\mathbf{B}$ represents the weights of the output of the current layer $\mathcal{T}$, and the last row $\text{diag}(\mathbf{A_r})$ represents the weights of the input. We use $\text{diag}(\mathbf{A_r})$ to represent the flatten vector of the diagonal entries of $\mathbf{A_r}$. 

The \textbf{ width-connections} matrix can be defined as follows, which is shown at Fig~\ref{fig:hc_overview} (b):

\begin{equation} \label{eq:wcm}
\mathcal{WC} = \begin{pmatrix}
\mathbf{A_m} & \mathbf{A_r}
\end{pmatrix} \in \mathbb{R}^{n\times (n+1)}.
\end{equation}

The algorithm that employs hyper-connections is presented in Algorithm \ref{alg:hyper_connections}. 



\subsection{Dynamic Hyper-Connections}

The entries of $\mathcal{HC}$ can dynamically depend on the input $\mathbf{H}$, which the matrix representation of dynamic hyper-connections (DHC) is defined as follows:

\begin{equation}
\mathcal{HC}(\mathbf{H}) = \begin{pmatrix}
\mathbf{0}_{1\times 1} & \mathcal{B}(\mathbf{H}) \\
\mathcal{A}_m(\mathbf{H}) & \mathcal{A}_r(\mathbf{H})
\end{pmatrix}
\end{equation}

Similarly, given a layer \(\mathcal{T}\) and input \(\mathbf{H}\), we obtain the output of the DHC as follows:
\begin{equation}
\mathbf{\hat{H}} = \mathcal{HC}(\mathbf H)(\mathcal{T}, \mathbf H).
\end{equation}

In practice, we combine the dynamic and static matrices to achieve DHC. The dynamic parameters are obtained through a linear transformation. To stabilize the training process, we introduce normalization before the linear transformation and apply the tanh activation function after it, scaling it by a small initial learnable factor. 
The following equations detail how these dynamic parameters are computed:

\begin{align}
\label{eq:norm}
\overline{\mathbf{H}} &= \texttt{norm}(\mathbf{H}) \\
\label{eq:B}
\mathcal{B}(\mathbf{H})     &= 
s_\beta \circ \texttt{tanh} (\overline{\mathbf{H}} \mathbf{W}_{\beta})^\intercal + \mathbf{B} \in \mathbb{R}^{1\times n}\\
\label{eq:Am}
\mathcal{A}_{m}(\mathbf{H}) &= 
s_\alpha \circ \texttt{tanh} (\overline{\mathbf{H}} \mathbf{W}_{m}) + \mathbf{A}_m \in \mathbb{R}^{n\times 1} \\
\label{eq:Ar}
\mathcal{A}_{r}(\mathbf{H})     &= 
s_\alpha \circ \texttt{tanh} (\overline{\mathbf{H}} \mathbf{W}_{r}) + \mathbf{A}_r \in \mathbb{R}^{n\times n} 
\end{align}


Our experiments in \S~\ref{sec:exp_language_modeling} demonstrate that dynamic hyper-connections outperform static hyper-connections in language modeling tasks. 
The PyTorch implementations for both the static and dynamic variants of hyper-connections are detailed in Algorithm \ref{alg:torch_hc} and \ref{alg:torch_trans_with_hc}.


\subsection{Initialization}
In order to make the initialization of the hyper-connections equivalent to the Pre-Norm residual connections, we adopt the following initialization strategy. The dynamic parameters $\mathbf{W}_{\beta}$, $\mathbf{W}_{m}$, and $\mathbf{W}_{r}$ in Eqs.~\ref{eq:B}, \ref{eq:Am}, and \ref{eq:Ar} are initialized to 0, while the static matrices are initialized as follows:
\begin{equation}
\label{eq:initialization}
\begin{pmatrix}
\mathbf{0}_{1\times1} & \mathbf{B}^k \\
\mathbf{A_m}^k & \mathbf{A_r}^k
\end{pmatrix}
=\begin{pmatrix}
\mathbf{0}_{1 \times 1} & \mathbf{1}_{1 \times n}\\
\mathbf{e}_{k \bmod n} & \mathbf{e}_{n\times n}
\end{pmatrix},
\end{equation}
where $k$ is the index of the layer. $\bmod$ denotes the modulo operation. 



\section{Why Hyper-Connections}
\label{anlyze}
In this section, we elucidate the rationale behind hyper-connections. We explore how variants of residual connections, namely Pre-Norm and Post-Norm, can be viewed as non-trainable hyper-connections, and introduce the concept of sequential-parallel duality, demonstrating how hyper-connections can dynamically optimize layer arrangements to enhance network performance. A visulize analysis of hyper-connections through an unfolded view is discussed in \S~\ref{sec:visualization}.

\subsection{Residual Connections as Non-trainable Hyper-Connections}
\label{sec:residual}
The Pre-Norm and Post-Norm residual connections can be represented as the following hyper-connections matrices with an expansion rate $n=1$:


\begin{multicols}{2}
  \begin{equation}
  \label{eq:hc_prenorm}
    \mathcal{HC}_{PreNorm}=\begin{pmatrix}
    0 & 1 \\
    1 & 1 \\
    \end{pmatrix},
    \end{equation}
  \break
  \begin{equation}
  \label{eq:hc_postnorm}
  \mathcal{HC}_{PostNorm}=
    \begin{pmatrix}
    0 & \frac{1}{\sqrt{\sigma_{i}^2+\sigma_{o}^2 + 2\sigma_{io}}} \\
    1 & \frac{1}{\sqrt{\sigma_{i}^2+\sigma_{o}^2 + 2\sigma_{io}}} \\
    \end{pmatrix},
  \end{equation}
\end{multicols}
where $\sigma_{i}$ and $\sigma_{o}$ denote the standard deviations of the input and output of the neural network layer, respectively, and $\sigma_{io}$ is the covariance between them.

For Pre-Norm, its hyper-connection matrix is a $2\times 2$ matrix where the bottom right triangular part is filled with $1$ and the rest is a placeholder $0$. For Post-Norm,  the weights depend on the variances and covariance of the input and output, forming a $2\times 2$ matrix. Therefore, their hyper-connection matrices are non-trainable. In this work, we propose hyper-connections that can be $(n+1)\times (n+1)$ matrices, with weights that are trainable or even predicted based on the input. The complete derivation is provided in Appendix~\ref{app:residual}.

\subsection{Sequential-Parallel Duality}
\label{seq_par_dual}
Given a series of neural network modules, we have the option to arrange them either sequentially or in parallel. However, hyper-connections offer an approach that learns to rearrange these layers in a configuration blending both sequential and parallel arrangements.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\textwidth]{fig/s_p_duality.pdf}
    \caption{Sequential and parallel arrangements of hyper-connections with $n=2$. }
    \label{fig:s_p_duality}
\end{figure}

Without loss of generality, we set the expansion rate to $n=2$. 
If the hyper-connections are learned as the following matrix, the neural network will be arranged sequentially:



\begin{equation}
\mathcal{HC}=\begin{pmatrix}
0 & 1 & 1\\
1 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}.
\end{equation}

In this case, the depth connection degenerates into a residual connection, as shown in Fig.~\ref{fig:s_p_duality} (a).

When the hyper-connections for odd and even layers (with layer numbering starting from 1) are defined by the following matrices, the neural network will be arranged in parallel every two consecutive layers, similar to the arrangement of parallel transformer blocks in transformers~\citep{mesh-transformer-jax}, as shown in Fig.~\ref{fig:s_p_duality} (b). The general and complete derivation is provided in Appendix~\ref{app:spd}.

\begin{multicols}{2}
  \begin{equation}
  \mathcal{HC}_{odd}=
    \begin{pmatrix}
    0 & 1 & 0\\
    1 & 1 & 1\\
    1 & 1 & 1
    \end{pmatrix},
  \end{equation}
  \break
  \begin{equation}
  \mathcal{HC}_{even}=
    \begin{pmatrix}
    0 & 0 & 1\\
    0 & 1 & 0\\
    1 & 0 & 1
    \end{pmatrix}.
  \end{equation}
\end{multicols}

Thus, learning the hyper-connection matrix in various forms can create layer arrangements that surpass traditional sequential and parallel configurations, resulting in a soft-mixture or even dynamic arrangement. For static hyper-connections, the layer arrangement within the network remains fixed after training. In contrast, dynamic hyper-connections allow the arrangement to adapt dynamically for each token.





\section{Results}
\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/train_loss_1b_tanh_exp_099.pdf}
        
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/train_loss_1b_linear_exp_099.pdf}
    \end{minipage}
    \caption{Comparison of training loss curves for different expansion rate. The left subfigure includes models with dynamic hyper-connections (DHC) at various expansion rates, while the right subfigure shows the effect of omitting the tanh function. Both subfigures illustrate how increasing the expansion rate leads to improved training loss performance over $500$B tokens. Results are smoothed using an exponential moving average with a coefficient of 0.99.}
    \label{fig:1b_500B_training_loss}
\end{figure}
\label{sec:exp_language_modeling}
\begin{table}[h]
\centering
\caption{Ablation study on  expansion rates $n$ with training on 500 B tokens.}
\begin{tabular}{lccccc}
\toprule
\textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Down Stream\\ Avg, Acc. $\uparrow$\end{tabular}}\\ 
\midrule
OLMo-1B & 2.811 & 18.023 & 2.544 & 14.229 & 62.5 \\
OLMo-1B-DHC$\times$1 \textcolor{gray}{W/O tanh} & 2.822 & 18.270 & 2.556 & 14.428 & 62.3 \\
OLMo-1B-DHC$\times$2 \textcolor{gray}{W/O tanh} & 2.792 & 17.663 & 2.537 & 14.033 & 63.8 \\
OLMo-1B-DHC$\times$4 \textcolor{gray}{W/O tanh} & 2.779 & 17.451 & 2.516 & 13.844 & \textbf{64.4} \\
OLMo-1B-DHC$\times$8 \textcolor{gray}{W/O tanh} & \textbf{2.777} & \textbf{17.425} & \textbf{2.514} & \textbf{13.819} & 63.8 \\
\midrule
OLMo-1B-DHC$\times$1 & 2.819 & 18.125 & 2.556 & 14.418 & 62.3 \\
OLMo-1B-DHC$\times$2 & 2.802 & 17.950 & 2.534 & 14.114 & 63.0 \\
OLMo-1B-DHC$\times$4 & 2.781 & 17.509 & \textbf{2.514} & \textbf{13.826} & \textbf{63.8} \\
OLMo-1B-DHC$\times$8 & \textbf{2.778} & \textbf{17.445} & 2.516 & 13.843 & 62.8 \\
\bottomrule
\end{tabular}
\label{tab:olmo_dhc}
\end{table}
We primarily conduct experiments on pre-training of large language model, including dense and 
Mixture-of-Experts (MoE)~\citep{shazeer2017sparsely} models, and extend to visual generation and classification tasks. Due to space constraints, we include the vision experiments in the Appendix~\ref{sec:vision_exp}.

% 有moe版本
\textbf{Experiment Settings.} We employ the experimental setup outlined by OLMo~\citep{groeneveld2024olmo} for dense models and by OLMoE~\citep{muennighoff2024olmoeopenmixtureofexpertslanguage} for MoE models. \textbf{For dense models}, we use \texttt{dolmap-v1.5-sample}~\citep{soldaini2024dolma} as our training dataset. We conduct ablation studies on $1$B models and assess the effectiveness of our method at the $7$B model scale. \textbf{For MoE models}, we train the \texttt{OLMoE-1B-7B} model, both with and without hyper-connections, on the \texttt{OLMOE-MIX} dataset. These models activate $1.3$B out of a total of $7$B parameters. All experiments are trained on $500$B tokens.

\textbf{Implementation.} We maintain the training configuration of the baseline model, replacing the residual connections with hyper-connections. The static component in Eqs.~\ref{eq:shc}, \ref{eq:B}, \ref{eq:Am}, \ref{eq:Ar} does not utilize weight decay, whereas the dynamic component does. Since the hyper hidden vectors of the final transformer block are ultimately summed, we ensure that the standard deviation (\texttt{std}) of the output (before the final layernorm and unembedding layers) remains consistent with the original. At initialization, we scale the \texttt{std} of the weights of the output module at all layers, including those of the second linear layer of the feedforward network and the output projector of the attention module, by a factor of $\sqrt{n}$, where $n$ represents the expansion rate. \newtext{The parameters and computational overhead introduced by hyper-connections is negligible, see Table.~\ref{tab:params_comparison} and ~\ref{tab:flops_comparison}.}

\textbf{Metrics.} In accordance with the methodology of OLMo~\citep{groeneveld2024olmo}, we report the average perplexities (PPL) and losses on both the V2 and V3 validation sets, along with the average metrics for zero-shot evaluation on downstream benchmarks (refer to Table~\ref{tab:combined-sets-benchmarks}). We observe significant volatility in the zero-shot performance indicators for the datasets (highlighted in grey in Table~\ref{tab:combined-sets-benchmarks}), with fluctuations exceeding 20\% across neighboring checkpoints. For more reliable and consistent results, we excludes these volatile datasets from our analysis. For the MoE models, in line with OLMoE, we also present  losses on V3 validation sets, and accuracies on downstream benchmarks (refer to Table~\ref{tab:olmoe-benchmarks}).

\subsection{Ablation Study}
We use the dynamic hyperconnections with an expansion rate of $n=4$ and include the tanh function as the default method, marked with the suffix -DHC, while -SHC denotes static hyper-connections.


The evaluation results are presented in Table~\ref{tab:olmo_dhc}, and the training loss curves are depicted in Fig.~\ref{fig:1b_500B_training_loss}. We observe that with an expansion rate of $n=1$, the performance of DHC is inferior to the baseline. However, for $n>1$, DHC significantly outperforms the baseline, achieving superior results at $n=4$, with the increase to $n=8$ providing minimal additional benefits. Notably, \texttt{OLMo-1B-DHC$\times$8 W/O tanh} excels on both V2 and V3 validation sets, with a reduction in \texttt{V2 Eval Loss} by \textbf{0.034} and \texttt{V3 Eval Loss} by \textbf{0.029} compared to the baseline. Furthermore, the decline rate of training losses for DHC ($n \geq 2$) is steeper than that of the baseline, and DHC demonstrates greater stability, with no spikes observed in any DHC experiments.




\textbf{Static and dynamic hyper-connections.}
Table~\ref{tab:1b_shc_dhc} presents an ablation study comparing SHC and DHC. All hyper-connection (HC) variants significantly outperform the baseline. At an expansion rate of 2, the improvements of DHC and SHC are similar. However, at an expansion rate of 4, DHC performs notably better than SHC.

\begin{table}[h]
\centering
\caption{Ablation study on static and dynamic hyper-connections with training on 500 B tokens.}
\begin{tabular}{lccccc}
\toprule
\textbf{Methods}  & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Down Stream\\ Avg, Acc. $\uparrow$\end{tabular}}\\ \midrule
OLMo-1B & 2.811 & 18.023 & 2.544 & 14.229 & 62.5 \\ 
OLMo-1B-SHC$\times$2 & 2.799 & 17.778 & 2.538 & 14.152 & 63.4 \\ 
OLMo-1B-DHC$\times$2 & 2.802 & 17.950 & 2.534 & 14.114 & 63.0 \\ 
OLMo-1B-DHC$\times$2 \textcolor{gray}{W/O tanh} & \textbf{2.792} & \textbf{17.663} & \textbf{2.529} & \textbf{14.033} & \textbf{63.8} \\ 
\cmidrule{1-6}
OLMo-1B-SHC$\times$4 & 2.791 & 17.671 & 2.528 & 14.025 & 63.6 \\ 
OLMo-1B-DHC$\times$4 & 2.781 & 17.509 & \textbf{2.515} & \textbf{13.826} & 63.8 \\ 
OLMo-1B-DHC$\times$4 \textcolor{gray}{W/O tanh} & \textbf{2.779} & \textbf{17.451} & 2.516 & 13.844 & \textbf{64.4} \\ 
\bottomrule
\end{tabular}
\label{tab:1b_shc_dhc}
\end{table}

\textbf{The importance of $\mathbf{B}$ and $\mathcal{WC}$.} As shown in Table~\ref{tab:dhcx4_wc_b}, not training $\mathcal{WC}$ leads to significant performance declines, with the V2 loss increasing by \textbf{0.021} and the V3 loss by \textbf{0.017}, as seen when comparing the 4th and 6th lines of Table~\ref{tab:dhcx4_wc_b}. In contrast, the impact is less pronounced when $\mathbf{B}$ is not trained. Therefore, ensuring the trainability of both $\mathcal{WC}$ and $\mathbf{B}$ is crucial.



\begin{table}[h]
\centering
\caption{Ablation study on OLMo-1B-DHC$\times$4. In the $\mathbf{B}$ or \textbf{$\mathcal{WC}$} column, the symbol "\ding{55}" denotes parameters that are not trainable from initialization.}
\begin{tabular}{ccc|ccccc}
\toprule
\textbf{$\mathcal{WC}$} & \textbf{$\mathbf{B}$} & \textbf{Tanh} & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Down Stream\\ Avg, Acc. $\uparrow$\end{tabular}} \\ \midrule
\ding{55} & \ding{51} & \ding{55} & 2.804 & 17.912 & 2.537 & 14.145 & 62.5 \\
\ding{51} & \ding{55} & \ding{55} & 2.781 & 17.493 & 2.518 & 13.874 & 63.6 \\
\ding{51} & \ding{51} & \ding{55} & \textbf{2.779} & \textbf{17.773} & \textbf{2.516} & \textbf{13.823} & \textbf{64.4} \\ \midrule
\ding{55} & \ding{51} & \ding{51} & 2.802 & 17.914 & 2.532 & 14.072 & 63.4 \\
\ding{51} & \ding{55} & \ding{51} & 2.783 & 17.504 & 2.520 & 13.906 & 63.4 \\
\ding{51} & \ding{51} & \ding{51} & \textbf{2.781} & \textbf{17.835} & \textbf{2.515} & \textbf{13.807} & \textbf{63.8} \\
\bottomrule
\end{tabular}
\label{tab:dhcx4_wc_b}
\end{table}

\subsection{Comparison with related works}
\begin{table}[h]
\centering
\caption{Performance of related methods on OLMo-1B models.}
\begin{tabular}{lccccc}
\toprule
\textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V2 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3 Eval\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Down Stream\\ Avg, Acc. $\uparrow$\end{tabular}}\\ \midrule
OLMo-1B & 2.811 & 18.023 & 2.544 & 14.229 & 62.5 \\ 
OLMo-1B-ResiDual & 2.825 & 18.375 & 2.551 & 14.346 & 62.0 \\ 
OLMo-1B-Altup$\times$2 & 2.827 & 18.268 & 2.558 & 14.454 & 62.4 \\
\midrule
OLMo-1B-DHC$\times$2 & 2.802 & 17.950 & 2.534 & 14.114 & 63.0 \\ 
OLMo-1B-DHC$\times$2 \textcolor{gray}{W/O tanh} & \textbf{2.792} & \textbf{17.663} & \textbf{2.529} & \textbf{14.033} & \textbf{63.8} \\ 
\bottomrule
\end{tabular}
\label{tab:related_methods}
\end{table}
We implemented the Altup \citep{baykal2024alternating} and ResiDual \citep{xie2023residual} methods in OLMo. Altup is motivated to widen the hidden dimension while maintaining low computation cost by passing only a part of hidden state to transformer blocks. By contrast, ResiDual is proposed to combine both Pre- and Post-Norm in a two-stream style. Both methods expand the hidden size by \(n\) times with negligible computational overhead, with ResiDual expanding it exactly \(2\) times. For a fair comparison, we set \(n=2\) in our experiments. Unfortunately, although these methods show gains in the early stages of training, they are gradually surpassed by the baseline, as demonstrated by the results in Table~\ref{tab:related_methods} and the training loss curves in Fig.~\ref{fig:related_method_loss}.


\subsection{7B Models}
\begin{figure}[h]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/train_loss_7b_exp_099.pdf}
        
    \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/7B_C4-en_Loss.pdf}

     \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/7B_hellaswag.pdf}

     \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/7B_sciq.pdf}
    \end{minipage}
    \caption{\textbf{(1)} and \textbf{(2)} Training loss (0.99 EMA smoothed) and C4-en validation loss for \texttt{OLMo-7B} and \texttt{OLMo-7B-DHC$\times$4} models. \textbf{(3)} and \textbf{(4)} Accuracy curves on \texttt{hellaswag} and \texttt{sciq}, demonstrating the superior performance of the \texttt{OLMo-7B-DHC$\times$4} model.}
    \label{fig:7b_training_loss}
\end{figure}
We evaluate the effectiveness of hyper-connections on the 7B model, training a model with DHCs with an expansion rate of 4, denoted as \texttt{OLMo-7B-DHC$\times$4}. According to Table~\ref{tab:7b}, \texttt{OLMo-7B-DHC$\times$4} significantly outperforms the baseline \texttt{OLMo-7B} model in all average metrics. In the V2 evaluation, \texttt{OLMo-7B-DHC$\times$4} shows improvements of \textbf{0.022} for loss and \textbf{0.293} for PPL. Furthermore, the average score of downstream benchmarks \textbf{0.710} surpasses the baseline 0.701, with the results of specific tasks shown in Fig.~\ref{fig:7b_full_results}.


\begin{table}[h]
\centering
\caption{Performance of 7B models. \newtext{FLOPs refers to the computation per token in the forward pass.}}
\begin{tabular}{lccccccc}
\toprule
\textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}Params\\ (B)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FLOPs\\ (G)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V2\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V2\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3\\ Loss $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}V3\\ PPL $\downarrow$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}} Tasks Avg.\\  Acc. $\uparrow$\end{tabular}}\\ 
\midrule
OLMo-7B & 6.9 & 13.36 & 2.581 & 14.316 & 2.322 & 11.324 & 70.1 \\ 
OLMo-7B-DHC$\times$4 & 6.9 & 13.38 & \textbf{2.559} & \textbf{14.023} & \textbf{2.304} & \textbf{11.120} & \textbf{71.0} \\ 
\bottomrule
\end{tabular}
\label{tab:7b}
\end{table}


Based on Fig~\ref{fig:7b_training_loss}, the \texttt{OLMo-7B-DHC$\times$4} model consistently shows better metrics compared to baseline, including training and validation loss and accuracy in downstream benchmarks. Notably, after 400 B tokens, the model maintains its improvement without the gains diminishing. This indicates that the \texttt{OLMo-7B-DHC$\times$4} model continues to provide consistent benefits in reducing loss, even at higher token counts. Furthermore, according to Fig.~\ref{fig:7b_training_loss}, the baseline model exhibits frequent spikes, while our model with DHCs shows no spikes throughout the training. This shows that our approach not only achieves better loss but also ensures more stable training.


\subsection{MoE Models}
\label{sec:moe_models}
% \begin{figure}[h]
%     \begin{center}
%     \includegraphics[width=1\textwidth]{fig/1B7B.pdf}
%     \end{center}
%   \caption{}
%     \label{fig:olmoe_results}
% \end{figure}

We evaluate the effectiveness of hyper-connections on the Mixture-of-Experts (MoE) model. We retrain the original \texttt{OLMoE-1B-7B} model as the baseline and train a model that applies Dynamic Hyper-Connections (DHC) with $n=4$, replacing the residual connections. The full results are shown in Fig.~\ref{fig:moe_1b7b_full_results}, which illustrates that hyper-connections outperform residual connections in almost all metrics. In many metrics, our method requires only \textbf{half} of the training tokens to achieve the same performance as the baseline. Fig.~\ref{fig:olmoe_curves} and Table~\ref{tab:olmoe_result} highlight some of the results, such as a reduction in training loss of approximately \textbf{0.027}, a reduction in loss on the C4-en validation set of \textbf{0.028}, an improvement of \textbf{6} points on the ARC-Challengeand an improvement of \textbf{1.2} points on MMLU Var.



\begin{table}[h!]
\centering
\caption{Downstream evaluations for MoE models training with 500B tokens under the OLMoE evaluation setting. ARC-C stands for ARC-Challenge, and ARC-E for ARC-Easy. MMLU Var is a modified version of MMLU that includes varying few-shot examples, providing stable feedback during early training, as outlined in the OLMoE setting \citep{muennighoff2024olmoeopenmixtureofexpertslanguage}.}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}MMLU \\ Var\\ \end{tabular}} & 
\textbf{\begin{tabular}[c]{@{}c@{}}Hella-\\Swag\\ \end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}ARC-C\\ \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}ARC-E\\ \end{tabular}} &
 \textbf{\begin{tabular}[c]{@{}c@{}}PIQA\\ \end{tabular}} &
 \textbf{\begin{tabular}[c]{@{}c@{}}Wino-\\Grande\\ \end{tabular}} & 
 \textbf{\begin{tabular}[c]{@{}c@{}}BoolQ\\ \end{tabular}} \\ 
\midrule
OLMoE-1B-7B              & 38.5 & 69.5 & 41.8 & 72.8 & 77.6 & 64.4 & 65.4 \\ 
OLMoE-1B-7B-DHC$\times$4 & \textbf{39.7} & \textbf{70.2} & \textbf{47.8} & \textbf{76.7} & \textbf{78.2} & \textbf{64.6} & \textbf{68.5} \\ 
\bottomrule
\end{tabular}
}
\label{tab:olmoe_result}
\end{table}




\subsection{Visualization Analysis}
\label{sec:visualization}
\input{sec_visualization}

\section{Related Work}

\textbf{Transformers} \citep{vaswani2017attention} have revolutionized various fields, particularly natural language processing and computer vision. They rely heavily on residual connections to facilitate the training of deep models. Our hyper-connections approach can replace residual connections, providing stable training and consistent improvements in both natural language processing and computer vision.

\textbf{The issues of gradient vanishing and representation collapse} \citep{bengio1994learning,glorot2010understanding,liu2020understanding} have been extensively studied. The combinations of normalization techniques~\citep{ioffe2015batch,ba2016layer} and residual connections~\citep{he2016deep}, like Pre-Norm and Post-Norm, actually reflects different emphases in solving these two issues. However, despite these advancements, the fundamental trade-off between gradient vanishing and representation collapse in deep networks remains a critical challenge. Building on these findings, our work introduces a novel approach that enables neural networks to autonomously learn the optimal strength of connections, potentially improving both gradient stability and representation quality.


\section{Conclusion}
In conclusion, we have introduced hyper-connections as an effective alternative to residual connections in transformers. Our analysis reveals that hyper-connections not only overcome the limitations of residuals but also enable dynamic adjustments in network architecture. Experimental results confirm their promising benefits across various tasks, including pre-training of large language model, image generation, and image classification. 



\section*{Acknowledgements}
This research was conducted at ByteDance Inc. We are grateful for the suggestions and assistance provided by Yaowei Zheng, Yuyu Zhang, Yunshui Li, Xiang Li, Bairen Yi, Zhenyi Lu and Xintian Han.


{
\bibliographystyle{iclr2025_conference}
\bibliography{iclr2025_conference}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{app}


\end{document}
