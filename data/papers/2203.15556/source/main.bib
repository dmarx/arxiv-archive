@article{nuanced_metrics,
  author    = {Daniel Borkan and
               Lucas Dixon and
               Jeffrey Sorensen and
               Nithum Thain and
               Lucy Vasserman},
  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text
               Classification},
  journal   = {CoRR},
  volume    = {abs/1903.04561},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04561},
  eprinttype = {arXiv},
  eprint    = {1903.04561},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-04561.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Dixon2018Measuring,
title	= {Measuring and Mitigating Unintended Bias in Text Classification},
author	= {Lucas Dixon and John Li and Jeffrey Sorensen and Nithum Thain and Lucy Vasserman},
year	= {2018}
}


@article{civil_comments,
  author    = {Daniel Borkan and
               Lucas Dixon and
               Jeffrey Sorensen and
               Nithum Thain and
               Lucy Vasserman},
  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification},
  journal   = {CoRR},
  volume    = {abs/1903.04561},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04561},
  archivePrefix = {arXiv},
  eprint    = {1903.04561},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
  }
  %
@article{selfdiagnosis,
  author    = {Timo Schick and
               Sahana Udupa and
               Hinrich Sch{\"{u}}tze},
  title     = {Self-Diagnosis and Self-Debiasing: {A} Proposal for Reducing Corpus-Based
               Bias in {NLP}},
  journal   = {CoRR},
  volume    = {abs/2103.00453},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00453},
  eprinttype = {arXiv},
  eprint    = {2103.00453},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00453.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%
@article{silver2016mastering,
  title={Mastering the game of {Go} with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den
  Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot,
  Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}


@inproceedings{wang2019superglue,
 author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
 url = {https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{jiao2019tinybert,
    title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
    author = "Jiao, Xiaoqi  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Chen, Xiao  and
      Li, Linlin  and
      Wang, Fang  and
      Liu, Qun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.372",
    doi = "10.18653/v1/2020.findings-emnlp.372",
    pages = "4163--4174",
    abstract = "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
}

@misc{kim2021scalable,
      title={Scalable and Efficient MoE Training for Multitask Multilingual Models}, 
      author={Young Jin Kim and Ammar Ahmad Awan and Alexandre Muzio and Andres Felipe Cruz Salinas and Liyang Lu and Amr Hendy and Samyam Rajbhandari and Yuxiong He and Hany Hassan Awadalla},
      year={2021},
      eprint={2109.10465},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@InProceedings{lewis2021base,
  title = 	 {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  author =       {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6265--6274},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lewis21a/lewis21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/lewis21a.html},
  abstract = 	 {We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released.}
}


@misc{mishra2021accelerating,
      title={Accelerating Sparse Deep Neural Networks}, 
      author={Asit Mishra and Jorge Albericio Latorre and Jeff Pool and Darko Stosic and Dusan Stosic and Ganesh Venkatesh and Chong Yu and Paulius Micikevicius},
      year={2021},
      eprint={2104.08378},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{q8bert,
  author    = {Ofir Zafrir and
               Guy Boudoukh and
               Peter Izsak and
               Moshe Wasserblat},
  title     = {{Q8BERT:} Quantized 8Bit {BERT}},
  journal   = {CoRR},
  volume    = {abs/1910.06188},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.06188},
  eprinttype = {arXiv},
  eprint    = {1910.06188},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-06188.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
Khandelwal2020Generalization,
title={Generalization through Memorization: Nearest Neighbor Language Models},
author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HklBjCEKvH}
}

@misc{guu2020realm,
      title={{REALM}: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
roller2021hash,
title={Hash Layers For Large Sparse Models},
author={Stephen Roller and Sainbayar Sukhbaatar and Arthur Szlam and Jason E Weston},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=lMgDDWb1ULW}
}


@article{sanh2019distilbert,
  publtype={informal},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  title={DistilBERT, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  year={2019},
  cdate={1546300800000},
  journal={CoRR},
  volume={abs/1910.01108},
  url={http://arxiv.org/abs/1910.01108}
}

@misc{so2021primer,
      title={Primer: Searching for Efficient Transformers for Language Modeling}, 
      author={David R. So and Wojciech Ma≈Ñke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},
      year={2021},
      eprint={2109.08668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{exploringsparsityrnns,
  author    = {Sharan Narang and
               Gregory F. Diamos and
               Shubho Sengupta and
               Erich Elsen},
  title     = {Exploring Sparsity in Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1704.05119},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05119},
  eprinttype = {arXiv},
  eprint    = {1704.05119},
  timestamp = {Mon, 13 Aug 2018 16:46:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NarangDSE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{schwartzgreenai,
author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
title = {Green AI},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3381831},
doi = {10.1145/3381831},
abstract = {Creating efficiency in {AI} research will decrease its carbon footprint and increase
its inclusivity as deep learning study should not require the deepest pockets.},
journal = {Commun. ACM},
month = nov,
pages = {54‚Äì63},
numpages = {10}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{liu2020fastbert,
  title={FastBERT: a Self-distilling {BERT} with Adaptive Inference Time},
  author={Liu, Weijie and Zhou, Peng and Wang, Zhiruo and Zhao, Zhe and Deng, Haotang and Ju, Qi},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={6035--6044},
  year={2020}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{
frankle2018lottery,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@inproceedings{han2015deep,
  author    = {Song Han and
               Huizi Mao and
               William J. Dally},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
               Quantization and Huffman Coding},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1510.00149},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/HanMD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{deeplearningdiminshingreturns,
title={Deep Learning's Diminishing Returns: The cost of improvement is becoming unsustainable},
author={Thompson, Neil and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel},
url={https://spectrum.ieee.org/deep-learning-computational-cost},
year={2021},
}

@misc{aiandcompute,
title={{AI} and Compute},
author={Amodei, Dario and Hernandez, Danny},
url={https://openai.com/blog/ai-and-compute/},
year={2018},
}

@misc{forecast_blog,
title={Updates and Lessons from {AI} Forecasting},
author={Jacob Steinhardt},
url={https://bounded-regret.ghost.io/ai-forecasting/},
year={2021},
}

@misc{qa_prompt,
title={QA Prompt},
author={Jacob Hilton},
url={https://github.com/sylinrl/TruthfulQA/blob/main/truthfulqa/presets.py#L32},
year={2021},
}

@article{fastsparseconvnets,
  author    = {Erich Elsen and
               Marat Dukhan and
               Trevor Gale and
               Karen Simonyan},
  title     = {Fast Sparse ConvNets},
  journal   = {CoRR},
  volume    = {abs/1911.09723},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.09723},
}

@article{acdctraining,
  author    = {Alexandra Peste and
               Eugenia Iofinova and
               Adrian Vladu and
               Dan Alistarh},
  title     = {{AC/DC:} Alternating Compressed/DeCompressed Training of Deep Neural
               Networks},
  journal   = {CoRR},
  volume    = {abs/2106.12379},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.12379},
  eprinttype = {arXiv},
  eprint    = {2106.12379},
  timestamp = {Wed, 30 Jun 2021 16:14:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-12379.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gpusparsematmul,
  author    = {Trevor Gale and
               Matei Zaharia and
               Cliff Young and
               Erich Elsen},
  title     = {Sparse {GPU} Kernels for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/2006.10901},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.10901},
}

@article{stateofsparsity,
  author    = {Trevor Gale and
               Erich Elsen and
               Sara Hooker},
  title     = {The State of Sparsity in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1902.09574},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.09574},
}

@article{stateofnnpruning,
  author    = {Davis W. Blalock and
               Jose Javier Gonzalez Ortiz and
               Jonathan Frankle and
               John V. Guttag},
  title     = {What is the State of Neural Network Pruning?},
  journal   = {CoRR},
  volume    = {abs/2003.03033},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.03033},
}

@article{decolonialai,
  author    = {Shakir Mohamed and
               Marie{-}Therese Png and
               William Isaac},
  title     = {Decolonial {AI:} Decolonial Theory as Sociotechnical Foresight in
               Artificial Intelligence},
  journal   = {CoRR},
  volume    = {abs/2007.04068},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.04068},
  eprinttype = {arXiv},
  eprint    = {2007.04068},
  timestamp = {Mon, 20 Jul 2020 14:20:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-04068.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{woodfisherprune,
  author    = {Sidak Pal Singh and
               Dan Alistarh},
  title     = {WoodFisher: Efficient second-order approximations for model compression},
  journal   = {CoRR},
  volume    = {abs/2004.14340},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.14340},
}

@article{mtpruning,
  author    = {Abigail See and
               Minh{-}Thang Luong and
               Christopher D. Manning},
  title     = {Compression of Neural Machine Translation Models via Pruning},
  journal   = {CoRR},
  volume    = {abs/1606.09274},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.09274},
}


@InProceedings{pmlr-v97-so19a,
  title = 	 {The Evolved Transformer},
  author =       {So, David and Le, Quoc and Liang, Chen},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5877--5886},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/so19a/so19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/so19a.html},
  abstract = 	 {Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments ‚Äì the Evolved Transformer ‚Äì demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT‚Äô14 English-German; at smaller sizes, it achieves the same quality as the original "big" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.}
}


@inproceedings{michel2019sixteen,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{voita2019analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
    abstract = "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",
}

@article{lin2021m610t,
      title={M6-10{T}: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining}, 
      author={Junyang Lin and An Yang and Jinze Bai and Chang Zhou and Le Jiang and Xianyan Jia and Ang Wang and Jie Zhang and Yong Li and Wei Lin and Jingren Zhou and Hongxia Yang},
      year={2021},
      eprint={2110.03888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
  journal={arXiv preprint arXiv:2005.07683},
  year={2020}
}


@inproceedings{jayakumar2021top,
 author = {Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {20744--20754},
 publisher = {Curran Associates, Inc.},
 title = {Top-{KAST}: Top-{K} {A}lways {S}parse {T}raining},
 url = {https://proceedings.neurips.cc/paper/2020/file/ee76626ee11ada502d5dbf1fb5aae4d2-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}

@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={103--112},
  year={2019}
}

@article{wikitext103,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={International Conference on Learning Representations},
  year={2017}
}


@article{kudo2018sentencepiece,
  title={Sentence{P}iece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@article{dai2019transformer,
  title={Transformer-{XL}: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}


@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={arXiv preprint arXiv:1910.07467},
  year={2019}
}


@misc{curationcorpusbase2020,
  title={Curation Corpus Base},
  author={Curation},
  year={2020},
  url={https://github.com/CurationCorp/curation-corpus}
}


@misc{clark2022unified,
  doi = {10.48550/ARXIV.2202.01169},
  url = {https://arxiv.org/abs/2202.01169},
  author = {Clark, Aidan and Casas, Diego de las and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and Driessche, George van den and Rutherford, Eliza and Hennigan, Tom and Johnson, Matthew and Millican, Katie and Cassirer, Albin and Jones, Chris and Buchatskaya, Elena and Budden, David and Sifre, Laurent and Osindero, Simon and Vinyals, Oriol and Rae, Jack and Elsen, Erich and Kavukcuoglu, Koray and Simonyan, Karen},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Unified Scaling Laws for Routed Language Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{thoppilan2022lamda,
    title={{L}a{MDA}: Language Models for Dialog Applications},
    author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
    year={2022},
    eprint={2201.08239},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{paperno2016lambada,
      title={The {LAMBADA} dataset: Word prediction requiring a broad discourse context}, 
      author={Denis Paperno and Germ√°n Kruszewski and Angeliki Lazaridou and Quan Ngoc Pham and Raffaella Bernardi and Sandro Pezzelle and Marco Baroni and Gemma Boleda and Raquel Fern√°ndez},
      year={2016},
      eprint={1606.06031},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}

@article{haiku2020github,
  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},
  title = {{H}aiku: {S}onnet for {JAX}},
  url = {http://github.com/deepmind/dm-haiku},
  version = {0.0.3},
  year = {2020},
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{kiritchenko2018sentiment,
  author    = {Svetlana Kiritchenko and
               Saif M. Mohammad},
  title     = {Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems},
  journal   = {CoRR},
  volume    = {abs/1805.04508},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.04508},
  eprinttype = {arXiv},
  eprint    = {1805.04508},
  timestamp = {Mon, 13 Aug 2018 16:47:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-04508.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{levine2020depth,
  title={The Depth-to-Width Interplay in Self-Attention},
  author={Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
  journal={arXiv preprint arXiv:2006.12467},
  year={2020}
}

@article{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{ben2021bitfit,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Ben Zaken, Elad and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv e-prints},
  pages={arXiv--2106},
  year={2021}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{jurassic,
  title={Jurassic-1: Technical Details and Evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  year={2021}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}



@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{pile,
  title={The {P}ile: An 800{GB} Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{lee2021dedup,
  author    = {Katherine Lee and
               Daphne Ippolito and
               Andrew Nystrom and
               Chiyuan Zhang and
               Douglas Eck and
               Chris Callison{-}Burch and
               Nicholas Carlini},
  title     = {Deduplicating Training Data Makes Language Models Better},
  journal   = {CoRR},
  volume    = {abs/2107.06499},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.06499},
  eprinttype = {arXiv},
  eprint    = {2107.06499},
  timestamp = {Wed, 21 Jul 2021 15:55:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-06499.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gehman2020realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
}

@inproceedings{welbl2021challenges,
    title = "Challenges in Detoxifying Language Models",
    author = "Welbl, Johannes  and
      Glaese, Amelia  and
      Uesato, Jonathan  and
      Dathathri, Sumanth  and
      Mellor, John  and
      Hendricks, Lisa Anne  and
      Anderson, Kirsty  and
      Kohli, Pushmeet  and
      Coppin, Ben  and
      Huang, Po-Sen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = Nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.210",
    pages = "2447--2469",
}

@article{rottgerhatecheck,
   title={HateCheck: Functional Tests for Hate Speech Detection Models},
   url={http://dx.doi.org/10.18653/v1/2021.acl-long.4},
   DOI={10.18653/v1/2021.acl-long.4},
   journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={R√∂ttger, Paul and Vidgen, Bertie and Nguyen, Dong and Waseem, Zeerak and Margetts, Helen and Pierrehumbert, Janet},
   year={2021}
}


@article{Hanna_2020,
   title={Towards a critical race methodology in algorithmic fairness},
   ISBN={9781450369367},
   url={http://dx.doi.org/10.1145/3351095.3372826},
   DOI={10.1145/3351095.3372826},
   journal={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
   year={2020},
   month={Jan}
}

@article{twitteraae,
  author    = {Su Lin Blodgett and
               Lisa Green and
               Brendan O'Connor},
  title     = {Demographic Dialectal Variation in Social Media: {A} Case Study of
               African-American English},
  journal   = {CoRR},
  volume    = {abs/1608.08868},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.08868},
  eprinttype = {arXiv},
  eprint    = {1608.08868},
  timestamp = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BlodgettGO16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@book{dewey1923relativ,
  title={Relativ [sic] frequency of English speech sounds},
  author={Dewey, Godfrey},
  year={1923},
  publisher={Harvard UP}
}

@article{turing1950computing,
  title={Computing Machinery and Intelligence},
  author={Turing, AM},
  journal={Mind},
  volume={59},
  number={236},
  pages={433--460},
  year={1950},
  publisher={JSTOR}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{werbos1974beyond,
  title={Beyond regression: new tools for prediction and analysis in the behavioral sciences},
  author={Werbos, Paul},
  journal={Ph. D. dissertation, Harvard University},
  year={1974}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{chorowski2015attention,
  title={Attention-based models for speech recognition},
  author={Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1506.07503},
  year={2015}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{truthfulqa,
  title={{TruthfulQA}: Measuring How Models Mimic Human Falsehoods},
  author={Lin, S. and Hilton, J. and Evans, O.},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@inproceedings{huang2020reducing,
    title = "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation",
    author = "Huang, Po-Sen  and
      Zhang, Huan  and
      Jiang, Ray  and
      Stanforth, Robert  and
      Welbl, Johannes  and
      Rae, Jack  and
      Maini, Vishal  and
      Yogatama, Dani  and
      Kohli, Pushmeet",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.7",
    doi = "10.18653/v1/2020.findings-emnlp.7",
    pages = "65--83",
}

@article{adiwardana2020meena,
       author = {{Adiwardana}, Daniel and {Luong}, Minh-Thang and {So}, David R. and {Hall}, Jamie and {Fiedel}, Noah and {Thoppilan}, Romal and {Yang}, Zi and {Kulshreshtha}, Apoorv and {Nemade}, Gaurav and {Lu}, Yifeng and {Le}, Quoc V.},
        title = {Towards a Human-like Open-Domain Chatbot},
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2020,
        month = jan,
          eid = {arXiv:2001.09977},
        pages = {arXiv:2001.09977},
archivePrefix = {arXiv},
       eprint = {2001.09977},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200109977A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{ptb,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    url = "https://aclanthology.org/J93-2004",
    pages = "313--330",
}

@InProceedings{mikolov2011empirical,
author = {Mikolov, Tomas and Deoras, Anoop and Kombrink, Stefan and Burget, Lukas and \v{C}ernock\'{y}, Jan Honza},
title = {Empirical Evaluation and Combination of Advanced Language Modeling Techniques},
booktitle = {Interspeech},
year = {2011},
}

@article{christiano2018amplification,
  title={Supervising strong learners by amplifying weak experts},
  author={Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
  journal={arXiv preprint arXiv:1810.08575},
  year={2018}
}

@article{irving2018debate,
  title={{AI} safety via debate},
  author={Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  journal={arXiv preprint arXiv:1805.00899},
  year={2018}
}

@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}

@article{irving2019social,
  author = {Irving, Geoffrey and Askell, Amanda},
  title = {{AI} Safety Needs Social Scientists},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/safety-needs-social-scientists},
  doi = {10.23915/distill.00014}
}

@article{wu2021recursively,
  title={Recursively Summarizing Books with Human Feedback},
  author={Wu, Jeff and Ouyang, Long and Ziegler, Daniel M and Stiennon, Nissan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  journal={arXiv preprint arXiv:2109.10862},
  year={2021}
}

@inproceedings{stiennon2020learning,
 author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {3008--3021},
 publisher = {Curran Associates, Inc.},
 title = {Learning to summarize with human feedback},
 url = {https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{bohm2019better,
  title={Better rewards yield better summaries: Learning to summarise without references},
  author={B{\"o}hm, Florian and Gao, Yang and Meyer, Christian M and Shapira, Ori and Dagan, Ido and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1909.01214},
  year={2019}
}

@article{stray2021you,
  title={What are you optimizing for? {A}ligning recommender systems with human values},
  author={Stray, Jonathan and Vendrov, Ivan and Nixon, Jeremy and Adler, Steven and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2107.10939},
  year={2021}
}

@article{jaques2020human,
  title={Human-centric dialog training via offline reinforcement learning},
  author={Jaques, Natasha and Shen, Judy Hanwen and Ghandeharioun, Asma and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang Shane and Picard, Rosalind},
  journal={arXiv preprint arXiv:2010.05848},
  year={2020}
}

@article{wang2021putting,
  title={Putting humans in the natural language processing loop: A survey},
  author={Wang, Zijie J and Choi, Dongjin and Xu, Shenyu and Yang, Diyi},
  journal={arXiv preprint arXiv:2103.04044},
  year={2021}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{artetxe2021efficient,
	title = {Efficient {Large} {Scale} {Language} {Modeling} with {Mixtures} of {Experts}},
	journal = {arXiv:2112.10684},
	author = {Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and Anantharaman, Giri and Li, Xian and Chen, Shuohui and Akin, Halil and Baines, Mandeep and Martin, Louis and Zhou, Xing and Koura, Punit Singh and O'Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Diab, Mona and Kozareva, Zornitsa and Stoyanov, Ves},
	year = {2021},
}


@article{lazaridou2021pitfalls,
  title={Pitfalls of Static Language Modelling},
  author={Lazaridou, Angeliki and Kuncoro, Adhiguna and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and d'Autume, Cyprien de Masson and Ruder, Sebastian and Yogatama, Dani and others},
  journal={arXiv preprint arXiv:2102.01951},
  year={2021}
}

@misc{carlini2021extracting,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2021},
      eprint={2012.07805},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{anil2021large,
  title={Large-Scale Differentially Private {BERT}},
  author={Anil, Rohan and Ghazi, Badih and Gupta, Vineet and Kumar, Ravi and Manurangsi, Pasin},
  journal={arXiv preprint arXiv:2108.01624},
  year={2021}
}

@article{xue2020mt5,
  title={{MT5}: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@article{kenton2021alignment,
  title={Alignment of Language Agents},
  author={Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2103.14659},
  year={2021}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@article{gebru2018datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  journal={arXiv preprint arXiv:1803.09010},
  year={2018}
}

@misc{buckman2021problematic,
  title = {Fair {ML} Tools Require Problematic {ML} Models},
  author = {Buckman, Jacob},
  howpublished = {\url{https://jacobbuckman.com/2021-02-15-fair-ml-tools-require-problematic-ml-models}},
  note = {Accessed: 2021-10-7}
}


@inproceedings{xu2021detoxifying,
    title = "Detoxifying Language Models Risks Marginalizing Minority Voices",
    author = "Xu, Albert  and
      Pathak, Eshaan  and
      Wallace, Eric  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Klein, Dan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.190",
    doi = "10.18653/v1/2021.naacl-main.190",
    pages = "2390--2397",
    abstract = "Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.",
}


@article{dodge2021documenting,
  author    = {Dodge, Jesse and Sap, Maarten and Marasovic, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Gardner, Matt},
  title     = {Documenting the {E}nglish Colossal Clean Crawled Corpus},
  journal   = {CoRR},
  volume    = {abs/2104.08758},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08758},
  eprinttype = {arXiv},
  eprint    = {2104.08758},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08758.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hadfield2016cooperative,
  title={Cooperative inverse reinforcement learning},
  author={Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3909--3917},
  year={2016}
}

@book{russell2020compatible,
  title = {Human Compatible},
  subtitle = {{AI} and the Problem of Control},
  author = {Russell, Stuart},
  year = {2020},
  publisher = {Penguin},
}

@article{gonen2019lipstick,
  title={Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them},
  author={Gonen, Hila and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1903.03862},
  year={2019}
}

@article{fang2017learning,
  title={Learning how to active learn: A deep reinforcement learning approach},
  author={Fang, Meng and Li, Yuan and Cohn, Trevor},
  journal={arXiv preprint arXiv:1708.02383},
  year={2017}
}

@misc{abubakar2021copilot,
  title = {{GitHub} {C}opilot {AI} Is Generating And Giving Out Functional {API} Keys},
  author = {Abubakar, Mohammed},
  howpublished = {\url{https://fossbytes.com/github-copilot-generating-functional-api-keys}},
  note = {Accessed: 2021-10-7},
  year = {2021}
}

@article{bommasani2021opportunities,
  title={On the Opportunities and Risks of Foundation Models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{camburu2018snli,
  title={e-{SNLI}: Natural language inference with natural language explanations},
  author={Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  journal={arXiv preprint arXiv:1812.01193},
  year={2018}
}

@article{rajani2019explain,
  title={Explain yourself! leveraging language models for commonsense reasoning},
  author={Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1906.02361},
  year={2019}
}

@article{wu2019self,
  title={Self-critical reasoning for robust visual question answering},
  author={Wu, Jialin and Mooney, Raymond J},
  journal={arXiv preprint arXiv:1905.09998},
  year={2019}
}

@inproceedings{perez2019finding,
    title = "Finding Generalizable Evidence by Learning to Convince {Q}{\&}{A} Models",
    author = "Perez, Ethan  and
      Karamcheti, Siddharth  and
      Fergus, Rob  and
      Weston, Jason  and
      Kiela, Douwe  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1244",
    doi = "10.18653/v1/D19-1244",
    pages = "2402--2411",
    abstract = "We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only {\textasciitilde}20{\%} of the full passage and (ii) QA models can generalize to longer passages and harder questions.",
}

@inproceedings{gupta2015deep,
  title={Deep learning with limited numerical precision},
  author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  booktitle={International conference on machine learning},
  pages={1737--1746},
  year={2015},
  organization={PMLR}
}

@inproceedings{
micikevicius2017mixed,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}

@incollection{rzepka2015eliza,
  title={ELIZA fifty years later: An automatic therapist using bottom-up and top-down approaches},
  author={Rzepka, Rafal and Araki, Kenji},
  booktitle={Machine Medical Ethics},
  pages={257--272},
  year={2015},
  publisher={Springer}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={arXiv preprint arXiv:2106.13884},
  year={2021}
}

@article{chen2015net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{kasirzadeh2021reasons,
  title={Reasons, values, stakeholders: a philosophical framework for explainable artificial intelligence},
  author={Kasirzadeh, Atoosa},
  journal={arXiv preprint arXiv:2103.00752},
  year={2021}
}

@article{coyle2020explaining,
  title={``{E}xplaining'' machine learning reveals policy challenges},
  author={Coyle, Diane and Weller, Adrian},
  journal={Science},
  volume={368},
  number={6498},
  pages={1433--1434},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{brants2007large,
    title = "Large Language Models in Machine Translation",
    author = "Brants, Thorsten  and
      Popat, Ashok C.  and
      Xu, Peng  and
      Och, Franz J.  and
      Dean, Jeffrey",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1090",
    pages = "858--867",
}


@book{jelinek1997statistical,
  title={Statistical methods for speech recognition},
  author={Jelinek, Frederick},
  year={1997},
  publisher={MIT press}
}
@inproceedings{brill2000improved,
  title={An improved error model for noisy channel spelling correction},
  author={Brill, Eric and Moore, Robert C},
  booktitle={Proceedings of the 38th annual meeting of the association for computational linguistics},
  pages={286--293},
  year={2000}
}

@article{brown1990statistical,
  title={A statistical approach to machine translation},
  author={Brown, Peter F and Cocke, John and Della Pietra, Stephen A and Della Pietra, Vincent J and Jelinek, Frederick and Lafferty, John and Mercer, Robert L and Roossin, Paul S},
  journal={Computational linguistics},
  volume={16},
  number={2},
  pages={79--85},
  year={1990}
}

@article{ney1994structuring,
  title={On structuring probabilistic dependences in stochastic language modelling},
  author={Ney, Hermann and Essen, Ute and Kneser, Reinhard},
  journal={Computer Speech \& Language},
  volume={8},
  number={1},
  pages={1--38},
  year={1994},
  publisher={Elsevier}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@inproceedings{
    lepikhin2020gshard,
    title={{GS}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=qrwe7XHTmYb}
}

@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}

@inproceedings{deng2009imagenet,
  title={Image{N}et: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={IEEE}
}

@inproceedings{rajpurkar2016squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@article{graves2013generating,
  title={Generating Sequences With Recurrent Neural Networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}

@article{jozefowicz2016exploring,
  title={Exploring the limits of language modeling},
  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal={arXiv preprint arXiv:1602.02410},
  year={2016}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@misc{tay2021scale,
    title={Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers},
    author={Yi Tay and Mostafa Dehghani and Jinfeng Rao and William Fedus and Samira Abnar and Hyung Won Chung and Sharan Narang and Dani Yogatama and Ashish Vaswani and Donald Metzler},
    year={2021},
    eprint={2109.10686},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{10.1145/3079856.3080246,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080246},
doi = {10.1145/3079856.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now
come from domain-specific hardware. This paper evaluates a custom ASIC---called a
Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates
the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit
MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS)
and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution
model is a better match to the 99th-percentile response-time requirement of our NN
applications than are the time-varying optimizations of CPUs and GPUs that help average
throughput more than guaranteed latency. The lack of such features helps explain why,
despite having myriad MACs and a big memory, the TPU is relatively small and low power.
We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which
are contemporaries deployed in the same datacenters. Our workload, written in the
high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and
LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization
for some applications, the TPU is on average about 15X -- 30X faster than its contemporary
GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5
memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the
GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {1‚Äì12},
numpages = {12},
keywords = {deep learning, DNN, CNN, TensorFlow, LSTM, TPU, accelerator, GPU, neural network, domain-specific architecture, RNN, MLP},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{tpu,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
volume = {45},
number = {2},
issn = {0163-5964},
journal = {SIGARCH Comput. Archit. News},
}

@inproceedings{broder1997resemblance,
  title={On the resemblance and containment of documents},
  author={Broder, Andrei Z},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)},
  pages={21--29},
  year={1997},
  organization={IEEE}
}

@article{weidinger2021harms,
  title={Ethical and social risks of harm from Language Models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
  journal={arXiv submission},
  year={2021}
}

@article{borgeaud2021retrieval,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and van den Driessche, George and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack William and Elsen, Erich and Sifre, Laurent},
  journal={arXiv 2112.04426},
  year={2021}
}

@unpublished{perez2021redteaming,
  title={Red Teaming Language Models with Language Models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  note={To appear},
  year={2022},
}

@article{gabriel2021challenge,
  title={The Challenge of Value Alignment: from Fairer Algorithms to {AI} Safety},
  author={Gabriel, Iason and Ghazavi, Vafa},
  journal={arXiv preprint arXiv:2101.06060},
  year={2021}
}

@inproceedings{Augenstein:etal:2019,
    title = "MultiFC: a Real-World Multi-Domain Dataset for Evidence-Based Fact Checking",
    author = "Augenstein, Isabelle and Lioma, Christina and Wang, Dongsheng and Lima, Lucas Chaves and Hansen,  Casper and Christian Hansen, Jakob Grue Simonsen",
    booktitle = "Transactions of the Association for Computational Linguistics (TACL), 2021",
    month = nov,
    year = "2019",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1909.03242",
}

@inproceedings{Lee:etal:2021,
    title = "Towards Few-shot Fact-Checking via Perplexity",
    author = "Lee, Nayeon  and
      Bang, Yejin  and
      Madotto, Andrea  and
      Fung, Pascale",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.158",
    doi = "10.18653/v1/2021.naacl-main.158",
    pages = "1971--1981",
    abstract = "Few-shot learning has drawn researchers{'} attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10{\%} on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.",
}

@inproceedings{fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1074",
    pages = "809--819",
}

@inproceedings{selbst2019fairness,
  title={Fairness and abstraction in sociotechnical systems},
  author={Selbst, Andrew D and Boyd, Danah and Friedler, Sorelle A and Venkatasubramanian, Suresh and Vertesi, Janet},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={59--68},
  year={2019}
}

@inproceedings{sambasivan2021re,
  title={Re-imagining algorithmic fairness in {India} and beyond},
  author={Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={315--328},
  year={2021}
}

@inproceedings{sanchez2020does,
  title={What does it mean to 'solve' the problem of discrimination in hiring? Social, technical and legal perspectives from the {UK} on automated hiring systems},
  author={S{\'a}nchez-Monedero, Javier and Dencik, Lina and Edwards, Lilian},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={458--468},
  year={2020}
}

@article{bolukbasi2016man,
  title={Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
  author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={4349--4357},
  year={2016}
}

@article{sun2019mitigating,
  title={Mitigating gender bias in natural language processing: Literature review},
  author={Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}
@article{costa2019analysis,
  title={An analysis of gender bias studies in natural language processing},
  author={Costa-juss{\`a}, Marta R},
  journal={Nature Machine Intelligence},
  volume={1},
  number={11},
  pages={495--496},
  year={2019},
  publisher={Nature Publishing Group}
}

@InProceedings{rudinger2018gender,
  author    = {Rudinger, Rachel  and  Naradowsky, Jason  and  Leonard, Brian  and  {Van Durme}, Benjamin},
  title     = {Gender Bias in Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics}
}

@article{cao2021toward,
  title={Toward Gender-Inclusive Coreference Resolution: An Analysis of Gender and Bias throughout the Machine Learning Lifecyle},
  author={Cao, Yang Trista and Daum{\'e}, Hal},
  journal={Computational Linguistics},
  pages={1--47},
  year={2021}
}

@article{sheng2019woman,
  title={The woman worked as a babysitter: On biases in language generation},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  journal={EMNLP},
  year={2019}
}
@article{nangia2020crows,
  title={CrowS-pairs: A challenge dataset for measuring social biases in masked language models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R},
  journal={EMNLP},
  year={2020}
}

@article{adiwardana2020towards,
  title={Towards a human-like open-domain chatbot},
  author={Adiwardana, Daniel and Luong, Minh-Thang and So, David R and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and others},
  journal={arXiv preprint arXiv:2001.09977},
  year={2020}
}

@inproceedings{nadeem2020stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
}

@inproceedings{roller2020recipes,
    title = "Recipes for Building an Open-Domain Chatbot",
    author = "Roller, Stephen  and
      Dinan, Emily  and
      Goyal, Naman  and
      Ju, Da  and
      Williamson, Mary  and
      Liu, Yinhan  and
      Xu, Jing  and
      Ott, Myle  and
      Smith, Eric Michael  and
      Boureau, Y-Lan  and
      Weston, Jason",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.24",
    doi = "10.18653/v1/2021.eacl-main.24",
    pages = "300--325",
    abstract = "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
}

@inproceedings{blodgett2021stereotyping,
  title={Stereotyping {N}orwegian salmon: an inventory of pitfalls in fairness benchmark datasets},
  author={Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1004--1015},
  year={2021}
}

@article{chen2017survey,
  title={A survey on dialogue systems: Recent advances and new frontiers},
  author={Chen, Hongshen and Liu, Xiaorui and Yin, Dawei and Tang, Jiliang},
  journal={Acm Sigkdd Explorations Newsletter},
  volume={19},
  number={2},
  pages={25--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{bucholtz2004language,
  title={Language and identity},
  author={Bucholtz, Mary and Hall, Kira},
  journal={A companion to linguistic anthropology},
  volume={1},
  pages={369--394},
  year={2004},
  publisher={Wiley Online Library}
}

@inproceedings{race,
    title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1082",
    doi = "10.18653/v1/D17-1082",
    pages = "785--794",
    abstract = "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students{'} ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43{\%}) and the ceiling human performance (95{\%}). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at \url{http://www.cs.cmu.edu/~glai1/data/race/}and the code is available at \url{https://github.com/qizhex/RACE_AR_baselines}.",
}

@article{race-sota,
  author    = {Yufan Jiang and
               Shuangzhi Wu and
               Jing Gong and
               Yahui Cheng and
               Peng Meng and
               Weiliang Lin and
               Zhibo Chen and
               Mu Li},
  title     = {Improving Machine Reading Comprehension with Single-choice Decision
               and Transfer Learning},
  journal={arXiv preprint arXiv:2011.03292},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.03292},
}

@inproceedings{sheng2021societal,
    title = "Societal Biases in Language Generation: Progress and Challenges",
    author = "Sheng, Emily  and
      Chang, Kai-Wei  and
      Natarajan, Prem  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.330",
    doi = "10.18653/v1/2021.acl-long.330",
    pages = "4275--4293",
}

@article{teukolsky1992numerical,
  title={Numerical recipes in C},
  author={Teukolsky, Saul A and Flannery, Brian P and Press, WH and Vetterling, WT},
  journal={SMR},
  volume={693},
  number={1},
  pages={59--70},
  year={1992}
}

@misc{alphacode,
    title={Competition-Level Code Generation with AlphaCode},
    author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and
    Schrittwieser, Julian and Leblond, R√©mi and Eccles, Tom and
    Keeling, James and Gimeno, Felix and Dal Lago, Agustin and
    Hubert, Thomas and Choy, Peter and de Masson d'Autume, Cyprien and
    Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and
    Gowal, Sven and Cherepanov, Alexey and Molloy, James and
    Mankowitz, Daniel and Sutherland Robson, Esme and Kohli, Pushmeet and
    de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
    year={2022},
    month={Feb}}
    
@article{polu2020generative,
  title={Generative language modeling for automated theorem proving},
  author={Polu, Stanislas and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2009.03393},
  year={2020}
}


@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yura and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{grave2017unbounded,
  title={Unbounded cache model for online language modeling with open vocabulary},
  author={Grave, Edouard and Cisse, Moustapha and Joulin, Armand},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6044--6054},
  year={2017}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International Conference on Machine Learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@inproceedings{lewisretrieval2020,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {9459--9474},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 volume = {33},
 year = {2020}
}

@inproceedings{khandelwal2019generalization,
  title={Generalization through Memorization: Nearest Neighbor Language Models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{yogatama2021adaptive,
  title={Adaptive Semiparametric Language Models},
  author={Yogatama, Dani and de Masson d‚ÄôAutume, Cyprien and Kong, Lingpeng},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={362--373},
  year={2021},
  publisher={MIT Press}
}

@article{rust2020good,
  title={How good is your tokenizer? {O}n the monolingual performance of multilingual language models},
  author={Rust, Phillip and Pfeiffer, Jonas and Vuli{\'c}, Ivan and Ruder, Sebastian and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2012.15613},
  year={2020}
}

@article{clark2021canine,
  title={{CANINE}: Pre-training an Efficient Tokenization-Free Encoder for Language Representation},
  author={Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
  year={2021}
}

@article{griewank2000algorithm,
  title={Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={26},
  number={1},
  pages={19--45},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@article{blodgett2020language,
  title={Language (technology) is power: A critical survey of "bias" in {NLP}},
  author={Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
  journal={ACL},
  year={2020}
}

@book{butler2002gender,
  title={Gender trouble},
  author={Butler, Judith},
  year={2002},
  publisher={routledge}
}

@article{keyes2018misgendering,
  title={The misgendering machines: Trans/HCI implications of automatic gender recognition},
  author={Keyes, Os},
  journal={Proceedings of the ACM on human-computer interaction},
  volume={2},
  number={CSCW},
  pages={1--22},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{keyes2021you,
  title={You Keep Using That Word: Ways of Thinking about Gender in Computing Research},
  author={Keyes, Os and May, Chandler and Carrell, Annabelle},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={5},
  number={CSCW1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT (1)},
  year={2019}
}

@inproceedings{wang2018glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{raffel2019exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@inproceedings{Dagan:2005,
  author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle = {Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment},
  title = {The {PASCAL} Recognising Textual Entailment Challenge},
  url = {http://www.cs.biu.ac.il/~glikmao/rte05/},
  year = 2005
}


@article{nlg530b,
      title={{U}sing {D}eepSpeed and {M}egatron to {T}rain {M}egatron-Turing {NLG} 530B, {A} {L}arge-{S}cale {G}enerative {L}anguage {M}odel}, 
      author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{caliskan2017semantics,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
  journal={Science},
  volume={356},
  number={6334},
  pages={183--186},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{jaegle2021perceiver,
  title={Perceiver {IO}: A general architecture for structured inputs \& outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}

@misc{zoph2022designing,
      title={Designing Effective Sparse Expert Models}, 
      author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
      year={2022},
      eprint={2202.08906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mccandlish2018empirical,
      title={An Empirical Model of Large-Batch Training}, 
      author={Sam McCandlish and Jared Kaplan and Dario Amodei and OpenAI Dota Team},
      year={2018},
      eprint={1812.06162},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{shallue2018measuring,
  title={Measuring the effects of data parallelism on neural network training},
  author={Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal={arXiv preprint arXiv:1811.03600},
  year={2018}
}

@inproceedings{NEURIPS2019_e0eacd98,
 author = {Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model},
 url = {https://proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{hellaswag,
    title={Hella{S}wag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{
yang2021tuning,
title={Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
author={Greg Yang and Edward J Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Bx6qKuBM2AD}
}

@misc{hernandez2021scaling,
      title={Scaling Laws for Transfer}, 
      author={Danny Hernandez and Jared Kaplan and Tom Henighan and Sam McCandlish},
      year={2021},
      eprint={2102.01293},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8732--8740},
  year={2020}
}

@article{socialiqa,
  title={Social{IQA}: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}

@inproceedings{piqa,
  title={{PIQA}: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}

@article{Young_Cambria_Chaturvedi_Zhou_Biswas_Huang_2018, title={Augmenting End-to-End Dialogue Systems With Commonsense Knowledge}, 
volume={32}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/11923},
number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Young, Tom and Cambria, Erik and Chaturvedi, Iti and Zhou, Hao and Biswas, Subham and Huang, Minlie}, year={2018}, month={Apr.} }

@inproceedings{zhou-etal-2018-commonsense,
  title     = {Commonsense Knowledge Aware Conversation Generation with Graph Attention},
  author    = {Hao Zhou and Tom Young and Minlie Huang and Haizhou Zhao and Jingfang Xu and Xiaoyan Zhu},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {4623--4629},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/643},
  url       = {https://doi.org/10.24963/ijcai.2018/643},
}


@inproceedings{gordon2013reporting,
  title={Reporting bias and knowledge acquisition},
  author={Gordon, Jonathan and Van Durme, Benjamin},
  booktitle={Proceedings of the 2013 workshop on Automated knowledge base construction},
  pages={25--30},
  year={2013}
}

@inproceedings{ConceptualCaptions,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2556--2565},
  year={2018}
}

@inproceedings{OKVQA,
  title={{OK-VQA}: A Visual Question Answering Benchmark Requiring External Knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3190--3199},
  year={2019},
  organization={IEEE Computer Society}
}

@inproceedings{burgess2019bfloat16,
  title={Bfloat16 processing for neural networks},
  author={Burgess, Neil and Milanovic, Jelena and Stephens, Nigel and Monachopoulos, Konstantinos and Mansell, David},
  booktitle={2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)},
  pages={88--91},
  year={2019},
  organization={IEEE}
}
@article{loper2002nltk,
  title={{NLTK}: The natural language toolkit},
  author={Loper, Edward and Bird, Steven},
  journal={ArXiv},
  year={2002},
  volume={abs/0205028}
}

@article{Liu2021PretrainPA,
  title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.13586}
}
@misc{lee2020language,
      title={Language Models as Fact Checkers?}, 
      author={Nayeon Lee and Belinda Z. Li and Sinong Wang and Wen-tau Yih and Hao Ma and Madian Khabsa},
      year={2020},
      eprint={2006.04102},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{lee_latent_2019,
	title = {Latent {Retrieval} for {Weakly} {Supervised} {Open} {Domain} {Question} {Answering}},
	journal = {arXiv:1906.00300 [cs]},
	author = {Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
	year = {2019},
	booktitle={ACL}
}


@article{naturalquestions,
title	= {Natural Questions: a Benchmark for Question Answering Research},
author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
year	= {2019},
journal	= {Transactions of the Association of Computational Linguistics}
}


@inproceedings{roberts_how_2020,
	address = {Online},
	title = {How {Much} {Knowledge} {Can} {You} {Pack} {Into} the {Parameters} of a {Language} {Model}?},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
	month = nov,
	year = {2020},
	pages = {5418--5426},
}

@article{triviaqa,
       author = {{Joshi}, Mandar and {Choi}, Eunsol and {Weld},
                 Daniel and {Zettlemoyer}, Luke},
        title = "{{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}",
      journal = {arXiv e-prints},
         year = 2017,
          eid = {arXiv:1705.03551},
        pages = {arXiv:1705.03551},
archivePrefix = {arXiv},
       eprint = {1705.03551},
}

@article{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{yang2019xlnet,
  title={{XLN}et: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{tpuacm,
author = {Jouppi, Norman P. and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Patil, Nishant and Laudon, James and Young, Cliff and Patterson, David},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3360307},
doi = {10.1145/3360307},
abstract = {Google's TPU supercomputers train deep neural networks 50x faster than general-purpose
supercomputers running a high-performance computing benchmark.},
journal = {Commun. ACM},
month = jun,
pages = {67‚Äì78},
numpages = {12}
}

@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={1137--1155},
  year={2003},
  publisher={JMLR.org}
}

@article{waugh1982marked,
  title={Marked and unmarked: A choice between unequals in semiotic structure},
  author={Waugh, Linda R},
  year={1982},
  publisher={Walter de Gruyter, Berlin/New York Berlin, New York}
}

@article{li2021systematic,
  title={A Systematic Investigation of Commonsense Understanding in Large Language Models},
  author={Li, Xiang Lorraine and Kuncoro, Adhi and d'Autume, Cyprien de Masson and Blunsom, Phil and Nematzadeh, Aida},
  journal={arXiv preprint arXiv:2111.00607},
  year={2021}
}

@inproceedings{kay2015unequal,
  title={Unequal representation and gender stereotypes in image search results for occupations},
  author={Kay, Matthew and Matuszek, Cynthia and Munson, Sean A},
  booktitle={Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
  pages={3819--3828},
  year={2015}
}

@inproceedings{rajpurkar2018know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@article{dastin2018Amazon,
      title={Amazon scraps secret {AI} recruiting tool that showed bias against women}, 
      author={Jeffrey Dastin},
      year={2018},
      publisher={Reuters}
}

@inproceedings{zhong2020reasoning,
    title = "Reasoning Over Semantic-Level Graph for Fact Checking",
    author = "Zhong, Wanjun  and
      Xu, Jingjing  and
      Tang, Duyu  and
      Xu, Zenan  and
      Duan, Nan  and
      Zhou, Ming  and
      Wang, Jiahai  and
      Yin, Jian",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.549",
    doi = "10.18653/v1/2020.acl-main.549",
    pages = "6170--6180",
    abstract = "Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.",
}

@InProceedings{soleimani2019bert,
author="Soleimani, Amir
and Monz, Christof
and Worring, Marcel",
editor="Jose, Joemon M.
and Yilmaz, Emine
and Magalh{\~a}es, Jo{\~a}o
and Castells, Pablo
and Ferro, Nicola
and Silva, M{\'a}rio J.
and Martins, Fl{\'a}vio",
title="{BERT} for Evidence Retrieval and Claim Verification",
booktitle="Advances in Information Retrieval",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="359--366",
abstract="We investigate BERT in an evidence retrieval and claim verification pipeline for the task of evidence-based claim verification. To this end, we propose to use two BERT models, one for retrieving evidence sentences supporting or rejecting claims, and another for verifying claims based on the retrieved evidence sentences. To train the BERT retrieval system, we use pointwise and pairwise loss functions and examine the effect of hard negative mining. Our system achieves a new state of the art recall of 87.1 for retrieving evidence sentences out of the FEVER dataset 50K Wikipedia pages, and scores second in the leaderboard with the FEVER score of 69.7.",
isbn="978-3-030-45442-5"
}



@inproceedings{parikh2016decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}

@inproceedings{automap2021,
title	= {Automap: Towards Ergonomic Automated Parallelism for ML Models},
author	= {Michael Schaarschmidt and Dominik Grewe and Dimitrios Vytiniotis and Adam Paszke and Georg Schmid and Tamara Norman and James Molloy and Jonathan Godwin and Norman A. Rink and Vinod Nair and Dan Belov},
year	= {2021},
booktitle	= {ML for Systems Workshop at NeurIPS 2021}
}

@article{kruengkrai2021multi,
  title={A Multi-Level Attention Model for Evidence-Based Fact Checking},
  author={Kruengkrai, Canasai and Yamagishi, Junichi and Wang, Xin},
  journal={arXiv preprint arXiv:2106.00950},
  year={2021}
}

@misc{sanh2021multitask,
      title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
      author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Stella Biderman and Leo Gao and Tali Bers and Thomas Wolf and Alexander M. Rush},
      year={2021},
      journal={arXiv preprint arXiv:2110.08207},
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{bigbench,
   title = "Beyond the Imitation Game: Measuring and extrapolating the capabilities of language models",
   author = "{BIG-bench collaboration}", 
   year = "2021",
   journal = "In preparation",
   url = "https://github.com/google/BIG-bench/"
   }
   
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{rosset2020turing,
  title={Turing-{NLG}: A 17-billion-parameter language model by {M}icrosoft},
  author={Rosset, Corby},
  journal={Microsoft Blog},
  volume={1},
  pages={2},
  year={2020}
}

@inproceedings{khashabi2020unifiedqa,
    title = "{UnifiedQA}: Crossing Format Boundaries with a Single {QA} System",
    author = "Khashabi, Daniel  and
      Min, Sewon  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.171",
    doi = "10.18653/v1/2020.findings-emnlp.171",
    pages = "1896--1907",
    abstract = "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.",
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{yu2021differentially,
  title={Differentially Private Fine-tuning of Language Models},
  author={Yu, Da and Naik, Saurabh and Backurs, Arturs and Gopi, Sivakanth and Inan, Huseyin A and Kamath, Gautam and Kulkarni, Janardhan and Lee, Yin Tat and Manoel, Andre and Wutschitz, Lukas and others},
  journal={arXiv preprint arXiv:2110.06500},
  year={2021}
}

@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{rae2020compressive,
  title={Compressive Transformers for Long-Range Sequence Modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P and Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6154--6158},
  year={2020},
  publisher={Association for Computing Machinery}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{sun2021long,
  title={Do Long-Range Language Models Actually Use Long-Range Context?},
  author={Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2109.09115},
  year={2021}
}

@inproceedings{lan2019albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{patterson2021carbon,
  author    = {David A. Patterson and
               Joseph Gonzalez and
               Quoc V. Le and
               Chen Liang and
               Lluis{-}Miquel Munguia and
               Daniel Rothchild and
               David R. So and
               Maud Texier and
               Jeff Dean},
  title     = {Carbon Emissions and Large Neural Network Training},
  journal   = {CoRR},
  volume    = {abs/2104.10350},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.10350},
  eprinttype = {arXiv},
  eprint    = {2104.10350},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-10350.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th USENIX Security Symposium (USENIX Security 19)},
  pages={267--284},
  year={2019}
}

@inproceedings{wallace-etal-2019-universal,
    title = "Universal Adversarial Triggers for Attacking and Analyzing {NLP}",
    author = "Wallace, Eric  and
      Feng, Shi  and
      Kandpal, Nikhil  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1221",
    doi = "10.18653/v1/D19-1221",
    pages = "2153--2162",
    abstract = "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94{\%} to 0.55{\%}, 72{\%} of {``}why{''} questions in SQuAD to be answered {``}to kill american people{''}, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",
}

@inproceedings{xu-etal-2021-bot,
    title = "Bot-Adversarial Dialogue for Safe Conversational Agents",
    author = "Xu, Jing  and
      Ju, Da  and
      Li, Margaret  and
      Boureau, Y-Lan  and
      Weston, Jason  and
      Dinan, Emily",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.235",
    doi = "10.18653/v1/2021.naacl-main.235",
    pages = "2950--2968",
    abstract = "Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or {''}baking-in{''} safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.",
}

@inproceedings{yu-sagae-2021-automatically,
    title = "Automatically Exposing Problems with Neural Dialog Models",
    author = "Yu, Dian  and
      Sagae, Kenji",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.37",
    pages = "456--470",
    abstract = "Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these problems are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as hate speech, while leaving systematic problems undercover. In this paper, we propose two methods including reinforcement learning to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models.",
}

@article{rae2021gopher,
  title={Scaling Language Models: Methods, Analysis \& Insights from Training {G}opher},
  author={Rae, Jack and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and de Masson d‚ÄôAutume, Cyprien and Li, Yujia and Terzi, Tayfun and Babuschkin, Igor and Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Bradbury, James and Johnson, Matthew and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  journal={arXiv 2112.11446},
  year={2021}
}

@article{chater1999search,
  title={The search for simplicity: A fundamental cognitive principle?},
  author={Chater, Nick},
  journal={The Quarterly Journal of Experimental Psychology Section A},
  volume={52},
  number={2},
  pages={273--302},
  year={1999},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{wolff1982language,
  title={Language acquisition, data compression and generalization},
  author={Wolff, J Gerard},
  journal={Language \& Communication},
  volume={2},
  number={1},
  pages={57--89},
  year={1982},
  publisher={Elsevier}
}


@article{legg2007universal,
  title={Universal intelligence: A definition of machine intelligence},
  author={Legg, Shane and Hutter, Marcus},
  journal={Minds and machines},
  volume={17},
  number={4},
  pages={391--444},
  year={2007},
  publisher={Springer}
}

@misc{moore1965cramming,
  title={Cramming more components onto integrated circuits},
  author={Moore, Gordon E and others},
  year={1965},
}

@article{askell2021general,
  title={A General Language Assistant as a Laboratory for Alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}


@misc{du2021glam,
  doi = {10.48550/ARXIV.2112.06905},
  url = {https://arxiv.org/abs/2112.06905},
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathy and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{rosenfeld2020a,
title={A Constructive Prediction of the Generalization Error Across Scales},
author={Jonathan S. Rosenfeld and Amir Rosenfeld and Yonatan Belinkov and Nir Shavit},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ryenvpEKDr}
}

@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019}
}


@article{robbins_stochastic_1951,
	title = {A {Stochastic} {Approximation} {Method}},
	volume = {22},
	number = {3},
	urldate = {2019-05-18},
	journal = {The Annals of Mathematical Statistics},
	author = {Robbins, Herbert and Monro, Sutton},
	month = sep,
	year = {1951},
	pages = {400--407},
}


@article{bubeck_convex_2015,
	title = {Convex {Optimization}: {Algorithms} and {Complexity}},
	volume = {8},
	shorttitle = {Convex {Optimization}},
	url = {http://www.nowpublishers.com/article/Details/MAL-050},
	language = {en},
	number = {3-4},
	urldate = {2019-02-05},
	journal = {Foundations and Trends in Machine Learning},
	author = {Bubeck, S√©bastien},
	year = {2015},
	pages = {231--357},
}



@article{siegel_approximation_2020,
	title = {Approximation rates for neural networks with general activation functions},
	volume = {128},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020301891},
	abstract = {We prove some new results concerning the approximation rate of neural networks with general activation functions. Our first result concerns the rate of approximation of a two layer neural network with a polynomially-decaying non-sigmoidal activation function. We extend the dimension independent approximation rates previously obtained to this new class of activation functions. Our second result gives a weaker, but still dimension independent, approximation rate for a larger class of activation functions, removing the polynomial decay assumption. This result applies to any bounded, integrable activation function. Finally, we show that a stratified sampling approach can be used to improve the approximation rate for polynomially decaying activation functions under mild additional assumptions.},
	language = {en},
	urldate = {2022-03-03},
	journal = {Neural Networks},
	author = {Siegel, Jonathan W. and Xu, Jinchao},
	month = aug,
	year = {2020},
	keywords = {Approximation theory, Neural networks, Stratified sampling},
	pages = {313--321},
}

@misc{izacard2020distilling,
      title={Distilling Knowledge from Reader to Retriever for Question Answering},
      author={Gautier Izacard and Edouard Grave},
      year={2020},
      eprint={2012.04584},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{nocedal_updating_1980,
	title = {Updating {Quasi}-{Newton} {Matrices} with {Limited} {Storage}},
	volume = {35},
	issn = {0025-5718},
	url = {https://www.jstor.org/stable/2006193},
	doi = {10.2307/2006193},
	abstract = {We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last \$m\$ iterations, where \$m\$ is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest information. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well-known methods.},
	number = {151},
	urldate = {2022-03-16},
	journal = {Mathematics of Computation},
	author = {Nocedal, Jorge},
	year = {1980},
	pages = {773--782},
}


@article{huber_robust_1964,
	title = {Robust {Estimation} of a {Location} {Parameter}},
	volume = {35},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full},
	doi = {10.1214/aoms/1177703732},
	number = {1},
	urldate = {2022-03-16},
	journal = {The Annals of Mathematical Statistics},
	author = {Huber, Peter J.},
	month = mar,
	year = {1964},
	pages = {73--101},
}
