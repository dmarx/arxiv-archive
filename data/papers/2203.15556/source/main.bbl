\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Artetxe et~al.(2021)Artetxe, Bhosale, Goyal, Mihaylov, Ott, Shleifer,
  Lin, Du, Iyer, Pasunuru, Anantharaman, Li, Chen, Akin, Baines, Martin, Zhou,
  Koura, O'Horo, Wang, Zettlemoyer, Diab, Kozareva, and
  Stoyanov]{artetxe2021efficient}
M.~Artetxe, S.~Bhosale, N.~Goyal, T.~Mihaylov, M.~Ott, S.~Shleifer, X.~V. Lin,
  J.~Du, S.~Iyer, R.~Pasunuru, G.~Anantharaman, X.~Li, S.~Chen, H.~Akin,
  M.~Baines, L.~Martin, X.~Zhou, P.~S. Koura, B.~O'Horo, J.~Wang,
  L.~Zettlemoyer, M.~Diab, Z.~Kozareva, and V.~Stoyanov.
\newblock Efficient {Large} {Scale} {Language} {Modeling} with {Mixtures} of
  {Experts}.
\newblock \emph{arXiv:2112.10684}, 2021.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell]{bender2021dangers}
E.~M. Bender, T.~Gebru, A.~McMillan-Major, and S.~Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 610--623, 2021.

\bibitem[{BIG-bench collaboration}(2021)]{bigbench}
{BIG-bench collaboration}.
\newblock Beyond the imitation game: Measuring and extrapolating the
  capabilities of language models.
\newblock \emph{In preparation}, 2021.
\newblock URL \url{https://github.com/google/BIG-bench/}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{piqa}
Y.~Bisk, R.~Zellers, J.~Gao, Y.~Choi, et~al.
\newblock {PIQA}: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem[Borgeaud et~al.(2021)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, van~den Driessche, Lespiau, Damoc, Clark, de~Las~Casas, Guy,
  Menick, Ring, Hennigan, Huang, Maggiore, Jones, Cassirer, Brock, Paganini,
  Irving, Vinyals, Osindero, Simonyan, Rae, Elsen, and
  Sifre]{borgeaud2021retrieval}
S.~Borgeaud, A.~Mensch, J.~Hoffmann, T.~Cai, E.~Rutherford, K.~Millican,
  G.~van~den Driessche, J.-B. Lespiau, B.~Damoc, A.~Clark, D.~de~Las~Casas,
  A.~Guy, J.~Menick, R.~Ring, T.~Hennigan, S.~Huang, L.~Maggiore, C.~Jones,
  A.~Cassirer, A.~Brock, M.~Paganini, G.~Irving, O.~Vinyals, S.~Osindero,
  K.~Simonyan, J.~W. Rae, E.~Elsen, and L.~Sifre.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock \emph{arXiv 2112.04426}, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin,
  G.~Necula, A.~Paszke, J.~Vander{P}las, S.~Wanderman-{M}ilne, and Q.~Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs.
\newblock 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,
  G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~Ziegler, J.~Wu, C.~Winter,
  C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark,
  C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Bubeck(2015)]{bubeck_convex_2015}
S.~Bubeck.
\newblock Convex {Optimization}: {Algorithms} and {Complexity}.
\newblock \emph{Foundations and Trends in Machine Learning}, 8\penalty0
  (3-4):\penalty0 231--357, 2015.
\newblock URL \url{http://www.nowpublishers.com/article/Details/MAL-050}.

\bibitem[Clark et~al.(2022)Clark, Casas, Guy, Mensch, Paganini, Hoffmann,
  Damoc, Hechtman, Cai, Borgeaud, Driessche, Rutherford, Hennigan, Johnson,
  Millican, Cassirer, Jones, Buchatskaya, Budden, Sifre, Osindero, Vinyals,
  Rae, Elsen, Kavukcuoglu, and Simonyan]{clark2022unified}
A.~Clark, D.~d.~l. Casas, A.~Guy, A.~Mensch, M.~Paganini, J.~Hoffmann,
  B.~Damoc, B.~Hechtman, T.~Cai, S.~Borgeaud, G.~v.~d. Driessche,
  E.~Rutherford, T.~Hennigan, M.~Johnson, K.~Millican, A.~Cassirer, C.~Jones,
  E.~Buchatskaya, D.~Budden, L.~Sifre, S.~Osindero, O.~Vinyals, J.~Rae,
  E.~Elsen, K.~Kavukcuoglu, and K.~Simonyan.
\newblock Unified scaling laws for routed language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.01169}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova]{clark2019boolq}
C.~Clark, K.~Lee, M.-W. Chang, T.~Kwiatkowski, M.~Collins, and K.~Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 2924--2936, 2019.

\bibitem[Du et~al.(2021)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson,
  Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du2021glam}
N.~Du, Y.~Huang, A.~M. Dai, S.~Tong, D.~Lepikhin, Y.~Xu, M.~Krikun, Y.~Zhou,
  A.~W. Yu, O.~Firat, B.~Zoph, L.~Fedus, M.~Bosma, Z.~Zhou, T.~Wang, Y.~E.
  Wang, K.~Webster, M.~Pellat, K.~Robinson, K.~Meier-Hellstern, T.~Duke,
  L.~Dixon, K.~Zhang, Q.~V. Le, Y.~Wu, Z.~Chen, and C.~Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts,
  2021.
\newblock URL \url{https://arxiv.org/abs/2112.06905}.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus2021switch}
W.~Fedus, B.~Zoph, and N.~Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}, 2021.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy]{pile}
L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang,
  H.~He, A.~Thite, N.~Nabeshima, S.~Presser, and C.~Leahy.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman2020realtoxicityprompts}
S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith.
\newblock {R}eal{T}oxicity{P}rompts: Evaluating neural toxic degeneration in
  language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 3356--3369, Online, Nov. 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.301}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.301}.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020realm}
K.~Guu, K.~Lee, Z.~Tung, P.~Pasupat, and M.-W. Chang.
\newblock {REALM}: Retrieval-augmented language model pre-training, 2020.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2020measuring}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and
  J.~Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and
  Babuschkin]{haiku2020github}
T.~Hennigan, T.~Cai, T.~Norman, and I.~Babuschkin.
\newblock {H}aiku: {S}onnet for {JAX}.
\newblock 2020.
\newblock URL \url{http://github.com/deepmind/dm-haiku}.

\bibitem[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and
  McCandlish]{hernandez2021scaling}
D.~Hernandez, J.~Kaplan, T.~Henighan, and S.~McCandlish.
\newblock Scaling laws for transfer, 2021.

\bibitem[Huber(1964)]{huber_robust_1964}
P.~J. Huber.
\newblock Robust {Estimation} of a {Location} {Parameter}.
\newblock \emph{The Annals of Mathematical Statistics}, 35\penalty0
  (1):\penalty0 73--101, Mar. 1964.
\newblock ISSN 0003-4851, 2168-8990.
\newblock \doi{10.1214/aoms/1177703732}.
\newblock URL
  \url{https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full}.

\bibitem[Izacard and Grave(2020)]{izacard2020distilling}
G.~Izacard and E.~Grave.
\newblock Distilling knowledge from reader to retriever for question answering,
  2020.

\bibitem[{Joshi} et~al.(2017){Joshi}, {Choi}, {Weld}, and
  {Zettlemoyer}]{triviaqa}
M.~{Joshi}, E.~{Choi}, D.~{Weld}, and L.~{Zettlemoyer}.
\newblock {{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset
  for Reading Comprehension}.
\newblock \emph{arXiv e-prints}, art. arXiv:1705.03551, 2017.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa,
  Bates, Bhatia, Boden, Borchers, Boyle, Cantin, Chao, Clark, Coriell, Daley,
  Dau, Dean, Gelb, Ghaemmaghami, Gottipati, Gulland, Hagmann, Ho, Hogberg, Hu,
  Hundt, Hurt, Ibarz, Jaffey, Jaworski, Kaplan, Khaitan, Killebrew, Koch,
  Kumar, Lacy, Laudon, Law, Le, Leary, Liu, Lucke, Lundin, MacKean, Maggiore,
  Mahony, Miller, Nagarajan, Narayanaswami, Ni, Nix, Norrie, Omernick,
  Penukonda, Phelps, Ross, Ross, Salek, Samadiani, Severn, Sizikov, Snelham,
  Souter, Steinberg, Swing, Tan, Thorson, Tian, Toma, Tuttle, Vasudevan,
  Walter, Wang, Wilcox, and Yoon]{10.1145/3079856.3080246}
N.~P. Jouppi, C.~Young, N.~Patil, D.~Patterson, G.~Agrawal, R.~Bajwa, S.~Bates,
  S.~Bhatia, N.~Boden, A.~Borchers, R.~Boyle, P.-l. Cantin, C.~Chao, C.~Clark,
  J.~Coriell, M.~Daley, M.~Dau, J.~Dean, B.~Gelb, T.~V. Ghaemmaghami,
  R.~Gottipati, W.~Gulland, R.~Hagmann, C.~R. Ho, D.~Hogberg, J.~Hu, R.~Hundt,
  D.~Hurt, J.~Ibarz, A.~Jaffey, A.~Jaworski, A.~Kaplan, H.~Khaitan,
  D.~Killebrew, A.~Koch, N.~Kumar, S.~Lacy, J.~Laudon, J.~Law, D.~Le, C.~Leary,
  Z.~Liu, K.~Lucke, A.~Lundin, G.~MacKean, A.~Maggiore, M.~Mahony, K.~Miller,
  R.~Nagarajan, R.~Narayanaswami, R.~Ni, K.~Nix, T.~Norrie, M.~Omernick,
  N.~Penukonda, A.~Phelps, J.~Ross, M.~Ross, A.~Salek, E.~Samadiani, C.~Severn,
  G.~Sizikov, M.~Snelham, J.~Souter, D.~Steinberg, A.~Swing, M.~Tan,
  G.~Thorson, B.~Tian, H.~Toma, E.~Tuttle, V.~Vasudevan, R.~Walter, W.~Wang,
  E.~Wilcox, and D.~H. Yoon.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock In \emph{Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, ISCA '17, page 1–12, New York, NY, USA, 2017.
  Association for Computing Machinery.
\newblock ISBN 9781450348928.
\newblock \doi{10.1145/3079856.3080246}.
\newblock URL \url{https://doi.org/10.1145/3079856.3080246}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child,
  S.~Gray, A.~Radford, J.~Wu, and D.~Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kudo and Richardson(2018)]{kudo2018sentencepiece}
T.~Kudo and J.~Richardson.
\newblock Sentence{P}iece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}, 2018.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Kelcey, Devlin, Lee, Toutanova, Jones,
  Chang, Dai, Uszkoreit, Le, and Petrov]{naturalquestions}
T.~Kwiatkowski, J.~Palomaki, O.~Redfield, M.~Collins, A.~Parikh, C.~Alberti,
  D.~Epstein, I.~Polosukhin, M.~Kelcey, J.~Devlin, K.~Lee, K.~N. Toutanova,
  L.~Jones, M.-W. Chang, A.~Dai, J.~Uszkoreit, Q.~Le, and S.~Petrov.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association of Computational Linguistics},
  2019.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{race}
G.~Lai, Q.~Xie, H.~Liu, Y.~Yang, and E.~Hovy.
\newblock {RACE}: Large-scale {R}e{A}ding comprehension dataset from
  examinations.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 785--794, Copenhagen, Denmark, Sept.
  2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D17-1082}.
\newblock URL \url{https://aclanthology.org/D17-1082}.

\bibitem[Levine et~al.(2020)Levine, Wies, Sharir, Bata, and
  Shashua]{levine2020depth}
Y.~Levine, N.~Wies, O.~Sharir, H.~Bata, and A.~Shashua.
\newblock The depth-to-width interplay in self-attention.
\newblock \emph{arXiv preprint arXiv:2006.12467}, 2020.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K\"{u}ttler, Lewis, Yih, Rockt\"{a}schel, Riedel, and
  Kiela]{lewisretrieval2020}
P.~Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal,
  H.~K\"{u}ttler, M.~Lewis, W.-t. Yih, T.~Rockt\"{a}schel, S.~Riedel, and
  D.~Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 9459--9474, 2020.

\bibitem[Lieber et~al.(2021)Lieber, Sharir, Lenz, and Shoham]{jurassic}
O.~Lieber, O.~Sharir, B.~Lenz, and Y.~Shoham.
\newblock Jurassic-1: Technical details and evaluation.
\newblock \emph{White Paper. AI21 Labs}, 2021.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{truthfulqa}
S.~Lin, J.~Hilton, and O.~Evans.
\newblock {TruthfulQA}: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish2018empirical}
S.~McCandlish, J.~Kaplan, D.~Amodei, and O.~D. Team.
\newblock An empirical model of large-batch training, 2018.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{wikitext103}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Mitchell et~al.(2019)Mitchell, Wu, Zaldivar, Barnes, Vasserman,
  Hutchinson, Spitzer, Raji, and Gebru]{mitchell2019model}
M.~Mitchell, S.~Wu, A.~Zaldivar, P.~Barnes, L.~Vasserman, B.~Hutchinson,
  E.~Spitzer, I.~D. Raji, and T.~Gebru.
\newblock Model cards for model reporting.
\newblock In \emph{Proceedings of the conference on fairness, accountability,
  and transparency}, pages 220--229, 2019.

\bibitem[Nocedal(1980)]{nocedal_updating_1980}
J.~Nocedal.
\newblock Updating {Quasi}-{Newton} {Matrices} with {Limited} {Storage}.
\newblock \emph{Mathematics of Computation}, 35\penalty0 (151):\penalty0
  773--782, 1980.
\newblock ISSN 0025-5718.
\newblock \doi{10.2307/2006193}.
\newblock URL \url{https://www.jstor.org/stable/2006193}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fernández]{paperno2016lambada}
D.~Paperno, G.~Kruszewski, A.~Lazaridou, Q.~N. Pham, R.~Bernardi, S.~Pezzelle,
  M.~Baroni, G.~Boleda, and R.~Fernández.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse
  context, 2016.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri,
  Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  de~Masson~d’Autume, Li, Terzi, Babuschkin, Clark, de~Las~Casas, Guy,
  Bradbury, Johnson, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell,
  Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and
  Irving]{rae2021gopher}
J.~Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song, J.~Aslanides,
  S.~Henderson, R.~Ring, S.~Young, E.~Rutherford, T.~Hennigan, J.~Menick,
  A.~Cassirer, R.~Powell, G.~van~den Driessche, L.~A. Hendricks, M.~Rauh, P.-S.
  Huang, A.~Glaese, J.~Welbl, S.~Dathathri, S.~Huang, J.~Uesato, J.~Mellor,
  I.~Higgins, A.~Creswell, N.~McAleese, A.~Wu, E.~Elsen, S.~Jayakumar,
  E.~Buchatskaya, D.~Budden, E.~Sutherland, K.~Simonyan, M.~Paganini, L.~Sifre,
  L.~Martens, X.~L. Li, A.~Kuncoro, A.~Nematzadeh, E.~Gribovskaya, D.~Donato,
  A.~Lazaridou, A.~Mensch, J.-B. Lespiau, M.~Tsimpoukelli, N.~Grigorev,
  D.~Fritz, T.~Sottiaux, M.~Pajarskas, T.~Pohlen, Z.~Gong, D.~Toyama,
  C.~de~Masson~d’Autume, Y.~Li, T.~Terzi, I.~Babuschkin, A.~Clark,
  D.~de~Las~Casas, A.~Guy, J.~Bradbury, M.~Johnson, L.~Weidinger, I.~Gabriel,
  W.~Isaac, E.~Lockhart, S.~Osindero, L.~Rimell, C.~Dyer, O.~Vinyals, K.~Ayoub,
  J.~Stanway, L.~Bennett, D.~Hassabis, K.~Kavukcuoglu, and G.~Irving.
\newblock Scaling language models: Methods, analysis \& insights from training
  {G}opher.
\newblock \emph{arXiv 2112.11446}, 2021.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, Lillicrap, Choromanski,
  Likhosherstov, Dohan, Song, Gane, Sarlos, et~al.]{rae2020compressive}
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, T.~P. Lillicrap, K.~Choromanski,
  V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarlos, et~al.
\newblock Compressive transformers for long-range sequence modelling.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6154--6158, 2020.

\bibitem[Raffel et~al.(2020{\natexlab{a}})Raffel, Shazeer, Roberts, Lee,
  Narang, Matena, Zhou, Li, and Liu]{raffel2019exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020{\natexlab{a}}.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Raffel et~al.(2020{\natexlab{b}})Raffel, Shazeer, Roberts, Lee,
  Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020{\natexlab{b}}.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2020zero}
S.~Rajbhandari, J.~Rasley, O.~Ruwase, and Y.~He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem[Robbins and Monro(1951)]{robbins_stochastic_1951}
H.~Robbins and S.~Monro.
\newblock A {Stochastic} {Approximation} {Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, Sept. 1951.

\bibitem[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and {Van
  Durme}]{rudinger2018gender}
R.~Rudinger, J.~Naradowsky, B.~Leonard, and B.~{Van Durme}.
\newblock Gender bias in coreference resolution.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, New Orleans, Louisiana, June 2018. Association for
  Computational Linguistics.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Le~Bras, Bhagavatula, and
  Choi]{winogrande}
K.~Sakaguchi, R.~Le~Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8732--8740, 2020.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{socialiqa}
M.~Sap, H.~Rashkin, D.~Chen, R.~LeBras, and Y.~Choi.
\newblock Social{IQA}: Commonsense reasoning about social interactions.
\newblock \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing}, 2019.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
C.~J. Shallue, J.~Lee, J.~Antognini, J.~Sohl-Dickstein, R.~Frostig, and G.~E.
  Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem[Siegel and Xu(2020)]{siegel_approximation_2020}
J.~W. Siegel and J.~Xu.
\newblock Approximation rates for neural networks with general activation
  functions.
\newblock \emph{Neural Networks}, 128:\penalty0 313--321, Aug. 2020.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0893608020301891}.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi,
  Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro]{nlg530b}
S.~Smith, M.~Patwary, B.~Norick, P.~LeGresley, S.~Rajbhandari, J.~Casper,
  Z.~Liu, S.~Prabhumoye, G.~Zerveas, V.~Korthikanti, E.~Zhang, R.~Child, R.~Y.
  Aminabadi, J.~Bernauer, X.~Song, M.~Shoeybi, Y.~He, M.~Houston, S.~Tiwary,
  and B.~Catanzaro.
\newblock {U}sing {D}eepspeed and {M}egatron to {T}rain {M}egatron-turing {NLG}
  530b, {A} {L}arge-{S}cale {G}enerative {L}anguage {M}odel.
\newblock \emph{arXiv preprint arXiv:2201.11990}, 2022.

\bibitem[Steinhardt(2021)]{forecast_blog}
J.~Steinhardt.
\newblock Updates and lessons from {AI} forecasting, 2021.
\newblock URL \url{https://bounded-regret.ghost.io/ai-forecasting/}.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Rao, Fedus, Abnar, Chung, Narang,
  Yogatama, Vaswani, and Metzler]{tay2021scale}
Y.~Tay, M.~Dehghani, J.~Rao, W.~Fedus, S.~Abnar, H.~W. Chung, S.~Narang,
  D.~Yogatama, A.~Vaswani, and D.~Metzler.
\newblock Scale efficiently: Insights from pre-training and fine-tuning
  transformers, 2021.

\bibitem[Thoppilan et~al.(2022)Thoppilan, Freitas, Hall, Shazeer, Kulshreshtha,
  Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang,
  Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhou, Chang, Krivokon,
  Rusch, Pickett, Meier-Hellstern, Morris, Doshi, Santos, Duke, Soraker,
  Zevenbergen, Prabhakaran, Diaz, Hutchinson, Olson, Molina, Hoffman-John, Lee,
  Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton, Cohen, Bernstein, Kurzweil,
  Aguera-Arcas, Cui, Croak, Chi, and Le]{thoppilan2022lamda}
R.~Thoppilan, D.~D. Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.-T. Cheng,
  A.~Jin, T.~Bos, L.~Baker, Y.~Du, Y.~Li, H.~Lee, H.~S. Zheng, A.~Ghafouri,
  M.~Menegali, Y.~Huang, M.~Krikun, D.~Lepikhin, J.~Qin, D.~Chen, Y.~Xu,
  Z.~Chen, A.~Roberts, M.~Bosma, Y.~Zhou, C.-C. Chang, I.~Krivokon, W.~Rusch,
  M.~Pickett, K.~Meier-Hellstern, M.~R. Morris, T.~Doshi, R.~D. Santos,
  T.~Duke, J.~Soraker, B.~Zevenbergen, V.~Prabhakaran, M.~Diaz, B.~Hutchinson,
  K.~Olson, A.~Molina, E.~Hoffman-John, J.~Lee, L.~Aroyo, R.~Rajakumar,
  A.~Butryna, M.~Lamm, V.~Kuzmina, J.~Fenton, A.~Cohen, R.~Bernstein,
  R.~Kurzweil, B.~Aguera-Arcas, C.~Cui, M.~Croak, E.~Chi, and Q.~Le.
\newblock {L}a{MDA}: Language models for dialog applications, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang,
  Cheng, Glaese, Balle, Kasirzadeh, Kenton, Brown, Hawkins, Stepleton, Biles,
  Birhane, Haas, Rimell, Hendricks, Isaac, Legassick, Irving, and
  Gabriel]{weidinger2021harms}
L.~Weidinger, J.~Mellor, M.~Rauh, C.~Griffin, J.~Uesato, P.-S. Huang, M.~Cheng,
  M.~Glaese, B.~Balle, A.~Kasirzadeh, Z.~Kenton, S.~Brown, W.~Hawkins,
  T.~Stepleton, C.~Biles, A.~Birhane, J.~Haas, L.~Rimell, L.~A. Hendricks,
  W.~Isaac, S.~Legassick, G.~Irving, and I.~Gabriel.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{arXiv submission}, 2021.

\bibitem[Welbl et~al.(2021)Welbl, Glaese, Uesato, Dathathri, Mellor, Hendricks,
  Anderson, Kohli, Coppin, and Huang]{welbl2021challenges}
J.~Welbl, A.~Glaese, J.~Uesato, S.~Dathathri, J.~Mellor, L.~A. Hendricks,
  K.~Anderson, P.~Kohli, B.~Coppin, and P.-S. Huang.
\newblock Challenges in detoxifying language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 2447--2469, Punta Cana, Dominican Republic, Nov. 2021.
  Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.210}.

\bibitem[Xu et~al.(2021)Xu, Pathak, Wallace, Gururangan, Sap, and
  Klein]{xu2021detoxifying}
A.~Xu, E.~Pathak, E.~Wallace, S.~Gururangan, M.~Sap, and D.~Klein.
\newblock Detoxifying language models risks marginalizing minority voices.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2390--2397, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.190}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.190}.

\bibitem[Yang et~al.(2021)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder,
  Pachocki, Chen, and Gao]{yang2021tuning}
G.~Yang, E.~J. Hu, I.~Babuschkin, S.~Sidor, X.~Liu, D.~Farhi, N.~Ryder,
  J.~Pachocki, W.~Chen, and J.~Gao.
\newblock Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Bx6qKuBM2AD}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock Hella{S}wag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue,
  and Grosse]{NEURIPS2019_e0eacd98}
G.~Zhang, L.~Li, Z.~Nado, J.~Martens, S.~Sachdeva, G.~Dahl, C.~Shallue, and
  R.~B. Grosse.
\newblock Which algorithmic choices matter at which batch sizes? insights from
  a noisy quadratic model.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf}.

\bibitem[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and
  Fedus]{zoph2022designing}
B.~Zoph, I.~Bello, S.~Kumar, N.~Du, Y.~Huang, J.~Dean, N.~Shazeer, and
  W.~Fedus.
\newblock Designing effective sparse expert models, 2022.

\end{thebibliography}
