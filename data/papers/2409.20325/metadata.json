{
  "arxivId": "2409.20325",
  "title": "Old Optimizer, New Norm: An Anthology",
  "authors": "Jeremy Bernstein, Laker Newhouse",
  "abstract": "Deep learning optimizers are often motivated through a mix of convex and\napproximate second-order theory. We select three such methods -- Adam, Shampoo\nand Prodigy -- and argue that each method can instead be understood as a\nsquarely first-order method without convexity assumptions. In fact, after\nswitching off exponential moving averages, each method is equivalent to\nsteepest descent under a particular norm. By generalizing this observation, we\nchart a new design space for training algorithms. Different operator norms\nshould be assigned to different tensors based on the role that the tensor plays\nwithin the network. For example, while linear and embedding layers may have the\nsame weight space of $\\mathbb{R}^{m\\times n}$, these layers play different\nroles and should be assigned different norms. We hope that this idea of\ncarefully metrizing the neural architecture might lead to more stable, scalable\nand indeed faster training.",
  "url": "https://arxiv.org/abs/2409.20325",
  "issue_number": 743,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/743",
  "created_at": "2025-01-02T19:30:50.474631",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2025-01-02T19:29:00.437Z",
  "main_tex_file": null,
  "published_date": "2024-09-30T14:26:12Z",
  "arxiv_tags": [
    "cs.LG",
    "math.OC"
  ]
}