\renewcommand{\arraystretch}{1.5}
\begin{table}
    \centering
    \begin{tabular}{cclcc}
        \toprule
        \textbf{Domain} & \textbf{Norm} & \hspace{1.3em}\textbf{Solution} & \textbf{Optimizer} & \textbf{Cousin}\\
        \midrule
        $\R^n$ & Euclidean $\ell_2$ & $\displaystyle\Delta \vw = -\tfrac{\norm{\vg}_2}{\lambda} \, \tfrac{\vg}{\norm{\vg}_2}$ & vanilla gradient descent & SGD \\
        $\R^n$ & infinity $\ell_\infty$ & $\displaystyle\Delta \vw = -\tfrac{\norm{\vg}_1}{\lambda} \sign(\vg)$ & sign descent & Adam\\
        \midrule
        $\R^{m\times n}$ & Frobenius $S_2$ & $\displaystyle\Delta \mW = -\tfrac{\norm{\mG}_F}{\lambda} \, \tfrac{\mG}{\norm{\mG}_F}$ & vanilla gradient descent & SGD\\
        $\R^{m\times n}$ & spectral $S_\infty$ & $\displaystyle\Delta \mW = -\tfrac{\trace \mSigma}{\lambda}\,\mU\mV^\top$ & spectral descent & Shampoo\\
        \bottomrule
    \end{tabular}
    \caption{Popular optimizers are related to steepest descent under different norms. For vector-valued optimization problems, we consider the steepest descent problem $\smash{\argmin_{\Delta \vw} \vg^\top \Delta \vw + \frac{\lambda}{2}\cdot \norm{\Delta \vw}^2}$. For matrix-valued problems, we consider $\smash{\argmin_{\Delta \mW} \, \langle\mG, \Delta \mW\rangle + \frac{\lambda}{2}\cdot \norm{\Delta \mW}^2}$, where $\langle\cdot,\cdot\rangle$ is the Frobenius inner product. We list the solution for different vector $\ell_p$ norms and Schatten $S_p$ norms. The Schatten $S_p$ norm of a matrix returns the $\ell_p$ norm of its vector of singular values. Finally, $\mG = \mU \mSigma \mV^\top $ is the reduced singular value decomposition of the gradient.}
    \label{tab:weight_update_rules}
\end{table}
\renewcommand{\arraystretch}{1}
