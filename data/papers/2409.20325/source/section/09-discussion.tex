\section*{Epilogue}
\label{sec:epilogue}

This anthology has presented new ways of understanding old optimizers. \cref{prop:steepest} decouples the optimizer design problem into two pieces: first choosing a norm and second finding a step size. This design space is already broad. We have argued that Adam chooses the infinity norm (\cref{prop:sign-descent}) or equivalently the max-of-max norm (\cref{prop:structural-sign-descent}), which respects a layered matrix structure. Shampoo chooses the spectral norm (\cref{prop:shampoo-steepest}). Prodigy chooses the same norm as Adam, and then uses a heuristic to automatically warm up to a good step size, as in \cref{eq:adaptive-prodigy}, which we term \textit{reaching escape velocity}.

Through the lens of steepest descent, the decisions that Adam, Shampoo and Prodigy make may seem arbitrary. In fact, we think that they \textit{are} somewhat arbitrary. And there may be more principled ways to make these decisions. To demonstrate this point, we now introduce a tool called the modular norm \citep{modula} and its corresponding steepest descent algorithm. The modular norm generalizes the norms that appeared in \cref{prop:structural-sign-descent} for Adam and \cref{prop:shampoo-steepest} for Shampoo. Formally:
\begin{myproposition}[Steepest descent under the modular norm]\label{prop:steepest-modular} Given scalar coefficients $s_1, \dots, s_L > 0$ and norms $\norm{\cdot}_1, \dots, \norm{\cdot}_L$, we define the modular norm as the mapping:
\begin{equation}
     \mW_1, \dots, \mW_L \mapsto \max\left\{s_1 \norm{\mW_1}_1, \dots, s_L \norm{\mW_L}_L \right\}.
\end{equation}
The corresponding steepest descent problem is given by:
\begin{equation}\label{eq:steepest-modular}
    \argmin_{\Delta \mW_1, \dots, \Delta \mW_L} \left[ \sum_{l=1}^L \langle \mG_l, \Delta \mW_l \rangle + \frac{\lambda}{2} \max_{l=1}^L s_l^2 \norm{\Delta\mW_l}_l^2 \right],
\end{equation}
where $\langle\cdot, \cdot\rangle$ denotes the Frobenius inner product, and for each $l=1,...,L$ the two matrices $\Delta \mW_l$ and $\mG_l$ are of the same shape. If we define the global step size $\eta = \frac{1}{\lambda}\sum_{k=1}^L \frac{1}{s_k}\norm{\mG_k}_k^\dagger$, then the solution to \cref{eq:steepest-modular} is given by:
\begin{align}
    \Delta \mW_l = - \frac{\eta}{s_l}\cdot \argmax_{\norm{\mT_l}_l=1} \, \langle\mG_l, \mT_l\rangle \quad\text{ for each layer } l = 1,...,L.
\end{align}
\end{myproposition}
In words, steepest descent under the modular norm updates each layer in a direction informed by that layer's norm and with a global step size computed as a weighted sum of the dual norms of the gradients over layers. The proof of this proposition is given in \cref{proof:steepest-modular}.

When confronted with the modular norm, it's natural to ask how one should assign norms to layers. And there are so many norms to choose from! Beyond the familiar $\ell_2 \to \ell_2$ spectral norm, many other induced operator norms are computationally tractable:
\begin{myproposition}[$\ell_1\to\ell_p$ and $\ell_p \to \ell_\infty$ induced operator norms are tractable]\label{prop:tractable-norms} For a matrix $\mM\in\R^{m\times n}$ with $m$ rows $\{\mathrm{row}_i(\mM)\}_{i=1}^m$ and $n$ columns $\{\mathrm{col}_j(\mM)\}_{j=1}^n$, and $1\leq p \leq \infty$:
    \begin{align}
        \norm{\mM}_{\ell_1\to \ell_p} = \max_j \norm{\mathrm{col}_j(\mM)}_p; \qquad
        \norm{\mM}_{\ell_p\to \ell_\infty}= \max_i \norm{\mathrm{row}_i(\mM)}_{\frac{p}{p-1}}.
\end{align}
\end{myproposition}
In words, the $\ell_1\to\ell_p$ operator norm is the largest $\ell_p$ norm of the columns; the $\ell_p\to\ell_\infty$ operator norm is the largest dual $\ell_p$ norm over the rows. The proof is given in \cref{proof:tractable-norms}.









To assign a norm to a layer, we believe that one should consider the role that layer plays in the neural network. For instance, since linear layers are typically used to map to and from vectors with roughly unit RMS norm, it is appropriate to equip linear layers with the induced RMS to RMS operator norm \citep{my-spectral}, which resolves to a rescaled spectral norm. And since embedding layers map from one-hot vectors to vectors with roughly unit RMS norm, it is appropriate to equip embedding layers with the $\ell_1$ to RMS operator norm, which resolves to a rescaled $\ell_1$ to $\ell_2$ operator norm. So embedding layers and linear layers should be equipped with different norms despite the weight space being a matrix space in both cases. In short, the algorithm designer has freedom to choose input and output norms for layers that capture differences in how the layers are used; inducing the corresponding operator norm on the layer's weights provides control over how the optimizer learns representations.


We believe that picking the right norms could improve the speed and scalability of neural network training. We are seeing evidence that equipping neural network layers with better norms can lead to learning rate transfer across scale \citep{my-spectral,modula}. And since Shampoo won the external tuning track of the 2024 AlgoPerf competition \citep{Dahl2023AlgoPerf}, it is garnering interest as a fast training method. The second story in our anthology shows that Shampoo is closely connected to the spectral norm.

In conclusion, this work highlights a perspective on optimizer design as choosing two things: a \textit{norm} and a \textit{step size}. We have shown that three popular methods---Adam, Shampoo and Prodigy---fit within this perspective. We hope that researchers can design improved training algorithms by choosing norms and step sizes more intentionally.

\vspace{1em}


\hfill \textit{``Though this be madness, yet there is method in't.''}\\
\vspace{-2ex}
\hfill Hamlet

















