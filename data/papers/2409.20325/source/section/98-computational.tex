\section{Computational Strategies for Shampoo}
\label{app:shampoo}

Let $\mG\in\R^{m\times n}$ be a gradient matrix with reduced SVD $\mG = \mU \mSigma \mV^\top$. By \cref{eq:funky-shampoo,eq:orthog-shampoo}, the corresponding Shampoo update (with EMA disabled) is given by:
\begin{equation}\label{eq:shampoo-abridged}
    \Delta \mW = - \eta \cdot (\mG\mG^\top)^{-\sfrac{1}{4}} \,\mG\, (\mG^\top\mG)^{-\sfrac{1}{4}} = - \eta \cdot \mU \mV^\top.
\end{equation}
Here we list every means we know of computing or approximating this equation. First, we mention that $(\mG\mG^\top)^{-\sfrac{1}{4}} \,\mG\, (\mG^\top\mG)^{-\sfrac{1}{4}} = (\mG\mG^\top)^{-\sfrac{1}{2}} \,\mG = \mG\, (\mG^\top\mG)^{-\sfrac{1}{2}}$, so if one is willing to compute inverse matrix roots, one need only compute either $(\mG\mG^\top)^{-\sfrac{1}{2}}$ or $(\mG^\top\mG)^{-\sfrac{1}{2}}$, whichever has smaller dimension. With that said, to compute \cref{eq:shampoo-abridged}, one may:
\begin{enumerate}
    \item \textbf{Do the SVD.} Apply an SVD routine to compute $\mU$, $\mSigma$ and $\mV^\top$ and just discard $\mSigma$.
    \item \textbf{Do sketching.} Sketching is a randomized method \citep{Martinsson_Tropp_2020} that can be used to approximate the SVD. See, for instance, Sketchy \citep{sketchy} and spectral descent for deep learning \citep{spectral-descent}.
    \item \textbf{Do Newton iteration for inverse $p$th roots.} Inverse matrix roots such as $(\mG\mG^\top)^{-\sfrac{1}{2}}$ can be computed via Newton iteration \citep{lakic}. This is discussed in Chapter 7 of \citet{higham}'s book. And see \citet{Anil2020ScalableSecondOrder}'s paper.
    \item \textbf{Do Newton-Schulz iteration.} We developed a ``Newton-Schulz iteration'' for computing $\mU\mV^\top$, adapted from Equation 5.22 in \citet{higham}'s book. In short, if we set $\mX_0 = \mG / \norm{\mG}_{\ell_2 \to \ell_2}$ (or alternatively $\mX_0 = \mG / \norm{\mG}_F$) and iterate:
    \begin{equation}\label{eq:newton-schulz}
        \mX_{t+1} = \frac{3}{2} \cdot \mX_t - \frac{1}{2} \cdot \mX_t \mX_t^\top \mX_t,
    \end{equation}
    then as $t\to\infty$, the sequence $\mX_t \to \mU \mV^\top$. To see this, one should plot the univariate cubic function $f(x) \defeq \tfrac{3}{2} \cdot x - \tfrac{1}{2}\cdot x^3$ and see that, for $0 < x < \sqrt{3}$, iterating this cubic will push $x$ closer and closer to $+1$. The final step is to realize that the effect of the iteration in \cref{eq:newton-schulz} is to apply this cubic $f(x)$ to each singular value of $\mX_t$. This also shows that the spectral normalization $\mX_0 = \mG / \norm{\mG}_{\ell_2 \to \ell_2}$ is stronger than what is required: we need only ensure that $\mX_0$ has all singular values greater than zero and less than $\sqrt{3}$ in order for the iteration to converge.

    There are in fact a family of degree $2n+1$ polynomial iterations of the form
    \begin{equation}
    \mX_{t+1} = a \cdot \mX_t + b  \cdot \mX_t \mX_t^\top \mX_t + c \cdot (\mX_t \mX_t^\top)^2 \mX_t + ... + z \cdot (\mX_t \mX_t^\top)^n \mX_t
    \end{equation}
    for suitable $a,b,c,...,z$ that could be used instead of \cref{eq:newton-schulz}. One should choose coefficients $a, b,c,...,z$ so that the univariate polynomial $g(x) = a \cdot x + b \cdot x^3 + c \cdot x^5 + ... + z\cdot x^{2n+1}$ is a suitable approximation to $\sign(x)$. The coefficients can be tuned graphically to achieve the fastest convergence.

    After posting the first version of this paper on arXiv, we found out that the iteration, at least for fixed coefficients, is classical \citep{kovarik1970iterative,bjoerck1971}.
\end{enumerate}
Which of these methods is most useful in practice may depend on factors such as the condition number of the matrix $\mG$ or the nature of the available computational resources.

