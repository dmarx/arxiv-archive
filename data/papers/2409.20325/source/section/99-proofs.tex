\section{Proofs}

\renewcommand{\proofname}{\hspace{-0.315em}}

\subsection*{\cref{prop:steepest}: \nameref*{prop:steepest}} \label{proof:steepest}

\begin{proof}
First, let's study the minimization under the change of variables $\Delta \vw = c \cdot \vt$, where $c \geq 0$ encodes the ``magnitude'' and $\vt$ is a unit vector ($\norm{\vt}=1$) encoding the ``direction'':
\begin{align}
    \min_{\Delta \vw \in \R^n} \left[\vg^\top \Delta \vw + \frac{\lambda}{2} \, \norm{\Delta \vw}^2 \right] &= \min_{c \geq 0} \min_{\vt\in\R^n : \norm{\vt}=1} \left[c \cdot \vg^\top \vt + \frac{\lambda}{2} c^2 \norm{\vt}^2 \right] \\
    &= \min_{c \geq 0} \left[c \cdot \min_{\vt\in\R^n : \norm{\vt}=1} \left[\vg^\top \vt\right] + \frac{\lambda}{2} c^2 \right] \label{eq:changed} \\
    &= \min_{c \geq 0} \left[ - c \cdot \norm{\vg}^\dagger+ \frac{\lambda}{2} c^2 \right], \label{eq:changed-again}
\end{align}
Inspecting \cref{eq:changed}, we see that the minimizer for the direction $\vt$ is given by:
\begin{align}
    \vt &= \argmin_{\vt\in\R^n : \norm{\vt}=1} \left[\vg^\top \vt\right] = - \argmax_{\vt\in\R^n : \norm{\vt}=1} \left[\vg^\top \vt\right]
\end{align}
And similarly, by inspecting \cref{eq:changed-again}, the minimizer for the magnitude $c$ is given by:
\begin{align}
    c &= \argmin_{c \geq 0} \left[ - c \cdot \norm{\vg}^\dagger + \frac{\lambda}{2} c^2 \right] = \frac{\norm{\vg}^\dagger}{\lambda}.
\end{align}
Multiplying these expressions, we obtain the minimizer for $\Delta \vw$, yielding the result.
\end{proof}




\subsection*{\cref{prop:sign-descent}: \nameref*{prop:sign-descent}} \label{proof:sign-descent}

\begin{proof}
The result follows by applying \cref{prop:steepest}. We just need that $\argmax_{\norm{\vt}_\infty = 1} \vg^\top \vt = \sign(\vg)$, and also that the dual norm $\norm{\vg}_\infty^\dagger \defeq \max_{\norm{\vt}_\infty = 1} \vg^\top \vt = \vg^\top\sign(\vg)= \norm{\vg}_1$.
\end{proof}




\subsection*{\cref{prop:structural-sign-descent}: \nameref*{prop:structural-sign-descent}} \label{proof:structural-sign-descent}

\begin{proof}
The result follows from \cref{prop:steepest-modular} by setting all the scalars $s_1,...,s_L$ to one and all the norms $\norm{\cdot}_1, ..., \norm{\cdot}_L$ to the $\ell_1$ to $\ell_\infty$ operator norm. All we need is to show that the argmax at each matrix space $l=1,...,L$ satisfies:
\begin{equation}
    \argmax_{\norm{\mT_l}_{\ell_1 \to \ell_\infty} = 1} \trace(\mG_l^\top \mT_l) = \sign(\mG_l).
\end{equation} 
But this holds because, by \cref{prop:tractable-norms}, $\norm{\mT}_{\ell_1 \to \ell_\infty} = \max_i \norm{\mathrm{col}_i(\mT)}_\infty = \max_{ij} \abs{\mT_{ij}}$, and therefore all components in the argmax must be of unit size and gradient aligned.
\end{proof}





\subsection*{\cref{prop:projection}: \nameref*{prop:projection}} \label{proof:projection}

\begin{proof}
To begin, we observe that the minimizer over semi-orthogonal matrices of the ``distance'' $\norm{\mA - \mG}_F$ is the same as the maximizer over semi-orthogonal matrices of the ``alignment'' $\langle \mA, \mG\rangle$, where $\langle\cdot,\cdot\rangle$ denotes the Frobenius inner product. This is because:
\begin{align}
    \norm{\mA - \mG}_F^2 &= \norm{\mA}_F^2 - 2 \cdot \langle \mA, \mG \rangle + \norm{\mG}_F^2,
\end{align}
and the term $\norm{\mA}_F^2$ is fixed at $\norm{\mA}_F^2 = \min(m,n)$ for a semi-orthogonal matrix $\mA \in \mathcal{O}_{m\times n}$.

Now, let $\mG = \sum_i \sigma_i \, \vu_i\vv_i^\top$ denote the SVD of $\mG$. Then the alignment satisfies:
\begin{align}\label{eq:bound}
    \langle \mA, \mG \rangle = \trace \sum_i \sigma_i \,\vv_i\vu_i^\top \mA = \sum_i \sigma_i \, \vu_i^\top \mA \vv_i \leq \sum_i \sigma_i,
\end{align}
where the second equality follows by the cyclic property of the trace, and the inequality is since $\mA$ being semi-orthogonal means that $\vu^\top \mA \vv \leq 1$ for any two unit vectors $\vu$ and $\vv$.

Next, observe that for the semi-orthogonal matrix $\mA_\star = \sum_i \vu_i \vv_i^\top$, we have that:
\begin{equation}
    \langle \mA_\star, \mG \rangle = \sum_i \sigma_i \sum_j \vu_i^\top \vu_j \vv_j^\top \vv_i = \sum_i \sigma_i,
\end{equation}
since the $\{\vu_i\}$ and $\{\vv_i\}$ are orthonormal. Comparing against \cref{eq:bound}, we see that $\mA_\star$ indeed maximizes the alignment, since it achieves the upper bound of $\sum_i \sigma_i$. And $\mA_\star$ therefore also minimizes the distance $\norm{\mA - \mG}_F$ amongst semi-orthogonal matrices $\mA$. Note that if $\mU$ is the matrix that has the $\{\vu_i\}$ as columns, and likewise for $\mV$ and the $\{\vv_i\}$, then this solution may equivalently be expressed as $\mA_\star = \mU\mV^\top$.

All that remains is to explore the uniqueness of this solution:
\begin{itemize}
    \item If $\mG$ is full rank, the solution $\mA_\star$ is unique. $\mG$ being full rank means that all the singular values $\sigma_i$ are positive. In this case, we see from \cref{eq:bound} that to maximize the alignment the semi-orthogonal matrix $\mA$ must satisfy $\vu_i^\top \mA \vv_i =1$ for all $i$. Since $\mA$ has spectral norm one, in turn this requires that
$\mA \vv_i = \vu_i$ and $\mA^\top \vu_i = \vv_i$ for all $i$. These conditions uniquely pick out $\mA = \sum_i \vu_i \vv_i^\top$.
    \item If $\mG$ is not full rank then the solution $\mA_\star$ is not unique. This solution is just as good:
    \begin{equation}
        \mA_\dagger = \sum_{i:\sigma_i > 0} \vu_i \vv_i^\top + \sum_{i:\sigma_i = 0}\vu_i (-\vv_i)^\top.
    \end{equation}
\end{itemize}
This completes the proof.\end{proof}

















\subsection*{\cref{prop:shampoo-steepest}: \nameref*{prop:shampoo-steepest}} \label{proof:shampoo-steepest}

\begin{proof}
First, we apply \cref{prop:steepest-modular} with scalars $s_1,...,s_L$ set to one and all norms set to $\norm{\cdot}_{\ell_2\to\ell_2}$. This tells us that the solution is given by $\Delta \mW_l = - \eta\cdot \argmax_{\norm{\mT_l}_l=1} \trace(\mG_l^\top \mT_l)$ for each $l=1,...,L$ and with $\eta = \frac{1}{\lambda}\sum_{k=1}^L \norm{\mG_k}_{\ell_2\to\ell_2}^\dagger$. We just need to resolve the dual norm and evaluate the argmax.

Let's start with the dual norm. For a matrix $\mG$ with SVD $\sum_i \sigma_i \, \vu_i \vv_i^\top = \mU \mSigma \mV^\top$ we have:
\begin{align}
    \norm{\mG}_{\ell_2\to\ell_2}^\dagger \defeq \max_{\norm{\mT}_{\ell_2\to\ell_2}=1}\trace \mG^\top \mT &= \max_{\norm{\mT}_{\ell_2\to\ell_2}=1} \trace \sum_i \sigma_i \, \vv_i \vu_i^\top \mT \\ &= \max_{\norm{\mT}_{\ell_2\to\ell_2}=1}\sum_i \sigma_i\, \vu_i^\top \mT \vv_i \leq \sum_i \sigma_i = \trace \mSigma,
\end{align}
where the upper bound follows from the spectral norm constraint on $\mT$. But this upper bound is attained by setting $\mT = \mU\mV^\top$ (also resolving the argmax) and so $\norm{\mG}_{\ell_2\to\ell_2}^\dagger = \trace \mSigma$.

The uniqueness claim follows by the same argument as for \cref{prop:projection}. \end{proof}

\subsection*{\cref{prop:majorization}: \nameref*{prop:majorization}} \label{proof:majorization}

\begin{proof}
    First observe that the square loss is quadratic in $\mW$ so there are no cubic terms or higher. The bound must agree to first-order with the first-order Taylor expansion of $\el(\mW+\Delta \mW)$, which is precisely $\el(\mW) + \langle\nabla_\mW \el(\mW), \Delta \mW\rangle$, since otherwise the bound would be violated for sufficiently small $\Delta\mW$. To obtain the second-order piece of the bound, it's easiest just to multiply out $\el(\mW+\Delta\mW)$ and see that the second-order piece of $\el(\mW+\Delta \mW)$ satisfies:
    \begin{align}
        \frac{1}{2n}\sum_{i=1}^n\frac{1}{d_\out}\norm{\Delta \mW \vx^{(i)}}_2^2 \leq \frac{1}{2n}\sum_{i=1}^n\frac{1}{d_\out}\norm{\Delta \mW}_{\ell_2\to\ell_2}^2 \cdot \norm{\vx^{(i)}}_2^2
        =\frac{1}{2}\frac{d_\inn}{d_\out}\norm{\Delta \mW}_{\ell_2\to\ell_2}^2,
    \end{align}
    where the last equality uses the input normalization $\norm{\vx^{(i)}}_2 = \sqrt{d_\inn}$. We are done.
\end{proof}

\subsection*{\cref{prop:steepest-modular}: \nameref*{prop:steepest-modular}} \label{proof:steepest-modular}


\begin{proof}
For each layer $l=1,...,L$, we decompose $\Delta\mW_l$ into its magnitude and direction: $\Delta \mW_l = c_l \cdot \mT_l$, for $c_l \geq 0$ and $\norm{\mT_l}_l = 1$. Under this change of variables, the minimization becomes:
\begin{align}
    &\min_{\Delta \mW_1, \dots, \Delta \mW_L} \left[ \sum_{l=1}^L \langle \mG_l, \Delta \mW_l \rangle + \frac{\lambda}{2} \max_{l=1}^L s_l^2 \norm{\Delta\mW_l}_l^2 \right] \\
    &\qquad= \min_{c_1, \dots, c_L \geq 0} \left[ \sum_{l=1}^L c_l \min_{\norm{\mT_l}_l=1} \langle \mG_l, \mT_l \rangle + \frac{\lambda}{2} \max_{l=1}^L s_l^2 c_l^2 \right] \label{eq:many-var}\\
    &\qquad= \min_{c_1, \dots, c_L \geq 0} \left[ -\sum_{l=1}^L c_l \norm{\mG_l}_l^\dagger + \frac{\lambda}{2} \max_{l=1}^L s_l^2 c_l^2 \right]\label{eq:many-var2}\\
    &\qquad= \min_{\eta\geq 0} \left[ -\sum_{l=1}^L \frac{\eta}{s_l} \norm{\mG_l}_l^\dagger + \frac{\lambda}{2} \eta^2 \right], \label{eq:one-variable}
\end{align}
where \cref{eq:one-variable} follows by observing that at the minimum we must have $s_1 c_1, ..., s_L c_L$ all taking the same value of $\eta\geq 0$ (still to be determined), since otherwise we could increase the sum $\sum_l c_l \norm{\mG_l}_l^\dagger$ by increasing any of the slack $c_l$ without paying a penalty in terms of the max. We can now read off the minimizers from \cref{eq:many-var,eq:many-var2,eq:one-variable}:
\begin{align}
    \mT_l &= \argmin_{\norm{\mT_l}_l = 1} \, \langle \mG_l, \mT_l \rangle = - \argmax_{\norm{\mT_l}_l = 1} \, \langle \mG_l, \mT_l \rangle; \\
    c_l &= \frac{\eta}{s_l}; \\
    \eta &= \frac{1}{\lambda} \sum_{k=1}^L \frac{1}{s_k} \norm{\mG_k}_k^\dagger. \label{eq:eta}
\end{align}
Combining, we obtain the overall minimizer for each $l=1,...,L$ via $\Delta \mW_l = c_l \cdot \mT_l = - \frac{\eta}{s_l} \argmax \, \langle \mG_l, \mT_l \rangle$, where $\eta$ is given by \cref{eq:eta}, proving the result.
\end{proof}

\subsection*{\cref{prop:tractable-norms}: \nameref*{prop:tractable-norms}} \label{proof:tractable-norms}

\begin{proof}
Let's start with the $\ell_1\to\ell_p$ operator norm. Here we observe that, in matrix-vector multiplication, each component of an input vector selects and scales a column of the matrix:
\begin{align}
    \norm{\mM}_{\ell_1 \to \ell_p} = \max_{\norm{\vx}_1 = 1} \norm{\mM \vx}_p = \max_{\norm{\vx}_1 = 1} \Big\|\sum_j \mathrm{col}_j(\mM) \vx_j\Big\|_p &\leq \max_{\norm{\vx}_1 = 1} \sum_j \abs{\vx_j}\cdot \norm{\mathrm{col}_j(\mM)}_p\\
    &\leq \max_{\norm{\vx}_1 = 1} \norm{\vx}_1 \cdot \max_j \norm{\mathrm{col}_j(\mM)}_p\\
    &= \max_j \norm{\mathrm{col}_j(\mM)}_p,\label{eq:upper-bound}
\end{align}
by the triangle inequality and H\"older's inequality. But the upper bound in \cref{eq:upper-bound} is attained by selecting the column index $j_\star = \argmax_j \norm{\mathrm{col}_j(\mM)}_p$ with the largest norm, then setting $\vx_{j_\star}=1$ and the other input components to zero. So $\norm{\mM}_{\ell_1 \to \ell_p} = \max_j \norm{\mathrm{col}_j(\mM)}_p.$

Next, let's deal with the $\ell_p \to \ell_\infty$ operator norm. Here we break up a matrix-vector product in terms of the dot product between the vector and the matrix rows:
\begin{align}
    \norm{\mM}_{\ell_p \to \ell_\infty} = \max_{\norm{\vx}_p = 1} \norm{\mM \vx}_\infty &= \max_{\norm{\vx}_p = 1} \max_i \abs{\vx^\top \mathrm{row}_i(\mM) } \\
    &= \max_i \max_{\norm{\vx}_p = 1} \abs{\vx^\top \mathrm{row}_i(\mM) } \\
    &= \max_i \norm{\mathrm{row}_i(\mM)}_p^\dagger.
\end{align}
The proof is completed by recalling that the vector $\ell_p$ norm is dual to the vector $\ell_q$ norm for $1/p + 1/q =1$. In other words, $\norm{\cdot}_p^\dagger = \norm{\cdot}_{\frac{p}{p-1}}$.
\end{proof}
