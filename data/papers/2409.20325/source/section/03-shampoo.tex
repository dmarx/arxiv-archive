\section{Shampoo as Steepest Descent under the Spectral Norm}
\label{sec:shampoo}

\lettrine{N}{ow}, dear reader, we turn our attention to Shampoo \citep{unified-shampoo,Gupta2018ShampooPS}. A variant of the Shampoo optimizer won the external tuning track of the \textit{2024 AlgoPerf: Training Algorithms} competition \citep{Dahl2023AlgoPerf}. While the method was originally motivated as a generalization of the AdaGrad convex optimizer \citep{Duchi2011AdaptiveSM} to tensor spaces, more recent work casts Shampoo as an approximate second-order method \citep{Anil2020ScalableSecondOrder,Morwani2024NewPerspective}. We will show that Shampoo---with accumulation disabled---is steepest descent under the max spectral norm over layers.

To begin, we show that Shampoo updates, without accumulation, are semi-orthogonal matrices. At time step $t$ and for each layer, Shampoo collects the gradient matrix $\mG_t$ and makes the following update to the weight matrix $\mW_t$:
\begin{align}
    \mL_t &= \mL_{t-1} + \mG_t \mG_t^T, \label{eq:L}\\
    \mR_t &= \smash{\mR_{t-1} + \mG_t^T \mG_t}, \label{eq:R}\\
    \mW_{t+1} &= \smash{\mW_t - \eta \cdot \mL_t^{-\sfrac{1}{4}} \mG_t \mR_t^{-\sfrac{1}{4}}}.
\end{align}
All operations, including the inverse fourth roots, are matrix operations. The accumulators $\mL_t$ and $\mR_t$ are referred to as the ``left and right pre-conditioners''. Practitioners usually replace the simple sums in \cref{eq:L,eq:R} with EMAs \citep{Shi2023DistributedShampoo}. If we disable the accumulation, setting $\mL_t = \mG_t\mG_t^\top$ and $\mR_t = \mG_t^\top \mG_t$, Shampoo reduces to:
\begin{align}
    \mW_{t+1} &= \smash{\mW_t - \eta \cdot (\mG_t\mG_t^\top)^{-\sfrac{1}{4}} \,\mG_t\, (\mG_t^\top\mG_t)^{-\sfrac{1}{4}}} \label{eq:funky-shampoo}\\
    &= \smash{\mW_t - \eta \cdot \mU_t \mV_t^\top}, \label{eq:orthog-shampoo}
\end{align}
where \cref{eq:orthog-shampoo} is reached by substituting the reduced singular value decomposition (SVD) of the gradient $\mG_t = \mU_t \mSigma_t \mV_t^\top$ into \cref{eq:funky-shampoo}. Notice that there is a direct parallel between \cref{eq:funky-adam,eq:sign-adam} for Adam and \cref{eq:funky-shampoo,eq:orthog-shampoo} for Shampoo. So, Shampoo without accumulation makes a \textit{semi-orthogonal weight update}. In fact:
\begin{myproposition}[Projection to the closest semi-orthogonal matrix]\label{prop:projection} Consider the\\semi-orthogonal matrices $\mathcal{O}_{m \times n} \defeq \left\{ \mA \in \R^{m \times n} : \mA\mA^\top = \Id_{m} \text{ or } \mA^\top \mA = \Id_{n}\right\}$ and let $\norm{\cdot}_F$ denote the Frobenius norm. For any matrix $\mG \in \R^{m \times n}$ with reduced SVD $\mG = \mU \mSigma \mV^\top$:
\begin{equation}
    \argmin_{\mA \in \mathcal{O}_{m \times n}} \norm{\mA - \mG}_F = \mU \mV^\top,
\end{equation}
where the minimizer $\mU\mV^\top$ is unique if and only if the matrix $\mG$ has full rank.
\end{myproposition}
So, Shampoo without accumulation projects the gradient matrix to the \textit{closest semi-orthogonal matrix in Frobenius norm}. The proof is in \cref{proof:projection}. \textit{Why might this be a good idea}, you ask? Well, for one thing, it's steepest descent---this time under the maximum \textit{spectral norm} $\norm{\cdot}_{\ell_2 \to \ell_2
}$ (\cref{def:induced}) over all the matrices in the network:

\begin{myproposition}[Shampoo as steepest descent under the spectral norm]\label{prop:shampoo-steepest} 
    For any list of gradient matrices $\mG_1,...,\mG_L$ and any sharpness $\lambda > 0$, consider the problem:
    \begin{equation}\label{eq:max-of-spectral}
        \argmin_{\Delta \mW_1,...,\Delta \mW_L} \left[ \sum_{l=1}^L \langle\mG_l, \Delta \mW_l\rangle + \frac{\lambda}{2} \, \max_{l=1}^L \norm{\Delta\mW_l}_{\ell_2\to\ell_2}^2 \right],
    \end{equation}
    where $\langle\cdot, \cdot\rangle$ denotes the Frobenius inner product and $\Delta \mW_l$ has the same shape as $\mG_l$. Suppose that $\mG_l$ has reduced SVD given by $\mG_l = \mU_l \mSigma_l \mV_l^\top$ for each $l=1,...,L$. Then \cref{eq:max-of-spectral} is solved with a step size $\eta = \frac{1}{\lambda}\sum_{l=1}^L \trace \mSigma_l$ and an update:
    \begin{equation}\label{eq:shampoo-layerwise}
        \Delta \mW_l = -\eta \cdot \mU_l \mV_l^\top \quad \text{ for each } l=1,...,L.
    \end{equation}
    This solution for $\Delta \mW_l$ is unique if and only if the matrix $\mG_l$ is of full rank.
\end{myproposition}
The proof is given in \cref{proof:shampoo-steepest}. A novelty of this proposition in contrast to prior work on stochastic spectral descent \citep{spectral-descent-2, spectral-descent-1} is our use of a max norm over layers to handle the multi-layer case. However, our main contribution here is to draw the connection between \cref{prop:shampoo-steepest} and Shampoo as in \cref{eq:funky-shampoo,eq:orthog-shampoo}.

So, Shampoo without accumulation is steepest descent under the spectral norm. \textit{Why might this be a good idea in deep learning?} The idea that we wish to advance is that one can derive upper bounds on the loss of machine learning models in terms of spectral norms. Here we present the simplest possible example: a linear model and the square loss.

\begin{myproposition}[Bounding the square loss of a linear predictor]\label{prop:majorization} Consider a matrix $\mW \in \R^{d_\mathrm{out}\times d _\mathrm{in}}$ that we shall think of as a linear predictor mapping an input $\vx \in \R^{d_\mathrm{in}}$ to an output $\vy = \mW \vx \in \R^{d_\mathrm{out}}$. Given a dataset of $n$ samples $\mathcal{D} = \{(\vx_1,\vy_1), ..., (\vx_n,\vy_n)\}$, where the $i$th input is normalized such that $\|\vx_i\|_2 = \sqrt{d_\inn}$, we can construct the ``square loss'':
\begin{equation}
    \el(\mW) \defeq \frac{1}{2n}\sum_{i=1}^n \frac{1}{d_\out}\norm{\vy_i - \mW \vx_i}_2^2.
\end{equation}
Then, for any matrix $\Delta\mW \in \R^{d_\mathrm{out}\times d _\mathrm{in}}$ thought of as a weight update, it holds that:
\begin{equation}\label{eq:square-majorization}
    \el(\mW + \Delta \mW) \leq \el(\mW) + \langle\nabla_\mW \el(\mW), \Delta \mW\rangle + \half \cdot \tfrac{d_\inn}{d_\out}\cdot\norm{\Delta \mW}_\mathrm{\ell_2\to\ell_2}^2,
\end{equation}
where $\langle\cdot,\cdot\rangle$ is the Frobenius inner product.
\end{myproposition}
In words: the square loss of a linear predictor admits an upper bound that is quadratic in the spectral norm of the weight perturbation. Choosing the weight perturbation to minimize this upper bound is precisely steepest descent under the spectral norm! The proof is given in \cref{proof:majorization}. This optimizer design pattern, which starts by deriving an upper bound on the loss (as in \cref{prop:majorization}) and then minimizes it (as in \cref{prop:shampoo-steepest}), is known generally as majorization-minimization \citep{mm}. It is an exact and first-principles design pattern, without Hessian approximations or appeals to convex theory. This design pattern is used extensively by \citet{spectral-descent-2, spectral-descent-1} to design optimizers for restricted Boltzmann machines and discrete graphical models. Generalizing the pattern to arbitrary network architectures and loss functions requires more advanced machinery \citep{agd-2023,streeter2023universal,modula}.


\ornament

And so, dear reader, we have reached the end of our second story. We have shown that Shampoo without accumulation corresponds to projecting the gradient matrix to the closest semi-orthogonal matrix, which solves the problem of steepest descent under the spectral norm. And we showed how steepest descent under the spectral norm emerges from upper bounding the square loss of a linear predictor. This perspective, of viewing Shampoo as a (smoothed out) projection to the space of semi-orthogonal matrices, grounds the algorithm in a prior literature on spectral descent \citep{spectral-descent-2,spectral-descent-1,spectral-descent-3}. And in \cref{app:shampoo}, we discuss how it might unlock new means for computing the Shampoo updates.

We summarize our first two stories in \cref{tab:weight_update_rules}. And we still have one more left to tell...
