\section{Prodigy: Automatically Computing the Escape Velocity}
\label{sec:prodigy}

\lettrine{F}{or} our final story, we speak of Prodigy \citep{Mishchenko2023Prodigy}. The Prodigy optimizer falls amid a series of recent works \citep{pmlr-v202-defazio23a,khaled2023dowg,Ivgi2023DoGIS} that attempt to apply convex theory to design and analyse deep learning optimizers that do not require tuning. In contrast, we argue that Prodigy (without EMA) is but another example of steepest descent, where instead of using the step size $\eta=\norm{\vg}^\dagger/\lambda$ from \cref{prop:steepest}, Prodigy uses a heuristic to automatically warm up to a good step size. This demonstrates the value of \cref{prop:steepest} for disentangling the optimizer design problem. If one knows a good norm $\norm{\cdot}$ but is ignorant of the sharpness parameter $\lambda$, then one may obtain the step direction by solving $\argmax_{\norm{\vt}=1} \vg^\top \vt$ from \cref{prop:steepest}, while using another means to find a good step size.

Then let us make our case. We focus on Algorithm 3 in the Prodigy paper, since this is the version used in their experiments. We first show that with EMA switched off, Prodigy implements sign gradient descent with a step size that warms up automatically. Ignoring the numerical stabilization and learning rate schedule, Prodigy is given by:
\begin{align}
    \vm_t &= \beta_1 \cdot \vm_{t-1} + (1 - \beta_1) \cdot \eta_t \,\vg_t, \\
    \vv_t &= \beta_2 \cdot \vv_{t-1} + (1 - \beta_2) \cdot \eta_t^2 \,\vg_t^2, \\
    r_t &= \sqrt{\beta_2} \cdot r_{t-1} + (1-\sqrt{\beta_2}) \cdot \eta_t^2 \, \vg_t^\top(\vw_0 - \vw_t), \label{eq:r-prodigy} \\
    \vs_t &= \sqrt{\beta_2} \cdot \vs_{t-1} + (1-\sqrt{\beta_2}) \cdot \eta_t^2 \, \vg_t, \label{eq:s-prodigy} \\
    \eta_{t+1} &= \max\left(\eta_t, \tfrac{r_t}{\norm{\vs_t}_1}\right), \label{eq:d-prodigy} \\
    \vw_{t+1} &= \vw_t - \eta_t \cdot \vm_t / \sqrt{\vv_t},
\end{align}
where $t$ denotes the time step and $\vg_t \in \R^n$ the gradient vector. While this system of updates may seem intimidating, if we switch off EMA by setting $\beta_1 = \beta_2 = 0$, the Prodigy updates simplify dramatically to just sign gradient descent with a dynamical step size as follows:
\begin{align}
    \eta_{t+1} &= \max\left(\eta_t, \tfrac{\vg_t^\top(\vw_0 - \vw_t)}{\norm{\vg_t}_1}\right), \label{eq:adaptive-prodigy}
    \\
    \vw_{t+1} &= \vw_t - \eta_t \cdot \sign (\vg_t).
    \label{eq:sign-prodigy}
\end{align}
But \cref{prop:sign-descent} showed that sign descent is steepest descent under the infinity norm. Therefore \cref{eq:adaptive-prodigy,eq:sign-prodigy} prove our claim that Prodigy without EMA is steepest descent, although with a dynamically chosen step size denoted $\eta_t$.

All that remains is to understand the dynamical rule, given by \cref{eq:adaptive-prodigy}, for choosing the step size $\eta_t$. We shall argue that this dynamical rule can be understood to approximate a heuristic algorithm for achieving, but not exceeding, what we shall call \textit{escape velocity}:
\begin{itemize}
    \item Choose a very small initial step size $\eta_0$---small enough to be \textit{a priori} sure that $\eta_0 \ll \eta_\star$, where $\eta_\star$ denotes \textit{escape velocity}: the unknown but optimal initial step size;
    \item At each step, check if the weights $\vw_t$ have escaped the linearization of the loss around the initial weights $\vw_0$---if not, double the step size according to $\eta_{t+1} = 2 \times \eta_t$;
    \item Once the weights $\vw_t$ have escaped the initial linearization, stop increasing the step size. We say that the step size $\eta_t$ has reached escape velocity $\eta_\star$.
\end{itemize}
The rationale behind this procedure is that if we knew the optimal initial step size $\eta_\star$, then the weights should escape the initial linearization of the loss in a single step. Formally, the directional derivative $(\vw_1 - \vw_0)^\top \vg_1$ must vanish if the step size is chosen optimally \citep{cauchy1847methode}. If the directional derivative in the direction of the first weight update is still negative $(\vw_1 - \vw_0)^\top \vg_1 < 0$, then we could have taken a larger step. Said another way, we can use the \textit{angle} that the gradient $\vg_1$ makes with the change in weights $\vw_1 - \vw_0$ to tell us whether or not we should increase the step size. Notice that procedure has no reliance on convexity. 

With this in mind, let us massage Prodigy's step size update (\cref{eq:adaptive-prodigy}) as follows:
\begin{align}\label{eq:rule}
    \eta_{t+1} = \max\left(\eta_t, \tfrac{\vg_t^\top(\vw_0 - \vw_t)}{\norm{\vg_t}_1}\right) = \max\left(\eta_t, \tfrac{\norm{\vg_t}_2}{\norm{\vg_t}_1}\times \norm{\vw_t - \vw_0}_2 \times \cos\theta\right),
\end{align}
where $\theta$ denotes the angle between the gradient $\vg_t$ and the difference in weights $\vw_0 - \vw_t$. To help make sense of this expression, we make two assumptions:
\begin{enumerate}
    \item The gradient is a ``dense'' vector in $\R^n$, meaning that $\norm{\vg_t}_2 / \norm{\vg_t}_1 \approx  1/ \sqrt{n}$; \label{ass:1}
    \item $\vw_t$ is still close enough to the initialization $\vw_0$ that $\cos\theta \approx 1$. \label{ass:2}
\end{enumerate}
Under these assumptions, \cref{eq:rule} becomes just $\eta_{t+1} \approx \max\left(\eta_t, \norm{\vw_t - \vw_0}_{\rms}\right)$, where the root mean square (RMS) norm is defined via $\norm{\cdot}_\rms \defeq \tfrac{1}{\sqrt{n}}\,\norm{\cdot}_2$. Combined with \cref{eq:sign-prodigy}, this allows us to estimate the size of the weight change at step $t+1$:
\begin{equation*}
    \norm{\vw_{t+2} - \vw_{t+1}}_\rms = \eta_{t+1} \cdot \norm{\sign(\vg_t)}_\rms \approx \max\left(\eta_t, \norm{\vw_t - \vw_0}_{\rms}\right) \geq \norm{\vw_t - \vw_0}_{\rms},
\end{equation*}
where we have used the fact that a sign vector has unit RMS norm. In words, while assumptions (\ref{ass:1}) and (\ref{ass:2}) hold, the step size at time $t+1$ is equivalent to the whole progress up to step $t$. This suggests exponential growth in the step size that continues until assumption (\ref{ass:2}) breaks, which we think of as the step size reaching the escape velocity $\eta_\star$

Now we wish to point out that this procedure is just one amongst a space of line search methods that one might consider \citep{armijo1966,rprop,kenneweg2024}. For instance, Prodigy's decision to only let $\eta_t$ increase and never decrease could be sub-optimal. And the decision to measure the angle between the gradient and the weight difference $\vw_t - \vw_0$ has alternatives. One could instead use the most recent weight difference $\vw_t - \vw_{t-1}$. Lastly, in place of relying on the norm ratio $\norm{\vg}_2/\norm{\vg_1}$ to implicitly convert the $\ell_2$ norm $\norm{\vw_t -\vw_0}_2$ into the RMS norm $\norm{\vw_t -\vw_0}_\rms$, one could consider a more explicit method. For instance, we found a rule akin to $\eta_{t+1} = \eta_t \times (1 + \cos\theta)$ to work well in some preliminary experiments.

\ornament

Our time grows short, dear reader, and our third story draws to an end. We have argued that Prodigy without EMA is sign descent---an example of steepest descent---with a particular mechanism for warming up the step size. Starting with a tiny initial step size, Prodigy multiplicatively increases the step size until the weights escape the initial locally linear region of the loss. Prodigy's step size adjustment is based on the angle between the gradient and the total weight change. This is a form of online line search. This highlights that once one has chosen a norm, the steepest descent framework allows freedom to estimate the step size in various different ways.













    
