\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2020)Anil, Gupta, Koren, Regan, and Singer]{Anil2020ScalableSecondOrder}
Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer.
\newblock Scalable second order optimization for deep learning.
\newblock \emph{arXiv:2002.09018}, 2020.

\bibitem[Armijo(1966)]{armijo1966}
Larry Armijo.
\newblock {Minimization of functions having Lipschitz continuous first partial derivatives}.
\newblock \emph{Pacific Journal of Mathematics}, 1966.

\bibitem[Balles and Hennig(2018)]{pmlr-v80-balles18a}
Lukas Balles and Philipp Hennig.
\newblock Dissecting {A}dam: The sign, magnitude and variance of stochastic gradients.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and Anandkumar]{signum}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
\newblock sign{SGD}: Compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Bernstein et~al.(2023)Bernstein, Mingard, Huang, Azizan, and Yue]{agd-2023}
Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue.
\newblock {A}utomatic {G}radient {D}escent: {D}eep {L}earning without {H}yperparameters.
\newblock \emph{arXiv:2304.05187}, 2023.

\bibitem[Bj\"{o}rck and Bowie(1971)]{bjoerck1971}
\r{A}ke Bj\"{o}rck and C.~Bowie.
\newblock An iterative algorithm for computing the best estimate of an orthogonal matrix.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1971.

\bibitem[Carlson et~al.(2015{\natexlab{a}})Carlson, Cevher, and Carin]{spectral-descent-2}
David Carlson, Volkan Cevher, and Lawrence Carin.
\newblock Stochastic spectral descent for {R}estricted {B}oltzmann {M}achines.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2015{\natexlab{a}}.

\bibitem[Carlson et~al.(2015{\natexlab{b}})Carlson, Collins, Hsieh, Carin, and Cevher]{spectral-descent}
David Carlson, Edo Collins, Ya-Ping Hsieh, Lawrence Carin, and Volkan Cevher.
\newblock Preconditioned spectral descent for deep learning.
\newblock In \emph{Neural Information Processing Systems}, 2015{\natexlab{b}}.

\bibitem[Carlson et~al.(2016)Carlson, Hsieh, Collins, Carin, and Cevher]{spectral-descent-1}
David Carlson, Ya-Ping Hsieh, Edo Collins, Lawrence Carin, and Volkan Cevher.
\newblock Stochastic spectral descent for discrete graphical models.
\newblock \emph{Selected Topics in Signal Processing}, 2016.

\bibitem[Cauchy(1847)]{cauchy1847methode}
Augustin-Louis Cauchy.
\newblock M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des syst{\`e}mes d'{\'e}quations simultan{\'e}es.
\newblock \emph{Comptes Rendus Hebdomadaires des S{\'e}ances de l'Acad{\'e}mie des Sciences}, 1847.

\bibitem[Chen et~al.(2023)Chen, Liang, Huang, Real, Wang, Pham, Dong, Luong, Hsieh, Lu, and Le]{chen2023symbolic}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc~V Le.
\newblock Symbolic discovery of optimization algorithms.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Dahl et~al.(2023)Dahl, Schneider, Nado, Agarwal, Sastry, Hennig, Medapati, Eschenhagen, Kasimbeg, Suo, Bae, Gilmer, Peirson, Khan, Anil, Rabbat, Krishnan, Snider, Amid, Chen, Maddison, Vasudev, Badura, Garg, and Mattson]{Dahl2023AlgoPerf}
George~E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli~Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel~L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris~J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, and Peter Mattson.
\newblock Benchmarking neural network training algorithms.
\newblock \emph{arXiv:2306.07179}, 2023.

\bibitem[Defazio and Mishchenko(2023)]{pmlr-v202-defazio23a}
Aaron Defazio and Konstantin Mishchenko.
\newblock Learning-rate-free learning by {D}-adaptation.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{Duchi2011AdaptiveSM}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{Journal Machine Learning Research}, 2011.

\bibitem[Fan(2017)]{spectral-descent-3}
Kai Fan.
\newblock Unifying the stochastic spectral descent for {R}estricted {B}oltzmann {M}achines with {B}ernoulli or {G}aussian inputs.
\newblock \emph{arXiv:1703.09766}, 2017.

\bibitem[Feinberg et~al.(2023)Feinberg, Chen, Sun, Anil, and Hazan]{sketchy}
Vladimir Feinberg, Xinyi Chen, Y.~Jennifer Sun, Rohan Anil, and Elad Hazan.
\newblock Sketchy: Memory-efficient adaptive regularization with frequent directions.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Gupta et~al.(2017)Gupta, Koren, and Singer]{unified-shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock A unified approach to adaptive regularization in online and stochastic optimization.
\newblock Technical report, Google Brain, 2017.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{Gupta2018ShampooPS}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Higham(2008)]{higham}
Nicholas~J. Higham.
\newblock \emph{Functions of Matrices}.
\newblock Society for Industrial and Applied Mathematics, 2008.

\bibitem[Ivgi et~al.(2023)Ivgi, Hinder, and Carmon]{Ivgi2023DoGIS}
Maor Ivgi, Oliver Hinder, and Yair Carmon.
\newblock {DoG} is {SGD}'s best friend: A parameter-free dynamic step size schedule.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Kenneweg et~al.(2024)Kenneweg, Kenneweg, and Hammer]{kenneweg2024}
Philip Kenneweg, Tristan Kenneweg, and Barbara Hammer.
\newblock Improving line search methods for large scale neural network training.
\newblock In \emph{International Conference on Artificial Intelligence, Computer, Data Sciences and Applications}, 2024.

\bibitem[Khaled et~al.(2023)Khaled, Mishchenko, and Jin]{khaled2023dowg}
Ahmed Khaled, Konstantin Mishchenko, and Chi Jin.
\newblock Do{WG} unleashed: An efficient universal parameter-free gradient descent method.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Kingma and Ba(2015)]{kingma_adam:_2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kovarik(1970)]{kovarik1970iterative}
Zdislav Kovarik.
\newblock Some iterative methods for improving orthonormality.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1970.

\bibitem[Lakić(1998)]{lakic}
Slobodan Lakić.
\newblock On the computation of the matrix k-th root.
\newblock \emph{Journal of Applied Mathematics and Mechanics}, 1998.

\bibitem[Lange(2016)]{mm}
Kenneth Lange.
\newblock \emph{{MM} Optimization Algorithms}.
\newblock Society for Industrial and Applied Mathematics, 2016.

\bibitem[Large et~al.(2024)Large, Liu, Huh, Bahng, Isola, and Bernstein]{modula}
Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, and Jeremy Bernstein.
\newblock Scalable optimization in the modular norm.
\newblock \emph{arXiv:2405.14813}, 2024.

\bibitem[Martinsson and Tropp(2020)]{Martinsson_Tropp_2020}
Per-Gunnar Martinsson and Joel~A. Tropp.
\newblock Randomized numerical linear algebra: Foundations and algorithms.
\newblock \emph{Acta Numerica}, 2020.

\bibitem[Mishchenko and Defazio(2023)]{Mishchenko2023Prodigy}
Konstantin Mishchenko and Aaron Defazio.
\newblock Prodigy: An expeditiously adaptive parameter-free learner.
\newblock \emph{arXiv:2306.06101}, 2023.

\bibitem[Morwani et~al.(2024)Morwani, Shapira, Vyas, Malach, Kakade, and Janson]{Morwani2024NewPerspective}
Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham Kakade, and Lucas Janson.
\newblock A new perspective on {S}hampoo's preconditioner.
\newblock \emph{arXiv:2406.17748}, 2024.

\bibitem[Riedmiller and Braun(1993)]{rprop}
Martin Riedmiller and Heinrich Braun.
\newblock A direct adaptive method for faster backpropagation learning: The {RPROP} algorithm.
\newblock In \emph{International Conference on Neural Networks}, 1993.

\bibitem[Shi et~al.(2023)Shi, Lee, Iwasaki, Gallego-Posada, Li, Rangadurai, Mudigere, and Rabbat]{Shi2023DistributedShampoo}
Hao-Jun~Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li, Kaushik Rangadurai, Dheevatsa Mudigere, and Michael Rabbat.
\newblock A distributed data-parallel {PyTorch} implementation of the distributed {S}hampoo optimizer for training neural networks at-scale.
\newblock \emph{arXiv:2309.06497}, 2023.

\bibitem[Streeter(2023)]{streeter2023universal}
Matthew Streeter.
\newblock Universal majorization-minimization algorithms.
\newblock \emph{arXiv:2308.00190}, 2023.

\bibitem[Sun and Spall(2021)]{sun-and-spall}
Shiqing Sun and James~C. Spall.
\newblock Connection of diagonal {H}essian estimates to natural gradients in stochastic optimization.
\newblock In \emph{Information Sciences and Systems}, 2021.

\bibitem[Tieleman and Hinton(2012)]{tieleman_rmsprop_2012}
Tijmen Tieleman and Geoffrey Hinton.
\newblock {RMSprop}.
\newblock \emph{Coursera: Neural Networks for Machine Learning}, Lecture 6.5, 2012.

\bibitem[Xie and Li(2024)]{xie2024implicit}
Shuo Xie and Zhiyuan Li.
\newblock Implicit bias of {AdamW}: $\ell_\infty$-norm constrained optimization.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Yang et~al.(2023)Yang, Simon, and Bernstein]{my-spectral}
Greg Yang, James~B. Simon, and Jeremy Bernstein.
\newblock A spectral condition for feature learning.
\newblock \emph{arXiv:2310.17813}, 2023.

\bibitem[Zhao et~al.(2024)Zhao, Morwani, Brandfonbrener, Vyas, and Kakade]{Zhao2024Deconstructing}
Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham Kakade.
\newblock Deconstructing what makes a good optimizer for language models.
\newblock \emph{arXiv:2407.07972}, 2024.

\end{thebibliography}
