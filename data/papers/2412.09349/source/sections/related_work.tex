\section{Related Work}
\textbf{Latent Image/Video Diffusion Models.} 
Diffusion-based models~\citep{ho2020denoising,song2020score,rombach2022high,zhang2023adding} have achieved remarkable success in the fields of image generation and video generation. 
Due to reasons such as computational intensity and information redundancy, diffusion models directly on the pixel space are hard to scale up. The latent diffusion model (LDM)~\citep{rombach2022high} introduces a technique for denoising in the latent space, which reduces the computational requirements while preserving the generation quality. In contrast to image generation, video generation requires more accurate modeling for temporal motion patterns. Recent video generation models~\citep{blattmann2023align,ge2023preserve,guo2023animatediff} utilize pre-trained image diffusion models to enhance the temporal modeling capability by inserting temporal mixing layers of various forms.


\textbf{Diffusion Models for Human Image Animation.}
Recent advancements in latent diffusion models have significantly contributed to the development of human image animation. Previous human image animation models~\citep{wang2024disco,xu2024magicanimate,chang2023magicpose} followed the same two-stage training paradigm. In the first stage, the pose-driven image model is trained on individual video frames and corresponding pose images. In the second stage, the temporal layer is inserted to capture temporal information while keeping the image generation model frozen. Based on this stage training paradigm, Animate Anyone~\citep{hu2023animate} utilizes ReferenceNet with UNet architecture to extract appearance features from reference characters. With the development of video diffusion modeling, recent work~\citep{zhang2024mimicmotion} has directly fine-tuned Stable Video Diffusion (SVD)~\citep{blattmann2023svd} to replace two-stage training. To prove the effectiveness of the proposed method, we integrate DisPose on both paradigms.

\textbf{Control Guidance in Human Image Animation.}
Human image animation typically uses the skeleton pose (e.g., OpenPose~\citep{cao2017openpose}) as the control guide. DWpose~\citep{yang2023dwpose} stands out as an augmented alternative to OpenPose~\citep{cao2017openpose} since it provides more accurate and expressive skeletons. Recent work has focused on introducing dense conditions to enhance the quality of the generated video. MagicAnimate~\citep{xu2024magicanimate} uses DensePose~\citep{guler2018densepose} instead of skeleton pose, which establishes a dense correspondence between RGB images and surface-based representations. Champ~\citep{zhu2024champ} renders depth maps, normal maps, and semantic maps from SMPL~\citep{SMPL:2015} to provide detailed pose information. However, these overly dense guidance techniques rely too much on the driving video and generate inconsistent results when the target identity is significantly different from the reference. In this paper, we propose a reference-based dense motion field that provides dense motion signals while avoiding strict geometric constraints.