\section{Preliminary}
We choose Stable Diffusion (SD) as the base diffusion model in this paper since it is the most popular open-source model and has a well-developed community. SD performs the diffusion process in the latent space of a pre-trained autoencoder.
The input image $I$ is transformed into a latent representation $\boldsymbol{z}_{{0}} = \mathcal{E}(I)$ using a frozen encoder $\mathcal{E}(\cdot)$.
The diffusion process involves applying a variance preserving Markov process to $\boldsymbol{z}_{{0}}$, where noise levels increase monotonically to generate diverse noisy latent representations:
% In training, the encoded image  is perturbed to $\boldsymbol{z}_{{0}}$ by forward diffusion:
\begin{equation}
    \boldsymbol{z}_{{t}}=\sqrt{\bar{\alpha_{t}}} z_{0}+\sqrt{1-\bar{\alpha_{t}}} \epsilon, \epsilon \sim \mathcal{N}(0, I),
\end{equation}
where $t\!=\!1,\cdots, T$ denotes the time steps within the Markov process, where $T$ is commonly configured to 1000, and $\bar{\alpha_{t}}$ represents the pre-defined noise intensity at each time step. 
The denoising network $\epsilon_{\theta}(\cdot)$ learns to reverse this process by predicting the added noise, encouraged by the mean squared error (MSE) loss:
\begin{equation}
    \mathcal{L}=\mathbb{E}_{\mathcal{E}\left(I\right), y, \epsilon \sim \mathcal{N}(0, I), t}\left[\left\|\epsilon-\epsilon_{\theta}\left(z_{t}, t, c_{\mathrm{text}}\right)\right\|_{2}^{2}\right],
\end{equation}
where $c_{\mathrm{text}}$ is the text embedding corresponding to $I$. The denoising network $\epsilon_{\theta}(\cdot)$ is typically implemented as a U-Net~\citep{ronneberger2015unet} consisting of pairs of down/up sample blocks at four resolution levels, as well as a middle block. Each network block consists of ResNet~\citep{he2016deep}, spatial self-attention layers, and cross-attention layers that introduce text conditions.