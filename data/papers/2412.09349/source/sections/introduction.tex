
\section{Introduction}
Controllable video generation~\citep{zhang2023controlvideo,yin2023dragnuwa, wang2024motionctrl,niu2025mofa,gu2024videoswap} has gained increasing attention for its ability to customize videos based on user preferences. In particular, controllable human image animation~\citep{hu2023animate,wang2024disco} has attracted significant interest due to its vast potential applications in art creation, social media, and digital humans. It aims to animate static images into realistic videos based on driving videos.
In contrast to other controllable video generation methods (e.g., camera control~\citep{he2024cameractrl, wang2024motionctrl, li2024image}, trajectory control~\citep{yin2023dragnuwa,wu2024draganything, wang2024motionctrl, li2024image}), controllable human image animation allows for more flexible motion regions, diverse motion paradigms, and complex character appearances.
Introducing precise pose control in existing video generation methods is challenging, but has significant value in achieving the desired results.
There are two challenges: (1) following the motion of the driving video, and (2) preserving the appearance information of the reference image.

% In general, the control signal is crucial for controllable generation. 
The motion control signal is critical to drive the animation. Controllable human image animation usually utilizes the skeleton pose~\citep{yang2023dwpose} as the control signal. Besides the fact that the skeleton pose is easy to obtain, the more important reason is that it is easier to adapt to different body shapes when the target body shape is significantly different from the reference image.
However, the skeleton pose as a sparse expression provides limited guidance information.
% (e.g., it cannot distinguish between foreground and background)
% and cannot adapt to complex motion generation.
To provide more control signals, recent work~\citep{zhu2024champ,xu2024magicanimate} has attempted to express human body geometry and motion regions by extracting various dense signals from the driving video, including DensePose~\citep{guler2018densepose}, SMPL~\citep{SMPL:2015} and depth map~\citep{yang2024depth}, etc. Unfortunately, these dense signals impose strict shape constraints on the generated characters, they are more difficult to adapt to reference images with different body shapes. Moreover, extracting dense signals accurately in complex motion videos is inherently difficult~\citep{zhang2024mimicmotion}. These overly dense guidance techniques exacerbate pose estimation errors and thus impair generation quality.
% In summary, existing character image animation methods~\citep{chang2023magicpose, hu2023animate, zhu2024champ} struggle with shape generalization and consistent generation.
Therefore, the existing methods~\citep{chang2023magicpose, hu2023animate, zhu2024champ, wang2024disco} are struggling to trade off generalizability and effectiveness between sparse and dense controls.
It would be beneficial to mine more generalizable and effective control signals from the skeleton pose map instead of dense control inputs.

On the other side, preserving appearance consistency from complex motion is also extremely challenging. Image-driven generation methods~\citep{zhang2023adding,ye2023ip,wang2024instantid} typically employ the CLIP~\citep{radford2021learning} image encoder as a substitute for the text encoder to introduce low-level details of the image.
{Inspired by dense reference image conditioning, recent works~\citep{hu2023animate, xu2024magicanimate} opt to train an additional reference network that uses the same initialization as the denoising network.} The feature maps from the reference network are injected into the denoising network through the attention mechanism. This dual U-Net architecture significantly increases the training cost.
Moreover, such dense reference image conditioning is ineffective for actions with body shape changes.
Existing works neglect the fact that the keypoints of the sparse skeleton pose correspond to appearance characteristics.
We argue that considering sparse skeleton pose keypoints as correspondences can provide effective appearance guidance while relaxing shape constraints.

To this end, we propose DisPose, a plug-and-play guidance module to disentangle pose guidance, which extracts robust control signals from only the skeleton pose map and reference image without additional dense inputs.
Specifically, we disentangle pose guidance into motion field estimation and keypoint correspondence. First, we compute the sparse motion field using the skeleton pose.
% Then, the sparse motion field is transformed into the dense motion field to provide region-level motion signals through motion propagation on the reference image.
We then introduce a reference-based dense motion field to provide region-level motion signals through condition motion propagation on the reference image.
To enhance appearance consistency, we extract diffusion features corresponding to key points in the reference image. These point features are transferred to the target pose by computing multi-scale point correspondences from the motion trajectory.
Architecturally, we implement these disentangled control signals in a ControlNet-like~\citep{zhang2023adding} manner to integrate them into existing methods.
Finally, motion fields and point embedding are injected into the latent video diffusion model resulting in accurate human image animation as shown in Figure.~\ref{fig:intro}. 
The contribution of this paper can be summarized as:
\begin{itemize}[topsep=0pt, partopsep=0pt, leftmargin=13pt, parsep=0pt, itemsep=3pt]
     \item  We propose a plug-and-play module for controllable human animation.
     \item  We innovatively disentangle motion field guidance and keypoint correspondence from pose control to provide efficient control signals without additional dense inputs.
     \item  Extensive qualitative and quantitative experiments demonstrate the superiority and generality of the proposed model.
\end{itemize}
