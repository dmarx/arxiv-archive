\begin{thebibliography}{114}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensor{F}low: Large-scale machine learning on heterogeneous
  distributed systems.
\newblock \emph{arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[Agarwal and Nenkova(2022)]{agarwal2022temporal}
Oshin Agarwal and Ani Nenkova.
\newblock Temporal effects on pre-trained models for language processing tasks.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 904--921, 2022.

\bibitem[Albalak et~al.(2023)Albalak, Raffel, and Wang]{albalak2023improving}
Alon Albalak, Colin Raffel, and William~Yang Wang.
\newblock Improving few-shot generalization by exploring and exploiting
  auxiliary data.
\newblock \emph{arXiv preprint arXiv:2302.00674}, 2023.

\bibitem[Altmann et~al.(2009)Altmann, Pierrehumbert, and
  Motter]{altmann2009beyond}
Eduardo~G Altmann, Janet~B Pierrehumbert, and Adilson~E Motter.
\newblock Beyond word frequency: Bursts, lulls, and scaling in the temporal
  distributions of words.
\newblock \emph{PLOS one}, 4\penalty0 (11):\penalty0 e7678, 2009.

\bibitem[{Anthropic AI}(2023)]{anthropic2023claude}
{Anthropic AI}.
\newblock Introducing {C}laude, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen,
  Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional {AI}: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Bandy and Vincent(2021)]{bandy2021addressing}
Jack Bandy and Nicholas Vincent.
\newblock Addressing ``documentation debt'' in machine learning research: A
  retrospective datasheet for bookcorpus.
\newblock \emph{arXiv preprint arXiv:2105.05241}, 2021.

\bibitem[Baumgartner et~al.(2020)Baumgartner, Zannettou, Keegan, Squire, and
  Blackburn]{baumgartner2020pushshift}
Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy
  Blackburn.
\newblock The {P}ushshift {R}eddit dataset.
\newblock In \emph{Proceedings of the international AAAI conference on web and
  social media}, volume~14, pages 830--839, 2020.

\bibitem[Bender(2013)]{bender2013linguistic}
Emily~M Bender.
\newblock Linguistic fundamentals for natural language processing: 100
  essentials from morphology and syntax.
\newblock \emph{Synthesis lectures on human language technologies}, 6\penalty0
  (3):\penalty0 1--184, 2013.

\bibitem[Bender and Friedman(2018)]{bender2018data}
Emily~M. Bender and Batya Friedman.
\newblock Data statements for natural language processing: Toward mitigating
  system bias and enabling better science.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  6:\penalty0 587--604, 2018.
\newblock \doi{10.1162/tacl_a_00041}.
\newblock URL \url{https://aclanthology.org/Q18-1041}.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien,
  Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
  O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai
  Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock \emph{arXiv preprint arXiv:2304.01373}, 2023.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, et~al.]{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock {GPT}-{N}eo{X}-20{B}: An open-source autoregressive language model.
\newblock \emph{Challenges \& Perspectives in Creating Large Language Models},
  page~95, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{chelba2013one}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv preprint arXiv:1312.3005}, 2013.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Chung,
  Sutton, Gehrmann, Schuh, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, et~al.
\newblock Pa{LM}: Scaling language modeling with {P}athways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Chung et~al.(2023)Chung, Garcia, Roberts, Tay, Firat, Narang, and
  Constant]{chung2023unimax}
Hyung~Won Chung, Xavier Garcia, Adam Roberts, Yi~Tay, Orhan Firat, Sharan
  Narang, and Noah Constant.
\newblock Unimax: Fairer and more effective language sampling for large-scale
  multilingual pretraining.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[{Cohere AI}(2023)]{cohere2023}
{Cohere AI}.
\newblock Cohere command nightly, 2023.
\newblock URL \url{https://docs.cohere.com/docs/command-beta}.

\bibitem[Cortes and Lawrence(2021)]{cortes2021inconsistency}
Corinna Cortes and Neil~D Lawrence.
\newblock Inconsistency in conference peer review: Revisiting the 2014
  {N}eur{IPS} experiment.
\newblock \emph{arXiv preprint arXiv:2109.09774}, 2021.

\bibitem[Dathathri et~al.(2020)Dathathri, Madotto, Lan, Hung, Frank, Molino,
  Yosinski, and Liu]{dathathriplug}
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
  Molino, Jason Yosinski, and Rosanne Liu.
\newblock Plug and play language models: A simple approach to controlled text
  generation.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{NAACL}, 2019.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dhingra et~al.(2022)Dhingra, Cole, Eisenschlos, Gillick, Eisenstein,
  and Cohen]{dhingra2022time}
Bhuwan Dhingra, Jeremy~R Cole, Julian~Martin Eisenschlos, Daniel Gillick, Jacob
  Eisenstein, and William~W Cohen.
\newblock Time-aware language models as temporal knowledge bases.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 257--273, 2022.

\bibitem[Dodge et~al.(2021)Dodge, Sap, Marasovi{\'c}, Agnew, Ilharco,
  Groeneveld, Mitchell, and Gardner]{dodge2021documenting}
Jesse Dodge, Maarten Sap, Ana Marasovi{\'c}, William Agnew, Gabriel Ilharco,
  Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
\newblock Documenting large webtext corpora: A case study on the {C}olossal
  {C}lean {C}rawled {C}orpus.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 1286--1305, 2021.

\bibitem[{Du} et~al.(2022){Du}, {Huang}, {Dai}, {Tong}, {Lepikhin}, {Xu},
  {Krikun}, {Zhou}, {Yu}, {Firat}, {Zoph}, {Fedus}, {Bosma}, {Zhou}, {Wang},
  {Wang}, {Webster}, {Pellat}, {Robinson}, {Meier-Hellstern}, {Duke}, {Dixon},
  {Zhang}, {Le}, {Wu}, {Chen}, and {Cui}]{du_glam_2021}
Nan {Du}, Yanping {Huang}, Andrew~M. {Dai}, Simon {Tong}, Dmitry {Lepikhin},
  Yuanzhong {Xu}, Maxim {Krikun}, Yanqi {Zhou}, Adams~Wei {Yu}, Orhan {Firat},
  Barret {Zoph}, Liam {Fedus}, Maarten {Bosma}, Zongwei {Zhou}, Tao {Wang},
  Yu~Emma {Wang}, Kellie {Webster}, Marie {Pellat}, Kevin {Robinson}, Kathleen
  {Meier-Hellstern}, Toju {Duke}, Lucas {Dixon}, Kun {Zhang}, Quoc~V {Le},
  Yonghui {Wu}, Zhifeng {Chen}, and Claire {Cui}.
\newblock {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts}.
\newblock \emph{ICML}, 2022.
\newblock URL \url{https://arxiv.org/abs/2112.06905}.

\bibitem[Eisenstein et~al.(2014)Eisenstein, O'Connor, Smith, and
  Xing]{eisenstein2014diffusion}
Jacob Eisenstein, Brendan O'Connor, Noah~A Smith, and Eric~P Xing.
\newblock Diffusion of lexical change in social media.
\newblock \emph{PloS one}, 9\penalty0 (11):\penalty0 e113114, 2014.

\bibitem[Endr{\'e}dy and Nov{\'a}k(2013)]{endredy2013more}
Istv{\'a}n Endr{\'e}dy and Attila Nov{\'a}k.
\newblock More effective boilerplate removal-the goldminer algorithm.
\newblock \emph{Polibits}, 48:\penalty0 79--83, 2013.

\bibitem[Fisch et~al.(2019)Fisch, Talmor, Jia, Seo, Choi, and
  Chen]{fisch2019mrqa}
Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen.
\newblock Mrqa 2019 shared task: Evaluating generalization in reading
  comprehension.
\newblock In \emph{Proceedings of the 2nd Workshop on Machine Reading for
  Question Answering}, pages 1--13, 2019.

\bibitem[Friedl(2023)]{friedl2023dis}
Paul Friedl.
\newblock Dis/similarities in the design and development of legal and
  algorithmic normative systems: the case of perspective api.
\newblock \emph{Law, Innovation and Technology}, pages 1--35, 2023.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gao et~al.(2021)Gao, Fisch, and Chen]{gao-etal-2021-making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{ACL}, 2021.
\newblock \doi{10.18653/v1/2021.acl-long.295}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.295}.

\bibitem[Gardner et~al.(2020)Gardner, Artzi, Basmov, Berant, Bogin, Chen,
  Dasigi, Dua, Elazar, Gottumukkala, et~al.]{gardner2020evaluating}
Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao
  Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et~al.
\newblock Evaluating models’ local decision boundaries via contrast sets.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 1307--1323, 2020.

\bibitem[Gargee et~al.(2022)Gargee, Gopinath, Kancharla, Anand, and
  Babu]{gargee2022analyzing}
SK~Gargee, Pranav~Bhargav Gopinath, Shridhar Reddy~SR Kancharla, CR~Anand, and
  Anoop~S Babu.
\newblock Analyzing and addressing the difference in toxicity prediction
  between different comments with same semantic meaning in {G}oogle’s
  {P}erspective {API}.
\newblock In \emph{ICT Systems and Sustainability: Proceedings of ICT4SD 2022},
  pages 455--464. Springer, 2022.

\bibitem[Gebru et~al.(2021)Gebru, Morgenstern, Vecchione, Vaughan, Wallach,
  au2, and Crawford]{gebru2021datasheets}
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer~Wortman Vaughan,
  Hanna Wallach, Hal Daumé~III au2, and Kate Crawford.
\newblock Datasheets for datasets, 2021.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Real{T}oxicity{P}rompts: Evaluating neural toxic degeneration in
  language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 3356--3369, 2020.

\bibitem[Gokaslan* et~al.(2019)Gokaslan*, Cohen*, Pavlick, and
  Tellex]{Gokaslan2019OpenWeb}
Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, and Stefanie Tellex.
\newblock Open{W}eb{T}ext corpus, 2019.
\newblock URL \url{http://Skylion007.github.io/OpenWebTextCorpus}.

\bibitem[Google(2023)]{google2023}
Google.
\newblock Pa{LM} 2 technical report, 2023.
\newblock URL \url{https://ai.google/static/documents/palm2techreport.pdf}.

\bibitem[{Google Cloud NLP}(2023{\natexlab{a}})]{gcloud_info}
{Google Cloud NLP}.
\newblock Google {C}loud infotype detector, 2023{\natexlab{a}}.
\newblock URL \url{https://cloud.google.com/dlp/docs/infotypes-reference}.

\bibitem[{Google Cloud NLP}(2023{\natexlab{b}})]{gcloud_sentiment}
{Google Cloud NLP}.
\newblock Google {C}loud analyzing sentiment, 2023{\natexlab{b}}.
\newblock URL
  \url{https://cloud.google.com/natural-language/docs/analyzing-sentiment}.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo,
  Beltagy, Downey, and Smith]{gururangan2020don}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A Smith.
\newblock Don’t stop pretraining: Adapt language models to domains and tasks.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 8342--8360, 2020.

\bibitem[Gururangan et~al.(2022)Gururangan, Card, Drier, Gade, Wang, Wang,
  Zettlemoyer, and Smith]{gururangan2022whose}
Suchin Gururangan, Dallas Card, Sarah~K Drier, Emily~K Gade, Leroy~Z Wang, Zeyu
  Wang, Luke Zettlemoyer, and Noah~A Smith.
\newblock Whose language counts as high quality? {M}easuring language
  ideologies in text data selection.
\newblock \emph{arXiv preprint arXiv:2201.10474}, 2022.

\bibitem[Hartvigsen et~al.(2022)Hartvigsen, Gabriel, Palangi, Sap, Ray, and
  Kamar]{hartvigsen2022toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
  and Ece Kamar.
\newblock Toxigen: A large-scale machine-generated dataset for adversarial and
  implicit hate speech detection.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 3309--3326,
  2022.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Iter and Grangier(2021)]{iter2021complementarity}
Dan Iter and David Grangier.
\newblock On the complementarity of data selection and fine tuning for domain
  adaptation.
\newblock \emph{arXiv preprint arXiv:2109.07591}, 2021.

\bibitem[Jaidka et~al.(2018)Jaidka, Chhaya, and
  Ungar]{jaidka-etal-2018-diachronic}
Kokil Jaidka, Niyati Chhaya, and Lyle Ungar.
\newblock Diachronic degradation of language models: Insights from social
  media.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 195--200,
  Melbourne, Australia, July 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P18-2032}.
\newblock URL \url{https://aclanthology.org/P18-2032}.

\bibitem[Jang et~al.(2022)Jang, Ye, Lee, Yang, Shin, Han, Kim, and
  Seo]{jang2022temporalwiki}
Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han,
  Gyeonghun Kim, and Minjoon Seo.
\newblock Temporal{W}iki: A lifelong benchmark for training and evaluating
  ever-evolving language models.
\newblock \emph{arXiv preprint arXiv:2204.14211}, 2022.

\bibitem[Kaddour(2023)]{kaddour2023minipile}
Jean Kaddour.
\newblock The minipile challenge for data-efficient language models.
\newblock \emph{arXiv preprint arXiv:2304.08442}, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.
\newblock URL \url{https://arxiv.org/abs/2001.08361}.

\bibitem[Khashabi et~al.(2020)Khashabi, Min, Khot, Sabharwal, Tafjord, Clark,
  and Hajishirzi]{khashabi-etal-2020-unifiedqa}
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord,
  Peter Clark, and Hannaneh Hajishirzi.
\newblock {UnifiedQA}: Crossing format boundaries with a single {QA} system.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, 2020.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.171}.

\bibitem[Kincaid et~al.(1975)Kincaid, Fishburne~Jr, Rogers, and
  Chissom]{kincaid1975derivation}
J~Peter Kincaid, Robert~P Fishburne~Jr, Richard~L Rogers, and Brad~S Chissom.
\newblock Derivation of new readability formulas (automated readability index,
  fog count and flesch reading ease formula) for navy enlisted personnel.
\newblock Technical report, Naval Technical Training Command Millington TN
  Research Branch, 1975.

\bibitem[Kreutzer et~al.(2022)Kreutzer, Caswell, Wang, Wahab, van Esch,
  Ulzii-Orshikh, Tapo, Subramani, Sokolov, Sikasote,
  et~al.]{kreutzer2022quality}
Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch,
  Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov,
  Claytone Sikasote, et~al.
\newblock Quality at a glance: An audit of web-crawled multilingual datasets.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 50--72, 2022.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee,
  et~al.]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, et~al.
\newblock Natural {Q}uestions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 453--466, 2019.

\bibitem[Labov(2011)]{labov2011principles}
William Labov.
\newblock \emph{Principles of linguistic change, volume 3: Cognitive and
  cultural factors}, volume~3.
\newblock John Wiley \& Sons, 2011.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2020albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock {ALBERT}: A lite {BERT} for self-supervised learning of language
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lauren{\c{c}}on et~al.(2022)Lauren{\c{c}}on, Saulnier, Wang, Akiki,
  del Moral, Le~Scao, Von~Werra, Mou, Ponferrada, Nguyen,
  et~al.]{laurenccon2022bigscience}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki,
  Albert~Villanova del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou,
  Eduardo~Gonz{\'a}lez Ponferrada, Huu Nguyen, et~al.
\newblock The {B}ig{S}cience {ROOTS} {C}orpus: A 1.6{TB} composite multilingual
  dataset.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem[Lazaridou et~al.(2021)Lazaridou, Kuncoro, Gribovskaya, Agrawal, Liska,
  Terzi, Gimenez, de~Masson~d'Autume, Kocisky, Ruder,
  et~al.]{lazaridou2021mind}
Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam
  Liska, Tayfun Terzi, Mai Gimenez, Cyprien de~Masson~d'Autume, Tomas Kocisky,
  Sebastian Ruder, et~al.
\newblock Mind the gap: Assessing temporal generalization in neural language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29348--29363, 2021.

\bibitem[Lee et~al.(2022)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch,
  and Carlini]{lee2022deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
  Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 8424--8445,
  2022.

\bibitem[Lees et~al.(2022)Lees, Tran, Tay, Sorensen, Gupta, Metzler, and
  Vasserman]{lees2022perspective}
Alyssa Lees, Vinh~Q Tran, Yi~Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler,
  and Lucy Vasserman.
\newblock A new generation of {P}erspective {API}: Efficient multilingual
  character-level transformers.
\newblock \emph{arXiv preprint arXiv:2202.11176}, 2022.
\newblock URL \url{http://perspectiveapi.com.com}.

\bibitem[Liska et~al.(2022)Liska, Kocisky, Gribovskaya, Terzi, Sezener,
  Agrawal, De~Masson~D'Autume, Scholtes, Zaheer, Young, Gilsenan-Mcmahon,
  Austin, Blunsom, and Lazaridou]{pmlr-v162-liska22a}
Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener,
  Devang Agrawal, Cyprien De~Masson~D'Autume, Tim Scholtes, Manzil Zaheer,
  Susannah Young, Ellen Gilsenan-Mcmahon, Sophia Austin, Phil Blunsom, and
  Angeliki Lazaridou.
\newblock {S}treaming{QA}: A benchmark for adaptation to new knowledge over
  time in question answering models.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, pages 13604--13622, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/liska22a/liska22a.pdf}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Ro{BERT}a: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Longpre et~al.(2019)Longpre, Lu, Tu, and
  DuBois]{longpre2019exploration}
Shayne Longpre, Yi~Lu, Zhucheng Tu, and Chris DuBois.
\newblock An exploration of data augmentation and sampling techniques for
  domain-agnostic question answering.
\newblock In \emph{Proceedings of the 2nd Workshop on Machine Reading for
  Question Answering}, pages 220--227, 2019.

\bibitem[Longpre et~al.(2021)Longpre, Perisetla, Chen, Ramesh, DuBois, and
  Singh]{longpre2021entity}
Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois,
  and Sameer Singh.
\newblock Entity-based knowledge conflicts in question answering.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7052--7063, 2021.

\bibitem[Longpre et~al.(2022)Longpre, Reisler, Huang, Lu, Frank, Ramesh, and
  DuBois]{longpre2022active}
Shayne Longpre, Julia~Rachel Reisler, Edward~Greg Huang, Yi~Lu, Andrew Frank,
  Nikhil Ramesh, and Christopher DuBois.
\newblock Active learning over multiple domains in natural language tasks.
\newblock In \emph{NeurIPS 2022 Workshop on Distribution Shifts: Connecting
  Methods and Applications}, 2022.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei, et~al.]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The {F}lan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}, 2023.

\bibitem[Luccioni and Viviano(2021)]{luccioni2021s}
Alexandra~Sasha Luccioni and Joseph~D Viviano.
\newblock What's in the box? a preliminary analysis of undesirable content in
  the {C}ommon {C}rawl corpus.
\newblock \emph{arXiv preprint arXiv:2105.02732}, 2021.

\bibitem[Luu et~al.(2021)Luu, Khashabi, Gururangan, Mandyam, and
  Smith]{luu2021time}
Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah~A
  Smith.
\newblock Time waits for no one! analysis and challenges of temporal
  misalignment.
\newblock \emph{arXiv preprint arXiv:2111.07408}, 2021.

\bibitem[Madaan et~al.(2022)Madaan, Zhou, Alon, Yang, and
  Neubig]{madaan2022language}
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig.
\newblock Language models of code are few-shot commonsense learners, 2022.

\bibitem[Meade et~al.(2022)Meade, Poole-Dayan, and Reddy]{meade2022empirical}
Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy.
\newblock An empirical survey of the effectiveness of debiasing techniques for
  pre-trained language models.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1878--1898,
  2022.

\bibitem[Movva et~al.(2022)Movva, Lei, Longpre, Gupta, and
  DuBois]{movva2022combining}
Rajiv Movva, Jinhao Lei, Shayne Longpre, Ajay Gupta, and Chris DuBois.
\newblock Combining compressions for multiplicative size scaling on natural
  language tasks.
\newblock In \emph{Proceedings of the 29th International Conference on
  Computational Linguistics}, pages 2861--2872, 2022.

\bibitem[Nichol(2022)]{dalle2pretraining2022}
Alex Nichol.
\newblock {DALL}-{E} 2 pre-training mitigations, 2022.
\newblock URL
  \url{https://openai.com/research/dall-e-2-pre-training-mitigations}.

\bibitem[Nichol et~al.(2022)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, Mcgrew,
  Sutskever, and Chen]{nichol2022glide}
Alexander~Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela
  Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock In \emph{International Conference on Machine Learning}, pages
  16784--16804. PMLR, 2022.

\bibitem[Nostalgebraist(2022)]{chinchilla2022implications}
Nostalgebraist.
\newblock Chinchilla's wild implications.
\newblock \emph{AI Alignment Forum}, 2022.
\newblock URL
  \url{https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications}.

\bibitem[{NYT}(2020)]{nyt2020moderation}
The Open~Team {NYT}.
\newblock To apply machine learning responsibly, we use it in moderation, 2020.
\newblock URL
  \url{https://open.nytimes.com/to-apply-machine-learning-responsibly-we-use-it-in-moderation-d001f49e0644}.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock {GPT}-4 technical report.
\newblock \emph{arXiv preprint arxiv:2303.08774}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2303.08774.pdf}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.02155}.

\bibitem[Papadimitriou and Jurafsky(2020)]{papadimitriou2020learning}
Isabel Papadimitriou and Dan Jurafsky.
\newblock Learning music helps you read: Using transfer to study linguistic
  structure in language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 6829--6839, 2020.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{peters-etal-2018-deep}
Matthew~E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock \emph{NAACL}, 2018.
\newblock URL \url{https://aclanthology.org/N18-1202}.

\bibitem[Pozzobon et~al.(2023)Pozzobon, Ermis, Lewis, and
  Hooker]{pozzobon2023challenges}
Luiza~Amador Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker.
\newblock On the challenges of using black-box {API}s for toxicity evaluation
  in research.
\newblock In \emph{ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale
  Machine Learning Models}, 2023.

\bibitem[Pruksachatkun et~al.(2020)Pruksachatkun, Phang, Liu, Htut, Zhang,
  Pang, Vania, Kann, and Bowman]{pruksachatkun2020intermediate}
Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu~Mon Htut, Xiaoyi Zhang,
  Richard~Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel Bowman.
\newblock Intermediate-task transfer learning with pretrained language models:
  When and why does it work?
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 5231--5247, 2020.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.
\newblock URL
  \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  {G}opher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--67,
  2020.
\newblock URL \url{https://arxiv.org/abs/1910.10683}.

\bibitem[Roberts et~al.(2022)Roberts, Chung, Levskaya, Mishra, Bradbury, Andor,
  Narang, Lester, Gaffney, Mohiuddin, Hawthorne, Lewkowycz, Salcianu, van Zee,
  Austin, Goodman, Soares, Hu, Tsvyashchenko, Chowdhery, Bastings, Bulian,
  Garcia, Ni, Chen, Kenealy, Clark, Lee, Garrette, Lee-Thorp, Raffel, Shazeer,
  Ritter, Bosma, Passos, Maitin-Shepard, Fiedel, Omernick, Saeta, Sepassi,
  Spiridonov, Newlan, and Gesmundo]{roberts2022t5x}
Adam Roberts, Hyung~Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury,
  Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin,
  Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin,
  Sebastian Goodman, Livio~Baldini Soares, Haitang Hu, Sasha Tsvyashchenko,
  Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo
  Ni, Andrew Chen, Kathleen Kenealy, Jonathan~H. Clark, Stephan Lee, Dan
  Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten
  Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick,
  Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea
  Gesmundo.
\newblock Scaling up models and data with $\texttt{t5x}$ and $\texttt{seqio}$.
\newblock \emph{arXiv preprint arXiv:2203.17189}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.17189}.

\bibitem[Rogers(2021)]{rogers2021changing}
Anna Rogers.
\newblock Changing the world by changing the data.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 2182--2194, 2021.

\bibitem[R{\"o}ttger and
  Pierrehumbert(2021)]{rottger-pierrehumbert-2021-temporal-adaptation}
Paul R{\"o}ttger and Janet Pierrehumbert.
\newblock Temporal adaptation of {BERT} and performance on downstream document
  classification: Insights from social media.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 2400--2412, Punta Cana, Dominican Republic, November 2021.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.206}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.206}.

\bibitem[Sambasivan et~al.(2021)Sambasivan, Kapania, Highfill, Akrong,
  Paritosh, and Aroyo]{sambasivan2021everyone}
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen
  Paritosh, and Lora~M Aroyo.
\newblock ``{E}veryone wants to do the model work, not the data work'': Data
  cascades in high-stakes {AI}.
\newblock In \emph{CHI}, CHI '21, New York, NY, USA, 2021. Association for
  Computing Machinery.
\newblock ISBN 9781450380966.
\newblock \doi{10.1145/3411764.3445518}.
\newblock URL \url{https://doi.org/10.1145/3411764.3445518}.

\bibitem[Sap et~al.(2020)Sap, Gabriel, Qin, Jurafsky, Smith, and
  Choi]{sap-etal-2020-social}
Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah~A. Smith, and
  Yejin Choi.
\newblock Social bias frames: Reasoning about social and power implications of
  language.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 5477--5490, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.486}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.486}.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock {BLOOM}: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Schulman(2023)]{schulman2023}
John Schulman.
\newblock Reinforcement learning from human feedback: Progress and challenges.
\newblock \emph{Berkeley EECS}, 2023.
\newblock URL \url{https://www.youtube.com/watch?v=hhiLw5Q_UFg}.

\bibitem[Sellam et~al.(2022)Sellam, Yadlowsky, Tenney, Wei, Saphra, D'Amour,
  Linzen, Bastings, Turc, Eisenstein, et~al.]{sellammultiberts}
Thibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra,
  Alexander D'Amour, Tal Linzen, Jasmijn Bastings, Iulia~Raluca Turc, Jacob
  Eisenstein, et~al.
\newblock The {M}ulti{BERT}s: {BERT} reproductions for robustness analysis.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Singh and Ortega(2022)]{singh2022addressing}
Ayush Singh and John~E Ortega.
\newblock Addressing distribution shift at test time in pre-trained language
  models.
\newblock \emph{arXiv preprint arXiv:2212.02384}, 2022.

\bibitem[Singh(2019)]{newamerica2019moderation}
Spandana Singh.
\newblock Everything in moderation: An analysis of how internet platforms are
  using artificial intelligence to moderate user-generated content.
\newblock \emph{{New America}}, 2019.
\newblock URL
  \url{https://www.newamerica.org/oti/reports/everything-moderation-analysis-how-internet-platforms-are-using-artificial-intelligence-moderate-user-generated-content/}.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford {A}lpaca: An instruction-following {LL}a{MA} model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran,
  Yogatama, and Metzler]{tay2022scaling}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung~Won Chung, William Fedus, Jinfeng
  Rao, Sharan Narang, Vinh~Q Tran, Dani Yogatama, and Donald Metzler.
\newblock Scaling laws vs model architectures: How does inductive bias
  influence scaling?
\newblock \emph{arXiv preprint arXiv:2207.10551}, 2022.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock La{MDA}: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.08239}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock {LL}a{MA}: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vidgen et~al.(2021)Vidgen, Thrush, Waseem, and
  Kiela]{vidgen-etal-2021-learning}
Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela.
\newblock Learning from the worst: Dynamically generated datasets to improve
  online hate detection.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 1667--1682,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.132}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.132}.

\bibitem[Villalobos et~al.(2022)Villalobos, Sevilla, Heim, Besiroglu, Hobbhahn,
  and Ho]{villalobos2022will}
Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius
  Hobbhahn, and Anson Ho.
\newblock Will we run out of data? an analysis of the limits of scaling
  datasets in machine learning.
\newblock \emph{arXiv preprint arXiv:2211.04325}, 2022.

\bibitem[Wang et~al.(2022)Wang, Roberts, Hesslow, Scao, Chung, Beltagy, Launay,
  and Raffel]{wang2022language}
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven~Le Scao, Hyung~Won Chung,
  Iz~Beltagy, Julien Launay, and Colin Raffel.
\newblock What language model architecture and pretraining objective work best
  for zero-shot generalization?
\newblock \emph{ICML}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.05832}.

\bibitem[Wang et~al.(2020)Wang, Tsvetkov, and Neubig]{wang2020balancing}
Xinyi Wang, Yulia Tsvetkov, and Graham Neubig.
\newblock Balancing training for multilingual neural machine translation.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 8526--8537, 2020.

\bibitem[Warstadt et~al.(2020)Warstadt, Zhang, Li, Liu, and
  Bowman]{warstadt2020learning}
Alex Warstadt, Yian Zhang, Haau-Sing Li, Haokun Liu, and Samuel~R Bowman.
\newblock Learning which features matter: {R}o{BERT}a acquires a preference for
  linguistic generalizations (eventually).
\newblock \emph{arXiv preprint arXiv:2010.05358}, 2020.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama,
  Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{TMLR}, 2022.
\newblock URL \url{https://openreview.net/forum?id=yzkSU5zdwD}.

\bibitem[Welbl et~al.(2021)Welbl, Glaese, Uesato, Dathathri, Mellor, Hendricks,
  Anderson, Kohli, Coppin, and Huang]{welbl2021challenges}
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
  Lisa~Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
  Huang.
\newblock Challenges in detoxifying language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 2447--2469, 2021.

\bibitem[Wu et~al.(2022)Wu, Li, and Liang]{wuinsights}
Yuhuai Wu, Felix Li, and Percy Liang.
\newblock Insights into pre-training via simpler synthetic tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Xie et~al.(2023{\natexlab{a}})Xie, Pham, Dong, Du, Liu, Lu, Liang, Le,
  Ma, and Yu]{xie2023doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy
  Liang, Quoc~V Le, Tengyu Ma, and Adams~Wei Yu.
\newblock {D}o{R}e{M}i: Optimizing data mixtures speeds up language model
  pretraining.
\newblock \emph{arXiv preprint arXiv:2305.10429}, 2023{\natexlab{a}}.

\bibitem[Xie et~al.(2023{\natexlab{b}})Xie, Santurkar, Ma, and
  Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{arXiv preprint arXiv:2302.03169}, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2021)Xu, Pathak, Wallace, Gururangan, Sap, and
  Klein]{xu-etal-2021-detoxifying}
Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan
  Klein.
\newblock Detoxifying language models risks marginalizing minority voices.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2390--2397, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.190}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.190}.

\bibitem[Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel]{xue2021mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 483--498, 2021.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock {XLN}et: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Yao et~al.(2022)Yao, Choi, Cao, Lee, Koh, and Finn]{yaowild}
Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang~Wei Koh, and Chelsea
  Finn.
\newblock Wild-{T}ime: A benchmark of in-the-wild distribution shift over time.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem[Zhang and Choi(2021)]{zhang2021situatedqa}
Michael Zhang and Eunsol Choi.
\newblock Situated{QA}: Incorporating extra-linguistic contexts into qa.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7371--7387, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT}: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,
  Dong, et~al.]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015aligning}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 19--27, 2015.

\end{thebibliography}
