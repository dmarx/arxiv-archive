@article{wu2022promptchainer,
  title={Prompt{C}hainer: Chaining Large Language Model Prompts through Visual Programming},
  author={Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J},
  journal={CHI Extended Abstracts},
  year={2022},
  url={https://dl.acm.org/doi/abs/10.1145/3491101.3519729}
}

@inproceedings{sambasivan2021everyone,
author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
title = {``{E}veryone Wants to Do the Model Work, Not the Data Work'': Data Cascades in High-Stakes {AI}},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445518},
doi = {10.1145/3411764.3445518},
booktitle = {CHI},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{lee2022deduplicating,
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8424--8445},
  year={2022}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@inproceedings{lan2020albert,
  title={{ALBERT}: A Lite {BERT} for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations},
  year={2020},
}

@article{chelba2013one,
  title={One billion word benchmark for measuring progress in statistical language modeling},
  author={Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
  journal={arXiv preprint arXiv:1312.3005},
  year={2013}
}

@article{wiegreffe-etal-2021-measuring,
    title = "{M}easuring Association Between Labels and Free-Text Rationales",
    author = "Wiegreffe, Sarah  and
      Marasovi{\'c}, Ana  and
      Smith, Noah A.",
    journal = "EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.804",
}

@article{clark-etal-2020-tydi,
    title = "{T}y{D}i{QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
    author = "Clark, Jonathan H.  and
      Choi, Eunsol  and
      Collins, Michael  and
      Garrette, Dan  and
      Kwiatkowski, Tom  and
      Nikolaev, Vitaly  and
      Palomaki, Jennimaria",
    journal = "TACL",
    year = "2020",
    url = "https://aclanthology.org/2020.tacl-1.30",
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={ICLR},
  year={2020},
  url={https://openreview.net/forum?id=d7KBjmI3GmQ},
}

@inproceedings{chiang-chen-2019-semantically,
    title = "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
    author = "Chiang, Ting-Rui  and
      Chen, Yun-Nung",
    booktitle = "NAACL",
    year = "2019",
    url = "https://aclanthology.org/N19-1272",
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021},
  url={https://arxiv.org/abs/2103.03874}
}

@article{andreas-etal-2018-learning,
    title = "Learning with Latent Language",
    author = "Andreas, Jacob  and
      Klein, Dan  and
      Levine, Sergey",
    journal = "NAACL",
    year = "2018",
    url = "https://aclanthology.org/N18-1197",
}

@article{ahn2022can,
  title={Do as {I} can, not as {I} say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022},
  url={https://arxiv.org/abs/2204.01691}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={NeurIPS},
  year={2022},
  url={https://arxiv.org/abs/2201.11903},
}

@article{wu2022ai,
  title={A{I} chains: Transparent and controllable human-{AI} interaction by chaining large language model prompts},
  author={Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
  journal={CHI},
  year={2022},
  url={https://dl.acm.org/doi/abs/10.1145/3491102.3517582},
}

@article{yan2020neural,
  title={Neural execution engines: Learning to execute subroutines},
  author={Yan, Yujun and Swersky, Kevin and Koutra, Danai and Ranganathan, Parthasarathy and Hashemi, Milad},
  journal={NeurIPS},
  year={2020},
  url={https://arxiv.org/abs/2006.08084},
}

@article{cai2017making,
  title={Making neural programming architectures generalize via recursion},
  author={Cai, Jonathon and Shin, Richard and Song, Dawn},
  journal={ICLR},
  year={2017},
  url={https://arxiv.org/abs/1704.06611}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021},
  url={https://arxiv.org/abs/2108.07732},
}

@article{ellis2019write,
  title={Write, execute, assess: Program synthesis with a repl},
  author={Ellis, Kevin and Nye, Maxwell and Pu, Yewen and Sosa, Felix and Tenenbaum, Josh and Solar-Lezama, Armando},
  journal={NeurIPS},
  year={2019},
  url={https://arxiv.org/abs/1906.04604},
}

@article{chen2022can,
  title={Can Rationalization Improve Robustness?},
  author={Chen, Howard and He, Jacqueline and Narasimhan, Karthik and Chen, Danqi},
  journal={NAACL},
  year={2022},
  url={https://arxiv.org/abs/2204.11790}
}

@article{yao2021refining,
  title={Refining language models with compositional explanations},
  author={Yao, Huihan and Chen, Ying and Ye, Qinyuan and Jin, Xisen and Ren, Xiang},
  journal={NeurIPS},
  year={2021},
  url={https://proceedings.neurips.cc/paper/2021/hash/4b26dc4663ccf960c8538d595d0a1d3a-Abstract.html}
}

@article{dua-etal-2020-benefits,
    title = "Benefits of Intermediate Annotations in Reading Comprehension",
    author = "Dua, Dheeru  and
      Singh, Sameer  and
      Gardner, Matt",
    journal = "ACL",
    url = "https://aclanthology.org/2020.acl-main.497",
    year = "2020"
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={NeurIPS},
  year={2022},
  url={https://arxiv.org/abs/2205.11916},
}

@article{zaidan-etal-2007-using,
    title = "Using {``}Annotator Rationales{''} to Improve Machine Learning for Text Categorization",
    author = "Zaidan, Omar  and
      Eisner, Jason  and
      Piatko, Christine",
    journal = "NAACL",
    year = "2007",
    url = "https://aclanthology.org/N07-1033",
}

@article{patel-etal-2021-nlp,
    title = "Are {NLP} Models really able to Solve Simple Math Word Problems?",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Goyal, Navin",
    journal = "NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.168.pdf"
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022},
  url={https://arxiv.org/abs/2202.12837}
}

@article{jie2022learning,
  title={Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction},
  author={Jie, Zhanming and Li, Jierui and Lu, Wei},
  journal={arXiv preprint arXiv:2203.10316},
  year={2022},
  url={https://arxiv.org/abs/2203.10316}
}

@article{wang2022benchmarking,
  title={Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022},
  url={https://arxiv.org/abs/2204.07705}
}

@article{so2021searching,
  title={Searching for Efficient Transformers for Language Modeling},
  author={So, David and Ma{\'n}ke, Wojciech and Liu, Hanxiao and Dai, Zihang and Shazeer, Noam and Le, Quoc V.},
  journal={Advances in Neural Information Processing Systems},
  year={2021},
  url={https://arxiv.org/abs/2109.08668},
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={ICLR 2022},
  year={2021},
  url={https://arxiv.org/abs/2110.08207}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022},
  url={https://arxiv.org/abs/2203.02155}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K. and Dasgupta, Ishita and Chan, Stephanie C.Y. and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L. and Wang, Jane X. and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022},
  url={https://arxiv.org/abs/2204.02329},
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014},
  url={https://arxiv.org/abs/1410.4615}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020},
  url={https://arxiv.org/abs/1910.10683}
}

@article{trinh2018simple,
  title={A Simple Method for Commonsense Reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@article{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    journal = "NAACL",
    year = "2018",
    url = "https://aclanthology.org/N18-1202",
}

@article{pi2022reasoning,
  title={Reasoning Like Program Executors},
  author={Pi, Xinyu and Liu, Qian and Chen, Bei and Ziyadi, Morteza and Lin, Zeqi and Gao, Yan and Fu, Qiang and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2201.11473},
  year={2022},
  url={https://arxiv.org/abs/2201.11473}
}

@article{zhou2020towards,
  title={Towards interpretable natural language understanding with explanations as latent variables},
  author={Zhou, Wangchunshu and Hu, Jinyi and Zhang, Hanlin and Liang, Xiaodan and Sun, Maosong and Xiong, Chenyan and Tang, Jian},
  journal={NeurIPS},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/file/4be2c8f27b8a420492f2d44463933eb6-Paper.pdf}
}

@article{wang2022self,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022},
  url={https://arxiv.org/abs/2203.11171}
}

@article{zelikman2022star,
  title={S{T}a{R}: Bootstrapping Reasoning With Reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Goodman, Noah D.},
  journal={arXiv preprint arXiv:2203.14465},
  year={2022},
  url={https://arxiv.org/abs/2203.14465},
}

@article{chowdhery2022palm,
    title={Pa{LM}: Scaling Language Modeling with {P}athways},
    author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten  Bosma and Gaurav Mishra and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and others},
    journal={arXiv preprint arXiv:2204.02311},
    year={2022},
    url={https://arxiv.org/abs/2204.02311}
}

@article{ling-etal-2017-program,
    title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
    author = "Ling, Wang  and
      Yogatama, Dani  and
      Dyer, Chris  and
      Blunsom, Phil",
    journal = "ACL",
    year = "2017",
    url = "https://aclanthology.org/P17-1015",
}

@article{thoppilan2022lamda,
  title={La{MDA}: Language Models for Dialog Applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022},
  url={https://arxiv.org/abs/2201.08239},
}

@article{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={ICML},
  year={2021},
  url={https://arxiv.org/abs/2102.09690}
}

@article{lan2021mwptoolkit,
  title={{MWPT}oolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers},
  author={Lan, Yihuai and Wang, Lei and Zhang, Qiyuan and Lan, Yunshi and Dai, Bing Tian and Wang, Yan and Zhang, Dongxiang and Lim, Ee-Peng},
  journal={arXiv preprint arXiv:2109.00799},
  year={2021},
  url={https://arxiv.org/abs/2109.00799}
}

@article{ran-etal-2019-numnet,
    title = "{N}um{N}et: Machine Reading Comprehension with Numerical Reasoning",
    author = "Ran, Qiu  and
      Lin, Yankai  and
      Li, Peng  and
      Zhou, Jie  and
      Liu, Zhiyuan",
    journal = "EMNLP",
    year = "2019",
    url = "https://aclanthology.org/D19-1251",
    doi = "10.18653/v1/D19-1251",
}

@article{gu2021dream,
  title={{DREAM}: Uncovering Mental Models behind Language Models},
  author={Gu, Yuling and Mishra, Bhavana Dalvi and Clark, Peter},
  journal={NAACL},
  year={2022},
  url={https://arxiv.org/pdf/2112.08656.pdf}
}

@article{liang-etal-2021-explainable,
    title = "Explainable Multi-hop Verbal Reasoning Through Internal Monologue",
    author = "Liang, Zhengzhong  and
      Bethard, Steven  and
      Surdeanu, Mihai",
    journal = "NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.97",
    doi = "10.18653/v1/2021.naacl-main.97",
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020},
  url={https://arxiv.org/abs/2001.08361},
}

@book{cuddon2012dictionary,
  title={A dictionary of literary terms and literary theory},
  author={Cuddon, John Anthony},
  year={2012},
  publisher={John Wiley \& Sons}
}

@article{mao2021grammar,
  title={Grammar-Based Grounded Lexicon Learning},
  author={Mao, Jiayuan and Shi, Freda and Wu, Jiajun and Levy, Roger and Tenenbaum, Josh},
  journal={NeurIPS},
  year={2021},
  url={https://proceedings.neurips.cc/paper/2021/hash/4158f6d19559955bae372bb00f6204e4-Abstract.html},
}

@article{dong2019neural,
  title={Neural logic machines},
  author={Dong, Honghua and Mao, Jiayuan and Lin, Tian and Wang, Chong and Li, Lihong and Zhou, Denny},
  journal={ICLR},
  year={2019},
  url={https://arxiv.org/abs/1904.11694}
}

@article{chen2019neural,
  title={Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension},
  author={Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Zhou, Denny and Song, Dawn and Le, Quoc V.},
  journal={ICLR},
  year={2019},
  url={https://openreview.net/forum?id=ryxjnREFwH}
}

@article{rae2021scaling,
  title={Scaling Language Models: Methods, Analysis \& Insights from Training {G}opher},
  author={Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021},
  url={https://arxiv.org/abs/2112.11446},
}

@article{stanovich2000individual,
  title={Individual differences in reasoning: Implications for the rationality debate?},
  author={Stanovich, Keith E and West, Richard F},
  journal={Behavioral and brain sciences},
  volume={23},
  number={5},
  pages={645--665},
  year={2000},
  publisher={Cambridge University Press},
  url={https://pubmed.ncbi.nlm.nih.gov/11301544/}
}

@article{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    journal = "NAACL",
    year = "2019",
    url = "https://aclanthology.org/N19-1423",
}

@article{miao-etal-2020-diverse,
    title = "A Diverse Corpus for Evaluating and Developing {E}nglish Math Word Problem Solvers",
    author = "Miao, Shen Yun  and
      Liang, Chao Chun  and
      Su, Keh Yih",
    journal = "ACL",
    year = "2020",
    url = "https://aclanthology.org/2020.acl-main.92",
    doi = "10.18653/v1/2020.acl-main.92",
}

@article{koncel-2015-parsing,
    author = {Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena
                            Dumas},
    title = "{Parsing Algebraic Word Problems into Equations}",
    journal = {TACL},
    year = {2015},
    doi = {10.1162/tacl_a_00160},
    url = {https://doi.org/10.1162/tacl\_a\_00160},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00160/1566814/tacl\_a\_00160.pdf},
}

@article{roy-roth-2015-solving,
    title = "Solving General Arithmetic Word Problems",
    author = "Roy, Subhro  and
      Roth, Dan",
    journal = "EMNLP",
    year = "2015",
    url = "https://aclanthology.org/D15-1202",
    doi = "10.18653/v1/D15-1202",
    
}

@article{roy-2016-reasoning,
    author = {Roy, Subhro and Vieira, Tim and Roth, Dan},
    title = "{Reasoning about Quantities in Natural Language}",
    journal = {TACL},
    year = {2015},
    doi = {10.1162/tacl_a_00118},
    url = {https://doi.org/10.1162/tacl\_a\_00118},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00118/1566734/tacl\_a\_00118.pdf},
}



@article{hosseini-etal-2014-learning,
    title = "Learning to Solve Arithmetic Word Problems with Verb Categorization",
    author = "Hosseini, Mohammad Javad  and
      Hajishirzi, Hannaneh  and
      Etzioni, Oren  and
      Kushman, Nate",
    journal = "EMNLP",
    year = "2014",
    url = "https://aclanthology.org/D14-1058",
    doi = "10.3115/v1/D14-1058",
    
}

@article{koncel-kedziorski-etal-2016-mawps,
    title = "{MAWPS}: A Math Word Problem Repository",
    author = "Koncel-Kedziorski, Rik  and
      Roy, Subhro  and
      Amini, Aida  and
      Kushman, Nate  and
      Hajishirzi, Hannaneh",
    journal = "NAACL",
    year = "2016",
    url = "https://aclanthology.org/N16-1136",
    doi = "10.18653/v1/N16-1136",
    
}

@inproceedings{amini-etal-2019-mathqa,
    title = "{M}ath{QA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
    author = "Amini, Aida  and
      Gabriel, Saadia  and
      Lin, Shanchuan  and
      Koncel-Kedziorski, Rik  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh",
    booktitle = "NAACL",
    year = "2019",
    url = "https://aclanthology.org/N19-1245",
    doi = "10.18653/v1/N19-1245",
}


@article{saeed-etal-2021-rulebert,
    title = "{R}ule{BERT}: Teaching Soft Rules to Pre-Trained Language Models",
    author = "Saeed, Mohammed  and
      Ahmadi, Naser  and
      Nakov, Preslav  and
      Papotti, Paolo",
    journal = "EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.110",
    doi = "10.18653/v1/2021.emnlp-main.110",
}

@article{recchia2021teaching,
  title={Teaching Autoregressive Language Models Complex Tasks By Demonstration},
  author={Recchia, Gabriel},
  journal={arXiv preprint arXiv:2109.02102},
  year={2021},
  url={https://arxiv.org/abs/2109.02102}
}

@article{clark2020transformers,
  title={Transformers as soft reasoners over language},
  author={Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  journal={IJCAI},
  year={2020},
  url={https://www.ijcai.org/proceedings/2020/0537.pdf}
}

@article{talmor2020leap,
  title={Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge},
  author={Talmor, Alon and Tafjord, Oyvind and Clark, Peter and Goldberg, Yoav and Berant, Jonathan},
  journal={NeurIPS},
  year={2020},
  url={https://arxiv.org/abs/2006.06609}
}

@article{wiegreffe2021reframing,
  title={Reframing Human-{AI} Collaboration for Generating Free-Text Explanations},
  author={Wiegreffe, Sarah and Hessel, Jack and Swayamdipta, Swabha and Riedl, Mark and Choi, Yejin},
  journal={NAACL},
  year={2022},
  url={https://arxiv.org/abs/2112.08674},
}

@article{hu-etal-2019-multi,
    title = "A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning",
    author = "Hu, Minghao  and
      Peng, Yuxing  and
      Huang, Zhen  and
      Li, Dongsheng",
    journal = "EMNLP",
    year = "2019",
    url = "https://aclanthology.org/D19-1170",
    doi = "10.18653/v1/D19-1170",
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021},
  url={https://arxiv.org/abs/2110.14168}
}

@article{reif2021recipe,
  title={A recipe for arbitrary text style transfer with large language models},
  author={Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Callison-Burch, Chris and Wei, Jason},
  journal={ACL},
  year={2022},
  url={https://arxiv.org/abs/2109.03910}
}

@article{nye2021show,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021},
  url={http://arxiv.org/abs/2112.00114},
}

@article{steinhardt2022mmluforecast,
  title={Updates and Lessons from AI Forecasting},
  author={Jacob Steinhardt},
  journal={Blog post.},
  year={2021},
  url={https://bounded-regret.ghost.io/ai-forecasting/},
  note={See \url{https://prod.hypermind.com/ngdp/en/showcase2/showcase.html?sc=JSAI}},
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={TMLR},
  year={2022},
  url={https://openreview.net/forum?id=yzkSU5zdwD},
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "EMNLP",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{huang2023selfimprove,
    title={Large Language Models Can Self-improve},
    author={Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
    booktitle={arxiv},
    year={2022},
    url={https://openreview.net/forum?id=NiEtU7blzN},
}

@inproceedings{tay2022transcending,
    title={Transcending scaling laws with 0.1\% extra compute},
    author={Yi Tay and Jason Wei and Hyung Won Chung and David R. So and Siamak Shakeri and Xavier Garcia and Vinh Q. Tran and Hauixiu Steven Zheng and Jinfeng Rao and Denny Zhou and Donald Metzler and Neil Houlsby and Quoc V. Le and Mostafa Dehghani},
    booktitle={arxiv},
    year={2022},
}

@article{wang2022language,
  title={What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?},
  author={Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  journal={ICML},
  year={2022},
  url={https://arxiv.org/abs/2204.05832},
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={arXiv preprint arXiv:2206.14858},
  year={2022},
  url={https://arxiv.org/abs/2206.14858},
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
}

@article{scialom2022continual,
  title={Continual-{T0}: Progressively Instructing 50+ Tasks to Language Models Without Forgetting},
  author={Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  journal={arXiv preprint arXiv:2205.12393},
  year={2022},
  url={https://arxiv.org/abs/2205.12393},
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={ICML},
  year={2018},
  url={https://arxiv.org/abs/1804.04235},
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022},
  url={https://arxiv.org/abs/2210.03057},
}

@article{suzgun2022challenging,
  title={Challenging {BIG-B}ench tasks and whether chain-of-thought can solve them},
  author={Mirac Suzgun and Nathan Scales and Nathaneal Scharli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny ZHou and Jason Wei},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022},
  url={https://arxiv.org/abs/2210.09261},
}

@inproceedings{camburu-etal-2020-make,
    title = "Make Up Your Mind! {A}dversarial Generation of Inconsistent Natural Language Explanations",
    author = "Camburu, Oana-Maria  and
      Shillingford, Brendan  and
      Minervini, Pasquale  and
      Lukasiewicz, Thomas  and
      Blunsom, Phil",
    booktitle = "ACL",
    year = "2020",
    url = "https://aclanthology.org/2020.acl-main.382",
}


@article{rajani-etal-2019-explain,
    title = "Explain Yourself! {L}everaging Language Models for Commonsense Reasoning",
    author = "Rajani, Nazneen Fatema  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard",
    journal = "ACL",
    year = "2019",
    url = "https://aclanthology.org/P19-1487",
    doi = "10.18653/v1/P19-1487",
}

@article{camburu2018snli,
  title={e-{SNLI}: Natural Language Inference with Natural Language Explanations},
  author={Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  journal={NeurIPS},
  year={2018},
  url={https://arxiv.org/pdf/1812.01193.pdf}
}

@article{zhong2021meta,
  title={Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections},
  author={Zhong, Ruiqi and Lee, Kristy and Zhang, Zheng and Klein, Dan},
  journal={EMNLP Findings},
  year={2021},
  url={https://aclanthology.org/2021.findings-emnlp.244/},
}

@inproceedings{ye2021crossfit,
  title={CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in {NLP}},
  author={Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  booktitle = {EMNLP},
  year={2021},
  url={https://arxiv.org/abs/2104.08835},
}

@article{xue-etal-2022-byt5,
    title = "{B}y{T}5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models",
    author = "Xue, Linting  and
      Barua, Aditya  and
      Constant, Noah  and
      Al-Rfou, Rami  and
      Narang, Sharan  and
      Kale, Mihir  and
      Roberts, Adam  and
      Raffel, Colin",
    journal = "TACL",
    year = "2022",
    url = "https://aclanthology.org/2022.tacl-1.17",
}

@article{bigbench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022},
  url={https://arxiv.org/abs/2206.04615},
}

@inproceedings{bach-etal-2022-promptsource,
    title = "{P}rompt{S}ource: An Integrated Development Environment and Repository for Natural Language Prompts",
    author = "Bach, Stephen  and
      Sanh, Victor  and
      Yong, Zheng Xin  and
      Webson, Albert  and
      Raffel, Colin  and
      Nayak, Nihal V.  and
      Sharma, Abheesht  and
      Kim, Taewoon  and
      Bari, M Saiful  and
      Fevry, Thibault  and
      Alyafeai, Zaid  and
      Dey, Manan  and
      Santilli, Andrea  and
      Sun, Zhiqing  and
      Ben-david, Srulik  and
      Xu, Canwen  and
      Chhablani, Gunjan  and
      Wang, Han  and
      Fries, Jason  and
      Al-shaibani, Maged  and
      Sharma, Shanya  and
      Thakker, Urmish  and
      Almubarak, Khalid  and
      Tang, Xiangru  and
      Radev, Dragomir  and
      Jiang, Mike Tian-jian  and
      Rush, Alexander",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.9",
    doi = "10.18653/v1/2022.acl-demo.9",
    pages = "93--104",
    abstract = "PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.",
}

@article{wiegreffe2021teach,
  title={Teach me to explain: A review of datasets for explainable {NLP}},
  author={Wiegreffe, Sarah and Marasovi{\'c}, Ana},
  journal={NeurIPS},
  year={2021},
  url={https://arxiv.org/abs/2102.12060}
}


@article{talmor2022commonsenseqa,
  title={Commonsense{QA} 2.0: {E}xposing the limits of AI through gamification},
  author={Talmor, Alon and Yoran, Ori and Bras, Ronan Le and Bhagavatula, Chandra and Goldberg, Yoav and Choi, Yejin and Berant, Jonathan},
  journal={NeurIPS Track on Datasets and Benchmarks},
  year={2021},
  url={https://arxiv.org/abs/2201.05320}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  journal={ICLR 2022},
  year={2021},
  url={https://openreview.net/forum?id=gEZrGCozdqR},
}

@article{sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    journal = "EMNLP (System Demonstrations)",
    year = "2018",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
}

@article{zoph2022designing,
  title={Designing effective sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022}
}

@article{liu2021pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={arXiv preprint arXiv:2107.13586},
  year={2021},
  url={https://arxiv.org/abs/2107.13586}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021},
  url={https://arxiv.org/abs/2107.03374}
}

@article{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    journal = "ACL",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
}

@article{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    journal = "EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
}

@article{gao-etal-2021-making,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    journal = "ACL",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.295",
    doi = "10.18653/v1/2021.acl-long.295",
}

@article{li2022advance,
  title={On the Advance of Making Language Models Better Reasoners},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2206.02336},
  year={2022}
}

@inproceedings{xu-etal-2021-fusing,
    title = "Fusing Context Into Knowledge Graph for Commonsense Question Answering",
    author = "Xu, Yichong  and
      Zhu, Chenguang  and
      Xu, Ruochen  and
      Liu, Yang  and
      Zeng, Michael  and
      Huang, Xuedong",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.102",
    doi = "10.18653/v1/2021.findings-acl.102",
    pages = "1201--1207",
}

@article{geva-etal-2021-aristotle,
    title = "Did Aristotle Use a Laptop? {A} Question Answering Benchmark with Implicit Reasoning Strategies",
    author = "Geva, Mor  and
      Khashabi, Daniel  and
      Segal, Elad  and
      Khot, Tushar  and
      Roth, Dan  and
      Berant, Jonathan",
    journal = "TACL",
    year = "2021",
    url = "https://aclanthology.org/2021.tacl-1.21",
    doi = "10.1162/tacl_a_00370",
}

@article{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    journal = "NAACL",
    year = "2019",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
}

@book{williamjames,
  title={The Principles of Psychology},
  author={James, William},
  volume={1},
  year={1890},
  publisher={Cosimo}
}

@article{narang2020wt5,
  title={{WT5}?! {T}raining text-to-text models to explain their predictions},
  author={Narang, Sharan and Raffel, Colin and Lee, Katherine and Roberts, Adam and Fiedel, Noah and Malkan, Karishma},
  journal={arXiv preprint arXiv:2004.14546},
  year={2020},
  url={https://arxiv.org/abs/2004.14546}
}

@book{joyce-1922-ulysses,
  title={Ulysses},
  author={James Joyce},
  year={1922},
  publisher={Shakespeare and Company},
}

@article{geva-etal-2020-injecting,
    title = "Injecting Numerical Reasoning Skills into Language Models",
    author = "Geva, Mor  and
      Gupta, Ankit  and
      Berant, Jonathan",
    journal = "ACL",
    year = "2020",
    url = "https://aclanthology.org/2020.acl-main.89",
    doi = "10.18653/v1/2020.acl-main.89",
}

@article{andor-etal-2019-giving,
    title = "Giving {BERT} a Calculator: Finding Operations and Arguments with Reading Comprehension",
    author = "Andor, Daniel  and
      He, Luheng  and
      Lee, Kenton  and
      Pitler, Emily",
    journal = "EMNLP",
    year = "2019",
    url = "https://aclanthology.org/D19-1609",
    doi = "10.18653/v1/D19-1609",
}

@article{piekos-etal-2021-measuring,
    title = "Measuring and Improving {BERT}{'}s Mathematical Abilities by Predicting the Order of Reasoning.",
    author = "Pi{\k{e}}kos, Piotr  and
      Malinowski, Mateusz  and
      Michalewski, Henryk",
    journal = "ACL",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-short.49",
    doi = "10.18653/v1/2021.acl-short.49",
}

@article{drori2021neural,
  title={A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More},
  author={Drori, Iddo and Tran, Sunny and Wang, Roman and Cheng, Newman and Liu, Kevin and Tang, Leonard and Ke, Elizabeth and Singh, Nikhil and Patti, Taylor L and Lynch, Jayson and others},
  journal={arXiv preprint arXiv:2112.15594},
  year={2021}
}

@article{hancock-etal-2018-training,
    title = "Training Classifiers with Natural Language Explanations",
    author = "Hancock, Braden  and
      Varma, Paroma  and
      Wang, Stephanie  and
      Bringmann, Martin  and
      Liang, Percy  and
      R{\'e}, Christopher",
    journal = "ACL",
    year = "2018",
    url = "https://aclanthology.org/P18-1175",
    doi = "10.18653/v1/P18-1175",
}

@article{rajagopal2021selfexplain,
    title = "{SelfExplain}: A Self-Explaining Architecture for Neural Text Classifiers",
    author = "Rajagopal, Dheeraj  and
      Balachandran, Vidhisha  and
      Hovy, Eduard H.  and
      Tsvetkov, Yulia",
    journal = "EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.64",
    doi = "10.18653/v1/2021.emnlp-main.64",
}

@article{marasovic-etal-2020-natural,
    title = "Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs",
    author = "Marasovi{\'c}, Ana  and
      Bhagavatula, Chandra  and
      Park, Jae sung  and
      Le Bras, Ronan  and
      Smith, Noah A.  and
      Choi, Yejin",
    journal = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = "2020",
    url = "https://aclanthology.org/2020.findings-emnlp.253",
    doi = "10.18653/v1/2020.findings-emnlp.253",
}

@article{bostrom-etal-2021-flexible,
    title = "Flexible Generation of Natural Language Deductions",
    author = "Bostrom, Kaj  and
      Zhao, Xinyu  and
      Chaudhuri, Swarat  and
      Durrett, Greg",
    journal = "EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.506",
    doi = "10.18653/v1/2021.emnlp-main.506",
}

@article{majumder2021rationale,
  title={Rationale-inspired natural language explanations with commonsense},
  author={Majumder, Bodhisattwa Prasad and Camburu, Oana-Maria and Lukasiewicz, Thomas and McAuley, Julian},
  journal={arXiv preprint arXiv:2106.13876},
  year={2021},
  url={https://arxiv.org/abs/2106.13876}
}

@inproceedings{min-etal-2022-metaicl,
    title = "{M}eta{ICL}: Learning to Learn In Context",
    author = "Min, Sewon  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    booktitle = "NAACL",
    year = "2022",
    url = "https://aclanthology.org/2022.naacl-main.201",
}

@inproceedings{aghajanyan-etal-2021-muppet,
    title = "Muppet: Massive Multi-task Representations with Pre-Finetuning",
    author = "Aghajanyan, Armen  and
      Gupta, Anchit  and
      Shrivastava, Akshat  and
      Chen, Xilun  and
      Zettlemoyer, Luke  and
      Gupta, Sonal",
    booktitle = "EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.468",
}

@inproceedings{anantha-etal-2021-open,
    title = "Open-Domain Question Answering Goes Conversational via Question Rewriting",
    author = "Anantha, Raviteja  and
      Vakulenko, Svitlana  and
      Tu, Zhucheng  and
      Longpre, Shayne  and
      Pulman, Stephen  and
      Chappidi, Srinivas",
    booktitle = "NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.44/",
}

@inproceedings{yasunaga2020graph,
  title={Graph-based, self-supervised program repair from diagnostic feedback},
  author={Yasunaga, Michihiro and Liang, Percy},
  booktitle={ICML},
  year={2020},
  url={http://go/arxiv/2005.10636},
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022},
  url={http://go/arxiv/2203.07814},
}

@inproceedings{byrne-etal-2019-taskmaster,
    title = "Taskmaster-1: {T}oward a Realistic and Diverse Dialog Dataset",
    author = "Byrne, Bill  and
      Krishnamoorthi, Karthik  and
      Sankar, Chinnadhurai  and
      Neelakantan, Arvind  and
      Goodrich, Ben  and
      Duckworth, Daniel  and
      Yavuz, Semih  and
      Dubey, Amit  and
      Kim, Kyu-Young  and
      Cedilnik, Andy",
    booktitle = "EMNLP",
    year = "2019",
    url = "https://aclanthology.org/D19-1459",
    doi = "10.18653/v1/D19-1459",
}

@InProceedings{pmlr-v162-dai22a,
  title = 	 {Dialog Inpainting: Turning Documents into Dialogs},
  author =       {Dai, Zhuyun and Chaganty, Arun Tejasvi and Zhao, Vincent Y and Amini, Aida and Rashid, Qazi Mamunur and Green, Mike and Guu, Kelvin},
  booktitle = 	 {ICML},
  year = 	 {2022},
  pdf = 	 {https://proceedings.mlr.press/v162/dai22a/dai22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/dai22a.html},
}


@article{hendricks2016generating,
  title={Generating visual explanations},
  author={Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
  journal={European conference on computer vision},
  pages={3--19},
  year={2016},
  organization={Springer}
}

@article{marasovic2021few,
  title={Few-Shot Self-Rationalization with Natural Language Prompts},
  author={Marasovi{\'c}, Ana and Beltagy, Iz and Downey, Doug and Peters, Matthew E},
  journal={NAACL Findings},
  year={2022},
  url={http://arxiv.org/abs/2111.08284},
}

@article{tay2022unifying,
  title={Unifying Language Learning Paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022},
  url={https://arxiv.org/abs/2205.05131},
}

@article{wang2019superglue,
  title={Super{G}lue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={NeurIPS},
  year={2019},
  url={http://go/arxiv/1905.00537}
}

@article{yordanov2021few,
  title={Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations},
  author={Yordanov, Yordan and Kocijan, Vid and Lukasiewicz, Thomas and Camburu, Oana-Maria},
  journal={arXiv preprint arXiv:2112.06204},
  year={2021},
  url={http://arxiv.org/abs/2112.06204},
}

@article{chen2020compositional,
  title={Compositional Generalization via Neural-Symbolic Stack Machines},
  author={Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Song, Dawn and Zhou, Denny},
  journal={NeurIPS},
  volume={33},
  year={2020}
}

@article{hase2021can,
  title={When can models learn from explanations? a formal framework for understanding the roles of explanation data},
  author={Hase, Peter and Bansal, Mohit},
  journal={ACL},
  year={2022},
  url={https://arxiv.org/abs/2102.02201},
}

@article{le-scao-rush-2021-many,
    title = "How many data points is a prompt worth?",
    author = "Le Scao, Teven  and
      Rush, Alexander",
    journal = "NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.208",
    doi = "10.18653/v1/2021.naacl-main.208",
}

@article{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  journal={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  year={2021},
  url={https://arxiv.org/abs/2102.07350},
}

@article{lev-etal-2004-solving,
    title = "Solving logic puzzles: From robust processing to precise semantics",
    author = "Lev, Iddo  and
      MacCartney, Bill  and
      Manning, Christopher  and
      Levy, Roger",
    journal = "Proceedings of the 2nd Workshop on Text Meaning and Interpretation",
    year = "2004",
    url = "https://aclanthology.org/W04-0902",
}

@article{Lourie2021UNICORNOR,
  title={{UNICORN} on {RAINBOW}: A Universal Commonsense Reasoning Model on a New Multitask Benchmark},
  author={Nicholas Lourie and Ronan {Le Bras} and Chandra Bhagavatula and Yejin Choi},
  journal={AAAI},
  year={2021},
  url={https://arxiv.org/abs/2103.13009}
}

@article{aribandi2021ext5,
  title={Ext5: Towards extreme multi-task scaling for transfer learning},
  author={Aribandi, Vamsi and Tay, Yi and Schuster, Tal and Rao, Jinfeng and Zheng, Huaixiu Steven and Mehta, Sanket Vaibhav and Zhuang, Honglei and Tran, Vinh Q and Bahri, Dara and Ni, Jianmo and others},
  journal={arXiv preprint arXiv:2111.10952},
  year={2021}
}

@ARTICLE{du_glam_2021,
       author = {{Du}, Nan and {Huang}, Yanping and {Dai}, Andrew M. and {Tong}, Simon and {Lepikhin}, Dmitry and {Xu}, Yuanzhong and {Krikun}, Maxim and {Zhou}, Yanqi and {Yu}, Adams Wei and {Firat}, Orhan and {Zoph}, Barret and {Fedus}, Liam and {Bosma}, Maarten and {Zhou}, Zongwei and {Wang}, Tao and {Wang}, Yu Emma and {Webster}, Kellie and {Pellat}, Marie and {Robinson}, Kevin and {Meier-Hellstern}, Kathleen and {Duke}, Toju and {Dixon}, Lucas and {Zhang}, Kun and {Le}, Quoc V and {Wu}, Yonghui and {Chen}, Zhifeng and {Cui}, Claire},
        title = "{GLaM: Efficient Scaling of Language Models with Mixture-of-Experts}",
      journal = {ICML},
         year = 2022,
         url = {https://arxiv.org/abs/2112.06905},
}

@inproceedings{paperno-etal-2016-lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534",
}

@inproceedings{khashabi-etal-2020-unifiedqa,
    title = "{UnifiedQA}: Crossing Format Boundaries with a Single {QA} System",
    author = "Khashabi, Daniel  and
      Min, Sewon  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = "2020",
    url = "https://aclanthology.org/2020.findings-emnlp.171",
}

@misc{https://doi.org/10.48550/arxiv.2101.02235,
  doi = {10.48550/ARXIV.2101.02235},
  url = {https://arxiv.org/abs/2101.02235},
  author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies}
},

@inproceedings{onoe2021creak,
  title={{CREAK}: A Dataset for Commonsense Reasoning over Entity Knowledge},
  author={Onoe, Yasumasa and Zhang, Michael JQ and Choi, Eunsol and Durrett, Greg},
  booktitle={NeurIPS Datasets and Benchmarks Track (Round 2)},
  year={2021},
  url={https://arxiv.org/abs/2109.01653},
}

@inproceedings{aggarwal-etal-2021-explanations,
    title = "{E}xplanations for {C}ommonsense{QA}: {N}ew {D}ataset and {M}odels",
    author = "Aggarwal, Shourya  and
      Mandowara, Divyanshu  and
      Agrawal, Vishwajeet  and
      Khandelwal, Dinesh  and
      Singla, Parag  and
      Garg, Dinesh",
    booktitle = "ACL",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.238",
}

@inproceedings{khot2020qasc,
  title={{QASC}: A dataset for question answering via sentence composition},
  author={Khot, Tushar and Clark, Peter and Guerquin, Michal and Jansen, Peter and Sabharwal, Ashish},
  booktitle={AAAI},
  year={2020},
  url={https://arxiv.org/abs/1910.11473},
}

@article{lamm2021qed,
  title={{QED}: A framework and dataset for explanations in question answering},
  author={Lamm, Matthew and Palomaki, Jennimaria and Alberti, Chris and Andor, Daniel and Choi, Eunsol and Soares, Livio Baldini and Collins, Michael},
  journal={TACL},
  volume={9},
  pages={790--806},
  year={2021},
  publisher={MIT Press},
  url={https://arxiv.org/abs/2009.06354},
}

@inproceedings{wang2019does,
  title={Does it Make Sense? {A}nd Why? {A} Pilot Study for Sense Making and Explanation},
  author={Wang, Cunxiang and Liang, Shuailong and Zhang, Yue and Li, Xiaonan and Gao, Tian},
  booktitle={ACL},
  year={2019},
  url={https://arxiv.org/abs/1906.00363},
}


@ARTICLE{2022_palm_saycan,
       author = {{Ahn}, Michael and {Brohan}, Anthony and {Brown}, Noah and {Chebotar}, Yevgen and {Cortes}, Omar and {David}, Byron and {Finn}, Chelsea and {Fu}, Chuyuan and {Gopalakrishnan}, Keerthana and {Hausman}, Karol and {Herzog}, Alex and {Ho}, Daniel and {Hsu}, Jasmine and {Ibarz}, Julian and {Ichter}, Brian and {Irpan}, Alex and {Jang}, Eric and {Jauregui Ruano}, Rosario and {Jeffrey}, Kyle and {Jesmonth}, Sally and {Joshi}, Nikhil J and {Julian}, Ryan and {Kalashnikov}, Dmitry and {Kuang}, Yuheng and {Lee}, Kuang-Huei and {Levine}, Sergey and {Lu}, Yao and {Luu}, Linda and {Parada}, Carolina and {Pastor}, Peter and {Quiambao}, Jornell and {Rao}, Kanishka and {Rettinghouse}, Jarek and {Reyes}, Diego and {Sermanet}, Pierre and {Sievers}, Nicolas and {Tan}, Clayton and {Toshev}, Alexander and {Vanhoucke}, Vincent and {Xia}, Fei and {Xiao}, Ted and {Xu}, Peng and {Xu}, Sichun and {Yan}, Mengyuan and {Zeng}, Andy},
        title = "{Do As I Can, Not As I Say: Grounding Language in Robotic Affordances}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics, Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2022,
        month = apr,
          eid = {arXiv:2204.01691},
        pages = {arXiv:2204.01691},
archivePrefix = {arXiv},
       eprint = {2204.01691},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220401691A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{roberts2022t5x,
  url = {https://arxiv.org/abs/2203.17189},
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
  journal={arXiv preprint arXiv:2203.17189},
  year = {2022},
}

@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{hao2022language,
  title={Language models are general-purpose interfaces},
  author={Hao, Yaru and Song, Haoyu and Dong, Li and Huang, Shaohan and Chi, Zewen and Wang, Wenhui and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2206.06336},
  year={2022}
}

@article{puri2022many,
  title={How Many Data Samples is an Additional Instruction Worth?},
  author={Puri, Ravsehaj Singh and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
  journal={arXiv preprint arXiv:2203.09161},
  year={2022},
  url={https://arxiv.org/abs/2203.09161},
}

@article{padmakumar2022exploring,
  title={Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning},
  author={Padmakumar, Vishakh and Lausen, Leonard and Ballesteros, Miguel and Zha, Sheng and He, He and Karypis, George},
  journal={arXiv preprint arXiv:2204.11117},
  year={2022},
  url={https://aclanthology.org/2022.naacl-main.183/},
}

@inproceedings{rudinger2018gender,
    title = "Gender Bias in Coreference Resolution",
    author = "Rudinger, Rachel  and
      Naradowsky, Jason  and
      Leonard, Brian  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2002",
    doi = "10.18653/v1/N18-2002",
    pages = "8--14",
    abstract = "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these {``}Winogender schemas,{''} we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.",
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@inproceedings{pushkarna2021data,
  title={Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI},
  author={Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
  booktitle={Data-centric AI Workshop at NeurIPS},
  year={2021},
  url={https://arxiv.org/abs/2204.01075},
}

@article{jouppi2020domain,
  title={A domain-specific supercomputer for training deep neural networks},
  author={Jouppi, Norman P and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Patil, Nishant and Laudon, James and Young, Cliff and Patterson, David},
  journal={Communications of the ACM},
  volume={63},
  number={7},
  pages={67--78},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{fisch2019mrqa,
  title={MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension},
  author={Fisch, Adam and Talmor, Alon and Jia, Robin and Seo, Minjoon and Choi, Eunsol and Chen, Danqi},
  booktitle={Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
  pages={1--13},
  year={2019}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={Real{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020}
}

@article{luu2021time,
  title={Time waits for no one! analysis and challenges of temporal misalignment},
  author={Luu, Kelvin and Khashabi, Daniel and Gururangan, Suchin and Mandyam, Karishma and Smith, Noah A},
  journal={arXiv preprint arXiv:2111.07408},
  year={2021}
}

@incollection{gargee2022analyzing,
  title={Analyzing and Addressing the Difference in Toxicity Prediction Between Different Comments with Same Semantic Meaning in {G}oogle’s {P}erspective {API}},
  author={Gargee, SK and Gopinath, Pranav Bhargav and Kancharla, Shridhar Reddy SR and Anand, CR and Babu, Anoop S},
  booktitle={ICT Systems and Sustainability: Proceedings of ICT4SD 2022},
  pages={455--464},
  year={2022},
  publisher={Springer}
}

@article{gao2020pile,
  title={The {P}ile: An 800{GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{friedl2023dis,
  title={Dis/similarities in the Design and Development of Legal and Algorithmic Normative Systems: the Case of Perspective API},
  author={Friedl, Paul},
  journal={Law, Innovation and Technology},
  pages={1--35},
  year={2023},
  publisher={Taylor \& Francis}
}

@article{black2022gpt,
  title={{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={Challenges \& Perspectives in Creating Large Language Models},
  pages={95},
  year={2022}
}

@article{lees2022perspective,
  title={A New Generation of {P}erspective {API}: Efficient Multilingual Character-level Transformers},
  author={Lees, Alyssa and Tran, Vinh Q and Tay, Yi and Sorensen, Jeffrey and Gupta, Jai and Metzler, Donald and Vasserman, Lucy},
  journal={arXiv preprint arXiv:2202.11176},
  url={http://perspectiveapi.com.com},
  year={2022}
}

@article{endredy2013more,
  title={More effective boilerplate removal-the goldminer algorithm},
  author={Endr{\'e}dy, Istv{\'a}n and Nov{\'a}k, Attila},
  journal={Polibits},
  volume={48},
  pages={79--83},
  year={2013},
  publisher={Instituto Polit{\'e}cnico Nacional, Centro de Innovaci{\'o}n y Desarrollo~…}
}

@inproceedings{gardner2020evaluating,
  title={Evaluating Models’ Local Decision Boundaries via Contrast Sets},
  author={Gardner, Matt and Artzi, Yoav and Basmov, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1307--1323},
  year={2020}
}

@inproceedings{dodge2021documenting,
  title={Documenting Large Webtext Corpora: A Case Study on the {C}olossal {C}lean {C}rawled {C}orpus},
  author={Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={1286--1305},
  year={2021}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Leo, Gao and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  note         = {{If you use this software, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@inproceedings{baumgartner2020pushshift,
  title={The {P}ushshift {R}eddit Dataset},
  author={Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={14},
  pages={830--839},
  year={2020}
}

@article{scao2022bloom,
  title={{BLOOM}: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{kwiatkowski2019natural,
  title={Natural {Q}uestions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press}
}

@inproceedings{rogers2021changing,
  title={Changing the World by Changing the Data},
  author={Rogers, Anna},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={2182--2194},
  year={2021}
}

@inproceedings{laurenccon2022bigscience,
  title={The {B}ig{S}cience {ROOTS} {C}orpus: A 1.6{TB} Composite Multilingual Dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Ponferrada, Eduardo Gonz{\'a}lez and Nguyen, Huu and others},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022}
}

@article{liu2019roberta,
  title={Ro{BERT}a: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{yang2019xlnet,
  title={{XLN}et: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@inproceedings{xue2021mt5,
  title={m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={483--498},
  year={2021}
}

@misc{stmoe,
  doi = {10.48550/ARXIV.2202.08906},
  url = {https://arxiv.org/abs/2202.08906},
  author = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{cortes2021inconsistency,
  title={Inconsistency in Conference Peer Review: Revisiting the 2014 {N}eur{IPS} Experiment},
  author={Cortes, Corinna and Lawrence, Neil D},
  journal={arXiv preprint arXiv:2109.09774},
  year={2021}
}

@article{touvron2023llama,
  title={{LL}a{MA}: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhang2022opt,
  title={{OPT}: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{kreutzer2022quality,
  title={Quality at a glance: An audit of web-crawled multilingual datasets},
  author={Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={50--72},
  year={2022},
  publisher={MIT Press}
}

@article{luccioni2021s,
  title={What's in the Box? A Preliminary Analysis of Undesirable Content in the {C}ommon {C}rawl Corpus},
  author={Luccioni, Alexandra Sasha and Viviano, Joseph D},
  journal={arXiv preprint arXiv:2105.02732},
  year={2021}
}

@inproceedings{welbl2021challenges,
  title={Challenges in Detoxifying Language Models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2447--2469},
  year={2021}
}

@inproceedings{meade2022empirical,
  title={An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models},
  author={Meade, Nicholas and Poole-Dayan, Elinor and Reddy, Siva},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1878--1898},
  year={2022}
}

@inproceedings{xu-etal-2021-detoxifying,
    title = "Detoxifying Language Models Risks Marginalizing Minority Voices",
    author = "Xu, Albert  and
      Pathak, Eshaan  and
      Wallace, Eric  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Klein, Dan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.190",
    doi = "10.18653/v1/2021.naacl-main.190",
    pages = "2390--2397",
}

@inproceedings{nichol2022glide,
  title={GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and Mcgrew, Bob and Sutskever, Ilya and Chen, Mark},
  booktitle={International Conference on Machine Learning},
  pages={16784--16804},
  year={2022},
  organization={PMLR}
}

@misc{dalle2pretraining2022,
  title={{DALL}-{E} 2 pre-training mitigations},
  author={Nichol, Alex},
  url={https://openai.com/research/dall-e-2-pre-training-mitigations},
  year={2022}
}

@article{gururangan2022whose,
  title={Whose Language Counts as High Quality? {M}easuring Language Ideologies in Text Data Selection},
  author={Gururangan, Suchin and Card, Dallas and Drier, Sarah K and Gade, Emily K and Wang, Leroy Z and Wang, Zeyu and Zettlemoyer, Luke and Smith, Noah A},
  journal={arXiv preprint arXiv:2201.10474},
  year={2022}
}

@book{labov2011principles,
  title={Principles of linguistic change, volume 3: Cognitive and cultural factors},
  author={Labov, William},
  volume={3},
  year={2011},
  publisher={John Wiley \& Sons}
}

@article{altmann2009beyond,
  title={Beyond word frequency: Bursts, lulls, and scaling in the temporal distributions of words},
  author={Altmann, Eduardo G and Pierrehumbert, Janet B and Motter, Adilson E},
  journal={PLOS one},
  volume={4},
  number={11},
  pages={e7678},
  year={2009},
  publisher={Public Library of Science San Francisco, USA}
}

@article{eisenstein2014diffusion,
  title={Diffusion of lexical change in social media},
  author={Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A and Xing, Eric P},
  journal={PloS one},
  volume={9},
  number={11},
  pages={e113114},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@article{lazaridou2021mind,
  title={Mind the gap: Assessing temporal generalization in neural language models},
  author={Lazaridou, Angeliki and Kuncoro, Adhi and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and de Masson d'Autume, Cyprien and Kocisky, Tomas and Ruder, Sebastian and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29348--29363},
  year={2021}
}


@InProceedings{pmlr-v162-liska22a,
  title = 	 {{S}treaming{QA}: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models},
  author =       {Liska, Adam and Kocisky, Tomas and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and De Masson D'Autume, Cyprien and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and Gilsenan-Mcmahon, Ellen and Austin, Sophia and Blunsom, Phil and Lazaridou, Angeliki},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13604--13622},
  year = 	 {2022},
  url = 	 {https://proceedings.mlr.press/v162/liska22a/liska22a.pdf}
}

@inproceedings{yaowild,
  title={Wild-{T}ime: A Benchmark of in-the-Wild Distribution Shift over Time},
  author={Yao, Huaxiu and Choi, Caroline and Cao, Bochuan and Lee, Yoonho and Koh, Pang Wei and Finn, Chelsea},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022}
}

@article{agarwal2022temporal,
  title={Temporal effects on pre-trained models for language processing tasks},
  author={Agarwal, Oshin and Nenkova, Ani},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={904--921},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{zhang2021situatedqa,
  title={Situated{QA}: Incorporating Extra-Linguistic Contexts into QA},
  author={Zhang, Michael and Choi, Eunsol},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7371--7387},
  year={2021}
}

@article{dhingra2022time,
  title={Time-aware language models as temporal knowledge bases},
  author={Dhingra, Bhuwan and Cole, Jeremy R and Eisenschlos, Julian Martin and Gillick, Daniel and Eisenstein, Jacob and Cohen, William W},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={257--273},
  year={2022},
  publisher={MIT Press}
}

@article{singh2022addressing,
  title={Addressing Distribution Shift at Test Time in Pre-trained Language Models},
  author={Singh, Ayush and Ortega, John E},
  journal={arXiv preprint arXiv:2212.02384},
  year={2022}
}

@inproceedings{jaidka-etal-2018-diachronic,
    title = "Diachronic degradation of language models: Insights from social media",
    author = "Jaidka, Kokil  and
      Chhaya, Niyati  and
      Ungar, Lyle",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2032",
    doi = "10.18653/v1/P18-2032",
    pages = "195--200",
}

@inproceedings{rottger-pierrehumbert-2021-temporal-adaptation,
    title = "Temporal Adaptation of {BERT} and Performance on Downstream Document Classification: Insights from Social Media",
    author = {R{\"o}ttger, Paul  and
      Pierrehumbert, Janet},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.206",
    doi = "10.18653/v1/2021.findings-emnlp.206",
    pages = "2400--2412",
}

@article{jang2022temporalwiki,
  title={Temporal{W}iki: A lifelong benchmark for training and evaluating ever-evolving language models},
  author={Jang, Joel and Ye, Seonghyeon and Lee, Changho and Yang, Sohee and Shin, Joongbo and Han, Janghoon and Kim, Gyeonghun and Seo, Minjoon},
  journal={arXiv preprint arXiv:2204.14211},
  year={2022}
}

@article{xie2023data,
  title={Data Selection for Language Models via Importance Resampling},
  author={Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2302.03169},
  year={2023}
}

@inproceedings{longpre2021entity,
  title={Entity-Based Knowledge Conflicts in Question Answering},
  author={Longpre, Shayne and Perisetla, Kartik and Chen, Anthony and Ramesh, Nikhil and DuBois, Chris and Singh, Sameer},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7052--7063},
  year={2021}
}

@article{villalobos2022will,
  title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  journal={arXiv preprint arXiv:2211.04325},
  year={2022}
}

@article{chinchilla2022implications,
  title={Chinchilla's Wild Implications},
  author={Nostalgebraist},
  journal={AI Alignment Forum},
  year={2022},
  url={https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications}
}

@inproceedings{li2022quantifying,
  title={Quantifying Adaptability in Pre-trained Language Models with 500 Tasks},
  author={Li, Belinda Z and Yu, Jane and Khabsa, Madian and Zettlemoyer, Luke and Halevy, Alon and Andreas, Jacob},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4696--4715},
  year={2022}
}

@inproceedings{longpre2022active,
  title={Active Learning Over Multiple Domains in Natural Language Tasks},
  author={Longpre, Shayne and Reisler, Julia Rachel and Huang, Edward Greg and Lu, Yi and Frank, Andrew and Ramesh, Nikhil and DuBois, Christopher},
  booktitle={NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications},
  year={2022}
}

@inproceedings{longpre2019exploration,
  title={An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering},
  author={Longpre, Shayne and Lu, Yi and Tu, Zhucheng and DuBois, Chris},
  booktitle={Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
  pages={220--227},
  year={2019}
}

@inproceedings{pruksachatkun2020intermediate,
  title={Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?},
  author={Pruksachatkun, Yada and Phang, Jason and Liu, Haokun and Htut, Phu Mon and Zhang, Xiaoyi and Pang, Richard Yuanzhe and Vania, Clara and Kann, Katharina and Bowman, Samuel},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5231--5247},
  year={2020}
}

@article{iter2021complementarity,
  title={On the Complementarity of Data Selection and Fine Tuning for Domain Adaptation},
  author={Iter, Dan and Grangier, David},
  journal={arXiv preprint arXiv:2109.07591},
  year={2021}
}

@article{inan2021training,
  title={Training data leakage analysis in language models},
  author={Inan, Huseyin A and Ramadan, Osman and Wutschitz, Lukas and Jones, Daniel and R{\"u}hle, Victor and Withers, James and Sim, Robert},
  journal={arXiv preprint arXiv:2101.05405},
  year={2021}
}

@inproceedings{wang2020balancing,
  title={Balancing Training for Multilingual Neural Machine Translation},
  author={Wang, Xinyi and Tsvetkov, Yulia and Neubig, Graham},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8526--8537},
  year={2020}
}

@inproceedings{papadimitriou2020learning,
  title={Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models},
  author={Papadimitriou, Isabel and Jurafsky, Dan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6829--6839},
  year={2020}
}

@inproceedings{gururangan2020don,
  title={Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@inproceedings{wuinsights,
  title={Insights into Pre-training via Simpler Synthetic Tasks},
  author={Wu, Yuhuai and Li, Felix and Liang, Percy},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{bender2013linguistic,
  title={Linguistic fundamentals for natural language processing: 100 essentials from morphology and syntax},
  author={Bender, Emily M},
  journal={Synthesis lectures on human language technologies},
  volume={6},
  number={3},
  pages={1--184},
  year={2013},
  publisher={Morgan \& Claypool Publishers}
}

@techreport{kincaid1975derivation,
  title={Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel},
  author={Kincaid, J Peter and Fishburne Jr, Robert P and Rogers, Richard L and Chissom, Brad S},
  year={1975},
  institution={Naval Technical Training Command Millington TN Research Branch}
}


@misc{gcloud_info,
  author = {{Google Cloud NLP}},
  title = {Google {C}loud InfoType Detector},
  url = {https://cloud.google.com/dlp/docs/infotypes-reference},
  date = {2023-03-9},
  year = {2023}
}

@misc{gcloud_sentiment,
  author = {{Google Cloud NLP}},
  title = {Google {C}loud Analyzing Sentiment},
  url = {https://cloud.google.com/natural-language/docs/analyzing-sentiment},
  date = {2023-03-9},
  year = {2023}
}
@article{shen2021towards,
  title={Towards out-of-distribution generalization: A survey},
  author={Shen, Zheyan and Liu, Jiashuo and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  journal={arXiv preprint arXiv:2108.13624},
  year={2021}
}

@article{abadi2016tensorflow,
  title={Tensor{F}low: Large-scale machine learning on heterogeneous distributed systems},
  author={Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
  journal={arXiv preprint arXiv:1603.04467},
  year={2016}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@inproceedings{aharoni2020unsupervised,
    title = "Unsupervised Domain Clusters in Pretrained Language Models",
    author = "Aharoni, Roee  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.692",
    doi = "10.18653/v1/2020.acl-main.692",
    pages = "7747--7763",
}

@misc{Gokaslan2019OpenWeb,
    title={Open{W}eb{T}ext Corpus},
    author={Gokaslan*, Aaron and Cohen*, Vanya and Pavlick, Ellie and Tellex, Stefanie},
    year={2019},
    url={http://Skylion007.github.io/OpenWebTextCorpus},
}

@misc{zoph2022stmoe,
      title={ST-MoE: Designing Stable and Transferable Sparse Expert Models}, 
      author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
      year={2022},
      eprint={2202.08906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tay2023ul2,
      title={UL2: Unifying Language Learning Paradigms}, 
      author={Yi Tay and Mostafa Dehghani and Vinh Q. Tran and Xavier Garcia and Jason Wei and Xuezhi Wang and Hyung Won Chung and Siamak Shakeri and Dara Bahri and Tal Schuster and Huaixiu Steven Zheng and Denny Zhou and Neil Houlsby and Donald Metzler},
      year={2023},
      eprint={2205.05131},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{openai2023gpt4,
      title={{GPT}-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      journal={arXiv preprint arxiv:2303.08774},
      url={https://arxiv.org/pdf/2303.08774.pdf},
}

@article{longpre2023flan,
  title={The {F}lan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{movva2022combining,
  title={Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks},
  author={Movva, Rajiv and Lei, Jinhao and Longpre, Shayne and Gupta, Ajay and DuBois, Chris},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={2861--2872},
  year={2022}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford {A}lpaca: An Instruction-following {LL}a{MA} model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@inproceedings{hartvigsen2022toxigen,
  title={ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3309--3326},
  year={2022}
}

@inproceedings{sap-etal-2020-social,
    title = "Social Bias Frames: Reasoning about Social and Power Implications of Language",
    author = "Sap, Maarten  and
      Gabriel, Saadia  and
      Qin, Lianhui  and
      Jurafsky, Dan  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.486",
    doi = "10.18653/v1/2020.acl-main.486",
    pages = "5477--5490",
}

@inproceedings{vidgen-etal-2021-learning,
    title = "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection",
    author = "Vidgen, Bertie  and
      Thrush, Tristan  and
      Waseem, Zeerak  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.132",
    doi = "10.18653/v1/2021.acl-long.132",
    pages = "1667--1682",
}

@article{zhao2023survey,
  title={A Survey of Large Language Models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@misc{nyt2020moderation,
  title={To Apply Machine Learning Responsibly, We Use It in Moderation},
  author={{NYT}, The Open Team},
  url={https://open.nytimes.com/to-apply-machine-learning-responsibly-we-use-it-in-moderation-d001f49e0644},
  year={2020}
}

@article{newamerica2019moderation,
  title={Everything in Moderation: An Analysis of How Internet Platforms Are Using Artificial Intelligence to Moderate User-Generated Content},
  author={Singh, Spandana},
  journal={{New America}},
  url={https://www.newamerica.org/oti/reports/everything-moderation-analysis-how-internet-platforms-are-using-artificial-intelligence-moderate-user-generated-content/},
  year={2019}
}

@misc{anthropic2023claude,
  title={Introducing {C}laude},
  author={{Anthropic AI}},
  url={https://www.anthropic.com/index/introducing-claude},
  year={2023}
}

@misc{cohere2023,
  title={Cohere Command Nightly},
  author={{Cohere AI}},
  url={https://docs.cohere.com/docs/command-beta},
  year={2023}
}

@article{schulman2023,
  title={Reinforcement Learning from Human Feedback: Progress and Challenges},
  author={John Schulman},
  journal={Berkeley EECS},
  url={https://www.youtube.com/watch?v=hhiLw5Q_UFg},
  year={2023}
}

@article{bandy2021addressing,
  title={Addressing ``Documentation Debt'' in machine learning research: A retrospective datasheet for bookcorpus},
  author={Bandy, Jack and Vincent, Nicholas},
  journal={arXiv preprint arXiv:2105.05241},
  year={2021}
}

@misc{biderman2023emergent,
      title={Emergent and Predictable Memorization in Large Language Models}, 
      author={Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raf},
      year={2023},
      eprint={2304.11158},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  journal={arXiv preprint arXiv:2304.01373},
  year={2023}
}

@article{warstadt2020learning,
  title={Learning which features matter: {R}o{BERT}a acquires a preference for linguistic generalizations (eventually)},
  author={Warstadt, Alex and Zhang, Yian and Li, Haau-Sing and Liu, Haokun and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2010.05358},
  year={2020}
}

@inproceedings{sellammultiberts,
  title={The {M}ulti{BERT}s: {BERT} Reproductions for Robustness Analysis},
  author={Sellam, Thibault and Yadlowsky, Steve and Tenney, Ian and Wei, Jason and Saphra, Naomi and D'Amour, Alexander and Linzen, Tal and Bastings, Jasmijn and Turc, Iulia Raluca and Eisenstein, Jacob and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@misc{google2023,
  title={Pa{LM} 2 Technical Report},
  author={Google},
  url={https://ai.google/static/documents/palm2techreport.pdf},
  year={2023}
}

@article{tay2022scaling,
  title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung Won and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh Q and Yogatama, Dani and Metzler, Donald},
  journal={arXiv preprint arXiv:2207.10551},
  year={2022}
}

@inproceedings{pozzobon2023challenges,
  title={On the Challenges of Using Black-Box {API}s for Toxicity Evaluation in Research},
  author={Pozzobon, Luiza Amador and Ermis, Beyza and Lewis, Patrick and Hooker, Sara},
  booktitle={ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models},
  year={2023}
}

@article{xie2023doremi,
  title={{D}o{R}e{M}i: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={arXiv preprint arXiv:2305.10429},
  year={2023}
}

@article{anil2023palm,
  title={PaLM 2 Technical Report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@inproceedings{dathathriplug,
  title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@misc{gebru2021datasheets,
      title={Datasheets for Datasets}, 
      author={Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal Daumé III au2 and Kate Crawford},
      year={2021},
      eprint={1803.09010},
      archivePrefix={arXiv},
      primaryClass={cs.DB}
}


@article{bender2018data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
}

@misc{madaan2022language,
      title={Language Models of Code are Few-Shot Commonsense Learners}, 
      author={Aman Madaan and Shuyan Zhou and Uri Alon and Yiming Yang and Graham Neubig},
      year={2022},
      eprint={2210.07128},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{albalak2023improving,
  title={Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data},
  author={Albalak, Alon and Raffel, Colin and Wang, William Yang},
  journal={arXiv preprint arXiv:2302.00674},
  year={2023}
}

@article{kaddour2023minipile,
  title={The MiniPile Challenge for Data-Efficient Language Models},
  author={Kaddour, Jean},
  journal={arXiv preprint arXiv:2304.08442},
  year={2023}
}

@inproceedings{chung2023unimax,
  title={UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining},
  author={Chung, Hyung Won and Garcia, Xavier and Roberts, Adam and Tay, Yi and Firat, Orhan and Narang, Sharan and Constant, Noah},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}