
\begingroup
\setlength{\tabcolsep}{4pt}
\begin{table*}[ht]
    \centering
    \small
    % \begin{tabular}{>{\raggedright}p{7cm} crll}
    \caption{
    \textbf{Finetuning and Evaluation Parameters for each set of Downstream Tasks.} We report the finetuning hyperparameter settings and evaluation metric used for finetunting and evaluating the pretrained models. We conduct finetuning for four sets of tasks: toxicity identification tasks (Toxigen, Social Bias Frames, and DynaHate), Natural Questions (for pretraining domain transfer analysis), general NLU performance (SuperGLUE), and the Time tasks (including PubCLS, NewSum, PoliAff, TwiERC, and AIC). For T5 Small models, we modify the number of training steps accordingly, as shown in the last row.}
    \begin{tabular}{l | cccc}
    \toprule
    \textsc{Parameter} & \textsc{Tox-Identify} & \textsc{Natural Qs} & \textsc{SuperGLUE} & \textsc{Time} \\
    \midrule
    \multicolumn{5}{c}{\textsc{\bigLM}} \\
    \midrule
    TPUs & 8x8 & 8x8 & 8x8 & 8x8 \\
    Sequence Length & 128 & 512 & 512 & 128 \\
    Batch Size & 128 & 128 & 128 & 128 \\
    Dropout & 0.1 & 0.1 & 0.1 & 0.1 \\
    Training Steps & 10k & 50k & 100k & See \cref{tab:time-hyperparams} \\
    Learning Rate & 1e-3 & 1e-3 & 1e-3 & See \cref{tab:time-hyperparams} \\
    Eval Metric & AUC-ROC & Acc & (By Dataset) & See \cref{tab:time-hyperparams} \\
    \midrule
    \multicolumn{5}{c}{\textsc{\smalLM} \emph{(where different)}} \\
    \midrule
    Training Steps & 30k & 50k & 100k & See \cref{tab:time-hyperparams} \\
    \bottomrule
    \end{tabular}
    \label{tab:finetune-tox-identification-hyperparams}
\end{table*}
\endgroup
