{
  "arxivId": "1901.10159",
  "title": "An Investigation into Neural Net Optimization via Hessian Eigenvalue\n  Density",
  "authors": "Behrooz Ghorbani, Shankar Krishnan, Ying Xiao",
  "abstract": "To understand the dynamics of optimization in deep neural networks, we\ndevelop a tool to study the evolution of the entire Hessian spectrum throughout\nthe optimization process. Using this, we study a number of hypotheses\nconcerning smoothness, curvature, and sharpness in the deep learning\nliterature. We then thoroughly analyze a crucial structural feature of the\nspectra: in non-batch normalized networks, we observe the rapid appearance of\nlarge isolated eigenvalues in the spectrum, along with a surprising\nconcentration of the gradient in the corresponding eigenspaces. In batch\nnormalized networks, these two effects are almost absent. We characterize these\neffects, and explain how they affect optimization speed through both theory and\nexperiments. As part of this work, we adapt advanced tools from numerical\nlinear algebra that allow scalable and accurate estimation of the entire\nHessian spectrum of ImageNet-scale neural networks; this technique may be of\nindependent interest in other applications.",
  "url": "https://arxiv.org/abs/1901.10159",
  "issue_number": 239,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/239",
  "created_at": "2024-12-27T08:37:06.795951",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 102,
  "last_read": "2024-12-27T08:37:06.798545",
  "last_visited": "2024-12-24T03:33:51.377Z",
  "main_tex_file": null,
  "published_date": "2019-01-29T08:24:10Z",
  "arxiv_tags": [
    "cs.LG",
    "stat.ML"
  ]
}