\begin{thebibliography}{10}

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In {\em OSDI}, volume~16, pages 265--283, 2016.

\bibitem{adams2018estimating}
Ryan~P Adams, Jeffrey Pennington, Matthew~J Johnson, Jamie Smith, Yaniv Ovadia,
  Brian Patton, and James Saunderson.
\newblock Estimating the spectral density of large implicit matrices.
\newblock {\em arXiv preprint arXiv:1802.03451}, 2018.

\bibitem{bellec2014concentration}
P~Bellec.
\newblock Concentration of quadratic forms under a {B}ernstein moment
  assumption.
\newblock Technical report, Technical report, Ecole Polytechnique, 2014.

\bibitem{demanet2010chebyshev}
Laurent Demanet and Lexing Ying.
\newblock On chebyshev interpolation of analytic functions.
\newblock {\em preprint}, 2010.

\bibitem{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock {\em arXiv preprint arXiv:1703.04933}, 2017.

\bibitem{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock {\em arXiv preprint arXiv:1803.00885}, 2018.

\bibitem{gil2007numerical}
Amparo Gil, Javier Segura, and Nico~M Temme.
\newblock {\em Numerical methods for special functions}, volume~99.
\newblock Siam, 2007.

\bibitem{inceptionaugmentation}
Github.
\newblock Tensorflow models.
\newblock https://github.com/tensorflow/models/blob/master/official/, 2017.

\bibitem{golub2009matrices}
Gene~H Golub and G{\'e}rard Meurant.
\newblock {\em Matrices, moments and quadrature with applications}, volume~30.
\newblock Princeton University Press, 2009.

\bibitem{golub1969calculation}
Gene~H Golub and John~H Welsch.
\newblock Calculation of gauss quadrature rules.
\newblock {\em Mathematics of computation}, 23(106):221--230, 1969.

\bibitem{goodfellow2014qualitatively}
Ian~J Goodfellow, Oriol Vinyals, and Andrew~M Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock {\em arXiv preprint arXiv:1412.6544}, 2014.

\bibitem{gur2018gradient}
Guy Gur-Ari, Daniel~A Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock {\em arXiv preprint arXiv:1812.04754}, 2018.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{jastrzkebski2017three}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd.
\newblock {\em arXiv preprint arXiv:1711.04623}, 2017.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{lanczos1950iteration}
Cornelius Lanczos.
\newblock {\em An iteration method for the solution of the eigenvalue problem
  of linear differential and integral operators}.
\newblock United States Governm. Press Office Los Angeles, CA, 1950.

\bibitem{li2018measuring}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock {\em arXiv preprint arXiv:1804.08838}, 2018.

\bibitem{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6391--6401, 2018.

\bibitem{lin2016approximating}
Lin Lin, Yousef Saad, and Chao Yang.
\newblock Approximating spectral densities of large matrices.
\newblock {\em SIAM review}, 58(1):34--65, 2016.

\bibitem{papyan2018full}
Vardan Papyan.
\newblock The full spectrum of deep net hessians at scale: Dynamics with sample
  size.
\newblock {\em arXiv preprint arXiv:1811.07062}, 2018.

\bibitem{pearlmutter1994fast}
Barak~A Pearlmutter.
\newblock Fast exact multiplication by the hessian.
\newblock {\em Neural computation}, 6(1):147--160, 1994.

\bibitem{sagun2016eigenvalues}
Levent Sagun, Leon Bottou, and Yann LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock {\em arXiv preprint arXiv:1611.07476}, 2016.

\bibitem{sagun2017empirical}
Levent Sagun, Utku Evci, V~Ugur Guney, Yann Dauphin, and Leon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em arXiv preprint arXiv:1706.04454}, 2017.

\bibitem{santurkar2018does}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?(no, it is not about
  internal covariate shift).
\newblock {\em arXiv preprint arXiv:1805.11604}, 2018.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{ubaru2017fast}
Shashanka Ubaru, Jie Chen, and Yousef Saad.
\newblock Fast estimation of tr(f(a)) via stochastic lanczos quadrature.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  38(4):1075--1099, 2017.

\bibitem{wu2017towards}
Lei Wu, Zhanxing Zhu, et~al.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock {\em arXiv preprint arXiv:1706.10239}, 2017.

\bibitem{yao2018hessian}
Zhewei Yao, Amir Gholami, Qi~Lei, Kurt Keutzer, and Michael~W Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock {\em arXiv preprint arXiv:1802.08241}, 2018.

\end{thebibliography}
