---
author:
- Randall Balestriero
- Ahmed Imtiaz Humayun
- Richard G. Baraniuk
bibliography:
- ref.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "**On the Geometry of Deep Learning**"
---





## Introduction

Machine learning has significantly advanced our ability to address a wide range of difficult computational problems and is the engine driving progress in modern artificial intelligence (AI). Today’s machine learning landscape is dominated by *deep (neural) networks*, which are compositions of a large number of simple parameterized linear and nonlinear operators. An all-too-familiar story of the past decade is that of plugging a deep network into an engineering or scientific application as a black box, learning its parameter values using copious training data, and then significantly improving performance over classical task-specific approaches based on erudite practitioner expertise or mathematical elegance.

Despite this exciting empirical progress, however, the precise mechanisms by which deep learning works so well remain relatively poorly understood, adding an air of mystery to the entire field. Ongoing attempts to build a rigorous mathematical framework have been stymied by the fact that, while deep networks are locally simple, they are globally complicated. Hence, they have primarily been studied as “black boxes” and mainly empirically. This approach greatly complicates analysis to understand the both the success and failure modes of deep networks. This approach also greatly complicates deep learning system design, which today proceeds alchemistically rather than from rigorous design principles. And this approach greatly complicates addressing higher level issues like trustworthiness (can we trust a black box?), sustainability (ever-growing computations lead to a growing environmental footprint), and social responsibility (fairness, bias, and beyond).

In this paper, we overview one promising avenue of progress at the mathematical foundation of deep learning: the connection between deep networks and function approximation by *affine splines* (continuous piecewise linear functions in multiple dimensions). In particular, we will overview work over the past decade on understanding certain geometrical properties of a deep network’s affine spline mapping, in particular how it tessellates its input space. As we will see, the affine spline connection and geometrical viewpoint provide a powerful portal through which to view, analyze, and improve the inner workings of a deep network.

There are a host of interesting open mathematical problems in machine learning in general and deep learning in particular that are surprisingly accessible once one gets past the jargon. Indeed, as we will see, the core ideas can be understood by anyone knowing some linear algebra and calculus. Hence, we will pose numerous open questions as they arise in our exposition in the hopes that they entice more mathematicians to join the deep learning community.

The state-of-the-art in deep learning is a rapidly moving target, and so we focus on the bedrock of modern deep networks, so-called feedforward neural networks employing piecewise linear activation functions. While our analysis does not fully cover some very recent methods, most notably transformer networks, the networks we study are employed therein as key building blocks. Moreover, since we focus on the affine spline viewpoint, we will not have the opportunity to discuss other interesting geometric work in deep learning, including tropical geometry and beyond. Finally, to spin a consistent story line, we will focus primarily on work from our group; we will however review several key results developed by others. Our bibliography is concise, and so the interested reader is invited to explore the extensive works cited in the papers we reference.

## Deep Learning

In supervised machine learning, we are given a collection of $n$ *training data* pairs $\left\{ (\bx_i, \by_i) \right\}_{i=1}^n$; $\bx_i$ is termed the *data* and $\by_i$ the *label*. Without loss of generality, we will take $\bx_i\in\R^D,\by_i\in\R^C$ to be column vectors, but in practice they are often tensors.

We seek a *predictor* or *model* $f$ with two basic properties. First, the predictor should *fit* the training data: $f(\bx_i)\approx \by_i$. When the predictor fits (near) perfectly, we say that it has *interpolated* the data. Second, the predictor should *generalize* to unseen data: $f(\bx')\approx \by'$, where $(\bx', \by')$ is *test data* that does not appear in the training set. When we fit the training data but do not generalize, we say that we have *overfit*.

One solves the prediction problem by first designing a parameterized model $f_\Theta$ with parameters $\Theta$ and then *learning* or *training* by optimizing $\Theta$ to make $f_\Theta(\bx_i)$ as close as possible to $\by_i$ on average in terms of some distance or *loss* function ${\cal L}$, which is often called the *training error*.

A *deep network* is a predictor or model constructed from the composition of $L$ intermediate mappings called *layers* $$%\widehat{y}=
f_\Theta(\bx)= \left(f^{(L)}_{\theta^{(L)}} \circ \dots \circ f^{(1)}_{\theta^{(1)}}\right)\!(\bx).
\label{eq:layers}$$ Here $\Theta$ is the collection of parameters from each layer, $\theta^{(\ell)}$, $\ell=1,\dots,L$. We will omit the parameters $\Theta$ or $\theta^{(\ell)}$ from our notation except where they are critical, since they are ever-present in the discussion below.

The $\ell$-th deep network layer $f^{(\ell)}$ takes as input the vector $\bz^{(\ell-1)}$ and outputs the vector $\bz^{(\ell)}$ by combining two simple operations $$\bz^{(\ell)} 
= f^{(\ell)}\left(\bz^{(\ell-1)}\right)
= \sigma \left(\bW^{(\ell)} \bz^{(\ell-1)} + \bb^{(\ell)} \right),
\label{eq:layer}$$ where $\bz^{(0)}=\bx$ and $\bz^{(L)} = \widehat{\by} = f(\bx)$. First the layer applies an *affine transformation* to its input. Second, in a standard abuse of notation, it applies a scalar nonlinear transformation — called the *activation* function $\sigma$ — to each entry in the result. The entries of $\bz^{(\ell)}$ are called the layer-$\ell$ *neurons* or *units*, and the *width* of the layer is the dimensionality of $\bz^{(\ell)}$. When layers of the form () are used in (), deep learners refer to the network as a *multilayer perceptron* (MLP).

The parameters $\theta^{(\ell)}$ of the layer are the elements of the *weight matrix* $\bW^{(\ell)}$ and the *bias vector* $\bb^{(\ell)}$. Special network structures have been developed to reduce the generally quadratic cost of multiplying by the $\bW^{(\ell)}$. One notable class of networks constraints $\bW^{(\ell)}$ to be a circulant matrix, so that $\bW^{(\ell)}\bz^{(\ell)}$ corresponds to a convolution, giving rise to the term *ConvNet* for such models. Even with this simplification, it is common these days to work with networks with billions of parameters.

The most widespread activation function in modern deep networks is the *rectified linear unit* (ReLU) $$\sigma(u) = \max\{ u, 0 \} =: {\sf ReLU}(u).
\label{eq:ReLU}$$ Throughout this paper, we focus on networks that use this activation, although the results hold for any continuous piecewise linear nonlinearity (e.g., absolute value $\sigma(u)=|u|$). Special activations are often employed at the last layer $f^{(L)}$, from the linear activation $\sigma(u)=u$ to the *softmax* that converts a vector to a probability histogram. These activations do not affect our analysis below. It is worth point out, but beyond the scope of this paper, that it is possible to generalize the results we review below to a much larger class of smooth activation functions (e.g., sigmoid gated linear units, Swish activation) by adopting a probabilistic viewpoint .

The term “network” is used in deep learning because compositions of the form () are often depicted as such; see Figure .

<figure id="fig:deepNet">
<p><span class="image placeholder" data-original-image-src="Figs/deepnet4.png" data-original-image-title="" width="1\linewidth">image</span> </p>
<figcaption> A 6-layer deep network. Purple, blue, and yellow nodes represent the input, neurons, and output, respectively. The width of layer 2 is 5, for example. The links between the nodes represent the elements of the weight matrices <span class="math inline">\(\bW^{(\ell)}\)</span>. The sum with the bias <span class="math inline">\(\bb^{(\ell)}\)</span> and subsequent activation <span class="math inline">\(\sigma(\cdot)\)</span> are implicitly performed at each neuron.</figcaption>
</figure>

To learn to fit the training data with a deep network, we tune the parameters $\bW^{(\ell)},\bb^{(\ell)}, \ell=1,\dots,L$ such that, on average, when training datum $\bx_i$ is input to the network, the output $\widehat{\by_i}=f(\bx_i)$ is close to $\by_i$ as measured by some loss function ${\cal L}$. Two loss functions are ubiquitous in deep learning; the first is the classical squared error based on the two-norm $${\cal L}(\Theta) :=
\frac{1}{n}
\sum_{i=1}^n \big\| \by_i - f_\Theta(\bx_i) \big\|_2^2
\label{eq:squared-loss}$$ The other is the cross-entropy which is oft-used in classification tasks.

Standard learning practice is to use some flavor of *gradient (steepest) descent* that iteratively reduces ${\cal L}$ by updating the parameters $\bW^{(\ell)},\bb^{(\ell)}$ by subtracting a small scalar multiple of the partial derivatives of ${\cal L}$ with respect to those parameters.

In practice, since the number of training data pairs $n$ can be enormous, one calculates the gradient of ${\cal L}$ for each iteration using only a subset of data points and labels called a *minibatch*.

Note that even a nice loss function like () will have a multitude of local minima due to the nonlinear activation at each layer coupled with the composition of multiple such layers. Consequently, numerous heuristics have been developed to help navigate to high-performing local minima. In modern deep networks, the number of neurons is usually so enormous that, by suitably optimizing the parameters, one can nearly interpolate the training data. (We often drop the “nearly” below for brevity.) What distinguishes the performance of one deep network architecture from another, then, is what it does away from the training points, i.e., how well it generalizes to unseen data.

Despite neural networks existing in some form for over 80 years, their success was limited in practice until the AI boom of 2012. This sudden growth was enabled by three converging factors: i) going deep with many layers, ii) training on enormous data sets, and iii) new computing architectures based on graphics processing units (GPUs).

The spark that ignited the AI boom was the Imagenet Challenge 2012 where teams competed to best classify a set of input images $\bx$ into one of 1000 categories. The Imagenet training data was about $n=1.3$ million, 150,000-pixel color digital images human-labeled into 1000 classes, such as ‘bird,’ ‘bus,’ ‘sofa.’ 2012 was the first time a deep network won the Challenge; *AlexNet*, a ConvNet with 62 million parameters in five convolutional layers followed by three fully connected layers achieved and accuracy of 60%. Subsequent competitions featured only deep networks, and, by the final competition in 2017, they had reached 81% accuracy, which is arguably better than most humans can achieve.

Deep networks with dozens of layers and millions of parameters are powerful for fitting and mimicking training data, but also inscrutable. Deep networks are created from such simple transformations (e.g., affine transform and thresholding) that it is maddening the composition of several of them so complicates analysis and defies deep understanding. Consequently, deep learning practitioners tend to treat them as black boxes and proceed empirically using an alchemical development process that focuses primarily on the inputs $\bx$ and outputs $f(\bx)$ of the network. To truly understand deep networks we need to be able to see inside the black box as a deep network is learning and predicting. In the sequel, we will discuss one promising line of work in this vein that leverages the fact that deep networks are affine spline mappings.

## Affine Splines

As we will now explain, deep networks are tractable multidimensional extensions of the familiar one-dimensional continuous piecewise linear functions depicted on the left in Figure . When such continuous piecewise functions are fit to training data, we refer to them as *affine splines* for short.

<figure id="fig:1Dspline">
<p><span class="image placeholder" data-original-image-src="Figs/spline1d.pdf" data-original-image-title="" height="19mm">image</span>    <span class="image placeholder" data-original-image-src="Figs/relu1.pdf" data-original-image-title="" height="19mm">image</span></p>
<figcaption> At left, a one-dimensional continuous piecewise linear function that we refer to as an affine spline. At right, the ReLU activation function (<span class="math inline">\(\ref{eq:ReLU}\)</span>) at the heart of many of today’s deep networks.</figcaption>
</figure>

Deep networks implement one particular extension of the affine spline concept to a multidimensional domain and range. As we will see in the next section, a deep network generalizes the intervals of the independent variable over which a piecewise affine function is simply affine (recall Figure ) to an irregular *tessellation* (tiling) of the network’s $D$-dimensional input space into *convex polytopes*. Let $\Omega$ denote the tessellation and $\omega\in\Omega$ an individual tile. (The jargon for the polytope tiles is “linear region” .)

Generalizing the straight lines defining the function on each interval in Figure , a deep network creates an affine transformation on each tile such that the overall collection is continuous. Figure  depicts an example for a toy deep network with a two-dimensional input space; here the tiles are polygons. This all can be written as $$%s(\bx) 
f(\bx) = \sum_{\omega \in \Omega}(\bA_{\omega}\bx + \bc_{\omega}) \mathds{1}_{\{\bx \in \omega\}},
\label{eq:spline1}$$ where the matrix $\bA_{\omega}$ and vector $\bc_{\omega}$ define the affine transformation from tile $\omega$ to the output.

<figure id="fig:CPA2D">
<span class="image placeholder" data-original-image-src="Figs/tiling4.pdf" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption> Input space tessellation <span class="math inline">\(\Omega\)</span> of the two-dimensional input space (below) and affine spline mapping <span class="math inline">\(f(\bx)\)</span> (above) for a toy deep network of depth <span class="math inline">\(L=4\)</span> and width 20. Also depicted is a training data pair <span class="math inline">\((\bx_i,y_i)\)</span> and the prediction <span class="math inline">\(\widehat{y_i}\)</span>.</figcaption>
</figure>

Both the tessellation $\Omega$ and $\bA_{\omega},\bc_{\omega}$ from the affine transformations are functions of the deep network weights $\bW^{(\ell)}$ and biases $\bb^{(\ell)}$. Geometrically, envision Figure  with a cloud of $n$ training data points $(\bx_i,y_i)$;[^1] learning uses optimization to adjust the weights and biases to create a tessellation and affine transformations such that the affine spline predictions $\widehat{y_i}$ come as close as possible to the true labels $y_i$ as measured by the squared error loss (), for example.

## Deep Network Tessellation

As promised, let us now see how a deep network creates its input space tessellation . Without loss of generality, we start with the first layer $f^{(1)}$ whose input is $\bx$ and output is $\bz^{(1)}$. The $k$-th entry in $\bz^{(1)}$ (the value of the $k$-th neuron) is calculated simply as $$z^{(1)}_k= \sigma \left( 
\bw^{(1)}_k {\boldsymbol{\cdot}}\, \bx + b_k^{(1)} 
  \right),
  \label{eq:neuron}$$ where the dot denotes the inner product, $\bw^{(1)}_k$ is the $k$-th row of the weight matrix $\bW^{(1)}$, and $\sigma$ is the ReLU activation function (). The quantity inside the activation function is the equation of a $D-1$-dimensional hyperplane in the input space $\R^D$ that is perpendicular to $\bw^{(1)}_k$ and offset from the origin by $b_k^{(1)}/\| \bw^{(1)}_k \|_2$. This hyperplane bisects the input space into two half-spaces; one where $z^{(1)}_k>0$ and one where $z^{(1)}_k=0$.

The collection of hyperplanes corresponding to each neuron in $\bz^{(1)}$ create a *hyperplane arrangement*. It is precisely the intersections of the half-spaces of the hyperplane arrangement that tessellate the input space into convex polytope tiles (see Figure ).

<figure id="fig:ha">
<span class="image placeholder" data-original-image-src="Figs/ha1.png" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption> A deep network layer tessellates its input space into convex polytopal tiles via a hyperplane arrangement, with each hyperplane corresponding to one neuron at the output of the layer. In this two-dimensional example assuming ReLU activation, the red line indicates the one-dimensional hyperplane corresponding to the <span class="math inline">\(k\)</span>-th neuron in the first layer.</figcaption>
</figure>

The weights and biases of the first layer determine not only the tessellation of the input space but also an affine transformation on each tile to implement (). Explicit formulas for $\bA_\omega,\bc_\omega$ are available in . It should be clear that, since all of the transformations in () are continuous, so must be the affine spline () corresponding to the first layer.

The tessellation corresponding to the composition of two or more layers follows an interesting *subdivision* process akin to a “tessellation of tessellations” . For example, the second layer creates a hyperplane arrangement in its input space, which happens to be the output space of layer one. Thus, these hyperplanes can be pulled back through layer one to its input space by performing the same process as above but on a tile-by-tile basis relative to the layer-one tessellation and its associated affine transforms. The effect on the layer-two hyperplanes is that they are *folded* each time they cross a hyperplane created by layer 1. Careful inspection of the tessellation in Figure  reveals many examples of such hyperplane folding. Similarly, the hyperplanes created by layer three will be folded every time they encounter a hyperplane in the input space from layers one or two.

Much can be said about this folding process, including a formula for the dihedral angle of a folded hyperplane as a function of the network’s weights and biases. However, the formulae for the angles and affine transformations unfortunately become unwieldy for more than two layers. Finding simplifications for these attributes is an interesting open problem as are the connections to other subdivision processes like wavelets and fractals.

The theory of hyperplane arrangements is rich and tells us that, generally speaking, the number of tiles grows rapidly with the number of neurons in each layer. Hence, we can expect even modestly sized deep networks to have an enormous number of tiles in their input space, each with a corresponding affine transformation from input to output space. Importantly, though, the affine transformations are highly coupled because the overall mapping () must remain continuous. This means that the class of functions that can be represented using a deep network is considerably smaller than if the mapping could be uncoupled and/or discontinuous. Understanding what deep learning practitioners call the network’s “implicit bias” remains an important open problem.

## Visualizing the Tessellation

The toy, low-dimensional examples in Figures  and  are useful for building intuition, but how can we gain insight into the tessellation of a deep network with thousands or more of input and output dimensions? One way to proceed is to compute summary statistics about the tessellation, such as how the number of tiles scales as we increase the width or depth of a network (e.g., ); more on this below. An alternative is to gain insight via direct visualization.

*SplineCam* is an exact method for computing and visualizing a deep network’s spline tessellation over a specified low-dimensional region of the input space, typically a bounded two-dimensional planar slice . SplineCam uses an efficient graph data structure to encode the intersections of the hyperplanes (from the various layers) that pass through the slice and then uses a fast heuristic breadth-first search algorithm to identify tiles from the graph. All of the computations besides the search can be vectorized and computed on GPUs to enable the visualization of even industrial-scale deep networks.

Figure  depicts a SplineCam slice along the plane defined by three training images for a 5-layer ConvNet trained to classify between Egyptian and Tabby cat photos. The first thing we notice is the extraordinarily large number of tiles in just this small region of the 4096-dimensional input space. It can be shown that the decision boundary separating Egyptian and Tabby cats corresponds to a single hyperplane in the final layer that is folded extensively from being pulled back through the previous four layers . Photos falling in the lower left of the slice are classified as Tabbies, while photos falling in the lower right are classified as Egyptians. The density of tiles is also not uniform and varies across the input space.

An interesting avenue for future research involves the efficient extension of SplineCam to higher-dimensional slices both for visualization and the computation of summary statistics.

<figure id="fig:cats">
<span class="image placeholder" data-original-image-src="Figs/cats2.pdf" data-original-image-title="" width="0.85\linewidth"></span>
<figcaption> SplineCam visualization of a two-dimensional slice through the affine spline tessellation of the 4096-dimensional input space of a 5-layer ConvNet of average width 160 trained to classify <span class="math inline">\(64 \times 64\)</span> digital photos of cats. The stars denote the three training images that define the plane and the red lines the decision boundaries between the two classes. (Adapted from <span class="citation" data-cites="splinecam"></span>.) </figcaption>
</figure>

The main goal of this paper is to demonstrate the broad range of insights that can be garnered into the inner workings of a deep network through a focused study of the geometry of its input space tessellation. To this end, we now tour five examples relating to deep network approximation, optimization, and data synthesis. But we would be remiss if we did not point to the significant progress that has been made leveraging other important aspects of the spline view of deep learning, such as understanding how affine splines emerge naturally from the regularization typically used in deep network optimization and what types of functions are learned by deep networks .

## The Self-Similar Geometry of the Tessellation

It has been known since the late 1980s that even a two-layer neural network is a *universal approximator*, meaning that, as the number of neurons grows, one can approximate an arbitrary continuous function over a Borel measurable set to arbitrary precision . But, unfortunately, while two-layer networks are easily capable of interpolating a set of training data, in practice they do a poor job generalizing to data outside of the training set. In contrast, deep networks with $L\gg 2$ layers have proved over the past 15 years that they are capable of both interpolating and generalizing well.

Several groups have investigated the connections between a network’s depth and its tessellation’s capacity to better approximate. was the first to quantify the advantage of depth by counting the number of tiles and showing that deep networks create more tiles (and hence are more expressive) than shallow networks.  
Further work has worked to link the self-similar nature of the tessellation to good approximation. Using self-similarity, one can construct new function spaces for which deeper networks provide better approximation rates (see and the references therein). The benefits of depth stem from the fact that the model is able to replicate a part of the function it is trying to approximate in many different places in the input space and with different scalings or orientations. Extending these results, which currently hold only for one-dimensional input and output spaces, to multidimensional signals is an interesting open research avenue.

## Geometry of the Loss Function

Frankly, it seems an apparent miracle that deep network learning even works. Because of the composition of nonlinear layers and the myriad local minima of the loss function, deep network optimization remains an active area of empirical research. Here we look at one analytical angle that exploits the affine spline nature of deep networks.

Over the past decade, a menagerie of different deep network architectures has emerged that innovate in different ways on the basic architecture (), (). A natural question for the practitioner is: Which architecture should be preferred for a given task? Approximation capability does not offer a point of differentiation, because, as we just discussed, as their size (number of parameters) grows, most deep networks attain a universal approximation capability.

Practitioners know that deep networks with *skip connections* $$\bz^{(\ell)} = \sigma\left(\bW^{(\ell)} \bz^{(\ell-1)} + \bb^{(\ell)} \right) + \bz^{(\ell-1)}
\label{eq:skip}$$ such as so-called *ResNets*, are much preferred over ConvNets, because empirically their gradient descent learning converges faster and more stably to a better minimum. In other words, it is not *what* a deep network can approximate that matters, but rather *how it learns* to approximate. Empirical studies have indicated that this is because the so-called *loss landscape* of the loss function ${\cal L}(\Theta)$ navigated by gradient descent as it optimizes the deep network parameters is much smoother for ResNets as compared to ConvNets (see Figure ). However, to date there has been no analytical work in this direction.

<figure id="fig:loss_surface">
<p><span class="image placeholder" data-original-image-src="Figs/loss-tmp1.pdf" data-original-image-title="" width="1\linewidth">image</span> </p>
<figcaption> Loss landscape <span class="math inline">\({\cal L}(\Theta)\)</span> of a ConvNet and ResNet (from <span class="citation" data-cites="li2018visualizing"></span>). Piecewise quadratic loss function.</figcaption>
</figure>

Using the affine spline viewpoint, it is possible to analytically characterize the local properties of the deep network loss landscape and quantitatively compare different deep network architectures. The key is that, for the deep networks under our consideration trained by minimizing the squared error (), the loss landscape ${\cal L}$ as a function of the deep network parameters $\bW^{(\ell)},\bb^{(\ell)}$ is a *continuous piecewise quadratic function* that is amenable to analysis (see Figure ).

The optimization of quadratic loss surfaces is well-understood. In particular, the eccentricity of a quadratic loss landscape is governed by the *singular values* of the Hessian matrix containing the second-order quadratic terms. Less eccentric (more bowl shaped) losses are easier for gradient descent to quickly navigate to the bottom. Similarly, the local eccentricity of a continuous piecewise quadratic loss function and the width of each local minimum basin are governed by the *singular values* of a “local Hessian matrix” that is a function of not only the deep network parameters but also the deep network architecture. This enables us to quantitatively compare different deep network architectures in terms of their singular values.

In particular, we can make a fair, quantitative comparison between the loss landscapes of the ConvNet and ResNet architectures by comparing their singular values. The key finding is that the condition number of a ResNet (the ratio of the largest to smallest singular value) is bounded, while that of the ConvNet is not . This means that the local loss landscape of a ResNet with skip connections is provably better conditioned than that of a ConvNet and thus less erratic, less eccentric, and with local minima that are more accommodating to gradient-based optimization.

Beyond analysis, one interesting future research avenue in this direction is converting this analytical understanding into new optimization algorithms that are more efficient than today’s gradient descent approaches.

## The Geometry of Initialization 

As we just discussed, even for the prosaic squared error loss function (), the loss landscape as a function of the parameters is highly nonconvex with myriad local minima. Since gradient descent basically descends to the bottom of the first basin it can find, where it starts (the initialization) really matters. Over the years, many techniques have been developed to improve the initialization and/or help gradient descent find better minima; here we look at one of them that is particularly geometric in nature.

With *batch normalization*, we modify the definition of the neural computation from () to $$z^{(1)}_k=
    \sigma\left(
    \frac{\bw^{(1)}_k {\boldsymbol{\cdot}}\, \bx 
    -\mu^{(1)}_k} {\nu^{(1)}_k}
    \right),
    \label{eq:batch normalization}$$ where $\mu^{(1)}_k$ and $\nu^{(1)}_k$ are not learned by gradient descent but instead are directly computed as the mean and standard deviation of $\bw^{(1)}_k \boldsymbol{\cdot}\, \bx_i$ over the training data inputs involved in each gradient step in the optimization. Importantly, this includes the very first step, and so batch normalization directly impacts the initialization from which we start iterating on the loss landscape.[^2]

Astute readers might see a connection to the standard data preprocessing step of data normalization and centering; the main difference is that this processing is performed before each and every gradient learning step. Batch normalization often greatly aids the optimization of a wide variety of deep networks, helping it to find a better (lower) minimum quicker. But the reasons for its efficacy are poorly understood.

We can make progress on understanding batch normalization by again leaning on the affine spline viewpoint. Let’s focus on the effect of batch normalization at initialization just before gradient learning begins; the effect is pronounced and it is then easy to extrapolate regarding what happens at subsequent gradient steps. Prior to learning, a deep network’s weights are initialized with random numbers. This means that the initial hyperplane arrangement is also random.

The key finding of is that batch normalization adapts the geometry of a deep network’s spline tessellation to focus the network’s attention on the training data $\bx_i$. It does this by adjusting the angles and offsets of the hyperplanes that form the boundaries of the polytopal tiles to increase their density in regions of the input space inhabited by the training data, thereby enabling finer approximation there.

More precisely, batch normalization directly adapts each layer’s input space tessellation to minimize the total least squares distance between the tile boundaries and the training data. The resulting data-adaptive initialization aligns the spline tessellation with the data not just at initialization but before every gradient step to give the learning algorithm a much better chance of finding a quality set of weights and biases. See Figure  for a visualization.

<figure id="fig:BN1">
<p><span class="image placeholder" data-original-image-src="Figs/2d_partition_before_3_2.pdf" data-original-image-title="" width="\linewidth">image</span><br />
<span>without batch norm</span></p>
<p><span class="image placeholder" data-original-image-src="Figs/2d_partition_after_3_2.pdf" data-original-image-title="" width="\linewidth">image</span><br />
<span>with batch norm</span></p>
<figcaption> Visualization of a set of two-dimensional data points <span class="math inline">\(\bx_i\)</span> (black dots) and the input-space spline tessellation of a 4-layer toy deep network with random weights <span class="math inline">\(\bW^{(\ell)}\)</span>. The grey lines correspond to (folded) hyperplanes from the first three layers. The blue lines correspond to folded hyperplanes from the fourth layer. (Adapted from <span class="citation" data-cites="BN-arxiv"></span>.) </figcaption>
</figure>

Figure  provides clear evidence of batch normalization’s adaptive prowess. We initialize an 11-layer deep network with a two-dimensional input space three different ways to train on data with a star-shaped distribution. We plot the density of the hyperplanes (basically, the number of hyperplanes passing through local regions of the input space) created by layers 3, 7, and 11 for three different layer configurations: i) the standard layer () with bias $\bb^{(\ell)}={\bf 0}$; ii) the standard layer () with random bias $\bb^{(\ell)}$; iii) the batch normalization layer (). In all three cases, the weights $\bW^{(\ell)}$ were initialized randomly. We can make several observations. First, constraining the bias to be zero forces the network into a central hyperplane arrangement tessellation that is not amenable to aligning with the data. Second, randomizing both the weights and biases splays the tiles over the entire input space, including many places where the training data is not. Third, batch normalization focuses the hyperplanes from all three of the layers onto the regions where the training data lives.

<figure id="fig:BN2">
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><span>data</span></td>
<td style="text-align: center;"><span><span class="math inline">\(\ell=3\)</span></span></td>
<td style="text-align: center;"><span><span class="math inline">\(\ell=7\)</span></span></td>
<td style="text-align: center;"><span><span class="math inline">\(\ell=11\)</span></span></td>
</tr>
<tr class="even">
<td style="text-align: right;"></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/x_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/zero_3_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/zero_7_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/zero_11_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/x_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/random_3_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/random_7_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/random_11_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/x_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/bn_3_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/bn_7_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/bn_11_1024_5.pdf" data-original-image-title="" width="0.23\linewidth">image</span></td>
</tr>
</tbody>
</table>
<figcaption> Densities of the hyperplanes created by layers 3, 7, and 11 in the two-dimensional input space of an 11-layer deep network of width 1024. The training data consists of 50 samples from a star-shaped distribution. (Adapted from <span class="citation" data-cites="BN-arxiv"></span>.) </figcaption>
</figure>

One interesting avenue for future research in this direction is developing new normalization schemes that replace the total least squares optimization to enforce a specific kind of adaptivity of the tessellation to the data and task at hand.

## The Dynamic Geometry of Learning

Current deep learning practice treats a deep network as a black box and optimizes its internal parameters (weights and biases) to minimize some end-to-end training error like the squared loss in (). While this approach has proved mightily successful empirically, it provides no insight into how learning is going on inside the network nor how to improve it. Clearly, as we adjust the parameters to decrease the loss function using gradient descent, the tessellation will change dynamically. Can we use this insight to learn something new about what goes on inside a deep network during learning?

Consider a deep network learning to classify photos of handwritten digits 0–9. Figure  deploys SplineCam to visualize a portion of a 2D slice of the input space of the network defined by three data points in the MNIST handwritten digit training dataset . At left, we see that the tessellation at initialization (before we start learning) is in disarray due to the random weights and biases and nonuse of batch normalization (more on this later). The tessellation is random, and the training error is large.

In the middle, we see the tessellation after convergence to near-zero training error, when most of the digits are on the correct side of their respective decision boundaries. Not shown by the figure is the fact that the network also generalizes well to unseen test data at this juncture. High density suggests that even a continuous piecewise affine function can be quite rugged around these points . Indeed, the false coloring indicates that the 2-norms of the $\bA_\omega$ matrices has increased around the training images, meaning that their “slopes” have increased. As a consequence, the overall spline mapping $f(\bx)$ is now likely more rugged and more sensitive to changes in the input $\bx$ as measured by a local (per-tile) Lipschitz constant. In summary, at (near) interpolation, the gradient learning iterations have in some sense accomplished their task (near zero training error) but with elevated sensitivity of $f(\bx)$ to changes in $\bx$ around the training data points as compared to the random initialization.

<figure id="fig:grok">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/splinecam_s0.jpg" data-original-image-title="" width="0.33\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/splinecam_s2668.png" data-original-image-title="" width="0.33\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="Figs/splinecam_s99383.png" data-original-image-title="" width="0.33\linewidth">image</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span><span class="math inline">\({\sl LC}=4.91\)</span></span></td>
<td style="text-align: center;"><span><span class="math inline">\({\sl LC}=2.96\)</span></span></td>
<td style="text-align: center;"><span><span class="math inline">\({\sl LC}=0.142\)</span></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span>initialization</span></td>
<td style="text-align: center;"><span>interpolation</span></td>
<td style="text-align: center;"><span>grokking</span></td>
</tr>
</tbody>
</table>
<figcaption> SplineCam visualization of a slice of the input space defined by three MNIST digits being classified by a 4-layer MLP of width 200. The false color map (vivirdis) encodes the 2-norm of the <span class="math inline">\(\bA_\omega\)</span> matrix defined on each tile according to purple (low), green (medium), yellow (high). The decision boundary is depicted in red. (Adapted from <span class="citation" data-cites="alwaysgrok"></span>.) </figcaption>
</figure>

Interpolation is the point that would typically be recommended to stop training and fix the network for use in an application. But let’s see what happens if we continue training about 37 times longer. At right in Figure , we see that, while the training error has not changed after continued training (it is still near zero, meaning correct classification of nearly all the training data), the tessellation has metamorphosed. There are now only half as many tiles in this region, and they have all migrated to define the decision boundary, where presumably they are being used to create sharp decisions. Around the training data, we now have a very low density of tiles with low 2-norm of their $\bA_\omega$ matrices, and thus presumably a much smoother mapping $f(\bx)$. Hence, the sensitivity of $f(\bx)$ as measured by a local Lipshitz constant will be much lower than just after interpolation.

We designate this state of affairs *delayed robustness*; it is one facet of the general phenomenon of *grokking* that has been discovered only recently . A dirty secret of deep networks is that $f(\bx)$ can be quite unstable to small changes in $\bx$ (which seems expected given the high degree of nonlinearity). This instability makes deep networks less robust and more prone to attacks like causing a ‘barn’ image to be classified as a ‘pig’ by adding a nearly undetectable but carefully designed attack signal to the picture of a barn. Continuing learning to achieve grokking and delayed robustness is a new approach to mitigating such attacks in particular and making deep learning more stable and predictable in general.

Can we translate the visualization of Figure  into a metric that can be put into practice to compare or improve deep networks? This is an open research question, but here are some first steps . Define the *local complexity* (LC) as the number of tiles in a neighborhood $V$ around a point $\bx$ in the input space. While exact computation of the LC is combinatorially complex, an upper bound can be obtained in terms of the number of hyperplanes that intersect $V$ according to Zaslavsky’s Theorem, with the assumption that $V$ is small enough that the hyperplanes are not folded inside $V$. Therefore, we can use the number of hyperplanes intersecting $V$ as a proxy for the number of tiles in $V$.

For the experiment reported in Figure , we computed the LC in the neighborhood of each training data point in the entire training dataset and then averaged those values. From the above discussion, high LC around a point $\bx$ in the input space implies small, dense tiles in that region and a potentially unsmooth and unstable mapping $f(\bx)$ around $\bx$. The values reported in Figure  confirm that the LC does indeed capture the intuition that we garnered visually. One interesting potential application of the LC is as a new *progress measure* that serves as a proxy for a deep network’s expressivity; LC is task-agnostic yet informative of the training dynamics.

Open research questions regarding the dynamics of deep network learning abound. At a high level, it is clear from Figure  that the classification function being learned has its curvature concentrated at the decision boundary. Approximation theory would suggest that a free-form spline should indeed concentrate its tiles around the decision boundary to minimize the approximation error. However, it is not clear why that migration occurs so late in the training process.

Another interesting research direction is the interplay between grokking and batch normalization, which we discussed above. Batch normalization provably concentrates the tessellation near the data samples, but to grok we need the tiles to move away from the samples. Hence, it is clear that batch normalization and grokking compete with each other. How to get the best of both worlds at both ends of the gradient learning timeline is an open question.

## The Geometry of Generative Models

A generative model aims to learn the underlying patterns in the training data in order to generate new, similar data. The current crop of deep generative models includes transformer networks that power large language models for text synthesis and chatbots and diffusion models for image synthesis. Here we investigate the geometry of models that until recently were state-of-the-art, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) that are often based on ReLU and other piecewise linear activation functions.

Deep generative models map from typically a low-dimensional Euclidean input space (called the parameter space) to a *manifold* ${\cal M}$ of roughly the same dimension in a high-dimensional output space. Each point $\bx$ in the parameter space synthesizes a corresponding output point $\widehat{\by}= f(\bx)$ on the manifold (e.g., a picture of a bedroom). Training on a large number of images $\by_i$ learns an approximation to the mapping $f$ from the parameter space to the manifold. It is beyond the scope of this review, but learning the parameters of a deep generative model is usually more involved than simple gradient descent . It is useful for both training and synthesis to view the points $\bx$ from the parameter space as governed by some probability distribution, e.g., uniform over a bounded region of the input space.

In the case of a GAN based on ReLU or similar activation functions, the manifold ${\cal M}$ is a *continuous piecewise affine manifold*;[^3] see Figure . Points on the manifold are given by () as $\bx$ sweeps through the input space.

<figure id="fig:manifold">
<p><span class="image placeholder" data-original-image-src="Figs/manifold4.png" data-original-image-title="" width="1\linewidth">image</span> </p>
<figcaption> A ReLU-based deep generative network manifold <span class="math inline">\({\cal M}\)</span> is continuous and piecewise affine. Each affine spline tile <span class="math inline">\(\omega\)</span> in the input space is mapped by an affine transformation to a corresponding tile <span class="math inline">\(M(\omega)\)</span> on the manifold.</figcaption>
</figure>

A major issue with deep generative models is that, if the training data is not carefully sourced and currated, then they can produce biased outputs. A deep generative model like a GAN or VAE is trained to approximate both the structure of the true data manifold from which the training data was sampled and the data distribution on that manifold. However, all too often in practice, training data are obtained based on preferences, costs, or convenience factors that produce artifacts in the training data distribution on the manifold. Indeed, it is common in practice for there to be more training data points in one part of the manifold than another. For example, a large fraction of the faces in the CelebA dataset are smiling, and a large fraction of those in the FFHQ dataset are female with dark hair. When one samples uniformly from a model trained with such biased data, the biases will be reproduced when sampling from the trained model, which has far-reaching implications for algorithmic fairness and beyond.

We can both understand and ameliorate sampling biases in deep generative models by again leveraging their affine spline nature. The key insight for the bias issue is that the tessellation of the input space is carried over onto the manifold. Each convex tile $\omega$ in the input space is mapped to a convex tile $M(\omega)$ on the manifold using the affine transform $$M(\omega) = \{\bA_{\omega} \bx + \bc_{\omega}, \: \bx \in \omega \},
    %\subseteq \mathbb{R}^{D},
    \label{eq:region_mapping}$$ and the manifold ${\cal M}$ is the union of the $M(\omega)$. This straightforward construction enables us to analytically characterize many properties of ${\cal M}$ via ().

In particular, it is easy to show that the mapping () from the input space to the manifold *warps* the tiles in the input space tessellation by $\bA_\omega$, causing their volume to expand or contract by $$\frac{{\sf vol}(M(\omega))}{{\sf vol}(\omega)} = \sqrt{\det(\bA_{\omega}^\top \bA_{\omega})}.
    \label{eq:vol}$$ Knowing this, we can take any trained and fixed generative model and determine a *nonuniform sampling* of the input space according to () such that the sampling on the manifold is provably uniform and free from bias. The bonus is that this procedure, which we call MAximum entropy Generative NETwork (MaGNET) , is simply a post-processing procedure that does not require any retraining of the network.

Figure  demonstrates MaGNET’s debiasing abilities. On the left are 18 faces synthesized by the StyleGAN2 generative model trained on the FFHQ face dataset. On the right are 18 faces synthesized by the same StyleGAN2 generative model but using a nonuniform sampling distribution on the input space based on (). MaGNET sampling yields a better gender, age, and hair color balance as well as more diverse backgrounds and accessories. In fact, MaGNET sampling produces 41% more male faces (as determined by the Microsoft Cognitive API) to balance out the gender distribution.

<figure id="fig:magnet">
<p><span class="image placeholder" data-original-image-src="Figs/magnet_vanilla2_18.png" data-original-image-title="" width="\linewidth">image</span><br />
<span>StyleGAN2</span></p>
<p><span class="image placeholder" data-original-image-src="Figs/magnet_stylespace1_18.png" data-original-image-title="" width="\linewidth">image</span><br />
<span>MaGNET-StyleGAN2</span></p>
<figcaption> Images synthesized by sampling uniformly from the input space of a StyleGAN2 deep generative model trained on the FFHQ face data set and nonuniformly according to (<span class="math inline">\(\ref{eq:vol}\)</span>) using MaGNET. (Adapted from <span class="citation" data-cites="magnet"></span>.) </figcaption>
</figure>

We can turn the volumetric deformation () into a tool to efficiently explore the data distribution on a deep generative model’s manifold. By following the MaGNET sampling approach but using an input sampling distribution based on $\det(\bA_{\omega}^\top \bA_{\omega})^\rho$ we can synthesize images in the *modes* (high probability regions of the manifold that are more “typical and high quality”) using ${\rho} \to -\infty$ and or in the *anti-modes* (low probability regions of the manifold that are more “diverse and exploratory”) using ${\rho} \to \infty$ . Setting $\rho=0$ returns the model to uniform sampling.

Like MaGNET, this *polarity sampling* approach applies to any pre-trained generative network and so has broad applicability. See Figure  for an illustrative toy example and for numerous examples with large-scale generative models, including using polarity sampling to boost the performance of existing generative models to state-of-the-art.

<figure id="fig:polarity">
<p><span class="image placeholder" data-original-image-src="Figs/2d_examples.png" data-original-image-title="" width="1\linewidth">image</span> </p>
<figcaption> Polarity-guided synthesis of points in the plane by a Wasserstein GAN generative model. When the polarity parameter <span class="math inline">\(\rho=0\)</span>, the model produces a data distribution closely resembling the training data. When the polarity parameter <span class="math inline">\(\rho\ll 0\)</span> (<span class="math inline">\(\rho\gg 0\)</span>), the WGAN produces a data distribution focusing on the modes (anti-modes), the high (low) probability regions of the training data. (From <span class="citation" data-cites="humayun2022polarity"></span>.)</figcaption>
</figure>

There are many interesting open research questions around affine splines and deep generative networks. One related to the MaGNET sampling strategy is that it assumes that the trained generative network actually learned a good enough approximation of the true underlying data manifold. One could envision exploring how MaGNET could be used to test such an assumption.

## Discussion and Outlook

While there are several ways to envision extending the concept of a one-dimensional affine spline (recall Figure ) to high-dimensional functions and operators, progress has been made only along the direction of forcing the tessellation of the domain to hew to some kind of grid (e.g., uniform or multiscale uniform for spline wavelets). Such constructions are ill-suited for machine learning problems in high dimensions due to the so-called curse of dimensionality that renders approximation intractable.

We can view deep networks as a tractable mechanism for emulating those most powerful of splines, the free-knot splines (splines like those in Figure  where the intervals partitioning the real line are arbitrary) in high dimensions. A deep network uses the power of a hyperplane arrangement to tractably create a myriad of flexible convex polytopal tiles that tessellate its input space plus affine transformations on each that result in quite powerful approximation capabilities in theory and in practice. There is much work to do in studying these approximations (e.g., developing realistic function approximation classes and proving approximation rates) as well as developing new deep network architectures that attain improved rates and robustness.

An additional timely research direction involves extending the ideas discussed here to deep networks like *transformers* that employ at least some nonlinearities that are not piecewise linear. The promising news is that the bulk of the learnable parameters in state-of-the-art transformers lie in readily analyzable affine spline layers within each transformer block of the network. Hence, we can apply many of the above ideas, including local complexity (LC) estimation, to study the smoothness, expressivity, and sensitivity characteristics of even monstrously large language models like the GPT, Gemini, and Llama series.

We hope that we have convinced you that viewing deep networks as affine splines provides a powerful geomeric toolbox to better understand how they learn, how they operate, and how they can be improved in a principled fashion. But splines are just one interesting research direction in the mathematics of deep learning. These are early days, and there are many more open than closed research questions.

## Acknowledgments

Thanks to T. Mitchell Roddenberry and Ali Siahkoohi for their comments on the manuscript. AIH and RGB were supported by NSF grants CCF-1911094, IIS-1838177, and IIS-1730574; ONR grants N00014-18-1-2571, N00014-20-1-2534, N00014-23-1-2714, and MURI N00014-20-1-2787; AFOSR grant FA9550-22-1-0060; DOI grant 140D0423C0076; and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047.

[^1]: We remove the boldface from the labels in this example because they are scalars.

[^2]: As implemented in practice, batch normalization has two additional parameters that are learned as part of the gradient descent; however shows that these parameters have no effect on the optimization initialization and only a limited effect during learning as compared to $\mu$ and $\nu$.

[^3]: We allow ${\cal M}$ to intersect itself transversally in this setting.
