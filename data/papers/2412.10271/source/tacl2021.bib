@article{reif2023visualizing,
  title={Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models},
  author={Reif, Emily and Kahng, Minsuk and Petridis, Savvas},
  journal={arXiv preprint arXiv:2305.11364},
  year={2023}
}

@inproceedings{
Caccia2020Language,
title={Language GANs Falling Short},
author={Massimo Caccia and Lucas Caccia and William Fedus and Hugo Larochelle and Joelle Pineau and Laurent Charlin},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJgza6VtPB}
}

@article{hayati2023far,
  title={How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!},
  author={Hayati, Shirley Anugrah and Lee, Minhwa and Rajagopal, Dheeraj and Kang, Dongyeop},
  journal={arXiv preprint arXiv:2311.09799},
  year={2023}
}


@inproceedings{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{wang2024can,
  title={Can language models solve graph problems in natural language?},
  author={Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{guo2023curious,
  title={The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text},
  author={Guo, Yanzhu and Shang, Guokan and Vazirgiannis, Michalis and Clavel, Chlo{\'e}},
  journal={arXiv preprint arXiv:2311.09807},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@misc{shaib2024standardizing,
      title={Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores}, 
      author={Chantal Shaib and Joe Barrow and Jiuding Sun and Alexa F. Siu and Byron C. Wallace and Ani Nenkova},
      year={2024},
      eprint={2403.00553},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{aulllanguage,
  title={Language Policing to Language Curiosity: Using Corpus Analysis to Foreground Linguistic Diversity},
  author={Aull, Laura},
  journal={Methods and Methodologies for Research in Digital Writing and Rhetoric},
  pages={133},
year={2022}
}

@article{fergadiotis2013measuring,
  title={Measuring lexical diversity in narrative discourse of people with aphasia.},
  author={Fergadiotis, G and Wright, HH and West, TM},
  journal={American Journal of Speech-language Pathology},
  volume={22},
  number={2},
  pages={S397--408},
  year={2013}
}

@article{mcnamara2010linguistic,
  title={Linguistic features of writing quality},
  author={McNamara, Danielle S and Crossley, Scott A and McCarthy, Philip M},
  journal={Written communication},
  volume={27},
  number={1},
  pages={57--86},
  year={2010},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@misc{stasaski2023pragmatically,
      title={Pragmatically Appropriate Diversity for Dialogue Evaluation}, 
      author={Katherine Stasaski and Marti A. Hearst},
      year={2023},
      eprint={2304.02812},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{jarvis2013capturing,
  title={Capturing the diversity in lexical diversity},
  author={Jarvis, Scott},
  journal={Language Learning},
  volume={63},
  pages={87--106},
  year={2013},
  publisher={Wiley Online Library}
}

@article{bestgen2023measuring,
  title={Measuring lexical diversity in texts: The twofold length problem},
  author={Bestgen, Yves},
  journal={Language Learning},
  year={2023},
  publisher={Wiley Online Library}
}


@article{fergadiotis2015psychometric,
  title={Psychometric evaluation of lexical diversity indices: Assessing length effects},
  author={Fergadiotis, Gerasimos and Wright, Heather Harris and Green, Samuel B},
  journal={Journal of Speech, Language, and Hearing Research},
  volume={58},
  number={3},
  pages={840--852},
  year={2015},
  publisher={ASHA}
}

@misc{padmakumar2024does,
      title={Does Writing with Language Models Reduce Content Diversity?}, 
      author={Vishakh Padmakumar and He He},
      year={2024},
      eprint={2309.05196},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
kirk2024understanding,
title={Understanding the Effects of {RLHF} on {LLM} Generalisation and Diversity},
author={Robert Kirk and Ishita Mediratta and Christoforos Nalmpantis and Jelena Luketina and Eric Hambro and Edward Grefenstette and Roberta Raileanu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=PXD3FAVHJT}
}

@misc{groeneveld2024olmo,
      title={OLMo: Accelerating the Science of Language Models}, 
      author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2402.00838},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{templin1957certain,
 ISBN = {9780816671045},
 URL = {http://www.jstor.org/stable/10.5749/j.ctttv2st},
 author = {Mildred C. Templin},
 edition = {NED - New edition},
 publisher = {University of Minnesota Press},
 title = {Certain Language Skills in Children: Their Development and Interrelationships},
 urldate = {2023-07-11},
 volume = {26},
 year = {1957}
}


@book{miller1981assessing,
  title={Assessing Language Production in Children: Experimental Procedures},
  author={Miller, J.F.},
  isbn={9780839115984},
  lccn={80015664},
  series={Assessing communicative behavior},
  url={https://books.google.fr/books?id=1gjbAAAAMAAJ},
  year={1981},
  publisher={University Park Press}
}

@article{richards1987type,
  title={Type/token ratios: What do they really tell us?},
  author={Richards, Brian},
  journal={Journal of child language},
  volume={14},
  number={2},
  pages={201--209},
  year={1987},
  publisher={Cambridge University Press}
}

@article{torruella2013lexical,
  title={Lexical statistics and tipological structures: a measure of lexical richness},
  author={Torruella, Joan and Capsada, Ram{\'o}n},
  journal={Procedia-Social and Behavioral Sciences},
  volume={95},
  pages={447--454},
  year={2013},
  publisher={Elsevier}
}

@article{johnson1944studies,
  title={Studies in language behavior: A program of research},
  author={Johnson, Wendell},
  journal={Psychological Monographs},
  volume={56},
  number={2},
  pages={1--15},
  year={1944}
}

@article{covington2010cutting,
  title={Cutting the Gordian knot: The moving-average type--token ratio (MATTR)},
  author={Covington, Michael A and McFall, Joe D},
  journal={Journal of quantitative linguistics},
  volume={17},
  number={2},
  pages={94--100},
  year={2010},
  publisher={Taylor \& Francis}
}

@article{kyle2021assessing,
  title={Assessing the validity of lexical diversity indices using direct judgements},
  author={Kyle, Kristopher and Crossley, Scott A and Jarvis, Scott},
  journal={Language Assessment Quarterly},
  volume={18},
  number={2},
  pages={154--170},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{zenker2021investigating,
  title={Investigating minimum text lengths for lexical diversity indices},
  author={Zenker, Fred and Kyle, Kristopher},
  journal={Assessing Writing},
  volume={47},
  pages={100505},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{xing2017topic,
  title={Topic aware neural response generation},
  author={Xing, Chen and Wu, Wei and Wu, Yu and Liu, Jie and Huang, Yalou and Zhou, Ming and Ma, Wei-Ying},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  year={2017}
}

@inproceedings{zhu2018texygen,
  title={Texygen: A benchmarking platform for text generation models},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={1097--1100},
  year={2018}
}


@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
alemohammad2023self,
title={Self-Consuming Generative Models Go {MAD}},
author={Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Babaei and Daniel LeJeune and Ali Siahkoohi and Richard Baraniuk},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ShjMHfmPs0}
}

@article{villalobos2022will,
  title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  journal={arXiv preprint arXiv:2211.04325},
  year={2022}
}

@article{OpenAI2023GPT4TR,
  added-at = {2023-07-01T22:03:32.000+0200},
  author = {OpenAI},
  biburl = {https://www.bibsonomy.org/bibtex/2b87062f1a9478148d2e5dd0006c9c455/tomvoelker},
  description = {This paper reports the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers.},
  interhash = {241e35649065841f159e6105eb87b1d3},
  intrahash = {b87062f1a9478148d2e5dd0006c9c455},
  journal = {ArXiv},
  keywords = {machine-learning GPT-4 OpenAI AI deep-learning},
  timestamp = {2023-07-01T23:51:55.000+0200},
  title = {GPT-4 Technical Report},
  volume = {abs/2303.08774},
  year = 2023
}

@inproceedings{ganguli2022predictability,
  title={Predictability and surprise in large generative models},
  author={Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1747--1764},
  year={2022}
}

@article{chang2023survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={arXiv preprint arXiv:2307.03109},
  year={2023}
}

@article{li2022contrastive,
  title={Contrastive decoding: Open-ended text generation as optimization},
  author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2210.15097},
  year={2022}
}

@article{Vijayakumar_Cogswell_Selvaraju_Sun_Lee_Crandall_Batra_2018, title={Diverse Beam Search for Improved Description of Complex Scenes}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/12340}, DOI={10.1609/aaai.v32i1.12340}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Vijayakumar, Ashwin and Cogswell, Michael and Selvaraju, Ramprasaath and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv}, year={2018}, month={Apr.} }

@inproceedings{
Welleck2020Neural,
title={Neural Text Generation With Unlikelihood Training},
author={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJeYe0NtvH}
}

@article{a89efe5d-217a-3260-b2b1-1437ae204234,
 ISSN = {00267902, 15404781},
 URL = {http://www.jstor.org/stable/44980980},
 abstract = {Syntactic and linguistic complexity have been studied extensively in applied linguistics as indicators of linguistic performance, development, and proficiency. Recent publications have equally highlighted the reductionist approach taken to syntactic complexity measurement, which often focuses on one or two measures representing complexity at the level of clause-linking or the sentence, but eschews complexity measurement at other syntactic levels, such as the phrase or the clause. Previous approaches have also rarely incorporated measures representing the diversity of syntactic structures in learner productions. Finally, complexity development has rarely been considered from a cross-linguistic perspective, so that many questions pertaining to the cross-linguistic validity of complexity measurement remain. This article reports on an empirical study on syntactic complexity development and introduces a range of syntactic diversity measures alongside frequently used measures of syntactic elaboration. The study analyzed 100 English and 100 French second language oral narratives from adolescent native speakers of Dutch, situated at 4 proficiency levels (beginner-advanced), as well as native speaker benchmark data from each language. The results reveal a gradual process of syntactic elaboration and syntactic diversification in both learner groups, while, especially in French, considerable differences between learners and native speakers reside in the distribution of specific clause types.},
 author = {Bastien De Clercq and Alex Housen},
 journal = {The Modern Language Journal},
 number = {2},
 pages = {315--334},
 publisher = {[National Federation of Modern Language Teachers Associations, Wiley]},
 title = {A Cross-Linguistic Perspective on Syntactic Complexity in L2 Development: Syntactic Elaboration and Diversity},
 urldate = {2023-11-08},
 volume = {101},
 year = {2017}
}

@article{padmakumar2023does,
  title={Does Writing with Language Models Reduce Content Diversity?},
  author={Padmakumar, Vishakh and He, He},
  journal={arXiv preprint arXiv:2309.05196},
  year={2023}
}

@inproceedings{NIPS2014_5ca3e9b1,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{LI202271,
title = {Data augmentation approaches in natural language processing: A survey},
journal = {AI Open},
volume = {3},
pages = {71-90},
year = {2022},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651022000080},
author = {Bohan Li and Yutai Hou and Wanxiang Che},
keywords = {Machine learning, Data augmentation, Natural language processing},
abstract = {As an effective strategy, data augmentation (DA) alleviates data scarcity scenarios where deep learning techniques may fail. It is widely applied in computer vision then introduced to natural language processing and achieves improvements in many tasks. One of the main focuses of the DA methods is to improve the diversity of training data, thereby helping the model to better generalize to unseen testing data. In this survey, we frame DA methods into three categories based on the diversity of augmented data, including paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods in detail according to the above categories. Further, we also introduce their applications in NLP tasks as well as the challenges. Some useful resources are provided in Appendix A.}
}

@article{dai2023chataug,
  title={Chataug: Leveraging chatgpt for text data augmentation},
  author={Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and Huang, Xiaoke and Wu, Zihao and Zhao, Lin and Liu, Wei and Liu, Ninghao and Li, Sheng and Zhu, Dajiang and others},
  journal={arXiv preprint arXiv:2302.13007},
  year={2023}
}

@article{bonifacio2022inpars,
  title={Inpars: Data augmentation for information retrieval using large language models},
  author={Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  journal={arXiv preprint arXiv:2202.05144},
  year={2022}
}

@article{marwala2023use,
  title={The Use of Synthetic Data to Train AI Models: Opportunities and Risks for Sustainable Development},
  author={Marwala, Tshilidzi and Fournier-Tombs, Eleonore and Stinckwich, Serge},
  journal={arXiv preprint arXiv:2309.00652},
  year={2023}
}

@misc{shumailov2023curse,
      title={The Curse of Recursion: Training on Generated Data Makes Models Forget}, 
      author={Ilia Shumailov and Zakhar Shumaylov and Yiren Zhao and Yarin Gal and Nicolas Papernot and Ross Anderson},
      year={2023},
      eprint={2305.17493},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{uchendu2023does,
  title={Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?},
  author={Uchendu, Adaku and Lee, Jooyoung and Shen, Hua and Le, Thai and Lee, Dongwon and others},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={11},
  pages={163--174},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhou2023multi,
  title={Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization},
  author={Zhou, Weixiao and Li, Gengyao and Cheng, Xianfu and Liang, Xinnian and Zhu, Junnan and Zhai, Feifei and Li, Zhoujun},
  journal={arXiv preprint arXiv:2310.10285},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{NIPS2015_afdec700,
 author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Teaching Machines to Read and Comprehend},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},
 volume = {28},
 year = {2015}
}

@inproceedings{
basu2021mirostat,
title={{\{}MIROSTAT{\}}: A {\{}NEURAL{\}} {\{}TEXT{\}} {\{}DECODING{\}} {\{}ALGORITHM{\}} {\{}THAT{\}} {\{}DIRECTLY{\}} {\{}CONTROLS{\}} {\{}PERPLEXITY{\}}},
author={Sourya Basu and Govardana Sachitanandam Ramachandran and Nitish Shirish Keskar and Lav R. Varshney},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=W1G1JZEIy5_}
}


@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}


@InProceedings{pmlr-v80-yarats18a,
  title = 	 {Hierarchical Text Generation and Planning for Strategic Dialogue},
  author =       {Yarats, Denis and Lewis, Mike},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5591--5599},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/yarats18a/yarats18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/yarats18a.html},
  abstract = 	 {End-to-end models for goal-orientated dialogue are challenging to train, because linguistic and strategic aspects are entangled in latent state vectors. We introduce an approach to learning representations of messages in dialogues by maximizing the likelihood of subsequent sentences and actions, which decouples the semantics of the dialogue utterance from its linguistic realization. We then use these latent sentence representations for hierarchical language generation, planning and reinforcement learning. Experiments show that our approach increases the end-task reward achieved by the model, improves the effectiveness of long-term planning using rollouts, and allows self-play reinforcement learning to improve decision making without diverging from human language. Our hierarchical latent-variable model outperforms previous work both linguistically and strategically.}
}



@inproceedings{
loshchilov2017decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{edwards1998diversity,
  title={Diversity in the lexical and syntactic abilities of fluent aphasic speakers},
  author={Edwards, Susan and Bastiaanse, Roelien},
  journal={Aphasiology},
  volume={12},
  number={2},
  pages={99--117},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{JMLR:v21:18-370,
  author  = {Giannis Siglidis and Giannis Nikolentzos and Stratis Limnios and Christos Giatsidis and Konstantinos Skianis and Michalis Vazirgiannis},
  title   = {GraKeL: A Graph Kernel Library in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {54},
  pages   = {1-5}
}

@article{10.5555/1953048.2078187, author = {Shervashidze, Nino and Schweitzer, Pascal and van Leeuwen, Erik Jan and Mehlhorn, Kurt and Borgwardt, Karsten M.}, title = {Weisfeiler-Lehman Graph Kernels}, year = {2011}, issue_date = {2/1/2011}, publisher = {JMLR.org}, volume = {12}, number = {null}, issn = {1532-4435}, abstract = {In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.}, journal = {J. Mach. Learn. Res.}, month = {nov}, pages = {2539–2561}, numpages = {23} }


@inproceedings{
Holtzman2020The,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}


@article{chakrabarty2023art,
  title={Art or Artifice? Large Language Models and the False Promise of Creativity},
  author={Chakrabarty, Tuhin and Laban, Philippe and Agarwal, Divyansh and Muresan, Smaranda and Wu, Chien-Sheng},
  journal={arXiv preprint arXiv:2309.14556},
  year={2023}
}

@incollection{MCCLOSKEY1989109,
title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor = {Gordon H. Bower},
booktitle={Psychology of learning and motivation},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {24},
pages = {109-165},
year = {1989},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
author = {Michael McCloskey and Neal J. Cohen}
}

@inproceedings{
helwe2021reasoning,
title={Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning},
author={Chadi Helwe and Chlo{\'e} Clavel and Fabian M. Suchanek},
booktitle={3rd Conference on Automated Knowledge Base Construction},
year={2021},
url={https://openreview.net/forum?id=Ozp1WrgtF5_}
}


@misc{huang2023survey,
      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}, 
      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
      year={2023},
      eprint={2311.05232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{
anonymous2024improving,
title={Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning},
author={Anonymous},
booktitle={Submitted to ACL Rolling Review - June 2024},
year={2024},
url={https://openreview.net/forum?id=BvFyvkzs41},
note={under review}
}

@article{geng2024chatgpt,
  title={Is ChatGPT Transforming Academics' Writing Style?},
  author={Geng, Mingmeng and Trotta, Roberto},
  journal={arXiv preprint arXiv:2404.08627},
  year={2024}
}

@inproceedings{kandpal2023large,
  title={Large language models struggle to learn long-tail knowledge},
  author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={15696--15707},
  year={2023},
  organization={PMLR}
}

@inproceedings{
merity2017pointer,
title={Pointer Sentinel Mixture Models},
author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Byj72udxe}
}


@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{dubey2024llama,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@article{zhang1989simple,
  title={Simple fast algorithms for the editing distance between trees and related problems},
  author={Zhang, Kaizhong and Shasha, Dennis},
  journal={SIAM journal on computing},
  volume={18},
  number={6},
  pages={1245--1262},
  year={1989},
  publisher={SIAM}
}

@article{xypolopoulos2024graph,
  title={Graph Linearization Methods for Reasoning on Graphs with Large Language Models},
  author={Xypolopoulos, Christos and Shang, Guokan and Fei, Xiao and Nikolentzos, Giannis and Abdine, Hadi and Evdaimon, Iakovos and Chatzianastasis, Michail and Stamou, Giorgos and Vazirgiannis, Michalis},
  journal={arXiv preprint arXiv:2410.19494},
  year={2024}
}

@article{10.1162/tacl_a_00689,
    author = {Chhun, Cyril and Suchanek, Fabian M. and Clavel, Chloé},
    title = "{Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {1122-1142},
    year = {2024},
    month = {09},
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@misc{ivison2023camelschangingclimateenhancing,
      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, 
      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2311.10702},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.10702}, 
}

@inproceedings{cui2024ultrafeedback,
  title={ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{
rafailov2023direct,
title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=HPuSIXJaa9}
}

@article{levelt1982surface,
  title={Surface form and memory in question answering},
  author={Levelt, Willem JM and Kelter, Stephanie},
  journal={Cognitive psychology},
  volume={14},
  number={1},
  pages={78--106},
  year={1982},
  publisher={Elsevier}
}

@article{healey2014divergence,
  title={Divergence in dialogue},
  author={Healey, Patrick GT and Purver, Matthew and Howes, Christine},
  journal={PloS one},
  volume={9},
  number={6},
  pages={e98598},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@inproceedings{
zheng2023judging,
title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=uccHPGDlao}
}

@article{guo2024large,
  title={Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs},
  author={Guo, Yanzhu and Conia, Simone and Zhou, Zelin and Li, Min and Potdar, Saloni and Xiao, Henry},
  journal={arXiv preprint arXiv:2410.15956},
  year={2024}
}

@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}

@inproceedings{
liang2024mapping,
title={Mapping the Increasing Use of {LLM}s in Scientific Papers},
author={Weixin Liang and Yaohui Zhang and Zhengxuan Wu and Haley Lepp and Wenlong Ji and Xuandong Zhao and Hancheng Cao and Sheng Liu and Siyu He and Zhi Huang and Diyi Yang and Christopher Potts and Christopher D Manning and James Y. Zou},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=YX7QnhxESU}
}