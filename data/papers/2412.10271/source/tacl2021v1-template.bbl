\begin{thebibliography}{65}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Aggarwal et~al.(2022)Aggarwal, Sun, and Peng}]{aggarwal-etal-2022-towards}
Arshiya Aggarwal, Jiao Sun, and Nanyun Peng. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.findings-emnlp.445} {Towards robust {NLG} bias evaluation with syntactically-diverse prompts}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 6022--6032, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023.
\newblock The falcon series of open language models.
\newblock \emph{arXiv preprint arXiv:2311.16867}.

\bibitem[{Bestgen(2023)}]{bestgen2023measuring}
Yves Bestgen. 2023.
\newblock Measuring lexical diversity in texts: The twofold length problem.
\newblock \emph{Language Learning}.

\bibitem[{Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and Tamchyna}]{bojar-etal-2014-findings}
Ond{\v{r}}ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale{\v{s}} Tamchyna. 2014.
\newblock \href {https://doi.org/10.3115/v1/W14-3302} {Findings of the 2014 workshop on statistical machine translation}.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine Translation}, pages 12--58, Baltimore, Maryland, USA. Association for Computational Linguistics.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:1877--1901.

\bibitem[{Caccia et~al.(2020)Caccia, Caccia, Fedus, Larochelle, Pineau, and Charlin}]{Caccia2020Language}
Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2020.
\newblock \href {https://openreview.net/forum?id=BJgza6VtPB} {Language gans falling short}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Chakrabarty et~al.(2022)Chakrabarty, Padmakumar, and He}]{chakrabarty-etal-2022-help}
Tuhin Chakrabarty, Vishakh Padmakumar, and He~He. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.emnlp-main.460} {Help me write a poem - instruction tuning as a vehicle for collaborative poetry writing}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 6848--6863, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Chhun et~al.(2024)Chhun, Suchanek, and Clavel}]{10.1162/tacl_a_00689}
Cyril Chhun, Fabian~M. Suchanek, and Chlo√© Clavel. 2024.
\newblock {Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:1122--1142.

\bibitem[{Clercq and Housen(2017)}]{a89efe5d-217a-3260-b2b1-1437ae204234}
Bastien~De Clercq and Alex Housen. 2017.
\newblock \href {http://www.jstor.org/stable/44980980} {A cross-linguistic perspective on syntactic complexity in l2 development: Syntactic elaboration and diversity}.
\newblock \emph{The Modern Language Journal}, 101(2):315--334.

\bibitem[{Cui et~al.(2024)Cui, Yuan, Ding, Yao, He, Zhu, Ni, Xie, Xie, Lin et~al.}]{cui2024ultrafeedback}
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et~al. 2024.
\newblock Ultrafeedback: Boosting language models with scaled ai feedback.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}.

\bibitem[{Dubuisson~Duplessis et~al.(2017)Dubuisson~Duplessis, Clavel, and Landragin}]{dubuisson-duplessis-etal-2017-automatic}
Guillaume Dubuisson~Duplessis, Chlo{\'e} Clavel, and Fr{\'e}d{\'e}ric Landragin. 2017.
\newblock \href {https://doi.org/10.18653/v1/W17-5510} {Automatic measures to characterise verbal alignment in human-agent interaction}.
\newblock In \emph{Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue}, pages 71--81, Saarbr{\"u}cken, Germany. Association for Computational Linguistics.

\bibitem[{Edwards and Bastiaanse(1998)}]{edwards1998diversity}
Susan Edwards and Roelien Bastiaanse. 1998.
\newblock Diversity in the lexical and syntactic abilities of fluent aphasic speakers.
\newblock \emph{Aphasiology}, 12(2):99--117.

\bibitem[{Fan et~al.(2018)Fan, Lewis, and Dauphin}]{fan-etal-2018-hierarchical}
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
\newblock \href {https://doi.org/10.18653/v1/P18-1082} {Hierarchical neural story generation}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 889--898, Melbourne, Australia. Association for Computational Linguistics.

\bibitem[{Fergadiotis et~al.(2013)Fergadiotis, Wright, and West}]{fergadiotis2013measuring}
G~Fergadiotis, HH~Wright, and TM~West. 2013.
\newblock Measuring lexical diversity in narrative discourse of people with aphasia.
\newblock \emph{American Journal of Speech-language Pathology}, 22(2):S397--408.

\bibitem[{Gallegos et~al.(2024)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed}]{gallegos-etal-2024-bias}
Isabel~O. Gallegos, Ryan~A. Rossi, Joe Barrow, Md~Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen~K. Ahmed. 2024.
\newblock \href {https://doi.org/10.1162/coli_a_00524} {Bias and fairness in large language models: A survey}.
\newblock \emph{Computational Linguistics}, 50(3):1097--1179.

\bibitem[{Giulianelli et~al.(2023)Giulianelli, Baan, Aziz, Fern{\'a}ndez, and Plank}]{giulianelli-etal-2023-comes}
Mario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fern{\'a}ndez, and Barbara Plank. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-main.887} {What comes next? evaluating uncertainty in neural text generators against human production variability}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 14349--14371, Singapore. Association for Computational Linguistics.

\bibitem[{Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu, Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik, Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Strubell, Subramani, Wortsman, Dasigi, Lambert, Richardson, Zettlemoyer, Dodge, Lo, Soldaini, Smith, and Hajishirzi}]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi~Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah~A. Smith, and Hannaneh Hajishirzi. 2024.
\newblock \href {http://arxiv.org/abs/2402.00838} {Olmo: Accelerating the science of language models}.

\bibitem[{Guo et~al.(2024{\natexlab{a}})Guo, Conia, Zhou, Li, Potdar, and Xiao}]{guo2024large}
Yanzhu Guo, Simone Conia, Zelin Zhou, Min Li, Saloni Potdar, and Henry Xiao. 2024{\natexlab{a}}.
\newblock Do large language models have an english accent? evaluating and improving the naturalness of multilingual llms.
\newblock \emph{arXiv preprint arXiv:2410.15956}.

\bibitem[{Guo et~al.(2024{\natexlab{b}})Guo, Shang, Vazirgiannis, and Clavel}]{guo-etal-2024-curious}
Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and Chlo{\'e} Clavel. 2024{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2024.findings-naacl.228} {The curious decline of linguistic diversity: Training language models on synthetic text}.
\newblock In \emph{Findings of the Association for Computational Linguistics: NAACL 2024}, pages 3589--3604, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[{Han et~al.(2022)Han, Kim, and Chang}]{han-etal-2022-measuring}
Seungju Han, Beomsu Kim, and Buru Chang. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.findings-emnlp.66} {Measuring and improving semantic diversity of dialogue generation}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 934--950, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Hasan et~al.(2021)Hasan, Bhattacharjee, Islam, Mubasshir, Li, Kang, Rahman, and Shahriyar}]{hasan-etal-2021-xl}
Tahmid Hasan, Abhik Bhattacharjee, Md.~Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M.~Sohel Rahman, and Rifat Shahriyar. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.findings-acl.413} {{XL}-sum: Large-scale multilingual abstractive summarization for 44 languages}.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pages 4693--4703, Online. Association for Computational Linguistics.

\bibitem[{Hayati et~al.(2023)Hayati, Lee, Rajagopal, and Kang}]{hayati2023far}
Shirley~Anugrah Hayati, Minhwa Lee, Dheeraj Rajagopal, and Dongyeop Kang. 2023.
\newblock How far can we extract diverse perspectives from large language models? criteria-based diversity prompting!
\newblock \emph{arXiv preprint arXiv:2311.09799}.

\bibitem[{Healey et~al.(2014)Healey, Purver, and Howes}]{healey2014divergence}
Patrick~GT Healey, Matthew Purver, and Christine Howes. 2014.
\newblock Divergence in dialogue.
\newblock \emph{PloS one}, 9(6):e98598.

\bibitem[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and Choi}]{Holtzman2020The}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 2020.
\newblock \href {https://openreview.net/forum?id=rygGQyrFvH} {The curious case of neural text degeneration}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, and Hajishirzi}]{ivison2023camelschangingclimateenhancing}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A. Smith, Iz~Beltagy, and Hannaneh Hajishirzi. 2023.
\newblock \href {http://arxiv.org/abs/2311.10702} {Camels in a changing climate: Enhancing lm adaptation with tulu 2}.

\bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}.

\bibitem[{Johnson(1944)}]{johnson1944studies}
Wendell Johnson. 1944.
\newblock Studies in language behavior: A program of research.
\newblock \emph{Psychological Monographs}, 56(2):1--15.

\bibitem[{Kandpal et~al.(2023)Kandpal, Deng, Roberts, Wallace, and Raffel}]{kandpal2023large}
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023.
\newblock Large language models struggle to learn long-tail knowledge.
\newblock In \emph{International Conference on Machine Learning}, pages 15696--15707. PMLR.

\bibitem[{Kirk et~al.(2024)Kirk, Mediratta, Nalmpantis, Luketina, Hambro, Grefenstette, and Raileanu}]{kirk2024understanding}
Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. 2024.
\newblock \href {https://openreview.net/forum?id=PXD3FAVHJT} {Understanding the effects of {RLHF} on {LLM} generalisation and diversity}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Lahoti et~al.(2023)Lahoti, Blumm, Ma, Kotikalapudi, Potluri, Tan, Srinivasan, Packer, Beirami, Beutel, and Chen}]{lahoti-etal-2023-improving}
Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-main.643} {Improving diversity of demographic representation in large language models via collective-critiques and self-voting}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 10383--10405, Singapore. Association for Computational Linguistics.

\bibitem[{Le~Bronnec et~al.(2024)Le~Bronnec, Verine, Negrevergne, Chevaleyre, and Allauzen}]{le-bronnec-etal-2024-exploring}
Florian Le~Bronnec, Alexandre Verine, Benjamin Negrevergne, Yann Chevaleyre, and Alexandre Allauzen. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.616} {Exploring precision and recall to assess the quality and diversity of {LLM}s}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 11418--11441, Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{Li et~al.(2017)Li, Su, Shen, Li, Cao, and Niu}]{li-etal-2017-dailydialog}
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017.
\newblock \href {https://aclanthology.org/I17-1099} {{D}aily{D}ialog: A manually labelled multi-turn dialogue dataset}.
\newblock In \emph{Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 986--995, Taipei, Taiwan. Asian Federation of Natural Language Processing.

\bibitem[{Liang et~al.(2024)Liang, Zhang, Wu, Lepp, Ji, Zhao, Cao, Liu, He, Huang, Yang, Potts, Manning, and Zou}]{liang2024mapping}
Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher~D Manning, and James~Y. Zou. 2024.
\newblock \href {https://openreview.net/forum?id=YX7QnhxESU} {Mapping the increasing use of {LLM}s in scientific papers}.
\newblock In \emph{First Conference on Language Modeling}.

\bibitem[{Liu et~al.(2023)Liu, Iter, Xu, Wang, Xu, and Zhu}]{liu-etal-2023-g}
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-main.153} {{G}-eval: {NLG} evaluation using gpt-4 with better human alignment}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 2511--2522, Singapore. Association for Computational Linguistics.

\bibitem[{Luo et~al.(2024)Luo, Cherry, and Foster}]{luo-etal-2024-diverge}
Jiaming Luo, Colin Cherry, and George Foster. 2024.
\newblock \href {https://doi.org/10.1162/tacl_a_00645} {To diverge or not to diverge: A morphosyntactic perspective on machine translation vs human translation}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:355--371.

\bibitem[{Maynez et~al.(2020)Maynez, Narayan, Bohnet, and McDonald}]{maynez-etal-2020-faithfulness}
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.173} {On faithfulness and factuality in abstractive summarization}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 1906--1919, Online. Association for Computational Linguistics.

\bibitem[{McNamara et~al.(2010)McNamara, Crossley, and McCarthy}]{mcnamara2010linguistic}
Danielle~S McNamara, Scott~A Crossley, and Philip~M McCarthy. 2010.
\newblock Linguistic features of writing quality.
\newblock \emph{Written communication}, 27(1):57--86.

\bibitem[{Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher}]{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017.
\newblock \href {https://openreview.net/forum?id=Byj72udxe} {Pointer sentinel mixture models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Miller(1981)}]{miller1981assessing}
J.F. Miller. 1981.
\newblock \href {https://books.google.fr/books?id=1gjbAAAAMAAJ} {\emph{Assessing Language Production in Children: Experimental Procedures}}.
\newblock Assessing communicative behavior. University Park Press.

\bibitem[{Padmakumar and He(2024)}]{padmakumar2024does}
Vishakh Padmakumar and He~He. 2024.
\newblock \href {http://arxiv.org/abs/2309.05196} {Does writing with language models reduce content diversity?}

\bibitem[{Qi et~al.(2020)Qi, Zhang, Zhang, Bolton, and Manning}]{qi-etal-2020-stanza}
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher~D. Manning. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-demos.14} {{S}tanza: A python natural language processing toolkit for many human languages}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations}, pages 101--108, Online. Association for Computational Linguistics.

\bibitem[{Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn}]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn. 2023.
\newblock \href {https://openreview.net/forum?id=HPuSIXJaa9} {Direct preference optimization: Your language model is secretly a reward model}.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Rei et~al.(2020)Rei, Stewart, Farinha, and Lavie}]{rei-etal-2020-comet}
Ricardo Rei, Craig Stewart, Ana~C Farinha, and Alon Lavie. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.213} {{COMET}: A neural framework for {MT} evaluation}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 2685--2702, Online. Association for Computational Linguistics.

\bibitem[{Reimers and Gurevych(2019)}]{reimers-gurevych-2019-sentence}
Nils Reimers and Iryna Gurevych. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1410} {Sentence-{BERT}: Sentence embeddings using {S}iamese {BERT}-networks}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 3982--3992, Hong Kong, China. Association for Computational Linguistics.

\bibitem[{Reimers and Gurevych(2020)}]{reimers-gurevych-2020-making}
Nils Reimers and Iryna Gurevych. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.365} {Making monolingual sentence embeddings multilingual using knowledge distillation}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 4512--4525, Online. Association for Computational Linguistics.

\bibitem[{Shaib et~al.(2024{\natexlab{a}})Shaib, Barrow, Sun, Siu, Wallace, and Nenkova}]{shaib2024standardizing}
Chantal Shaib, Joe Barrow, Jiuding Sun, Alexa~F. Siu, Byron~C. Wallace, and Ani Nenkova. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2403.00553} {Standardizing the measurement of text diversity: A tool and a comparative analysis of scores}.

\bibitem[{Shaib et~al.(2024{\natexlab{b}})Shaib, Elazar, Li, and Wallace}]{shaib-etal-2024-detection}
Chantal Shaib, Yanai Elazar, Junyi~Jessy Li, and Byron~C Wallace. 2024{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2024.emnlp-main.368} {Detection and measurement of syntactic templates in generated text}.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 6416--6431, Miami, Florida, USA. Association for Computational Linguistics.

\bibitem[{Shervashidze et~al.(2011)Shervashidze, Schweitzer, van Leeuwen, Mehlhorn, and Borgwardt}]{10.5555/1953048.2078187}
Nino Shervashidze, Pascal Schweitzer, Erik~Jan van Leeuwen, Kurt Mehlhorn, and Karsten~M. Borgwardt. 2011.
\newblock Weisfeiler-lehman graph kernels.
\newblock \emph{J. Mach. Learn. Res.}, 12(null):2539‚Äì2561.

\bibitem[{Siglidis et~al.(2020)Siglidis, Nikolentzos, Limnios, Giatsidis, Skianis, and Vazirgiannis}]{JMLR:v21:18-370}
Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Konstantinos Skianis, and Michalis Vazirgiannis. 2020.
\newblock Grakel: A graph kernel library in python.
\newblock \emph{Journal of Machine Learning Research}, 21(54):1--5.

\bibitem[{Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, Hofmann, Jha, Kumar, Lucy, Lyu, Lambert, Magnusson, Morrison, Muennighoff, Naik, Nam, Peters, Ravichander, Richardson, Shen, Strubell, Subramani, Tafjord, Walsh, Zettlemoyer, Smith, Hajishirzi, Beltagy, Groeneveld, Dodge, and Lo}]{soldaini-etal-2024-dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li~Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz~Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.840} {Dolma: an open corpus of three trillion tokens for language model pretraining research}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15725--15788, Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{Stasaski and Hearst(2022)}]{stasaski-hearst-2022-semantic}
Katherine Stasaski and Marti Hearst. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.naacl-main.6} {Semantic diversity in dialogue with natural language inference}.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 85--98, Seattle, United States. Association for Computational Linguistics.

\bibitem[{Team et~al.(2024)Team, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e} et~al.}]{team2024gemma}
Gemma Team, Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al. 2024.
\newblock Gemma 2: Improving open language models at a practical size.
\newblock \emph{arXiv preprint arXiv:2408.00118}.

\bibitem[{Templin(1957)}]{templin1957certain}
Mildred~C. Templin. 1957.
\newblock \href {http://www.jstor.org/stable/10.5749/j.ctttv2st} {\emph{Certain Language Skills in Children: Their Development and Interrelationships}}, ned - new edition edition, volume~26.
\newblock University of Minnesota Press.

\bibitem[{Tevet and Berant(2021)}]{tevet-berant-2021-evaluating}
Guy Tevet and Jonathan Berant. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.eacl-main.25} {Evaluating the evaluation of diversity in natural language generation}.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pages 326--346, Online. Association for Computational Linguistics.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Uchendu et~al.(2023)Uchendu, Lee, Shen, Le, Lee et~al.}]{uchendu2023does}
Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Dongwon Lee, et~al. 2023.
\newblock Does human collaboration enhance the accuracy of identifying llm-generated deepfake texts?
\newblock In \emph{Proceedings of the AAAI Conference on Human Computation and Crowdsourcing}, volume~11, pages 163--174.

\bibitem[{Wang et~al.(2024)Wang, Ma, Zhang, Ni, Chandra, Guo, Ren, Arulraj, He, Jiang et~al.}]{wang2024mmlu}
Yubo Wang, Xueguang Ma, Ge~Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et~al. 2024.
\newblock Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.
\newblock \emph{arXiv preprint arXiv:2406.01574}.

\bibitem[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang et~al.}]{yang2024qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et~al. 2024.
\newblock Qwen2 technical report.
\newblock \emph{arXiv preprint arXiv:2407.10671}.

\bibitem[{Yarats and Lewis(2018)}]{pmlr-v80-yarats18a}
Denis Yarats and Mike Lewis. 2018.
\newblock \href {https://proceedings.mlr.press/v80/yarats18a.html} {Hierarchical text generation and planning for strategic dialogue}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages 5591--5599. PMLR.

\bibitem[{Zhang et~al.(2021)Zhang, Duckworth, Ippolito, and Neelakantan}]{zhang-etal-2021-trading}
Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. 2021.
\newblock \href {https://aclanthology.org/2021.humeval-1.3} {Trading off diversity and quality in natural language generation}.
\newblock In \emph{Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)}, pages 25--33, Online. Association for Computational Linguistics.

\bibitem[{Zhang and Shasha(1989)}]{zhang1989simple}
Kaizhong Zhang and Dennis Shasha. 1989.
\newblock Simple fast algorithms for the editing distance between trees and related problems.
\newblock \emph{SIAM journal on computing}, 18(6):1245--1262.

\bibitem[{Zhang et~al.(2020)Zhang, Kishore, Wu, Weinberger, and Artzi}]{bert-score}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q. Weinberger, and Yoav Artzi. 2020.
\newblock \href {https://openreview.net/forum?id=SkeHuCVFDr} {Bertscore: Evaluating text generation with bert}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Zhang et~al.(2024)Zhang, Lei, Wu, Sun, Huang, Long, Liu, Lei, Tang, and Huang}]{zhang-etal-2024-safetybench}
Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.830} {{S}afety{B}ench: Evaluating the safety of large language models}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15537--15553, Bangkok, Thailand. Association for Computational Linguistics.

\end{thebibliography}
