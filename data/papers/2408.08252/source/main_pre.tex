\vspace{-2mm}
\section{Preliminaries and Goal}
\vspace{-2mm}
We describe the standard method for training diffusion models and outline the objective of our work: optimizing downstream reward functions given pre-trained diffusion models.

\vspace{-2mm}
\subsection{Diffusion Models }\label{sec:diffusion}
\vspace{-2mm}


In diffusion models \citep{ho2020denoising,song2020denoising}, our goal is to learn a sampler $p(x) \in \Delta(\Xcal)$ given data consisting of $x \in \Xcal$. The training process for a standard diffusion model is summarized as follows. First, we introduce a (fixed) forward process $q_t:\Xcal  \to \Delta(\Xcal)$. Next, we aim to learn a backward process: $\{p_t\}$ where each $p_t$ is $\Xcal \to \Delta(\Xcal)$ so that the distributions induced by the forward process and backward process match marginally. For this purpose, by parametrizing the backward processes with $\theta \in \RR^d$, we typically use the following loss function:
\begin{align*} 
 \EE_{x_1,\cdots,x_T\sim q(\cdot|x_0)}\left [-\log p_0(x_0|x_1) + \sum_{t=1}^{T-1}\KL(q_{t}(\cdot \mid x_{t-1},x_0) \| p_{t}(\cdot \mid x_{t+1};\theta)) +  \KL( q_{T}(\cdot) \| p_{T}(\cdot) ) \right],
\end{align*}
which is derived from the variational lower bound of the negative log-likelihood (\textit{i.e.}, ELBO). 

Here are two examples of concrete parameterizations. Let $\alpha_t \in \RR$ be a noise schedule.  

\begin{example}[Continuous space]
When $\Xcal$ is Euclidean space, we typically use the Gaussian distribution $q_t(\cdot\mid x) = \Ncal(\sqrt{ \alpha_t}x, (1- \alpha_t) )$. Then, the backward process is parameterized as 
\begin{align*}
    \Ncal\left (\frac{\sqrt{ \alpha_t}(1- \bar \alpha_{t-1}) x_t +  \sqrt{ \bar \alpha_{t-1}}(1-\alpha_t)\hat x_0(x_t;\theta) }{1 - \bar \alpha_t},\frac{(1-\alpha_t)(1-\bar \alpha_{t-1}) }{1-\bar \alpha_t }  \right),  
\end{align*}
where $\bar \alpha_t= \prod_{i=1}^t \alpha_i $. Here, $\hat{x}_0(x_t; \theta)$ is a neural network that predicts $x_0$ from $x_t$ ($\EE_{q}[x_0|x_t]$). 
\end{example}

\begin{example}[Discrete space in \citet{sahoo2024simple,shi2024simplified}]\label{exa:discrete}
Let $\Xcal$ be a space of one-hot column vectors $\{x\in\{0,1\}^K:\sum_{i=1}^K x_i =1\}$, and $\mathrm{Cat}(\pi)$ be the categorical distribution over $K$ classes with probabilities given by $\pi \in \Delta^K$ where $\Delta^K$ denotes the K-simplex. A typical choice is  
$q_t(\cdot\mid x) = \mathrm{Cat}(\alpha_t x+ (1-\alpha_t)\mathbf{m} )$ where $\mathbf{m}=[0,\cdots,0,\mathrm{Mask}]$. Then, the backward process is parameterized as 
\begin{align*}
   \begin{cases}
   \mathrm{Cat}(x_t ),\quad \mathrm{if}\, x_t\neq \mathbf{m},  \\
       \mathrm{Cat}\left ( \frac{(1-\bar \alpha_{t-1})\mathbf{m}  + (\bar \alpha_{t-1}- \bar \alpha_t) \hat x_0(x_t;\theta) }{ 1 - \bar \alpha_t } \right),\quad \mathrm{if}\,x_t= \mathbf{m}, 
   \end{cases}  
\end{align*}
where $\bar \alpha_t= \prod_{i=1}^t \alpha_i $. Here, $\hat{x}_0(x_t; \theta)$ is a neural network that predicts $x_0$ from $x_t$. Note that when considering a sequence of $L$ tokens ($x^{1:L})$, we use the direct product: $p_t(x^{1:L}_t|x^{1:L}_{t+1}) = \prod_{l=1}^L p_t(x^l_t|x^{1:L}_{t+1})$. 
\end{example}

After learning the backward process, we can sample from a distribution that emulates training data distribution (\textit{i.e.}, $p(x)$) by sequentially sampling $\{p_t\}_{t=T}^0$ from $t=T$ to $t=0$.

\vspace{-1mm}
\paragraph{Notation.} The notation $\delta_{a}$ denotes a Dirac delta distribution centered at $a$. The notation $\propto$ indicates that the distribution is equal up to a normalizing constant. With slight abuse of notation, we often denote $p_T(\cdot|\cdot,\cdot)$ by $p_{T}(\cdot)$. 

\vspace{-2mm}
\subsection{Objective: Generating Samples with High Rewards While Preserving Naturalness}
\vspace{-2mm}

We consider a scenario where we have a pre-trained diffusion model, which is trained using the loss function explained in \pref{sec:diffusion}. These pre-trained models are typically designed to excel at characterizing the natural design space (\textit{e.g.}, image space, biological sequence space, or chemical space) by emulating the extensive training dataset. Our work focuses on obtaining samples that also optimize downstream reward functions $r:\Xcal \to \RR$ (\textit{e.g.}, Quantitative Estimate of Druglikeness (QED) and Synthetic Accessibility (SA) in molecule generation), while maintaining the naturalness by leveraging pre-trained diffusion models. We formalize this goal as follows. 

Given a pre-trained model $\{p^{\pre}_t\}_{t=T}^0$, we denote the induced distribution by $p^{\pre} \in \Delta(\Xcal)$ (\textit{i.e.}, $p^{\pre}(x_0)= \int \{ \prod_{t=T+1}^1 p^{\pre}_{t-1}(x_{t-1}|x_t)\}dx_{1:T}$). We aim to sample from the following distribution: 
\begin{align*}
 p^{(\alpha)}(x)& :=   \argmax_{p\in [\Delta(\Xcal)] }\underbrace{\EE_{x\sim p(\cdot)}[r(x)]}_{\textbf{term (a)}}-\alpha \underbrace{\KL( p(\cdot) \| p^{\pre}(\cdot))}_{\textbf{term(b)}} \propto \exp(r(x)/\alpha)p^{\pre}(x). 
\end{align*}
Here, term (a) is introduced to optimize the reward function, while term (b) is used to maintain the naturalness of the generated samples. 

\vspace{-2mm}
\paragraph{Existing methods.}


Several existing approaches target this goal (or its variant), as discussed in \pref{sec:related_works}, including classifier guidance, fine-tuning (RL-based or classifier-free), and Best-of-N. In our work, we focus on non-fine-tuning-based methods; specifically, we aim to address the limitations of these methods: the requirement for differentiable proxy models in classifier guidance and the inefficiency of Best-of-N.

Finally, we note that all results discussed in this paper can be easily extended to cases where the pre-trained model is a conditional diffusion model. For example, in our image experiments (\pref{sec:experiment}), the pre-trained model is a conditional diffusion model conditioned on text (e.g., Stable Diffusion).




