{
  "arxivId": "2405.17472",
  "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via\n  Selective Tensor Freezing",
  "authors": "Kai Huang, Haoming Wang, Wei Gao",
  "abstract": "Text-to-image diffusion models can be fine-tuned in custom domains to adapt\nto specific user preferences, but such adaptability has also been utilized for\nillegal purposes, such as forging public figures' portraits, duplicating\ncopyrighted artworks and generating explicit contents. Existing work focused on\ndetecting the illegally generated contents, but cannot prevent or mitigate\nillegal adaptations of diffusion models. Other schemes of model unlearning and\nreinitialization, similarly, cannot prevent users from relearning the knowledge\nof illegal model adaptation with custom data. In this paper, we present\nFreezeAsGuard, a new technique that addresses these limitations and enables\nirreversible mitigation of illegal adaptations of diffusion models. Our\napproach is that the model publisher selectively freezes tensors in pre-trained\ndiffusion models that are critical to illegal model adaptations, to mitigate\nthe fine-tuned model's representation power in illegal adaptations, but\nminimize the impact on other legal adaptations. Experiment results in multiple\ntext-to-image application domains show that FreezeAsGuard provides 37% stronger\npower in mitigating illegal model adaptations compared to competitive\nbaselines, while incurring less than 5% impact on legal model adaptations. The\nsource code is available at: https://github.com/pittisl/FreezeAsGuard.",
  "url": "https://arxiv.org/abs/2405.17472",
  "issue_number": 167,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/167",
  "created_at": "2024-12-24T19:44:46.609725",
  "state": "open",
  "labels": [
    "paper",
    "rating:downvote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null
}