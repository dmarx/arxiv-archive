{
  "arxivId": "2412.19302",
  "title": "RecLM: Recommendation Instruction Tuning",
  "authors": "Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang",
  "abstract": "Modern recommender systems aim to deeply understand users' complex\npreferences through their past interactions. While deep collaborative filtering\napproaches using Graph Neural Networks (GNNs) excel at capturing user-item\nrelationships, their effectiveness is limited when handling sparse data or\nzero-shot scenarios, primarily due to constraints in ID-based embedding\nfunctions. To address these challenges, we propose a model-agnostic\nrecommendation instruction-tuning paradigm that seamlessly integrates large\nlanguage models with collaborative filtering. Our proposed Recommendation\nLanguage Model (RecLM) enhances the capture of user preference diversity\nthrough a carefully designed reinforcement learning reward function that\nfacilitates self-augmentation of language models. Comprehensive evaluations\ndemonstrate significant advantages of our approach across various settings, and\nits plug-and-play compatibility with state-of-the-art recommender systems\nresults in notable performance enhancements.",
  "url": "https://arxiv.org/abs/2412.19302",
  "issue_number": 544,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/544",
  "created_at": "2024-12-30T04:40:50.211722",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-30T04:39:04.507Z",
  "main_tex_file": null,
  "published_date": "2024-12-26T17:51:54Z",
  "arxiv_tags": [
    "cs.IR"
  ]
}