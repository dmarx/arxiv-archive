\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos]{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication.
\newblock \emph{arXiv preprint arXiv:2303.09540}, 2023.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[AI@Meta(2024)]{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang, Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock \emph{arXiv preprint arXiv:2402.16827}, 2024.

\bibitem[Allamanis(2019)]{allamanis2019adverse}
Miltiadis Allamanis.
\newblock The adverse effects of code duplication in machine learning models of code.
\newblock In \emph{Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software}, pp.\  143--153, 2019.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: harmlessness from ai feedback. 2022.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2397--2430. PMLR, 2023.

\bibitem[Borodin \& Olshanski(2000)Borodin and Olshanski]{borodin2000distributions}
Alexei Borodin and Grigori Olshanski.
\newblock Distributions on partitions, point processes and the hypergeometric kernel.
\newblock \emph{Communications in Mathematical Physics}, 211:\penalty0 335--358, 2000.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Ralph~Allan Bradley and Milton~E Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brown(2020)]{brown2020language}
Tom~B Brown.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cao et~al.(2023)Cao, Kang, and Sun]{cao2023instruction}
Yihan Cao, Yanbin Kang, and Lichao Sun.
\newblock Instruction mining: High-quality instruction data selection for large language models.
\newblock \emph{arXiv preprint arXiv:2307.06290}, 2023.

\bibitem[Cassano et~al.(2022)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman, et~al.]{cassano2022multipl}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, et~al.
\newblock Multipl-e: A scalable and extensible approach to benchmarking neural code generation.
\newblock \emph{arXiv preprint arXiv:2208.08227}, 2022.

\bibitem[{Cerebras Systems}(2023)]{CerebrasSlimPajama2023}
{Cerebras Systems}.
\newblock Slimpajama: A 627b token cleaned and deduplicated version of redpajama.
\newblock Blog post on Cerebras Systems, 2023.
\newblock \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}.

\bibitem[Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang, et~al.]{chen2023alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al.
\newblock Alpagasus: Training a better alpaca with fewer data.
\newblock \emph{arXiv preprint arXiv:2307.08701}, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code, 2021.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Civril \& Magdon-Ismail(2007)Civril and Magdon-Ismail]{civril2007finding}
Ali Civril and Malik Magdon-Ismail.
\newblock Finding maximum volume sub-matrices of a matrix.
\newblock \emph{RPI Comp Sci Dept TR}, pp.\  07--08, 2007.

\bibitem[{Common Crawl}(2024)]{commoncrawl}
{Common Crawl}.
\newblock {Common Crawl Dataset}.
\newblock \url{https://commoncrawl.org}, 2024.

\bibitem[Conneau \& Lample(2019)Conneau and Lample]{conneau2019cross}
Alexis Conneau and Guillaume Lample.
\newblock Cross-lingual language model pretraining.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, et~al.]{du2022glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5547--5569. PMLR, 2022.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Dubois et~al.(2024)Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin, Liang, and Hashimoto]{dubois2024alpacafarm}
Yann Dubois, Chen~Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy~S Liang, and Tatsunori~B Hashimoto.
\newblock Alpacafarm: A simulation framework for methods that learn from human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Fernandes et~al.(2023)Fernandes, Deutsch, Finkelstein, Riley, Martins, Neubig, Garg, Clark, Freitag, and Firat]{fernandes2023devil}
Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr{\'e}~FT Martins, Graham Neubig, Ankush Garg, Jonathan~H Clark, Markus Freitag, and Orhan Firat.
\newblock The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation.
\newblock \emph{arXiv preprint arXiv:2308.07286}, 2023.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gautier et~al.(2019)Gautier, Polito, Bardenet, and Valko]{GPBV19}
Guillaume Gautier, Guillermo Polito, R{\'{e}}mi Bardenet, and Michal Valko.
\newblock {DPPy: DPP Sampling with Python}.
\newblock \emph{Journal of Machine Learning Research - Machine Learning Open Source Software (JMLR-MLOSS)}, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/19-179.html}.
\newblock Code at http://github.com/guilgautier/DPPy/ Documentation at http://dppy.readthedocs.io/.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hough et~al.(2006)Hough, Krishnapur, Peres, and Vir{\'a}g]{hough2006determinantal}
J~Ben Hough, Manjunath Krishnapur, Yuval Peres, and B{\'a}lint Vir{\'a}g.
\newblock Determinantal processes and independence.
\newblock 2006.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister]{hsieh2023distilling}
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
\newblock Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.
\newblock \emph{arXiv preprint arXiv:2305.02301}, 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Huang et~al.(2024)Huang, Siddarth, Lovitt, Liao, Durmus, Tamkin, and Ganguli]{huang2024collective}
Saffron Huang, Divya Siddarth, Liane Lovitt, Thomas~I Liao, Esin Durmus, Alex Tamkin, and Deep Ganguli.
\newblock Collective constitutional ai: Aligning a language model with public input.
\newblock In \emph{The 2024 ACM Conference on Fairness, Accountability, and Transparency}, pp.\  1395--1417, 2024.

\bibitem[Hunter(2004)]{hunter2004mm}
David~R Hunter.
\newblock Mm algorithms for generalized bradley-terry models.
\newblock \emph{The annals of statistics}, 32\penalty0 (1):\penalty0 384--406, 2004.

\bibitem[Javaheripi et~al.(2023)Javaheripi, Bubeck, Abdin, Aneja, Bubeck, Mendes, Chen, Del~Giorno, Eldan, Gopi, et~al.]{javaheripi2023phi}
Mojan Javaheripi, S{\'e}bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C{\'e}sar~Teodoro Mendes, Weizhu Chen, Allie Del~Giorno, Ronen Eldan, Sivakanth Gopi, et~al.
\newblock Phi-2: The surprising power of small language models.
\newblock \emph{Microsoft Research Blog}, 2023.

\bibitem[Jiang et~al.(2022)Jiang, Yuan, Chen, Cheng, Wang, Chen, and Ma]{jiang2022fuzzydedup}
Tao Jiang, Xu~Yuan, Yuan Chen, Ke~Cheng, Liangmin Wang, Xiaofeng Chen, and Jianfeng Ma.
\newblock Fuzzydedup: Secure fuzzy deduplication for cloud storage.
\newblock \emph{IEEE Transactions on Dependable and Secure Computing}, 2022.

\bibitem[Kendall(1938)]{kendall1938new}
Maurice~G Kendall.
\newblock A new measure of rank correlation.
\newblock \emph{Biometrika}, 30\penalty0 (1-2):\penalty0 81--93, 1938.

\bibitem[Kool et~al.(2019)Kool, Van~Hoof, and Welling]{kool2019stochastic}
Wouter Kool, Herke Van~Hoof, and Max Welling.
\newblock Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3499--3508. PMLR, 2019.

\bibitem[Kulesza et~al.(2012)Kulesza, Taskar, et~al.]{kulesza2012determinantal}
Alex Kulesza, Ben Taskar, et~al.
\newblock Determinantal point processes for machine learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 5\penalty0 (2--3):\penalty0 123--286, 2012.

\bibitem[Lauren{\c{c}}on et~al.(2022)Lauren{\c{c}}on, Saulnier, Wang, Akiki, Villanova~del Moral, Le~Scao, Von~Werra, Mou, Gonz{\'a}lez~Ponferrada, Nguyen, et~al.]{laurenccon2022bigscience}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova~del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou, Eduardo Gonz{\'a}lez~Ponferrada, Huu Nguyen, et~al.
\newblock The bigscience roots corpus: A 1.6 tb composite multilingual dataset.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 31809--31826, 2022.

\bibitem[Lee et~al.(2021)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch, and Carlini]{lee2021deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock \emph{arXiv preprint arXiv:2107.06499}, 2021.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Li, Chen, Chen, Cheng, Wang, Zhou, and Xiao]{li2023quantity}
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao.
\newblock From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
\newblock \emph{arXiv preprint arXiv:2308.12032}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang, and Hashimoto]{li2023alpacaeval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models, 2023{\natexlab{b}}.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{maas-EtAl:2011:ACL-HLT2011}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, pp.\  142--150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/P11-1015}.

\bibitem[Macchi(1975)]{macchi1975coincidence}
Odile Macchi.
\newblock The coincidence approach to stochastic point processes.
\newblock \emph{Advances in Applied Probability}, 7\penalty0 (1):\penalty0 83--122, 1975.

\bibitem[Mu et~al.()Mu, Helyar, Heidecke, Achiam, Vallone, Kivlichan, Lin, Beutel, Schulman, and Weng]{murule}
Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng.
\newblock Rule based rewards for language model safety.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock \emph{arXiv preprint arXiv:2306.01116}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Reimers(2019)]{reimers2019sentence}
N~Reimers.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock \emph{arXiv preprint arXiv:1908.10084}, 2019.

\bibitem[Sachdeva et~al.(2024)Sachdeva, Coleman, Kang, Ni, Hong, Chi, Caverlee, McAuley, and Cheng]{sachdeva2024train}
Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed~H Chi, James Caverlee, Julian McAuley, and Derek~Zhiyuan Cheng.
\newblock How to train data-efficient llms.
\newblock \emph{arXiv preprint arXiv:2402.09668}, 2024.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning, Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, pp.\  1631--1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/D13-1170}.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Sun et~al.(2024)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan]{sun2024principle}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[{Together AI}(2023)]{RedPajama2023}
{Together AI}.
\newblock Red pajama.
\newblock Blog post on Together AI, 2023.
\newblock \url{https://www.together.ai/blog/redpajama}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Tremblay et~al.(2018)Tremblay, Barthelme, and Amblard]{tremblay2018optimized}
Nicolas Tremblay, Simon Barthelme, and Pierre-Olivier Amblard.
\newblock Optimized algorithms to sample determinantal point processes.
\newblock \emph{arXiv preprint arXiv:1802.08471}, 2018.

\bibitem[Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang, Sreedhar, and Kuchaiev]{wang2024helpsteer2}
Zhilin Wang, Yi~Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy~J Zhang, Makesh~Narsimhan Sreedhar, and Oleksii Kuchaiev.
\newblock Helpsteer2: Open-source dataset for training top-performing reward models.
\newblock \emph{arXiv preprint arXiv:2406.08673}, 2024.

\bibitem[Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave]{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock \emph{arXiv preprint arXiv:1911.00359}, 2019.

\bibitem[Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen]{wettig2024qurating}
Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.
\newblock Qurating: Selecting high-quality data for training language models.
\newblock \emph{arXiv preprint arXiv:2402.09739}, 2024.

\bibitem[Wu et~al.(2023)Wu, Lu, Xu, Lin, Su, and Zhou]{wu2023self}
Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi~Su, and Chang Zhou.
\newblock Self-evolved diverse data sampling for efficient instruction tuning.
\newblock \emph{arXiv preprint arXiv:2311.08182}, 2023.

\bibitem[Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen]{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
\newblock Less: Selecting influential data for targeted instruction tuning.
\newblock \emph{arXiv preprint arXiv:2402.04333}, 2024.

\bibitem[Xie et~al.(2024)Xie, Santurkar, Ma, and Liang]{xie2024data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yadav et~al.(2019)Yadav, Bethard, and Surdeanu]{yadav2019quick}
Vikas Yadav, Steven Bethard, and Mihai Surdeanu.
\newblock Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering.
\newblock \emph{arXiv preprint arXiv:1911.07176}, 2019.

\bibitem[Yang et~al.(2024)Yang, Wang, Wen, and Zhang]{yang2024p3}
Yingxuan Yang, Huayi Wang, Muning Wen, and Weinan Zhang.
\newblock P3: A policy-driven, pace-adaptive, and diversity-promoted framework for optimizing llm training.
\newblock \emph{arXiv preprint arXiv:2408.05541}, 2024.

\bibitem[Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston]{yuan2024self}
Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
\newblock Self-rewarding language models.
\newblock \emph{arXiv preprint arXiv:2401.10020}, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2023opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models, 2022.
\newblock \emph{URL https://arxiv. org/abs/2205.01068}, 3:\penalty0 19--0, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, et~al.]{zhou2024lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}
