---
abstract: |
  The quality of training data significantly impacts the performance of large language models (LLMs). There are increasing studies using LLMs to rate and select data based on several human-crafted metrics (rules). However, these conventional rule-based approaches often depend too heavily on human heuristics, lack effective metrics for assessing rules, and exhibit limited adaptability to new tasks. In our study, we introduce an innovative rule-based framework that utilizes the orthogonality of score vectors associated with rules as a novel metric for rule evaluations. Our approach includes an automated pipeline that first uses LLMs to generate a diverse set of rules, encompassing various rating dimensions to evaluate data quality. Then it rates a batch of data based on these rules and uses the determinantal point process (DPP) from random matrix theory to select the most orthogonal score vectors, thereby identifying a set of independent rules. These rules are subsequently used to evaluate all data, selecting samples with the highest average scores for downstream tasks such as LLM training. We verify the effectiveness of our method through two experimental setups: 1) comparisons with ground truth ratings and 2) benchmarking LLMs trained with the chosen data. Our comprehensive experiments cover a range of scenarios, including general pre-training and domain-specific fine-tuning in areas such as IMDB, Medical, Math, and Code. The outcomes demonstrate that our DPP-based rule rating method consistently outperforms other approaches, including rule-free rating, uniform sampling, importance resampling, and QuRating, in terms of both rating precision and model performance.
author:
- Xiaomin Li [^1]
- Mingye Gao [^2]
- Zhiwei Zhang [^3]
- Chang Yue [^4]
- Hong Hu[^5]
bibliography:
- reference.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Rule-based Data Selection for Large Language Models
---





# Introduction

Large language models (LLMs) have been widely utilized across a diverse range of applications. Pretraining and fine-tuning these models typically require large and diverse datasets. Studies have found that data quality is critical for training good LLMs . For instance, Meta’s LIMA paper demonstrated that using only 1000 carefully curated data samples can achieve better performance than using the original 50k samples. Similar phenomena have been observed in other studies where selecting a subset of high-quality datasets increases the training convergence and model performance .

Recent studies now adopt an approach that employs LLM-as-a-judge to grade data quality according to a set of designed metrics (which we call rules) . For example, rates the pre-training data using LLMs according to four predefined rules. RedPajama is continuously developing a pool of rules for users to select from, which currently contains over 40 basic criteria that LLM data should satisfy. On specific aspects such as safety, Constitutional AI proposed their “constitution"—a set of standard safety criteria—to generate safe synthetic data, and in they have developed 133 rules. Most recently, OpenAI’s Rule-based Rewarding proposed 21 general safety rules and injected them into the RLHF (reinforcement learning with human feedback) process. This rule-based rating provides greater explainability of data quality and breaks down the challenge of assigning a data point one overall quality score into a simpler task of giving several rule-specific scores. Evidence suggests that this fine-grained approach also yields more accurate rating outcomes .

Nonetheless, there are several critical problems and challenges. First, designing an effective set of rules is quite difficult, a fact acknowledged by most of the papers above. Current designs in these papers all rely heavily on human heuristics and are sometimes too broad for effective rating. Second, as far as we know, the metrics to evaluate rules are lacking, and there has been no systematic exploration of how different rule choices and sizes impact the outcomes. In previous experiments , a subset of rules is selected (typically randomly) during the rating process. This selection can significantly influence the rating results and consequently the quality of the sampled data. Furthermore, the utility and impact of rules can vary significantly, and some rules are strongly correlated with each other, as highlighted in , which introduces redundancy and bias into the rating procedure. Therefore, a critical question arises: with a “constitution" (a pool of rules) in hand, exactly which “laws" (a subset of task-related rules) should be applied to a specific task? Random selection as in may not be the optimal strategy. A third major drawback is the inflexibility of these rules; they are often designed for specific settings, such as pre-training or safety tasks, not generally applicable across different settings.

In our work, we aim to address these challenges. First, we leverage an LLM (GPT-4 ) for automatic rule generation, where we include the descriptions of the task and source dataset in the prompt. At this stage, some generated rules are found to be repetitive or redundant, similar to the issues in human-designed rules. Our strategy is to first generate a comprehensive set of rules to ensure a broad diversity that covers various aspects we seek to evaluate in the data, and then filter out repetitive rules. Hence the second step of our approach is to select a subset of rules that are relatively uncorrelated/independent. This is achieved by first using the rules to rate a batch of data, creating one score vector for each rule, and then assessing the independence of rule subsets through the overall orthogonality of their corresponding score vectors. We propose the formula in Section  to measure the orthogonality and use determinant point process (DPP) sampling to identify a subset of independent rules. Once the rules are determined, the third step is to use them to rate all source data and select the high-quality ones. Combining these steps of rule generation, rule-based rating, rule selection by DPP, and data selection, our method establishes a fully automated framework for rule-based data selection (illustrated in Figure ). Notably, we are the first to introduce the mathematical rule evaluation metric, based on the orthogonality of their score vectors. Moreover, our pipeline does not need human intervention in designing and selecting rules at all. For every new task, one can use the pipeline to get a set of high-quality, task-specific rules at low cost. These address the challenges of existing methods mentioned above. Another advantage of our method is its natural extension, allowing for customization such as re-weighting particular rules, and this is only feasible when the selected rules are relatively “orthonormal".

Note that our data selection methodology is highly versatile and applicable to a variety of scenarios, including LLM pre-training, fine-tuning on specific domains, RLHF preference data, etc. We conduct experiments to cover a range of tasks and datasets, including general pre-training data and domain-specific data in four domains: IMDB, Medical, Math, and Code.[^6] We show that our rule-based data selection typically yields more accurate rating results, thereby enhancing data quality and leading to better performance of the LLM trained with the data. Here is a list of the main contributions of our work:

1.  **Rule-free vs. Rule-based Rating.** Our systematic experiments demonstrate that fine-grained rule-based rating outperforms rule-free methods, producing more precise data quality assessments, leading to improved benchmark performance of LLMs.

2.  **Rule Evaluation Metric:** We introduce a novel rule evaluation metric designed to promote low correlation and high diversity among rules. We propose the method of using DPP on task-aware rule-rating vectors to select a subset of independent rules.

3.  **Automated Rule-based Rating and Selection Pipeline.** We confirm that LLMs are effective rule generators, eliminating the need for manual rule crafting. Our automated pipeline generates the rules, selects the rules, and then chooses data according to rule-based ratings. This entire process operates independently of human heuristics and is free from human biases.

4.  **Cross-Domain and Cross-Model.** We validate our method through two approaches: A) comparison with ground truth ratings, and B) training LLMs with selected data and assessing performance across various benchmarks. Our experiments span multiple models, including Pythia-1B and Llama3-8B (fine-tuned with LoRA), and cover diverse domains such as IMDB, Medical, Math, and Code, confirming the versatility and model independence of our approach.

# Related Work

**LLM data selection.** There are different genres of data selection approaches for LLMs. Basic filterings, such as setting thresholds on word lengths, are used in many studies to eliminate low-quality data . Fuzzy deduplication is another approach which removes repetitive or similar data samples . Another method is “heuristic classification”, selecting data based on a predefined quality score, typically measured by similarity to formal sources such as Wikipedia or other human-generated, high-quality datasets . In contrast to this, directly querying LLMs to rate data and use the scores as the quality indicator has become a standard practice in many studies .

**Rule-based rating.** There are studies adopting a more fine-grained approach to data quality, distilling it into a finite set of metrics which we refer to as “rules". For instance, RedPajama provides over 40 quality rules that serve as basic quality metrics for the users to choose from. More pertinent to our research, there are papers that apply this rule-based idea to rate LLM data. For example, assigns a score out of 5 to each data point, awarding 1 point for each of the 5 predefined criteria met. In , the authors designed four general rules to rate and select data for LLM pre-training. proposed 16 human-crafted rules to evaluate the desirable quality of response data. The rule-based approach is also utilized in more targeted applications, such as ensuring data safety. Constitutional AI designed 16 general safety critique rules to revise synthetic data, enhancing data safety . This revision process involves iterative steps where a random subset of rules from the “constitution" (the entire set of rules) is applied. Additionally, in , the score generated by an LLM grader according to a set of 21 safety rules is integrated directly into the RLHF process as an additional reward. In , they design a composite reward model in RLHF, trained using rule-based ratings. As noted earlier in the introduction, the rules employed in the literature exhibit several critical issues. They often depend heavily on human heuristics for design, lack robust rule evaluation metrics and exploration of rule sizes, and demonstrate limited versatility for new tasks or for customization. Our goal is to address these challenges using our proposed framework.

# Methodology

## Definitions and Notations

We introduce the definitions of the primary objects considered in our method:

- $R$: the total number of available rules.

- $r$: the number of selected rules, using a specified rule selection method.

- $\mathcal{D}$: the set of all data samples, with its size denoted by $N \stackrel{\text{def}}{=}|\mathcal{D}|$.

- $\mathcal{B} \subseteq \mathcal{D}$: a batch of data samples, randomly selected for evaluating the correlation of rules during the rule selection step, with its size denoted by $n \stackrel{\text{def}}{=}|\mathcal{B}|$.

- $\mS \in \R^{n \times R}$: the rating matrix $\mS$ where each entry $S_{i,j}$ represents the score of the $i$-th data sample according to the $j$-th rule and is constrained to the interval $[0,1]$.

- $\bar{\mS}\in \R^{n \times r}$: a submatrix of $\mS$ consisting of the $r$ selected columns from $\mS$, corresponding to the $r$ selected rules.

**Measure orthogonality:** We propose a metric for selecting rules based on the orthogonality of score vectors. Here we introduce a mathematical definition to quantify the orthogonality or correlation of a set of score vectors. Given a score matrix $\bar{\mS}\in \R^{n \times r}$ such that the columns are the score vectors of dimensions $n$. We begin by computing the covariance matrix $Cov(\bar{\mS})$ for the columns of $\bar{\mS}$, whose entries are defined by $$\begin{aligned}
    &Cov(\bar{\mS})_{i,j} \stackrel{\text{def}}{=}\frac{1}{n} \sum_{k=1}^n (S_{k,i}-\mu_i)(S_{k,j}-\mu_j), \qquad 1\leq i,j \leq r,
\end{aligned}$$ where each $\mu_i \stackrel{\text{def}}{=}\frac{1}{n} \sum_{k=1}^n S_{k,i}$ is the sample mean for rule $i$. Then define the sample correlation matrix as $Corr(\bar{\mS}) \in \R^{r \times r}$ where $$\begin{aligned}
    Corr(\bar{\mS})_{i,j} \stackrel{\text{def}}{=}\frac{Cov(\bar{\mS})_{i,j}}{\sqrt{Cov(\bar{\mS})_{i,i} \cdot Cov(\bar{\mS})_{j,j}}}, \qquad 1\leq i,j \leq r.
\end{aligned}$$

Commonly used libraries such as `Numpy` provide straightforward functions to compute the correlation matrix. We introduce the concept of *rule correlation*, which quantifies the degree of correlation/dependence for a given rating submatrix $\bar{\mS}$, defined as follows: $$\label{eq:rule_correlation}
\rho(\bar{\mS}) \stackrel{\text{def}}{=}\frac{1}{r}\|Corr(\bar{\mS}) - \mI_r\|_F = \frac{1}{r}\sqrt{\sum_{i\neq j} Corr(\bar{\mS})_{i,j}^2}.$$ Here $\mI_r \in \R^{r \times r}$ is the identity matrix, and $\|\cdot\|_F$ represents the Frobenius norm. This metric quantifies how much the columns of $\bar{\mS}$ deviate from orthogonality, by measuring the deviation of its correlation matrix from the identity matrix. The second equality in provides another intuitive understanding: $\rho(\bar{\mS})$ essentially aggregates the correlations of all pairwise correlations of rules $(i, j)$ for $i\neq j$.

## Determinantal point process (DPP)

The optimal solution to this mathematical problem of selecting the most orthogonal subset of a set of vectors is NP-hard but we use DPP sampling to provide a relatively good solution. The determinant point process (DPP) is a probabilistic model that describes the likelihood of selecting diverse subsets from a larger set . Mathematically, a DPP is defined by a kernel matrix that describes the similarities between elements in a set. The probability of selecting a particular subset is proportional to the determinant of the corresponding submatrix of this kernel matrix. Intuitively, subsets with highly similar items (leading to higher correlation in the submatrix) have smaller determinants and are thus less likely to be chosen.

**DPP Definitions.** Given a discrete ground set $\mathcal{Y}$, without loss of generality we let $\mathcal{Y} = \{1, 2, \dots, R\}$, a (discrete) DPP defines a probability measure over $2^{\mathcal{Y}}$, the power set of $\mathcal{Y}$. Let $Y$ be a randomly chosen subset. Then for any subset $A \subseteq \mathcal{Y}$, the probability of $A$ being chosen by a DPP is given by: $$\mathbb{P}(A \subseteq Y) = \det(\mK_A)$$ where $\mK \in \R^{R\times R}$ is a real positive-semidefinite matrix called the *kernel matrix* and $\mK_A \stackrel{\text{def}}{=}[\mK]_{i,j \in A}$ is the submatrix of $\mK$ indexed by elements in $A$.

**Kernel Matrix.** Each entry $K_{ij}$ in the kernel matrix $\mK$ describes the similarity between elements $i$ and $j$ in $\mathcal{Y}$. For our purpose of selecting orthogonal rules, we will define $\mK$ as the Gram matrix of the score vectors: $\mK \stackrel{\text{def}}{=}\mS^\top \mS$.

**DPP Sampling.** To sample a diverse subset using DPP, there are several existing algorithms and the Python library `DPPy` implements some of them. The computation of the DPP sampling primarily hinges on the overhead of computing the inner product kernel matrix $\mK$ and its eigendecomposition. In our case, $\mK \in \R^{R \times R}$ and hence it requires $O(R^3)$ time, where $R$ is the number of all rules. Nonetheless, we set $R=50$ in our experiments, therefore our DPP rule selection algorithm is extremely fast (typically within 0.1 seconds). Further details about DPP sampling algorithms and their time complexities can be found in Appendix .

## DPP rule-based rating algorithm

<figure id="fig:pipeline">
<span class="image placeholder" data-original-image-src="figures/pipeline.pdf" data-original-image-title="" width="100%"></span>
<figcaption>Pipeline for rule-based data rating and selection. Step 1. Use LLM to generate a comprehensive set of <span class="math inline">\(R\)</span> rules. Step 2. Rate a batch of <span class="math inline">\(n\)</span> data according to <span class="math inline">\(R\)</span> rules and form the score matrix <span class="math inline">\(\mS \in \R^{n \times R}\)</span>. Step 3. Select <span class="math inline">\(r\)</span> rules that correspond to the columns sampled by the DPP in the score matrix. Step 4. Rate the full dataset using the <span class="math inline">\(r\)</span> selected rules and (stochastically) select data with the highest averaged ratings. Step 5. Application of chosen data on downstream tasks, such as for LLM training.</figcaption>
</figure>

The pipeline of our rule-based data selection method is illustrated in Figure  and comprises the following steps:

**Step 1. Rule generation.** We query GPT-4 to generate $R$ rules. In the prompt, we include the goal, the description of the source data, and the description of the downstream task to help GPT-4 generate relevant task-related rules.

**Step 2. Rule-based rating**: Recall the definitions in Section . We employ LLM, particularly Llama3-8B-Instruct , to rate the batch data $\mathcal{B}$ according to $R$ rules, resulting in the matrix $\mS \in \R^{n \times R}$.

**Step 3. Rule selection using DPP**: From $\mS$, we aim to select $r$ relatively independent columns using a DPP, forming the submatrix $\bar{\mS}\in \R^{n \times r}$. We define the kernel matrix of DPP as follows: $$\label{eq:kernel}
    \mK \stackrel{\text{def}}{=}\mS^\top \mS \in \R^{R\times R},$$ where each entry $K_{i,j} = \langle S_i, S_j \rangle$ (each $S_i$ is the $i$-th column of $\mS$), representing the similarity between rule $i$ and rule $j$. We then employ the DPP sampling algorithm to select $r$ indices from $\{1, 2, \dots, R\}$, corresponding to the $r$ chosen rules.

Note that the cost of generating $R$ rules is negligible, requiring just a single GPT-4 query, and the cost of obtaining the rating matrix $\mS$ can be managed by adjusting the batch size $n$. The motivation to select a fixed small number of $r$ rules is driven by the computational costs associated with using LLMs for data rating and the need to maintain a consistent dimensionality for explaining data quality. These practical considerations lead us to treat $r$ as a hyperparameter. Discussions on the optimal choices of $r$ are explored in Section  and Appendix .

Another important remark is that, even with the same set of rules, they could have different correlations conditioned on a specific task or dataset. Therefore during DPP selection, instead of employing fixed representations such as semantic encodings—which result in static rule representations and selections across all tasks—we use *task-aware* score vectors to adaptively represent the rules. These vectors allow the entire pipeline to be customized for a particular downstream task.

**Step 4. Stochastic data selection**: We extend the rating process to cover all data samples using the selected $r$ rules, expanding the rating matrix $\bar{\mS}$ from $n \times r$ to $N \times r$. We then aggregate these fine-grained ratings by averaging across the $r$ columns of $\bar{\mS}$, resulting in a score vector $\vv = [v_1, v_2, \dots, v_N]$ that assigns a quality score to each of the $N$ samples.

Given the $N$ scores and a fixed budget of selecting $k$ samples for training, rather than choose the traditional top-$k$ approach, (selecting the $k$ highest scored samples), we adopt a stochastic sampling strategy, where we sample $k$ data points according to the distribution: $$\label{eq:top-k-sampling}
    p(\vx_i) = \frac{e^{v_i}}{\sum_{j=1}^N e^{v_i}}$$ for each data point $\vx_i \in \mathcal{D}$. This stochastic data selection mechanism introduces greater diversity into the sampling process and is used in several other papers ().

**Step 5.** Apply the selected data on given downstream tasks, such as for LLM pre-training and domain fine-tuning.

# Evaluation A: Evaluating Against Ground Truth Ratings

We evaluate our method in two ways: A. by comparing the rating results against the ground truth rating of the dataset. Smaller deviations from the ground truth scores indicate better performance. Specifically, we rely on pairwise comparisons generated by GPT-4 and apply the Bradley-Terry model to compute $n$ scores, treating them as the ground truth. B. by training an LLM (Llama3-8B) with the selected data and assessing its performance through both general and domain-specific benchmarks. In this section, we present the first set of experiments (corresponding to Evaluation A), while the second set of experiments (based on Evaluation B) is discussed in Section . The low cost of Evaluation A enables us to explore various aspects such as the rule-size scaling law, different rating schemes (pairwise vs. single), and the impact of model sizes (Llama3-8B and Llama3-70B). These experiments provide preliminary evaluations of our method.

## Experiments Setup

**Datasets:** We consider two datasets: CommonCrawl , containing raw web-crawled data, and IMDB , a dataset of 50K movie reviews, representing general and domain-specific settings, respectively. For each dataset, we collect the first 50 examples and apply a pairwise comparison scheme for data rating (prompt templates are available in Appendix ), which requires comparison on 2,450 ordered pairs.

**Ground truth scores:** Ground truth scores are generated as follows: we prompt GPT-4 to compare each pair of data samples $(i, j)$ and then reversing the comparison for $(j, i)$. We only keep the pairs where both comparisons are consistent, filtering out cases where GPT-4 performs poorly. After filtering, approximately 1000 comparisons remain for CommonCrawl and 1800 for IMDB. From these outcomes, we calculate scores for the 50 samples using the Bradley-Terry model (details can be found in Appendix ).

**Rating:** Now with the ground truth scores, we use our rule-based approach to rate the same data. For each rule $i \in \{1,2,\dots, R\}$ ($R=50$), we employ Llama3-8B-Instruct as our comparison rater and similarly use the Bradley-Terry model to compute a score vector $S_i \in \R^n$ ($n$ is also 50 here), thereby forming the rating matrix $\mS \in \R^{n \times R}$. Recall we denote $\bar{\mS}$ as the submatrix of $\mS$ containing $r$ columns indexed by the $r$ selected rules. To assess the rating results in $\bar{\mS}$ against the ground truth, we compute the mean squared error (MSE): $$\label{eq:mse}
   \epsilon(\bar{\mS}) \stackrel{\text{def}}{=}\frac{1}{n} \left\|\frac{1}{r}\sum_{j=1}^r \bar{S}_j - S_{GT} \right\|^2_2$$ where $S_{GT} \in \R^{n}$ is the ground truth score vector and $\bar{S}_j$ is the $j$-th column of $\bar{\mS}$. Furthermore, to establish comparative baselines, we implemented the same rating procedure (pairwise comparisons and score calculations via the Bradley-Terry model) using both the four designed rules in QuRating (see ) and a rule-free approach, referred to as the “NoRule” setting.

Our experiments in this section aim to address the following research questions: **(Q1)** Does greater rule diversity lead to more accurate ratings? **(Q2)** Does rule-based selection generally outperform rule-free methods? **(Q3)** How does our DPP-based rule selection compare to human-designed rules and ratings without rules? **(Q4)** Does DPP select better rules than randomly chosen ones? **(Q5)** How do different rating schemes and rater models impact the performance of our method?

## Results

**Correlation of $\rho(\bar{\mS})$ and the MSE $\epsilon(\bar{\mS})$ (answer to **Q1**).** For each $r \in \{1,2,\dots, 50\}$, we sample $\min\{10000, \binom{50}{r}\}$ sets of indices of size $r$, which are used to choose rules and form $\bar{\mS}$. We then calculate its rule correlation $\rho(\bar{\mS})$ and MSE $\epsilon(\bar{\mS})$. We compute their Pearson correlation and observe positive values for both IMDB and CommonCrawl datasets (see Figures  and ). This confirms that higher rule diversity is positively correlated with the accuracy of rating results. In other words, the correlation or redundancy of rules is positively correlated with the error $\epsilon(\bar{\mS})$.

**Rule-based v.s. Rule-free (answer to **Q2**):** We sample $10^6$ possible rule subsets with size $r$ from all 50 rules and calculate the corresponding MSE, comparing it to the MSE from the NoRule setting. The results in Figures  and demonstrate that using rule-based rating is mostly guaranteed to give better results than rating without rules, no matter applied to general data like CommonCrawl or domain-specific data like IMDB. When compared to QuRating MSE, the results show that QuRating is outperformed by most randomly selected rule subsets, highlighting the limitations of human-designed rules.

<figure id="fig:EvalA_CommonCrawl_Pairwise_histogram">
<figure id="fig:EvalA_IMDB_Pairwise_pearson">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Pairwise_pearson.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Pairwise_pearson">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Pairwise_pearson.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Pairwise_histogram">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Pairwise_histogram.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Pairwise_histogram">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Pairwise_histogram.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figcaption>(a) and (b): Pearson correlation of the rule correlation <span class="math inline">\(\rho(\bar{\mS})\)</span> and the MSE <span class="math inline">\(\epsilon(\bar{\mS})\)</span> for IMDB and CommonCrawl datasets respectively. (c) and (d): Distribution of MSE across <span class="math inline">\(10^6\)</span> random rule subsets with size <span class="math inline">\(r\)</span>, for IMDB and CommonCrawl datasets respectively, with vertical lines representing the MSE values of QuRating and NoRule.</figcaption>
</figure>

**DPP v.s. QuRating v.s. NoRule (answer to **Q3**).** For each $r \in \{1, 2, \dots, 50\}$, we use DPP to sample $r$ rules and conduct 100 trials. Then compare the averaged MSE against the MSEs from QuRating and NoRule, recording the winning rates of the DPP rules (see Figure ). For the IMDB dataset, we found that once $r$ reaches a certain threshold, DPP rules consistently achieve near-perfect winning rates against both NoRule and QuRating. Interestingly, for CommonCrawl, DPP underperforms QuRating when $r$ is too small or too large. This suggests that while QuRating rules are effective for general pre-training data, they lack the flexibility to adapt to other settings or domains.

**DPP rules v.s. Randomly selected rules (answer to **Q4**).** We compare DPP-selected rules with randomly selected rules of the same size $r$, evaluating both the rule correlation $\rho(\bar{\mS})$ and MSE $\epsilon(\bar{\mS})$ for their corresponding score submatrices $\bar{\mS}$. The results show that DPP consistently produces rules with lower correlation and MSE, regardless of the value $r$ (see Figures  and ). Another key observation is that the MSE for DPP rules increases when $r$ is either too small or too large, with the optimal $r$ falling somewhere in the middle. This matches our intuition: when $r$ is too small, there are too few rules to achieve sufficient rating diversity, and when $r$ is too large, rule redundancy can negatively affect the rating outcomes. In fact, this motivates our selection of $r=10$ in this paper.

<figure id="fig:test">
<figure id="fig:EvalA_IMDB_Pairwise_winning_rate">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Pairwise_winning_rate.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Pairwise_dpp_vs_random_RC">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Pairwise_dpp_vs_random_RC.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Pairwise_dpp_vs_random_MSE">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Pairwise_dpp_vs_random_MSE.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Pairwise_winning_rate">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Pairwise_winning_rate.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Pairwise_dpp_vs_random_RC">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Pairwise_dpp_vs_random_RC.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Pairwise_dpp_vs_random_MSE">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Pairwise_dpp_vs_random_MSE.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figcaption>(a) Winning rate of DPP-selected rules compared to QuRating’s four rules and the NoRule setting, based on MSE across 100 DPP trials. (b) Comparison of rule correlation between DPP-selected and randomly selected rules, averaged across 100 trials. (c) Comparison of MSE between DPP-selected and randomly selected rules, averaged across 100 trials. Plots (a), (b), and (c) display results for the IMDB dataset, while (d), (e), and (f) for the CommonCrawl dataset.</figcaption>
</figure>

**Variations in rating schemes and rater models (answer to **Q5**)**. To verify that our method works across different rating schemes and rater models, we explored the following variations: *1. Pairwise v.s. individual rating.* While the pairwise ratings provide more reliable comparisons, individual rating requires only $O(n)$ computation. We observed similar results as in Section  (see Appendix ). Notably, individual ratings on the IMDB dataset showed a Pearson correlation between rule correlation $\rho(\bar{\mS})$ and MSE $\epsilon(\bar{\mS})$ of up to 0.6, and the winning rates show that DPP significantly outperforms both QuRating and the NoRule. *2. Llama3-8B v.s. Llama3-70B.* We tested the influence of rater model capability by switching to Llama3-70B (instruction-tuned version), using the individual rating scheme on IMDB. The results are similar to earlier and we also noted a high Pearson correlation (over 0.6) between rule correlation and MSE, along with a high winning rate of DPP compared to QuRating and NoRule. Furthermore, randomly selected rules perform significantly better than both QuRating and NoRule. See Appendix  for further details.

# Evaluation B: Data Selection for LLM Fine-tuning

In this section, we follow the pipeline outlined in Section  and conduct experiments based on Evaluation B, where we train an LLM (Llama3-8B) using the selected data and assess its performance. This setup closely reflects real-world applications of LLM data selection. We benchmark our method against several baselines, such as uniform sampling, direct rating without rules, QuRating rules , and DSIR (a commonly used baseline for LLM data selection). We condcut experiments in this section to explore the following research questions: **(Q1)** How does data selected by rule-based methods enhance model fine-tuning compared to rule-free methods? **(Q2)** How do the rules generated by our automated framework compare to human-designed rules? **(Q3)** How does DPP rule selection perform compared to random rule selection?

## Experiments Setup

**Evaluation Benchmarks.** To systematically evaluate the effectiveness of our framework, we use following benchmarks: For experiments on general continued pre-training, we utilize ARC-Easy , ARC-Challenge , Winogrande , MMLU, and SST-2 . Then we employ domain-specific datasets to do fine-tuning: For IMDB, we use the IMDB sentiment analysis dataset . For Code, we use benchmarks for code generation, including HumanEval , MBPP , Multiple-py and Multiple-cpp . For Math and Medical domains, we choose subsets from MMLU corresponding to Math subject and Medical subject respectively. More details about these benchmarks are summarized in Appendix .

**Data Source.** SlimPajama is a large, deduplicated, multi-corpus open-source dataset specifically designed for training large language models . We randomly sampled 1 million data points (around 1 billion tokens) from SlimPajama as our initial data source $\mathcal{D}$. From this pool, we employ our selection methods to choose data for training.

**Models.** We train Pythia-1B on general continued pre-training and domain fine-tuning for IMDB and Medical. We intentionally selected Pythia-1B because it is known to be pre-trained on the Pile dataset , making it a better choice than models that possibly included SlimPajama in their pre-training corpus. To validate the transferability of our framework across different LLMs, we train Llama3-8B with LoRA for the Math and Code domains.

**Compared Methods.** We compare our method against the following baselines, including both rule-free and rule-based data selection methods. For rule-free methods, we have: *Uniform sampling*: select the data randomly, *No Rule:* prompt Llama3-8B-Instruction to individually rate the data without rules, and then apply the same sampling procedure as described in , *DSIR* : importance resampling of data that resemble a target dataset (we use Wikipedia as the target for the general continued pre-training and benchmark test datasets for the domain fine-tuning). For rule-based methods, we include: *QuRating* : data rating and selection using four human-designed rules, and *GPT-Uncorrelated*: Directly prompting GPT-4 to generate 10 uncorrelated rules for data rating and selection. We have comparison against more baseline methods in Appendix .

For our automated rule-based selection algorithm, we set $R=50$, as in Section , and select $r=10$ for rule selection. As inferences of LLM are a lot less computing-consuming than model training, we set $n=10^4$ in all our experiments. The choice of $r$ as a hyperparameter is based on experimental observations from Section , where very small or very large values of $r$ did not yield optimal results. Discussion and exploration of different values for $r$ are provided in Appendix , and details of the rule generation prompts, rating prompts, generated and selected rules are in Appendix . To demonstrate the effectiveness of our automated orthogonal-rule-based selection algorithm, we evaluate the performance of the following methods developed within our framework, and we also conduct comparisons among these methods to demonstrate the advantages of DPP in rule selection.

- *All 50 Rules*: Average score vectors from all 50 rules to rate and select data.

- *Random 10 Rules*: Randomly choose 10 rules and average the score vectors to rate and select data.

- *DPP 10 Rules*: Use DPP to sample 10 rules, and then apply them to rate and select data.

Note that for uniform sampling and the methods involving randomness in the selection of rules, we considered 3 independent trials and averaged the results (see details in Appendix ).

## General Continued Pre-training

We selected 20K samples from our data source using the methods described above for continued pre-training of Pythia-1B, then benchmarked the model’s performance. The choice of 20K samples was constrained by our GPU resources. Despite 20K being significantly smaller than the pre-training corpus size, we still observed improvements in benchmark results shown in Table , with DPP leading in most metrics. We anticipate these differences will be more obvious in domain-specific fine-tuning settings, demonstrated in below.

## Domain-specifc Fine-tuning

We now focus on domain-specific fine-tuning across four domains: IMDB, Medical, Math, and Code. By selecting 20K domain-related data samples from our source for model training, we aim to enhance domain-specific task performance. As demonstrated in Tables  and , domain-specific fine-tuning yields more significant improvements than general continued pre-training, where the latter often needs larger datasets to enhance performance due to the broader nature of the training data. Notably, rule-based methods consistently outperform rule-free approaches in general, especially when comparing against *Random Select* and *No Rule*. Among all rule-based methods, *QuRating* underperforms. As previously noted, such human-designed rules are inherently limited due to varying preferences of designers, introducing bias in rules. Additionally, human designers may not capture the data-dependent correlation between rules effectively. The *GPT-Uncorrelated* rules face a similar issue where the rule selection process is entirely independent of the data. In contrast, our framework begins by automatically generating a diverse set of rules and then selecting an orthogonal subset. Furthermore, our method employs the data-dependent score vector to represent rules and utilizes a quantitative measure to accurately assess their correlation.

Within our framework, DPP demonstrated superior performance compared to using all 50 rules or selecting 10 rules randomly. This aligns with our argument of the importance of rule orthogonality, as well as the intuition that the optimal $r$ is not near the boundaries (both validated by the previous experiments on Section ). This underscores the effectiveness of a rule-based strategy, which introduces more *balanced* diversity in the data rating aspects and selects better training data. Furthermore, it also demonstrates that our application of DPP in rule selection effectively identifies a core set of high-quality rules, thereby enhancing data quality and ultimately improving model performance.

# Conclusion

We have introduced an automated, rule-based framework for selecting high-quality LLM data, utilizing LLMs to generate a diverse set of rules and the DPP method to eliminate redundancy. Our work is the first to introduce an automated rule evaluation metric and we also propose a rule-based selection pipeline that demonstrates substantial generalizability across various settings, effectively overcoming the limitations of human-designed rules and addressing the challenges associated with the lack of robust rule evaluations. We first demonstrated that our approach enhances the accuracy of data ratings using a dataset with given ground truth scores. Then we conduct experiments that train LLMs with selected data and have shown that our method outperforms various other approaches, both in general pre-training and fine-tuning across four domains. The results indicate that our method successfully generates high-quality, diverse rules, and thereby improves quality of selected data, which in turn leads to improved model performance after trained with the chosen data.

# Appendix

## Orthogonality Measures

**Volume of parallelepiped.** In our experiments, we also considered another measure of orthogonality, defined as the “volume” of the parallelepiped formed by vectors. This is mathematically described as: $$\label{eq:volume}
    \textbf{Vol}(\bar{\mS}) \stackrel{\text{def}}{=}\frac{\sqrt{\det(\bar{\mS}^\top \bar{\mS})}}{\Pi_{i=1}^r \|\vv_i\|},$$ where $\vv_i$ are the columns of $\bar{\mS}$. The determinant of $\bar{\mS}^\top \bar{\mS}$ geometrically represents the squared volume of the parallelepiped formed by the columns of $\bar{\mS}$ . We normalize by the product of the vector norms since both the magnitude of the vectors and their mutual correlation influence the volume: larger norms increase the volume, whereas higher correlation reduces it. Thus, after normalization, the value of **Vol** serves as an indicator of the overall orthogonality among the column vectors of $\bar{\mS}$. The phenomena under the usage of this measure are similar to the ones under . Therefore we only presented results using the rule correlation.

## DPP Sampling

**Intuition by $r=2$ case.** Here we use the $r=2$ case to illustrate the intuition behind DPP and explain why it tends to choose items that are relatively uncorrelated. Using the same notation as in , let $\mK$ be the kernel matrix and $\mathcal{Y}, Y$ be the ground set and selected subset respectively. When $r=2$, consider items $A = \{i,j\}$. Then the probability of both items being selected together is given by: $$\begin{aligned}
    \mathbb{P}(A \subseteq  Y) 
    &= K_{i,i}K_{j,j} - K_{i,j}K_{j,i}\\
    &= \mathbb{P}(i \in Y)\mathbb{P}(j \in Y) - K_{i,j}^2\\
    &= \mathbb{P}(i\text{ is chosen})\mathbb{P}(j\text{ is chosen}) - (\text{similarity of items $i,j$})^2,
\end{aligned}$$ since $\mK$ is symmetric by our definition. Larger similarity of $i,j$ reduces the probability $\mathbb{P}(A \subseteq  Y)$, indicating that similar items are less likely to be chosen simultaneously. This underscores the DPP’s capacity to promote diversity by favoring the selection of dissimilar items.

**DPP Sampling Algorithm:** The sampling algorithm can be found in Algorithm 1 of . The sampling process starts by decomposing the kernel matrix $\mK$ and involves two main stages: 1. Selecting eigenvectors by sampling from a Bernoulli distribution based on the eigenvalues, and 2. Sampling a subset from the ground set using an iterative conditional distribution method to ensure diversity, as detailed in . We utilize the `DPPy` Python library for efficient DPP initialization and sampling.

**Time Complexity:** Finding the submatrix (subset of columns) of a matrix to maximize the orthogonality is NP-hard . DPP provides us a relatively good solution. In practice, the computational complexity of sampling from a DPP depends primarily on the eigendecomposition of the kernel matrix $\mK$. In our case, $\mK \in \R^{R \times R}$ and therefore it requires $O(R^3)$ time, where $R$ is the number of rules. In the `DPPy` package it uses the spectral sampler by default, so the actual run-time of our DPP implementation is $O(R^3)$.

**DPP Sampling for Data Selection:** We noticed that in a concurrent work , the authors also use DPP to perform data selection, but directly applied to the data itself. However, the approach to directly perform data selection using DPP requires the computation based on the kernel matrix with dimension $N$ (number of samples), which is usually huge in the context of LLM data. Moreover, while DPP inherently prioritizes diversity in data selection, it does not address other quality dimensions. In contrast, our rule-based approach assesses multiple aspects of data quality, ensuring a more comprehensive and robust selection process.

## Stochastic data selection: Gumbel top-$k$ trick:

Imagine the cases where the target dataset distribution shows a long-tail pattern with respect to our quality measure, using a deterministic quality score as the cutoff could exclude many possibly valuable data . Hence, our stochastic sampling in effectively balances the quality and diversity of the selected data. Nonetheless, instead of doing actual sampling according to Equation , we use the Gumbel top-$k$ trick similar as in , which is a sampling technique used to efficiently and probabilistically select the top-$k$ items from a discrete probability distribution. Specifically, each item $i$ in the distribution is assigned a score using the formula: $$s_i = \log p_i + g_i,$$ where $p_i$ is the probability of item $i$, and $g_i$ is a noise term drawn from a Gumbel distribution, which can be generated using $g_i = -\log(-\log(u_i))$. In other words, we could add a Gumbel noise vector to the log of the sampling probability in Equation  and then choose the top-$k$ data points with the highest sums. This is statistically equivalent to sampling according to Equation  .

## Limitations and Future Directions

We have developed an automated, rule-based selection framework for identifying high-quality LLM data. Below, we outline some limitations of our approach and suggest potential directions for future research:

**Adjusting hyperparameters.** Recall that our hyperparameter $r$ determines the number of rules selected for rating, influencing the diversity and coverage of the selected rules. We have explored the effect of $r$ in Section  and also in Appendix . We leave a comprehensive study of its optimal values for future work.

**Data sampling method.** There are variations of the stochastic top-$k$ sampling, such as incorporating a temperature parameter $\tau$ (see ). Replacing with its variations or exploring other data sampling methods represents another research direction.

**Rule format.** In this study, we only focus on natural language rules, which are straightforward to design and offer significant explainability. However, rules in other formats can also be integrated into our pipeline.

**Other rule evaluations metrics.** We have proposed multiple metrics in and to measure rule quality, but all based on the correlation/orthogonality of rules. Evaluating rules from other aspects is another intriguing topic for future work.

## Appendix for Evaluation A

### Bradley Terry Model

The Bradley-Terry model is a probabilistic model used to estimate the latent “strength” of teams based on pairwise competitions. The model is parameterized as follows: $$\label{eq:bradley-terry}
    \mathbb{P}(i \text{ beats }j) = \frac{v_i}{v_i + v_j} = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}},$$ where exponential functions are used to model the scores $v_i \stackrel{\text{def}}{=}e^{\beta_i}$ and $v_j \stackrel{\text{def}}{=}e^{\beta_j}$. In other words, the difference of their scores determines the the log-odds of team $i$ beating team $j$. Sometimes an intercept term $\alpha$ is added to adjust for any influence of the order (for example, imagine that $i$ is the home team and has home-court advantage), then the probability becomes $$\label{eq:bradley-terry2}
    \mathbb{P}(i \text{ beats }j) = \frac{e^{\alpha + \beta_i}}{e^{\alpha + \beta_i} + e^{\beta_j}},$$ The most straightforward method for estimating these parameters is through maximum likelihood estimation, which optimizes the likelihood of the observed outcomes based on the model and its parameters. More details can be found in .

### Error Metrics

**Ranking-difference error.** To assess the deviation of rating scores from the ground truth, instead of using the mean squared error in , an alternative intuitive approach is to compare the rankings derived from the data scores with those of the ground truth. This approach is based on the premise that for data selection purposes, if two sets of scores yield identical rankings, they will select the same high-scoring data samples. An example of such a ranking metric is the Kendall rank correlation coefficient (Kendall’s tau) . However, we opted against this type of metric for two critical reasons: First, it lacks the granularity needed to evaluate errors effectively. For instance, two sets of scores like \[0.01, 0.98, 0.99\] and \[0.01, 0.02, 0.03\] share exactly the same ranking yet differ significantly in their actual scores. Second, our method involves stochastic data selection, not a straightforward top-$k$ selection, meaning that a higher score increases the likelihood of a data point being chosen. Hence, a ranking difference, which overlooks the absolute values of scores and focuses solely on their relative comparisons, is not ideal here.

### Rating scheme Variation: Individual rating

Here we present the results after replacing the pair-wise rating with the direct individual rating in Section :

<figure id="fig:EvalA_CommonCrawl_Single_histogram">
<figure id="fig:EvalA_IMDB_Single_pearson">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single_pearson.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Single_pearson">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Single_pearson.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Single_histogram">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single_histogram.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Single_histogram">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Single_histogram.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figcaption>(a) and (b): Pearson correlation of the rule correlation <span class="math inline">\(\rho(\bar{\mS})\)</span> and the MSE <span class="math inline">\(\epsilon(\bar{\mS})\)</span>, for IMDB and CommonCrawl datasets respectively. (c) and (d): Distribution of MSE from <span class="math inline">\(10^6\)</span> possible rule subsets with size <span class="math inline">\(r\)</span>, for IMDB and CommonCrawl datasets respectively. Two vertical lines represent the MSE values of QuRating and NoRule.</figcaption>
</figure>

<figure id="fig:EvalA_CommonCrawl_Single_dpp_vs_random_MSE">
<figure id="fig:EvalA_IMDB_Single_winning_rate">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single_winning_rate.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Single_dpp_vs_random_RC">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single_dpp_vs_random_RC.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Single_dpp_vs_random_MSE">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single_dpp_vs_random_MSE.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Single_winning_rate">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Single_winning_rate.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Single_dpp_vs_random_RC">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Single_dpp_vs_random_RC.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_CommonCrawl_Single_dpp_vs_random_MSE">
<span class="image placeholder" data-original-image-src="figures/EvalA_CommonCrawl_Single_dpp_vs_random_MSE.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figcaption>(a) Winning rate of DPP-selected rules compared to QuRating’s four rules and the NoRule setting, based on MSE across 100 DPP trials. (b) Comparison of DPP rule correlation vs. random rule correlation (averaged over 100 trials). (c) Comparison of MSE between DPP-selected and randomly selected rules, averaged across 100 trials. Plots (a), (b), and (c) display results for the IMDB dataset, while (d), (e), and (f) for the CommonCrawl dataset.</figcaption>
</figure>

### Rater Model Size Variation: Llama3-70B-Instruct

Here we present the results after replacing the rater model from Llama3-8B-Instruct model with the stronger Llama3-70B-Instruct in Section :

<figure id="fig:EvalA_IMDB_Single70B_histogram">
<figure id="fig:EvalA_IMDB_Single70B_pearson">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single70B_pearson.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Single70B_histogram">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single70B_histogram.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption></figcaption>
</figure>
<figcaption>(a): Pearson correlation of the rule correlation <span class="math inline">\(\rho(\bar{\mS})\)</span> and the MSE <span class="math inline">\(\epsilon(\bar{\mS})\)</span> (b): Distribution of MSE from <span class="math inline">\(10^6\)</span> possible rule subsets with size <span class="math inline">\(r\)</span>. Two vertical lines represent the MSE values of QuRating and NoRule.</figcaption>
</figure>

<figure id="fig:EvalA_IMDB_Single70B_dpp_vs_random_MSE">
<figure id="fig:EvalA_IMDB_Single70B_winning_rate">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single70B_winning_rate.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Single70B_dpp_vs_random_RC">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single70B_dpp_vs_random_RC.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figure id="fig:EvalA_IMDB_Single70B_dpp_vs_random_MSE">
<span class="image placeholder" data-original-image-src="figures/EvalA_IMDB_Single70B_dpp_vs_random_MSE.pdf" data-original-image-title="" width="1.0\linewidth"></span>
<figcaption></figcaption>
</figure>
<figcaption>(a) Winning rate of DPP-selected rules compared to QuRating’s four rules and the NoRule setting, based on MSE across 100 DPP trials. (b) Comparison of rule correlation between DPP-selected and randomly selected rules, averaged across 100 trials. (c) Comparison of MSE between DPP-selected and randomly selected rules, averaged across 100 trials.</figcaption>
</figure>

### Rule Generator Variation: Claude-3.5-Sonnet

To verify that GPT-4 is a reliable rule generator, we compare it with Claude-3.5-Sonnet. For each of the five tasks (General, IMDB, Medical, Math, Code), we prompt GPT and Claude to generate 100 rules for each, and then study the distribution of the rules. Specifically, we use Sentence-Transformer to generate the embedding vectors and then use PCA to project them onto the top two principal components for 2-dimensional visualization. From Figure  below, we observe that the two groups of rules generated by the two models completely overlap, demonstrating no distinct separation. This suggests that GPT-4 functions effectively as an unbiased rule generator.

<figure id="fig:rules_embeddings">
<figure id="fig:general_rules">
<span class="image placeholder" data-original-image-src="figures/PCA_plots/General_rules_embeddings.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>General Rules</figcaption>
</figure>
<figure id="fig:imdb_rules">
<span class="image placeholder" data-original-image-src="figures/PCA_plots/IMDB_rules_embeddings.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>IMDB Rules</figcaption>
</figure>
<figure id="fig:medical_rules">
<span class="image placeholder" data-original-image-src="figures/PCA_plots/Medical_rules_embeddings.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>Medical Rules</figcaption>
</figure>
<p></p>
<figure id="fig:math_rules">
<span class="image placeholder" data-original-image-src="figures/PCA_plots/Math_rules_embeddings.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>Math Rules</figcaption>
</figure>
<figure id="fig:code_rules">
<span class="image placeholder" data-original-image-src="figures/PCA_plots/Code_rules_embeddings.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>Code Rules</figcaption>
</figure>
<figcaption>Embeddings of rules generated by GPT and Claude across different domains.</figcaption>
</figure>

To further quantify the distribution differences, we studied the Wasserstein distance within and between the two rule sets. Specifically, we compute the distance between the GPT rules and Claude rules. Then we randomly split GPT rules into two parts and computed their Wasserstein distance and similarly for the Claude rules (averaged over 10 trials). By comparing these values (see Table ), we found no clear distribution bias when switching from one rule generator to the other.

<div id="tab:model_metrics">

|              | General | IMDB  | Medical | Math  | Code  |     |
|:-------------|:-------:|:-----:|:-------:|:-----:|:-----:|:---:|
| Intra-GPT    |  0.432  | 0.438 |  0.502  | 0.584 | 0.673 |     |
| Intra-Claude |  0.455  | 0.445 |  0.515  | 0.592 | 0.668 |     |
| Inter-Model  |  0.468  | 0.548 |  0.522  | 0.612 | 0.684 |     |

Comparison of Intra-Model and Inter-Model Metrics across Domains

</div>

Now we have compared the rule generators using GPT and Claude above. In order to address potential biases in rule generation by these large language models (LLMs) compared to human-generated rules, we prompt GPT-4 to generate 133 ethical and safety rules and compare the GPT-generated rules with the public and standard constitutions in (we make the rules all start from “Choose the response that" for a fair comparison). We asked 3 authors who have not seen the constitutions in to distinguish the rules blindly. We get an average accuracy of $20.7\%$, suggesting it is indeed hard to distinguish between rules generated by GPT and those designed by humans. All these discussions underscores the potential of GPT as a reliable rule generator that is capable of producing rules that are comparable to those crafted by human experts.

### Prompts and Generated Rules

**Comparison prompt:** Below is the template used to compare two data samples according to a specific rule. For the rule-free version, simply omit the sentence involving the rule. Replace DATASET_NAME with “IMDB reviews" or “Common Crawl data" to correspond to the two data sources discussed in Section .

<figure id="tab:rule_comparison_prompt">

<figcaption>Template of rule-based comparison prompt.</figcaption>
</figure>

**Generated IMDB rules:**

**Generated CommonCrawl rules:**

## Appendix for Evaluation B

### Model training

For training Pythia-1B and Llama3-8B, we loaded both models using `bfloat16` precision and used one `NVIDIA A100-80GB` for each training job. Below are the training parameters:

<div id="tab:Appendix-EvalB-training">

| **Model**            |    Pythia-1B     |    Llama3-8B     |
|:---------------------|:----------------:|:----------------:|
| **Num of epochs**    |        1         |        1         |
| **Batch size**       |        1         |        1         |
| **Learning rate**    | $2\cdot 10^{-5}$ | $2\cdot 10^{-5}$ |
| **Token max length** |       2048       |       4096       |
| **LoRA**             |        No        |  Yes (rank=64)   |

Comparison of Model Parameters

</div>

### More Baseline Methods

Here we add two more baseline methods: *LESS* : selecting data based on the estimated data influences, and *DiverseEvol* : an iterative sampling algorithm to ensure data diversity. It is important to note that *DiverseEvol* focuses solely on a single quality aspect: the diversity of data, while our method ensures diversity across multiple rating aspects. Another remark is that in the original papers, these methods were specifically used for instruction tuning data. We copy the three rule-based methods in our framework from for comparison purposes. From the results below, we see these two methods, while being computationally expensive, are not showing good performance under our experiment settings.

### Variance of Trials

Due to computational resource constraints, we were unable to perform multiple repetitions of all experiments. However, as mentioned in Section , we conducted 3 independent trials in four domains for *Uniform Sampling* and methods involving randomness in rule selections, including *GPT-Uncorrelated*, *Random 10 Rules*, and *DPP 10 Rules* (note that DPP sampling is also non-deterministic) to mitigate the effects of randomness, and we report their standard deviations here.

Here we perform the $t$-test to demonstrate that the advantage of *DPP 10 Rules* is significant compared to other methods. We include the $t$-statistics and $p$-values in the table. If we choose the significance threshold $p=0.05$, then we see that all the comparisons are significant.

<div id="tab:method_comparison_imdb_medical">

| Comparison              |          IMDB           |    Medical average     |
|:------------------------|:-----------------------:|:----------------------:|
| DPP vs GPT-Uncorrelated | $t=4.912$, $p=0.00881$  | $t=8.353$, $p=0.00408$ |
| DPP vs Uniform Sampling | $t=13.371$, $p=0.00086$ | $t=5.361$, $p=0.01218$ |
| DPP vs Random 10 Rules  | $t=5.054$, $p=0.02255$  | $t=4.877$, $p=0.01065$ |

Comparison of DPP method with other methods in IMDB and Medical AVG domains using Welch’s t-test.

</div>

<div id="tab:method_comparison_math_code">

| Comparison              |      Math average       |      Code average       |
|:------------------------|:-----------------------:|:-----------------------:|
| DPP vs GPT-Uncorrelated | $t=8.724$, $p=0.00293$  | $t=24.085$, $p=0.00005$ |
| DPP vs Uniform Sampling | $t=13.426$, $p=0.00099$ | $t=17.762$, $p=0.00195$ |
| DPP vs Random 10 Rules  | $t=5.166$, $p=0.02415$  | $t=5.557$, $p=0.02204$  |

Comparison of DPP method with other methods in Math and Code domains using Welch’s t-test.

</div>

### Evaluation Benchmarks

In this section, we provide detailed descriptions of the benchmarks utilized for our evaluation. We considered the following benchmarks for general continued pre-training: ARC-Challenge (15), Winogrande (15), MMLU (5), SST-2 (0), where the numbers in parenthesis indicate the number of shots we use in few-shot benchmark setting. For domain fine-tuning, we use zero-shot in IMDB, and 5-shot for Medical and Math (which uses subsets of MMLU). Moreover, Math and Medical domains, we use the subject-related subsets from MMLU, specifically ElementaryMathematics, HighSchoolMathematics, and CollegeMathematics for Math, and CollegeMedicine, ProfessionalMedicine, and MedicalGenetics for Medical. For Code, we tested code generation and for each code benchmark, we use the pass@k setting and specify the number of code generation samples. See detailed explanations below.

- **MMLU** : MMLU is a comprehensive multitask test comprises multiple-choice questions from a wide range of knowledge domains. It spans subjects across the humanities, social sciences, hard sciences, and other critical learning areas, encompassing 57 tasks such as elementary mathematics, US history, computer science, law, and more. To achieve high accuracy on this test, models need to demonstrate extensive world knowledge and robust problem-solving capabilities.

- **IMDB** : The IMDB dataset comprises 50,000 movie reviews and is designed for binary sentiment classification. For our evaluation, we select 25,000 test samples.

- **Winogrande** : WinoGrande is a collection of 44,000 problems inspired by the Winograd Schema Challenge. It has been adjusted to enhance scale and robustness against dataset-specific bias. Designed as a fill-in-the-blank task with binary options, WinoGrande requires users to select the correct option for a given sentence based on commonsense reasoning.

- **SST-2** : SST-2, or the Stanford Sentiment Treebank binary classification dataset, is a widely used resource for sentiment analysis tasks. Derived from movie reviews, it consists of 11,855 single sentences, each annotated for sentiment polarity.

- **ARC-Easy and ARC-Challenge** : The AI2’s Reasoning Challenge (ARC) dataset is designed for evaluating multiple-choice question-answering systems. It consists of science exam questions for grades 3 to 9 and is divided into two subsets: Easy and Challenge. The Challenge subset comprises more complex questions that necessitate advanced reasoning skills. Typically, questions offer four answer choices, although a small fraction (less than 1%) may present three or five options. The dataset also features a Knowledge Base (KB) containing 14.3 million unstructured text passages to support reasoning and answer generation.

- **HumanEval** : The HumanEval benchmark evaluates Python programming skills with 164 problems, each comprising a function signature, docstring, function body, and unit tests. In a zero-shot setting, models generate code using top-p sampling (p=0.95) until stop words are reached. Pass@k metrics (k=1, 10, 100) are calculated with n=200 samples per problem, estimating the success rate following Chen et al.’s approach. Success is determined by whether at least one solution is correct within k attempts, with temperature controlling randomness in generation. This benchmark measures model performance in solving programming tasks with increasing attempts.

- **MBPP** : The MBPP benchmark contains around 1,000 crowd-sourced Python programming problems, designed for entry-level programmers. Each problem includes a task description, a code solution, and 3 test cases. The evaluation is performed on the test set from index 11 to 511. In a few-shot setting, the InCoder-style prompt is used, where the task description and one solution are provided to guide the model. The prompt format is `f’"""{description}{test_example}"""’`. By default, `prompt_type_mbpp` is set to `incoder`, and optionally, the solution can be included using `include_solution_mbpp=True`. We use single generation per problem (pass@1), and for pass@k estimation, we generate n=15 samples per problem, similar to the HumanEval approach. The evaluation focuses on pass@1 success rates.

- **Multiple-py and Multiple-cpp** : MultiPL-E: is a benchmark for evaluating large language models for code generation that supports 18 programming languages. It takes the OpenAI “HumanEval" Python benchmark and uses little compilers to translate them to other languages. We use similar implementation as the original repository and evaluation parameters are similar to HumanEval.

### Number of selected rules

We modified the number of rules, $r$, from 10 to 20 and repeated the experiments for the Code domain. Compared to the 10-rule results presented in Table , we observed some discrepancies. For instance, the performance score on HumanEval is less than the 10-rule results, whereas the results for Multiple-cpp improved. The number of rules indeed alters the criteria used for data selection, thereby influencing the distribution of the selected data. Determining the optimal $r$ represents a valuable direction for future exploration.

### Size of Sampled Data

We investigated the impact of varying training data sizes on performance, specifically within the context of the *DPP 10 rules* and the Medical domain. Our observations reveal that increasing the amount of training data does not always enhance performance; in fact, performance may decline beyond a certain data threshold. This phenomenon is consistent with findings from the LIMA paper , which suggests that data quality is often more important than quantity for LLMs. Balancing data quality with quantity is another challenging but valuable topic.

### Distribution of Selected Data

Evaluating and contrasting the quality of data subsets selected by different methods is challenging and often necessitates extensive human intervention. To address this, the authors examined the initial 100 examples selected by each method. This examination revealed notable distinctions in the relevance and domain specificity of the data selected. Specifically, our DPP rule-based approach demonstrated a marked ability to identify and select examples that were highly pertinent to specific domains. For instance, in experiments focused on the Code domain, this method favored the inclusion of data containing code. In contrast, other less targeted methods, such as QuRating and Uniform Sampling, often yield selections that lack domain-specific relevance. This insight underscores the efficacy of using tailored, rule-based methods over generic ones for tasks where domain alignment is critical.

Although it is hard to compare the distribution of the selected data, we provide a visual representation in Figure  below, showcasing the meta-data (categories of the data samples) distributions for the Code domain as a representative example. Notably, the DPP methods with 10 and 50 rules tend to select more data from GitHub and StackExchange for Code fine-tuning.

Moreover for IMDB domain, in Figure  we investigated the text length distribution. We see that the QuRating is very close to the original SlimiPajama distribution, where we conjecture that in this case the data distribution is very close to uniformly sampled data. The methods within our framework have a tendency toward longer texts. Additionally, in Figure  we use bigram entropy (the Shannon entropy of the distribution over the unique bigrams) as an indicator of the text diversity. We again see that the entropy distribution of QuRating is very close to the original SlimPajama, where our methods generally select data with higher entropy/diversity and the entropy distributions are more concentrated.

<figure id="fig:data_distribution_Code_meta">
<figure>
<span class="image placeholder" data-original-image-src="figures/Code_data_distribution_plots/data_distribution_ft_10rules_dpp.png" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption>DPP 10 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/Code_data_distribution_plots/data_distribution_ft_10rules_randomC.png" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption>Random 10 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/Code_data_distribution_plots/data_distribution_ft_all50rules.png" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption>All 50 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/Code_data_distribution_plots/data_distribution_ft_4rules_QuRating.png" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption>QuRating</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/Code_data_distribution_plots/data_distribution_SlimPajama1M.png" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption>SlimPajama1M</figcaption>
</figure>
<figcaption>Comparison of meta-data distribution across different methods. The last is the original distribution of our source data.</figcaption>
</figure>

<figure id="fig:data_distribution_IMDB_length">
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_data_length_distribution_plots/data_length_distribution_ft_10rules_dpp.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>DPP 10 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_data_length_distribution_plots/data_length_distribution_ft_10rules_randomC.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>Random 10 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_data_length_distribution_plots/data_length_distribution_ft_all50rules.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>All 50 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_data_length_distribution_plots/data_length_distribution_ft_4rules_QuRating.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>QuRating</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_data_length_distribution_plots/data_length_distribution_SlimPajama1M.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>SlimPajama1M</figcaption>
</figure>
<figcaption>Comparison of text length distribution across different methods. The last is the original distribution of our source data.</figcaption>
</figure>

<figure id="fig:data_distribution_IMDB_entropy">
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_entropy_distribution_plots/entropy_distribution_ft_10rules_dpp.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>DPP 10 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_entropy_distribution_plots/entropy_distribution_ft_10rules_randomC.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>Random 10 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_entropy_distribution_plots/entropy_distribution_ft_all50rules.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>All 50 rules</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_entropy_distribution_plots/entropy_distribution_ft_4rules_QuRating.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>QuRating</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/IMDB_entropy_distribution_plots/entropy_distribution_SlimPajama1M.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>SlimPajama1M</figcaption>
</figure>
<figcaption>Comparison of text diversity distribution across different methods. The last is the original distribution of our source data.</figcaption>
</figure>

### Rule Correlation of selected rules

Here we provide in Table  the rule indices (in the range $\{0,1,\dots, 49\}$) for the rules selected by DPP and random selection (in one trial). The fullest of all generated $50$ rules for each domain is provided in . For each set of selected rules, we also calculate their rule correlation value $\rho$ (defined in ). We confirm that indeed DPP selects rules with lower rule correlation than random selected rules.

<div id="tab:Appendix-EvalB-RuleIndices">

| **Domain** |           **Method**            |              **Rule Indices**              |
|:----------:|:-------------------------------:|:------------------------------------------:|
|    IMDB    |   DPP 10 rules ($\rho=0.42$)    |  \[2, 3, 13, 21, 28, 36, 37, 45, 46, 49\]  |
|            | Random 10 rules A ($\rho=0.53$) |  \[2, 6, 10, 11, 15, 21, 28, 42, 43, 48\]  |
|            | Random 10 rules B ($\rho=0.51$) |  \[1, 9, 12, 14, 25, 26, 27, 37, 38, 40\]  |
|  Medical   |   DPP 10 rules ($\rho=0.55$)    |  \[1, 9, 10, 25, 29, 30, 32, 38, 42, 47\]  |
|            | Random 10 rules A ($\rho=0.69$) | \[11, 13, 14, 16, 25, 33, 34, 43, 45, 49\] |
|            | Random 10 rules B ($\rho=0.66$) | \[6, 17, 20, 28, 29, 37, 40, 41, 47, 48\]  |
|    Math    |   DPP 10 rules ($\rho=0.40$)    |  \[0, 4, 13, 26, 27, 31, 33, 38, 44, 45\]  |
|            | Random 10 rules A ($\rho=0.65$) |  \[0, 2, 11, 16, 17, 18, 27, 28, 34, 39\]  |
|            | Random 10 rules B ($\rho=0.61$) |  \[3, 4, 25, 20, 23, 13, 15, 24, 35, 39\]  |
|    Code    |   DPP 10 rules ($\rho=0.54$)    |  \[2, 3, 13, 21, 28, 36, 37, 45, 46, 49\]  |
|            | Random 10 rules A ($\rho=0.59$) |  \[5, 7, 10, 13, 17, 19, 21, 26, 30, 34\]  |
|            | Random 10 rules B ($\rho=0.58$) |  \[2, 4, 8, 14, 16, 20, 23, 33, 37, 44\]   |

Rule correlation and indices of the selected rules by DPP and random selection.

</div>

### GPT 10 uncorrelated rules

Another straightforward rule generation method is to directly prompt GPT-4 to generate 10 uncorrelated rules and rely on its understanding of the correlation between the rules. We have explored this by using a similar rule generation prompt as in , where we provide the task description and data description, but this time we request for 10 rules and added one sentence “make sure the rules are uncorrelated" to further require the independence of the rules. The 10 “uncorrelated" GPT rules are provided in Table  and below. Following this, we rated the data according to the 10 GPT rules and calculated the rule correlation $\rho$ of the score vectors (in one *GPT-Uncorrelated* trial). We tested for Code and Math domain and got $\rho_{Code} = 0.65$ and $\rho_{Math}=0.56$, both are significantly higher than DPP correlation values in Tabel . For Code, even random 10 rules selected from a pool of 50 rules provide lower correlation than the 10 rules directly generated by GPT that are claimed to be “uncorrelated". This shows that our two-step approach—first generating enough rules to ensure diversity, followed by employing DPP on the rating vectors to select rules—is superior and also more task-specific.

### Use GPT to select 10 uncorrelated rules

In this part, we discuss a very similar setting to the previous section. However, instead of directly prompting GPT to generate 10 rules, we let GPT to replace the role of DPP and select 10 “uncorrelated" rules out of the 50-rule pool. First, in Table  below, we calculate the rule correlation similarly as in Table . We see again although we prompt GPT-4 to select “uncorrelated" rules, the rule-correlation of the selected 10 rules are still higher than our DPP-selected rules in Table . Moreover, we fine-tuned with the selected data and benchmarked the LLM performance. From the results in Table  and Table , we again see that it underperforms compared to our method.

<div id="tab:Appendix-EvalB-RuleIndices-GPTselected10">

| **Domain** |              **Method**               |            **Rule Indices**             |
|:----------:|:-------------------------------------:|:---------------------------------------:|
|    IMDB    | GPT selected 10 rules ($\rho = 0.67$) | \[0, 1, 4, 10, 13, 17, 25, 31, 40, 49\] |
|    2-3     | GPT selected 10 rules ($\rho = 0.40$) | \[0, 4, 7, 11, 15, 24, 29, 34, 42, 49\] |
|    Math    | GPT selected 10 rules ($\rho = 0.56$) | \[0, 3, 7, 11, 17, 24, 28, 38, 44, 48\] |
|    Code    | GPT selected 10 rules ($\rho = 0.65$) | \[0, 4, 9, 12, 16, 23, 29, 34, 43, 49\] |

Rule correlation and indices of the selected rules by DPP and random selection.

</div>

### Prompts and Generated Rules

For brevity, we provide the templates for both the rule generation and rating prompts for the Math domain. To adapt these templates for other domains, replace terms specific to Math (such as “mathematical tasks” and “mathematical reasoning and analysis”) with relevant terminology from the desired domain. We use GPT-4 to help us generate these task description and data descriptions.

**Rule Generation Prompts:**

<figure id="tab:rule_generation_prompt">

<figcaption>Example of a rule-generation prompt used to create 50 data rating rules for the Math domain.</figcaption>
</figure>

**Rating Prompts:**

<figure id="tab:rule_rating_prompt">

<figcaption>Example of rule-rating prompt. Here we query the LLM to rate a single data sample based on a specific Math-related rule.</figcaption>
</figure>

**Task and data descriptions:**

<div id="tab: task_description">

|          **Type**           | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|:---------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| SlimPajama data description | The SlimPajama dataset is a large-scale dataset. It is designed to be a compact, high-quality dataset curated for pre-training large language models. The dataset includes a diverse range of texts, sourced from various domains such as web pages, books, and academic articles, providing a rich and varied training corpus for developing robust and versatile language models.                                                                                                                                                                                                                                                                                                                                                                             |
|    IMDB task description    | The IMDB review dataset, created by StanfordNLP, is a widely used dataset for sentiment analysis. It contains 50,000 highly polar movie reviews. Each review is labeled as either positive or negative, making it an ideal dataset for binary sentiment classification tasks. The dataset provides a challenging benchmark for evaluating the performance of sentiment analysis models.                                                                                                                                                                                                                                                                                                                                                                         |
|  Medical task description   | The MMLU (Massive Multitask Language Understanding) includes three medical-related subsets: mmlu_college_medicine, mmlu_medical_genetics, and mmlu_professional_medicine. These subsets test a language model’s understanding of general medical knowledge, genetic concepts, and advanced professional medical practices, respectively, through multiple-choice questions tailored to assess both foundational and specialized medical expertise.                                                                                                                                                                                                                                                                                                              |
|    Math task description    | The MMLU (Massive Multitask Language Understanding) includes a range of subsets designed to evaluate language models across various academic subjects, including mathematics. The Math subsets specifically assess a model’s capability to understand and solve mathematical problems. These are categorized into multiple difficulty levels—from elementary mathematics to college-level topics like abstract algebra. Each subset consists of multiple-choice questions that test different areas of mathematical knowledge, aiming to measure both basic arithmetic skills and more complex mathematical reasoning. This structure allows researchers to gauge a model’s proficiency in mathematical logic and its application to solve real-world problems. |
|    Code task description    | The Code Generation LM Evaluation Harness, part of the BigCode project, is a framework designed to evaluate large language models (LLMs) on their ability to generate code. It provides a structured environment to assess the performance of these models across various programming tasks and languages. The harness supports automated evaluation metrics and facilitates benchmark comparisons, making it a valuable tool for researchers and developers aiming to enhance the code generation capabilities of LLMs.                                                                                                                                                                                                                                        |

Data descriptions of SlimPajama and task descriptions of four domains.

</div>

**Generated 50 rules for each of four domains:** Note that the IMDB rules here are used to select data for LLM training, whereas the IMDB rules in are used for data comparison in order to eventually calculate quality scores for the $50$ IMDB reviews. Although similar, they are not the same set of rules.

[^1]: John A. Paulson School of Engineering and Applied Sciences, Harvard University. Email: xiaominli@g.harvard.edu

[^2]: Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology

[^3]: College of Information Sciences and Technology, The Pennsylvania State University

[^4]: School of Electrical and Computer Engineering, Princeton University

[^5]: McKelvey School of Engineering, Washington University in St. Louis

[^6]: Code of our experiments: https://anonymous.4open.science/r/DataSelection-F118/
