\begin{abstract}
Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): \textit{attention sinks}, \textit{value-state drains}, and \textit{residual-state peaks}, collectively referred to as \textit{extreme-token phenomena}. These phenomena are characterized by certain so-called ``sink tokens'' receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability.

We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures---transformers with one to three layers---trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an \textit{active-dormant mechanism}, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a \textit{mutual reinforcement mechanism}. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar \textit{active-dormant mechanism} as in the BB task, and that the \textit{mutual reinforcement mechanism} also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs. 

\blfootnote{Code for our experiments is available at~\url{https://github.com/GuoTianYu2000/Active-Dormant-Attention}}.
% These findings reveal that our simple model captures both the static and dynamic properties of extreme-token phenomena in pretrained LLMs. \sm{modify}

%Our results study the mechanisms behind extreme-token phenomena in both synthetic and real settings and offer potential mitigation strategies. 

% Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads follow a similar \textit{active-dormant mechanism} as in the BB task. We further demonstrate that the \textit{mutual reinforcement mechanism} also drives the emergence of extreme-token phenomena during LLM pretraining. \sm{Merge the two sentences.}

% We investigate the mechanisms behind three puzzling phenomena observed in transformer-based large language models (LLMs): \textit{attention sinks}, \textit{value-state drains}, and \textit{residual-state peaks}, collectively referred to the \textit{extreme-token phenomena}. \sm{Briefly describe what are these phenomenon, and what is the consequence.} 

% First, we demonstrate that these phenomena also arise in simpler architectures—transformers with one to three layers—trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an \textit{active-dormant mechanism} that causes attention heads to become attention sinks for certain domain-specific inputs while remaining non-sinks for others. We further develop a precise theoretical characterization of the training dynamics that lead to these phenomena, revealing that they are driven by a \textit{mutual reinforcement mechanism}. Using the theoretical insights, we demonstrate ways to avoid extreme-token phenomena during pretraining. 

% Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, revealing that many attention heads are governed by a similar active-dormant mechanism as in the BB task. We further show that the same mutual reinforcement mechanism drives the emergence of extreme-token phenomena during LLM pretraining. Our results study the mechanisms behind extreme-token phenomena in both synthetic and real settings and offer potential mitigation strategies. 

% Finally, we offer predictions on removing extreme-token phenomena by adjusting the model architecture and training dynamics, which we validate in the BB task. \tianyu{change it to emphases the interpretability}
\end{abstract}

% \tianyu{old version:}
% Both the training and inference phases in large language models (LLMs) exhibit mysterious phenomena which may affect the model's capability or stability. In this paper, we examine the root cause of three such phenomena which have been empirically shown to occur in trained transformers including LLMs: \textit{attention sinks}, \textit{small value states}, and \textit{massive norms}, which we refer to as \textit{outlier phenomena}. Specifically, we first investigate these phenomena in a toy setting, which we call the Bigram-Backcopy (BB) task, using small one- to three-layer transformers. We develop a precise theoretical characterization, verified through careful experiments, of when and how outlier phenomena occur, and the training dynamics which lead to them in this setting. We then show that our characterization of these phenomena extends to LLMs. We unveil a circuit and training dynamics, both essentially predicted by the BB task, showing how the outlier phenomena occurs in LLMs. In the process, we uncover a so-called \textit{switching mechanism} which ensures that attention heads are attention sinks when the input is text drawn from certain domains, while not being attention sinks for other inputs. Our results reveal the mechanisms behind the ubiquitous outlier phenomena in LLMs.

% We investigate the mechanisms behind three puzzling phenomena observed in transformer-based large language models (LLMs): \textit{attention sinks}, \textit{value-state sinks}, and \textit{residual-state peaks}, collectively referred to the \textit{extreme-token phenomena}. First, we demonstrate that these phenomena also arise in simpler architectures—transformers with one to three layers—trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an \textit{active-dormant mechanism} that causes attention heads to become attention sinks for certain domain-specific inputs while remaining non-sinks for others. We further develop a precise theoretical characterization of the training dynamics that lead to these phenomena, revealing that they are driven by a \textit{cyclic-reinforcement mechanism}. 

% Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, revealing that many attention heads are governed by similar active-dormant mechanism as in the BB task. We further show that the same cyclic-reinforcement mechanism drives the emergence of extreme-token phenomena during LLM pretraining. Finally, we offer predictions on how to mitigate extreme-token phenomena by adjusting the model architecture and training dynamics, which we validate in the BB task. 