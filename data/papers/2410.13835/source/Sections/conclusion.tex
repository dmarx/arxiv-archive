\section{Conclusions} \label{sec:conclusion}

In this work, we investigated the \textit{extreme-token phenomena}, specifically \textit{attention sinks}, \textit{value-state drains}, and \textit{residual-state peaks}. We analyzed simple transformers trained on the Bigram-Backcopy (BB) task, both theoretically and empirically, demonstrating that these models exhibit the same extreme-token phenomena observed in large language models (LLMs). Building on the insights from the BB task, we made several detailed predictions about the behavior of extreme-token phenomena in LLMs. In particular, we identified the \textit{active-dormant mechanism} governing attention heads in both the BB model and LLMs, with attention sinks and value-state drains serving as indicators of dormant phase, and a \textit{mutual reinforcement mechanism} that induces these phenomena during pretraining. Using insights from these mechanisms, we applied simple modifications to the model architecture and optimization procedure, effectively mitigating the extreme-token phenomena in the BB model. Overall, our work uncovers the underlying mechanisms of extreme-token phenomena and suggests potential pathways to mitigate these issues during LLM pretraining.

We believe the most compelling direction for future work is to explore whether eliminating the extreme-token phenomena is essential or beneficial for building powerful transformer-based LLMs. While it is possible to mitigate these phenomena through simple modifications to the architecture or training algorithms, it remains unclear whether their elimination significantly improves downstream tasks such as inference and quantization. Given the resource-intensive nature of pretraining large-scale LLMs, we anticipate that pretraining a model at the scale of GPT-2 could both provide valuable insight into this issue and help point the way to architectures that can reduce the pretraining burden. 

% We believe the most compelling direction for future work in this area is as follows. Specifically, one could build more performant and scalable modifications which would eliminate extreme-token phenomena and observe the effect on training dynamics and the pretrained model. This would make it easier to understand whether extreme token phenomena are necessary to build a powerful transformer-based LLM, whether they are merely helpful, or whether they are completely incidental to the particular architecture and optimization algorithms used by the community. 