\DP{write intro for this}

Causal compression algorithm: at sequence index \(t\) with final index \(T\), threshold out all but \(\approx \frac{1}{2}t\) or \(\frac{1}{10}t\) tokens. Non-causal compression algorithm: threshold out all but \(\approx \frac{1}{2}T\) or \(\frac{1}{10}T\) tokens. When \(T \gg t\) (i.e., infinite-streaming contexts), the former is more restrictive than the latter, and also may scale much better.

For all tasks we report the raw zero-shot accuracy as computed by EleutherAI's evaluation harness \citep{eval-harness}, at this \href{https://github.com/EleutherAI/lm-evaluation-harness/tree/cffc1bd3fd69453eaa75da891256682123226f0f}{permalink}.

\begin{table}[H]
    \centering
    \resizebox{0.99\textwidth}{!}{
        \begin{tabular}{@{}llllllllll@{}}
        \toprule
        Model     & Compression Scheme                          & Compression Ratio & ARC-C & ARC-E & HellaSwag & LAMBADA & OpenBookQA & PIQA  & Winogrande \\ \midrule
        Llama2-7B & No Compression                              & N/A               & 0.434 & 0.763 & 0.571     & 0.683   & 0.314      & 0.781 & 0.691      \\
                  & Adaptive Per-Layer Compression              & 50\%              & 0.435 & 0.763 & 0.571     & 0.683   & 0.314      & 0.781 & 0.691      \\
                  &                                             & 90\%              & 0.431 & 0.746 & 0.572     & 0.679   & 0.272      & 0.772 & 0.698      \\
                  & Adaptive Per-Head Compression               & 50\%              & 0.434 & 0.763 & 0.571     & 0.683   & 0.314      & 0.781 & 0.691      \\
                  &                                             & 90\%              & 0.416 & 0.734 & 0.567     & 0.681   & 0.28       & 0.766 & 0.706      \\
                  & Adaptive Per-Query-Token Causal Compression & 50\%              & 0.437 & 0.761 & 0.571     & 0.681   & 0.312      & 0.780 & 0.691      \\
                  &                                             & 90\%              & 0.404 & 0.720 & 0.546     & 0.643   & 0.268      & 0.763 & 0.658      \\ \bottomrule
        \end{tabular}
    }
    \caption{\textbf{Comparison of our attention head compression methods on zero-shot accuracy on standard benchmarks.} We observe that all compression methods preserve the performance of the base Llama2 model. The Per-Query-Token Causal Compression method performs the worst with 90\% of tokens compressed, but still preserves most of the performance and scales the best.}
    \label{tab:comparing_our_different_compression_methods}
\end{table}


\begin{table}[H]
    \centering
    \resizebox{0.99\textwidth}{!}{
        \begin{tabular}{@{}llllllllll@{}}
            \toprule
            Model     & Compression Scheme                          & Compression Ratio & ARC-C & ARC-E & HellaSwag & LAMBADA & OpenBookQA & PIQA  & Winogrande \\ \midrule \midrule
            Llama2-7B & No Compression                              & N/A               & 0.434 & 0.763 & 0.571     & 0.683   & 0.314      & 0.781 & 0.691      \\
                      & StreamingLLM Compression                    & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & StreamingLLM Causal Compression             & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & Adaptive Per-Query-Token Causal Compression & 50\%              & 0.437 & 0.761 & 0.571     & 0.681   & 0.312      & 0.780 & 0.691      \\
                      &                                             & 90\%              & 0.404 & 0.720 & 0.546     & 0.643   & 0.268      & 0.763 & 0.658      \\ \midrule
            Mistral-7B& No Compression                              & N/A               &       &       &           &         &            &       &            \\
                      & StreamingLLM Compression                    & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & StreamingLLM Causal Compression             & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & Adaptive Per-Query-Token Causal Compression & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\ \midrule
            Gemma-7B  & No Compression                              & N/A               &       &       &           &         &            &       &            \\
                      & StreamingLLM Compression                    & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & StreamingLLM Causal Compression             & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & Adaptive Per-Query-Token Causal Compression & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\ \midrule
            Pythia-7B & No Compression                              & N/A               &       &       &           &         &            &       &            \\
                      & StreamingLLM Compression                    & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & StreamingLLM Causal Compression             & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\
                      & Adaptive Per-Query-Token Causal Compression & 50\%              &       &       &           &         &            &       &            \\
                      &                                             & 90\%              &       &       &           &         &            &       &            \\ \bottomrule
        \end{tabular}
    }
    \caption{\textbf{Comparison of our adaptive causal attention compression algorithm against the previous one posed by StreamingLLM \citep{xiao2023efficient} on zero-shot accuracy on standard benchmarks.} We observe that our method succeeds most of the time in retaining performance, at least on these benchmarks, much better than StreamingLLM. This holds even if we do not force their mechanism to be causal.}
    \label{tab:compare_ours_streamingllm_zeroshot}
\end{table}



We also evaluate their perplexity on LAMBADA and the first row of the PG19 dataset (an extremely long document), chunked into \DP{INSERT HERE} subsets of \DP{INSERT HERE} contiguous tokens over which the perplexity is averaged.

\begin{table}[H]
    \centering
    \resizebox{0.99\textwidth}{!}{
        \begin{tabular}{@{}lllll@{}}
            \toprule
            Model     & Compression Scheme                          & Compression Ratio & LAMBADA Perplexity & PG19 Document 1 Perplexity \\ \midrule
            Llama2-7B & No Compression                              & N/A               & 4.13               &                            \\
                      & StreamingLLM Compression                    & 50\%              &                    &                            \\
                      &                                             & 90\%              &                    &                            \\
                      & StreamingLLM Causal Compression             & 50\%              &                    &                            \\
                      &                                             & 90\%              &                    &                            \\
                      & Adaptive Per-Layer Compression              & 50\%              & 4.12               &                            \\
                      &                                             & 90\%              & 4.16               &                            \\
                      & Adaptive Per-Head Compression               & 50\%              & 4.13               &                            \\
                      &                                             & 90\%              & 4.19               &                            \\
                      & Adaptive Per-Query-Token Causal Compression & 50\%              & 4.14               &                            \\
                      &                                             & 90\%              & 5.06               &                            \\ \bottomrule
        \end{tabular}
    }
    \caption{\textbf{Comparison of our attention-head compression algorithms against the previous one posed by StreamingLLM \citep{xiao2023efficient} on average perplexity on the LAMBADA task and the first document of the PG19 dataset.} We observe that our adaptive methods generally maintain significantly better perplexity than the previous StreamingLLM-derived methods on both short and long contexts.}
    \label{tab:compare_ourrs_streamingllm_ppl}
\end{table}