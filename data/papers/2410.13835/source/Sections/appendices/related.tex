\subsection{Related work}
Several studies independently identified the ``attention sink'' phenomenon in language models and vision transformers,  where attention weights were found to be concentrated on a few tokens \citep{xiao2023efficient, darcet2023vision, han2023lm, zhai2023stabilizing, elhage2023privileged, dettmers2022gpt3}. Recent research has provided more detailed characterizations of this attention pattern and the attention sink phenomenon \citep{fu2024attentionpattern, sun2024massive}. \citet{sun2024massive} attributed the attention sink to the massive activation of the hidden representations of the corresponding tokens. Both \citet{sun2024massive} and \citet{zhai2023stabilizing} discussed methods for mitigating the attention sink by modifying the model and training recipes. Additionally, recent studies have leveraged the attention sink phenomenon to develop improved quantization and more efficient inference algorithms \citep{liu2024intactkv, chen2024image, yu2024unveiling, son2024prefixing, lin2024duquant, bondarenko2023quantizable, hu2024outlier}. A concurrent work by \citet{gu2024attention} studied how optimization, data distribution, loss function, and model architecture in LM pre-training influence the emergence of attention sink, showing that replacing the softmax function with sigmoid can prevent attention sink emergence in models up to 1B parameters.

The dynamics of transformers are studied under various simplifications, including linear attention structures \citep{zhang2023trained,ahn2024transformers}, reparametrizations \citep{tian2023joma}, NTK \citep{deora2023optimization}, often in the setting of in-context linear regression \citep{ahn2023linear,wu2023many,zhang2024context} and structured sequences \citep{bietti2024birth,nichani2024transformers,tian2023scan}. Notably, \citet{zhang2023trained, huang2023context, kim2024transformers} demonstrate that a one-layer attention head trained via gradient descent converges to a model that effectively performs in-context regression. \cite{bietti2024birth} shows the fast learning of bigram memorization and the slow development of in-context abilities. \cite{tian2023scan} shows the scan and snap dynamics in reparametrized one-layer transformers. \cite{reddy2023mechanistic} simplifies the structure of the induction head, showing the connection between the sharp transitions of in-context learning dynamics and the nested nonlinearities of multi-layer operations.

Mechanistic interpretability is a growing field focused on understanding the internal mechanisms of language models in solving specific tasks \citep{elhage2021mathematical, geva2023dissecting, meng2022locating, nanda2023progress, olsson2022context, bietti2024birth, wang2022interpretability, feng2023language, todd2023function}. This includes mechanisms like the induction head and function vector for in-context learning \citep{elhage2021mathematical, olsson2022context, todd2023function, bietti2024birth}, the binding ID mechanism for binding tasks \citep{feng2023language}, association-storage mechanisms for factual identification tasks \citep{meng2022locating}, and a complete circuit for indirect object identification tasks \citep{wang2022interpretability}. The task addressed in this paper is closely related to \cite{bietti2024birth}, who explored synthetic tasks where tokens are generated from either global or context-specific bigram distributions. Several other studies have also employed synthetic tasks to explore neural network mechanisms \citep{charton2022my, liu2022towards, nanda2023progress, allen2023physics, zhu2023physics, guo2023transformers, zhang2022unveiling, lin2023transformers}. 

A line of work focuses on quantizing neural networks using low-bit fixed-point representations \citep{jacob2018quantization,zafrir2019q8bert,lin2020towards,nagel2021white,gholami2022survey}, such as \texttt{INT8} \citep{lin2020towards,dettmers2022gpt3} or \texttt{INT4} \citep{yao2206efficient,wu2023understanding,dettmers2023case} to save memory usage and computational cost. In LLMs, the extreme-token phenomena lead to substantial performance degradation after quantization \citep{bondarenko2021understanding} and have become a key focus of recent research \citep{fan2020training,yao2022zeroquant,lin2024duquant,hu2024outlier}. \citet{dettmers2022gpt3} and \citet{lin2024awq} propose mixed-precision approaches, using \texttt{FP16} for outlier values and \texttt{INT8} for others, enabling large model quantization without performance loss. \citet{xiao2023smoothquant} rescales the weights and activations to reduce magnitudes of outliers, and \citet{bondarenko2023quantizable} proposes modified attention structures to remove outliers, making language models easier to quantize. 


We note that \citet{gurnee2024universal} proposed Attention Deactivation Neurons, \citet{bondarenko2023quantizable} proposed the ``no-op'' hypothesis, and \citet{xiao2023efficient} proposed the ``dump unnecessary attention'' conjecture as mechanisms of attention sinks. In contrast, we explain the extreme-token phenomena through the active-dormant and mutual reinforcement mechanisms, offering the proof of their emergence within training dynamics in a toy model and providing empirical evidence of these mechanisms in LLMs.



% For a positive integer \(n\) let \([n]\) be the set \(\{0, 1, \dots, n - 1\}\).\footnote{Throughout this paper, we will use zero-indexing, in order to keep consistency between code and math.} For a matrix \(\bX\), we denote its \(i^{\mathrm{th}}\) column by \(\bx_{i}\), its \((i, j)^{\mathrm{th}}\) entry by \(X_{ij}\), and its transpose by \(\bX^{\top}\).

% \begin{itemize}
% \item Methods: \cite{ramachandran2017searching}
% \end{itemize}


