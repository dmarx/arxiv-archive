\section{Proofs of Theorem~\ref{thm:construction} and~\ref{thm:main}}
\label{sec:proof}
% \subsection{More Detailed Notation}\label{sub:app_notation}

% As our investigation will become more detailed and in-depth, we introduce a more detailed notation system than that which was provided in \Cref{sub:prelim}.

% \paragraph{Basic notation.} For a finite set \(\gX\), we denote the set of probability distributions on \(\gX\) as \(\Delta(\gX)\). For two matrices \(\bX, \bY\) of the same size, we denote their element-wise product by \(\bX \odot \bY\).

% \paragraph{Notation for our setup.} We consider a transformer-based autoregressive language model as a function \(\mathrm{LM} \colon \gV^{N} \to \Delta(\gV)\), where \(\gV\) is the vocabulary and \(N\) is the sequence length. Here \(\Delta(\gV)\) denotes the estimated probability distribution of token \(N\) given tokens \(0\) through \(N - 1\). An autoregressive language model is composed of a \textit{tokenizer} \(\mathrm{Tok} \colon \gV^{N} \to \R^{D \times N}\), where \(D\) is the token dimension, a \textit{transformer} neural network \(\TF \colon \R^{D \times N} \to \R^{D \times N}\), and a \textit{classification head} \(\mathrm{Cls} \colon \R^{D \times N} \to \Delta(\gV)\), such that 
% \begin{equation}
%     \mathrm{LM} = \mathrm{Cls} \circ \TF \circ \mathrm{Tok}.
% \end{equation}
% In this paper, we focus mainly on the transformer component of the language model. As a deep neural network, we can write the transformer as a composition of \(L\) layers, i.e., 
% \begin{equation}
%     \TF = \mathrm{Layer}^{L - 1} \circ \cdots \circ  \mathrm{Layer}^{0}.
% \end{equation}
% Each layer (\(\mathrm{Layer}^{\ell} \colon \R^{D \times N} \to \R^{D \times N}\)) can be further decomposed into a \textit{self-attention block} (\(\Attn^{\ell} \colon \R^{D \times N} \to \R^{D \times N}\)) and
% a \textit{multi-layer perceptron} (\(\MLP^{\ell} \colon \R^{D \times N} \to \R^{D \times N}\)), defined as follows. Let \(D_{\sf attn} \doteq D / K\), and let \(\phi \colon \R^{N \times N} \to \R^{N \times N}\) be the column-wise softmax, i.e., 
% \begin{equation}
%     \phi(\mat{C})_{ij} = \frac{e^{C_{ij}}}{\sum_{i^{\prime} = 0}^{N - 1}e^{C_{i^{\prime}j}}}, \qquad \forall \mat{C} \in \R^{N \times N}.
% \end{equation}
% Let \(\mathrm{RoPE}^{\ell} \colon \R^{D_{\sf attn} \times N} \to \R^{D_{\sf attn} \times N}\) be the rotary positional embedding \citep{su2024roformer} at layer \(\ell\). Then we have
% \begin{align}\label{eq:attn_def}
%     \Attn^{\ell}(\bH) 
%     &\doteq \bH + \sum_{k = 0}^{K - 1}\bO_{k}^{\ell}(\bV_{k}^{\ell}\bH)\bA_{k}^{\ell}(\bH), \qquad \forall \bH \in \R^{D \times N} \\ 
%     \text{where} \quad  \bA_{k}^{\ell}(\bH) 
%     &\doteq \phi\left(\frac{\mathrm{RoPE}^{\ell}(\bK_{k}^{\ell}\bH)^{\top}\mathrm{RoPE}^{\ell}(\bQ_{k}^{\ell}\bH)}{\sqrt{D_{\sf attn}}}\right), \qquad \forall \bH \in \R^{D \times N},
% \end{align}
% where \(\bQ_{k}^{\ell}, \bK_{k}^{\ell}, \bV_{k}^{\ell} \in \R^{D_{\sf attn} \times D}, \bO_{k}^{\ell} \in \R^{D \times D_{\sf attn}}, \theta^{\ell} \in \R\) are parameters of the network for each \(k \in [K], \ell \in [L]\).

% Now, let \(\silu \colon \R^{D_{\sf mlp} \times N} \to \R^{D_{\sf mlp} \times N}\) be the element-wise SiLU function \citep{ramachandran2017searching}. Then we have
% \begin{align}\label{eq:mlp_def}
%     \MLP^{\ell}(\bH) 
%     &\doteq \bH + \Wdown^{\ell} (\silu(\Wgate^{\ell}\bH + \bgate^{\ell}\mathbf{1}^{\top}) \odot (\Wup^{\ell}\bH + \bm{e}p^{\ell}\mathbf{1}^{\top})) \\ 
%     &\qquad\ + \bdown^{\ell}\mathbf{1}^{\top},\qquad \forall \bH \in \R^{D \times N}.
% \end{align}
% Here, \(\Wup^{\ell}, \Wgate^{\ell} \in \R^{D_{\sf mlp} \times D}, \bm{e}p^{\ell}, \bgate^{\ell} \in \R^{D_{\sf mlp}}, \Wdown^{\ell} \in \R^{D \times D_{\sf mlp}}, \bdown^{\ell} \in \R^{D}\) are network parameters for each \(\ell \in [L]\). Each layer then just writes 
% \begin{equation}
%     \mathrm{Layer}^{\ell} = \MLP^{\ell} \circ \Attn^{\ell}
% \end{equation}
% Let the \textit{input} to layer \(\ell\) be \(\bH^{\ell} = [\bh_{1}^{\ell}, \dots, \bh_{N}^{\ell}] \in \R^{D \times N}\), and the \textit{input} to the MLP block in layer \(\ell\) be \(\bH_{+}^{\ell} = [\bh_{+1}^{\ell}, \dots, \bh_{+N}^{\ell}] \in \R^{D \times N}\), so that layer \(\ell\) maps \(\bH^{\ell}\) to \(\bH^{\ell + 1}\), like so:
% \begin{equation}
%     \mathrm{Layer}^{\ell} \colon \bH^{\ell} \xrightarrow{\hspace{1em}\Attn^{\ell}\hspace{1em}} \bH_{+}^{\ell} \xrightarrow{\hspace{1em}\MLP^{\ell}\hspace{1em}} \bH^{\ell + 1}.
% \end{equation}
% In this notation, observations in previous works can be written succintly as follows.
% \begin{itemize}
%     \item The ``massive activation'' phenomenon in \citet{sun2024massive} is that some hidden states \(H_{ij}^{\ell}\) have very large magnitudes, i.e., \(\abs{H_{ij}^{\ell}} \gg 1\), for layers \(\ell \geq 2\).
%     \item The ``attention sink'' phenomenon in \citet{xiao2023efficient} is that \(\sum_{i = 0}^{N - 1}A_{k}^{\ell}(\bH^{\ell})_{i 0} \approx 1\), for some \((k, \ell) \in [K] \times [L]\).
% \end{itemize}


% \sm{In writing the statement of Theorem/Proposition/Lemma, we hope the statement (together with the few paragraphs before the statement) is as self-contained as possible. Do the following: (1) For important quantities inside the Theorem and Lemma (and proof), refer to the equation (add labels if it was not labeled) where the quantities are defined. Following this convention makes it easy for writers to find undefined notations. It also makes readers easier to follow the statement. (2) If there is any assumption that is inherited from the higher level Theorem to be proved, write in the statement that "follow the assumptions of Theorem ---ref---"; if not all assumptions are used from the Theorem, write in the statement that "Let Assumption xxx holds" or "Assume xxx". }

% \sm{Do the following in writing all proofs: (1) Use [Proof of Theorem/Lemma/Proposition~---ref---] after every begin proof, even if the begin proof is stated immediately after the statement of the theorem, or inside a section with section title named "Proof of Theorem/Lemma/Proposition xxx". This makes readers clearer on which theorem is proving. (2) Add "This finishes the proof of Theorem --ref--" at the end of the proof. The link makes it easier for writers and readers to check whether the theorem is actually proved. (3) Add "The proof of Theorem ---ref-- can be found in Section ---ref---". This makes the writers and readers easier to go back and forth between the theorem and the proof. }

% \sm{More details in writing proofs: (1) The comma and full stops are important. The way to use them: try to read out the sentence (with the equations) you wrote, and use comma and full-stop wherever there should be; (2) Important equations should give a number and a label. When wrote "combining the above equations", it is better to write "combining equations (xx), (xx), and (xx)". Give more links could make it easier for writers to ensure the notation consistency. (3) At important locations (statement of theorem or the definitions of things before the statement), it is better the write the dimension of vectors and matrices ($\boldsymbol{x} = x \bm{1}_D \in \R^D$), and the input and output domain of functions $f: \mathcal{X} \to \R^D$. This makes it easier for writers to find bugs in proofs. (4) When referring to something important (and possibly appeared in a place that is far away from the current location), use both plain languages and math notations (the attention logits $\alpha$ grows). This makes readers easier to follow. }


We introduce new notations that are frequently used in the proofs. Recall that in Eq.~\eqref{eqn:total_loss}, we used $\{\pi_v\}_{v \in \vocab}$ to denote the stable distribution across all tokens. We further define the stable distribution excluding trigger tokens as follows:
\begin{equation}\label{appeqn:stable-dist}
\Tilde{\bm{\stable}}\in \R^\vocabsize,~~~\Tilde{\stable}_i=\stable_i \bm{1}\{i\in\vocab\setminus\cT\}.
\end{equation}
Section~\ref{sec:simple-model} defines the bigram transition probability in the Bigram-Backcopy task as $\transition_{\tok\tokk}=\sf{P}(\tokk\mid\tok)$.
We further define the bigram transition probability matrix as %\sm{Link to the previous definition of this in main text. Corresponding to xxxx}
\begin{equation}\label{appeqn:P-matrix}
\Transition = \left(\begin{matrix}
\transition_{11} & \ldots & \transition_{1\vocabsize}\\
\vdots & \ddots & \vdots \\
\transition_{\vocabsize 1} & \ldots & \transition_{\vocabsize\vocabsize}\\
\end{matrix}\right) = \left(\begin{matrix}
\bm{\transition}_1^\top\\
\vdots \\
\bm{\transition}_\vocabsize^\top\\
\end{matrix}\right).
\end{equation}
Given a token $\tok$, define the predicted probability at token $\tok$ as the logit output passed through the softmax activation. Let $\bH=[\bos; v_{1:n-1}; v]$. Using the form of $\TF(\bH)_n$ defined in Eq.~\eqref{eqn:q}, we denote %\sm{Link to the previous definition of this in main text. Corresponding to xxxx} \tianyu{Re-define $q$. It is the output corresponds to the simplified loss}
\begin{equation}\label{appeqn:pred-prob}
\bm{\ppred}_{\tok} = \softmax(\TF(\bH)_n)=(\ppred_{\tok 1},\ldots,\ppred_{\tok \vocabsize}),~~~\text{with}~~~\ppred_{\tok \toki}=\frac{\transition_{\tok\toki} \exp\Big[\frac{\mass_\toki\xi_\toki+e^{\sink}\ivalue_\toki}{e^{\sink}+\mass}\Big]}{\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk}\exp\Big[\frac{\mass_\tokk\xi_\tokk+e^{\sink}\ivalue_\tokk}{e^{\sink}+\mass}\Big]}.
\end{equation}
Similar to Eq.~\eqref{appeqn:P-matrix}, we define the full output probability matrix as
% \sm{$\Ppred$ notation collision}, 
\begin{equation}\label{appeqn:Q-matrix}
\Ppred = \left(\begin{matrix}
\ppred_{11} & \ldots & \ppred_{1\vocabsize}\\
\vdots & \ddots & \vdots \\
\ppred_{\vocabsize 1} & \ldots & \ppred_{\vocabsize\vocabsize}\\
\end{matrix}\right) = \left(\begin{matrix}
\bm{\ppred}_1^\top\\
\vdots \\
\bm{\ppred}_\vocabsize^\top\\
\end{matrix}\right).
\end{equation}
Using the notation $\bm{\ppred}_\tok$ and $\Tilde{\stable}_\tok$, we can rewrite the loss functions defined in Eq.~\eqref{eqn:loss_single} and Eq.~\eqref{eqn:total_loss} as follows:
\begin{equation}\label{appeqn:loss}
\loss_\tok(\sink_\tok,\vecvalue) = -\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} \log \ppred_{\tok\tokk},~~~~\loss_\tok(\vecsink,\vecvalue) = \sum_{\tok=1}^\vocabsize \Tilde{\stable}_\tok \loss_\tok (\sink_\tok,\vecvalue).
\end{equation}
We always have that $\sum_{\tokk}\transition_{\tok\tokk}=1$ and $\sum_{\tokk}\ppred_{\tok\tokk}=1$. The total variation norm and KL-divergence are then defined as:  
\begin{equation}\label{appeqn:kl-divergence}
\| \bm{\transition}_\tok - \bm{\ppred}_\tok\|_{\text{TV}} = \sum_{\tokk} |\transition_{\tok\tokk}-\ppred_{\tok\tokk}|,~~~~\text{KL}(\bm{\transition}_\tok~||~\bm{\ppred}_\tok) = -\sum_{\tokk} \transition_{\tok\tokk} \log(\ppred_{\tok\tokk}/\transition_{\tok\tokk}).
\end{equation}
Given any vector $\bm{u}=[u_1;\ldots;u_d]$, define the corresponding diagonal matrix as
\[
\diag(\bm{u}) = \left(\begin{matrix}
u_{1} & 0 & \ldots & 0\\
\vdots & \ddots &  & \vdots \\
\vdots & & \ddots & \vdots \\
0 & \ldots & 0 & u_d \\
\end{matrix}\right).
\]
Given any $\bm{\transition}_\tok$ defined in Eq.~\eqref{appeqn:P-matrix}, denote 
\begin{equation}\label{appeqn:g-probs}
\gppred_\tok^{\Transition} = \diag(\bm{\transition}_\tok) - \bm{\transition}_\tok \bm{\transition}_\tok^\top, \quad \gppred_\tok^{\Ppred} = \diag(\bm{\ppred}_\tok) - \bm{\ppred}_\tok \bm{\ppred}_\tok^\top.
\end{equation}
% Denote \sm{refer to}
% \begin{equation}\label{appeqn:z}
% \bm{z} = \mass \cdot \vecvalue - \bm{\mass} \circ \bm{\xi}.
% \end{equation}
We now present technical lemmas concerning $\gppred_\tok^{\Transition}$ and $\gppred_\tok^{\ppred}$.
\begin{lemma}\label{appthm:positive-definite}
The matrices $\gppred^\Transition_\tok \in \R^{V \times V}$ and $\gppred^\Ppred_\tok \in \R^{V \times V}$ are positive semi-definite for any $\tok \in \vocab$. 
\end{lemma}
\begin{proof}[Proof of \Cref{appthm:positive-definite}]
Since $\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} = 1$ and $\sum_{\tokk=1}^\vocabsize \ppred_{\tok\tokk} = 1$ for any $\tok$, we have that
\begin{align*}
(\gppred^\Transition_\tok)_{\toki\toki}=\transition_\toki - \transition_{\toki}^2 & = \transition_\toki(\sum_{\tokk\neq\toki} \transition_\tokk) \geq \sum_{\tokk\neq\toki} |(\gppred^\Transition_\tok)_{\toki\tokk}|, \\
(\gppred^\Ppred_\tok)_{\toki\toki}=\ppred_\toki - \ppred_{\toki}^2 & = \ppred_\toki(\sum_{\tokk\neq\toki} \ppred_\tokk) \geq \sum_{\tokk\neq\toki} |(\gppred^\Ppred_\tok)_{\toki\tokk}|.
\end{align*}
This shows that both $\gppred^\Transition_\tok$ and $\gppred^\Ppred_\tok$ are diagonally dominant matrices. By Corollary 6.2.27 in \citet{horn2012matrix}, they are positive semi-definite.
\end{proof}
\begin{lemma}\label{appthm:min-eigenvalue}
Suppose that $\Tilde{\stable}_\tok > 0$ for any $\tok\in\vocab\setminus \cT$. For any $\bm{\eta} \in \R^\vocabsize$ with $\bm{\eta} \perp \bm{1}$, there exists $\omega>0$ such that
\[
\bm{\eta}^\top \Big[ \sum_{\tokk=1}^\vocabsize \Tilde{\stable}_\tokk \gppred_{\tokk}^{\Transition} \Big] \bm{\eta} \geq \omega \|\bm{\eta}\|_2^2.
\]
\end{lemma}
\begin{proof}[Proof of Lemma \ref{appthm:min-eigenvalue}]
Denote the null spaces of $\gppred_{\tok}^{\Transition}$ for $\tok \in \vocab$ as $\sf S_\tok$. We solve for each $\sf S_\tok$. Setting $\gppred_{\tok}^\Transition \bm{\eta}=0$ gives that 
\[
[\transition_{\tok\tokj} - \transition_{\tok\tokj}(\sum_{\tokk} \transition_{\tok\tokk})] \eta_\tokj = 0~\text{for any $\tokj\in\vocab$.}
\]
If $\transition_{\tok\tokj}\neq 0$, we divide each side with $\transition_{\tok\tokj}$ and get that $\eta_\tokj = \sum_{\tokk} \transition_{\tok\tokk} \eta_\tokk$. As a result, we get that 
\[
\text{\sf{S}}_\tok = \set{\bm{\eta}\mid \eta_\tokj\text{ is constant for } \transition_{\tok\tokj}\neq0}.
\]
Since all ${\stable}_\tokk > 0$, for any $\tokk\in\vocab\setminus \cT$, there is $\tok\in\vocab\setminus\cT$ such that $\transition_{\tok\tokk} > 0$, we get that
\[
\cap_{\tok\in\vocab\setminus\cT} \text{\sf{S}}_\tok = \set{c \cdot \bm{1}\mid c\in\R}.
\]
Since $\bm{\eta} \perp \bm{1}$, we get that $\bm{\eta} \perp \cap_{\tok\in\vocab\setminus\cT} \sf{S}_\tok$. We denote the minimal non-zero eigenvalues of $\gppred_{\tok}^\Ppred$ for $\tok\in\vocab\setminus\cT$ as $\lambda$. We get that
\[
\bm{\eta}^\top \Big[ \sum_{\tokk=1}^\vocabsize \Tilde{\stable}_\tokk\gppred_{\tokk}^{\Transition} \Big] \bm{\eta} \geq \Big[\min_{\tok \in \vocab\setminus \cT} \Tilde{\stable}_\tok\Big] \lambda \|\bm{\eta}\|_2^2.
\]
Setting $\omega = \lambda \cdot \min_{\tok \in \vocab\setminus \cT} \Tilde{\stable}_\tok>0$, this proves Lemma \ref{appthm:min-eigenvalue}.
\end{proof}

\begin{lemma}\label{appthm:min-eigenvalue-q}
Given $\omega$ defined in Lemma \ref{appthm:min-eigenvalue}, 
suppose that 
\begin{equation}\label{appeqn:error-q}
\max_{\tok,\tokk} \abs{\transition_{\tok\tokk}-\ppred_{\tok\tokk}} = \delta \leq \min\set{\omega/(6\vocabsize), 1}.
\end{equation}
For any $\bm{\eta} \in \R^\vocabsize$ with $\bm{\eta} \perp \bm{1}$, we have that
\[
\bm{\eta}^\top \Big[ \sum_{\tokk=1}^\vocabsize \Tilde{\stable}_\tokk \gppred_{\tokk}^{\Ppred} \Big] \bm{\eta} \geq \frac{\omega}{2} \|\bm{\eta}\|_2^2.
\]
\end{lemma}
\begin{proof}[Proof of Lemma \ref{appthm:min-eigenvalue-q}]
Denote $\delta = \max_{\tok,\tokk} \abs{\transition_{\tok\tokk}-\ppred_{\tok\tokk}}$. Suppose that $\delta \leq 1$.
For any $\tokk\in \vocab\setminus\cT$, we can verify that
\[\Big|(\gppred_\tokk^\Transition)_{ij}-(\gppred_\tokk^\Ppred)_{ij} \Big|\leq 3 \delta,\]
for any $\toki,\tokj\in [\vocabsize]$.
We denote 
\[
\bE = \sum_{\tokk=1}^\vocabsize \Tilde{\stable}_{\tokk} \gppred_\tokk^\Transition - \sum_{\tokk=1}^\vocabsize \Tilde{\stable}_{\tokk} \gppred_\tokk^\Ppred.
\]
Therefore, $|\bE_{\toki\tokj}|\leq 3\delta$ for any $\toki,\tokj \in [\vocabsize]$.
This means that
\[
\bm{\eta}^\top \bE \bm{\eta} \leq \|\bE\|_2 \|\bm{\eta}\|_2^2 \leq \|\bE\|_{F} \|\bm{\eta}\|_2^2 \leq V\cdot 3\delta \cdot \|\bm{\eta}\|_2^2.
\]
As a result, when $\delta \leq \min\set{\omega/(6\vocabsize), 1}$, we get that
\[
\bm{\eta}^\top \Big[ \sum_{\tokk=1}^\vocabsize \Tilde{\stable}_\tokk \gppred_{\tokk}^{\Ppred} \Big] \bm{\eta} \geq \omega \|\bm{\eta}\|_2^2 - \bm{\eta}^\top \bE \bm{\eta} \geq \frac{\omega}{2} \|\bm{\eta}\|_2^2.
\]
This proves Lemma \ref{appthm:min-eigenvalue-q}.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:construction}}\label{app:proof-construction}

% \sm{Give a formal statement of Theorem 1. Assume the assumption A. Furthermore, instead of saying generate the ground truth dynamics of the BB task, write down the mathematical equations.}


We denote the hidden dimension as $d$ and the sequence length as $N$. Recall that the token $\tok$ at position $\toki$ is encoded as $\embd_\toki(\tok)$. We begin with the assumption regarding the transformer's embedding dimension:
\begin{assumption}\label{ass:linear_indp}
We have $\set{\embd_0(\bos)} \cup \set{\embd_\toki(\tok)}_{\toki\in\{ 0 \} \cup [N-1],\tok\in\vocab} \subseteq \R^d$, where the embedding dimension  $d\geq \vocabsize N + 1$. % are linearly independent. There exists an orthogonal basis $\set{\bm{e}_{\tok}}_{\tok\in\vocab} $ that is in the null space of the span of $\set{\embd_\toki(\tok)}_{\toki\in[N],\tok\in\vocab}$.
\end{assumption}
% \begin{assumption}\label{ass:linear_indp}
% The vectors $\set{\embd_\toki(\tok)}_{\toki\in[N],\tok\in\vocab}$ are linearly independent. There exists an orthogonal basis $\set{\bm{e}_{\tok}}_{\tok\in\vocab} $ that is in the null space of the span of $\set{\embd_\toki(\tok)}_{\toki\in[N],\tok\in\vocab}$.
% \end{assumption}
Assumption~\ref{ass:linear_indp} requires a large embedding dimension $d\geq \vocabsize N +  1$.  This assumption is used to ensure that there are enough orthonormal bases in the embedding space. Given the fact that there are $\text{O}(\exp(d))$ approximately linearly independent vectors for large $d$ \citep{vershynin2018high}, it is possible to relax the assumption to be $d \gg \log(\vocabsize N)$. However, since Assumption~\ref{ass:linear_indp} pertains only to the construction of $\lambda$ for trigger tokens and is unrelated to Theorem~\ref{thm:main}, we adopt it to simplify the proof of Theorem~\ref{thm:construction}.


\begin{theorem}[Formal statement of Theorem \ref{thm:construction}] \label{appthm:formal-contruct} Let Assumption \ref{ass:linear_indp} hold. For any parameters $(\vecsink \in \R^{V}, \vecvalue \in \R^V, \bm{\xi} \in \R^V, \lambda \in \R)$, there exists a one-layer transformer (\ref{eqn:simplified_transformer}) with weight matrices $(\bQ, \bK, \bV, \bW_1, \bW_2)$ such that Eq. (\ref{eqn:simplification_TF_1}), (\ref{eqn:simplification_TF_2}), and (\ref{eqn:simplification_TF_3}) hold. Consider the Bigram-Backcopy task, where given an input $\bH = [\bos; \tok_{1:n-1}, \tok]$, the ground-truth transition gives ${\sf P}(\tok^\prime\mid \bH) = \transition_{\tok \tok^\prime}$ for $\tok\in\vocab\setminus\cT$, and ${\sf P}(\tok^\prime\mid \bH ) = \indic{\tok^\prime=\tok_{n-1}}$ for $\tok \in \cT$. 
There exists a sequence $\min_{\tok\in\vocab} \sink_\tok \to \infty$, $\min_{\tok\in\vocab} \xi_\tok \to \infty$, $\lambda\to\infty$, and $\vecvalue=\bm{0}$ such that this transformer generates the ground-truth transition in the limit, i.e., 
\begin{equation}\label{eqn:TF_ground_truth_match}
\softmax(\TF(\bH)_n) \to {\sf P}(\, \cdot\, | \bH).
\end{equation}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{appthm:formal-contruct}] ~
%  We concatenate the one-hot encoding $\bm{e}_\tok$ with $d-\vocabsize$ zeros, augmenting it to $\bm{e}_\tok\in\R^d$. Define the positional embeddings as $\Pos_\toki = [\bm{0}_{\vocabsize+1}; \br_\toki]$ for $\toki=0,\ldots,N$, with $\br_\toki \in \R^{d-\vocabsize-1}$ The input of the attention layer becomes 
% \[
% \embd(\tok_\toki) = \bm{e}_{\tok_\toki} + \Pos_\toki.
% \] 
% We define a matrix $\bA\in \R^{d\times d}$ such that $\bA\Pos_{\toki} = \Pos_{\toki-1}$ for $\toki\in[N]$ and $A \bm{e}_\tok=0$ for any $\tok\in\vocab$. Since $\{\bm{e}_\tok,\Pos_{\toki}\}_{\tok\in\vocab,\toki\leq N}$ is linear independent, the existence is guaranteed. Similarly, we define a matrix $\bB \in \R^{d\times d}$ such that $\bB \embd(\tok_{\toki}) = \bm{0}$ for any $\tok_\toki\in\cT$, $\bB \bm{e}_\tok = 0$ for any $\tok\in\vocab\setminus \cT$, and $(\lambda-1) \Pos_{\tokj}^\top \bB \Pos_{\toki} + \sink \Pos_{\tokj}^\top \bB \bm{e}_{\bos}=0$. \tianyu{check}
% Define the value, key, and query matrices as 
% \[
% V = \sum_{\tok\in\vocab\setminus\cT}
% \]

% Then, we have that 
% We augment the one-hot encoding $\bm{e}_\tok$ to $\R^d$, concatenating it with $d-\vocabsize$ zeros. 

\noindent
{\bf Step 1. Construction for the attention head.} We let $\{\embd_0(\bos)\} \cup \{ \embd_\toki(\tok) \}_{i \in \{ 0 \} \cup [N-1], \tok \in \cV} \cup \set{\bm{e}_\tok}_{\tok\in\vocab}$ to be a set of orthonormal basis in $\R^d$, and denote $\set{\bm{\eta}_\toki}_{\toki\in\{ 0 \} \cup [N-1]} \subseteq \R^d$ by a set of orthonormal basis in $\R^d$ (the existence is guaranteed by Assumption~\ref{ass:linear_indp}). Therefore, for any parameters $(\vecsink \in \R^{V}, \vecvalue \in \R^V, \bm{\xi} \in \R^V, \lambda \in \R)$, there exists a query matrix $\bQ \in \R^{d \times N}$ such that
\begin{equation}\label{appeqn:mlp-construct}
\begin{aligned}
&~\bQ \cdot \embd_\toki(\tok) = \lambda\bm{\eta}_{i-1}~~~\text{for }i>1,~~\tok\in\cT,\\
&~\bQ \cdot \embd_\toki(\tok) = \sink_{\tok} \bm{\eta}_{0}~~~\text{for }i>0,~~\tok\in\vocab\setminus\cT.\\
\end{aligned}
\end{equation}
Meanwhile, there is a key matrix $\bK \in \R^{d \times N}$ such that
\begin{equation}
\begin{aligned}
&~\bK \cdot \embd_\toki(\tok) = \bm{\eta}_{i}~~~\text{for }i>0,~~\tok\in\vocab,\\
&~\bK \cdot \embd_0(\bos) = \bm{\eta}_{0}.
\end{aligned}
\end{equation}
Denote $\set{\bm{e}_\tok}_{\tok\in\vocab}$ as an orthonormal basis in $\R^\vocabsize$.
There is a matrix $\bV\in \R^{d \times \vocabsize}$ such that %\sm{Modify the output of V}
\begin{equation}
\begin{aligned}
&~ \bV \cdot \embd_i(\tok)  = \xi_\tok \bm{e}_{\tok} \in \R^{\vocabsize},  ~~~\text{with $\xi_\tok=0$ for $\tok\in\cT$, and $\xi_\tok\geq 0$ for $\tok\in\vocab\setminus\cT$.}\\
% &~ \bV \cdot \embd_\toki(\tok) = 0~~~\text{for }i> 1,~~\tok\in\cT,\\
% &~ \bV \cdot \embd_\toki(\tok) = \xi_{\tok}\bm{e}_{\tok}~~~\text{for }i> 0,~~\tok\in\vocab\setminus\cT,\\
&~ \bV \cdot \embd_0(\bos) = \vecvalue\in\R^\vocabsize.\\
\end{aligned}
\end{equation}
This construction matches Eq. (\ref{eqn:simplification_TF_1}) and (\ref{eqn:simplification_TF_2}). 

As a result, for $\tok_n \in \vocab\setminus \cT$, by Eq.~(\ref{eqn:simplified_transformer}), denoting $\bH=[\bos; v_{1:n-1}; v_n]$ and $\attn(\bH)_n$ to be the last column of $\attn(\bH)$, we have
\begin{align*}
\attn(\bH)_n = &~ \sum_{i=0}^n \frac{\exp[ \embd_n(\tok_n)^\top \bQ^\top \bK\cdot \embd_i(\tok_i)] \bV \cdot  \embd_\toki(\tok_\toki)}{\sum_{j=0}^n\exp[\embd_n(\tok_n)^\top\bQ^\top \bK\cdot \embd_j(\tok_j)]}\\
= &~ \frac{\exp[\sink_{\tok_n} \bm{\eta}_{0}^\top \bm{\eta}_{0}]\cdot \bbeta + \sum_{i=1}^n \set{\exp[\sink_{\tok_n} \bm{\eta}_{0}^\top \bm{\eta}_{i}] \xi_{\tok_i} \cdot \bm{e}_{\tok_i}}}{\exp[\sink_{\tok_n} \bm{\eta}_{0}^\top \bm{\eta}_{0}]+\sum_{j=1}^n\exp[\sink_{\tok_n} \bm{\eta}_{0}^\top \bm{\eta}_{j}]}\\
= &~  \frac{e^{\sink_{\tok_n}}}{e^{\sink_{\tok_n}} + n} \cdot \bbeta +  \sum_{i=1}^n \frac{1}{e^{\sink_{\tok_n}} + n} \cdot \xi_{\tok_i} \bm{e}_{\tok_i}.
\end{align*}
For $\tok_n \in \cT$, we have
\begin{align*}
\attn(\bH)_n = &~ \sum_{i=0}^n \frac{\exp[\embd_n(\tok_n)^\top \bQ^\top \bK\cdot\embd_i(\tok_i)] \bV \cdot \embd_\toki(\tok_\toki)}{\sum_{j=0}^n\exp[\embd_n(\tok_n)^\top \bQ^\top \bK\cdot \embd_j(\tok_j)]}\\
= &~ \frac{\exp[\lambda \bm{\eta}_{n-1}^\top \bm{\eta}_{0}]\cdot \bbeta + \sum_{i=1}^n \set{\exp[\lambda \bm{\eta}_{n-1}^\top \bm{\eta}_{i}] \xi_{\tok_i} \cdot \bm{e}_{\tok_i}}}{\exp[\lambda \bm{\eta}_{n-1}^\top \bm{\eta}_{0}]+\sum_{j=1}^n\exp[\lambda \bm{\eta}_{n-1}^\top \bm{\eta}_{j}]}\\
= &~  \frac{1}{e^\lambda + n} \cdot \bbeta +  \sum_{i\neq n-1} \frac{1}{e^\lambda + n} \cdot \xi_{\tok_i} \bm{e}_{\tok_i} + \frac{e^\lambda}{e^\lambda + n} \cdot \xi_{\tok_{n-1}} \bm{e}_{\tok_{n-1}}.
\end{align*}

\noindent
{\bf Step 2. Construction for the MLP layer.} Further, define the weights for the \mlp~layer such that
\begin{equation}
\begin{aligned}
&~ \bW_1 \cdot \embd_\toki(\tok) = \bm{e}_\tok \in \R^\vocabsize, ~~~\bW_2\bm{e}_\tok =\log \bm{\transition}_{\tok} \cdot 1\{ \tok \not\in \cT  \} \in \R^\vocabsize  ~~~ \text{for } i\in[N],~~\tok \in \vocab,
\end{aligned}
\end{equation}
where $\set{\bm{e}_\tok}$ is the eorthonormal basis in $\R^\vocabsize$ and $\bm{\transition}_\tok \in \R^\vocabsize$ is defined in Eq.~(\ref{appeqn:P-matrix}). As a result, $\mlp(\bH)_n = \bW_2\text{ReLU}(\bW_1 \embd_n(\tok)) =\bW_2 \bm{e}_\tok =\log \bm{\transition}_\tok \cdot \bm{1}\set{\tok \notin \cT} $.
 This matches the Eq.~(\ref{eqn:simplification_TF_3}). 

\noindent
{\bf Step 3. The output of the transformer.} By Eq.~(\ref{eqn:simplified_transformer}) again, on non-trigger token $\tok\in\vocab\setminus \cT$, the transformer output gives that %\sm{Parallel architecture without residual}
\begin{align*}
\TF(\bH)_n&=\mlp(\embd_n(\tok))+\attn(\bH)_n\\
&=\log \bm{\transition}_\tok + \frac{e^{\sink_{\tok_n}}}{e^{\sink_{\tok_n}} + n} \cdot \bbeta +  \sum_{i=0}^n \frac{1}{e^{\sink_{\tok_n}} + n} \cdot \xi_{\tok_i} \bm{e}_{\tok_i}.
\end{align*}
On trigger token $\tok\in\cT$, the transformer output gives that
\begin{align*}
\TF(\bH)_n&=\mlp(\embd_n(\tok))+\attn(\bH)_n\\
&=\frac{1}{e^\lambda + n} \cdot \bbeta +  \sum_{i\neq n-1} \frac{1}{e^\lambda + n} \cdot \xi_{\tok_i} \bm{e}_{\tok_i} + \frac{e^\lambda}{e^\lambda + n} \cdot \xi_{\tok_{n-1}} \bm{e}_{\tok_{n-1}}.
\end{align*}
% It matches the model in Figure~\ref{figure:simple-model}, Eq. (\ref{eqn:simplification_TF_1}), (\ref{eqn:simplification_TF_2}), and (\ref{eqn:simplification_TF_3}). 

There exists a sequence $\min_{v \in \vocab} \sink_v \to\infty$, $\min_{v \in \vocab} \xi_v \to\infty$, $\lambda\to \infty$, and $\vecvalue=0$, we get that
\begin{equation*}
\softmax[\TF(\bH)_n] \to \bm{\transition}_{\tok_n} ~~~\text{for }n>0,~~\tok_n\in\vocab \setminus \cT,
\end{equation*}
\begin{equation*}
\softmax[\TF(\bH)_n] \to ( 1\{ v = \tok_{n-1} \} )_{v \in \cV}~~~\text{for }n>0,~~\tok_n\in\cT.
\end{equation*}
This proves Eq.~(\ref{eqn:TF_ground_truth_match}), indicating that the transformer output matches the ground truth transition. This finishes the proof of Theorem~\ref{appthm:formal-contruct}. 
\end{proof}

\subsection{Proof of Theorem~\ref{thm:main}(c): Stable phase}\label{app:proof-main-3}

% \sm{Add (a) (b) (c) to refer to the statement}

% \sm{what is the definition of $q$? }

% \tianyu{We first prove the part (c) in \Cref{thm:main}?} 

We first state \Cref{appthm:g-ppred} and \Cref{appthm:gradients} that are used to prove~\Cref{thm:main}(c). Lemma~\ref{appthm:g-ppred} computes the gradients of $\ppred_{\toki\tokk}$ as defined in Eq. \eqref{appeqn:pred-prob}. %\sm{Define $q$ }
\begin{lemma}\label{appthm:g-ppred} 
Given $\ppred_{\toki\tokk}$ defined in Eq.~\eqref{appeqn:pred-prob}, for any $\toki$, $\tokk$, $\tok$,  and any value of $\sink_\tok$ and $\ivalue_\tok$, we have that
\begin{align*}
\frac{\partial \ppred_{\toki\tokk}}{\partial \sink_\tok} = &~ \frac{\bm{1}\{\toki=\tok\} \ppred_{\toki\tokk}e^{\sink_\toki}}{(e^{\sink_\toki}+\mass)^2} \Big[\mass\ivalue_\tokk-\mass_\tokk \xi_\tokk - \sum_{\tokj=1}^\vocabsize \ppred_{\toki\tokj} (\mass\ivalue_\tokj -\mass_\tokj\xi_\tokj)\Big],\\
\frac{\partial \ppred_{\toki\tokk}}{\partial \ivalue_\tok} = &~ \frac{e^{\sink_\toki}}{e^{\sink_\toki}+\mass}[\ppred_{\toki\tokk}\bm{1}\{\tokk=\tok\} - \ppred_{\toki\tokk}\ppred_{\toki\tok}].\\
\end{align*}
Furthermore, we have
\[
\sum_{\tokk=1}^\vocabsize \frac{\partial \ppred_{\toki\tokk}}{\partial \sink_\tok} = 0~~~\text{for any }\toki,\ \tok,\ \vecsink,\ \text{and }\vecvalue, \quad \sum_{\tok=1}^\vocabsize \frac{\partial \ppred_{\toki\tokk}}{\partial \ivalue_\tok} = 0~~~\text{for any }\toki,\ \tokk,\ \vecsink,\ \text{and }\vecvalue.
\]
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{appthm:g-ppred}]
We repeatedly use the following two facts:
\begin{align*}
\frac{\partial\Big\{\exp\Big[\frac{\mass_\tokk\xi_\tokk+e^{\sink_\toki}\ivalue_\tokk}{e^{\sink_\toki}+\mass}\Big]\Big\}}{\partial \sink_\tok} = &~ \frac{\bm{1}\{\toki=\tok\}e^{\sink_\toki}(\mass\ivalue_\tokk-\mass_\tokk\xi_\tokk)}{(e^{\sink_\toki}+\mass)^2} \exp\Big[\frac{\mass_\tokk\xi_\tokk+e^{\sink_\toki}\ivalue_\tokk}{e^{\sink_\toki}+\mass}\Big],
\nonumber\\
\frac{\partial\Big\{\exp\Big[\frac{\mass_\tokk\xi_\tokk+e^{\sink_\toki}\ivalue_\tokk}{e^{\sink_\toki}+\mass}\Big]\Big\}}{\partial \ivalue_\tok} = &~ \frac{\bm{1}\{\tokk=\tok\}e^{\sink_\toki}}{e^{\sink_\toki}+\mass} \exp\Big[\frac{\mass_\tokk\xi_\tokk+e^{\sink_\toki}\ivalue_\tokk}{e^{\sink_\toki}+\mass}\Big].
\nonumber
\end{align*}

When $\toki\neq \tok$, $\ppred_{\toki\tokk}$ has zero gradients with respect to $\sink_\tok$. When $\toki=\tok$, we have that
\begin{align*}
\frac{\partial \ppred_{\tok\tokk}}{\partial \sink_\tok} = &~ \ppred_{\tok\tokk} e^{\sink_\tok} \Big[\frac{\mass\ivalue_\tokk-\mass_\tokk\xi_\tokk}{(e^{\sink_\tok}+\mass)^2}\Big] - \frac{\ppred_{\tok\tokk}\sum_{\toki=1}^\vocabsize \transition_{\tok\toki} e^{\sink_\tok} \Big[\frac{\mass\ivalue_\toki-\mass_\toki\xi_\toki}{(e^{\sink_\tok}+\mass)^2}\Big]\exp\Big[\frac{\mass_\toki\xi_\toki+e^{\sink_\tok}\ivalue_\toki}{e^{\sink_\tok}+\mass}\Big]}{\sum_{\toki=1}^\vocabsize \transition_{\tok\toki}\exp\Big[\frac{\mass_\toki\xi_\toki+e^{\sink_\tok}\ivalue_\toki}{e^{\sink_\tok}+\mass}\Big]}\\
= &~ \frac{e^{\sink_\tok}}{(e^{\sink_\tok}+\mass)^2} \Big\{ \ppred_{\tok\tokk} [\mass\ivalue_\tokk-\mass_\tokk \xi_\tokk] - \ppred_{\tok\tokk}\sum_{\tokj=1}^\vocabsize \ppred_{\tok\tokj} (\mass\ivalue_\tokj -\mass_\tokj\xi_\tokj)\Big\},
\end{align*}
and
\begin{align*}
\frac{\partial \ppred_{\toki\tokk}}{\partial \ivalue_\tok} = &~ \Big[\frac{e^{\sink_\toki}}{e^{\sink_\toki}+\mass}\Big]\ppred_{\toki\tokk} \bm{1}\{\tokk=\tok\} - \frac{\Big[\frac{e^{\sink_\toki}}{e^{\sink_\toki}+\mass}\Big]\transition_{\toki\tok}\exp\Big[\frac{\mass_\tok\xi_\tok+e^{\sink_\toki}\ivalue_\tok}{e^{\sink_\toki}+\mass}\Big]\transition_{\toki\tokk}\exp\Big[\frac{\mass_\tokk\xi_\tokk+e^{\sink_\toki}\ivalue_\tokk}{e^{\sink_\toki}+\mass}\Big]}{\Big(\sum_{j=1}^\vocabsize \transition_{\toki\tokj}\exp\Big[\frac{\mass_\tokj\xi_\tokj+e^{\sink_\toki}\ivalue_\tokj}{e^{\sink_\toki}+\mass}\Big]\Big)^2}\\
= &~ \Big[\frac{e^{\sink_\toki}}{e^{\sink_\toki}+\mass}\Big] [ \ppred_{\toki\tokk} \bm{1}\{\tokk=\tok\} - \ppred_{\toki\tokk} \ppred_{\toki\tok} ].
\end{align*}
We can verify that 
\begin{align*}
\sum_{\tokk=1}^\vocabsize \frac{\partial \ppred_{\toki\tokk}}{\partial \sink_\tok} = &~  \frac{e^{\sink_\tok}}{(e^{\sink_\tok}+\mass)^2} \sum_{\tokk=1}^\vocabsize \Big\{ \ppred_{\tok\tokk} [\mass\ivalue_\tokk-\mass_\tokk \xi_\tokk] - \ppred_{\tok\tokk}\sum_{\tokj=1}^\vocabsize \ppred_{\tok\tokj} (\mass\sink_\tokj -\mass_\tokj\xi_\tokj)\Big\}\\
= &~ \frac{e^{\sink_\tok}}{(e^{\sink_\tok}+\mass)^2}  \Big\{\sum_{\tokk=1}^\vocabsize \ppred_{\tok\tokk} [\mass\ivalue_\tokk-\mass_\tokk \xi_\tokk] - \sum_{\tokj=1}^\vocabsize \ppred_{\tok\tokj} (\mass\sink_\tokj -\mass_\tokj\xi_\tokj)\Big\}\\
= &~ 0,
\end{align*}
and
\begin{align*}
\sum_{\tok=1}^\vocabsize \frac{\partial \ppred_{\toki\tokk}}{\partial \ivalue_\tok} = &~   \Big[\frac{e^{\sink_\toki}}{e^{\sink_\toki}+\mass}\Big] \sum_{\tok=1}^\vocabsize [\ppred_{\toki\tokk} \bm{1}\{\tokk=\tok\} - \ppred_{\toki\tokk} \ppred_{\toki\tok} ]\\
= &~ \Big[\frac{e^{\sink_\toki}}{e^{\sink_\toki}+\mass}\Big] [\ppred_{\toki\tokk} - \ppred_{\toki\tokk} ]\\
= &~ 0.
\end{align*}
This finishes the proof of Lemma~\ref{appthm:g-ppred}.
\end{proof}

Proposition~\ref{appthm:gradients} computes the gradient of $\loss$ with respect to $\vecsink$ and $\vecvalue$, giving the ODE of the gradient flow.
\begin{proposition}\label{appthm:gradients}
Consider the gradient flow of optimizing $\loss(\vecsink, \vecvalue)$ given by 
\begin{equation}\label{appeqn:def-gradient-flow}
\dot{\vecsink}(t) = -\nabla_{\vecsink} \loss(\vecsink(t),\vecvalue(t)), ~~~ \dot{\vecvalue}(t) = -\nabla_{\vecvalue} \loss(\vecsink(t),\vecvalue(t)).
\end{equation}
Simplifying the dynamics using Lemma~\ref{appthm:g-ppred} gives that
\begin{align*}
\dot{\sink}_\tok(t) & = \frac{\Tilde{\stable}_\tok e^{\sink_\tok}}{(e^{\sink_\tok}+\mass)^2} 
\sum_{\toki=1}^\vocabsize({\transition}_{\tok\toki}-{\ppred}_{\tok\toki})(\mass\ivalue_\toki-\mass_\toki\xi_\toki),\\
\dot{\ivalue}_\tok(t) & = \sum_{\tokk=1}^\vocabsize \Big\{\frac{\Tilde{\stable}_\tokk e^{\sink_\tokk} [\transition_{\tokk\tok} - \ppred_{\tokk\tok}]}{e^{\sink_\tokk}+\mass}\Big\}.
\end{align*}
\end{proposition}
\begin{proof}[Proof of Proposition~\ref{appthm:gradients}]
% The gradient flow gives that
% \[
% \dot{\sink}_\tok(t) = - \frac{\partial \loss(\vecsink,\vecvalue)}{\partial \sink_\tok}, \quad \text{and}\quad \dot{\ivalue}_\tok(t)=-\frac{\partial\loss(\vecsink, \vecvalue)}{\partial \ivalue_\tok}.
% \]
Taking the derivative of $\loss(\vecsink,\vecvalue)$ gives that
\begin{align*}
\frac{\partial \loss(\vecsink,\vecvalue)}{\partial \sink_\tok}
= ~& \Tilde{\stable}_\tok \sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} \cdot \frac{-1}{\ppred_{\tok\toki}}\cdot \frac{\partial \ppred_{\tok\toki}}{\partial \sink_\tok}\\
= ~& \frac{\Tilde{\stable}_\tok e^{\sink_\tok}}{(e^{\sink_\tok}+\mass)^2} \Big\{  \sum_{\toki=1}^\vocabsize \ppred_{\tok\toki} [\mass\ivalue_\toki -\mass_\toki \xi_\toki] - \sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} [\mass\ivalue_\tokk-\mass_\tokk\xi_\tokk]\Big\}\\
= ~& \frac{\Tilde{\stable}_\tok e^{\sink_\tok}}{(e^{\sink_\tok}+\mass)^2} \sum_{\tokk=1}^\vocabsize \Big\{  [\ppred_{\tok\tokk}-\transition_{\tok\tokk}] [\mass\ivalue_\tokk-\mass_\tokk\xi_\tokk]\Big\}.
\end{align*}
Similarly, we have that
\begin{align*}
\frac{\partial \loss(\vecsink,\vecvalue)}{\partial \ivalue_\tok} 
= ~& \sum_{j=1}^\vocabsize \Tilde{\stable}_\tokj \sum_{\tokk=1}^\vocabsize \transition_{\tokj\tokk} \Big\{ \frac{e^{\sink_\tokj} \ppred_{\tokj\tok}}{e^{\sink_\tokj}+\mass} - \frac{e^{\sink_\tokj} \bm{1}\{\tokk=\tok\}}{e^{\sink_\tokj}+\mass} \Big\}\\
= ~& \sum_{j=1}^\vocabsize \Big\{\frac{\Tilde{\stable}_{\tokj}e^{\sink_\tokj}[\ppred_{\tokj\tok}-\transition_{\tokj\tok}]}{e^{\sink_\tokj}+\mass}\Big\}.
\end{align*}
Plug them in Eq.~\eqref{appeqn:def-gradient-flow} proves Proposition~\ref{appthm:gradients}.
\end{proof}
% \begin{theorem}[Restatement of Theorem~\ref{thm:main}]\label{appthm:main}
% Consider the gradient flow of optimizing $\loss(\vecsink, \vecvalue)$. We have that 
% \begin{enumerate}
%     \item (identical attention logits) The gradient flow has equilibria 
%     \[\vecsink = \sink \bm{1}, \quad \vecvalue = c \cdot \bm{1} - e^{-\sink} \cdot \bm{\mass}\circ \bm{\xi}.\]
%     \item (\textit{attention sink}) Fixing $\vecvalue= c \cdot \bm{1}$, with any initial value $\vecsink(0)$, there exists $\bm{r}(t)$ with bounded norm such that
%     \[\vecsink(t) = \frac{1}{2} \log t \cdot \bm{1} + \bm{r}(t).\]
%     \item (\textit{small value states}) Fixing $\vecsink = \sink \cdot \bm{1}$, with any initial value $\vecvalue(0)$, we have for any $\toki$,
%     \[\abs{\ivalue_\toki(t) - \bar\ivalue(0) - e^{-\sink} \mass_\toki \xi_\toki} \leq \delta e^{-\mu t},\]
%     where $\bar{\ivalue}(0) = V^\inv[\sum_{\tok} \ivalue_\tok(0)]$, $\delta>0$, and $\mu>0$.
% \end{enumerate}
% \end{theorem}
\begin{theorem}[Restatement the stable phase part in Theorem~\ref{thm:main}(c)]\label{appthm:main-1} 
Assume $\xi_\tok \ge 0$ for any $\tok$, $\stable_\tok > 0$ for any $\tok\in\vocab$, and $\{ \mass_i \cdot \xi_i \}_{i \in \vocab}$ are not all equal. Consider the gradient flow over the variables $(\vecsink, \vecvalue)$, i.e., $(\dot{\vecsink}(t), \dot{\vecvalue}(t)) = - \nabla_{\vecsink, \vecvalue}\loss(\vecsink(t), \vecvalue(t))$. Any vector of the following form
    \begin{equation}\vecsink^\star = \sink \cdot \bm{1}, \quad \vecvalue^\star = c \cdot \bm{1} - e^{-\sink} \cdot \bm{\mass} \circ \bm{\xi},  ~~~ \sink, c\in\R \end{equation}
 is a stationary point. These are all global minimizers of $\loss(\vecsink,\vecvalue)$.
% \sm{Revise the statement}
\end{theorem}
\begin{proof}[Proof of Theorem~\ref{appthm:main-1}]
When $\vecsink=\vecsink^\star$ and $\vecvalue=\vecvalue^\star$, given $\ppred_{\tok\toki}$ defined in Eq.~\eqref{appeqn:pred-prob} with any $\tok$ and $\toki$, we have that
\begin{align*}
\ppred_{\tok\toki} = &~ \frac{\transition_{\tok\toki} \exp\Big[\frac{\mass_\toki\xi_\toki+e^{\sink}\ivalue_\toki}{e^{\sink}+\mass}\Big]}{\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk}\exp\Big[\frac{\mass_\tokk\xi_\tokk+e^{\sink}\ivalue_\tokk}{e^{\sink}+\mass}\Big]} \\
 = &~ \frac{\transition_{\tok\toki} \exp\Big[\frac{e^{\sink}c}{e^{\sink}+\mass}\Big]}{\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk}\exp\Big[\frac{e^{\sink}c}{e^{\sink}+\mass}\Big]} \\
 = &~ \transition_{\tok\toki}.
\end{align*}
Plug $\ppred_{\tok\toki}$ into $\partial \loss(\vecsink,\vecvalue)/\partial \vecsink$ and $\partial \loss(\vecsink,\vecvalue)/\partial \vecvalue$, we have
\begin{align*}
\frac{\partial \loss(\vecsink,\vecvalue)}{\partial \sink_\tok}\Big |_{\vecsink^\star,\vecvalue^\star} & = \frac{\Tilde{\stable}_\tok e^{\sink_\tok}}{(e^{\sink_\tok}+\mass)^2} 
\sum_{\tokk=1}^\vocabsize \Big\{(\ppred_{\tok\tokk}-\transition_{\tok\tokk}) [W \ivalue_\tokk -\mass_\tokk \xi_\tokk]
\Big\} = 0,\\
\frac{\partial \loss(\vecsink,\vecvalue)}{\partial \ivalue_\tok}\Big |_{\vecsink^\star,\vecvalue^\star} & = \sum_{\tokk=1}^\vocabsize \Big\{\frac{\Tilde{\stable}_\tokk e^{\sink_\tokk} [\ppred_{\tokk\tok}-\transition_{\tokk\tok} ]}{e^{\sink_\tokk}+\mass}\Big\} = 0.
\end{align*}
This shows that $\vecsink=\vecsink^\star$ and $\vecvalue=\vecvalue^\star$ are stationary points. We further compute the second-order derivative using Lemma~\ref{appthm:g-ppred}. To simplify the notation, we use $z_\tokk = W \ivalue_\tokk -\mass_\tokk \xi_\tokk$ and $\bm{z}=[z_1,\ldots,z_\vocabsize]$. We have that
\begin{align*}
\frac{\partial^2 \loss(\vecsink, \vecvalue)}{\partial \sink_\toki \partial \sink_\tok} \Big |_{\vecsink^\star,\vecvalue^\star} =  ~& \bm{1}\{\tok=\toki\}\cdot \frac{\Tilde{\stable}_\tok e^{\sink}}{(e^{\sink}+\mass)^2} 
\sum_{\tokk=1}^\vocabsize \Big\{\frac{\partial \ppred_{\toki\tokk}}{\partial \sink_\tok} z_\tokk
\Big\}\\
=  ~& \bm{1}\{\tok=\toki\}\cdot \frac{\Tilde{\stable}_\tok e^{2\sink}}{(e^{\sink}+\mass)^4} 
\Big\{ \sum_{\tokk=1}^\vocabsize \ppred_{\toki\tokk} z_\tokk^2 - \Big[\sum_{\tokk=1}^\vocabsize \ppred_{\toki\tokk}z_\tokk \Big]^2
\Big\}\\
=  ~& \bm{1}\{\tok=\toki\}\cdot \frac{\Tilde{\stable}_\tok e^{2\sink}}{(e^{\sink}+\mass)^4} 
\Big\{ \sum_{\tokk=1}^\vocabsize \transition_{\toki\tokk} z_\tokk^2 - \Big[\sum_{\tokk=1}^\vocabsize \transition_{\toki\tokk}z_\tokk \Big]^2
\Big\},
\end{align*}
where in the last line, we plugged in $\ppred_{\tok\toki}=\transition_{\tok\toki}$ for any $\tok$ and $\toki$. Similarly, we compute the second order derivatives with respect to $\sink_\toki$ and $\ivalue_\tok$,
\begin{align*}
\frac{\partial^2 \loss(\vecsink, \vecvalue)}{\partial \sink_\toki \partial \ivalue_\tok} \Big |_{\vecsink^\star,\vecvalue^\star} =  ~& \frac{\Tilde{\stable}_\toki e^{\sink}}{(e^{\sink}+\mass)^2} 
\sum_{\tokk=1}^\vocabsize \Big\{\frac{\partial \ppred_{\toki\tokk}}{\partial \ivalue_\tok} z_\tokk
\Big\}\\
=  ~& \frac{\Tilde{\stable}_\toki e^{2\sink}}{(e^{\sink}+\mass)^3} 
\Big\{ \transition_{\toki\tok} z_\tokk - \transition_{\toki\tok} \sum_{\tokk=1}^\vocabsize  \transition_{\toki\tokk}z_\tokk
\Big\}.
\end{align*}
With the same manner, we compute the second order derivatives with respect to $\ivalue_\toki$ and $\ivalue_\tok$, 
\begin{align*}
\frac{\partial^2 \loss(\vecsink, \vecvalue)}{\partial \ivalue_\toki \partial \ivalue_\tok} \Big |_{\vecsink^\star,\vecvalue^\star} =  ~& \sum_{\tokk=1}^\vocabsize \Big\{\frac{\partial \ppred_{\tokk\toki}}{\partial \ivalue_\tok}\frac{\Tilde{\stable}_\tokk e^{\sink}}{e^{\sink}+\mass}\Big\}\\
= ~& \frac{e^{2\sink}}{(e^\sink+\mass)^2}\sum_{\tokk=1}^\vocabsize \{ \Tilde{\stable}_\tokk [\bm{1}\{\tok=\toki\} \transition_{\tokk\tok} - \transition_{\tokk\toki}\transition_{\tokk\tok}] \}.
\end{align*}
Combining the above computations gives that
\begin{align*}
\text{Hessian}(\loss(\vecsink^\star, \vecvalue^\star)) = \left(\begin{matrix}
\nabla^2_{\vecsink} \loss(\vecsink,\vecvalue) & \nabla_{\vecsink} \nabla_{\vecvalue}  \loss(\vecsink,\vecvalue)\\
\nabla_{\vecvalue} \nabla_{\vecsink} \loss(\vecsink,\vecvalue) & \nabla^2_{\vecsink} \loss(\vecsink,\vecvalue)\\
\end{matrix}\right),
\end{align*}
with
\begin{align*}
\nabla^2_{\vecsink} \loss(\vecsink,\vecvalue) = &~ \frac{e^{2\sink}}{(e^\sink+\mass)^4}\diag\Big\{ \Tilde{\stable} \circ [\bz^\top \gppred^\Transition_1\bz;\ldots;\gppred^\Transition_\vocabsize \bz] \Big\},\\
\nabla_{\vecsink} \nabla_{\vecvalue} \loss(\vecsink,\vecvalue) = &~ \frac{e^{2\sink}}{(e^\sink+\mass)^3}\diag\Big\{ \Tilde{\stable} \Big\} [ \bz^\top\gppred^\Transition_1;\ldots; \bz^\top\gppred^\Transition_\vocabsize] ,\\
\nabla^2_{\vecvalue} \loss(\vecsink,\vecvalue) = &~ \frac{ e^{2\sink}}{(e^{\sink}+\mass)^2} \sum_{\tokk=1}^\vocabsize  \Tilde{\stable}_\tokk \gppred^\Transition_\tokk,
\end{align*}
where $\bG^\Transition_\tokk$ is defined in Eq.~\eqref{appeqn:g-probs}. Furthermore, there exists $\bm{U}$ such that $\bm{U} \text{Hessian}(\loss(\vecsink^\star, \vecvalue^\star)) \bm{U}^\top = \text{Diag-Hessian}(\loss(\vecsink^\star, \vecvalue^\star))$, with
\[
\text{Diag-Hessian}(\loss(\vecsink^\star, \vecvalue^\star)) = \left(\begin{matrix}
\nabla^2_{\vecsink} \loss(\vecsink,\vecvalue) & 0\\
0 & \frac{ e^{2\sink}}{(e^{\sink}+\mass)^2} \bB\\
\end{matrix}\right),
\]
where the $\bB$ is given by
\begin{align*}
\bB = \sum_{\tokk=1}^\vocabsize  \Tilde{\stable}_\tokk \Big(\gppred^\Transition_\tokk - (\bm{z}^\top \gppred_\tokk^\Transition \bm{z})^{-1}\gppred^\Transition_\tokk\bm{z}\bm{z}^\top\gppred^\Transition_\tokk\Big).
\end{align*}
To prove that $\bB$ is positive semi-definite, consider any vector $\bm{\eta}$ with $\|\bm{\eta}\|_2=1$: 
\begin{align*}
\bm{\eta}^\top \bB \bm{\eta} = &~ \sum_{\tokk=1}^\vocabsize  \Tilde{\stable}_\tokk \Big(\bm{\eta}^\top\gppred^\Transition_\tokk\bm{\eta} - \frac{\bm{\eta}^\top\gppred^\Transition_\tokk\bm{z}\bm{z}^\top\gppred^\Transition_\tokk\bm{\eta}}{\bm{z}^\top \gppred_\tokk^\Transition \bm{z}}\Big).
\end{align*}
Since $\gppred^\Transition_\tokk$ is positive semi-definite, the Cauchy inequality gives that
\[
\bm{z}^\top\gppred^\Transition_\tokk\bm{\eta} \leq \sqrt{\bm{z}^\top \gppred_\tokk^\Transition \bm{z} \bm{\eta}^\top \gppred_\tokk^\Transition \bm{\eta}}.
\]
As a result, we have that
\begin{align*}
\bm{\eta}^\top \bB \bm{\eta} \geq &~ \sum_{\tokk=1}^\vocabsize  \Tilde{\stable}_\tokk \Big(\bm{\eta}^\top\gppred^\Transition_\tokk\bm{\eta} - \frac{\bm{z}^\top \gppred_\tokk^\Transition \bm{z} \bm{\eta}^\top \gppred_\tokk^\Transition \bm{\eta}}{\bm{z}^\top \gppred_\tokk^\Transition \bm{z}}\Big) = 0.
\end{align*}
This shows that $\bB$ is positive semi-definite. Therefore, $\text{Hessian}(\loss(\vecsink^\star, \vecvalue^\star))$ is positive semi-definte.  This proves Theorem~\ref{appthm:main-1}.
\end{proof}

% We prove Theorem~\ref{appthm:main-1} through direct computation. Due to the non-linearity, it's unclear whether other stationary points exist. However, we observe that all of our simulations converge to the given stationary points. 
\subsection{Proof of Theorem~\ref{thm:main}(a): Attention sinks}\label{appsec:proof-main-1}
\begin{theorem}[Restatement of the attention sink part in Theorem~\ref{thm:main}(a)]\label{appthm:main-2}
Assume $\xi_\tok \ge 0$ for any $\tok$, $\stable_\tok > 0$ for any $\tok\in\vocab$, and $\{ \mass_i \cdot \xi_i \}_{i \in \vocab}$ are not all equal. Fix $\vecvalue= \beta \cdot \bm{1}$ for a constant $\beta$, and consider the gradient flow of the loss function $\loss(\vecsink, \vecvalue)$ over $\vecsink$, i.e., $\dot{\vecsink}(t) = - \nabla \loss(\vecsink(t), \vecvalue)$. With any initial value $\vecsink(0)$, there exists $\bm{r}(t)$ with norm uniformly bounded in time, such that 
    \begin{equation}
    \textstyle \vecsink(t) = \frac{1}{2} \log t \cdot \bm{1} + \bm{r}(t).
    \end{equation}
\end{theorem}
\begin{proof}[Proof of Theorem~\ref{appthm:main-2}]
We separately analyze each entry of $\vecsink$. Focusing on $\sink_\tok$, to simplify the notation, we introduce a random variable $\varphi$ such that $$\P(\varphi=\mass_\tokk\xi_\tokk)=\transition_{\tok\tokk}.$$
Denote 
\[
u = e^{\sink_\tok}.
\]
Therefore, using Lemma~\ref{appthm:gradients}, we get that
\begin{align*}
\frac{\mathrm{d} u}{\mathrm{d} t} = \frac{\Tilde{\stable}_\tok e^{2\sink_\tok}}{(e^{\sink_\tok}+\mass)^2} 
\sum_{\toki=1}^\vocabsize({\transition}_{\tok\toki}-{\ppred}_{\tok\toki})(\mass\ivalue_\toki-\mass_\toki\xi_\toki).
\end{align*}
We take in $\vecvalue = c\cdot \bm{1}$ and expand the expression of $\mathrm{d}u/\mathrm{d}t$. This gives us that
\begin{align*}
\frac{\mathrm{d} u}{\mathrm{d} t} = &~ \frac{\Tilde{\stable}_\tok u^2}{(u+\mass)^2} \frac{\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} e^{\mass_\tokk\xi_\tokk/(u+\mass)}\mass_\tokk\xi_\tokk - \sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} e^{\mass_\tokk\xi_\tokk/(u+\mass)}\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} \mass_\tokk\xi_\tokk}{\sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} e^{\mass_\tokk\xi_\tokk/(u+\mass)}}\\
= &~ \frac{\Tilde{\stable}_\tok u^2}{(u+\mass)^2} \frac{\Cov(e^{\frac{\varphi}{u+\mass}},\varphi)}{\E e^{\frac{\varphi}{u+\mass}}}.
\end{align*}
Since both $e^{x/(u+\mass)}$ and $x$ are monotonically increasing with respect to $x$, $\mathrm{d}u/\mathrm{d}t \geq 0$. Therefore, $u$ is monotonically increasing, and we have that
\[
\frac{u(t)^2}{[u(t)+\mass]^2} \geq \frac{u(0)^2}{[u(0)+\mass]^2},\quad \E e^{\frac{\varphi}{u(t)+\mass}} \leq \E e^{\frac{\varphi}{u(0)+\mass}}.
\]
Meanwhile, the first and second order Taylor expansions of $e^{\varphi/(u+\mass)}$ give that
\[
e^{\frac{\varphi}{u+\mass}} = 1 + \frac{\theta_1(\varphi) \varphi}{u+\mass},\quad e^{\frac{\varphi}{u+\mass}} = 1 + \frac{\varphi}{u+\mass} + \theta_2(\varphi) \Big[\frac{\varphi}{u+\mass}\Big]^2,
\]
where both $\theta_1(\varphi)$ and $\theta_2(\varphi)\varphi^2$ are  monotonically increasing functions of $\varphi$. We also have the bound that
\[
\theta(\varphi) \leq \Big[\exp\Big\{\frac{\max_\tokk \mass_\tokk \xi_\tokk}{u(0)+\mass}\Big\}-1\Big]/\Big[\frac{\max_\tokk \mass_\tokk \xi_\tokk}{u(0)+\mass}-1\Big] = C_\theta.
\]
Therefore, we get two more inequalities:
\[
\Cov(\theta_1(\varphi)\varphi,\varphi)\leq C_\theta \E(\varphi^2),\quad \Cov(\theta_2(\varphi)\varphi^2,\varphi)\geq 0.
\]
We bound $\mathrm{d}u/\mathrm{d}t$ and get that
\begin{align*}
\frac{\mathrm{d}u}{\mathrm{d}t} \leq &~ \Tilde{\stable}_\tok \Cov(e^{\frac{\varphi}{u+\mass}}, \varphi)\\
= &~ \Tilde{\stable}_\tok\Cov(1 + \frac{\theta_1(\varphi)\varphi}{u+\mass}, \varphi)\\
\leq &~ \frac{\Tilde{\stable}_\tok C_\theta \E(\varphi^2)}{u}.
\end{align*}
By solving the ODE, we get that
\begin{align*}
u \leq \sqrt{2 \Tilde{\stable}_\tok C_\theta \E(\varphi^2) t + C_1}.
\end{align*}
To give a lower bound, we have that
\begin{align*}
\frac{\mathrm{d}u}{\mathrm{d}t} \geq &~ \frac{u(0)^2}{[u(0)+\mass]^2}\frac{\Tilde{\stable}_\tok \Cov(e^{\frac{\varphi}{u+\mass}},\varphi)}{\E e^{\frac{\varphi}{u(0)+\mass}}} \\
= &~ \frac{u(0)^2}{[u(0)+\mass]^2}\frac{\Tilde{\stable}_\tok}{\E e^{\frac{\varphi}{u(0)+\mass}}} \Cov(1+\frac{\varphi}{u+\mass}+\theta_2(\varphi) \Big[\frac{\varphi}{u+\mass}\Big]^2,\varphi)\\
\geq &~ \frac{u(0)^2}{[u(0)+\mass]^2}\frac{\Tilde{\stable}_\tok}{\E e^{\frac{\varphi}{u(0)+\mass}}} \frac{\Var(\varphi)}{u+\mass}\\
\geq &~ \frac{u(0)^2}{[u(0)+\mass]^2}\frac{\Tilde{\stable}_\tok}{\E e^{\frac{\varphi}{u(0)+\mass}}} \cdot \frac{u(0)}{u(0)+\mass} \cdot \frac{\Var(\varphi)}{u}\\
= &~  \frac{\Tilde{C}}{u}.
\end{align*}
Therefore, $u \geq \sqrt{\Tilde{C} t + \Tilde{C}_2}$. In conclusion, we have that
\[
y_{\tok} = \log u = \frac{1}{2}\log t + r_{\tok},
\]
with $r_{\tok}$ bounded. This proves Theorem~\ref{appthm:main-2}.
\end{proof}
\subsection{Proof of Theorem~\ref{thm:main}(b): Value-state drains}\label{appsec:proof-main-2}
\begin{theorem}[Restatement of Theorem~\ref{thm:main}(b)]\label{appthm:main-3}
Assume $\xi_\tok \ge 0$ for any $\tok$, $\stable_\tok > 0$ for any $\tok\in\vocab$, and $\{ \mass_i \cdot \xi_i \}_{i \in \vocab}$ are not all equal. Fix $\vecsink = \sink \cdot \bm{1}$ for a constant $\sink$, define $\bar{\ivalue}(0) = V^\inv[\sum_{\tok} \ivalue_\tok(0)]$ and $\meanvalue=\vocabsize^\inv[\sum_{\tok} \mass_\tok \xi_\tok]$. Consider the gradient flow of the loss function $\loss(\vecsink, \vecvalue)$ over $\vecvalue$ for fixed $\vecsink$, i.e., $\dot{\vecvalue}(t) = - \nabla_{\vecvalue} \loss(\vecsink, \vecvalue(t))$. As $t \to \infty$, we have
    \begin{equation}\vecvalue(t) \to \vecvalue^\star = [\bar\ivalue(0)+e^{-\sink} \meanvalue] \cdot \bm{1} - e^{-\sink} \cdot \bm{\mass}\circ \bm{\xi}.\end{equation}
\end{theorem}
\begin{proof}[Proof of  Theorem~\ref{appthm:main-3}]
% Given any $\sink\in\R$, Theorem~\ref{appthm:main-1} has already verified that $\vecvalue=c \cdot\bm{1}-e^{-\sink}\cdot \bm{\mass}\circ \bm{\xi}$ are stationary points for any $c$.
We plug $\vecvalue^\star$ into the $\loss$ and get that $\loss(\vecsink, \vecvalue^\star) = \sum_{\tok=1}^\vocabsize \Tilde{\stable}_{\tok} \sum_{\tokk=1}^\vocabsize \transition_{\tok\tokk} \log \transition_{\tok\tokk} $. Computing $\nabla^2_{\vecvalue} \loss(\vecsink,\vecvalue)$, we get that
\begin{equation*}
\nabla^2_{\vecvalue} \loss(\vecsink,\vecvalue) = \sum_{\tokk=1}^\vocabsize \Tilde{\stable}_\tokk \gppred^\Ppred_\tokk,
\end{equation*}
where $\gppred^\Ppred_\tokk$ is defined in Eq.~\eqref{appeqn:g-probs}.
Lemma~\ref{appthm:positive-definite} indicates that it is positive semi-definite. Therefore, we have that
\[
\loss(\vecsink, \vecvalue(t))\to \loss(\vecsink, \vecvalue^\star)~~\text{as }t\to \infty.
\]
We choose $\delta$ as defined in Eq.\eqref{appeqn:error-q}. When $t$ is sufficiently large, we have that
\[
\loss(\vecsink, \vecvalue(t))\leq \loss(\vecsink, \vecvalue^\star) + \frac{1}{\min_{\tokk\in\vocab\setminus\cT}\Tilde{\stable}_\tokk}\cdot 2 \delta^2. 
\]
The convexity further implies that for any $\Tilde{\vecvalue}=\theta \vecvalue(t) + (1-\theta)\vecvalue^\star$ ($\theta\in(0,1)$), we have that 
\[
\loss(\vecsink, \Tilde{\vecvalue})\leq \loss(\vecsink, \vecvalue^\star) + \frac{1}{\min_{\tokk\in\vocab\setminus\cT}\Tilde{\stable}_\tokk}\cdot 2 \delta^2. 
\]
Denote $\Tilde{\bm{\ppred}}_\tok={\bm{\ppred}}_\tok(\vecsink, \Tilde{\vecvalue})$ as $\bm{\ppred}$ evaluated on $(\vecsink,\Tilde{\vecvalue})$. Using the definition of the KL-divergence in Eq.~\eqref{appeqn:kl-divergence}, we have that
\[
\sum_{\tok=1}^\vocabsize \Tilde{\stable}_\tok KL(\bm{\transition}_\tok~||~\Tilde{\bm{\ppred}}_\tok) = \loss(\vecsink, \vecvalue(t))- \loss(\vecsink, \vecvalue^\star) \leq \frac{1}{\min_{\tokk\in\vocab\setminus\cT} \Tilde{\stable}_\tokk}\cdot 2 \delta^2.
\]
This further implies that $KL(\bm{\transition}_\tok~||~\Tilde{\bm{\ppred}}_\tok)\leq 2 \delta^2$ for any $\tok$. Using Pinsker's inequality, we get that
\[
\sum_{\tokk=1}^\vocabsize |\transition_{\tok\tokk}-\Tilde\ppred_{\tok\tokk}| = \| \bm{\transition}_\tok - \bm{\ppred}_\tok \|_{\text{TV}} \leq \sqrt{KL(\bm{\transition}_\tok~||~\Tilde{\bm{\ppred}}_\tok)/2} \leq \delta.
\]
Therefore, $\max_{\tok,\tokk}\abs{\transition_{\tok\tokk}-\Tilde\ppred_{\tok\tokk}}\leq \delta$. 
Lemma~\ref{appthm:g-ppred} gives that  $\sum_{\tok=1}^\vocabsize \dot{\ivalue}_\tok(t)=0$. Therefore, $\sum_{\tok=1}^\vocabsize \ivalue_{\tok}(t)/\vocabsize=\bar{\ivalue}(0)$. The choice of $\vecvalue^\star$ guarantees that $\bar{\ivalue}^\star=\bar{\ivalue}(0)$. This shows that $\vecvalue(t)-\vecvalue^\star \perp \bm{1}$. Using Lemma \ref{appthm:min-eigenvalue-q}, there exists $\omega>0$ such that
\[
(\vecvalue(t)-\vecvalue^\star)^\top \nabla^2_{\vecvalue} \loss(\vecsink,\vecvalue) (\vecvalue(t)-\vecvalue^\star) = (\vecvalue(t)-\vecvalue^\star)^\top \Big[\sum_{\tokk=1}^\vocabsize \Tilde{\stable}_\tokk \gppred^\Ppred_\tokk \Big] (\vecvalue(t)-\vecvalue^\star) \geq \frac{\omega}{2} \|\vecvalue(t)-\vecvalue^\star\|_2^2.
\]
Using Taylor expansion, we have that
\begin{align*}
\loss(\vecsink,\vecvalue^\star)-\loss(\vecsink,\vecvalue(t)) = &~ -\nabla_\beta \loss(\vecsink,\vecvalue(t)) (\vecvalue(t)-\vecvalue^\star) + \frac{1}{2}(\vecvalue(t)-\vecvalue^\star)^\top \nabla^2_{\vecvalue} \loss(\vecsink,\Tilde{\vecvalue}) (\vecvalue(t)-\vecvalue^\star)\\
\geq &~ -\nabla_\beta \loss(\vecsink,\vecvalue(t)) (\vecvalue(t)-\vecvalue^\star) + \frac{\omega}{2} \|\vecvalue(t)-\vecvalue^\star\|_2^2\\
\geq &~ - \frac{1}{2\omega} \| \nabla_\beta \loss(\vecsink,\vecvalue(t))\|_2^2.
\end{align*}
This shows that $\loss(\vecsink,\vecvalue(t))$ satisfies the Polyak-Lojasiewicz (PL) condition \citep{karimi2016linear} when $t$ is sufficiently large. 
This proves Theorem~\ref{appthm:main-3}. 
\end{proof}
% \begin{remark}
% If we assume that $\transition_{\tok\tokk}>0$ for any $\tok$, $\tokk$ and suppose that the initial value $\vecvalue(0)$ is close enough to $\vecvalue^\star$, it is possible to prove the fast convergence of $\vecvalue(t)$ to $\vecvalue^\star$.
% \begin{equation*}
% \|\vecvalue(t) - \vecvalue^\star\|^2_2 \leq \delta e^{-\mu t}.
% \end{equation*}
% \end{remark}
