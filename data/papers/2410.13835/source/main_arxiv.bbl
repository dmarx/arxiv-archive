\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Song, Yun, Jadbabaie, and
  Sra]{ahn2023linear}
Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit
  Sra.
\newblock Linear attention is (maybe) all you need (to understand transformer
  optimization).
\newblock \emph{arXiv preprint arXiv:2310.01082}, 2023.

\bibitem[Ahn et~al.(2024)Ahn, Cheng, Daneshmand, and Sra]{ahn2024transformers}
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra.
\newblock Transformers learn to implement preconditioned gradient descent for
  in-context learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Allen-Zhu and Li(2023)]{allen2023physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: {P}art 1, context-free grammar.
\newblock \emph{arXiv preprint arXiv:2305.13673}, 2023.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley,
  O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff,
  et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley,
  Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit,
  USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages
  2397--2430. PMLR, 2023.

\bibitem[Bietti et~al.(2024)Bietti, Cabannes, Bouchacourt, Jegou, and
  Bottou]{bietti2024birth}
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon
  Bottou.
\newblock Birth of a transformer: A memory viewpoint.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Bondarenko et~al.(2021)Bondarenko, Nagel, and
  Blankevoort]{bondarenko2021understanding}
Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
\newblock Understanding and overcoming the challenges of efficient transformer
  quantization.
\newblock \emph{arXiv preprint arXiv:2109.12948}, 2021.

\bibitem[Bondarenko et~al.(2023)Bondarenko, Nagel, and
  Blankevoort]{bondarenko2023quantizable}
Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
\newblock Quantizable transformers: Removing outliers by helping attention
  heads do nothing.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:\penalty0 75067--75096, 2023.

\bibitem[Charton(2022)]{charton2022my}
Fran{\c{c}}ois Charton.
\newblock What is my math transformer doing? {T}hree results on
  interpretability and generalization.
\newblock \emph{arXiv preprint arXiv:2211.00170}, 2022.

\bibitem[Chen et~al.(2024)Chen, Zhao, Liu, Bai, Lin, Zhou, and
  Chang]{chen2024image}
Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and
  Baobao Chang.
\newblock An image is worth 1/2 tokens after layer 2: Plug-and-play inference
  acceleration for large vision-language models.
\newblock \emph{arXiv preprint arXiv:2403.06764}, 2024.

\bibitem[Computer(2023)]{together2023redpajama}
Together Computer.
\newblock Red{P}ajama: An open source recipe to reproduce {Llama} training
  dataset, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Darcet et~al.(2023)Darcet, Oquab, Mairal, and
  Bojanowski]{darcet2023vision}
Timoth{\'e}e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski.
\newblock Vision transformers need registers.
\newblock \emph{arXiv preprint arXiv:2309.16588}, 2023.

\bibitem[Deora et~al.(2023)Deora, Ghaderi, Taheri, and
  Thrampoulidis]{deora2023optimization}
Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis.
\newblock On the optimization and generalization of multi-head attention.
\newblock \emph{arXiv preprint arXiv:2310.12680}, 2023.

\bibitem[Dettmers and Zettlemoyer(2023)]{dettmers2023case}
Tim Dettmers and Luke Zettlemoyer.
\newblock The case for 4-bit precision: k-bit inference scaling laws.
\newblock In \emph{International Conference on Machine Learning}, pages
  7750--7774. PMLR, 2023.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022gpt3}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock {LLM.int8()}: 8-bit matrix multiplication for transformers at scale.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30318--30332, 2022.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman,
  Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
  Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
  et~al.
\newblock The {Llama} 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1:\penalty0 1, 2021.

\bibitem[Elhage et~al.(2023)Elhage, Lasenby, and Olah]{elhage2023privileged}
Nelson Elhage, Robert Lasenby, and Christopher Olah.
\newblock Privileged bases in the transformer residual stream.
\newblock \emph{Transformer Circuits Thread}, 2023.

\bibitem[Fan et~al.(2020)Fan, Stock, Graham, Grave, Gribonval, Jegou, and
  Joulin]{fan2020training}
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R{\'e}mi Gribonval,
  Herve Jegou, and Armand Joulin.
\newblock Training with quantization noise for extreme model compression.
\newblock \emph{arXiv preprint arXiv:2004.07320}, 2020.

\bibitem[Feng and Steinhardt(2023)]{feng2023language}
Jiahai Feng and Jacob Steinhardt.
\newblock How do language models bind entities in context?
\newblock \emph{arXiv preprint arXiv:2310.17191}, 2023.

\bibitem[Fu(2024)]{fu2024attentionpattern}
Yao Fu.
\newblock How do language models put attention weights over long context?
\newblock \emph{Yao Fu’s Notion}, 2024.
\newblock URL
  \url{https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context-10250219d5ce42e8b465087c383a034e?pvs=4}.

\bibitem[Geva et~al.(2023)Geva, Bastings, Filippova, and
  Globerson]{geva2023dissecting}
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.
\newblock Dissecting recall of factual associations in auto-regressive language
  models.
\newblock \emph{arXiv preprint arXiv:2304.14767}, 2023.

\bibitem[Gholami et~al.(2022)Gholami, Kim, Dong, Yao, Mahoney, and
  Keutzer]{gholami2022survey}
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael~W Mahoney, and Kurt
  Keutzer.
\newblock A survey of quantization methods for efficient neural network
  inference.
\newblock In \emph{Low-Power Computer Vision}, pages 291--326. Chapman and
  Hall/CRC, 2022.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney,
  Tafjord, Jha, Ivison, Magnusson, Wang, et~al.]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind
  Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv preprint arXiv:2402.00838}, 2024.

\bibitem[Gu et~al.(2024)Gu, Pang, Du, Liu, Zhang, Du, Wang, and
  Lin]{gu2024attention}
Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du,
  Ye~Wang, and Min Lin.
\newblock When attention sink emerges in language models: An empirical view.
\newblock \emph{arXiv preprint arXiv:2410.10781}, 2024.

\bibitem[Guo et~al.(2023)Guo, Hu, Mei, Wang, Xiong, Savarese, and
  Bai]{guo2023transformers}
Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and
  Yu~Bai.
\newblock How do transformers learn in-context beyond simple functions? {A}
  case study on learning with representations.
\newblock \emph{arXiv preprint arXiv:2310.10616}, 2023.

\bibitem[Guo et~al.(2024)Guo, Kamigaito, and Watanabe]{guo2024attention}
Zhiyu Guo, Hidetaka Kamigaito, and Taro Watanabe.
\newblock Attention score is not all you need for token importance indicator in
  {KV} cache reduction: {V}alue also matters.
\newblock \emph{arXiv preprint arXiv:2406.12335}, 2024.

\bibitem[Gurnee et~al.(2024)Gurnee, Horsley, Guo, Kheirkhah, Sun, Hathaway,
  Nanda, and Bertsimas]{gurnee2024universal}
Wes Gurnee, Theo Horsley, Zifan~Carl Guo, Tara~Rezaei Kheirkhah, Qinyi Sun,
  Will Hathaway, Neel Nanda, and Dimitris Bertsimas.
\newblock Universal neurons in {GPT2} language models.
\newblock \emph{arXiv preprint arXiv:2401.12181}, 2024.

\bibitem[Han et~al.(2023)Han, Wang, Xiong, Chen, Ji, and Wang]{han2023lm}
Chi Han, Qifan Wang, Wenhan Xiong, Yu~Chen, Heng Ji, and Sinong Wang.
\newblock {LM-Infinite}: Simple on-the-fly length generalization for large
  language models.
\newblock \emph{arXiv preprint arXiv:2308.16137}, 2023.

\bibitem[Horn and Johnson(2012)]{horn2012matrix}
Roger~A Horn and Charles~R Johnson.
\newblock \emph{Matrix Analysis}.
\newblock Cambridge University Press, 2012.

\bibitem[Hu et~al.(2024)Hu, Chang, Luo, Chen, Li, Wang, and Liu]{hu2024outlier}
Jerry Yao-Chieh Hu, Pei-Hsuan Chang, Robin Luo, Hong-Yu Chen, Weijian Li,
  Wei-Po Wang, and Han Liu.
\newblock Outlier-efficient hopfield layers for large transformer-based models.
\newblock \emph{arXiv preprint arXiv:2404.03828}, 2024.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023context}
Yu~Huang, Yuan Cheng, and Yingbin Liang.
\newblock In-context convergence of transformers.
\newblock \emph{arXiv preprint arXiv:2310.05249}, 2023.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2704--2713, 2018.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23,
  2016, Proceedings, Part I 16}, pages 795--811. Springer, 2016.

\bibitem[Kim et~al.(2024)Kim, Nakamaki, and Suzuki]{kim2024transformers}
Juno Kim, Tai Nakamaki, and Taiji Suzuki.
\newblock Transformers are minimax optimal nonparametric in-context learners.
\newblock \emph{arXiv preprint arXiv:2408.12186}, 2024.

\bibitem[Lin et~al.(2024{\natexlab{a}})Lin, Xu, Wu, Cui, Zhang, Mou, Song, Sun,
  and Wei]{lin2024duquant}
Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi
  Song, Zhenan Sun, and Ying Wei.
\newblock Duquant: Distributing outliers via dual transformation makes stronger
  quantized llms.
\newblock \emph{arXiv preprint arXiv:2406.01721}, 2024{\natexlab{a}}.

\bibitem[Lin et~al.(2024{\natexlab{b}})Lin, Tang, Tang, Yang, Chen, Wang, Xiao,
  Dang, Gan, and Han]{lin2024awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang,
  Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.
\newblock Awq: Activation-aware weight quantization for on-device llm
  compression and acceleration.
\newblock \emph{Proceedings of Machine Learning and Systems}, 6:\penalty0
  87--100, 2024{\natexlab{b}}.

\bibitem[Lin et~al.(2023)Lin, Bai, and Mei]{lin2023transformers}
Licong Lin, Yu~Bai, and Song Mei.
\newblock Transformers as decision makers: Provable in-context reinforcement
  learning via supervised pretraining.
\newblock \emph{arXiv preprint arXiv:2310.08566}, 2023.

\bibitem[Lin et~al.(2020)Lin, Li, Liu, Xiao, Liu, and Zhu]{lin2020towards}
Ye~Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, and Jingbo Zhu.
\newblock Towards fully 8-bit integer inference for the transformer model.
\newblock \emph{arXiv preprint arXiv:2009.08034}, 2020.

\bibitem[Liu et~al.(2024)Liu, Bai, Lin, Li, Gao, Xu, Hou, Yao, and
  Yuan]{liu2024intactkv}
Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu~Hou,
  Jun Yao, and Chun Yuan.
\newblock Intactkv: Improving large language model quantization by keeping
  pivot tokens intact.
\newblock \emph{arXiv preprint arXiv:2403.01241}, 2024.

\bibitem[Liu et~al.(2022)Liu, Kitouni, Nolte, Michaud, Tegmark, and
  Williams]{liu2022towards}
Ziming Liu, Ouail Kitouni, Niklas~S Nolte, Eric Michaud, Max Tegmark, and Mike
  Williams.
\newblock Towards understanding grokking: An effective theory of representation
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 34651--34663, 2022.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17359--17372, 2022.

\bibitem[Nagel et~al.(2021)Nagel, Fournarakis, Amjad, Bondarenko, Van~Baalen,
  and Blankevoort]{nagel2021white}
Markus Nagel, Marios Fournarakis, Rana~Ali Amjad, Yelysei Bondarenko, Mart
  Van~Baalen, and Tijmen Blankevoort.
\newblock A white paper on neural network quantization.
\newblock \emph{arXiv preprint arXiv:2106.08295}, 2021.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and
  Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2301.05217}, 2023.

\bibitem[Nichani et~al.(2024)Nichani, Damian, and Lee]{nichani2024transformers}
Eshaan Nichani, Alex Damian, and Jason~D Lee.
\newblock How transformers learn causal structure with gradient descent.
\newblock \emph{arXiv preprint arXiv:2402.14735}, 2024.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Reddy(2023)]{reddy2023mechanistic}
Gautam Reddy.
\newblock The mechanistic basis of data dependence and abrupt learning in an
  in-context classification task.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson,
  Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson,
  Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,
  et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model
  pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Son et~al.(2024)Son, Park, Han, Kim, and Lee]{son2024prefixing}
Seungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, and Jaeho Lee.
\newblock Prefixing attention sinks can mitigate activation outliers for large
  language model quantization.
\newblock \emph{arXiv preprint arXiv:2406.12016}, 2024.

\bibitem[Sun et~al.(2024)Sun, Chen, Kolter, and Liu]{sun2024massive}
Mingjie Sun, Xinlei Chen, J~Zico Kolter, and Zhuang Liu.
\newblock Massive activations in large language models.
\newblock \emph{arXiv preprint arXiv:2402.17762}, 2024.

\bibitem[Tian et~al.(2023{\natexlab{a}})Tian, Wang, Chen, and Du]{tian2023scan}
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon~S Du.
\newblock Scan and {S}nap: Understanding training dynamics and token
  composition in 1-layer transformer.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:\penalty0 71911--71947, 2023{\natexlab{a}}.

\bibitem[Tian et~al.(2023{\natexlab{b}})Tian, Wang, Zhang, Chen, and
  Du]{tian2023joma}
Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.
\newblock Joma: Demystifying multilayer transformers via joint dynamics of mlp
  and attention.
\newblock \emph{arXiv preprint arXiv:2310.00535}, 2023{\natexlab{b}}.

\bibitem[Todd et~al.(2023)Todd, Li, Sharma, Mueller, Wallace, and
  Bau]{todd2023function}
Eric Todd, Millicent~L Li, Arnab~Sen Sharma, Aaron Mueller, Byron~C Wallace,
  and David Bau.
\newblock Function vectors in large language models.
\newblock \emph{arXiv preprint arXiv:2310.15213}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vaswani(2017)]{vaswani2017attention}
A~Vaswani.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-Dimensional Probability: An Introduction with Applications
  in Data Science}, volume~47.
\newblock Cambridge University Press, 2018.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and
  Steinhardt]{wang2022interpretability}
Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob
  Steinhardt.
\newblock Interpretability in the wild: a circuit for indirect object
  identification in {GPT}-2 small.
\newblock \emph{arXiv preprint arXiv:2211.00593}, 2022.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Zou, Chen, Braverman, Gu, and
  Bartlett]{wu2023many}
Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and
  Peter~L Bartlett.
\newblock How many pretraining tasks are needed for in-context learning of
  linear regression?
\newblock \emph{arXiv preprint arXiv:2310.08391}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Li, Aminabadi, Yao, and
  He]{wu2023understanding}
Xiaoxia Wu, Cheng Li, Reza~Yazdani Aminabadi, Zhewei Yao, and Yuxiong He.
\newblock Understanding int4 quantization for language models: latency speedup,
  composability, and failure cases.
\newblock In \emph{International Conference on Machine Learning}, pages
  37524--37539. PMLR, 2023{\natexlab{b}}.

\bibitem[Xiao et~al.(2023{\natexlab{a}})Xiao, Lin, Seznec, Wu, Demouth, and
  Han]{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock In \emph{International Conference on Machine Learning}, pages
  38087--38099. PMLR, 2023{\natexlab{a}}.

\bibitem[Xiao et~al.(2023{\natexlab{b}})Xiao, Tian, Chen, Han, and
  Lewis]{xiao2023efficient}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock \emph{arXiv preprint arXiv:2309.17453}, 2023{\natexlab{b}}.

\bibitem[Yao et~al.()Yao, Aminabadi, Zhang, Wu, Li, and He]{yao2206efficient}
Z~Yao, RY~Aminabadi, M~Zhang, X~Wu, C~Li, and Y~Zeroquant He.
\newblock Efficient and affordable post-training quantization for large-scale
  transformers, 2022.
\newblock \emph{URL https://arxiv. org/abs/2206.01861}.

\bibitem[Yao et~al.(2022)Yao, Yazdani~Aminabadi, Zhang, Wu, Li, and
  He]{yao2022zeroquant}
Zhewei Yao, Reza Yazdani~Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
  Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27168--27183, 2022.

\bibitem[Yu et~al.(2024)Yu, Wang, Fu, Shi, Shaikh, and Lin]{yu2024unveiling}
Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, and
  Yingyan~Celine Lin.
\newblock Unveiling and harnessing hidden attention sinks: Enhancing large
  language models without training through attention calibration.
\newblock \emph{arXiv preprint arXiv:2406.15765}, 2024.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat]{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock Q8bert: Quantized 8bit bert.
\newblock In \emph{2019 Fifth Workshop on Energy Efficient Machine Learning and
  Cognitive Computing-NeurIPS Edition (EMC2-NIPS)}, pages 36--39. IEEE, 2019.

\bibitem[Zhai et~al.(2023)Zhai, Likhomanenko, Littwin, Busbridge, Ramapuram,
  Zhang, Gu, and Susskind]{zhai2023stabilizing}
Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason
  Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua~M Susskind.
\newblock Stabilizing transformer training by preventing attention entropy
  collapse.
\newblock In \emph{International Conference on Machine Learning}, pages
  40770--40803. PMLR, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Wu, and Bartlett]{zhang2024context}
Ruiqi Zhang, Jingfeng Wu, and Peter~L Bartlett.
\newblock In-context learning of a linear transformer block: Benefits of the
  {MLP} component and one-step {GD} initialization.
\newblock \emph{arXiv preprint arXiv:2402.14951}, 2024.

\bibitem[Zhang et~al.(2022)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and
  Wagner]{zhang2022unveiling}
Yi~Zhang, Arturs Backurs, S{\'e}bastien Bubeck, Ronen Eldan, Suriya Gunasekar,
  and Tal Wagner.
\newblock Unveiling transformers with {LEGO}: A synthetic reasoning task.
\newblock \emph{arXiv preprint arXiv:2206.04301}, 2022.

\bibitem[Zhu et~al.(2024)Zhu, Huang, Zhang, Jordan, Jiao, Tian, and
  Russell]{zhu2024towards}
Hanlin Zhu, Baihe Huang, Shaolun Zhang, Michael Jordan, Jiantao Jiao, Yuandong
  Tian, and Stuart Russell.
\newblock Towards a theoretical understanding of the `reversal curse' via
  training dynamics.
\newblock \emph{arXiv preprint arXiv:2405.04669}, 2024.

\bibitem[Zhu and Li(2023)]{zhu2023physics}
Zeyuan~Allen Zhu and Yuanzhi Li.
\newblock Physics of language models: {P}art 3.1, knowledge storage and
  extraction.
\newblock \emph{arXiv preprint arXiv:2309.14316}, 2023.

\end{thebibliography}
