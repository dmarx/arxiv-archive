\contentsline {section}{\numberline {1}Introduction}{1}{section.1}%
\contentsline {subsection}{\numberline {1.1}Related work}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Preliminaries and notations}{4}{subsection.1.2}%
\contentsline {section}{\numberline {2}Extreme-token Phenomena in the Bigram-Backcopy Task}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}One-layer transformer exhibits attention sinks and value-state drains}{5}{subsection.2.1}%
\contentsline {paragraph}{The active-dormant mechanism\nobreakspace {}of the attention head.}{6}{section*.4}%
\contentsline {paragraph}{The growth of attention logits on the \texttt {$\langle \texttt {s}\rangle $}\nobreakspace {}token and the decrease in its value state norms.}{6}{section*.7}%
\contentsline {subsection}{\numberline {2.2}Analysis of a minimally-sufficient transformer architecture}{7}{subsection.2.2}%
\contentsline {paragraph}{Simplification and reparameterization of the model.}{8}{section*.9}%
\contentsline {paragraph}{Dynamic analyses of the reparameterized model.}{9}{section*.10}%
\contentsline {paragraph}{The formation of attention sinks and value-state drains.}{10}{section*.11}%
\contentsline {paragraph}{Experimental verification of the quantitative prediction.}{10}{section*.13}%
\contentsline {paragraph}{Generality of the theoretical prediction.}{10}{section*.14}%
\contentsline {paragraph}{Replacing SoftMax by ReLU attention removes attention sinks and value-state drains.}{11}{section*.16}%
\contentsline {subsection}{\numberline {2.3}The emergence of residual-state peaks}{11}{subsection.2.3}%
\contentsline {paragraph}{Massive residual state at layer 0 output induces attention sinks and value-state drains in the middle layer.}{11}{section*.17}%
\contentsline {paragraph}{Linear growth of residual-state norm with Adam training.}{11}{section*.18}%
\contentsline {paragraph}{Switching from Adam to SGD and switching from SoftMax to ReLU attention eliminates the residual-state peaks.}{12}{section*.19}%
\contentsline {section}{\numberline {3}Extreme-token Phenomena in pretrained LLMs}{12}{section.3}%
\contentsline {subsection}{\numberline {3.1}Active-dormant mechanism in LLMs}{12}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Extreme-token phenomena along training dynamics of LLMs}{13}{subsection.3.2}%
\contentsline {section}{\numberline {4}Conclusions}{14}{section.4}%
\contentsline {section}{\numberline {A}Proofs of Theorem\nobreakspace {}\ref {thm:construction} and\nobreakspace {}\ref {thm:main}}{21}{appendix.A}%
\contentsline {subsection}{\numberline {A.1}Proof of Theorem\nobreakspace {}\ref {thm:construction}}{23}{subsection.A.1}%
\contentsline {subsection}{\numberline {A.2}Proof of Theorem\nobreakspace {}\ref {thm:main}(c): Stable phase}{24}{subsection.A.2}%
\contentsline {subsection}{\numberline {A.3}Proof of Theorem\nobreakspace {}\ref {thm:main}(a): Attention sinks}{28}{subsection.A.3}%
\contentsline {subsection}{\numberline {A.4}Proof of Theorem\nobreakspace {}\ref {thm:main}(b): Value-state drains}{29}{subsection.A.4}%
\contentsline {section}{\numberline {B}The Linear Growth of the Residual States}{31}{appendix.B}%
\contentsline {subsection}{\numberline {B.1}The minimal model structure to recapitulate residual state peak}{31}{subsection.B.1}%
\contentsline {subsection}{\numberline {B.2}Additional plots for the three-layer transformer trained on BB task}{31}{subsection.B.2}%
\contentsline {subsection}{\numberline {B.3}Potential mechanism for linear growth of the residual state peak in multi-layer models}{31}{subsection.B.3}%
\contentsline {section}{\numberline {C}Ablations}{34}{appendix.C}%
\contentsline {subsection}{\numberline {C.1}Experimental details}{34}{subsection.C.1}%
\contentsline {subsection}{\numberline {C.2}Additional attention plots of a 1-layer transformer trained on the BB task}{34}{subsection.C.2}%
\contentsline {subsection}{\numberline {C.3}Statics and dynamics of the simplified model in Theorem\nobreakspace {}\ref {thm:main}}{34}{subsection.C.3}%
\contentsline {subsection}{\numberline {C.4}The Bigram-Backcopy task without the \texttt {$\langle \texttt {s}\rangle $}\nobreakspace {}token.}{34}{subsection.C.4}%
\contentsline {section}{\numberline {D}More Attention Heads in Dormant and Active Phase}{36}{appendix.D}%
\contentsline {section}{\numberline {E}Fine-Grained Static Mechanisms for Extreme-Token Phenomena}{37}{appendix.E}%
\contentsline {paragraph}{Attention sinks and global contextual semantics.}{37}{section*.39}%
\contentsline {paragraph}{Value-state drains.}{38}{section*.41}%
\contentsline {paragraph}{Residual state peaks.}{38}{section*.42}%
\contentsline {section}{\numberline {F}Extreme-Token Phenomena Over Many Samples}{39}{appendix.F}%
\contentsline {section}{\numberline {G}Assorted Caveats}{41}{appendix.G}%
\contentsline {subsection}{\numberline {G.1}Multiple attention sinks vs. one attention sink}{41}{subsection.G.1}%
\contentsline {subsection}{\numberline {G.2}The role of a fixed \texttt {$\langle \texttt {s}\rangle $}\nobreakspace {} token in the Active-Dormant mechanism}{41}{subsection.G.2}%
