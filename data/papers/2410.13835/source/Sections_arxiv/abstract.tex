\begin{abstract}
We investigate the mechanisms behind three puzzling phenomena observed in transformer-based large language models (LLMs): \textit{attention sinks}, \textit{value-state drains}, and \textit{residual-state peaks}, collectively referred to the \textit{extreme-token phenomena}. \sm{Briefly describe what are these phenomenon, and what is the consequence.} First, we demonstrate that these phenomena also arise in simpler architectures—transformers with one to three layers—trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an \textit{active-dormant mechanism} that causes attention heads to become attention sinks for certain domain-specific inputs while remaining non-sinks for others. We further develop a precise theoretical characterization of the training dynamics that lead to these phenomena, revealing that they are driven by a \textit{mutual reinforcement mechanism}. By small interventions, we demonstrate ways to avoid extreme-token phenomena during pre-training. Next, we extend our analysis to pre-trained LLMs, including Llama and OLMo, revealing that many attention heads are governed by a similar active-dormant mechanism as in the BB task. We further show that the same mutual reinforcement mechanism drives the emergence of extreme-token phenomena during LLM pre-training. Our results study the mechanisms behind extreme-token phenomena in both synthetic and real settings and offer potential mitigation strategies. % Finally, we offer predictions on removing extreme-token phenomena by adjusting the model architecture and training dynamics, which we validate in the BB task. \tianyu{change it to emphases the interpretability}
\end{abstract}

% \tianyu{old version:}
% Both the training and inference phases in large language models (LLMs) exhibit mysterious phenomena which may affect the model's capability or stability. In this paper, we examine the root cause of three such phenomena which have been empirically shown to occur in trained transformers including LLMs: \textit{attention sinks}, \textit{small value states}, and \textit{massive norms}, which we refer to as \textit{outlier phenomena}. Specifically, we first investigate these phenomena in a toy setting, which we call the Bigram-Backcopy (BB) task, using small one- to three-layer transformers. We develop a precise theoretical characterization, verified through careful experiments, of when and how outlier phenomena occur, and the training dynamics which lead to them in this setting. We then show that our characterization of these phenomena extends to LLMs. We unveil a circuit and training dynamics, both essentially predicted by the BB task, showing how the outlier phenomena occurs in LLMs. In the process, we uncover a so-called \textit{switching mechanism} which ensures that attention heads are attention sinks when the input is text drawn from certain domains, while not being attention sinks for other inputs. Our results reveal the mechanisms behind the ubiquitous outlier phenomena in LLMs.

% We investigate the mechanisms behind three puzzling phenomena observed in transformer-based large language models (LLMs): \textit{attention sinks}, \textit{value-state sinks}, and \textit{residual-state peaks}, collectively referred to the \textit{extreme-token phenomena}. First, we demonstrate that these phenomena also arise in simpler architectures—transformers with one to three layers—trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an \textit{active-dormant mechanism} that causes attention heads to become attention sinks for certain domain-specific inputs while remaining non-sinks for others. We further develop a precise theoretical characterization of the training dynamics that lead to these phenomena, revealing that they are driven by a \textit{cyclic-reinforcement mechanism}. 

% Next, we extend our analysis to pre-trained LLMs, including Llama and OLMo, revealing that many attention heads are governed by similar active-dormant mechanism as in the BB task. We further show that the same cyclic-reinforcement mechanism drives the emergence of extreme-token phenomena during LLM pre-training. Finally, we offer predictions on how to mitigate extreme-token phenomena by adjusting the model architecture and training dynamics, which we validate in the BB task. 