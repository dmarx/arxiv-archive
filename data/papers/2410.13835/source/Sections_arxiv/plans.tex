\section*{Outline}
\begin{itemize}
    \item \sm{Title: Birth of Attention Sink in Large Language Models}
    \item \tianyu{Title: Switching Attention Heads: Mechanistically Demystifying Outlier Phenomena in LLMs}
    \item \DP{Title: Outlier Phenomena in Large Language Models}
\end{itemize}
% \paragraph{Some concepts: }
% \begin{itemize}
%     \item Dormant attention head: given an input, a head that effectively outputs a bias, i.e., doesn't have cross token interactions
%     \item Active attention head: a head which is not dormant on the particular input
%     \item Relate dormant attention head to attention sink: the attention sink is nearly the only possible mechanism for a head to be dormant while remaining the ability to be active
%     \item Functional head: head which has some defined function(s) during forward pass
%     \item Indexing: from 0
%     \item Attention weights: The attention weight from token $i$ to the BOS is large.
%     \item Massive token \& Massive activation (need to clarify in the preliminaries)
%     \item Dormant head token score = attention score of BOS + all delimiter tokens
%     \item Dormant head detection metric = (maximum/mean) dormant head token score over all tokens in the sample
%     \item Dormant head detection thresholds = bound for dormant head detection metric, above (resp.~below) which the head is classified as dormant (resp.~active) on the given sample
%     \item \sm{Suggested name for "Dormant copy model": Bigram-Backcopy Model (BBM) or Markov Backcopy Model (MBM). }
%     \item  \sm{Message: The observed sink heads are not always dormant. Every head has a dormant phase and an active phase. In the active phase, the head transports information from previous tokens to the current one. In the dormant phase, the head behaves as if it is constantly biased. Plot the histogram of dormant metric for every head over different prompts. Use L16H25 Llama 2 as a concrete example. }
%     \item \sm{Title: Birth of Attention Sink in Large Language Models}
%     \item \tianyu{Title: Attention heads learn to have dormant phase: demystifying (attention sink; massive norm; value states) extreme values in LLMs}
% \end{itemize}

% \paragraph{Outline: }
% \begin{itemize}
%     \item Section 1: Intro and related work
%     \begin{itemize}
%         \item Introduction (TODO: DP)
%         \item Related work (TODO: SM? + DP)
%     \end{itemize}
%     \item Section 2: Case study as motivation for conjecture
%     \begin{itemize}
%         \item Notations (TODO: DP)
%         \item Preliminaries (what is a transformer, what is attention, etc) (TODO: DP)
%         \item Preliminaries part 2 (what are massive activations, what are massive tokens, what are attention sinks) --- point out the figure for Layer 16 Head 25, point out it's massive activation, massive token, attention sink properties (TODO: DP)
%         \item Llama2 Layer 16 Head 25: what do we observe about its behavior on Wiki vs coding? Use the best precision we can give in the time remaining  (TODO: DP)
%         \begin{itemize}
%             \item Two side-by-side figures of attention weights on this head on Wikipedia and Github
%             inputs (TODO: DP)
%             \item Same for logits (TODO for figure generation: DP, for detailed analysis: TG)
%         \end{itemize}
%     \end{itemize}
%     \item Section 3: Conjecture and evidence 
%     \begin{itemize}
%         \item Describe dormant head conjecture in as precise language as possible (TODO: DP + TG?)
%         %\item Theoretical evidence: linear algebraic mechanism + empirical evidence: attention sinks + massive activation (TODO: DP + TG)
%         \begin{itemize}
%             \item Attention sink is nearly the only possible mechanism to keep head dormant on some inputs and active on others
%         \end{itemize}
%         \item Evidence 1: circuit in Llama2 which causes dormant head (TODO: TG)
%         \begin{itemize}
%             \item a flowchart (TG + DP)
%             \item some supporting findings: norm bar-plot of tokens? table of (mean, var) of attention logits on bos tokens and other tokens (some randomly selected tokens) 
%         \end{itemize}
        
%         \item Evidence 2: intervention study on Llama 2. Goals: show that our automated dormant head detection system is very effective with strong quantitative behavior connecting to qualitative observations, and then show that it finds many heads (TODO: DP)
%         \begin{itemize}
%             \item Graph all heads. x axis = max metric among 32 samples of Wikipedia + 32 samples of Github, y axis = min metric, top right = always dormant, bottom left = always active, etc. Red color for L16H25. (TODO: DP)
%             \item Display \# of heads detected on Wikipedia, Github, and Wikipedia+Github (TODO: DP; also include common crawl?)
%         \end{itemize}
%         \item Evidence 3: pretraining on simple tasks? (TODO: TG)
%         \begin{itemize}
%             \item Attention weights heatmap for the ``dormant copy'' experiment;
%             \item Training Dynamics of the ``dormant copy'' experiment;
%         \end{itemize}
%     \end{itemize}
%     \item Section 4: Discussions, implications on interpretability? (TODO: DP? + TG)
%     \begin{itemize}
%         \item Conjecture on the emergence of dormant heads during pretraing: task variety -> value states monosemanticity -> dormant mechanism (the supporting experiment is in appendix)
%         \item Conjecture on massive tokens and massive activations
%         \item Some parts of Circuit are not fully understood.
%         \item Contradicting observations of dormant conjecture: Relationship of dormant head to head irrelevant for task (i.e., zeroing out doesn't change PPL)?
%         \begin{itemize}
%             \item Table documenting the perplexity before and after zeroing out L16H25 on Wikipedia and Github
%             \item Table relating attention sink score to PPL difference when zeroing out; shows weak trend (\DP{TODO: need to do it for new metrics})
%         \end{itemize}
%     \end{itemize}
%     \item Section 5: Conclusion (TODO: DP)

%     \item Appendix:
%     \begin{itemize}
%         \item Supporting results for Evidence 1:
%         \begin{itemize}
%             \item An illustration figure to explain the method to find the circuit
%         \end{itemize}
%         \item Supporting results for Evidence 2:
%         \begin{itemize}
%             \item Include plots for 32 samples on Wiki/Github for the given special head L16H25
%             \item Ablation on the different dormant head token score mechanism + dormant head detection metric --- lots of figures, pretty convincing that these choices are fairly robust
%         \end{itemize}
%         \item Supporting results for Evidence 3:
%         \begin{itemize}
%             \item Ablations: Biette's setting;
%             \item Ablations part2: another one;
%             \item Interpolating between dormant copy and dormant markov
%             \item Massive tokens
%         \end{itemize}
%     \end{itemize}
%     \item REMEMBER LLAMA3 EXPS
% \end{itemize}

% \paragraph{New outline: }
% \begin{itemize}
%     \item Section 1: Intro and related work
%     \begin{itemize}
%         \item Introduction (TODO: DP)
%         \item Related work (TODO: SM + DP)
%     \end{itemize}
%     \item Section 2: Case study as motivation for investigation
%     \begin{itemize}
%         \item Notations (TODO: DP)
%         \item Preliminaries (what is a transformer, what is attention, etc) (TODO: DP)
%         \item Preliminaries part 2 (what are massive activations, what are massive tokens, what are attention sinks (TODO: DP)
%         \item  \sm{Our preliminary findings: The observed sink heads are not always dormant. Every head has a dormant phase and an active phase. In the active phase, the head transports information from previous tokens to the current one. In the dormant phase, the head behaves as if it is constantly biased. Plot the histogram of dormant metric for every head over different prompts. }
%         \item An example of the above point: Llama2 Layer 16 Head 25: what do we observe about its behavior on Wiki vs coding? Use the best precision we can give in the time remaining  (TODO: DP)
%         \begin{itemize}
%             \item Two side-by-side figures of attention weights on this head on Wikipedia and Github
%             inputs (TODO: DP)
%             \item Same for logits (TODO for figure generation: DP, for detailed analysis: TG)
%         \end{itemize}
%     \end{itemize}
%     \item Section 3: Static mechanism of attention sink, small value state, and massive norm. 
%     \begin{itemize}
%     \item Mechanism in Llama 2. 
%     \item Mechanism in Llama 3.1. 
%     \item Pretraining of Bigram-Backcopy task. One-layer TF demonstrates the attention sink and small value state phenomena. Three-layer TF demonstrates the massive norm phenomenon. 
%     \end{itemize}
%     \item Section 4: Dynamical mechanism of attention sink, small value state, and massive norm. 
%     \begin{itemize}
%     \item Statement of the mechanism: For any attention head with a certain prompt, if the value vector of any seen token is not useful in predicting the next token, the gradient dynamics, together with the softmax mechanism, will push the attention weights to the token with a small value vector. For tokens that are non-useful in predicting the next token in all prompts, since their attention weights are large, gradient dynamics will push down their value vector norm. So, large attention weights and small value vector norms reinforce each other, making such tokens sink tokens. Gradient dynamics favors pushing early tokens to be sink tokens. \sm{How to state the mechanism of the massive norm? }
%     \item Experimental evidence in BBM. (ReLU attention does not exhibit the behavior. ) 
%     \item Theoretical explanation in BBM. 
%     \item Experimental evidence in the pre-training checkpoint of Pythia. 
%     \end{itemize}
%     \item Section 5: Conclusion (TODO: DP)
% \end{itemize}

% \paragraph{New outline-copy: }
% \begin{itemize}
%     \item Section 1: Intro and related work
%     \begin{itemize}
%         \item Introduction (TODO: DP)
%         \item Related work (TODO: SM + DP)
%         \item Notations (TODO: DP)
%         \item Preliminaries (what is a transformer, what is attention, etc) (TODO: DP)
%         \item Preliminaries part 2 (what are massive activations, what are massive tokens, what are attention sinks (TODO: DP
%     \end{itemize}
%     \item Section 2: State our ``conjecture'' and show all experiments on LLMs.
    
% )
% \begin{itemize}
%         \item  \sm{Our preliminary findings: The observed sink heads are not always dormant. Every head has a dormant phase and an active phase. In the active phase, the head transports information from previous tokens to the current one. In the dormant phase, the head behaves as if it is constantly biased. Plot the histogram of dormant metric for every head over different prompts. }
%         \item An example of the above point: Llama2 Layer 16 Head 25: what do we observe about its behavior on Wiki vs coding? Use the best precision we can give in the time remaining  (TODO: DP)
%         \begin{itemize}
%             \item Two side-by-side figures of attention weights on this head on Wikipedia and Github
%             inputs (TODO: DP)
%             \item Same for logits (TODO for figure generation: DP, for detailed analysis: TG)
%         \end{itemize}
%     \item Mechanism in Llama 2. 
%     \item Mechanism in Llama 3.1. 
%     \item Experimental evidence in the pre-training checkpoint of Pythia. 
%     \end{itemize}
%     \item Section 3: Controlled experiments. 
%     \begin{itemize}
%     \item Pretraining of Bigram-Backcopy task. One-layer TF demonstrates the attention sink and small value state phenomena. Three-layer TF demonstrates the massive norm phenomenon. 
%     \item dynamics
%     \item (the importance of value states meaning)
%     \end{itemize}
%     \item Section 4: Dynamical mechanism of attention sink, small value state, and massive norm. 
%     \begin{itemize}
%     \item Theoretical explanation in BBM. (motivated by the results in Section 3)
%     \item (Statement of the mechanism): For any attention head with a certain prompt, if the value vector of any seen token is not useful in predicting the next token ({\color{red} the value vectors for seen tokens are useful for a different task}), the gradient dynamics, together with the softmax mechanism, will push the attention weights to the token with a small value vector. For tokens that are non-useful in predicting the next token in all prompts, since their attention weights are large, gradient dynamics will push down their value vector norm. So, large attention weights and small value vector norms reinforce each other, making such tokens sink tokens. Gradient dynamics favors pushing early tokens to be sink tokens. \sm{How to state the mechanism of the massive norm? }
%     \item massive norm: (1. related to attention sink; 2. the influence function is constant; 3. the Adam update is constant.)
%     \item (with our theory, we predict some experimental results): Show more experimental evidence in BBM. (ReLU attention does not exhibit the behavior; Simplified model shows similar dynamics (+value state, +massive norm); SGD eliminates the massive norms. )
    
    
%     \end{itemize}
%     \item Section 5: Conclusion (TODO: DP)
% \end{itemize}

Some token is a $\{$attention sink, value-state drain, residual-state peak$\}$. 

Some token has $\{$dominant attention score, small value state, massive residual state$\}$. 

"Outlier phenomena" vs "Extreme-value phenomena" vs "Extreme-token phenomena" vs "Sink Phenomena". 

"Active phase and dormant phase", "Active phase and idle phase". 

Mechanism: "active-dormant mechanism", "active-idle mechanism", "switching mechanism". 

Dynamical mechanism: "Cyclic-Reinforcement Mechanism", "Mutual Reinforcement Mechanism"


\paragraph{NEW New outline-copy (based on Tianyu's Slack proposal): }
\begin{itemize}
    \item Section 1: Intro and related work
    \begin{itemize}
        \item Introduction (TODO: DP)
        \item Related work (TODO: SM + DP)
        \item Notations (TODO: DP)
        \item Preliminaries (what is a transformer, what is attention, etc) (TODO: DP)
        \item Preliminaries part 2 (what are massive activations, what are massive tokens, what are attention sinks (TODO: DP
    \end{itemize}

    \item Section 2: Bigram-Backcopy task to isolate attention sink phenomenon. Statics and Dynamics.
    \begin{itemize}
    \item Motivate Bigram-Backcopy task. The optimal implementation should result in a ``controlled'' attention sink.
    \item Theoretical results of training dynamics in bigram-backcopy task.
    \item Empirical results of pretraining in BB task.
    \begin{itemize}
        \item Statics. One-layer TF demonstrates the attention sink and small value state phenomena. Three-layer TF demonstrates the massive norm phenomenon. 
        \item Circuit analysis and dynamics. Statement of the mechanism of attention sink in BB task: For any attention head with a certain prompt, if the value vector of any seen token is not useful in predicting the next token ({\color{red} the value vectors for seen tokens are useful for a different task}), the gradient dynamics, together with the softmax mechanism, will push the attention weights to the token with a small value vector. For tokens that are non-useful in predicting the next token in all prompts, since their attention weights are large, gradient dynamics will push down their value vector norm. So, large attention weights and small value vector norms reinforce each other, making such tokens sink tokens. Gradient dynamics favors pushing early tokens to be sink tokens. \sm{How to state the mechanism of the massive norm? } 
        \item massive norm: (1. related to attention sink; 2. the influence function is constant; 3. the Adam update is constant.) When there are multiple layers (>=3), the output of the first layer (both attn and mlp) facilitates the formation of attention sink in the upper layers (denote this output as $\boldsymbol{m}$)-> the scale of the $\nabla_{\text{Params in layer 0}} \ltwo{\boldsymbol{m}}$ remains constant -> due to the layer norm in upper layers, the norm of the projection of $\nabla_{\text{Params in layer 0}} \text{Loss} $ onto the direction of $\nabla_{\text{Params in layer 0}} \ltwo{\boldsymbol{m}}$ shrinks quickly. -> due to Adam, the small gradients lead to constant update. \DP{Check this in OLMO...}
        
        \item Additional predictions from this theoretical understanding. Show more experimental evidence in BBM. (ReLU attention does not exhibit the behavior; Simplified model shows similar dynamics (+value state, +massive norm); SGD eliminates the massive norms. )
        \item \DP{Major point: the predictions made in this section NEED to transfer to LLMs.}
    \end{itemize}
    \end{itemize}

    \item Section 3: Show that predictions from BB task translate to LLMs.
    \begin{itemize}
        \item  \sm{Our preliminary findings: The observed sink heads are not always dormant. Every head has a dormant phase and an active phase. In the active phase, the head transports information from previous tokens to the current one. In the dormant phase, the head behaves as if it is constantly biased. Plot the histogram of dormant metric for every head over different prompts. }
        \item An example of the above point: Llama2 Layer 16 Head 25: what do we observe about its behavior on Wiki vs coding? Use the best precision we can give in the time remaining  (TODO: DP) \DP{Characterize dormant vs active phase in Llama 2 L16H25}
        \begin{itemize}
            \item Two side-by-side figures of attention weights on this head on Wikipedia and Github
            inputs (TODO: DP)
            \item Same for logits (TODO for figure generation: DP, for detailed analysis: TG)
        \end{itemize}
    \item Mechanism in Llama 2. 
    \item Mechanism in Llama 3.1. \DP{this is substantially easier; the main point is to note that the mechanism presents differently with multiple layers, but the idea carries through.}
    \item Experimental evidence in the pre-training checkpoint of OLMo. \DP{all dynamics results: value state norms, attention weights, and norms of tokens.}
    \end{itemize}
    \item Section 4: Conclusion (TODO: DP)
\end{itemize}


% \begin{itemize}
% \item Prelim, report existing findings.
% \item Linear algebraic mechanism
% \begin{itemize}
% \item Attention concentration (correlation, not massive norm), value anti-concentration.
% \item Birth of sink tokens: mechanism of MLP1, effect of Rope, ...
% \end{itemize}
% \item Dormant head hypothesis
% \begin{itemize}
% \item Dormant head in LlaMa2: L16H25.~\yub{Need a bunch of control experiments.}~\yub{Ideally, find other dormant heads, say still use RedPajama dataset to identify.}
% \item \yub{something else?} (Find more signals for dormant head in LlaMa2.)
% \item \yub{For \emph{every} attention sink head (can use some heuristic method to define what an attention sink head is), can we find sentence to activate it and sentence to deactivate it?}
% \item \yub{Theory? e.g. approximation results for specific kinds of heads with/without attention sink (or BOS token). }
% \item \yub{Any attention head can be modified to a dormant head, which is the same head on some input sequence, and deactivated on some other input sequence (with some separation from the original inputs).}
% \end{itemize}
% \item Pretraining with small transformers and synthetic data
% \begin{itemize}
% \item Dormant head phenomenon in Bietti's experiment
% \begin{itemize}
% \item Attention concentration -> massive norm -> massive activation.~\yub{Any experiments that show this quantitatively?}
% % ~\yub{GPT2 does not use RoPE, so do not expect massive activation. But massive activation paper they report massive activation for GPT2??? LayerNorm also makes standard coordinates special.}
% \end{itemize}
% \item 50-50 mixture of pure Markov transition prediction and pure induction head phenomenon.~\yub{Intuition: In RepICL where every sequence requires the same task (Rep + ridge regression), there is no attention sink. }
% \item \yub{TODO: Try full sequence copying after some special trigger token.}
% \item 50-50 mixture of any two tasks. Can get you more deactivated heads / attention sink.
% \item \yub{Try ReLU transformer on Bietti's experiment.}
% \end{itemize}
% \item Qualitative difference between LlaMa2 and LlaMa3?
% \end{itemize}

% \section*{TODOs}

% \begin{itemize}
%     % \item Conjecture: Where attention sink appears $\approx$ that layer is not very important, does some simple functional. Can we characterize that functional in function space?
%     \item Attention sink $\equiv$ bias (added after QKV)?
%     \begin{align*}
%         \bh_i' = \bh_i + \sum_{j=1}^i \softmax\paren{ (\<\bq_i, \bk_j\>)_{j=1}^i }_j \bv_j \approx \bh_i + \sum_{j=2}^i \softmax\paren{ (\<\bq_i, \bk_j\>)_{j=1}^i }_j \bv_j + \bbb
%     \end{align*}
%     Conjecture: Training an architecture with bias, can alleviate all the unbalancedness, and get rid of the streaming LLM problem.

%     \item Training dynamics of GPT2?
%     \begin{itemize}
%         \item Plot the gradient.
%     \end{itemize}
% \end{itemize}