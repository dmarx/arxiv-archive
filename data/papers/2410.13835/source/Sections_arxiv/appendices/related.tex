\section{Related works}
Several studies independently identified the ``attention sink'' phenomenon in language models and vision transformers,  where attention weights were found to be concentrated on a few tokens \citep{xiao2023efficient, darcet2023vision, han2023lm, zhai2023stabilizing, elhage2023privileged, dettmers2022gpt3}. Recent research has provided more detailed characterizations of this attention pattern and the attention sink phenomenon \citep{fu2024attentionpattern, sun2024massive}. \citet{sun2024massive} attributed the attention sink to the massive activation of the hidden representations of the corresponding tokens. Both \citet{sun2024massive} and \citet{zhai2023stabilizing} discussed methods for mitigating the attention sink by modifying the model and training recipes. Additionally, recent studies have leveraged the attention sink phenomenon to develop improved quantization and more efficient inference algorithms \citep{liu2024intactkv, chen2024image, yu2024unveiling, son2024prefixing}.

The dynamics of transformers are studied under various simplifications, including linear attention structures \citep{zhang2023trained,ahn2024transformers}, reparametrizations \citep{tian2023joma}, NTK \citep{deora2023optimization}, often in the setting of in-context linear regressions \citep{ahn2023linear,wu2023many,zhang2024context} and structured sequence  \citep{bietti2024birth,nichani2024transformers,tian2023scan}. Notably, \citet{zhang2023trained} proves that a one-layer linear attention head trained with gradient descent converges to a model that implements the in-context linear regression algorithm.
\cite{huang2023context,kim2024transformers} extend this to non-linear settings. \cite{bietti2024birth} shows the fast learning of bigram memorization and the slow development of in-context abilities. \cite{tian2023scan} shows the scan and snap dynamics in reparametrized one-layer transformers. \cite{reddy2023mechanistic} simplifies the structure of the induction head, showing the connection between the sharp transitions of in-context learning dynamics and the nested nonlinearities of multi-layer operations.

Mechanistic interpretability is a growing field focused on understanding the internal mechanisms of language models in solving specific tasks \citep{elhage2021mathematical, geva2023dissecting, meng2022locating, nanda2023progress, olsson2022context, bietti2024birth, wang2022interpretability, feng2023language, todd2023function}. This includes mechanisms like the induction head and function vector for in-context learning \citep{elhage2021mathematical, olsson2022context, todd2023function, bietti2024birth}, the binding ID mechanism for binding tasks \citep{feng2023language}, association-storage mechanisms for factual identification tasks \citep{meng2022locating}, and a complete circuit for indirect object identification tasks \citep{wang2022interpretability}. The task addressed in this paper is closely related to \cite{bietti2024birth}, which explored synthetic tasks where tokens are generated from either global or context-specific bigram distributions. Several other studies have also used synthetic tasks to investigate neural network mechanisms \citep{charton2022my, liu2022towards, nanda2023progress, allen2023physics, zhu2023physics, guo2023transformers, zhang2022unveiling}. 




We note that \citet{gurnee2024universal} proposed Attention Deactivation Neurons, a concept similar to Dormant Attention Heads. \citet{gurnee2024universal} hypothesized that when such a head attends to the first token, it indicates that the head is deactivated and has minimal effect. 







% For a positive integer \(n\) let \([n]\) be the set \(\{0, 1, \dots, n - 1\}\).\footnote{Throughout this paper, we will use zero-indexing, in order to keep consistency between code and math.} For a matrix \(\bX\), we denote its \(i^{\mathrm{th}}\) column by \(\bx_{i}\), its \((i, j)^{\mathrm{th}}\) entry by \(X_{ij}\), and its transpose by \(\bX^{\top}\).

% \begin{itemize}
% \item Methods: \cite{ramachandran2017searching}
% \end{itemize}


