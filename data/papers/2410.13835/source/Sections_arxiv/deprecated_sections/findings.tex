\section{Findings}

Attention sink phenomenon: For autoregressive LLMs such as Llama2, in intermediate layers (all but layers 0, 1, 2, and -1) and most heads, the $N\times N$ attention matrix is heavily concentrated on the initial token.

Consider a fixed (layer, head) in which ``attention sink'' happens. Let $(\bq_i, \bk_i, \bv_i)_{i\in[N]}$ denote the (query, key, value) vectors for this head. Let $(\bh_i)_{i\in[N]}$ denote the hidden state vector. Let $s_{ij}$ denote the attention score of query token $i$ with respect to KV token $j\in[i]$. Attention sink refers to the phenomenon that
\begin{align*}
    s_{i1} \approx 1, \quad s_{ij}\approx 0~~\textrm{for}~~2\le j\le i.
\end{align*}

\paragraph{Our findings}
\begin{enumerate}
    \item Value norms exhibit ``reverse sink'':
    \begin{align}
    \label{eqn:value}
        \ltwo{\bv_1} \ll \ltwo{\bv_j}~~\textrm{for}~~2\le j\le i,
    \end{align}
    and they balance out the attention sink in the sense that
    \begin{talign*}
        \ltwo{s_{i1}\bv_1} \approx \ltwo{\sum_{j=2}^i s_{ij}\bv_j},
    \end{talign*}
    where ``$\approx$'' denotes the same order of magnitude. This means that the head is still non-degenerate, and is not approximable by a fixed value vector. Also, the ``sparse attention'' interpretation~\citep{block2023llm} is wrong.

    % \yub{However, we can use this to by-pass the partition function in softmax and speed up the attention computation (even in FlashAttention?)}

    \item Hidden state norms exhibit a sink:
    \begin{align*}
        \ltwo{\bh_1} \gg \ltwo{\bh_j}~~\textrm{for}~~2\le j\le i.
    \end{align*}
    Combined with~\cref{eqn:value}, it must happen that the value matrix $\bV$ is nearly orthogonal to $\bh_1$, compared with to $(\bh_j)_{j\ge 2}$.
\end{enumerate}

Ablations:
\begin{enumerate}
\item Most LLMs exhibit this phenomenon (this is more exacerbated with RoPE embedding, but standard trainable positional encoding schemes like GPT-2 also exhibits this).

\item This phenomenon shows up very early in training, but not at init. 
\end{enumerate}