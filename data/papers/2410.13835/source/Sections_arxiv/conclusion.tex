\section{Conclusion} \label{sec:conclusion}

In this work, we investigated the \textit{extreme-token phenomena}, namely \textit{attention sinks}, \textit{value state drains}, and \textit{residual state peaks}. We analyzed a simple evocative model called the Bigram-Backcopy task, and theoretically and empirically showed that it exhibited the same extreme-token phenomena as in LLMs. Based on the Bigram-Backcopy task, we made several detailed predictions about the behavior of extreme-token phenomena in LLMs. In particular, we identified the \textit{active-dormant mechanism} for attention heads in both the BB model and LLMs, of which attention sinks and value state drains are indicators, and a \textit{mutual reinforcement mechanism} by which these phenomena are induced during pretraining. Using intuition about these mechanisms, we applied minor interventions to the model architecture and optimization procedure which disabled extreme-token phenomena within the BB model. Overall, our work uncovers the causes of extreme-token phenomena and points to possible pathways to eliminate them during LLM training.

We believe the most compelling direction for future work in this area is as follows. Specifically, one could build more performant and scalable interventions which would eliminate extreme-token phenomena and observe the effect on training dynamics and the finished model. This would make it easier to understand whether extreme token phenomena are necessary to build a powerful transformer-based LLM, whether they are merely helpful, or whether they are completely incidental to the particular architecture and optimization algorithms used by the community.