---
abstract: |
  In this technical report, we present Zyda-2: a five trillion token dataset for language model pretraining. Zyda-2 was used to train our Zamba2 series of models which are state-of-the-art for their weight class. We build Zyda-2 by collating high-quality open-source tokens such as FineWeb and DCLM, then distilling them to the highest-quality subset via cross-deduplication and model-based quality filtering. Zyda-2 is released under a permissive open license, and is available at <https://huggingface.co/datasets/Zyphra/Zyda-2>.
author:
- |
  Yury Tokpanov $\quad$ Paolo Glorioso $\quad$ Quentin Anthony $\quad$ Beren Millidge  
  {yury, paolo, quentin beren}@zyphra.com   
    
  Zyphra, Palo Alto, CA   
bibliography:
- main.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "Zyda-2: a 5 Trillion Token High-Quality Dataset"
---





# Introduction

Two of the primary determinants of the quality of a language model are the quality and scale of the datasets used to train it. For data quality, initial models such as GPT3  were trained purely on web datasets coarsely filtered from Common Crawl[^1]. From there, improving quality via diversity of data sources and the importance of filtering and deduplicating sources have been championed by datasets such as The Pile and FineWeb , respectively. The scale of datasets used for pretraining has also increased dramatically: while GPT3  was trained on only a few hundred billion tokens, current much-smaller models are trained on trillions of tokens. One example of this is Llama3-8b , which was trained on 15 trillion tokens. For open-source models to remain on par with closed ones, open-source datasets also need to remain competitive in both quality and scale. To meet this need for open-source, large-scale, and high-quality datasets, sophisticated data pipelines have emerged which scrape, filter, deduplicate, and mix data sources.

Open-source datasets have been a key driver of model quality, with C4 and the Pile being the first open language modeling dataset of sufficient size to serve as a pretraining dataset. More recent datasets such as Dolma and RefinedWeb have added additional levels of syntactic filtering and deduplication, raising their quality. A more recent trend has been the use of small classifier models to perform syntactic filtering. For instance, filtering for some proxy of ‘educational content’ has been used to great effect in the Phi series of models which—although most details are not published—appear to utilize both heavy filtering and a large amount of synthetic data generation. Open datasets utilizing this model-based filtering approach include FineWeb-Edu and DCLM , which perform extremely well compared to prior datasets.

At Zyphra, one of our key priorities is producing the highest-quality and most efficient models for a given parameter budget, with a special focus on small, highly powerful models which can automate many tasks cheaply and be utilized on consumer and edge devices. This has driven our innovations in model architecture and also necessitates a focus on constructing strong pretraining and annealing datasets in order to maximize the performance per FLOP and per parameter during training. High-quality datasets appear especially important for smaller models, since they have less total capacity and hence are more affected by significant quantities of noise or low-quality tokens in their training datasets.

Our general approach to dataset creation is to collect all openly available and highly-performing open-source datasets and improve their quality further by removing duplicates and adding additional filtering steps. We then weight the resulting dataset mixture to obtain highest-quality subset that meets our training budget. Our previous dataset constructed with this approach was Zyda-1 , which was used to train Zamba1-7B . Zyda-1 outperformed all major language modeling datasets at the time, such as Dolma-1.6 , FineWeb , and RefinedWeb. Since then, however, new datasets utilizing model-based filtering have been released which gave a significant boost to performance and eclipsed Zyda-1. For Zyda-2, we aimed to significantly increase the scale and quality of the dataset beyond Zyda-1, to enable us to reach the frontier of performance for small language models.

Zyda-2 was used to train our Zamba2 series of models that achieve state-of-the-art performance at all of the 1.2B, 2.7B, and 7B parameter brackets, beating strong comparable models such as Meta’s Llama3 series and Google’s Gemma series . As such, Zyda-2 provides important information about what kind of pretraining dataset is necessary to reach the performance frontier at present.

Zyda-2 is released under a permissive open-source license (ODC-BY) and can be found on HuggingFace at <https://huggingface.co/datasets/Zyphra/Zyda-2>

# Dataset Construction

<figure id="dataset_construction">
<span class="image placeholder" data-original-image-src="figures/Zyda-2_creation_process.png" data-original-image-title="" width="0.9\linewidth"></span>
<figcaption>Dataset creation process for Zyda-2. We first collated the best open-source sets available, then ran cross-deduplication between these datasets, since they all ultimately derive mostly from a common source (common-crawl). Finally, we applied model-based quality filtering to the two unfiltered datasets (Zyda-1 and Dolma-CC).</figcaption>
</figure>

Zyda-2 was built upon the following data sources: DCLM-baseline-1.0 (DCLM for short), FineWeb-Edu-score-2 (FineWeb-Edu2 for short), Zyda-1 and the Common Crawl portion of Dolma v1.7 (Dolma-CC for short)[^2]. We chose the FineWeb-Edu2 version of FineWeb-Edu in order to have a much bigger starting pool of documents. These datasets were put through a two-stage pipeline: a cross-deduplication phase between all datasets, followed by a model-based filtering stage applied to the Zyda-1 and Dolma-CC datasets (see Fig. ).

## Cross-Deduplication

Given that all open datasets ultimately originated from similar Common Crawl scrapes, we expect them to contain significant fractions of duplicated documents. Deduplication has been found to generally improve language modeling performance , although recent papers have claimed its effects can be neutral and perhaps negative when applied in the wrong context .

We used approximate minhash LSH deduplication for our deduplication pipeline with the following parameters: minhash with signature size of 128 computed on character-based 25-grams signatures and split into 8 bands, giving roughly 85% Jaccard similarity threshold. We then constructed an undirected graph with nodes being documents and edges being duplicates, and found connected components in it, which provided us with clusters of duplicates. From each cluster, we selected the top document to keep and removed the rest. We selected the remaining document according its origin in the following ranking of datasets: FineWeb-Edu2 $>$ DCLM $>$ Zyda-1 $>$ Dolma-CC; that is, when choosing a document to keep, we kept the one from the highest-ranked dataset.

As expected, we found a significant number of duplicated documents across datasets, which resulted in the removal of approximately 11% of the total tokens (or 13% of total documents) from Zyda-2 compared to its component datasets.

Additionally, we performed a full intra-dataset deduplication of both DCLM and FineWeb-Edu, since we found that both datasets contained a very large number of internal duplicates (about 80% for both datasets). Both the DCLM and FineWeb-Edu papers claim that internal dataset deduplication did not improve their results and so they did not pursue full deduplication or release a deduplicated version of their datasets. In our initial ablation experiments, we partly confirmed their results that this deduplication had only a small effect. This raises the future research questions of: 1) When is duplication harmful and when is it not? 2) Why are models seemingly robust to being trained on duplicate data?

## Model-based filtering

The second phase of processing was model-based filtering. We only applied model-based filtering to Zyda-1 and Dolma-CC. This is because Zyda-1 and Dolma-CC are less filtered and contain a higher variety of internet documents which are not all designed to be educational. DCLM and FineWeb-Edu, however, have already undergone significant quality filtering as a core component of their creation, and indeed we did not observe benefits in performing additional filtering from our training ablations.

For this step, we experimented with the quality-classifier-deberta model provided in NeMo Curator . We applied this model to filter both Zyda-1 and Dolma-CC, and experimented with either removing only the ‘low’ quality documents or keeping only the ‘high’ quality ones. In an ablation study where we trained a 1.4B parameter transformer for 50B tokens, we found that keeping only the highest quality 10-20% of the documents significantly improved model performance for both Zyda-1 and Dolma-CC (see Figure ), while only removing ‘low’ quality documents had less effect.

<figure id="fig:model_filtering_ablation">
<span class="image placeholder" data-original-image-src="figures/model_filtering_ablation.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>The performance of a 1.4B model trained on 50B tokens with and without model-based filtering on the Zyda-1 and Dolma-CC datasets. The aggregate evaluation score is the mean across the following standard language modeling benchmarks: Hellaswag, PIQA, OpenBookQA, Arc-Challenge, Arc-Easy, and Winogrande. For the quality filtering we kept only those documents labeled as ‘high-quality’ by the model-based classifier.</figcaption>
</figure>

This performance improvement likely occurs because these datasets are not pre-filtered by similar classifiers and demonstrates that, even though both datasets have undergone thorough syntactic filtering, significant gains can be had by using model-based quality classifiers. However, re-filtering already filtered datasets such as DCLM provided a negligible gain.

As an additional step, we filtered FineWeb-Edu2 by its educational score. This step essentially converts it to FineWeb-Edu, a higher-quality subset of FineWeb-Edu2 comprising samples with the higher score 3 based on the FineWeb-Edu quality filter model. This scoring method yielded the best performance in the ablations conducted during the creation of FineWeb-Edu. Notably, we applied this filtering after cross-deduplicating DCLM against FineWeb-Edu2, which involved eliminating all samples in DCLM that were deemed duplicates with FineWeb-Edu2. This sequence of steps indirectly applied the FineWeb-Edu quality classifier to DCLM, effectively removing the samples in DCLM with lower educational content.

## Composition

Following the dataset processing steps that we described above resulted in a total dataset consisting of approximately five trillion tokens. The number of tokens removed in each step of our pipeline is presented in . Additionally, the fraction of the total dataset comprised by each of the component datasets is presented in Figure .

<div class="center">

</div>

<figure id="fig:uniform_weighting_pie">
<span class="image placeholder" data-original-image-src="figures/Zyda_uniform_pie_chart.png" data-original-image-title="" width="0.9\linewidth"></span>
<figcaption>Composition of Zyda-2</figcaption>
</figure>

The majority of documents come from DCLM, which is expected given its large size compared to the other datasets. The second-largest dataset is FineWeb-Edu which comprises approximately one quarter of the dataset, with Zyda-1 and Dolma-CC having smaller contributions.

# Performance

<figure id="fig:Zyda-2-perf">
<span class="image placeholder" data-original-image-src="figures/Zyda-2_perf_barchart.png" data-original-image-title="" width="1.1\linewidth"></span>
<figcaption>Performance of Zyda-2 vs other datasets as aggregate weighted evaluation score. This score is an average of MMLU, Hellaswag, PIQA, OpenBookQA, Arc-Challenge, Arc-Easy, and Winogrande. These scores are collected by annealing the base version of Zamba2-2.7B for roughly 40B tokens on each dataset.</figcaption>
</figure>

<figure id="optimal_weighted_pie_chart">
<span class="image placeholder" data-original-image-src="figures/optimal_weighted_pie_chart.png" data-original-image-title="" width="0.9\linewidth"></span>
<figcaption>The proportion of each of the component datasets comprising Zyda-2 using the optimal weighting. FineWeb-Edu and DCLM each account for approximately the same total proportion of the dataset.</figcaption>
</figure>

Rigorously evaluating the quality of datasets is challenging without training large-scale models upon them, which costs significant compute per experiment. One typical alternative to get a reliable signal of dataset quality is to train small models on small slices of the dataset (for instance a 300 million-parameter model on 50 billion tokens) and then compare the evaluation performance of such models. This approach is taken by the majority of recent datasets released. This approach has advantages in that it is now computationally feasible to run many ablation experiments even on relatively constrained resources. However, many standard evaluations do not show strong signal for small models and low token counts (for instance, MMLU remains at chance for such models).

An alternative approach is to utilize the newly popular annealing approach to training LLMs , which involves training an LLM on a large amount of data with a relatively slow learning-rate decay followed by a rapid learning rate decay over a high quality dataset. proposes utilizing the annealing phase as a method to test the performance of different datasets. While not requiring significantly more compute than training a small model from scratch, annealing enables larger and already trained base models to be used which can show signal on important evaluation metrics such as MMLU. We follow this methodology and indeed observe significantly more signal on our dataset ablations when testing with annealing rather than training small models from scratch. We perform our annealing ablations on our pre-annealing checkpoint of our Zamba2-2.7B model which we run for 40 billion annealing tokens. Since our pre-annealing Zamba2-2.7B model is significantly above chance at MMLU and is already highly trained on approximately 3 trillion token, we observe a clear signal on all standard ablations and additionally greater sensitivity in evaluation scores to changes in dataset composition. We observe that Zyda-2 outperforms the currently leading datasets in aggregate evaluation scores and performs significantly better than any of its component datasets. This is due both to our additional filtering and deduplication pipeline and also an ensembling effect whereby adding in more varied data sources improves overall performance and robustness versus any one specific dataset.

## Subset Weightings

To boost performance further, we investigated the optimal weightings of Zyda-2’s component datasets. We conducted a series of experiments to determine this using our annealing scheme described previously. We found that a uniform weighting scheme, where each dataset is weighted by its total number of tokens, is suboptimal. Instead, upweighting FineWeb-Edu such that it became of equal proportion to DCLM exceeded the performance of the uniform weighting approach. With this weighting, due to their smaller sizes, Zyda-1 and Dolma-CC in total make up approximately 5% of the total dataset. However, we found that removing Zyda-1 and Dolma-CC entirely worsened performance, demonstrating that although their total token count is small, adding these datasets brings much-needed diversity of sources to the Zyda-2 dataset.

# Discussion

In this paper we have presented the Zyda-2 dataset, which achieves leading performance across many language evaluations. At 5 trillion tokens, Zyda-2 is perfect for large-scale pretraining. Zyda-2 is primarily focused on natural language capabilities, and, for a generalist language model, we recommend augmenting it with specialized datasets especially for coding, mathematical reasoning, and any other niche capabilities required. We constructed Zyda-2 by collecting the best existing open datasets and applying a two-stage process of cross-deduplication and model-based filtering. We found that model-based filtering significantly improved the performance of the existing unfiltered datasets while having little effect on the already filtered datasets, showing that performing additional educational quality filtering on an already-filtered dataset is not helpful.

While performing deduplication of DCLM and FineWeb-Edu, we found that both datasets contained large fractions of internal duplicates. This result raises several interesting questions. First, it is unclear why removing duplicates does not improve performance. This implies that documents with large numbers of identical tokens provide approximately the same benefit as fresh tokens from the same distribution. We can perhaps consider a highly duplicated dataset like FineWeb-Edu as equivalent to performing a 2-5 epoch shuffled run on a much smaller deduplicated dataset, implying that a small number of epochs do not particularly harm language model performance at these scales. However, this effect may not hold at larger scales, where language models are more sample-efficient at memorization of the training dataset, thus requiring more regularization to counter overfitting. On the other hand, an argument can be made that samples that are repeated more may be of higher quality, although having looked at such highly repetitive samples we doubt this since most seem to be related to common preambles or other features of websites. If this is true, however, it is unclear why earlier results showed positive effects of deduplication . One possible hypothesis is that the early datasets where deduplication improved performance were not strongly filtered by model-based classifiers like DCLM and FineWeb-Edu, and thus the deduplication step may simply be removing many low-quality spam documents that occur often in unfiltered web data. In more stringently filtered datasets, the magnitude of this effect could diminish. However, we do note that large clusters of duplicates in DCLM still contain low-quality documents: the most frequent duplicate in DCLM is indeed a spam message. Additionally, we notice that on supposedly duplicate data, the scores assigned by the quality-filtering model often vary significantly, despite only very minor differences in text. This is perhaps indicative of the relative lack of robustness in some of these classifiers or the presence of significant fractions of false-positives in the deduplication step.

Model-based quality filtering based on ‘educational quality’ as performed by FineWeb-Edu and DCLM has created a marked increase in language modeling performance, at least on commonly used language modeling benchmarks such as MMLU and the ARC tasks, which is where we observe the largest increases. Interesting questions remain around the degree to which model-based filtering can be further improved beyond current methods. Synthetic data , including augmentations of existing datasets , shows considerable promise , as does methods for using other filtering approaches such as language modeling perplexity .

# Author Contributions

**Yury** — Project lead. Performed majority of implementation and obtained experimental results. Primary coordinator with Nvidia.

**Paolo** — Contributed to model-based-filtering and deduplication.

**Quentin** — Contributed to model-based-filtering and deduplication. Contributed to dataset conceptualization. Secondary coordinator with Nvidia.

**Beren** — Overall project supervisor. Contributed to dataset conceptualization. Contributed to model-based-filtering and deduplication. Primary writer of technical report.

# Acknowledgements

We would like to acknowledge Anna Golubeva’s very helpful comments and edits on the draft of this paper. We would also like to acknowledge NVIDIA for their technical assistance and support with our use of their NeMoCurator library.

# Appendix

## Analysis of Global Duplicates

We present histograms depicting the distribution of duplicate cluster sizes in all the datasets (see Fig.-). Please, note that all the figures uselog-log scale. We see a significant drop in the number of duplicate clusters starting from around a size of 100 duplicates. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. and respectively), and most likely is explained by a combination of the deduplication strategy when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it’s not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements, which was somehow accepted by their quality filter.

We find that both Zyda-1 and Dolma-CC contain only a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure, or false positives from our own deduplication. Note, that the distribution of duplicate clusters sizes of these two datasets (Fig. and ) don’t contain any sharp drops, but rather hyper exponentially decreases with cluster size.

<figure id="fig:distr_clusters_fwe2">
<span class="image placeholder" data-original-image-src="figures/distr_clusters_overall.png" data-original-image-title="" width="\textwidth"></span>
<span class="image placeholder" data-original-image-src="figures/distr_clusters_dclm.png" data-original-image-title="" width="\textwidth"></span>
<span class="image placeholder" data-original-image-src="figures/distr_clusters_fwe2.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>Distribution of cluster sizes of duplicates in FineWeb-Edu2 (log-log scale).</figcaption>
</figure>

<figure id="fig:distr_clusters_dolma-cc">
<span class="image placeholder" data-original-image-src="figures/distr_clusters_zyda.png" data-original-image-title="" width="\textwidth"></span>
<span class="image placeholder" data-original-image-src="figures/distr_clusters_dolma-cc.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>Distribution of cluster sizes of duplicates in Dolma-CC v1.7 (log-log scale).</figcaption>
</figure>

## Largest cluster in DCLM

Below is an example of the document from the largest cluster (roughly 1M documents) of duplicates in DCLM (quality score 0.482627):

As can be observed this appears to be some kind of fairly low quality advertisement for some computer security product.

## Examples of varying quality score in DCLM in a cluster

To get a sense of what the DCLM quality filter is doing, we present below a few documents, which are selected from the same duplicate cluster, but with different quality scores from DCLM. Quality score varies from  0.2 (high quality) to  0.04 (low quality).

**Quality score of: 0.19616**

**Quality score: 0.091928**

**Quality score: 0.072259**

**Quality score: 0.0424**

[^1]: <https://commoncrawl.org/>

[^2]: The other portions of Dolma are separate math, code, and instruct datasets which can be mixed in independently or in an annealing phase.
