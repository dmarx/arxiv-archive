\begin{thebibliography}{}

\bibitem[Abdin et~al., 2024]{abdin2024phi}
Abdin, M., Jacobs, S.~A., Awan, A.~A., Aneja, J., Awadallah, A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., et~al. (2024).
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock {\em arXiv preprint arXiv:2404.14219}.

\bibitem[Ankner et~al., 2024]{ankner2024perplexed}
Ankner, Z., Blakeney, C., Sreenivasan, K., Marion, M., Leavitt, M.~L., and Paul, M. (2024).
\newblock Perplexed by perplexity: Perplexity-based data pruning with small reference models.
\newblock {\em arXiv preprint arXiv:2405.20541}.

\bibitem[Anthony et~al., 2024]{anthony2024blackmamba}
Anthony, Q., Tokpanov, Y., Glorioso, P., and Millidge, B. (2024).
\newblock Blackmamba: Mixture of experts for state-space models.
\newblock {\em arXiv preprint arXiv:2402.01771}.

\bibitem[Blakeney et~al., 2024]{blakeney2024does}
Blakeney, C., Paul, M., Larsen, B.~W., Owen, S., and Frankle, J. (2024).
\newblock Does your data spark joy? performance gains from domain upsampling at the end of training.
\newblock {\em arXiv preprint arXiv:2406.03476}.

\bibitem[Brown et~al., 2020]{gpt3}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020).
\newblock Language models are few-shot learners.
\newblock {\em CoRR}, abs/2005.14165.

\bibitem[Gao et~al., 2020]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al. (2020).
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}.

\bibitem[{Gemma Team} et~al., 2024]{team2024gemma}
{Gemma Team}, Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi{\`e}re, M., Kale, M.~S., Love, J., et~al. (2024).
\newblock Gemma: Open models based on gemini research and technology.
\newblock {\em arXiv preprint arXiv:2403.08295}.

\bibitem[Glorioso, 2024]{glorioso2024zamba2}
Glorioso, P. (2024).
\newblock Zamba2.

\bibitem[Glorioso et~al., 2024]{glorioso2024zamba}
Glorioso, P., Anthony, Q., Tokpanov, Y., Whittington, J., Pilault, J., Ibrahim, A., and Millidge, B. (2024).
\newblock Zamba: A compact 7b ssm hybrid model.

\bibitem[Hu et~al., 2024]{hu2024minicpm}
Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., et~al. (2024).
\newblock Minicpm: Unveiling the potential of small language models with scalable training strategies.
\newblock {\em arXiv preprint arXiv:2404.06395}.

\bibitem[Lee et~al., 2021]{lee2021deduplicating}
Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2021).
\newblock Deduplicating training data makes language models better.
\newblock {\em arXiv preprint arXiv:2107.06499}.

\bibitem[Li et~al., 2024]{li2024dclm}
Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S., Bansal, H., Guha, E., Keh, S., Arora, K., Garg, S., Xin, R., Muennighoff, N., Heckel, R., Mercat, J., Chen, M., Gururangan, S., Wortsman, M., Albalak, A., Bitton, Y., Nezhurina, M., Abbas, A., Hsieh, C.-Y., Ghosh, D., Gardner, J., Kilian, M., Zhang, H., Shao, R., Pratt, S., Sanyal, S., Ilharco, G., Daras, G., Marathe, K., Gokaslan, A., Zhang, J., Chandu, K., Nguyen, T., Vasiljevic, I., Kakade, S., Song, S., Sanghavi, S., Faghri, F., Oh, S., Zettlemoyer, L., Lo, K., El-Nouby, A., Pouransari, H., Toshev, A., Wang, S., Groeneveld, D., Soldaini, L., Koh, P.~W., Jitsev, J., Kollar, T., Dimakis, A.~G., Carmon, Y., Dave, A., Schmidt, L., and Shankar, V. (2024).
\newblock Datacomp-lm: In search of the next generation of training sets for language models.

\bibitem[Maini et~al., 2024]{maini2024rephrasing}
Maini, P., Seto, S., Bai, H., Grangier, D., Zhang, Y., and Jaitly, N. (2024).
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock {\em arXiv preprint arXiv:2401.16380}.

\bibitem[Meta, 2024]{llama3}
Meta (2024).
\newblock {Introducing Meta Llama 3: The most capable openly available LLM to date}.
\newblock https://ai.meta.com/blog/meta-llama-3/.
\newblock {Accessed: \today}.

\bibitem[Nvidia, 2024]{nemo_curator}
Nvidia (2024).
\newblock Nemo-curator: a toolkit for data curation.

\bibitem[Parmar et~al., 2024]{parmar2024nemotron}
Parmar, J., Prabhumoye, S., Jennings, J., Patwary, M., Subramanian, S., Su, D., Zhu, C., Narayanan, D., Jhunjhunwala, A., Dattagupta, A., et~al. (2024).
\newblock Nemotron-4 15b technical report.
\newblock {\em arXiv preprint arXiv:2402.16819}.

\bibitem[Penedo et~al., 2024]{penedo2024fineweb}
Penedo, G., Kydlíček, H., allal, L.~B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L.~V., and Wolf, T. (2024).
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.

\bibitem[Penedo et~al., 2023]{penedo2023refinedweb}
Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. (2023).
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock {\em arXiv preprint arXiv:2306.01116}.

\bibitem[Raffel et~al., 2020]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J. (2020).
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67.

\bibitem[Soldaini et~al., 2024]{soldaini2024dolma}
Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A.~H., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M.~E., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, P., Zettlemoyer, L., Smith, N.~A., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J., and Lo, K. (2024).
\newblock Dolma: an open corpus of three trillion tokens for language model pretraining research.

\bibitem[Tokpanov et~al., 2024]{tokpanov2024zyda}
Tokpanov, Y., Millidge, B., Glorioso, P., Pilault, J., Ibrahim, A., Whittington, J., and Anthony, Q. (2024).
\newblock Zyda: A 1.3 t dataset for open language modeling.
\newblock {\em arXiv preprint arXiv:2406.01981}.

\end{thebibliography}
