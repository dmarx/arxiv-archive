{
  "arxivId": "2411.06068",
  "title": "Zyda-2: a 5 Trillion Token High-Quality Dataset",
  "authors": "Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge",
  "abstract": "In this technical report, we present Zyda-2: a five trillion token dataset\nfor language model pretraining. Zyda-2 was used to train our Zamba2 series of\nmodels which are state-of-the-art for their weight class. We build Zyda-2 by\ncollating high-quality open-source tokens such as FineWeb and DCLM, then\ndistilling them to the highest-quality subset via cross-deduplication and\nmodel-based quality filtering. Zyda-2 is released under a permissive open\nlicense, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2",
  "url": "https://arxiv.org/abs/2411.06068",
  "issue_number": 679,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/679",
  "created_at": "2024-12-30T20:00:50.356278",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 33,
  "last_read": "2024-12-30T21:17:55.179882",
  "last_visited": "2024-12-30T21:17:03.498Z",
  "main_tex_file": null,
  "published_date": "2024-11-09T04:57:41Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI"
  ]
}