\begin{thebibliography}{10}

\bibitem{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card, 2024.

\bibitem{bae2024training}
Juhan Bae, Wu~Lin, Jonathan Lorraine, and Roger Grosse.
\newblock Training data attribution via approximate unrolled differentiation, 2024.

\bibitem{bae2022if}
Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger~B Grosse.
\newblock If influence functions are the answer, then what is the question?
\newblock {\em Advances in Neural Information Processing Systems}, 35:17953--17967, 2022.

\bibitem{barshan2020relatif}
Elnaz Barshan, Marc-Etienne Brunet, and Gintare~Karolina Dziugaite.
\newblock Relatif: Identifying explanatory training samples via relative influence.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 1899--1909. PMLR, 2020.

\bibitem{basu2020influence}
Samyadeep Basu, Philip Pope, and Soheil Feizi.
\newblock Influence functions in deep learning are fragile.
\newblock {\em arXiv preprint arXiv:2006.14651}, 2020.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In {\em International Conference on Machine Learning}, pages 2397--2430. PMLR, 2023.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{chen2023which}
Yixiong Chen, Alan Yuille, and Zongwei Zhou.
\newblock Which layer is learning faster? a systematic exploration of layer-wise convergence rate for deep neural networks.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{choe2024making}
Sang Choe, Sanket~Vaibhav Mehta, Hwijeen Ahn, Willie Neiswanger, Pengtao Xie, Emma Strubell, and Eric Xing.
\newblock Making scalable meta learning practical.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{dubey1981value}
Pradeep Dubey, Abraham Neyman, and Robert~James Weber.
\newblock Value theory without efficiency.
\newblock {\em Mathematics of Operations Research}, 6(1):122--128, 1981.

\bibitem{engstrom2024dsdm}
Logan Engstrom, Axel Feldmann, and Aleksander Madry.
\newblock Dsdm: Model-aware dataset selection with datamodels.
\newblock {\em arXiv preprint arXiv:2401.12926}, 2024.

\bibitem{feldman2020neural}
Vitaly Feldman and Chiyuan Zhang.
\newblock What neural networks memorize and why: Discovering the long tail via influence estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 33:2881--2891, 2020.

\bibitem{fernandez2023data}
Raul~Castro Fernandez.
\newblock Data-sharing markets: Model, protocol, and algorithms to incentivize the formation of data-sharing consortia.
\newblock In {\em Proceedings ACMSIGMOD International Conference on Management of Data}, 2023.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{ghorbani2019data}
Amirata Ghorbani and James Zou.
\newblock Data shapley: Equitable valuation of data for machine learning.
\newblock In {\em International conference on machine learning}, pages 2242--2251. PMLR, 2019.

\bibitem{gokaslan2019openwebtext}
Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex.
\newblock Openwebtext corpus, 2019.

\bibitem{grosse2023studying}
Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel~R. Bowman.
\newblock Studying large language model generalization with influence functions, 2023.

\bibitem{grosse2016kronecker}
Roger Grosse and James Martens.
\newblock A kronecker-factored approximate fisher matrix for convolution layers.
\newblock In {\em International Conference on Machine Learning}, pages 573--582. PMLR, 2016.

\bibitem{guo2020fastif}
Han Guo, Nazneen~Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong.
\newblock Fastif: Scalable influence functions for efficient model interpretation and debugging.
\newblock {\em arXiv preprint arXiv:2012.15781}, 2020.

\bibitem{hampel1974influence}
Frank~R Hampel.
\newblock The influence curve and its role in robust estimation.
\newblock {\em Journal of the american statistical association}, 69(346):383--393, 1974.

\bibitem{han2020explaining}
Xiaochuang Han, Byron~C Wallace, and Yulia Tsvetkov.
\newblock Explaining black box predictions and unveiling data artifacts through influence functions.
\newblock {\em arXiv preprint arXiv:2005.06676}, 2020.

\bibitem{hanawa2020evaluation}
Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, and Kentaro Inui.
\newblock Evaluation of similarity-based explanations.
\newblock {\em arXiv preprint arXiv:2006.04528}, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{hu2021lora}
Edward~J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{huang2023citation}
Jie Huang and Kevin Chen-Chuan Chang.
\newblock Citation: A key to building responsible and accountable large language models.
\newblock {\em arXiv preprint arXiv:2307.02185}, 2023.

\bibitem{ilyas2022datamodels}
Andrew Ilyas, Sung~Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry.
\newblock Datamodels: {P}redicting predictions from training data, 2022.

\bibitem{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{jia2019towards}
Ruoxi Jia, David Dao, Boxin Wang, Frances~Ann Hubis, Nick Hynes, Nezihe~Merve G{\"u}rel, Bo~Li, Ce~Zhang, Dawn Song, and Costas~J Spanos.
\newblock Towards efficient data valuation based on the shapley value.
\newblock In {\em The 22nd International Conference on Artificial Intelligence and Statistics}, pages 1167--1176. PMLR, 2019.

\bibitem{jlversusalphabet}
{J.L. et al. v. Alphabet Inc.}
\newblock Case 3:23-cv-03416, {N.D. Cal.}, 2023.

\bibitem{johnson2019billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with gpus.
\newblock {\em IEEE Transactions on Big Data}, 7(3):535--547, 2019.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.

\bibitem{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em International conference on machine learning}, pages 1885--1894. PMLR, 2017.

\bibitem{kokhlikyan2020captum}
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, et~al.
\newblock Captum: A unified and generic model interpretability library for pytorch.
\newblock {\em arXiv preprint arXiv:2009.07896}, 2020.

\bibitem{kwon2024datainf}
Yongchan Kwon, Eric Wu, Kevin Wu, and James Zou.
\newblock Datainf: Efficiently estimating data influence in lo{RA}-tuned {LLM}s and diffusion models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{kwon2021beta}
Yongchan Kwon and James Zou.
\newblock Beta shapley: a unified and noise-reduced data valuation framework for machine learning.
\newblock {\em arXiv preprint arXiv:2110.14049}, 2021.

\bibitem{liu2021influence}
Zhuoming Liu, Hao Ding, Huaping Zhong, Weijia Li, Jifeng Dai, and Conghui He.
\newblock Influence selection for active learning.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 9274--9283, 2021.

\bibitem{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate curvature.
\newblock In {\em International conference on machine learning}, pages 2408--2417. PMLR, 2015.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv preprint arXiv:1609.07843}, 2016.

\bibitem{metz2022lawsuit}
Cade Metz.
\newblock Lawsuit takes aim at the way {A.I.} is built.
\newblock {\em New York Times}, 2022.

\bibitem{nanda2022transformerlens}
Neel Nanda and Joseph Bloom.
\newblock Transformerlens.
\newblock \url{https://github.com/TransformerLensOrg/TransformerLens}, 2022.

\bibitem{nohyun2023data}
Ki~Nohyun, Hoyong Choi, and Hye~Won Chung.
\newblock Data valuation without training of a model.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{park2023trak}
Sung~Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry.
\newblock {TRAK}: {A}ttributing model behavior at scale.
\newblock In {\em International Conference on Machine Learning}, pages 27074--27113. PMLR, 2023.

\bibitem{pruthi2020estimating}
Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan.
\newblock Estimating training data influence by tracing gradient descent.
\newblock {\em Advances in Neural Information Processing Systems}, 33:19920--19930, 2020.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem{Razeghi2022ImpactOP}
Yasaman Razeghi, IV~RobertL.Logan, Matt Gardner, and Sameer Singh.
\newblock Impact of pretraining term frequencies on few-shot numerical reasoning.
\newblock In {\em Conference on Empirical Methods in Natural Language Processing}, 2022.

\bibitem{schioppa2022scaling}
Andrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov.
\newblock Scaling up influence functions.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pages 8179--8186, 2022.

\bibitem{shi2019understanding}
Shaohuai Shi, Xiaowen Chu, Ka~Chun Cheung, and Simon See.
\newblock Understanding top-k sparsification in distributed deep learning.
\newblock {\em arXiv preprint arXiv:1911.08772}, 2019.

\bibitem{sim2022data}
Rachael Hwee~Ling Sim, Xinyi Xu, and Bryan Kian~Hsiang Low.
\newblock Data valuation in machine learning:" ingredients", strategies, and open challenges.
\newblock In {\em IJCAI}, pages 5607--5614, 2022.

\bibitem{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock {\em arXiv preprint arXiv:2402.00159}, 2024.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2023data}
Jiachen~T Wang and Ruoxi Jia.
\newblock Data banzhaf: A robust data valuation framework for machine learning.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 6388--6421. PMLR, 2023.

\bibitem{Wen2017TernGradTG}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai~Helen Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed deep learning.
\newblock In {\em Neural Information Processing Systems}, 2017.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online, October 2020. Association for Computational Linguistics.

\bibitem{worledge2023unifying}
Theodora Worledge, Judy~Hanwen Shen, Nicole Meister, Caleb Winston, and Carlos Guestrin.
\newblock Unifying corroborative and contributive attributions in large language models.
\newblock {\em arXiv preprint arXiv:2311.12233}, 2023.

\bibitem{wu2022davinz}
Zhaoxuan Wu, Yao Shu, and Bryan Kian~Hsiang Low.
\newblock Davinz: Data valuation using deep neural networks at initialization.
\newblock In {\em International Conference on Machine Learning}, pages 24150--24176. PMLR, 2022.

\bibitem{wu2024pyvene}
Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah~D Goodman, Christopher~D Manning, and Christopher Potts.
\newblock pyvene: A library for understanding and improving pytorch models via interventions.
\newblock {\em arXiv preprint arXiv:2403.07809}, 2024.

\bibitem{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
\newblock Less: Selecting influential data for targeted instruction tuning.
\newblock {\em arXiv preprint arXiv:2402.04333}, 2024.

\bibitem{yeh2022first}
Chih-Kuan Yeh, Ankur Taly, Mukund Sundararajan, Frederick Liu, and Pradeep Ravikumar.
\newblock First is better than last for language data influence.
\newblock {\em Advances in Neural Information Processing Systems}, 35:32285--32298, 2022.

\bibitem{yoon2020data}
Jinsung Yoon, Sercan Arik, and Tomas Pfister.
\newblock Data valuation using reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 10842--10851. PMLR, 2020.

\bibitem{zhao2023addressing}
Boxin Zhao, Boxiang Lyu, Raul~Castro Fernandez, and Mladen Kolar.
\newblock Addressing budget allocation and revenue allocation in data market environments using an adaptive sampling algorithm.
\newblock In {\em International Conference on Machine Learning}, pages 42081--42097. PMLR, 2023.

\end{thebibliography}
