@inproceedings{park2023trak,
  title={{TRAK}: {A}ttributing Model Behavior at Scale},
  author={Park, Sung Min and Georgiev, Kristian and Ilyas, Andrew and Leclerc, Guillaume and Madry, Aleksander},
  booktitle={International Conference on Machine Learning},
  pages={27074--27113},
  year={2023},
  organization={PMLR}
}

@inproceedings{liu2021influence,
  title={Influence selection for active learning},
  author={Liu, Zhuoming and Ding, Hao and Zhong, Huaping and Li, Weijia and Dai, Jifeng and He, Conghui},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9274--9283},
  year={2021}
}

@article{han2020explaining,
  title={Explaining black box predictions and unveiling data artifacts through influence functions},
  author={Han, Xiaochuang and Wallace, Byron C and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2005.06676},
  year={2020}
}

@article{feldman2020neural,
  title={What neural networks memorize and why: Discovering the long tail via influence estimation},
  author={Feldman, Vitaly and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2881--2891},
  year={2020}
}

@inproceedings{kong2021resolving,
  title={Resolving training biases via influence-based data relabeling},
  author={Kong, Shuming and Shen, Yanyan and Huang, Linpeng},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{engstrom2024dsdm,
  title={DsDm: Model-Aware Dataset Selection with Datamodels},
  author={Engstrom, Logan and Feldmann, Axel and Madry, Aleksander},
  journal={arXiv preprint arXiv:2401.12926},
  year={2024}
}

@article{hampel1974influence,
  title={The influence curve and its role in robust estimation},
  author={Hampel, Frank R},
  journal={Journal of the american statistical association},
  volume={69},
  number={346},
  pages={383--393},
  year={1974},
  publisher={Taylor \& Francis}
}

@article{yeh2022first,
  title={First is better than last for language data influence},
  author={Yeh, Chih-Kuan and Taly, Ankur and Sundararajan, Mukund and Liu, Frederick and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32285--32298},
  year={2022}
}

@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{guo2020fastif,
  title={Fastif: Scalable influence functions for efficient model interpretation and debugging},
  author={Guo, Han and Rajani, Nazneen Fatema and Hase, Peter and Bansal, Mohit and Xiong, Caiming},
  journal={arXiv preprint arXiv:2012.15781},
  year={2020}
}

@inproceedings{schioppa2022scaling,
  title={Scaling up influence functions},
  author={Schioppa, Andrea and Zablotskaia, Polina and Vilar, David and Sokolov, Artem},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={8179--8186},
  year={2022}
}

@misc{ilyas2022datamodels,
      title={Datamodels: {P}redicting Predictions from Training Data}, 
      author={Andrew Ilyas and Sung Min Park and Logan Engstrom and Guillaume Leclerc and Aleksander Madry},
      year={2022},
      eprint={2202.00622},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

@misc{grosse2023studying,
      title={Studying Large Language Model Generalization with Influence Functions}, 
      author={Roger Grosse and Juhan Bae and Cem Anil and Nelson Elhage and Alex Tamkin and Amirhossein Tajdini and Benoit Steiner and Dustin Li and Esin Durmus and Ethan Perez and Evan Hubinger and Kamilė Lukošiūtė and Karina Nguyen and Nicholas Joseph and Sam McCandlish and Jared Kaplan and Samuel R. Bowman},
      year={2023},
      eprint={2308.03296},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{pruthi2020estimating,
  title={Estimating training data influence by tracing gradient descent},
  author={Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19920--19930},
  year={2020}
}

@article{worledge2023unifying,
  title={Unifying corroborative and contributive attributions in large language models},
  author={Worledge, Theodora and Shen, Judy Hanwen and Meister, Nicole and Winston, Caleb and Guestrin, Carlos},
  journal={arXiv preprint arXiv:2311.12233},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{yeh2018representer,
  title={Representer point selection for explaining deep neural networks},
  author={Yeh, Chih-Kuan and Kim, Joon and Yen, Ian En-Hsu and Ravikumar, Pradeep K},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{tsai2024sample,
  title={Sample based explanations via generalized representers},
  author={Tsai, Che-Ping and Yeh, Chih-Kuan and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@article{bae2022if,
  title={If influence functions are the answer, then what is the question?},
  author={Bae, Juhan and Ng, Nathan and Lo, Alston and Ghassemi, Marzyeh and Grosse, Roger B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17953--17967},
  year={2022}
}


@article{basu2020influence,
  title={Influence functions in deep learning are fragile},
  author={Basu, Samyadeep and Pope, Philip and Feizi, Soheil},
  journal={arXiv preprint arXiv:2006.14651},
  year={2020}
}

@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}

@article{kokhlikyan2020captum,
  title={Captum: A unified and generic model interpretability library for pytorch},
  author={Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and others},
  journal={arXiv preprint arXiv:2009.07896},
  year={2020}
}

@article{wang2024error,
  title={Error Discovery by Clustering Influence Embeddings},
  author={Wang, Fulton and Adebayo, Julius and Tan, Sarah and Garcia-Olano, Diego and Kokhlikyan, Narine},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@misc{udandarao2024zeroshot,
      title={No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance}, 
      author={Vishaal Udandarao and Ameya Prabhu and Adhiraj Ghosh and Yash Sharma and Philip H. S. Torr and Adel Bibi and Samuel Albanie and Matthias Bethge},
      year={2024},
      eprint={2404.04125},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Razeghi2022ImpactOP,
  title={Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning},
  author={Yasaman Razeghi and IV RobertL.Logan and Matt Gardner and Sameer Singh},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:256631096}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{eloundou2023gpts,
  title={Gpts are gpts: An early look at the labor market impact potential of large language models},
  author={Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  journal={arXiv preprint arXiv:2303.10130},
  year={2023}
}

@article{huang2023citation,
  title={Citation: A key to building responsible and accountable large language models},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2307.02185},
  year={2023}
}

@inproceedings{
min2024silo,
title={{SILO} Language Models: Isolating Legal Risk In a Nonparametric Datastore},
author={Sewon Min and Suchin Gururangan and Eric Wallace and Weijia Shi and Hannaneh Hajishirzi and Noah A. Smith and Luke Zettlemoyer},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ruk0nyQPec}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}


@article{zhao2023pytorch,
  title={Pytorch fsdp: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{gokaslan2019openwebtext,
  title={Openwebtext corpus},
  author={Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
  year={2019}
}

@article{hooker2019benchmark,
  title={A benchmark for interpretability methods in deep neural networks},
  author={Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{barshan2020relatif,
  title={Relatif: Identifying explanatory training samples via relative influence},
  author={Barshan, Elnaz and Brunet, Marc-Etienne and Dziugaite, Gintare Karolina},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1899--1909},
  year={2020},
  organization={PMLR}
}

@article{xia2024less,
  title={Less: Selecting influential data for targeted instruction tuning},
  author={Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
  journal={arXiv preprint arXiv:2402.04333},
  year={2024}
}

@article{johnson2019billion,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@article{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{ghorbani2019data,
  title={Data shapley: Equitable valuation of data for machine learning},
  author={Ghorbani, Amirata and Zou, James},
  booktitle={International conference on machine learning},
  pages={2242--2251},
  year={2019},
  organization={PMLR}
}

@inproceedings{jia2019towards,
  title={Towards efficient data valuation based on the shapley value},
  author={Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and G{\"u}rel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1167--1176},
  year={2019},
  organization={PMLR}
}

@article{gul1989bargaining,
  title={Bargaining foundations of Shapley value},
  author={Gul, Faruk},
  journal={Econometrica: Journal of the Econometric Society},
  pages={81--95},
  year={1989},
  publisher={JSTOR}
}

@book{roth1988shapley,
  title={The Shapley value: essays in honor of Lloyd S. Shapley},
  author={Roth, Alvin E},
  year={1988},
  publisher={Cambridge University Press}
}

@article{kwon2021beta,
  title={Beta shapley: a unified and noise-reduced data valuation framework for machine learning},
  author={Kwon, Yongchan and Zou, James},
  journal={arXiv preprint arXiv:2110.14049},
  year={2021}
}

@misc{jlversusalphabet,
    author={{J.L. et al. v. Alphabet Inc.}},
    year={2023},
    title={Case 3:23-cv-03416, {N.D. Cal.}},
    url={https://storage.courtlistener.com/recap/gov.uscourts.cand.415223/gov.uscourts.cand.415223.1.0.pdf}
}

@article{metz2022lawsuit,
  title={Lawsuit Takes Aim at the Way {A.I.} Is Built},
  author={Cade Metz},
  journal={New York Times},
  year={2022}
}

@inproceedings{wang2023data,
  title={Data banzhaf: A robust data valuation framework for machine learning},
  author={Wang, Jiachen T and Jia, Ruoxi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={6388--6421},
  year={2023},
  organization={PMLR}
}

@inproceedings{yoon2020data,
  title={Data valuation using reinforcement learning},
  author={Yoon, Jinsung and Arik, Sercan and Pfister, Tomas},
  booktitle={International Conference on Machine Learning},
  pages={10842--10851},
  year={2020},
  organization={PMLR}
}

@inproceedings{wu2022davinz,
  title={Davinz: Data valuation using deep neural networks at initialization},
  author={Wu, Zhaoxuan and Shu, Yao and Low, Bryan Kian Hsiang},
  booktitle={International Conference on Machine Learning},
  pages={24150--24176},
  year={2022},
  organization={PMLR}
}

@inproceedings{
nohyun2023data,
title={Data Valuation Without Training of a Model},
author={Ki Nohyun and Hoyong Choi and Hye Won Chung},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XIzO8zr-WbM}
}

@article{choe2024making,
  title={Making scalable meta learning practical},
  author={Choe, Sang and Mehta, Sanket Vaibhav and Ahn, Hwijeen and Neiswanger, Willie and Xie, Pengtao and Strubell, Emma and Xing, Eric},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{sim2022data,
  title={Data Valuation in Machine Learning:" Ingredients", Strategies, and Open Challenges.},
  author={Sim, Rachael Hwee Ling and Xu, Xinyi and Low, Bryan Kian Hsiang},
  booktitle={IJCAI},
  pages={5607--5614},
  year={2022}
}

@misc{bae2024training,
      title={Training Data Attribution via Approximate Unrolled Differentiation}, 
      author={Juhan Bae and Wu Lin and Jonathan Lorraine and Roger Grosse},
      year={2024},
      eprint={2405.12186},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
chen2023which,
title={Which Layer is Learning Faster? A Systematic Exploration of Layer-wise Convergence Rate for Deep Neural Networks},
author={Yixiong Chen and Alan Yuille and Zongwei Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=wlMDF1jQF86}
}


@inproceedings{
kwon2024datainf,
title={DataInf: Efficiently Estimating Data Influence in Lo{RA}-tuned {LLM}s and Diffusion Models},
author={Yongchan Kwon and Eric Wu and Kevin Wu and James Zou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=9m02ib92Wz}
}

@article{dubey1981value,
  title={Value theory without efficiency},
  author={Dubey, Pradeep and Neyman, Abraham and Weber, Robert James},
  journal={Mathematics of Operations Research},
  volume={6},
  number={1},
  pages={122--128},
  year={1981},
  publisher={INFORMS}
}

@article{wang2024rethinking,
  title={Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits},
  author={Wang, Jiachen T and Yang, Tianji and Zou, James and Kwon, Yongchan and Jia, Ruoxi},
  journal={arXiv preprint arXiv:2405.03875},
  year={2024}
}

@misc{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{soldaini2024dolma,
  title={Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  journal={arXiv preprint arXiv:2402.00159},
  year={2024}
}

@article{shi2019understanding,
  title={Understanding top-k sparsification in distributed deep learning},
  author={Shi, Shaohuai and Chu, Xiaowen and Cheung, Ka Chun and See, Simon},
  journal={arXiv preprint arXiv:1911.08772},
  year={2019}
}

@inproceedings{Wen2017TernGradTG,
  title={TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning},
  author={Wei Wen and Cong Xu and Feng Yan and Chunpeng Wu and Yandan Wang and Yiran Chen and Hai Helen Li},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:3747520}
}

@article{hanawa2020evaluation,
  title={Evaluation of similarity-based explanations},
  author={Hanawa, Kazuaki and Yokoi, Sho and Hara, Satoshi and Inui, Kentaro},
  journal={arXiv preprint arXiv:2006.04528},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{grosse2016kronecker,
  title={A kronecker-factored approximate fisher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016},
  organization={PMLR}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{fernandez2023data,
  title={Data-Sharing Markets: Model, Protocol, and Algorithms to Incentivize the Formation of Data-Sharing Consortia},
  author={Fernandez, Raul Castro},
  booktitle={Proceedings ACMSIGMOD International Conference on Management of Data},
  year={2023}
}

@inproceedings{zhao2023addressing,
  title={Addressing budget allocation and revenue allocation in data market environments using an adaptive sampling algorithm},
  author={Zhao, Boxin and Lyu, Boxiang and Fernandez, Raul Castro and Kolar, Mladen},
  booktitle={International Conference on Machine Learning},
  pages={42081--42097},
  year={2023},
  organization={PMLR}
}

@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

@article{wu2024pyvene,
  title={pyvene: A library for understanding and improving PyTorch models via interventions},
  author={Wu, Zhengxuan and Geiger, Atticus and Arora, Aryaman and Huang, Jing and Wang, Zheng and Goodman, Noah D and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:2403.07809},
  year={2024}
}