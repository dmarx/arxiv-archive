\section{Scalability Bottlenecks in Influence Functions}
\label{sec:background}
Most data valuation algorithms (\eg,\ data Shapley~\cite{ghorbani2019data}) evaluate the contribution or value of a specific example $x$ on the utility $v$ (\eg,\ test loss), that can further be used for crediting data providers, by measuring the overall change in the utility $v$ when in/excluding $x$ as follows:

\begin{align}
    \textsc{Value}(x;v) = \sum_{S\subseteq D\backslash \{x\}}w\big(v(S\cup\{x\}) - v(S)\big)\label{eq:shapley}
\end{align}
where $D$ is the training dataset,
$S$ is a subset of $D$, and $w$ is an (algorithm-specific) weighting term. Intuitively, the larger the utility gain from an inclusion of $x$ is, the larger the value of $x$ is.

One popular instantiation of Eq.~\eqref{eq:shapley} is the leave-one-out error~\cite{koh2017understanding}, a semivalue~\cite{dubey1981value} that is a basis for most data attribution methods and only considers $S$ with $|S|=|D|-1$ (\ie,\ leaving one example $x$ from the entire dataset $D$).
However, naively computing the leave-one-out-error requires retraining the model multiple times for each $x\in D$, which is hardly affordable even in small-scale setups.
To overcome this issue, influence functions, a representative gradient-based method, efficiently \textit{simulates} the effect of model retraining without an example $x_{tr}$ on the utility using gradient information as:
\begin{align}
    \textsc{Influence}(x_{tr}, x_{te}) = g_{te}^\top H^{-1}g_{tr}\label{eq:if}
\end{align}
where $g_{tr}$ and $g_{te}$ are train and test gradients respectively, and $H$ is the Hessian matrix. Concretely, influence functions approximate the effect of removing $x_{tr}$ by updating the model parameters with a Newton step in the direction of $H^{-1}g_{tr}$, and uses a first-order Taylor approximation to estimate how this update will affect the test utility. In practice, computing influence functions involves two key steps of (1) solving the inverse Hessian-vector product (iHVP) with $g_{te}$, and (2) taking the dot product of this iHVP with the gradient $g_{tr}$ for each training example.

Despite their comparative efficiency, influence functions remain difficult to scale to recent LLMs due to the high compute and memory costs associated with both steps.
First, space and time complexity of naive iHVP are respectively $O(n^2)$ and $O(n^3)$, both of which are impractical in recent LLMs with $n>10^9$ parameters. To address this issue, various tricks for efficiency, such as iterative methods \cite{koh2017understanding} or EKFAC approximation~\cite{grosse2023studying}, have been proposed. Second, to ensure fair valuation, one must compute influence scores with \textit{all} training data, which requires access to their gradients. However, computing gradients for all training data approximately amounts to one-epoch training, the cost of which often exceeds \$1M in the context of LLM (pre)training. If training gradients were to be recomputed frequently for regular data valuation, the total cost can quickly become astronomical. Thus, while it is technically possible to run a few influence function analyses to interpret interesting LLM outputs using efficient iHVP tricks~\cite{grosse2023studying}, doing it in a scalable and sustainable way to build a practical data valuation system remains a significant challenge.

In an attempt to mitigate the aforementioned cost issues, Arnoldi IF~\cite{schioppa2022scaling} and TRAK~\cite{park2023trak} recently explored the strategy of projecting gradients onto a low-dimensional space and computing influence scores on the subspace spanned by the projection matrix as follows:
\begin{align}
    \textsc{Influence}(x_{tr}, x_{te};P) =\big(Pg_{te}\big)^\top\big(PHP^\top\big)^{-1}\big(Pg_{tr}\big)\label{eq:if_proj}
\end{align}
where $P\in\mathbb{R}^{k\times n}$ is the projection matrix given the model and projection dimensions of $n$ and $k$. Under this strategy, the iHVP operation also occurs in a low-dimensional space, meaning that $n$ in memory and compute complexity of iHVP gets replaced with $k\ll n$. Furthermore, low-rank projection enables writing projected gradients for all training data to disk once and simply reading them as new test data arrives without costly re-computations. This converts an influence function problem  into a vector similarity search problem, for which various system optimizations exist~\cite{johnson2019billion}.

In essence, this strategy significantly reduces both iHVP and training gradient recomputation costs by introducing an additional process of low-rank gradient projection $Pg$. However, the additional compute/memory costs and accuracy degradation incurred from low-rank gradient projection has not been thoroughly studied to date. First, assuming that the batch size is $b$, the compute cost of naive batched gradient projection is $O(bkn)$. Noting that the compute cost of backpropgation is $O(bn)$ (or $O(btn)$ if we consider the time dimension), the cost of gradient projection is usually larger than that of backpropagation given a reasonably large $k$ for the expressivity. Second, the memory costs for full per-sample gradient and the projection matrix are $O(bn)$ and $O(kn)$. If an 8B model were to be used, each of these costs amounts to 32GB$\times b$ (or $\times k$) GPU memory. While Arnoldi IF and TRAK attempt to address the memory costs of the per-sample gradient and projection matrix respectively with forward-mode Jacobian-vector products and a custom CUDA kernel trick, neither of them are able to solve both issues altogether. This leads Arnoldi IF and TRAK to use very small $k$ and $b$, each of which results in decreased accuracy of influence scores due to limited expressivity and poor efficiency from low GPU utilization. Since accuracy and efficiency are both critical for effective data valuation, we deduce that further advancements in the gradient projection approach are necessary.