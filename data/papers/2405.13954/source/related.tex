\section{Related Work}
\label{sec:related}
\textbf{Data Valuation.\hspace{2.5mm}} Measuring the value (or contribution) of training data on the model outputs has gained lots of attention recently. Exemplified by Data Shapley~\cite{ghorbani2019data}, a flurry of prior work~\cite{jia2019towards,kwon2021beta,wang2023data} proposed exploiting the Shapley value or concepts from cooperative game theory to address the data valuation problem. However, most existing approaches in this line require repeated retraining of the model, a cost of which is hardly affordable even with small models. In addition to game-theoretic approaches, data valuation has also been tackled using reinforcement learning~\cite{yoon2020data}, meta learning~\cite{choe2024making}, and training-free methods~\cite{nohyun2023data,wu2022davinz}. Nevertheless, these works either suffer from high complexity from the need to train other models~\cite{choe2024making,yoon2020data} or high computational costs~\cite{nohyun2023data}. We direct readers to Sim et al.~\cite{sim2022data} for a more extensive survey on diverse data valuation approaches.

% \textbf{Influence Functions.\hspace{2.5mm}} Influence functions, a classic concept from robust statistics~\citep{hampel1974influence}, estimate the infinitesimal effect of removing or adding a training data point without the need to retrain the model. Apart from data valuation, influence functions have various applications in machine learning, such as interpreting a model's behavior, detecting mislabeled data examples, and curating training datasets~\cite{liu2021influence}. Besides gradient projection, another common trick for efficiently computing influence functions include computing influence functions only on the last (few) layer~\cite{koh2017understanding,schioppa2022scaling}. However, past work have shown that influence functions computed on a single layer are not expressive enough to capture influential training sequences. Various filtering strategies, such as using similarity in the model's representation space and using token overlap such as TF-IDF~\cite{yeh2022first}, has also proposed to reduce the cost of the dot product. While we can also use this, this may miss interesting sequences such as don't share much tokens but are semenatically related. Recently, similar to our \method, DataInf~\cite{kwon2024datainf} and LESS~\cite{xia2024less} proposed using LoRA to efficiently compute influence functions. However, they are only applicable in finetuning settings, whereas \method\ also supports influence analyses for pertaining.

\textbf{Influence Functions.\hspace{2.5mm}} Influence functions, a classic concept from robust statistics \cite{hampel1974influence}, estimate the infinitesimal effect of removing or adding a training data point without model retraining. They have various applications in machine learning, such as interpreting the model's behavior~\cite{han2020explaining,park2023trak,grosse2023studying} and curating training datasets~\cite{liu2021influence,engstrom2024dsdm}. However, when applied to large neural networks, the computation of the iHVP and its dot product with all training examples introduce scalability challenges. Besides gradient projection, past works have explored computing influence functions only on the last (few) layers~\cite{koh2017understanding,schioppa2022scaling} to mitigate these challenges. However, subsequent works~\cite{feldman2020neural,grosse2023studying} have shown that the influence on only a subset of layers is insufficient to capture the overall influence of a training data point. To avoid computing the gradient of all training examples, various filtering strategies, such as using the similarity in the model's representation space~\cite{guo2020fastif} or TF-IDF~\cite{yeh2022first,grosse2023studying}, have also been proposed. While it is possible to adopt these filtering strategies for \method, they may introduce bias in the selection of the most influential sequences. For example, filtering candidate training sequences with TF-IDF might miss interesting influential sequences that do not share many tokens but are semantically related. Recently, similarly to \method, DataInf~\cite{kwon2024datainf} and LESS~\cite{xia2024less} proposed using LoRA to efficiently compute influence functions. However, these approaches are only applicable in finetuning settings, whereas \method\ also supports influence analyses for pretraining. 

% Influence functions, classic concept from robust statistics, estimate the infitenstimal effect of removing or adding a data point without the need to retrain the model. Apart data valuation, influence functions have various applications in machine learning, such as interpreting model's behavior, detecting mislabeled data examples, and curating training dataset. Apart from gradient projection, another common trick for efficiently computing influence functions include computing influence functions only on the last layer~\cite {koh2017understanding}. However, past works have shown that influence functions computed on a single layer are not expressive enough to capture influential training sequences. Various filtering strategies, such as using similarity in the model's representation space and using token overlap such as TF-IDF, has also proposed to reduce the cost of the dot product. While we can also use this, this may miss interesting seqeucens such as don't share much tokens but are semenatically related. Recently, similar to our Logra, DataInf proposed using LoRA to efficiently compute influence functions. However, DataInf is only applicable in finetuning settings, whereas Logra also supports influence computation for pretraining.

% Another popular approach to the data valuation problem is influence functions~\cite{grosse2023studying,jia2019towards,koh2017understanding}, or more generally gradient-based methods~\cite{pruthi2020estimating}. \sang{maybe discuss LESS and DataInf, but point out that they are only applicable to finetuning?}

% \textbf{Training Data Attribution.\hspace{2.5mm}} Training data attribution (TDA) techniques aim to identify the training examples that significantly influence a model's behavior. Modern TDA methods for neural networks can be categorized into three main approaches: sampling-based, representation-based, and gradient-based. For a comprehensive overview of these techniques, we refer readers to the survey works of X and Y.
