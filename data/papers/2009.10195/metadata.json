{
  "arxivId": "2009.10195",
  "title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving\n  Out-of-Domain Robustness",
  "authors": "Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi",
  "abstract": "Models that perform well on a training domain often fail to generalize to\nout-of-domain (OOD) examples. Data augmentation is a common method used to\nprevent overfitting and improve OOD generalization. However, in natural\nlanguage, it is difficult to generate new examples that stay on the underlying\ndata manifold. We introduce SSMBA, a data augmentation method for generating\nsynthetic training examples by using a pair of corruption and reconstruction\nfunctions to move randomly on a data manifold. We investigate the use of SSMBA\nin the natural language domain, leveraging the manifold assumption to\nreconstruct corrupted text with masked language models. In experiments on\nrobustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently\noutperforms existing data augmentation methods and baseline models on both\nin-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews,\n1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
  "url": "http://arxiv.org/abs/2009.10195v2",
  "issue_number": 97,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/97",
  "created_at": "2024-12-22T21:38:47.338171",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_minutes": 0,
  "last_read": null
}