\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm,mathrsfs,xcolor,bm,bbm, mathtools, mathdots}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{setspace}    
\usepackage{natbib}
\usepackage{fancyhdr}
%\usepackage{showlabels}
%\bibliographystyle{plainnat}
\bibliographystyle{abbrvnat}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue,bookmarks=true]{hyperref}
%\setcitestyle{author,open={((},close={))}} %Citation-related commands
\onehalfspacing
%\doublespacing
\usepackage{sectsty}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage[title]{appendix}

%\renewcommand\qedsymbol{$\blacksquare$}
\newcommand{\highlight}[1]{{\color{red}#1}}

\def\NN{\mathbb N}
\def\ZZ{\mathbb Z}
\def\RR{\mathbb R}
\def\CC{\mathbb C}

\setcounter{equation}{0}
\numberwithin{equation}{section}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

%\newtheorem{assumption}{Assumption}
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{proposition}{Proposition}
%newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
%\newtheorem{remark}{Remark}
%\newtheorem{example}{Example}

\title{Approximation of RKHS Functionals by Neural Networks}
\author{Tian-Yi Zhou$^1$ \and Namjoon Suh$^2$ \and Guang Cheng$^2$ \and Xiaoming Huo$^1$}
\date{%
    $^1$H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology\\%
    $^2$Department of Statistics \& Data Science, UCLA\\[2ex]%
    %\today
}

\begin{document}

\maketitle 
\begin{abstract}
 \noindent  Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to $\RR$ (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. 
Moreover, we apply our findings to functional regression, proving that neural networks
can accurately approximate the regression maps in generalized functional linear models.
Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. 
By leveraging the interpolating orthogonal
projections in RKHS's, our proposed network is much simpler in that we use point evaluations to replace basis function expansions.  

\end{abstract} 
{\it{\bf Keywords:} neural networks, reproducing kernel Hilbert space, approximation theory, functional learning, functional data analysis}
 \section{Introduction}
This paper studies the approximation of smooth functionals defined over a reproducing kernel
Hilbert space (RKHS) using {\it tanh neural networks}.  A functional maps from a space of functions that has infinite dimensions to $\RR$. 
In recent years, neural networks have been widely employed in operator learning tasks. We are interested in investigating their capability to approximate nonlinear functionals, a special type of operator. 
%We will refer to these functionals as {\it RKHS functionals}.

 Neural networks have been known as universal approximators since \citep{cybenko1989approximation}, i.e., to approximate any continuous function, mapping a finite-dimensional input space into
another finite-dimensional output space, to arbitrary accuracy. 
These days, many interesting tasks entail learning operators, i.e., mappings between an infinite-dimensional input Banach space and (possibly) an infinite-dimensional output space.
A prototypical example in scientific computing is to map the initial datum into the (time series of) solution of a nonlinear time-dependent partial differential equation (PDE).
A priori, it is unclear if neural networks can be successfully employed to learn such operators from data, given that their universality only pertains to finite-dimensional functions.

One of the first successful uses of neural networks in the context of operator learning was provided by \citep{chen1995universal}. A shallow neural network-based architecture, termed as \textit{neural operator}, is proposed and is proved to possess an universal approximation property for nonlinear operators from an infinite-dimensional space to a finite-dimensional Euclidean space.
Recently, there has been an active line of research developing new methods for learning operators by deep neural networks, such as the \textit{DeepONet} \citep{lu2021learning}, \textit{physics-informed neural networks} (PINNs) \citep{raissi2019physics, wang2021learning}, \textit{Fourier Neural Operator} (FNO) \citep{li2020fourier}, and \textit{DeepGreen} \citep{gin2021deepgreen}.
Although these methods have shown success through numerical experiments in approximating operators in many interesting examples, theoretical guarantees in this context are relatively scant. 
Some recent quantitative approximation results are obtained in \citep{kovachki2021universal, lanthaler2022error, liu2022deep}. 

This work focuses on the approximation of functionals by neural networks.
Approximating functionals by neural networks is a crucial step in approximating operators by neural networks in the \textit{DeepONet} and similar methods \citep{lanthaler2022error}, as used in \citep{chen1995universal}. 
In particular, we consider the approximation of functionals on reproducing kernel Hilbert
spaces (RKHS's), where a major advantage of RKHS is that it enables us to replace integration-type basis function expansions by simple point evaluations on a discrete subset of the domain. 
%As mentioned earlier, neural networks are widely and successfully employed in operator learning these days, but the underlying theory remains largely unknown. 

%In particular, we will show that a tanh neural network with two hidden layers suffices to approximate nonlinear functionals at comparable or better rates than existing works.
We adopt neural networks equipped with tanh activation functions.
Most of the recent results on neural network approximation focus on  ReLU networks. 
%In other words, these results quantitatively relate the size (and architecture) of ReLU networks to their approximation accuracy.
%A seminal work in this direction is \citep{yarotsky2017error}, where the author derived explicit estimates on the width and depth of a ReLU network for approximating Lipschitz functions to any given accuracy in the $L^\infty$-norm. 
Although ReLU activation functions are very common in practical applications of deep learning, there are many areas where other activation functions are employed.  Among the most popular activations are the tanh activation function and the related sigmoid or logistic function (a scaled and shifted tanh). 
In the area of operator learning, smooth activation functions such as tanh are preferred over ReLU, especially in  
PINNs, for solving forward and inverse problems for PDEs (see \citep{raissi2019physics} and references therein). 
Also, tanh activation functions are the basis of heavily used recurrent neural networks (RNN), such as Long Short-term Memory (LSTM) \citep{hochreiter1997long} and Gated Recurrent Unit (GRU) \citep{cho2014learning}. 

To establish a theoretical guarantee that neural networks can learn functionals well, we need to show the convergence of {\it generalization error}, which measures the distance between the ground-truth functional and the estimated functional given by a certain class of neural networks.  
In this paper, we show that a tanh neural network with two hidden layers suffices to approximate a functional on RKHS to any desired accuracy. 
%at comparable or better rates than existing works.
Deriving approximation error is the first step of generalization analysis. 
We also apply our findings to functional data analysis (FDA) and prove that neural networks can approximate regression maps in generalized functional linear models well without any assumptions on the model parameters. Our approximation result will help theoretically justify the use of deep neural networks in functional data analysis. In fact, by following \citep{shang2015nonparametric}, we are now able to derive generation error and even statistical inferences for generalized functional linear regression models, which will be a future topic. 
%Mathematically, the generalization error consists of the approximation error and the so-called {\it sample error}. 
%This paper focuses on bounding the uniform approximation error. 
%The sample error measures the complexity of a class of neural networks and will be left for future study. 
More discussions regarding generalization studies of neural networks can be found in the recent survey paper \citep{suh2024survey}.

 
%A wide range of tasks in statistics and engineering require learning nonlinear functionals, especially RKHS functionals. 
%For example, in functional regression problems, we aim to learn a regression map from a function covariate to a univariate response.
%Also, solving PDEs aims to learn a
%map from the parametric function space to the PDE solution space.
% We delay the discussions on these applications to Section \ref{sec:motivation}. 

\subsection{Related Works}
%Researchers are showing a growing interest in adopting neural networks to learn functionals, driven by the abundance of data of a functional nature and the broad range of applications functionals offer.
%Although some attempts have been made in applications to learn functionals using neural networks, the underlying theory is largely unknown.

In an earlier work \citep{mhaskar1997neural}, a shallow neural network with infinitely differentiable activation function is used to approximate nonlinear, continuous functionals on $L^p$ spaces, for $1 \leq p \leq \infty$. 
Though the approximation rate given there does not seem satisfactory --- when measuring the approximation error on the unit ball of a H\"{o}lder space, the number of network parameters grows exponentially
with a large exponent as the accuracy increases --- it is proven that such a rate is, in fact, optimal.
Later in \citep{stinchcombe1999neural}, a generalized neural network is introduced to approximate functionals whose domain is a locally convex topological vector space.   
Based on the aforementioned work, a novel functional multi-layer perception (MLP) is introduced by \citep{rossi2005functional}, which can be directly applied to functional data. Recently in \citep{thind2023deep}, an optimization algorithm is proposed to implement the functional MLP in practice.
The functional MLP is based on, but different from, a classical fully-connected neural network. 

To illustrate this special network's design, we first formulate a standard fully-connected network. 
For $\mathbf b= (b_1,\ldots, b_r)\in \RR^r$, let $\sigma_{\mathbf b}: \RR^r \rightarrow \RR^r$ be the shifted activation function as 
\begin{equation*}
    \sigma_{\mathbf b}\begin{pmatrix}
    x_1\\
    \vdots\\
    x_r
    \end{pmatrix}:= \begin{pmatrix}
    \sigma (x_1-b_1)\\
    \vdots\\
    \sigma (x_r-b_r)
    \end{pmatrix},
\end{equation*}
where $\sigma(x)$ is an activation function such as tanh: $\sigma(x) = \frac{e^x- e^{-x}}{e^x+e^{-x}}$ and ReLU: $\sigma(x) = \max \{x,0\}$. 
We use the vector $\mathbf d= (d_1,\ldots, d_L)\in \NN^L$ to indicate the width in each hidden layer.  
\begin{definition}[Standard Fully-connected Network]\label{def:classic}
A fully-connected neural network $\widehat{G} : \RR^{N} \rightarrow \RR$ with $L$ hidden layers is a function that takes the form
\begin{equation*}
    \widehat{G}(x) = a^T\sigma_{\mathbf b_L}(W_L \sigma_{\mathbf b_{L-1}}(W_{L-1}\cdots  \sigma_{\mathbf b_1}(W_1 x))),
\end{equation*}
where $x \in \RR^{N}$ is the input, $a\in \RR^{d_L}$ is the outer weight, $d_0 = N$, $W_\ell \in \RR^{d_\ell \times d_{\ell-1}}$ denote the weight matrices, $b_\ell \in \RR^{d_\ell}$ denote the bias vectors over the layers $\ell =1,\ldots, L,$ in the network.
\end{definition}

The functional MLP introduced in \citep{rossi2005functional} is designed to approximate functionals defined on $L^p[-1,1]^s$ for $p\geq 1$ and $s \in \NN \geq 1$, where the network input is a function covariate $f(x): L^p[-1,1]^s \rightarrow \RR$, $x\in [-1,1]^s$. We give the formulation of functional MLP below. 
\begin{definition}[Functional MLP]
A functional MLP $\Theta:L^p[-1,1]^s \rightarrow \RR$ with $L$ hidden layers is any functional that takes the form
\begin{equation*}
    \Theta(f) = a^T\sigma_{\mathbf b_L}W_L \sigma_{\mathbf b_{L-1}}W_{L-1}\cdots  \sigma_{\mathbf b_2}W_2 T(f),
\end{equation*}
where $f\in L^p[-1,1]^s$, $T:L^p[-1,1]^s \rightarrow \RR^{d_1}$ is a bounded linear operator with 
\begin{equation}\label{firstlayer}
    T(f) = \left(\int_{[-1,1]^s}f(x)g_1(x)dx, \cdots, \int_{[-1,1]^s}f(x)g_{d_1}(x)dx\right)^T,
\end{equation}
where $\{g_k\}_{k=1}^{d_1}\in L^q[-1,1]^s$ is a set of pre-selected basis functions and  $\frac{1}{p}+ \frac{1}{q} = 1$.
\end{definition}
As we can see in (\ref{firstlayer}), 
the initial layer of this network converts the function $f$ into a vector form that can be fed directly into a standard neural network via basis function expansion. 
%neurons on the first layer of the functional MLP compute the integrations of products of the input $f$ and the basis functions  $\{g_k\}_{k=1}^{d_1}$.
This is referred to as the \textit{discretization step}. 
%One drawback of such architecture is that a separate learning algorithm is needed to identify the weight functions $\{g_k\}_{k=1}^{d_1}$. 
One drawback of such a design is that a prior selection of basis functions is required, often without information about the task at hand. 
A universal approximation theorem of the functional MLP is established in \citep{rossi2005functional},
%where it is proved that such a network can approximate functionals in $L^p$ to any arbitrary accuracy. 
where a scheme to learn $\{g_k\}_{k=1}^{d_1}$ is introduced by assuming them in a parametric function space.

More recently, two variants of functional MLP have been introduced in \citep{yao2021deep} and \citep{song2023approximationarxiv}.
In \citep{yao2021deep}, each basis function is parameterized with a micro neural network embedded in the MLP; 
while in \citep{song2023approximationarxiv}, the basis functions are specifically chosen to be some families of orthogonal polynomials.  
%Their network is equipped with the ReLU activation functions. 
For approximating functionals on $L^p$ spaces ($p\geq 1$), the network proposed in \citep{song2023approximationarxiv} achieves a similar approximation rate as in \citep{mhaskar1997neural}. 
At this point, a question arises: can we adopt a simpler network to approximate nonlinear functionals without integration in the first layer?

All the above-mentioned functional learning methods are based on a set of basis functions defined on a domain in an Euclidean space. 
In this paper, we study functionals on RKHS's, and the discretization of functions can be simply done by point evaluations.
In other words, we adopt the standard fully-connected neural networks with the input vector given by the discretely observed functional values. 
Our design eliminates the need for a preprocessing step in the form of integration that typically requires a manually prespecified choice of basis function. 
%We do not require an initial network layer to perform an integration-type discretization. 
Our network speaks directly to the state-of-the-art operator learning methods, as \textit{DeepONet}, PINNs, \textit{DeepGreen} all do not involve the basis expansion of network input. 

\subsection{Problem Formulation}
We first define RKHS. Let $K$ be a  Mercer kernel on a compact metric space $(\mathcal{X}, d_\mathcal{X})$, which induces an unique RKHS \citep{aronszajn1950theory} defined as:
\begin{definition}[RKHS]
The RKHS $\mathcal{H}_K$ is the closure of the set $\left\{\sum_{j=1}^p \alpha_j K(\cdot, x_j): p\in \NN, \alpha_j \in \RR, x_j \in \mathcal{X}\right\}$ under the metric induced by the inner product $ \langle \cdot, \cdot \rangle_{\mathcal{H}_K}$ 
 given by $$\left\langle \sum_{i=1}^q \beta_i K(\cdot, y_i), \sum_{j=1}^p \alpha_j K(\cdot, x_j) \right\rangle_{\mathcal{H}_K}  = \sum_{i=1}^q \sum_{j=1}^p \beta_i \alpha_j K(y_i, x_j).$$
\end{definition}
The RKHS  $\mathcal{H}_K$ is characterized by the property that the point evaluation functional $L_x$ on $\mathcal{H}_K$ given by $f \mapsto f(x)$ is continuous for every $ x \in \mathcal{X}$.

Denote by $\|\cdot\|_{\mathcal{H}_K}$ the RKHS norm induced by the inner product $ \langle \cdot, \cdot \rangle_{\mathcal{H}_K}$. 
We define the unit ball of $\mathcal{H}_K$ given by 
$
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}.
$
%We are interested in the problem of approximating nonlinear functionals defined over  $\mathcal{K}$ (that is, $F:\mathcal{K} \rightarrow \RR$) by tanh neural networks. 
We are interested in  approximating the functional $F:\mathcal{K} \rightarrow \RR$, where $\mathcal{K}$ is regarded as a compact subset of $C(\mathcal{X})$, the space of all continuous functions on $\mathcal{X}$ with norm $\|f\|_\infty = \sup_{x\in \mathcal{X}}|f(x)|$. Throughout this paper, we assume that $F$ is $s$-H\"{o}lder continuous for $0<s \leq 1$ with constant $C_F \geq 0$:
\begin{equation}
    |F(f) - F(\Tilde{f})| \leq C_F \|f-\Tilde{f}\|^s_\infty, \qquad \forall f, \Tilde{f} \in \mathcal{K}.
\end{equation}

In this paper, we adopt the classic fully-connected neural network defined in Definition \ref{def:classic} equipped with the tanh activation function $\sigma(x) = \frac{e^x- e^{-x}}{e^x+e^{-x}}$.
We aim to construct tanh neural networks $\widehat{G}: \RR^N \to \RR$ and obtain an upper bound on the following uniform approximation error
\begin{equation}\label{eq:uniapprox}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|,
\end{equation}
where the network inputs $f(\Bar{t}) = (f(t_1), \ldots, f(t_{N}))\in \RR^{N}$ are function values at sufficient but finitely many discrete locations $\Bar{t} = \{t_1, \ldots, t_N\}\subset \mathcal{X}$. 
Uniform spacing of locations is not necessary. 
Our method does not require basis function expansion. 



\subsection{Main Contributions}
%Our contributions are summarized below. 
We consider fully-connected tanh neural networks with two hidden layers. 
The novelty of our proposed networks is that we evaluate the input $f \in \mathcal{H}_K$ by simply taking its function values at some discrete points in the domain $\mathcal{X}$, that is, $f(\Bar{t}) = (f(t_1), \ldots, f(t_{N}))\in \RR^{N}$ with $\Bar{t}=\{t_1, \ldots, t_N\} \subset \mathcal{X}$.
We do not require uniform partition of these discrete points.
Our design is motivated by the practice of representing continuous signals or images by discrete objects in various fields of science and engineering. 
To represent continuous inputs, such as continuous images and signals defined on a compact domain, it is common to take measurements of values over a discrete subset of the domain as discrete images or signals. 
%Our design of neural networks matches the representations of continuous signals in signal-processing tasks. 
Conversely, the integration-type function expansion with a set of pre-specified basis functions employed in previous works is not a typical approach in practice.
We adopt a much simpler network by leveraging the interpolating orthogonal projections in RKHS's. 

Our results are summarized below. 
We establish novel error bounds for the approximation of functionals on the RKHS induced by a general Mercer kernel using tanh neural networks (Theorem \ref{thm:general}).
Prior works on functional approximation study only those defined on $L^p$ spaces, making our results unprecedented in the literature. 
The approximation error bound exhibits a trade-off associated with $N\in \NN$ (i.e., the number of discrete points used to evaluate our input $f$): when $N$ is small, the power function (which measures the distance between a function and its interpolant) increases; while when $N$ is too large, the network size becomes too big and can results in overfitting. 
By determining the optimal choices of $N$, 
we derive explicit rates of approximating functionals on Sobolev space (Theorem \ref{thm:Sobolev}), and on RKHS's induced by inverse multiquadric kernel (Theorem \ref{thm:multi}) and Gaussian kernel (Theorem \ref{thm:Gaussian}), respectively.
These kernels are commonly considered in kernel methods, and their translation-invariance properties facilitate our theoretical analysis. 
Our results show that neural networks can achieve any desired approximation accuracy with sufficiently many network parameters.
Particularly, for the inverse multiquadric and Gaussian kernels, we achieve approximation rates faster than logarithmic rates, w.r.t. the number of network parameters.  
Our results have shown improvement compared to the previous results on $L^p$ functionals in \citep{mhaskar1997neural} and \citep{song2023approximationarxiv}.
%Our analysis uses some classic results on power functions in the kernel literature.
%While most related works require an initial neural network layer to perform a discretization step via integrations, we do not need such integrations in our proposed network. 

%We also make use of the translation invariance properties, such as bounds of the norm of the inverse of the Gram matrices, of the specific kernels of interest. 
%Our analysis uses some classic results on power functions in the kernel literature. 

This is a theoretical work on the approximation theory of RKHS functionals. We leave generalization analysis and numerical studies for future studies.


 \subsection{Organization of this work}
 The rest of the paper is organized as follows: 
In Section \ref{sec:motivation}, we present applications that require learning nonlinear functionals. We then give our main results in Section \ref{sec:mainresultI} and \ref{sec:mainresultII}. We broadly divide our main results into two parts;  in Section \ref{sec:mainresultI}, we give explicit rates of approximating RKHS functionals induced by specific kernels by tanh neural networks, which include Sobolev kernel (Theorem \ref{thm:Sobolev}), inverse multiquadric kernel (Theorem \ref{thm:multi}), and Gaussian kernel (Theorem \ref{thm:Gaussian}); in Section \ref{sec:mainresultII}, we give approximation results for general RKHS functionals using tanh networks (Theorem \ref{thm:general}). 
In Section \ref{sec: regression}, we apply our theoretical findings to approximate regression maps in generalized functional regression models (Corollary \ref{corollary:FLM}). 
In Section \ref{sec:method}, we discuss the general approaches and methods for establishing our main results.
 Concluding remarks are given in Section \ref{sec:conclusion}. 
 Finally, Appendix \ref{sec:append_remark} contains the proof of some remarks, Appendix \ref{sec:appendA} contains the proof of some supporting lemmas, and Appendix \ref{sec:appendB} contains the proof of our main results.  
 
\section{Motivations}
\label{sec:motivation}
Many tasks in statistics and machine learning require learning nonlinear functionals, including functional regression, ODE/PDE learning,  and distribution regression. 
Our work is motivated by the broad applications functionals offer, as well as the abundance of functional data. 
In the following, we present some related applications.   

\vspace{2pt}
\noindent \underline{\bf Example 1: Functional Regression}
\vspace{2pt}

With the progress of technology, we observe more and more data of a functional nature, such as curves, trajectories, time series, and images. 
The term \textit{functional data} traditionally refers to  repeated measurements of an individual over an interval, although it
is broader in meaning; for example, it can refer to functions on higher dimensional
domains (e.g., images over domains in $\RR^2$ or $\RR^3$) \citep{greven2017general}. 

As functional data become more common, researchers are increasingly interested
in regression
models for functional data (i.e., relating functional variables to other variables of interest). A classical functional regression model is the \textit{generalized functional linear model} (FLM) introduced by \citep{muller2005generalized}.  
In this model, we aim to learn a regression map from a function covariate to a scaler response.
% This map is a (usually nonlinear) functional where the nonlinearity comes from the link function. 
 
 Suppose $X$ is a square-integrable random function defined over a compact interval $\mathcal{X}$ and $Y$ is a scaler response. 
With the regressor $F:L^2(\mathcal{X})\rightarrow \RR$, the generalized FLM can be formulated as
\begin{equation}
     Y =  g\left(\int_\mathcal{X} \beta(t) X(t) dt\right) + \varepsilon, \qquad t\in \mathcal{X},
\end{equation}
where $\beta \in L^2(\mathcal{X})$ is the unknown functional parameter,
%with $\int_{\mathcal{T}} \beta_i^2(x) dx = 1$  
$g$ is a link function that may possess high-order smoothness (e.g., a logit link), and $\varepsilon$ is a centered noise random variable. 
A fundamental problem in statistics is to determine the regression map $F$ and to subsequently retrieve the regression mean $\mathbb{E}[Y|X]:= g\left(\int_\mathcal{X} \beta(t) X(t) \right)$. 
%To illustrate how GFLM is applied in practice, we consider an example of predicting the life expectancy of fruit flies based on their egg-laying trajectories. We can adopt a GFLM where the egg-laying trajectory over a 30-day period --- $X(t), t\in \mathcal{T}=[0,30]$ --- is the function covariate, and the response $Y = \text{sgn} (F(X) - 1/2) \in \{-1,1\}$ is a binary variable indicating whether the fruit fly is short- or long-lived \citep{carey1998relationship, muller2005generalized}.  

To solve the functional regression problem, it is crucial to learn the regression map $F:L^2(\mathcal{T})\rightarrow \RR$. While the powerful capabilities of neural networks have been clearly demonstrated for vector/text/graph data, how to adapt these models to functional data remains under-explored. 
In this paper, we will show that for any given function covariate $X$ residing in a RKHS inside the $L^2$ space, we can approximate the regression map of generalized FLM using fully-connected neural networks. 
We assume $g$ is Lipschitz continuous. This is a novel result that has not been proven before in statistics literature. Detailed discussions can be found in Section \ref{sec: regression}. 

\vspace{3pt}

\noindent \underline{\bf Example 2: ODE/PDE Learning}

\vspace{3pt}

Systems of coupled ODEs and PDEs are undoubtedly the most widely used models in science and engineering, as they often capture the dynamical behavior of physical systems.
Even simple ODE functions can describe complex dynamical behaviors. Recently, an active line of research \citep{raissi2019physics, wang2021learning, li2020fourier} has been developing deep learning tools to infer solutions of ODEs/PDEs with lower cost compared to traditional tools (e.g., finite element methods). 
%https://www.science.org/doi/epdf/10.1126/sciadv.abi8605
%https://proceedings.mlr.press/v80/heinonen18a/heinonen18a.pdf


In solving ODEs and PDEs, we aim to learn a map from the parametric function space to the ODE/PDE solution space. Consider a very simple initial value problem of an ODE: 
%as in \citep{song2023approximationarxiv}: 
\begin{equation}\label{ODE}
    \begin{cases}
        \frac{dh(x)}{dx} = g(x,f(x),h(x)), \qquad x \in [a,b],\\
        h(a) = h_0.
    \end{cases}
\end{equation}
We aim to learn $h(b)$ for any function $f$, that is,  a map $F$ from an input $f$ to an output $h(b)\in \RR$. If we denote the solution to (\ref{ODE}) as $G(f)$, then 
\begin{equation}
   F(f) = G(f)(b),
\end{equation}
where $G$ is a nonlinear operator  being the unique solution to the integral equation
\begin{equation}
    G(f)(x) = h_0 + \int_{a}^x g(t, f(t), G(f)(t))dt.  
\end{equation}

Our work contributes to the area of ODE/PDE learning by proving that a two-layer standard neural network is sufficient to approximate (to any accuracy) a functional that is a solution map to a problem like (\ref{ODE}). 

\vspace{3pt}

\noindent \underline{\bf Example 3: Distribution Regression}

\vspace{3pt}

We find unique applications that require functionals on RKHS's.
One of the motivations for learning RKHS functionals is {\it distribution regression}: regressing from probability measures to real-valued responses or, more generally, to vector-valued outputs.  
%(belonging to an arbitrary separable Hilbert space). 

Let $(\mathcal{X}, \tau)$  be a topological space and let $\mathcal{B}(\mathcal{X})$ be the Borel $\sigma$-algebra induced by
the topology $\tau$. Let $\mathcal{M}^+(\mathcal{X})$ denote the set of Borel probability measures on $(\mathcal{X}, \tau)$, and $\mathcal{Y}$ be a separable Hilbert space. 
Consider $z=\{(X_i, y_i)\}_{i=1}^\ell$, where $X_i\in \mathcal{M}^+(\mathcal{X})$, $y_i\in \mathcal{Y}$ is its observable label, each $(X_i, y_i)$ pair is drawn i.i.d. from a joint meta distribution $\mathcal{M}$.
We do not observe $X$ directly; rather, we observe a sample $x_{i,1}, \ldots, x_{i,N_i} \overset{i.i.d.}{\sim} X_i$.
%Suppose that our data consists of $z=\{(X_i, y_i)\}_{i=1}^\ell$, where $X_i\in \mathcal{M}^+(\mathcal{X})$ ($X_i$ is a probability distribution), $y_i\in \mathcal{Y}$ is its label which is observable (in the simplest case $y_i \in \RR$ or $y_i \in \RR^d$), and each $(X_i, y_i)$ pair is drawn i.i.d. from a joint meta distribution $\mathcal{M}$.   
%We do not observe $X_i$ directly; rather, we observe a sample $x_{i,1}, \ldots, x_{i,N_i} \overset{i.i.d.}{\sim} X_i$.  
Thus the observed data are $\tilde z=\{(\{x_{i,n}\}_{n=1}^{N_i}, y_i)\}_{i=1}^\ell$.  The goal of distribution regression is to learn the map between the random distribution $X$ and its label $y$ based on the observed data $\tilde z$. %This problem involves two levels of randomness.
%$z$ is first drawn from $\mathcal{M}$, and then $\tilde z$ is generated by sampling points from $x_i$ for $i=1,\ldots, \ell$.

To solve the distribution regression problem, an approach via kernel ridge regression is proposed \citep{szabo2016learning} and a fast convergence is proven. 
%Existing techniques with consistency guarantees for distribution regression require kernel density estimation as an intermediate step. 
%Unfortunately, density estimation in high dimensional spaces often suffers from slow convergence rates. 
%An alternative approach is proposed in \citep{szabo2016learning} to tackle the distribution regression problem via kernel ridge regression. Such an approach works directly on distribution embeddings and does not use density estimation as an intermediate step.
%Here, we demonstrate how learning RKHS functionals is essential in the approach proposed by \citep{szabo2016learning}.
The approach involves kernel mean embedding as an intermediate step. Denote by
$
    \mu(\mathcal{M}^+(\mathcal{X})) = \{\mu_X:X\in \mathcal{M}^+\} \subseteq \mathcal H_K \text{ with } \mu_X = \int_{\mathcal{X}} \textbf{Ker}(\cdot,u)dX(u) \in  \mathcal H_K
$
the set of mean embeddings of $X$ to a RKHS $\mathcal H_K$. %(the map $X \mapsto \mu_X$ is defined for $X\in \mathcal{M}^+$ if $\textbf{Ker}(\cdot,\cdot)$ is bounded).
We also let $H_K $ be the $Y$-valued RKHS of $\mu(\mathcal{M}^+(\mathcal{X})) \rightarrow \mathcal{Y}$ functions with reproducing kernels $\textbf{Ker}: \mu(\mathcal{M}^+(\mathcal{X})) \times \mu(\mathcal{M}^+(\mathcal{X})) \rightarrow \mathcal{Y}$. 
The distribution $X\in \mathcal{M}^+$ is first mapped to $\mu(\mathcal{M}^+(\mathcal{X})) \subseteq \mathcal H_K$ by the mean embedding $\mu$, then the result is composed with function $f\in H_K$:
\begin{equation*}
    \mathcal{M}^+ \overset{\mu} {\rightarrow} \mu(\mathcal{M}^+(\mathcal{X}))\overset{f\in H_K}{\longrightarrow} \mathcal{Y}. 
\end{equation*}
An important part of the method is to learn the functionals that map $\mu(\mathcal{M}^+(\mathcal{X}))$ (a subset of RKHS $\mathcal H_K$)  to $\mathcal{Y}$. 
%This can be addressed by using the parametric representation of functions as follows.
%Let $d_0 \in \NN$, suppose we have a set of known functions $\{v_i\}_{i=1}^{d_0}$ that does not need to be learned.
%\begin{definition}[Parametric Functional MLP]
%A parametric functional net $\mathbf{\Theta}_\nu: \mathcal{X} \rightarrow \RR$ with $L$ hidden layers is any functional that takes the form
%\begin{equation}
    %\mathbf{\Theta}_\nu(f) = a^T\sigma_{\mathbf b_L}W_L \sigma_{\mathbf b_{L-1}}W_{L-1}\cdots  \sigma_{\mathbf b_1}W_1 \nu, 
%\end{equation}
%where $f \in \mathcal{X}$.
%Here, each element of $\nu:= \{\nu_1,\ldots, \nu_{d_0}\}$ is defined as \begin{equation*}
    %\nu_i = \int_{[-1,1]^s}f(x)v_i(x)dx.
%\end{equation*}
%\end{definition}

In this paper, we give a theoretical guarantee that neural networks can approximate functionals on RKHS well. Our findings suggest that neural networks have the potential to tackle distribution regression problems, as they can be used to find the map from $\mu(\mathcal{M}^+(\mathcal{X}))$ to $\mathcal{Y}$. 

\section{Main Results I: Approximation of RKHS Functionals induced by specific kernels}\label{sec:mainresultI}

We consider the domain  $\mathcal{X} = [0,1]^d$. We present the first part of our main results here.
The second part of our main results is in Section \ref{sec:mainresultII}, where we give approximation results for functionals on general RKHS's. 
In this section, we give explicit error rates of approximating functionals on RKHS's--- induced by the following three specific reproducing kernels --- to demonstrate our methods and approaches: 
\begin{itemize}
\item Sobolev space $W^r_2(\mathcal{X})$. Although this is not a kernel, it is known that the Sobolev space $W^r_2(\RR^d)$ of order $r$ is a RKHS if $r>d/2$ (see, e.g., \citep{brezis2011functional, wainwright2019high}). 

\item Inverse multiquadric kernel: $K(u,v) = (\sigma^2 + |u-v|^2)^{-\beta}$, where $u,v \in \mathcal{X}$ and $\sigma, \beta >0$. The parameter $\sigma$ is often called the {\it shape parameter}.

\item Gaussian kernel: $K(u,v)= e^{-\frac{|u-v|^2}{2\sigma^2}}$, where $u,v \in \mathcal{X}$ and $\sigma>0$.



\end{itemize} 

We would like to point out that both the inverse multiquadratic kernel and the Gaussian kernel are important examples of {\it translation-invariant kernels} (also known as {\it convolution-type kernels}).  
The definition of a translation-invariant kernel is given below. 

\begin{definition}[Definition of translation-invariant kernel]\label{def:translation}
    A kernel $K:\RR^d \times \RR^d \rightarrow \RR$ is called translation-invariant (or convolution-type) if it only depends on the difference between its arguments, that is,
    \begin{equation}\label{eq:translation}
        K(u,v) = \phi(u-v), \qquad  u,v \in \RR^d
    \end{equation}
    for some function $\phi:\RR^d \rightarrow \RR$. Such a function $\phi$ is considered positive definite if the corresponding kernel $K$ is positive definite.
\end{definition}
Note that we can represent a translation-invariant kernel both as  $K:\RR^d \times \RR^d \rightarrow \RR$ with two arguments and as $\phi:\RR^d \rightarrow \RR$ with only one argument.

The Fourier transform of a function $\phi$ in $\RR^d$ is the function $\widehat{\phi}:  \RR^d \rightarrow \mathbb{C}$ defined by
\begin{equation}
    \widehat{\phi}(\xi) = \int_{\RR^d} \phi(x) e^{-2\pi i\xi \cdot x} dx, \qquad \xi\in \RR^d.
\end{equation}
From the  inverse form of the Fourier transform, we know
\begin{equation}\label{eq:inversefourier}
    \phi(x) = \int_{\RR^d} \widehat{\phi}(\xi) e^{2\pi i\xi \cdot x} d\xi, \qquad x\in \RR^d.
\end{equation}
%Note that the Fourier transform, or the inverse transform, of a real-valued function is (in general) complex-valued. 
It is sometimes difficult to check if a kernel is positive definite. But if a kernel is translation-invariant, we can do so by easily checking its Fourier transform.   


\begin{lemma} \label{lemma:trans}
    A translation-invariant kernel $K$ induced by $\phi: \RR^d \to \RR$ is a reproducing kernel (i.e., positive semi-definite) if its Fourier transform $\widehat{\phi}$ is real-valued and non-negative. 
\end{lemma}
In fact, the positivity of the Fourier transform
almost everywhere is sufficient for the positive definiteness of a kernel \citep{schaback2006kernel}. The proof of Lemma \ref{lemma:trans} is
relegated to Appendix \ref{sec:appendA}. 

In the following, we present explicit error bounds for approximating functionals on Sobolev space and RKHSs induced by inverse multiquadric and Gaussian Kernels. We adopt standard neural networks with two hidden layers. 
The network input is a function $f$ evaluated at some discrete points in $\mathcal{X}$, that is, $\{f(t_1), \ldots, f(t_N)\}$.
%We treat the discretely observed functional values as a vector and then input this vector into the network. 
We want to highlight that the points $\{t_1,\ldots, t_N\}$ need not be uniformly partitioned in $\mathcal{X}$. 
Since we only consider RKHS's induced by translation-invariant kernels in this section, it seems natural to choose $\{t_1,\ldots, t_N\} = \{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ for $m\in \NN$ and $N = (m+1)^d$. 

\subsection{Approximation of Functionals on Sobolev Space $W^r_2(\mathcal{X})$}\label{Sobolev}
We consider the Sobolev Space $W^r_2(\mathcal{X})$ of order $r$  for $r\in \RR_+$ and $d\in \NN$. 
%For a multi-index $a= (a_1,  \cdots, a_d)$, define $|a| =a_1+ \cdots+ a_d$.
%The Sobolev space $W^r_2(\RR^d)$ is defined by 
%\begin{equation*}
  %  W^r_2(\RR^d) := \{f\in L_2(\RR^d): D^a f \in L_2(\RR^d), \quad \forall |a| \leq r\}.
%\end{equation*}
%Here, $D^a$ is the differential operator
%\begin{equation*}
    %D^af(x) := \frac{\partial^{|a|}}{\partial x_1^{a_1} \partial x_2^{a_2} \cdots \partial x_d^{a_d}}, \qquad \forall x\in \RR^d.
%\end{equation*}
 %It is known that $W^r_2(\RR^d)$ is a separable Hilbert space equipped with the inner product 
%\begin{equation*}
%\langle f,g \rangle_{W^r_2(\RR^d)} = \sum_{|a| \leq r}  \langle D^af, D^ag \rangle_{L_2(\RR^d)}, \qquad \forall f,g \in W^r_2(\RR^d). 
%\end{equation*}
We allow $r$ to be any arbitrary real number (i.e., $r$ need not be an integer), and the corresponding Sobolev space $W^r_2(\mathcal{X})$ is often referred to as a {\it fractional Sobolev space}. 
%According to the classic Sobolev Embedding Theorem, when $r>d/2$, we can treat the Sobolev space $W^r_2(\RR^d)$ as a space of continuous functions and function values are continuous linear functionals.
The following well-known result suggests that when the embedding condition $r>d/2$ holds, $W^r_2(\mathcal{X})$ is a RKHS. 
\begin{proposition}
    The standard Sobolev space $W^r_2(\RR^d)$, with arbitrary positive  $r$ and $d$ for which $r>d/2$, is a RKHS induced by a translation-invariant  kernel $K(u,v) = \phi (u-v)$, where its Fourier transform is the function $\widehat{\phi}(\xi)$ given by
\begin{equation}\label{fourier_Sobolev}
    \widehat \phi (\xi) = (1+|\xi|^2)^{-r}, \qquad  \xi \in \RR^d.
\end{equation}
\end{proposition}

From Lemma \ref{lemma:trans}, since $\widehat \phi(\xi) = (1+|\xi|^2)^{-r} >0$ for all $\xi \in \RR^d$, $K$ (or $\phi$) is a reproducing kernel.
It is somehow surprising that it is difficult to find explicit formulas for reproducing kernels of Sobolev spaces, except for the univariate case $d=1$. 
More discussions can be found in \citep{novak2018reproducing} and references therein. 

%$K_x = K(\cdot, x) \in W^r_2(\RR^d)$ for all $x \in \RR^d$. 

The classic Sobolev Embedding Theorem tells us that, for $r>d/2$ and $r-d/2 \notin \NN$, we have $W^r_2(\mathcal{X}) \subset W_\infty^{r-d/2}(\mathcal{X})$, where $W_\infty^{r-d/2}(\mathcal{X})$ represents the H\"older space of order $r-d/2$. 
Moreover, we have
\begin{equation*}
    \|f\|_{W_\infty^{r-d/2}} \leq C_{r,d}  \|f\|_{W^r_2(\RR^d)},
\end{equation*}
where $C_{r,d}$ is a constant depending on only $r$ and $d$.
When $r>d/2$ and $r-d/2 \in \NN$, we can easily extend the above results to get $W^r_2(\mathcal{X}) \subset W_\infty^{r-d/2-\varepsilon}(\mathcal{X})$ and $\|f\|_{W_\infty^{r-d/2 - \varepsilon}} \leq C_{r,d}  \|f\|_{W^r_2(\RR^d)}$, with any $0< \varepsilon < r-d/2$. Hereafter we assume $r-d/2 \notin \NN$.

The following result establishes explicit error bound in approximating functionals on Sobolev Space $W^r_2(\mathcal{X})$  using two-layer tanh neural networks. Its proof is relegated to Appendix \ref{sec:appendB}.

\begin{theorem}[Approximation of Functionals on Sobolev Space]\label{thm:Sobolev}
    Let $r\in \RR_+, d\in \NN$, $\mathcal{X} = [0,1]^d$, and $r-d/2>1$. We assume that $r-d/2 \notin \NN$.  
    Consider $
    \mathcal{K} := \{f\in W^r_2(\mathcal{X}): \|f\|_{W^r_2(\mathcal{X})} \leq 1\}
$. 
Suppose $F$ is  $s$-H\"{o}lder continuous with $s \in (0,1]$. 
    There exists some $M_0\in \NN$ such that for every $M\in \NN$ with $M>M_0$, by taking $m=\left\lceil M^{\frac{1}{2s(2r-1)}}\right\rceil$, $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \subset \mathcal{X}$, and $N= (m+1)^d$, we have a tanh neural network $\widehat{G}: \RR^N \to \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left(\frac{N+1}{2}\right) (5M)^N$ satisfying 
\begin{equation*}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq C_{r, s, d, F} \ d^{s(r+\frac{1}{2})} \left(\frac{1}{M}\right)^{\frac{2r-d}{2(2r-1)}},
\end{equation*}
where $C_{r, s, d, F}$ is a constant depending on $r, s, d$ and the Lipschitz constant of $F$.
\end{theorem}


\begin{remark}\label{remark:rate_thm3}
The maximum total number of parameters in this network is $\mathcal{N} = \mathcal{O}(N(5M)^N)$, where $N=(m+1)^d = \left(\left\lceil M^{\frac{1}{2s(2r-1)}}\right\rceil +1 \right)^d$. 
As $M$ increases (i.e., the network widths increase), the approximation error converges to $0$. 
The convergence rate given in the above theorem --- w.r.t. the number of parameters $\mathcal{N}$ --- achieves a logarithmic rate. We defer the technical proof of this statement to Appendix \ref{sec:append_remark} for conciseness. 
\end{remark}


\subsection{Approximation of RKHS Functionals Induced by Inverse Multiquadric Kernel }
Here, we present the result on the approximation of functionals defined over $\mathcal{K}$, an RKHS induced by the inverse multiquadric kernel 
\begin{equation}\label{multi}
    K(u,v) = (\sigma^2 + |u-v|^2)^{-\beta}, \qquad  u,v \in \mathcal{X}, \ \sigma, \beta >0
\end{equation} using two-layer tanh neural networks. 
%The multiquadric kernel is an example of a translation-invariant kernel (corresponding to $\phi(x) = (\sigma^2 + |x|^2)^{-\beta}$). 
The inverse multiquadric kernel is a reproducing kernel as long as $\sigma,\beta >0$ (see, \citep{bozzini2015radial, tolstikhin2017minimax}). 
The proof of the following theorem is given in Appendix \ref{sec:appendB}. 
%We make use of the fact that multiquadric kernel is $\alpha$-H\"{o}lder continuous for $\alpha = 1$ with constant $C_K = 2\sqrt{d}\beta  \sigma^{-2\beta-2}$.

\begin{theorem}[Approximation of RKHS Functionals induced by inverse multiquadric Kernel]\label{thm:multi}
Let $d\in \NN$, $\sigma, \beta >0$, $\mathcal{X} = [0,1]^d$, and $M_d = 12 \left(\frac{\pi \Gamma^2(\frac{d+2}{2})}{9}\right) \leq 6.38d$. 
Consider $
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}
$ with $\mathcal{H}_K$ induced by an inverse multiquadric kernel given in (\ref{multi}). 
Suppose $F$ is  $s$-H\"{o}lder continuous with $s \in (0,1]$. 
There exists some $M_0\in \NN$ such that for every $M\in \NN$ with $M>M_0$, by taking $m=\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil$, $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \subset \mathcal{X}$, and $N= (m+1)^d$, we have a tanh neural network $\widehat{G}: \RR^N \to \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying 
\begin{equation*}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq C_{\sigma, \beta, s, d, F} (\log (M))^{\max\{0, 2d-s\beta\}}\left(\frac{1}{M}\right)^{\frac{c}{4M_d \sqrt{d}\sigma+c}},
\end{equation*}
where $C_{\sigma, \beta, s, d, F}$ is a constant depending on $\sigma, \beta, s, d$ and the Lipschitz constant of $F$, $c>0$ is a positive constant given for the multiquadric kernel by (\ref{f-Pf_multi}). 
\end{theorem}

\begin{remark}\label{remark:rate_thm1}
The maximum number of network parameters is $\mathcal{N} = \mathcal{O}\left(N(5M)^N\right)$, where $N = (m+1)^d = \left(\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil +1\right)^d$.
As $M$ increases, the approximation error converges to $0$. 
The convergence rate of the approximation error given in the above theorem --- w.r.t. the number of network parameters $\mathcal{N}$ --- is always faster than the logarithmic rate but slower than the polynomial rate. 
We defer the technical proof of this statement to Appendix \ref{sec:append_remark} for conciseness. 
\end{remark}

\begin{remark}
It is indeed quite disappointing that the tanh neural network cannot achieve a polynomial rate for approximating RKHS functionals.
%At this point, a question arises: Do we have a lower bound for such approximation error in the literature? 
The only lower bound of approximation of continuous functionals is established in \citep[Theorem 2.2]{mhaskar1997neural}. They studied the approximation of $F: L^p (\mathcal{X}) \rightarrow \RR$ for $1 \leq p \leq \infty$ on some certain compact domain $K \subset W^\beta_\infty (\mathcal{X})$, the set of functions satisfying
a Hölder condition of order $\beta >0$. They obtained a lower bound, with $0 < \lambda \leq 1$, 
$$\sup_{f\in K}\left|F(f) - \widehat{G}(f)\right|\geq C(\log (\mathcal N))^{-\frac{\beta \lambda}{d}},$$ where $\widehat{G}$ represents neural networks equipped with sigmoid-type infinitely differentiable activation functions (e.g., tanh function), and $\mathcal{N}$ is the total number of network parameters. 
This is a logarithmic rate of approximation error. 
We cannot directly compare our results to their lower bound because we study  RKHS functionals instead of $L^p$ functionals. 
However, such a lower bound suggested that the approximation of continuous functionals may be generally slow.  
\end{remark}

\subsection{Approximation of RKHS Functionals Induced by Gaussian Kernel}
Gaussian kernel is an important translation-invariant kernel given by
\begin{equation}\label{Gaussian}
    K(u,v)= e^{-\frac{|u-v|^2}{2\sigma^2}}, \qquad u,v \in \mathcal{X}, \sigma >0,
\end{equation}
or $\phi(x) = e^{-\frac{x^2}{2\sigma^2}}$ for $x\in\mathcal{X}$. 
From Lemma \ref{lemma:trans}, the Gaussian kernel is a reproducing kernel because its Fourier transform is known to be $\widehat{\phi}(\xi) = (2\sigma^2\pi)^{\frac{d}{2}}e^{-2\sigma^2\pi^2 |\xi|^2}> 0$ for all $\xi \in \RR^d$. 
The following result gives an explicit error bound of approximation of functional $F$ over $\mathcal{K}$, an RKHS induced by a Gaussian kernel, using two-layer tanh neural networks. Its proof is given in Appendix \ref{sec:appendB}. 

\begin{theorem}[Approximation of RKHS Functionals induced by Gaussian Kernel]\label{thm:Gaussian}
Let $d\in \NN$, $\sigma >0$, $\mathcal{X} = [0,1]^d$. We also let $c>0$ be some positive constant. 
Consider $
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}
$ with $\mathcal{H}_K$ induced by a Gaussian kernel given in (\ref{Gaussian}). 
Suppose $F$ is  $s$-H\"{o}lder continuous with $s \in (0,1]$. 
There exists some $M_0\in \NN$ such that for every $M\in \NN$ with $M>M_0$, by taking
\begin{equation}\label{m}
        m = \left\lceil \frac{2 \log(M)}{\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}}\right \rceil > \sqrt{d},
    \end{equation} 
$\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \subset \mathcal{X}$, and $N= (m+1)^d$, we have a tanh neural network $\widehat{G}: \RR^N \to \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying 
\begin{equation}\label{thm:Gaussian_bound}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq C_{\sigma, \pi, s, d, F} (\log (M))^{d}\left(\frac{1}{M}\right)^{\frac{1}{2(1+ \sigma \pi d)}
    \left(\frac{1}{2}\log\log(M) - \log(cs+ \sigma \pi d\sqrt{s})\right)},
\end{equation}
where $C_{\sigma, \pi, s, d, F}$ is a constant depending on $\sigma, \pi, s, d$ and the Lipschitz constant of $F$.
\end{theorem}

\begin{remark}
The maximum total number of parameters of the network is $\mathcal{N}=  \mathcal{O}\left(N(5M)^N\right)$, where $N= (m+1)^d$ with $m$ given in \eqref{m}.  
As $M$ increases, the error bound given in (\ref{thm:Gaussian_bound}) converges to $0$. 
Similar to Remark \ref{remark:rate_thm1} for Theorem \ref{thm:multi}, we can check that the convergence rate (w.r.t. $\mathcal{N}$) is faster than a logarithmic rate but slower than a polynomial rate. 
\end{remark}


In the next section, we give our result for approximating functionals on general RKHS's.

\section{Main Results II: Approximation of general RKHS Functionals}\label{sec:mainresultII}
In this section, we give our results on approximating functionals on RKHS's induced by a general Mercer kernel using tanh neural networks. 
We consider an RKHS $\mathcal{H}_K$ induced by some Mercer kernel $K: \mathcal{X} \times \mathcal{X}  \rightarrow \RR$. 
We assume $K$ is $\alpha$-H\"{o}lder continuous for $0<\alpha \leq 1$ with constant $C_K \geq 0$,  that is, 
\begin{equation}\label{reg_K}
    |K(u,v)-K(u,\Tilde{v})| \leq C_K (d_\mathcal{X}(v,\Tilde{v}))^\alpha , \qquad  u,v,\Tilde{v} \in \mathcal{X}.
\end{equation}

In $\mathcal{X}$, we choose a set of finitely many locations $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$ for $N\in\NN$ such that the corresponding Gram matrix \begin{equation}\label{gram}
    K[\Bar{t} ] = (K(t_i, t_j))_{i,j=1}^N 
    =  \begin{bmatrix}
    K(t_1, t_1) & K(t_1, t_2) & \cdots & K(t_1, t_N)\\
    K(t_2, t_1) & K(t_2, t_2) & \cdots & K(t_2, t_N)\\
    \vdots & \vdots & \ddots & \vdots\\
    K(t_N, t_1) & K(t_N, t_2) & \cdots & K(t_N, t_N)
      \end{bmatrix}
\end{equation} is invertible. We do not need these locations to be uniformly spaced in $\mathcal{X}$. 
In general, we select $\Bar{t}$  such that
\begin{equation*}
    \max_{x\in \mathcal{X}} \min_{t\in \Bar{t}} \ d_\mathcal{X}(x,t) \rightarrow 0, \qquad \text{as } N \rightarrow \infty.
\end{equation*}
Later, we will show that the norm of the inverse of $K[\Bar{t} ]$ (i.e., $\|(K[\Bar{t}])^{-1 }\|_{op}$) has a significant influence on the approximation rate of RKHS functionals. 

Before presenting our last theorem, we introduce two geometric quantities employed in our analysis: the {\it fill distance} and the {\it separation radius}.  
For any $x\in \mathcal{X}$ and $\Bar{t} \subset \mathcal{X}$, let $t_{j_x}$ be the closest point in $\Bar{t}$ to $x$, with $j_x \in \{1,\ldots, N\}$. The fill distance of $\Bar{t}$  is given by 
\begin{eqnarray}\label{fill}
    h_{\Bar{t}} := h_{\Bar{t},\mathcal{X}} := \sup_{x\in \mathcal{X}} \min_{1\leq i \leq N}d_\mathcal{X}(x, t_i) = \sup_{x\in \mathcal{X}} d_\mathcal{X}(x,t_{j_x}) , 
\end{eqnarray}
which indicates how well the points in $\Bar{t}$ fill out the domain $\mathcal{X}$. 
It measures the radius of the largest possible empty ball that can be placed among the points in $\Bar{t}$. 
The second geometric quantity is the separation radius of $\Bar{t}$, which is half
the distance between the two closest points in $\Bar{t}$. It is given by 
\begin{equation}\label{separation}
    q_{\Bar{t}} := \frac{1}{2} \min_{u \neq v \in\bar{t}} |u-v|.
\end{equation}
We can see that $h_{\Bar{t}}$ and $q_{\Bar{t}}$ decreases as  $N\in \NN$ increases.

In order to quantify the worst-case (i.e., for any $f$ in ${\mathcal{H}_K}$) uniform error between a function and its interpolant, the so-called {\it power function} is widely adopted in kernel interpolation literature (see, e.g., \citep{fasshauer2005meshfree, pazouki2011bases, kanagawa2018gaussian}). In our analysis, we use the power function to, in turn, measure the regularity of a Mercer kernel $K$. For a kernel $K$ and $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$, the power function is defined as 
%The following function is widely used to measure the regularity of a Mercer kernel $K$ and is often referred to as the \textit{power function} in kernel interpolation literature :
\begin{equation}\label{power}
    \epsilon_K(\Bar{t}):= \max_{x\in \mathcal{X}} \min_{c\in\RR^N} \left\|K_x -  \sum_{i=1}^N c_i  K_{t_i}\right\|_{\mathcal{H}_K} 
    = \max_{x\in \mathcal{X}} \min_{c\in\RR^N} \left\{ K(x,x) -2 \sum_{i=1}^N c_i K(x,t_i)+ \sum_{i=1}^N \sum_{j=1}^N c_i  c_j K(t_i, t_j)\right\}^{\frac{1}{2}}. 
\end{equation}

For any $\Bar{t} = \{t_i\}_{i=1}^N \subset \mathcal{X}$, by the Gram matrix given in (\ref{gram}), the fill distance given in (\ref{fill}), and the power function defined in (\ref{power}), we are able to derive an error bound for the approximation of functional on RKHS induced by a general Mercer kernel. 

\begin{theorem}[Approximation of general RKHS Functionals]\label{thm:general}
    Let $d\in \NN$, $\mathcal{X} = [0,1]^d$. 
    Consider $
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}
$ with $\mathcal{H}_K$ induced by some Mercer kernel $K$ which is $\alpha$-H\"{o}lder continuous for $\alpha \in (0, 1]$ with constant $C_K \geq 0$. 
Suppose $F$ is  $s$-H\"{o}lder continuous for $s \in (0,1]$ with constant $C_F \geq 0$. 
    There exists some $M_0\in \NN$ such that for every $M\in \NN$ with $M>M_0$, by taking some
    $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$ with $N\in \NN$, we have a tanh neural network $\widehat{G}$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying 
\begin{equation}\label{thm:general_bound}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq  C_F (\epsilon_K(\Bar{t}))^s + \frac{7N^2C_G}{M},
\end{equation}
with $$C_G = C_F\left(1+ \|(K[\Bar{t}])^{-1 }\|_{op}\sqrt{N} C_K \left(h_{\Bar{t}}\right)^\alpha\right)^s,$$ 
where the Gram matrix $K[\Bar{t}]$ is given in (\ref{gram}), the fill distance $h_{\Bar{t}}$ is given in (\ref{fill}), and the power function $\epsilon_K(\Bar{t})$ is defined in (\ref{power}). 
\end{theorem}

Let us look closely at the approximation error bound we obtained in \eqref{thm:general_bound}, particularly how the number of locations $N\in \NN$ in $\Bar{t}$ affects the bound. 
The error bound consists of two components: the former term corresponds to the power function $\epsilon_K(\Bar{t})$ which typically decreases as $N$ increases; the latter term is of $\mathcal{O}(N^{2+\frac{s}{2}})$ which obviously increases with $N$. 
We can see a trade-off associated with $N$ between these two terms. 
We shall strike the right balance and find the ``sweet spot" of $N$ so neither of these terms blows up. 
%In order to minimize the approximation error, we balance the trade-off by finding an optimal choice of $N$, so that neither of these terms blows up. 
Later, in the proofs of our main results in Appendix \ref{sec:appendB}, we give detailed derivations of the optimal choices of $N$.  


\section{Approximate
 Regression Map in Functional Regression Using Neural Network}\label{sec: regression}
In this section, we apply our findings to approximate regression maps in generalized functional linear regression models. 

Despite their widespread success, the application of neural networks to functional data remains scarce today. 
Extensively studied in the statistics literature, functional data appear frequently in scientific studies, such as in datasets of air pollution, fMRI scans, and growth curves \citep{yao2005functional, leng2006classification, chiou2012dynamical}.
Functional regression with observed random functions as predictors coupled with scalar responses is one of the core tools of functional data analysis \citep{morris2015functional, wang2016functional}.  
The classical model of
functional regression is the functional linear model (FLM), where the response $Y\in \RR$ is related to a  square-integrable random function $X\in L^2([0,1])$  through
\begin{equation}\label{flm}
    Y= \int_0^1 X(t) \beta(t) dt + \varepsilon  = \langle X, \beta \rangle_{L^2([0,1])} + \varepsilon. 
\end{equation}
Here the unknown functional parameter $\beta \in L^2([0,1])$ is referred to as {\it slope function}, and $\varepsilon$ is a centered noise random variable. 
An extension of the above model is a generalized functional linear model \citep{muller2005generalized} given by 
\begin{equation}\label{gflm}
    Y= g\left(\int_0^1 X(t) \beta(t) dt\right) + \varepsilon =  g\left(\langle X, \beta \rangle_{L^2([0,1])}\right) + \varepsilon,
\end{equation}
where $g:\RR \to \RR$ is a link function. We shall assume $g$ is Lipschitz continuous in $\RR$. 
A fundamental problem in statistics is estimating the slope function $\beta$ in the FLM based on a training sample 
consisting of $n$ independent copies of $(X, Y)$ (see, e.g., \citep{muller2005generalized, chen2011single, yuan2010reproducing}). An efficient estimator of $\beta$ is crucial
for subsequently retrieving
the regression mean 
    $\mathbb{E}[Y|X]:= g\left(\int_0^1 X(t) \beta(t) dt\right)$.
This paper studies the approximation using neural networks; estimation based on training samples is left for future study.  
Here, we aim to approximate the regression map of generalized FLM by neural networks and derive the rate of convergence of approximation error. 

For a function covariate $X\in L^2([0,1])$, let $F:L^2([0,1]) \to \RR$ be the regression map of generalized FLM given by
\begin{equation}\label{func_gflm}
    F: X \mapsto g\left(\int_0^1 X(t) \beta(t) dt\right), \qquad X\in L^2([0,1]).
\end{equation}   
We shall restrict the domain of $F$ onto a subset of  a RKHS $\mathcal{H}_{K_0}$ induced by a Mercer kernel $K_0$ on $[0,1] \times [0,1]$.
Consider the unit ball $\mathcal{K}_0 := \{f\in \mathcal{H}_{K_0} \subset L^2([0,1]): \|f\|_{\mathcal{H}_{K_0}} \leq 1\}$. 
We assume $\mathcal{H}_{K_0}$ is dense in $L^2([0,1])$.
%(i.e., any function in $L^2([0,1])$ can be approximated by a sequence of functions in $\mathcal{K}$). 
It is known that the RKHS induced by an inverse multiquadric kernel or a Gaussian kernel is dense in $L^2$ space, as well as the Sobolev space in $\RR$ with order $r>1/2$. 
%Note that $F$ is Lipschitz continuous if $X\in L^2$. 
We define a RKHS functional 
$F|_{\mathcal{H}_{K_0}}:\mathcal{K}_0 \to \RR$ by 
\begin{equation}\label{regmap}
    F|_{\mathcal{H}_{K_0}}: X \mapsto g\left(\int_0^1 X(t) \beta(t) dt\right), \qquad X\in \mathcal{K}_0 \subset L^2([0,1]).
\end{equation}
 


 
Applying our previous theoretical findings, we prove that two-layer tanh neural networks can approximate the generalized functional regression map well. Concretely, for any function covariate $X\in \mathcal{K}_0$ and some discrete points  $\bar{t} = \{t_1, \cdots, t_N\}\subset [0,1]$ (even spacing is not necessary), there exists a network $\widehat G:\RR^N \to \RR$ that takes $X(\bar{t})=(X(t_1), \cdots, X(t_N))\in\RR^N$ as input such that 
$\sup_{X\in \mathcal{K}_0}\left|F|_{\mathcal{H}_{K_0}}(X) - \widehat{G}(X(\Bar{t}))\right| \to 0$  as network widths increases.

For a function $f:[-T, T] \to \RR$, we define its Lipschitz seminorm as $$\|f\|_{Lip[-T,T]}:= \sup_{x,y\in [-T, T], x \neq y}\frac{|f(x) - f(y)|}{|x-y|}.$$
 With a slight abuse of notation, we use $\|\cdot\|_{L^2}:= \|\cdot\|_{L^2([0,1])}$ denote the $L^2$ norm over $[0,1]$.
The following result follows from Theorem \ref{thm:general} and Theorem \ref{thm:Gaussian}.
Its proof is relegated to Appendix \ref{sec:appendB}. 
\begin{corollary}[Approximation of Regression Map in Generalized FLM] \label{corollary:FLM}
Let $d=1$ and $\mathcal{X} = [0,1]$.
Assume $\mathcal{H}_{K_0} $ is dense in $L^2([0,1])$, where $\mathcal{H}_{K_0}$ is a RKHS induced by some Mercer kernel $K_0$ which is $\alpha$-H\"{o}lder continuous for some $\alpha \in (0, 1]$ with constant $C_{K_0} \geq 0$.  
Let $\kappa=\sup_{t\in[0,1]}\sqrt{K_0(t,t)}$. 
%Let $\kappa = \sqrt{\int_{0}^1K_0(t,t) dt}$.
%Assume $g$ is Lipschitz in $\RR$. 
Consider the regression map $F|_{\mathcal{H}_{K_0}}$ defined in \eqref{regmap}.
 There exists some $M_0\in \NN$ such that for every $M\in \NN$ with $M>M_0$, by taking some
    $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$ with $N\in \NN$, we have a tanh neural network $\widehat{G}: \RR^N \to \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying 
\begin{equation}\label{thm:general_bound}
    \sup_{X\in \mathcal{K}_0}\left|F|_{\mathcal{H}_{K_0}}(X) - \widehat{G}(X(\Bar{t}))\right|\leq  C_F (\epsilon_{K_0}(\Bar{t})) + \frac{7N^2C_G}{M},
\end{equation}
with $$C_F= \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])} \|\beta\|_{L^2}\kappa, C_G = C_F\left(1+ \|(K_0[\Bar{t}])^{-1 }\|_{op}\sqrt{N} C_K \left(h_{\Bar{t}}\right)^\alpha\right),$$ 
where the Gram matrix $K_0[\Bar{t}]$ is defined in (\ref{gram}), the fill distance $h_{\Bar{t}}$ is given in (\ref{fill}), and the power function $\epsilon_{K_0}(\Bar{t})$ is defined in (\ref{power}).

In particular, if we take $K_0$ to be a Gaussian kernel with $\sigma >0$,  
        $m = \left\lceil \frac{2 \log(M)}{c + \sqrt{c^2+4\sigma^2\pi^2 \log(M)}}\right \rceil > 1$,
$\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}$, and $N= m+1$, we have 
\begin{equation}
    \sup_{X\in \mathcal{K}}\left|F|_{\mathcal{H}_K}(X) - \widehat{G}(X(\Bar{t}))\right|\leq C_{\sigma, \pi, F} \log (M)\left(\frac{1}{M}\right)^{\frac{1}{2(1+ \sigma \pi )}
    \left(\frac{1}{2}\log\log(M) - \log(c+ \sigma \pi)\right)},
\end{equation}
where $C_{\sigma, \pi, F}$ is a constant depending on $\sigma, \pi$ and $C_F$, $c$ is a positive constant given by $\eqref{f-Pf_Gaussian}$. 
\end{corollary}

Note that we can also take $K_0$ to be an inverse multiquadric kernel or a Sobolev kernel in $\RR$. Then, by applying Theorem \ref{thm:multi} or Theorem \ref{thm:Sobolev}, we can obtain the corresponding approximation error bound. 

By the assumption that $\mathcal{H}_{K_0}$ is a dense subset, for any $X\in cl(\mathcal{K})$ (i.e., closure of $\mathcal{K}$), there exists a sequence of functions $\{X_n: \mathcal{K} \to \RR, n\in \NN\}$ uniformly convergent to $X$ (i.e., $\sup_{t\in [0,1]}|X_n(t) - X(t)| \to 0$ as $n\to \infty$). 
We can see that the functional $F|_{\mathcal{H}_K}$ can be extended to $F$ on $cl(\mathcal{K})$ as
%For a function sequence $\{X_n\}$, the limit of  $F|_{\mathcal{H}_K}(X_n)$ uniquely exists and is equal to $F(X)$. 
     \begin{equation*}
         F(X) = \lim_{n\to \infty} F|_{\mathcal{H}_K} (X_n) = \lim_{n\to \infty} g\left(\int_0^1 X_n(t) \beta(t) dt\right),\qquad X\in cl(\mathcal{K})\subset L^2([0,1])
     \end{equation*}
     where $\{X_n: \mathcal{K} \to \RR, n\in \NN\}$ is a sequence of functions uniformly convergent to $X$. 

   In this paper, we only consider approximating functional regression maps using neural networks. Based on our results, we find that the choice of $\Bar{t}$ and the smoothness of the kernel that induces a RKHS play important roles in improving the approximation abilities. 
   To prove that neural networks can learn functional regression maps well, we shall extend our work to a generalization analysis in the future.
   Mathematically, the generalization error consists of the approximation error and the estimation error (based on training samples of independent copies of $(X,Y)$). 
   We may assume that the slope function $\beta$ resides in RKHS in $L^2$ space to determine the estimation error. 
   This is, in fact, a common assumption imposed in the statistics literature on the estimation and inference of FLM (see, \citep{yuan2010reproducing, shang2015nonparametric,balasubramanian2022unified}).  
 %\begin{proof}
 %Suppose the function sequences $\{X_n: \mathcal{K} \to \RR, n\in \NN\}$ and $\{U_n: \mathcal{K} \to \RR, n\in \NN\}$ both converge to $X$. 
 
 %By the Cauchy–Schwarz inequality, for $n\in \NN$,
%$$|\langle X_n, \beta \rangle_{L^2([0,1])}| \leq \|\beta\|_{L^2}  \|X_n\|_{L^2}\leq \|\beta\|_{L^2}\kappa  \|X_n\|_{\mathcal{H}_K} \leq \kappa \|\beta\|_{L^2},$$
%where $\kappa:= \int_{0}^1 |K(t,t)| dt$ is  bounded. 

%For $n,m \in \NN$, 
%\begin{eqnarray*}
   %\left |g\left(\langle X_n, \beta \rangle_{L^2([0,1])}\right) - g\left(\langle X_m, \beta \rangle_{L^2([0,1])}\right)\right|
   %&\leq& \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])}|\langle X_n, \beta \rangle_{L^2([0,1])} - \langle X_m, \beta \rangle_{L^2([0,1])}|\\
   %&\leq& \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])}\|\beta\|_{L^2} \|X_n-X_m\|_{L^2}\\
  % & &\to 0,  \text{ as } n,m \to \infty,
%\end{eqnarray*}
%because $\|X_n-X_m\|_{L^2} $ as $n,m \to \infty$. We can see that $\lim_{n\to \infty} g\left(\int_0^1 X_n(t) \beta(t) dt\right)$ exists. Next, we will prove that such a limit is well-defined. 

%For $n\in \NN$, 
%\begin{eqnarray*}
   %\left |g\left(\langle X_n, \beta \rangle_{L^2([0,1])}\right) - g\left(\langle U_n, \beta \rangle_{L^2([0,1])}\right)\right|
  % &\leq& \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])}\|\beta\|_{L^2} \|X_n-U_n\|_{L^2} \\
   %&& \to 0,  \text{ as }  n \to \infty,
%\end{eqnarray*}
%because $\|X_n-U_n\|_{L^2} \to 0$ as $n \to \infty$. The proof is complete. 
% \end{proof}
 
%A seminal work in this direction is \citep{yuan2010reproducing}, where a  RKHS-based penalized linear least-squares estimator  (for a penalty parameter $\lambda >0$), given by  $\widehat \beta:= \arg \min_{\beta \in \mathcal{H}_K} \frac{1}{n}\sum_{i=1}^n \left(Y_i - \int_0^1 X_i(t) \beta(t) dt \right)^2 + \lambda \|\beta \|^2_{\mathcal{H}_K}$, was proposed and analyzed. 
%While the minimization of the regularized objective is over a possibly infinite-dimensional RKHS $\mathcal{H}_K$, using the classical ideas of the representer theorem, it has been shown in \citep{yuan2010reproducing} that the estimator $\widehat \beta$ can be computed by solving a finite-dimensional regularized linear inverse problem. 
%For the case $\mathcal{H}_K = W^r_2([0,1])$ for $r>1/2$, the  estimation error converges to $0$ with a rate $n^{-(2r+2)/(2r+3)}$. 




The next section discusses the general approaches and methods for establishing our theorems.

\section{Proof Sketch of Main Results}\label{sec:method}
This section presents the general methods for approximating RKHS functionals by tanh neural networks. 

Our proofs are constructive and rely on three key ideas: (i) the construction of a projection operator on RKHS using a set of nodal functions, (ii) the estimation of power functions, which is widely adopted in the kernel interpolation literature to quantify the worst-case uniform error between a function and its interpolant, and (iii) the approximation of Lipschitz continuous functions by tanh
neural networks.

We begin our analysis with a decomposition of the uniform approximation error $\sup_{f\in \mathcal{K}} \left|F(f) - \widehat{G}(f(\Bar{t}))\right|$.
Consider the following error decomposition:
\begin{eqnarray}\label{decomposition}
         \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right| &\leq& 
         \underbrace{\sup_{f\in \mathcal{K}}| F(f) - F(Pf)|}_{\text{error term (I)}} +  \underbrace{\sup_{f\in \mathcal{K}}\left|F(Pf) - \widehat{G}(f(\Bar{t}))\right|}_{\text{error term (II)}}, 
\end{eqnarray}
where $Pf: \mathcal{X} \to \RR$ is a function carefully chosen. 
    
We will first introduce our choice of $Pf$ in the Subsection \ref{subsec:nodal} and show that $\|f-Pf\|_\infty:= \max_{x\in \mathcal{X}}|f(x) - Pf(x)|$ can be estimated by the power function given in (\ref{power}). 
Then, we will estimate the power function for different reproducing kernels of our interest in Subsection \ref{subsec:power}. 
This is how we bound the error term (I). 
Next, to bound the second error term (II), we bound the operator norm $\|(K[\Bar{t}])^{-1 }\|_{op}$ (in Subsection \ref{subsec:norm}) and then apply some existing results in neural network approximation (in Subsection \ref{subsec:appox}).
 
    
\subsection{Approximation of Functions via Nodal functions}\label{subsec:nodal}

Recall that we choose a set of data locations $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$  such that the Gram matrix $K[\Bar{t} ] = (K(t_i, t_j))_{i,j=1}^N$ given in \eqref{gram} is invertible.  We construct a set of \textit{nodal functions} $\{\psi_i\}_{i=1}^N$ on $\mathcal{X}$ associated with $\Bar{t}$ as 
\begin{equation}
    \psi_i(x) := \sum_{j=1}^N c_j K_{t_j}(x),\qquad   c_j\in \RR,i=1,\ldots, N ,
\end{equation}
such that 
\begin{equation*}
    \psi_i(t_j) = \begin{cases}
      1, & \text{if } j=i,\\
      0, & \text{if } j \neq i.
    \end{cases}
\end{equation*}
We can easily check that the set of nodal functions  $\{\psi_i\}_{i=1}^N$ are linearly independent.
%\begin{figure}[h]
%\caption{An example of a nodal function $\psi_i$}
%\centering
%\includegraphics[width=0.6\textwidth]{nodalfunction.png}
%\end{figure}

Next, with the chosen $\Bar{t}=\{t_i\}_{i=1}^N$, we define a projection operator $P: \mathcal{H}_K \rightarrow \mathcal{H}_K$ as 
\begin{equation}
   Pf = \sum_{i=1}^N f(t_i)\psi_i, \qquad f\in \mathcal{H}_K. 
\end{equation}
 $Pf$ is the orthogonal projection onto the finite-dimensional space $\text{span}\{K_{t_i}\}_{i=1}^N$, as shown in the following lemma about the interpolation and projection properties of $P$. 
 %The proof of the following Lemma is given in Appendix \ref{sec:appendA}. 
\begin{lemma}\label{lemma:ortho}
    For any $f \in \mathcal{H}_K$ and $\Bar{t} \subset \mathcal{X}$, we have
\begin{enumerate}
    \item $f(t_\ell) = Pf(t_\ell), \qquad \ell = 1,\ldots, N,$ \label{item1}
    \item $\langle f -Pf, K_{t_\ell}\rangle_{\mathcal{H}_K} = 0, \qquad \ell = 1,\ldots, N.$ \label{item2}
\end{enumerate}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:ortho}]
   The nodal functions  $\{\psi_i\}_{i=1}^N$ can be expressed explicitly as 
\begin{equation}\label{psi}
       \begin{bmatrix}
           \psi_1\\
           \vdots\\
           \psi_N
       \end{bmatrix}= ( K[\Bar{t} ])^{-1} \begin{bmatrix}
           K_{t_1}\\
           \vdots\\
           K_{t_N}
       \end{bmatrix}. 
    \end{equation}
    In this way, we have
    \begin{eqnarray*}
       \begin{bmatrix}
           \psi_1\\
           \vdots\\
           \psi_N
       \end{bmatrix}(t_\ell) 
       &=& ( K[\Bar{t} ])^{-1} \begin{bmatrix}
           K(t_1,t_\ell)\\
           \vdots\\
            K(t_N,t_\ell)
       \end{bmatrix}\\
       &=&\left((K[\Bar{t} ])^{-1} K[\Bar{t} ]\right)_{\cdot , \ell}\qquad (\text{the $\ell$-th column of }(K[\Bar{t} ])^{-1} K[\Bar{t} ])\\
       &=&I_{\cdot , \ell}. \qquad (\text{the $\ell$-th column of identity matrix }I)
     \end{eqnarray*}
    We can see that 
    \begin{equation*}
    \psi_i(t_\ell) = \begin{cases}
      1, & \text{if } i=\ell,\\
      0, & \text{if } i \neq \ell.
    \end{cases}
\end{equation*}
Then we have
\begin{eqnarray*}
    Pf(t_\ell) = \sum_{i=1}^N f(t_i)\psi_i(t_\ell) = f(t_\ell)\psi_\ell(t_\ell) = f(t_\ell).
\end{eqnarray*}
The proof of statement \ref{item1}. is complete. 

Recall the reproducing property of RKHS:  $f(x) = \langle f, K_x \rangle_{\mathcal{H}_K}$. 
Since $f(t_\ell)- Pf(t_\ell) = 0$, we know 
\begin{equation*}
    \langle f,K_{t_\ell} \rangle_{\mathcal{H}_K} - \langle Pf,K_{t_\ell} \rangle_{\mathcal{H}_K}= f(t_\ell) - Pf(t_\ell) =0 \iff  \langle f-Pf,K_{t_\ell} \rangle_{\mathcal{H}_K}=0.
\end{equation*}
The proof of statement \ref{item2}. is complete. 
\end{proof}

The next lemma presents a uniform bound of $|f (x)- Pf(x)|$ for any $x\in \mathcal{X}$, implying that $Pf$ gives a good approximation of $f$ on $\mathcal{X}$.  We use the power function given in (\ref{power}) for the measurement: 
\begin{equation*}
    \epsilon_K(\Bar{t}):= \max_{x\in \mathcal{X}} \min_{c\in\RR^N} \left\|K_x -  \sum_{i=1}^N c_i  K_{t_i}\right\|_{\mathcal{H}_K} =\max_{x\in \mathcal{X}} \min_{c\in\RR^N} \left\{ K(x,x) -2 \sum_{i=1}^N c_i K(x,t_i)+ \sum_{i=1}^N \sum_{j=1}^N c_i  c_j K(t_i, t_j)\right\}^{\frac{1}{2}}. 
\end{equation*}
 Lemma \ref{f-Pf} is a well-known result in the kernel literature. We provide its proof for completeness in Appendix \ref{sec:appendA}. 

\begin{lemma}\label{f-Pf}
    For any $\Bar{t} \subset \mathcal{X}$ and $\{\psi_i\}_{i=1}^N$ defined explicitly in (\ref{psi}), we have 
    \begin{equation}
      \|f- Pf\|_\infty \leq \|f\|_{\mathcal{H}_K}  \epsilon_K(\Bar{t}), \qquad f\in \mathcal{H}_K.
    \end{equation}
\end{lemma}

Recall that we assume, throughout the paper, $F$ is $s$-H\"{o}lder continuous for $0<s \leq 1$ with constant $C_F \geq 0$. As a consequence of Lemma \ref{f-Pf}, we know 
\begin{equation*}
    |F(f) - F(Pf)| \leq C_F \|f-Pf\|^s_\infty \leq C_F \|f\|^s_{\mathcal{H}_K}  (\epsilon_K(\Bar{t}))^s , \qquad \forall f \in \mathcal{H}_K.
\end{equation*}
 We consider $
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}
$ with $\mathcal{H}_K$ induced by some Mercer kernel $K$. We then have, 
\begin{equation}\label{eq:firstterm}
 \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| \leq  C_F   (\epsilon_K(\Bar{t}))^s.
\end{equation}
This is an upper bound for the error term (I) in (\ref{decomposition}) for a general Mercer kernel $K$.
Notice that this is the first term in the error bound presented in \eqref{thm:general_bound} in Theorem \ref{thm:general}. 

Now the important questions are: how should we choose $\Bar{t}=\{t_i\}_{i=1}^N$, and how do we estimate the power function $\epsilon_K(\Bar{t})$ of a Mercer kernel of interest?

\subsection{Estimation of the Power Function}\label{subsec:power}
From now on, we take  a set of $N=(m+1)^d$ data locations $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X} = [0,1]^d$, for some $m\in\NN$. 
This is our choice of $\Bar{t}$ in Theorem \ref{thm:multi}, \ref{thm:Gaussian}, and \ref{thm:Sobolev}.
We can see that $$ \max_{x\in \mathcal{X}} \min_{t\in \Bar{t}} \ d_\mathcal{X}(x,t) \rightarrow 0 \quad \text{ as } N \rightarrow \infty.$$

For any $x\in \mathcal{X}$, let $t_{j_x}$ be the closest point in $\Bar{t}$ to $x$, with $j_x \in \{1,\ldots, N\}$. Then, \begin{equation*}
    |x_k - (t_{j_x})_k| \leq \frac{1}{m}, \qquad k=1,\ldots, d
\end{equation*}
and 
\begin{eqnarray*}
    d_\mathcal{X}(x, t_{j_x}) \leq \sqrt{d\left(\frac{1}{m}\right)^2} = \frac{\sqrt{d}}{m},
\end{eqnarray*}
and thus the fill distance of $\Bar{t}$ is \begin{equation}\label{fill_tbar}
    h_{\Bar{t}} = \sup_{x\in \mathcal{X}} d_\mathcal{X}(x,t_{j_x})\leq  \frac{\sqrt{d}}{m}.
\end{equation}

Recall that we assume the Mercer kernel $K$ is $\alpha$-H\"{o}lder continuous for $0 < \alpha \leq 1$ with constant $C_K \geq 0$. The following lemma presents an estimate of the power function $\epsilon_K(\Bar{t})$ of a general $\alpha$-H\"{o}lder continuous Mercer kernel $K$, and a uniform bound of $|f-Pf|$ for $f\in \mathcal{H}_K$ induced by such a kernel $K$.
\begin{lemma}\label{lemma:power}
For $\mathcal{X} = [0,1]^d$, choose $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ for some $m\in \NN$. We have
\begin{equation}
\epsilon_K(\Bar{t}) \leq 2C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha \quad
\text { and thus } \quad
  \|f-Pf\|_\infty \leq 2C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.
\end{equation}   
\end{lemma}

The proof of Lemma \ref{lemma:power} is given in Appendix \ref{sec:appendA}. In the following, we give explicit bounds of $\|f-Pf\|_\infty$ for specific kernels of interest.

For an inverse multiquadric kernel given in (\ref{multi}), an upper bound of $\epsilon_K(\Bar{t})$ and $\|f-Pf\|_\infty$ are established in  \citep[Equation (15)]{fasshauer2005meshfree} for some positive constant $c$: 
\begin{equation}\label{f-Pf_multi}
\epsilon_K(\Bar{t}) \leq e^{\frac{-cm}{\sqrt{d}}} \quad \text{and thus }\quad  
    \|f-Pf\|_\infty \leq e^{\frac{-cm}{\sqrt{d}}}\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.
\end{equation}

For Gaussian kernel given in (\ref{Gaussian}), an upper bound of $\epsilon_K(\Bar{t})$ and $\|f-Pf\|_\infty$ are established in  \citep[Equation (14)]{fasshauer2005meshfree} for some positive constant $c$: 
\begin{equation}
\epsilon_K(\Bar{t}) \leq e^{\frac{-cm\left|\log \left(\frac{\sqrt{d}}{m}\right)\right|}{\sqrt{d}}} \quad \text{and thus }\quad
    \|f-Pf\|_\infty \leq e^{\frac{-cm\left|\log \left(\frac{\sqrt{d}}{m}\right)\right|}{\sqrt{d}}}\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.
\end{equation}
While $d\in \NN$ represents the data dimension, $m\in \NN$ measures the number of locations in $\Bar{t}$ and is a variable of our choice. If we choose $m$ such that $m>\sqrt{d}$, we have$\left|\log \left(\frac{\sqrt{d}}{m}\right)\right| = \log \left(\frac{m}{\sqrt{d}}\right)$ and thus
\begin{equation}\label{f-Pf_Gaussian}
    \|f-Pf\|_\infty \leq e^{\frac{-cm\log \left(\frac{m}{\sqrt{d}}\right)}{\sqrt{d}}}\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.
\end{equation}

We now turn our attention to the Sobolev Space $W^r_2(\mathcal{X})$ as an RKHS $\mathcal{H}_K$. According to \citep[Theorem 6.5.3]{schaback1997reconstruction}, when $r>d/2$ and $r-d/2 \notin \NN$, an upper bound of the power function is derived from the well-known Sobolev Embedding Theorem as 
\begin{equation*}
\epsilon_K(\Bar{t})  \leq C^\prime_{r,d}d^{r-\frac{d}{2}}m^{d-2r},
\end{equation*}
where $C^\prime_{r,d}$ is some positive constant depending on $r$ and $d$. Thus, we have 
\begin{equation}\label{f-Pf_Sobolev}
\|f-Pf\|_\infty  \leq C^\prime_{r,d}d^{r-\frac{d}{2}}m^{d-2r}\|f\|_{\mathcal{H}_K}.
\end{equation} 
Later, we will use the above uniform bounds of $\|f-Pf\|_\infty$ to prove our main theorems. 
   
\subsection{A New Function $G$ and its Regularity}   

Now, we move on to bound the error term (II) in (\ref{decomposition}): $\sup_{f\in \mathcal{K}}\left|F(Pf) - \widehat{G}(f(\Bar{t}))\right|$.

%Previously in Subsection \ref{subsec:nodal}, we have shown that $F(Pf)$ can approximate $F(f)$ well. 
In order to approximate  $F(Pf) = F\left(\sum_{i=1}^N f(t_i) \psi_i\right)$, we define another function $G:[0,1]^N \rightarrow \RR$ by 
\begin{equation}\label{G}
    G(c) = F\left(\sum_{i=1}^N c_i \psi_i\right), \qquad  c\in [0,1]^N. 
\end{equation}
Then, we have $$G(f(\Bar{t})) = F\left(\sum_{i=1}^N f(t_i) \psi_i\right) = F(Pf).$$ 


The idea of approximating the functional $F$ is by approximating the function $G$ using a fully-connected neural network. For this purpose, we need to figure out the regularity of $G$, presented in the following lemma. 
Its proof is relegated to Appendix \ref{sec:appendA}. 
\begin{lemma}[Regularity of the function $G$]
\label{lemma:reg_G}
For any $x\in \mathcal{X}$ and $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$, if the functional $F:\mathcal{K} \rightarrow \RR$ is $s$-H\"{o}lder continuous for $0<s \leq 1$ with constant $C_F \geq 0$, then the associated function $G$ is $s$-H\"{o}lder continuous with constant $$C_G = C_F\left(1+ \|(K[\Bar{t}])^{-1 }\|_{op}\sqrt{N} C_K \left(h_{\Bar{t}}\right)^\alpha\right)^s,$$ 
where $\|T\|_{op} =\underset{\|x\|=1}{\sup} \|T(x)\|$ denotes the operator norm of $T$, $h_{\Bar{t}}$ denotes the fill distance of  $\Bar{t}$  given in (\ref{fill}).
\end{lemma}

To know the H\"{o}lder constant $C_G$ explicitly, we proceed to compute the operator norm $\|(K[\Bar{t}])^{-1 }\|_{op}$. Since $K[\Bar{t}]$ is a real, symmetric matrix, it is diagonalizable, and so is $(K[\Bar{t}])^{-1 }$. Let $\{\lambda_1, \ldots, \lambda_N\}$ denote the non-increasing sequence of eigenvalues of $K[\Bar{t}]$. We know
\begin{equation*}
    K[\Bar{t}]=V \Lambda V^{-1} \qquad \text{and } \qquad K[\Bar{t}]^{-1}=V \Lambda^{-1} V^{-1}, 
\end{equation*}
where $\Lambda$ is a $N\times N$ diagonal matrix whose diagonal entries are the eigenvalues $\{\lambda_1, \ldots, \lambda_N\}$, and the columns of $V$ are eigenvectors of $K[\Bar{t}]$. We then have  \begin{equation}\label{operatornorm}
    \|(K[\Bar{t}])^{-1}\|_{op} = \frac{1}{\min_{1 \leq j \leq N} \lambda_j} =: \frac{1}{\lambda_{N}},
\end{equation}
where $\lambda_{N}$ is the smallest eigenvalue of $K[\Bar{t}]$.

Since $K[\Bar{t}] v = \lambda_{N} v$ for some eignevector $v=(v_1, \cdots, v_N)\in \RR^N$ of norm $1$, we have
\begin{eqnarray} \label{lambda_N}
    \lambda_{N} = \lambda_{N} v^T v = v^T K[\Bar{t}] v 
    = \sum_{i=1}^N  \sum_{\ell=1}^N v_i v_\ell K(t_i, t_\ell). 
\end{eqnarray}
In other words, to compute $\|(K[\Bar{t}])^{-1}\|_{op}$, it suffices to compute  $\lambda_{N} = \sum_{i=1}^N  \sum_{\ell=1}^N v_i v_\ell K(t_i, t_\ell)$. 

\subsection{Bounding the Operator Norm $\|(K[\Bar{t}])^{-1}\|_{op}$}\label{subsec:norm}
In general, computing the operator norm $\|(K[\Bar{t}])^{-1}\|_{op}$ can be challenging, especially when we do not have information about the eigenvalues or eigenvectors of the Gram matrix $K[\Bar{t}]$. 
The lemma below tells us that if kernel $K$ is translation-invariant, we can bound $\|(K[\Bar{t}])^{-1}\|_{op}$ by the Fourier transform of $K$: 


\begin{lemma}\label{lemma:norminverse}
    Let $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X}$ for some $m\in \NN$. For a translation-invariant and reproducing kernel $K$ (or $\phi)$ defined over $\RR^d$, we have \begin{equation}
        \|(K[\Bar{t}])^{-1}\|_{op} = \frac{1}{\lambda_{N}} \leq  \frac{1}{m\Gamma_m} \quad \text{with } \Gamma_m :=  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi)> 0,
    \end{equation}
    where $\lambda_{N}$ is the smallest eigenvalue of $K[\Bar{t}]$, $\widehat{\phi}$ is the Fourier transform of $\phi$.
\end{lemma}
The proof of Lemma \ref{lemma:norminverse} is relegated to Appendix \ref{sec:appendA}.

Next, we apply Lemma \ref{lemma:reg_G} and Lemma \ref{lemma:norminverse} to bound $\|(K[\Bar{t}])^{-1}\|_{op}$ and subsequently estimate the regularity of $G$ corresponds to functional $F$ on Sobolev space, and on RKHS's induced by the inverse multiquadric kernel and Gaussian kernel, respectively. 

\subsubsection{Sobolev Space $W^r_2(\mathcal{X})$}
From Subsection \ref{Sobolev}, we know that $W^r_2(\mathcal{X})$ with order $r>d/2$ is a RKHS induced by a translation-invariant kernel, whose Fourier transform is given by 
    $\widehat \phi (\xi) = (1+|\xi|^2)^{-r}, \xi \in \RR^d$.
For $\widehat \phi$ and any $m\in \NN$, 
\begin{equation}\label{Gamma_m_Sob}
    \Gamma_m =  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi) \geq \left(1+ \frac{dm^2}{4}\right)^{-r}
\end{equation}
because $|\xi|^2 \leq \frac{dm^2}{4}$ for $ \xi \in [-\frac{m}{2},\frac{m}{2}]^d$. Using this fact, we establish the regularity of the function $G$ corresponding to a functional $F$ on $W^r_2(\mathcal{X})$, presented in the lemma below. Its proof is relegated to Appendix \ref{sec:appendA}. 
\begin{lemma}\label{lemma:C_G_Sob}
    Let $d\in \NN, r\in \RR_+,  \mathcal{X} = [0,1]^d$, and $r-d/2>1$. Take $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X}$ for some $m\in \NN$, and $N=(m+1)^d$. 
    Consider $
    \mathcal{K} := \{f\in W^r_2(\mathcal{X}): \|f\|_{W^r_2(\mathcal{X})} \leq 1\}
$. If $F: \mathcal{K} \to \RR$ is  $s$-H\"{o}lder continuous with $s \in (0,1]$ and constant $C_F\geq 0$, the corresponding function  
    $G:[0,1]^N \to \RR$  given by \eqref{G} is  $s$-H\"{o}lder continuous with constant
\begin{equation*}
    C_G =  C_F C_{r,d,s} m^{2rs+\frac{ds}{2}-2s} d^{rs+\frac{s}{2}}, 
\end{equation*}
where $C_{r,d,s}$ is a positive constant depends only on $r, d$, and $s$.
\end{lemma}




\subsubsection{Inverse Multiquadric Kernel}
The inverse multiquadric kernel 
is a translation-invariant and reproducing kernel.  
We know $|u-v|\leq \underbrace{\sqrt{1+\cdots+1}}_{d \text{ times}}=\sqrt{d}$ for $u,v \in \mathcal{X}$. 
By the Mean Value Theorem, we have for $u,v,\Tilde{v} \in \mathcal{X}$,
\begin{eqnarray*}     
\  | K(u,v) - K(u,\Tilde{v})| &= &\left|(\sigma^2 + |u-v|^2)^{-\beta}- (\sigma^2 + |u-\Tilde{v}|^2)^{-\beta}\right| \\
&\leq& \beta \sigma^{-2\beta-2} \left|\sigma^2 +|u-v|^2- \sigma^2 -|u-\Tilde{v}|^2\right|\\
&\leq& \beta   \sigma^{-2\beta-2} (|u-v|+|u-\Tilde{v}|)(|u-v|-|u-\Tilde{v}|)\\
&\leq&2\sqrt{d}\beta  \sigma^{-2\beta-2}|v-\Tilde{v}|.
\end{eqnarray*}
We see that this kernel is $\alpha$-H\"{o}lder continuous for $\alpha = 1$ with constant $C_K = 2\sqrt{d}\beta  \sigma^{-2\beta-2}$. 
Based on this information, we can deduce the regularity of function $G$ (given by \eqref{G}) corresponds to the functional $F$ on inverse multiquadric kernel-induced RKHS's. The proof of the following lemma is given in Appendix \ref{sec:appendA}.

\begin{lemma}\label{lemma: C_G_multi}
     Let $d\in \NN, \sigma,\beta >0, \mathcal{X} = [0,1]^d$, and $M_d = 12 \left(\frac{\pi \Gamma^2(\frac{d+2}{2})}{9}\right) \leq 6.38d$. Take $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \in \mathcal{X}$ for some $m\in \NN$, and $N=(m+1)^d$.
     Consider $
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}$
 with $\mathcal{H}_K$ induced by a inverse multiquadric kernel. 
If $F: \mathcal{K}\to \RR$ is  $s$-H\"{o}lder continuous with $s \in (0,1]$ and constant $C_F\geq 0$, the function $G:[0,1]^N \rightarrow \RR$ given by \eqref{G} is $s$-H\"{o}lder continuous with constant
\begin{equation}\label{C_G_multi}
    C_G = C_F\left(1+ \frac{4d\beta \sigma^{-2\beta-2}}{C_{\sigma, \beta,d}}(2m)^{-\beta -\frac{1}{2}}e^{4\sigma M_d m}\right)^s,
\end{equation}
where $C_{\sigma, \beta,d}$ is a constant depending only on $\sigma, \beta,d$.
\end{lemma}


\subsubsection{Gaussian Kernel}
The Gaussian kernel $K(u,v)= e^{-\frac{|u-v|^2}{2\sigma^2}}$ for $u,v \in \mathcal{X}$ is translation-invariant and reproducing, corresponding to  $\phi(x) = e^{-\frac{x^2}{2\sigma^2}}$. 
 Then, by the Mean Value Theorem, we have for $u,v,\Tilde{v} \in \mathcal{X}$, 
\begin{eqnarray*}     
\  | K(u,v) - K(u,\Tilde{v})| &=& \left|e^{-\frac{|u-v|^2}{2\sigma^2}} - e^{-\frac{|u-\Tilde{v}|^2}{2\sigma^2}}\right| \\
&\leq& 1 \cdot \left|\frac{|u-v|^2}{2\sigma^2} - \frac{|u-\Tilde{v}|^2}{2\sigma^2}\right|\\
&\leq&\frac{|u-v|+|u-\Tilde{v}|}{2\sigma^2} (|u-v|-|u-\Tilde{v}|)\\
&\leq& \frac{\sqrt{d}}{\sigma^2}|v-\Tilde{v}|.
\end{eqnarray*}
We see that the Gaussian kernel is $\alpha$-H\"{o}lder continuous for $\alpha = 1$ with constant $C_K = \frac{\sqrt{d}}{\sigma^2}$. The following lemma presents the regularity of function $G$ corresponding to functional $F$ on Gaussian kernel-induced RKHS's. Its proof is relegated to Appendix \ref{sec:appendA}.

\begin{lemma}\label{lemma:C_G_Gaussian}
    Let $d\in \NN, \sigma >0,$ and $\mathcal{X} = [0,1]^d$. Take $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \in \mathcal{X}$ for some $m\in \NN$, and $N=(m+1)^d$.
    Consider $
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}
$ with $\mathcal{H}_K$ induced by a Gaussian kernel. If $F: \mathcal{K}\to \RR$ is  $s$-H\"{o}lder continuous with $s \in (0,1]$ and constant $C_F\geq 0$, the corresponding function $G:[0,1]^N \rightarrow \RR$ given by \eqref{G} is $s$-H\"{o}lder continuous with constant
\begin{equation}\label{C_G_Gaussian}
    C_G = C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}, 
\end{equation}
where $C_{\sigma,d,s}$ is a constant depending only on $\sigma,d,$ and $s$.
\end{lemma}




Now that we have established the regularity of function $G$, we proceed to approximate $G$ using tanh neural networks. 

\subsection{Approximation of Function using Tanh Neural Networks}
\label{subsec:appox}

Here, we would like to apply a recent result in approximating  Lipschitz continuous functions by tanh neural network from \citep{de2021approximation}. 
We will use this existing result to approximate the function $G$ using two-layer tanh neural networks. 

\begin{lemma}[Corollary 5.4 in \citep{de2021approximation}]\label{lemma:tim}
Let $N\in \NN$ and let $f:[0,1]^N \rightarrow \RR$ be a Lipschitz continuous function with Lipschitz constant $L>0$. For every $M\in \NN$ with $M>5N^2$, there exists a tanh neural network $\widehat{f}$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil    
\begin{pmatrix}
        2N-1\\
        N
    \end{pmatrix}M^N$ (or $M-1$ and $6M$ for $N=1$), such that 
    \begin{eqnarray*}
        \left\|\widehat{f}-f\right\|_{L^\infty [0,1]^N} \leq \frac{7N^2L}{M}. 
    \end{eqnarray*}
\end{lemma}
\begin{remark}
It is shown in \citep[Lemma 2.1]{de2021approximation} that $ \begin{pmatrix}
        2N-1\\
        N
    \end{pmatrix}< 5^N$. The total number of parameters in the network is at most $\mathcal{O}(N(5M)^N)$ with $M>5N^2$. 
\end{remark}


We first present an approximation result regarding a general translation-invariant Mercer kernel $K$ and $\mathcal{X}=[0,1]^d$.

\begin{theorem}\label{thm:app_G}
Let $m\in \NN, d\in \NN, \mathcal{X}=[0,1]^d$, and $K$ is a translation-invariant kernel. 
Also let $\Gamma_m =  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi)> 0$.
We take a set of $N=(m+1)^d$ points $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X}$.
Consider the function $G:[-1,1]^N \rightarrow \RR$ defined in (\ref{G}). 
It follows from Lemma \ref{lemma:reg_G} and Lemma \ref{lemma:norminverse} that $G$ is $s$-H\"{o}lder continuous for $0 \leq s \leq 1$ with constant $$C_G= C_F\left(1+ \frac{1}{m\Gamma_m}\sqrt{N} C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha\right)^s.$$
From Lemma \ref{lemma:tim}, for every $M\in \NN$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$, such that 
\begin{eqnarray*}
   \left \|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} \leq \frac{7N^2C_G}{M}. 
\end{eqnarray*}

    %For every $W,L\in \NN^+$, there exists a ReLU neural network $\widehat{G}$ with widths $3^{N+3}\max\{N \lfloor W^{1/N} \rfloor, W+2\}$ and depth $11L+18+2N$, such that 
   % \begin{eqnarray*}
       % \|\widehat{G}-G\|_{L^\infty [-1,1]^N} \leq 131 C_G \sqrt{N}\left(W^2 L^2 \log_3(W+2)\right)^{-\frac{s}{N}}. 
    %\end{eqnarray*}
\end{theorem}
As consequences of the above Theorem \ref{thm:app_G}, we can derive explicit error bounds on the approximation of $G$ functions induced from Reproducing kernel in Sobolev space $W^r_2(\mathcal{X})$ (Corollary \ref{G-Ghat_Sob}), inverse multiquadric kernel (Corollary \ref{G-Ghat_multi}), and Gaussian Kernel (Corollary \ref{G-Ghat_Gaussian}), respectively.  

\begin{corollary}[Reproducing kernel in Sobolev space $W^r_2(\mathcal{X})$]\label{G-Ghat_Sob}
Suppose $K$ is a reproducing kernel in $W^r_2(\mathcal{X})$. 
We take $\mathcal{X}, N,$ and $\Bar{t}$ as in Theorem \ref{thm:app_G}.
Consider the function $G:[-1,1]^N \rightarrow \RR$ defined in (\ref{G}).
We know that the corresponding $G$ function is $s$-Holder with constant $$C_G =  C_F C_{r,d,s} m^{2rs+\frac{ds}{2}-2s} d^{rs+\frac{s}{2}}.$$
For every $M\in \NN$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$ (or $M-1$ and $6M$ for $N=1$), such that 
\begin{eqnarray*}
    \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} =  \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}} \leq \frac{7(m+1)^{2d}}{M}C_F C_{r,d,s} m^{2rs+\frac{ds}{2}-2s} d^{rs+\frac{s}{2}}. 
\end{eqnarray*}
\end{corollary}



\begin{corollary}[Inverse multiquadric kernel] \label{G-Ghat_multi}
Suppose $K$ is an inverse multiquadric kernel given by (\ref{multi}). 
We take $\mathcal{X}, N,$ and $\Bar{t}$ as in Theorem \ref{thm:app_G}.
Consider the function $G:[-1,1]^N \rightarrow \RR$ defined in (\ref{G}). From Lemma \ref{lemma:reg_G} and (\ref{C_G_multi}), the function $G$ is $s$-H\"{o}lder for $0 \leq s \leq 1$ with constant $$C_G = C_FC_{\sigma, \beta,d,s}(2m)^{-s\beta -\frac{s}{2}}e^{4\sigma M_d sm},$$
where $M_d = 12 \left(\frac{\pi \Gamma^2(\frac{d+2}{2})}{9}\right) \leq 6.38d$.  For every $M\in \NN$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$, such that 
\begin{eqnarray*}
    \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} =  \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}} \leq \frac{7(m+1)^{2d}(2m)^{-s\beta-s/2}C_FC_{\sigma, \beta,d,s}e^{4\sigma M_d sm}}{M}. 
\end{eqnarray*}
\end{corollary}


\begin{corollary}[Gaussian kernel]\label{G-Ghat_Gaussian}
Suppose $K$ is a Gaussian kernel given in (\ref{Gaussian}). We take $\mathcal{X}, N,$ and $\Bar{t}$ as in Theorem \ref{thm:app_G}. 
Consider the function $G:[-1,1]^N \rightarrow \RR$ defined in (\ref{G}). 
It follows from Lemma \ref{lemma:reg_G} and (\ref{C_G_Gaussian}) that $G$ is $s$-H\"{o}lder continuous for $0 \leq s \leq 1$ with constant
$$ C_G = C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}.$$
For every $M\in \NN$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \RR$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$, such that 
\begin{eqnarray*}
    \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} =  \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}} 
    \leq \frac{7(m+1)^{2d}}{M} C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}. 
\end{eqnarray*}
\end{corollary}



%\section{Discussions and Related Work}\label{sec:discuss}


\section{Remarks and Conclusions}\label{sec:conclusion}
%In summary, we establish universality on the approximation of RKHS functionals induced by a general Mercer kernel using tanh neural networks. 
While it is widely acknowledged that neural networks are universal function approximators, whether such universality can be extended to infinite-dimensional spaces remains unknown. This paper investigates the approximation theory of functionals, which are mappings from a space of functions to $\RR$.

In summary, we establish explicit error bounds, with respect to the number of network parameters, on the approximations of functionals on Sobolev space and RKHS's induced by the inverse multiquadric kernel and Gaussian kernel, respectively. 
We adopt standard neural networks equipped with tanh activation functions, commonly used in operator learning these days \citep{lu2021learning, wang2021learning}. 
Our main theorems suggest that such networks can achieve any arbitrary approximation accuracy if there are sufficiently many network parameters. 
We achieve near polynomial approximation rates for approximating RKHS functionals induced by the inverse multiquadric and Gaussian kernels. 
Our results have shown improvement compared to the previous rates given in \citep{mhaskar1997neural} and \citep{song2023approximationarxiv}.

%Moreover, we present a general error bound on approximating functionals on the RKHS induced some general Mercer kernel. 
Most existing works \citep{rossi2005functional, song2023approximationarxiv} require a set of handcrafted basis functions to convert functional inputs to vectors via basis function expansion; our design evaluates the input function at some discrete points in the domain where uniform spacing is not necessary. 
Our method is much simpler than those in the literature. 

Moreover, we present a general error bound on approximating functionals on the RKHS induced by some general Mercer kernel. This error bound exhibits a trade-off associated with $N$, which denotes the number of points we use to evaluate our input function. 
When $N$ is too small, we observe an increase in the power function (measures the distance between a function and its interpolant), while with a very large $N$, the network size becomes too big and results in overfitting. 
We shall choose an optimal $N$ to strike a balance.
In the previously mentioned theorems, we present our choice of $N$;  its derivation is given in the proofs. 
Furthermore, we apply our findings to functional regression, proving that tanh neural networks can accurately approximate the regression maps in generalized functional linear models. 
This novel result provides an insight that deep learning may be applied successfully in functional data analysis. 

To our knowledge, this work is the first to study the approximation of functionals on RKHS's. All existing works in this regard focus on functionals on $L^p$ spaces. 
We also want to highlight that no method other than neural networks, such as spline or sieve, has been studied for approximating functionals. 
By considering RKHS, 
our methods allow extensions to low-dimensional manifolds embedded in Euclidean spaces. 
This is beyond the scope of this paper with $\mathcal{X} = [0,1]^d$ and will be discussed in our further study.

\section{Acknowledgments and Disclosure of Funding}
This material is based upon work supported by the National Science Foundation under grant no. 2229876 is partly supported by funds provided by the National Science Foundation, the Department of Homeland Security, and IBM.
Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or its federal agency and industry partners.
Tian-Yi Zhou and Xiaoming Huo are also partially sponsored by NSF grants DMS 2015363 and the A. Russell Chandler III Professorship at Georgia Institute of Technology. Guang Cheng and Namjoon Suh are partially supported by the Office of Naval Research under ONR N00014-22-1-2680 and the National Science Foundation under SCALE MoDL 2134209. 
Namjoon Suh is also partially supported by the Institute of Digital Research \& Education (IDRE) Postdoctoral Fellowship at UCLA. 


\begin{appendices}
\section{Proof of Statements in Remarks} \label{sec:append_remark}
Here, we present the technical proofs of the statements given in Remark \ref{remark:rate_thm3} and Remark \ref{remark:rate_thm1}, respectively.


\begin{proof}[Proof of Remark \ref{remark:rate_thm3}]
Recall that in Theorem \ref{thm:Sobolev}, we established an approximation error bound of  $\mathcal{O}\left(M^{-\frac{2r-d}{2(2r-1)}}\right)$, where $r\in \RR_+, d\in \NN$, and $r-d/2 >1$. The maximum total number of network parameters is $\mathcal{N} = \mathcal{O}\left(N(5M)^N\right)$, where $N=(m+1)^d = \left(\left\lceil M^{\frac{1}{2s(2r-1)}}\right\rceil +1 \right)^d$. We will prove that the convergence rate of the approximation error achieves a logarithmic rate (in terms of $\mathcal{N}$).

Notice that
\begin{eqnarray*}
    \log (\mathcal{N}) \leq  C^\prime N \log (M) = C^\prime(m+1)^d \log (M) =  C^\prime\left(\left\lceil M^{\frac{1}{2s(2r-1)}}\right\rceil+1\right)^d \log (M),
\end{eqnarray*}
where $C^\prime\leq 4 \log (C) \log (5)$.
We thus have 
\begin{eqnarray*}
    \log (\mathcal{N}) \leq C^\prime\left(M^{\frac{1}{2s(2r-1)}}+2\right)^d \log (M) \leq C^\prime 3^d M^{\frac{d}{2s(2r-1)}}\log (M).
\end{eqnarray*}

We also know that $\mathcal{N}$ is at least $m$ and thus
\begin{eqnarray*}
    \log (\mathcal{N}) \geq C^{\prime\prime} M^{\frac{d}{2s(2r-1)}}
\end{eqnarray*}
and 
\begin{eqnarray*}
    \log(\log(\mathcal{N})) \geq \log (C^{\prime\prime})+  \frac{d}{2s(2r-1)}\log(M),
\end{eqnarray*}
where $C^{\prime\prime}$ is a positive constant. Then, we get
\begin{eqnarray*}
    \frac{\log (\mathcal{N})}{\log(\log(\mathcal{N}))} 
    \leq \frac{C^\prime 3^d M^{\frac{d}{2s(2r-1)}}\log (M)}{ \log (C^{\prime\prime})+  \frac{d}{2s(2r-1)}\log(M)} 
    \leq \frac{C^\prime 3^d M^{\frac{d}{2s(2r-1)}}}{  \frac{d}{2s(2r-1)}} = C^\prime3^d \frac{2s(2r-1)}{d}M^{\frac{d}{2s(2r-1)}}
\end{eqnarray*} and thus
\begin{eqnarray*}
    M \geq \frac{d}{C^\prime3^d2s(2r-1)}\left(\frac{\log (\mathcal{N})}{\log(\log(\mathcal{N}))}\right)^{\frac{2s(2r-1)}{d}}.
\end{eqnarray*}

Finally, we obtain
\begin{eqnarray*}
    M^{-\frac{2r-d}{2(2r-1)}} \leq C_{d,r,s}\left(\frac{\log (\mathcal{N})}{\log(\log(\mathcal{N}))}\right)^{-\frac{s(2r-d)}{d}}, 
\end{eqnarray*}
where $C_{d,r,s}= \left(\frac{d}{C^\prime3^d2s(2r-1)}\right)^{-\frac{2r-d}{2(2r-1)}}$. The proof is complete.
\end{proof}

Next, we present the proof of the statement in Remark \ref{remark:rate_thm1}.

\begin{proof}[Proof of Remark \ref{remark:rate_thm1}]
Recall in Theorem \ref{thm:multi}, we establish an approximation error bound of order $$\mathcal{O}\left((\log (M))^{\max\{0, 2d-s\beta\}}\left(\frac{1}{M}\right)^{\frac{c}{4M_d \sqrt{d}\sigma+c}}\right),$$ where $d \in \NN, \sigma, \beta >0, s \in (0,1], M_d \leq 6.38d$, and $c>0$ is a positive constant given in (\ref{f-Pf_multi}). The maximum number of network parameters is $\mathcal{N} = \mathcal{O}\left(N(5M)^N\right)$, where $N = (m+1)^d = \left(\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil +1\right)^d$.

We first prove that the convergence rate of the approximation error is faster than the logarithmic rate w.r.t. $\mathcal{N}$. 
We know $\mathcal{N} \leq C N (5M)^N$ for some positive constant $C$. We then have  
\begin{eqnarray}\label{logN}
    \log (\mathcal{N}) \leq C^\prime N \log (M) &=& C^\prime(m+1)^d  \log (M) \nonumber\\    &=&C^\prime\left(\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil + 1 \right)^d \log (M) 
    = C_{\sigma, s, d} (\log (M))^{d+1},
\end{eqnarray}
where $C^\prime \leq 4 \log (C) \log (5)$, $C_{\sigma, s, d}\leq C^\prime\left(\frac{1}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}+2\right )^d$.

We thus have \begin{equation}\label{logM}
    \log (M) \geq \left(\frac{\log (\mathcal{N})}{C_{\sigma, s, d}}\right)^{\frac{1}{d+1}}.
\end{equation}
 We can express $\left(\frac{1}{M}\right)^{\frac{c}{4M_d \sqrt{d}\sigma+c}}$ in terms of $\mathcal{N}$:
\begin{eqnarray*}
    M^{-\frac{c}{4M_d \sqrt{d}\sigma+c}} = \exp\left(-\frac{c}{4M_d \sqrt{d}\sigma+c} \log (M)\right) &\leq& \exp\left(-\frac{c}{4M_d \sqrt{d}\sigma+c} \left(\frac{\log (\mathcal{N})}{C_{\sigma, s, d}}\right)^{\frac{1}{d+1}} \right) \\
    &=& B^{(\log (\mathcal{N}))^{\frac{1}{d+1}}},
\end{eqnarray*}
where $B = \exp\left(-\frac{c}{4M_d \sqrt{d}\sigma+c} \left(\frac{1}{C_{\sigma, s, d}}\right)^{\frac{1}{d+1}} \right) \in (0,1)$.

Note that $\mathcal{N}$ must be at least $M$, implying that $\log(\mathcal{N}) \geq \log (M)$. 
Using this fact, we get
\begin{eqnarray*}
   && (\log (M))^{\max\{0, 2d-s\beta\}} M^{-\frac{c}{4M_d \sqrt{d}\sigma+c}} \\
    &\leq& (\log (\mathcal{N}))^{\max\{0, 2d-s\beta\}}B^{(\log (\mathcal{N}))^{\frac{1}{d+1}}} \\
    &=& (\log (\mathcal{N}))^{\frac{1}{d+1}\max\{0, 2d-s\beta\}(d+1)}B^{(\log (\mathcal{N}))^{\frac{1}{d+1}}}\\
    &\stackrel{\text{let $u= (\log (\mathcal{N}))^{\frac{1}{d+1}}$}}{=}& u ^{\max\{0, 2d-s\beta\}(d+1)}B^u.
\end{eqnarray*}
As $u = (\log (\mathcal{N}))^{\frac{1}{d+1}}$ increases, $u ^{\max\{0, 2d-s\beta\}(d+1)}$ converges to infinity polynomially w.r.t. $u$, while $B^u$ converges to $0$ exponentially w.r.t. $u$. Thus, the term $(\log (M))^{\max\{0, 2d-s\beta\}} M^{-\frac{c}{4M_d \sqrt{d}\sigma+c}}$ converges to $0$.

Since $u ^{\max\{0, 2d-s\beta\}(d+1)}B^u = o (u^{-\ell})$ for any $\ell \in \NN$, we have
\begin{eqnarray*}
    (\log (M))^{\max\{0, 2d-s\beta\}} M^{-\frac{c}{4M_d \sqrt{d}\sigma+c}}= o \left((\log (\mathcal{N}))^{-\frac{\ell}{d+1}}\right) = o\left((\log (\mathcal{N}))^{-k}\right), \qquad \forall k>0,
\end{eqnarray*}
because for any $k>0$, there always exists some $\ell \in \NN$ such that $\frac{\ell}{d+1} \geq k$.

Since  $(\log (M))^{\max\{0, 2d-s\beta\}}\left(\frac{1}{M}\right)^{\frac{c}{4M_d \sqrt{d}\sigma+c}} =  o\left((\log (\mathcal{N}))^{-k}\right)$ for any $k>0$, we can see that the above theorem gives a convergence rate w.r.t. $\mathcal{N}$ faster than the logarithmic rate. 

At the same time, we can show that the convergence rate we obtained is slower than the polynomial rate (i.e., a rate of the form $\mathcal{O}(\mathcal{N}^{-\varepsilon})$ for $\varepsilon>0$). From (\ref{logN}) and (\ref{logM}), we can give $(\log (M))^{\max\{0, 2d-s\beta\}}\left(\frac{1}{M}\right)^{\frac{c}{4M_d \sqrt{d}\sigma+c}}$ a lower bound as follows:
\begin{eqnarray*}
    (\log (M))^{\max\{0, 2d-s\beta\}}M^{-\frac{c}{4M_d \sqrt{d}\sigma+c}}
    &\overset{(\ref{logM})}{\geq}& \left(\frac{\log (\mathcal{N})}{C_{\sigma, s, d}}\right)^{\frac{1}{d+1}{\max\{0, 2d-s\beta\}}}M^{(m+1)^d \left(-\frac{c}{4M_d \sqrt{d}\sigma+c}\cdot \frac{1}{(m+1)^d}\right)} \\
    &\geq& \left(\frac{\log (\mathcal{N})}{C_{\sigma, s, d}}\right)^{\frac{1}{d+1}{\max\{0, 2d-s\beta\}}}\mathcal{N}^{-\frac{c}{4M_d \sqrt{d}\sigma+c} \cdot \frac{1}{(m+1)^d}}\\
    &\overset{(\ref{logN})}{\geq}&\left(\frac{\log (\mathcal{N})}{C_{\sigma, s, d}}\right)^{\frac{1}{d+1}{\max\{0, 2d-s\beta\}}}\mathcal{N}^{-\frac{c}{4M_d \sqrt{d}\sigma+c} \cdot \frac{\log (M)}{\log(\mathcal{N})}}\\
    &\overset{(\ref{logM})}{\geq}& \left(\frac{\log (\mathcal{N})}{C_{\sigma, s, d}}\right)^{\frac{1}{d+1}{\max\{0, 2d-s\beta\}}}\mathcal{N}^{-\frac{c}{4M_d \sqrt{d}\sigma+c} \cdot (\log (\mathcal{N}))^{-d/(d+1)}\cdot (C_{\sigma, s, d})^{-1/(d+1)}},
\end{eqnarray*}
where $C_{\sigma, s, d}\leq \left(\frac{1}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}+2\right )^d$. 
%Here, notice that as $\mathcal{N}$ gets larger, $(\log (\mathcal{N}))^{-d/(d+1)}$ gets smaller. 
Thus, as $\mathcal{N}$ grows,
$(\log (M))^{\max\{0, 2d-s\beta\}}\left(\frac{1}{M}\right)^{\frac{c}{4M_d \sqrt{d}\sigma+c}}$ always decays slower than $\mathcal{N}^{-\varepsilon}$ for some $\varepsilon > 0$. The proof is complete. 
\end{proof}



\section{Proof of Supporting Lemmas}\label{sec:appendA}
Here, we present the proofs of supporting lemmas. We begin with the proof of Lemma \ref{lemma:trans}.

\begin{proof}[Proof of Lemma \ref{lemma:trans}]

   The proof is relatively straightforward. 
   Suppose $K$ is a translation-invariant kernel. For any $c,u\in \RR^d$, 
\begin{eqnarray*}
    \sum_{k,j=1}^d c_k c_j K(u_k,u_j) = \sum_{k,j=1}^d c_kc_j \phi(u_k-u_j) 
    &\overset{(\ref{eq:inversefourier})}{=}&  \sum_{k,j=1}^d c_k c_j \int_{\RR^d} \widehat{\phi}(\xi) e^{2\pi i\xi \cdot (u_k-u_j)} d\xi \\
    &=&  \int_{\RR^d} \widehat{\phi}(\xi)\sum_{k=1}^d c_k e^{2\pi i\xi \cdot u_k} \sum_{j=1}^d c_j  e^{-2\pi i\xi \cdot u_j} d\xi\\
     &=&  \int_{\RR^d} \widehat{\phi}(\xi)\left|\sum_{k=1}^d c_k e^{2\pi i\xi \cdot u_k}\right|^2 d\xi.
\end{eqnarray*}
Thus, if $\widehat{\phi}$ is non-negative, then  $\sum_{k,j=1}^d c_k c_j K(u_k,u_j)\geq 0$ and thus $K$ is positive semi-definite. 
\end{proof}




Next, let us present the proof of Lemma \ref{f-Pf}.
\begin{proof}[Proof of Lemma \ref{f-Pf}]
    With $\{\psi_i\}_{i=1}^N$ defined in (\ref{psi}), for  $x\in \mathcal{X}$, by the reproducing property and the Cauchy–Schwarz inequality, we have 
    \begin{eqnarray*}
        |f(x) - Pf(x)| = \left|f(x)-\sum_{i=1}^N f(t_i)\psi_i(x)\right| &=& \left|\langle f, K_x \rangle_{\mathcal{H}_K} -\sum_{i=1}^N \langle f, K_{t_i}\rangle_{\mathcal{H}_K} \psi_i(x) \right|\\
        &=& \left|\langle f, K_x -  \sum_{i=1}^N \psi_i(x)  K_{t_i} \rangle_{\mathcal{H}_K}\right|\\
        &\leq&  \|f\|_{\mathcal{H}_K} \left\|K_x -  \sum_{i=1}^N \psi_i(x)  K_{t_i}\right\|_{\mathcal{H}_K}.
    \end{eqnarray*}
    For $c\in \RR^N$, let the function $H$ be $$H(c) : = \left\|K_x -  \sum_{i=1}^N c_i  K_{t_i}\right\|^2_{\mathcal{H}_K} = K(x,x) -2  \sum_{i=1}^N c_i K(t_i,x)+ \sum_{i,j=1}^N c_i c_j K(t_i, t_j).$$ 
    Then \begin{eqnarray*}
        \frac{\partial H}{\partial c} = 0 \iff 2 K[\Bar{t}] c = 2 \begin{bmatrix}
            K(x, t_1)\\
            \vdots\\
            K(x,t_N)
        \end{bmatrix} \iff c = (K[\Bar{t}])^{-1 } \begin{bmatrix}
            K(x, t_1)\\
            \vdots\\
            K(x,t_N)
        \end{bmatrix} = \begin{bmatrix}
           \psi_1(x)\\
           \vdots\\
           \psi_N(x)
       \end{bmatrix}.
    \end{eqnarray*}
    Thus, $\|K_x -  \sum_{i=1}^N \psi_i(x)  K_{t_i}\|_{\mathcal{H}_K} = \min_{c\in \RR^N} \sqrt{H(c)} \leq \max_{x\in \mathcal{X}} \min_{c\in\RR^N}  \sqrt{H(c)} =  \epsilon_K(\Bar{t})$. This is valid for every $x \in \mathcal{X}$. The proof is complete.
\end{proof}  

Next, we give the proof of Lemma \ref{lemma:power}.
\begin{proof}[Proof of Lemma \ref{lemma:power}]
Recall from Lemma \ref{f-Pf}: 
\begin{equation*}
    |f(x) - Pf(x)|  \leq \|f\|_{\mathcal{H}_K} \cdot \min_{c\in\RR^N}\left\|K_x - \sum_{i=1}^N c_i K_{t_i}\right\|_{\mathcal{H}_K}, \qquad f\in \mathcal{H}_K, x\in \mathcal{X}, \Bar{t} \subset \mathcal{X}.
\end{equation*}

From (\ref{fill_tbar}), $h_{\Bar{t}} = \sup_{x\in \mathcal{X}} d_\mathcal{X}(x,t_{j_x})\leq  \frac{\sqrt{d}}{m}$, where $t_{j_x}$ is the closest point in $\Bar{t}$ to  any $x\in \mathcal{X}$. Choose $c$ such that 
\begin{equation}
    c_i =  \begin{cases}
      1, & \text{if $i=j_x$ },\\
      0, & \text{otherwise}.
    \end{cases}  
\end{equation}
Then, by the $\alpha$-H\"{o}lder regularity of $K$  and symmetry, we get 
\begin{eqnarray*}
    K(x,x) -2 \sum_{i=1}^N c_i K(x,t_i)+ \sum_{i=1}^N \sum_{j=1}^N c_i  c_j K(t_i, t_j)
    &= & K(x,x) -2  K(x,t_{j_x})+ K(t_{j_x}, t_{j_x})\\
    &\leq& 2C_K (d_\mathcal{X}(x,t_{j_x}))^\alpha \leq 2C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha,
\end{eqnarray*}
and thus 
    $\epsilon_K(\Bar{t}) \leq 2C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha$.
The Lemma follows after Lemma \ref{f-Pf}.
\end{proof}

Next, we present the proof of Lemma \ref{lemma:reg_G}. 
\begin{proof}[Proof of Lemma \ref{lemma:reg_G}]
For any $c, \Tilde{c} \in \RR^N$, we have 
\begin{equation}
   | G(c) - G(\Tilde{c})| = \left|F\left(\sum_{i=1}^N c_i \psi_i\right) - F\left(\sum_{i=1}^N \Tilde{c_i} \psi_i\right)\right| \leq C_F \left\|\sum_{i=1}^N (c_i - \Tilde{c_i}) \psi_i\right\|^s_\infty.
\end{equation}
We will proceed to bound $\|\sum_{i=1}^N a_i\psi_i\|_\infty$ for any $a\in \RR^N$. 
For $x\in \mathcal{X}$, let $t_{j_x}$ be the closest point in $\bar{t}$ to $x$. Observe that $\left(\sum_{i=1}^N a_i\psi_i\right)(t_{j_x}) = a_{j_x}$ and thus 
\begin{equation*}
    \left|\sum_{i=1}^N a_i\psi_i(x)\right| = \left|\sum_{i=1}^N a_i\psi_i(t_{j_x}) + \sum_{i=1}^N a_i (\psi_i(x) - \psi_i(t_{j_x}))\right| = \left|a_{j_x} + \sum_{i=1}^N a_i (\psi_i(x) - \psi_i(t_{j_x}))\right|.
\end{equation*}
Next, we have \begin{eqnarray*}
    \left|\sum_{i=1}^N a_i (\psi_i(x) - \psi_i(t_{j_x}))\right| &=& \left|a^T (K[\Bar{t}])^{-1 } \begin{bmatrix}
        K(t_1, x) - K(t_1, t_{j_x})\\
        \vdots\\
        K(t_N, x) - K(t_N, t_{j_x})
    \end{bmatrix}\right|\\
    &=& \left|((K[\Bar{t}])^{-1 }a)^T \begin{bmatrix}
        K(t_1, x) - K(t_1, t_{j_x})\\
        \vdots\\
        K(t_N, x) - K(t_N, t_{j_x})
    \end{bmatrix}\right|\\
    &\leq& \|(K[\Bar{t}])^{-1 }\|_{op} \|a\| \sqrt{\sum_{i=1}^N | K(t_i, x) - K(t_i, t_{j_x})|^2}.
    \end{eqnarray*}  
    Since $K$ is $\alpha$-H\"older continuous, we know that  
      $| K(t_i, x) - K(t_i, t_{j_x})| \leq  C_K\left(d_\mathcal{X}(x,t_{j_x})\right)^\alpha. $ Hence, we know 
 \begin{eqnarray*}
    \left|\sum_{i=1}^N a_i (\psi_i(x) - \psi_i(t_{j_x}))\right| 
     &\leq& \|(K[\Bar{t}])^{-1 }\|_{op} \|a\| \sqrt{N} C_K\left(\sup_{x\in \mathcal{X}}d_\mathcal{X}(x,t_{j_x})\right)^\alpha. 
    \end{eqnarray*}     
          Thus, we have
\begin{eqnarray*}
    | G(c) - G(\Tilde{c})| 
    &\leq& C_F \left\{\left|c_{j_x} - \Tilde{c}_{j_x}\right| + \|(K[\Bar{t}])^{-1 }\|_{op} \|c - \Tilde{c}\|\sqrt{N} C_K\left(\sup_{x\in \mathcal{X}} d_\mathcal{X}(x,t_{j_x})\right)^\alpha\right\}^s\\
    &\leq& C_F \|c - \Tilde{c}\|^s \left\{1 + \|(K[\Bar{t}])^{-1 }\|_{op} \sqrt{N} C_K\left(\sup_{x\in \mathcal{X}} d_\mathcal{X}(x,t_{j_x})\right)^\alpha\right\}^s.
\end{eqnarray*}
The proof is complete. 
\end{proof}

Next, we give the proof of Lemma \ref{lemma:norminverse}.

\begin{proof}[Proof of Lemma \ref{lemma:norminverse}]
From Lemma \ref{lemma:trans}, we know that the Fourier transform of a translation-invariant and reproducing kernel $K$ is real-valued and non-negative. 
We also know that, from (\ref{operatornorm}) and (\ref{lambda_N}), to estimate  $\|(K[\Bar{t}])^{-1}\|_{op}$ for a translation-invariant $K$ over $\RR^d$, it suffices to estimate the smallest eigenvalue of $K[\Bar{t}]$: $\lambda_{N}
= \sum_{k=1}^N \sum_{\ell=1}^N v_k v_\ell K(t_k, t_\ell)$, where $v\in \RR^N$ is an eigenvector of $K[\Bar{t}]$ of norm $1$. 
    
For any $\xi \in \RR^d$, we have 
\begin{eqnarray*}
    \lambda_{N}
    = \sum_{k=1}^N \sum_{\ell=1}^N v_k v_\ell K(t_k, t_\ell) &=&  \int_{\RR^d} \widehat{\phi}(\xi)\left|\sum_{k=1}^N v_k e^{2\pi i\xi \cdot t_k}\right|^2 d\xi
   \  \geq \ \int_{[-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi)\left|\sum_{k=1}^N v_k e^{2\pi i\xi \cdot t_k}\right|^2 d\xi.
\end{eqnarray*}
Let $\Gamma_m = \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi)$. 
%(we expect  $\Gamma_m \rightarrow 0$ as $m \rightarrow \infty$) 
Then, we have 
\begin{eqnarray}\label{Gamma_m}
     \lambda_{N}  \geq \Gamma_m\int_{[-\frac{m}{2},\frac{m}{2}]^d} \left|\sum_{k=1}^N v_k e^{2\pi i\xi \cdot t_k}\right|^2 d\xi \nonumber 
     &\geq& \Gamma_m\int_{[-\frac{1}{2},\frac{1}{2}]^d} \left|\sum_{k\in \{0,1,\ldots,m\}^d} v_k e^{2\pi i\xi \cdot k}\right|^2 md\xi \nonumber\\
      &=& m \Gamma_m \|v\|^2 \nonumber\\ 
      &=& m\Gamma_m.
\end{eqnarray}
Here, we have used the fact that $\{e^{2\pi i\xi \cdot k}\}_{k\in \ZZ^d}$ is an orthonormal basis of $L_2 [(-1/2, 1/2)^d]$.
\end{proof}

Next, we present the proof of Lemma \ref{lemma:C_G_Sob}.

\begin{proof}[Proof of Lemma \ref{lemma:C_G_Sob}]
We know that the Sobolev space $W^r_2(\mathcal{X})$ with $r>d/2$ is an RKHS induced by a translation-invariant kernel, whose Fourier transform is $\widehat \phi (\xi) = (1+|\xi|^2)^{-r}$, for $ \xi \in \RR^d$. From \eqref{Gamma_m_Sob}, we know  $\Gamma_m =  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi) \geq \left(1+ \frac{dm^2}{4}\right)^{-r}$
for this function $\widehat \phi$. 
According to Lemma  \ref{lemma:norminverse},  
\begin{equation*}
    \|(K[\Bar{t}])^{-1}\|_{op} \leq  \frac{1}{m\Gamma_m} \leq \frac{1}{m}\left(1+ \frac{dm^2}{4}\right)^{r}. 
\end{equation*}

For the Sobolev Space $W^r_2(\mathcal{X})$ and $\mathcal{X}= [0,1]^d$, it follows from Lemma \ref{lemma:reg_G} the associated $G$ function  $G(c) = F\left(\sum_{i=1}^N c_i \psi_i\right)$ is $s$-H\"{o}lder continuous $s \in (0,1]$ with constant 
\begin{eqnarray*}
    & & C_F\left(1+ \|(K[\Bar{t}])^{-1 }\|_{op}\sqrt{N} C_K \left(h_{\Bar{t}}\right)^\alpha\right)^s\\
    &\leq& C_F\left(1+ \frac{1}{m}\left(1+ \frac{dm^2}{4}\right)^{r}(m+1)^{\frac{d}{2}}  C_{r,d} \left(\frac{\sqrt{d}}{m}\right)^{\min\{1,r-d/2\}}\right)^s\\
     &\leq& C_F\left(1+ \frac{1}{m}\left(\frac{d^rm^{2r}}{2^r}\right)(m+1)^{\frac{d}{2}}  C_{r,d} \left(\frac{\sqrt{d}}{m}\right)\right)^s\\
     &\leq& C_F\left(1+ C_{r,d}\frac{2^{\frac{d}{2}}}{2^r} m^{2r+\frac{d}{2}-2} d^{r+\frac{1}{2}} \right)^s. 
\end{eqnarray*}
Thus, $G$ is  $s$-H\"{o}lder continuous with constant
\begin{equation*}
    C_G =  C_F C_{r,d,s} m^{2rs+\frac{ds}{2}-2s} d^{rs+\frac{s}{2}}, 
\end{equation*}
where $C_{r,d,s}$ is a positive constant depends only on $r, d$, and $s$.
\end{proof}


Next, we give the proof of Lemma \ref{lemma: C_G_multi}.
\begin{proof}[Proof of Lemma \ref{lemma: C_G_multi}]
For an inverse multiquadric kernel $K$, a lower bound of $\lambda_N$  (i.e., the smallest eigenvalue of the Gram matrix $K[\Bar{t}]$)  is given in \citep[Lemma 8.2]{fasshauer2005meshfree} in terms of the separation distance $q_{\bar{t}}=\frac{1}{2} \min_{u \neq v \in\bar{t}} |u-v|$ as
\begin{equation}
    \lambda_N \geq C_{\sigma, \beta,d}q_{\Bar{t}}^{-\beta +\frac{1-d}{2}}e^{-\frac{2\sigma M_d}{q_{\Bar{t}}}},
\end{equation}
where $M_d = 12 \left(\frac{\pi \Gamma^2(\frac{d+2}{2})}{9}\right) \leq 6.38d$, and $C_{\sigma, \beta,d}$ is another explicitly known constant depending on $\sigma, \beta,$ and $d$.

With our choice of $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$, we have $q_{\Bar{t}}=\frac{1}{2m}.$
Then, we know
\begin{equation*}
    \lambda_N \geq C_{\sigma, \beta,d}(2m)^{\beta +\frac{d-1}{2}}e^{-4\sigma M_d m},
\end{equation*}
and it follows from Lemma \ref{lemma:norminverse} that 
\begin{equation}
    \|(K[\Bar{t}])^{-1}\|_{op} = \frac{1}{\lambda_{N}} \leq \frac{1}{mC_{\sigma, \beta,d}} (2m)^{-\beta -\frac{d-1}{2}}e^{4\sigma M_d m}.
\end{equation}
For the inverse multiquadric kernel $K$, it follows from Lemma \ref{lemma:reg_G} that the associated $G$ function  $G(c) = F\left(\sum_{i=1}^N c_i \psi_i\right)$ is $s$-H\"{o}lder continuous with constant 
$$C_F\left(1+ 2d\beta \sigma^{-2\beta-2}\frac{1}{C_{\sigma, \beta,d}}(2m)^{-\beta -\frac{d-1}{2}}e^{4\sigma M_d m}\frac{(m+1)^{\frac{d}{2}}}{m}\right)^s.$$ 
Thus, $G$ is  $s$-H\"{o}lder continuous with constant
$C_G = C_F\left(1+ \frac{4d\beta \sigma^{-2\beta-2}}{C_{\sigma, \beta,d}}(2m)^{-\beta -\frac{1}{2}}e^{4\sigma M_d m}\right)^s$.
The proof is complete. 
\end{proof}

Next, we give the proof of Lemma \ref{lemma:C_G_Gaussian}. 
\begin{proof}[Proof of Lemma \ref{lemma:C_G_Gaussian}]
The Fourier transform of Gaussian kernel is $\widehat{\phi}(\xi) = (2\sigma^2\pi)^{\frac{d}{2}}e^{-2\sigma^2\pi^2 |\xi|^2}> 0$ for all $\xi \in \RR^d$.  For $\xi \in [-\frac{m}{2},\frac{m}{2}]^d$, we have $|\xi|^2 \leq d\left(\frac{m}{2}\right)^2$. 
Then, we have 
\begin{equation*}
    \Gamma_m =  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi) 
    \geq (2\sigma^2\pi)^\frac{d}{2}e^{-2\sigma^2\pi^2 d\left(\frac{m}{2}\right)^2} = (2\sigma^2\pi)^\frac{d}{2}e^{-\sigma^2\pi^2 dm^2/2}.
\end{equation*}
It follows from Lemma \ref{lemma:norminverse} that
\begin{equation*}
    \|(K[\Bar{t}])^{-1}\|_{op} \leq \frac{1}{m}(2\sigma^2\pi)^{-\frac{d}{2}}e^{\sigma^2\pi^2 dm^2/2}.
\end{equation*}

For Gaussian kernel $K$, it follows from Lemma \ref{lemma:reg_G} that the associated $G$ function  $G(c) = F\left(\sum_{i=1}^N c_i \psi_i\right)$ is $s$-H\"{o}lder continuous for $0 < s \leq 1$ with constant 
\begin{equation*}
   C_F\left(1+(2\sigma^2\pi)^{-\frac{d}{2}}e^{\sigma^2\pi^2 dm^2/2}\frac{d(m+1)^{\frac{d}{2}}}{m\sigma^2}\right)^s. 
\end{equation*}
Because $\frac{(m+1)^{\frac{d}{2}}}{m} = \mathcal{O}(e^{\sigma^2\pi^2 dm^2/2})$, we know $G$ is $s$-H\"{o}lder continuous with constant
\begin{equation}
    C_G = C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}, 
\end{equation}
where $C_{\sigma,d,s}$ is a constant depending only on $\sigma,d,$ and $s$.
\end{proof}




\section{Proof of Main Results}\label{sec:appendB}

We are in a position to prove our main results. 
We first present the proof of Theorem \ref{thm:Sobolev} below. 


\begin{proof}[Proof of Theorem \ref{thm:Sobolev}]
    Recall that from the Sobolev Embedding Theorem, we know that the reproducing kernel $K$ in $W^r_2(\mathcal{X})$ is $\alpha$-H\"{o}lder continuous with $\alpha = r-d/2$ and the constant $C_K = C_{r,d}>0$ depends only on $r$ and $d$. In our analysis, we require  $\alpha \leq 1$, which is why we assume  $\alpha \leq 1$ throughout the paper. Thus, we assume that $\alpha =\min\{1,r-d/2\}$.
    
    We consider the error decomposition in (\ref{decomposition}), that is,
    \begin{eqnarray*}
         \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right| &\leq  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|F(Pf) - \widehat{G}(f(\Bar{t}))| \\
        &=  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|G(f(\Bar{t})) - \widehat{G}(f(\Bar{t}))|.
    \end{eqnarray*}
Here, we have used $G(f(\Bar{t})) = F\left(\sum_{i=1}^N f(t_i) \psi_i\right) = F(Pf)$, where the function $G$ is defined previously in equation (\ref{G}). 


    With Corollary \ref{G-Ghat_Sob}, we get 
   \begin{eqnarray*}
       \sup_{f\in \mathcal{K}}\left|G(f(\Bar{t})) - \widehat{G}(f(\Bar{t}))\right| \leq \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}}&\leq&\frac{C_FC_{r,d,s}}{M}m^{2rs+\frac{ds}{2}+2d-2s} d^{rs+\frac{s}{2}}\\
       &\leq&\frac{C_FC_{r,d,s}}{M}m^{s(2r+d-2)} d^{s(r+\frac{1}{2})}. 
   \end{eqnarray*}

   According to \citep{schaback1997reconstruction}, when $r>d/2$ and $r-d/2 \notin \NN$, an upper bound of the power function is derived by Sobolev Embedding Theorem as 
   \begin{equation*}
     \epsilon_K(\Bar{t})  \leq C^\prime_{r,d}d^{r-\frac{d}{2}}m^{d-2r},
   \end{equation*}
   where $C^\prime_{r,d}$ is some positive constant depends on $r$ and $d$. Thus, we have 
   \begin{equation*}
      \|f-Pf\|_\infty \leq \epsilon_K(\Bar{t})\|f\|_{\mathcal{H}_K} \leq C^\prime_{r,d}d^{r-\frac{d}{2}}m^{d-2r}
   \end{equation*} 
   and 
   \begin{equation*}
      \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| \leq  C_F \|f-Pf\|^s_\infty \leq  C_F  (C^\prime_{r,d})^s d^{s(r-\frac{d}{2})}m^{s(d-2r)}. 
   \end{equation*}
   Now combining the two error bounds together, we get 
   \begin{eqnarray*}      \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right| &\leq& C_F (C^\prime_{r,d})^s d^{s(r-\frac{d}{2})}m^{s(d-2r)} + \frac{C_FC_{r,d,s}}{M}m^{s(2r+d-2)} d^{s(r+\frac{1}{2})}\\
       &\leq& C_F \max\{(C^\prime_{r,d})^s, C_{r,d,s} \}\left(d^{s(r-\frac{d}{2})}m^{s(d-2r)} + \frac{1}{M}m^{s(2r+d-2)} d^{s(r+\frac{1}{2})}\right).     
   \end{eqnarray*}
    We take $m=\left\lceil M^{\frac{1}{s(4r-2)}}\right\rceil \in \NN$ such that  $m^{s(d-2r)} \approx \frac{1}{M} m^{s(2r+d-2)}$.
  Then,
   \begin{equation*}
       m^{s(d-2r)} \leq M^{\frac{s(d-2r)}{s(4r-2)}} =  M^{-\frac{2r-d}{2(2r-1)}}.
   \end{equation*}
   We know $$\frac{1}{M} m^{s(2r+d-2)} \leq  2 ^{s(2r+d-2)}M^{-\frac{2r-d}{2(2r-1)}}.$$
   We also know that $d^{s(r-\frac{d}{2})} \leq d^{s(r+\frac{1}{2})}$. The proof is complete. 
\end{proof}

Next, we present the proof of Theorem \ref{thm:multi}.

\begin{proof}[Proof of Theorem \ref{thm:multi}]
We consider the error decomposition in (\ref{decomposition}), that is, 
\begin{eqnarray*}
    \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right| &\leq  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|F(Pf) - \widehat{G}(f(\Bar{t}))| \\
    &=  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|G(f(\Bar{t})) - \widehat{G}(f(\Bar{t}))|.
\end{eqnarray*}
Here, we have used $G(f(\Bar{t})) = F\left(\sum_{i=1}^N f(t_i) \psi_i\right) = F(Pf)$, where the function $G$ is defined previously in equation (\ref{G}). 

With the assumption that $F$ is  $s$-H\"{o}lder continuous with constant $C_F \geq 0$ and the upper bound of $\|f-Pf\|_\infty$ given in (\ref{f-Pf_multi}), we have
\begin{equation*}
      \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| \leq  C_F \|f-Pf\|^s_\infty \leq  C_F e^{\frac{-csm}{\sqrt{d}}}\|f\|^s_{\mathcal{H}_K} \leq C_F e^{\frac{-csm}{\sqrt{d}}}. 
\end{equation*}
From Corollary \ref{G-Ghat_multi}, we get 
\begin{eqnarray*}
       \sup_{f\in \mathcal{K}}\left|G(f(\Bar{t})) - \widehat{G}(f(\Bar{t}))\right| \leq \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}}\leq \frac{(2m)^{2d-s\beta}}{M}C_FC_{\sigma, \beta,d,s}e^{4M_d\sigma sm}.
\end{eqnarray*}
Then, combining the above two error bounds together, we get
\begin{eqnarray*}
       \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right|& \leq& C_F e^{\frac{-csm}{\sqrt{d}}} + \frac{(2m)^{2d-s\beta}}{M}C_FC_{\sigma, \beta,d,s}e^{4M_d\sigma sm}\\
       & \leq&  C_F C_{\sigma, \beta,d,s} \left(e^{\frac{-csm}{\sqrt{d}}} + \frac{(2m)^{2d-s\beta}}{M}e^{4M_d\sigma sm}\right).
\end{eqnarray*}
We can see that there is a trade-off of $m$ between $e^{\frac{-csm}{\sqrt{d}}}$ and $\frac{(2m)^{2d-s\beta}}{M}e^{4M_d\sigma sm}$. To balance the trade-off, we choose $m\in \NN$ such that $e^{\frac{-csm}{\sqrt{d}}} \approx \frac{1}{M} e^{4M_d\sigma sm}$. 
We let 
\begin{eqnarray*}
       && e^{\frac{-csm}{\sqrt{d}}} =\frac{1}{M} e^{4M_d\sigma sm}\\
     &\iff&   \log (M)  = 4M_d\sigma sm + \frac{csm}{\sqrt{d}} = m \left(4M_d\sigma s + \frac{cs}{\sqrt{d}}\right)\\
      &\iff& m=\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}.
    \end{eqnarray*}
We take $m=\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil \in \NN$. 
Then,
\begin{eqnarray*}
        e^{\frac{-csm}{\sqrt{d}}} = \exp\left(-\frac{cs}{\sqrt{d}}\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil\right) \leq \exp\left(-\frac{c\log (M)}{4M_d \sqrt{d}\sigma+c}\right) = M^{-\frac{c}{4M_d \sqrt{d}\sigma+c}}. 
\end{eqnarray*}
We know $ \frac{1}{M} e^{4M_d\sigma sm} = e^{\frac{-csm}{\sqrt{d}}} \leq M^{-\frac{c}{4M_d \sqrt{d}\sigma+c}}$. Note that $2d-s\beta$ may be negative. We have
\begin{eqnarray*}
        (2m)^{2d-s\beta}\leq (2m)^{\max\{0,2d-s\beta\}} \leq \left(\frac{2\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}+2\right)^{\max\{0,2d-s\beta\}}\leq C_{\sigma, \beta,d,s} (\log (M))^{\max\{0,2d-s\beta\}}.
    \end{eqnarray*}
The proof is complete. 
\end{proof}

Next, We combine the error bound in (\ref{f-Pf_Gaussian}) and Corollary \ref{G-Ghat_Gaussian} together to prove Theorem \ref{thm:Gaussian}. 

\begin{proof}[Proof of Theorem \ref{thm:Gaussian}]
     Again, we use the error decomposition:
    \begin{eqnarray*}
         \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right| &\leq  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|F(Pf) - \widehat{G}(f(\Bar{t}))| \\
        &=  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|G(f(\Bar{t})) - \widehat{G}(f(\Bar{t}))|.
    \end{eqnarray*}

   Recall that we assume $F$ is  $s$-H\"{o}lder continuous with constant $C_F \geq 0$. Suppose $m > \sqrt{d}$, it follows from (\ref{f-Pf_Gaussian}) that
       \begin{equation*}
      \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| \leq  C_F \|f-Pf\|^s_\infty \leq  C_F e^{\frac{-csm\log \left(\frac{m}{\sqrt{d}}\right)}{\sqrt{d}}}\|f\|^s_{\mathcal{H}_K} \leq C_F e^{\frac{-csm\log \left(\frac{m}{\sqrt{d}}\right)}{\sqrt{d}}}. 
   \end{equation*}
   
   With Corollary \ref{G-Ghat_Gaussian}, we have 
   \begin{eqnarray*}
       \sup_{f\in \mathcal{K}}\left|G(f(\Bar{t})) - \widehat{G}(f(\Bar{t}))\right| \leq \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}}&\leq&\frac{7(m+1)^{2d}}{M} C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}\\
       &\leq&\frac{7(2m)^{2d}}{M} C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}.
   \end{eqnarray*}
   
Combining the above two error bounds, we get
 \begin{eqnarray*}
       \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right|& \leq& C_F e^{\frac{-csm\log \left(\frac{m}{\sqrt{d}}\right)}{\sqrt{d}}} + \frac{7(2m)^{2d}}{M} C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}\\
       & \leq&  C_F C_{\sigma,d,s} \left(e^{\frac{-csm\log \left(\frac{m}{\sqrt{d}}\right)}{\sqrt{d}}} + \frac{7(2m)^{2d}}{M} e^{\sigma^2\pi^2 dsm^2}\right).
   \end{eqnarray*}
   We let 
   \begin{eqnarray*}
      && e^{\frac{-csm\log \left(\frac{m}{\sqrt{d}}\right)}{\sqrt{d}}} = \frac{1}{M}e^{\sigma^2\pi^2 dsm^2} \\
    &\iff& \sigma^2\pi^2 dsm^2 + \frac{csm}{\sqrt{d}}\log \left(\frac{m}{\sqrt{d}}\right) = \log (M).
   \end{eqnarray*}
We solve $m$ for the following quadratic equation:
\begin{eqnarray*}
    \sigma^2\pi^2 dsm^2 + \frac{cs}{\sqrt{d}}m- \log (M) = 0
\end{eqnarray*}
and we take the positive solution of $m$ given by 
\begin{eqnarray*}
    m &=& \frac{-\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}}{2\sigma^2\pi^2ds}\\
    &=& \frac{- {\frac{c^2s^2}{d}+4\sigma^2\pi^2 ds \log(M) + \frac{c^2s^2}{d}}}{2\sigma^2\pi^2ds\left(\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}\right)}\\
    &=& \frac{2 \log(M)}{\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}}.
\end{eqnarray*}
    We choose 
    \begin{equation}\label{choice_m}
        m = \left\lceil \frac{2 \log(M)}{\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}} \right \rceil \in \NN. 
    \end{equation}
    We can easily check that the chosen $m$ is greater than $\sqrt{d}$. 
    With our choice of $m$ in (\ref{choice_m}), we know that 
    \begin{eqnarray}\label{lower_m}
        m &\geq& \frac{2 \log(M)}{\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}}\nonumber\\
        &\geq& \frac{2 \log(M)}{\frac{cs}{\sqrt{d}} + \frac{cs}{\sqrt{d}}+ 2\sigma \pi \sqrt{ds \log(M)}}\nonumber\\
        &\geq& \frac{\log(M)}{\frac{cs}{\sqrt{d}} + \sigma \pi \sqrt{ds \log(M)}}
    \end{eqnarray}
    and 
    \begin{eqnarray}\label{upper_m}
        m &\leq& \frac{-\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}}{2\sigma^2\pi^2ds}\nonumber\\
        &\leq&  \frac{ 2\sigma \pi \sqrt{ds \log(M)}}{2\sigma^2\pi^2ds}\nonumber\\
        &\leq&  \frac{1}{\sigma \pi}\sqrt{\frac{\log(M)}{ds}}.
    \end{eqnarray}
Then, from (\ref{lower_m}), we have 
\begin{eqnarray*}
  e^{\frac{-csm\log \left(\frac{\sqrt{d}}{m}\right)}{\sqrt{d}}} 
  &\leq& \exp\left(-\frac{cs}{\sqrt{d}}\frac{\log(M)}{\frac{cs}{\sqrt{d}} + \sigma \pi \sqrt{ds \log(M)}}\log \left(\frac{\log (M)}{cs+ \sigma \pi d\sqrt{s\log (M)}}\right)\right)\\
   &\leq& \exp\left(-\frac{cs\log(M)}{cs\sqrt{\log(M)} (1+ \sigma \pi d)}\log \left(\frac{\log (M)}{\sqrt{\log (M)}(cs+ \sigma \pi d\sqrt{s})}\right)\right)\\
    &\leq& \exp\left(-\frac{\sqrt{\log(M)}}{1+ \sigma \pi d}\log \left(\frac{\sqrt{\log (M)}}{cs+ \sigma \pi d\sqrt{s}}\right)\right)\\
     &\leq& \exp\left(-\frac{\log(\sqrt{M})}{1+ \sigma \pi d}\log \left(\frac{\sqrt{\log (M)}}{cs+ \sigma \pi d\sqrt{s}}\right)\right)\\
     &=& \sqrt{M}^{-\frac{1}{1+ \sigma \pi d}\log \left(\frac{\sqrt{\log(M)}}{cs+ \sigma \pi d\sqrt{s}}\right)}\\
     &=& M^{-\frac{1}{2(1+ \sigma \pi d)}\log \left(\frac{\sqrt{\log(M)}}{cs+ \sigma \pi d\sqrt{s}}\right)}.
\end{eqnarray*}
We know that 
\begin{equation*}
    \frac{1}{M}e^{\sigma^2\pi^2dsm^2} = e^{\frac{-csm\log \left(\frac{\sqrt{d}}{m}\right)}{\sqrt{d}}} \leq M^{-\frac{1}{2(1+ \sigma \pi d)}\log \left(\frac{\sqrt{\log(M)}}{cs+ \sigma \pi d\sqrt{s}}\right)}. 
\end{equation*}
It follows from (\ref{upper_m}) that 
\begin{eqnarray*}
    (2m)^{2d} \leq \left(\frac{2}{\sigma \pi}\sqrt{\frac{\log(M)}{ds}}\right)^{2d} \leq \left(\frac{2}{\sigma \pi \sqrt{ds}}\right)^{2d}(\log (M))^d.
\end{eqnarray*}
The proof is complete.
\end{proof}



Next, we give the proof of Theorem \ref{thm:general}.
\begin{proof}[Proof of Theorem \ref{thm:general}]
    The proof is rather straightforward. Here, we consider  $
    \mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}
$ with $\mathcal{H}_K$ induced by some Mercer kernel $K$ which is $\alpha$-H\"{o}lder continuous for $\alpha \in (0, 1]$ with constant $C_K \geq 0$. Note that we do not require $K$ to be translation-invariant. Again, we consider the error decomposition 
        \begin{eqnarray*}
         \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right| &\leq  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|F(Pf) - \widehat{G}(f(\Bar{t}))| \\
        &=  \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| +  \sup_{f\in \mathcal{K}}|G(f(\Bar{t})) - \widehat{G}(f(\Bar{t}))|.
    \end{eqnarray*}
    The upper bound of the first term is derived earlier in (\ref{eq:firstterm}), which is a consequence of Lemma \ref{f-Pf}. 
    The theorem follows directly after Lemma \ref{lemma:reg_G} and  Theorem \ref{lemma:tim}.
\end{proof}

Lastly, we give the proof of Corollary \ref{corollary:FLM}. 
\begin{proof}[Proof of Corollary \ref{corollary:FLM}]
 We first prove $F|_{\mathcal{H}_K}$ is H\"{o}lder continuous.
 By the Cauchy–Schwarz inequality,
$$|\langle X, \beta \rangle_{L^2([0,1])}| \leq \|\beta\|_{L^2}  \|X\|_{L^2}\leq \|\beta\|_{L^2}\kappa  \|X\|_{\mathcal{H}_{K_0}} \leq \kappa \|\beta\|_{L^2}.$$
For $X, U \in \mathcal{K}$, we have 
 \begin{eqnarray*}
    \left |F|_{\mathcal{H}_K} (X) - F|_{\mathcal{H}_K} (U)\right| 
     = \left|g\left(\langle X, \beta \rangle_{L^2([0,1])}\right) - g\left(\langle U, \beta \rangle_{L^2([0,1])}\right)\right|
     &\leq&\|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])}\|\beta\|_{L^2} \|X-U\|_{L^2}\\
     &\leq& \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])} \|\beta\|_{L^2}\kappa  \|X-U\|_{\mathcal{H}_K}.
 \end{eqnarray*}
 We see that $F|_{\mathcal{H}_K}$ is $s$-H\"{o}lder continuous with $s=1$ and constant $C_F = \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])} \|\beta\|_{L^2}\kappa$.
 Substitute $d=1$, $s=1$, and $C_F = \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])} \|\beta\|_{L^2}\kappa$ into Theorem \ref{thm:general} and Theorem \ref{thm:Gaussian}, the proof is complete. 
 \end{proof}
\end{appendices}    



%\bibliographystyle{ieeetr}
\small
\bibliography{reference}
\end{document}
