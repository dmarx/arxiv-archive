---
abstract: |
  Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to $\mathbb R$ (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS’s) using neural networks. We establish the universality of the approximation of functionals on the RKHS’s. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS’s, our proposed network is much simpler in that we use point evaluations to replace basis function expansions.
author:
- Tian-Yi Zhou$^1$
- Namjoon Suh$^2$
- Guang Cheng$^2$
- Xiaoming Huo$^1$
bibliography:
- reference.bib
citation-style: ieee
date: |
  $^1$H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology  
  $^2$Department of Statistics & Data Science, UCLA  
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Approximation of RKHS Functionals by Neural Networks
---





***Keywords:** neural networks, reproducing kernel Hilbert space, approximation theory, functional learning, functional data analysis*

# Introduction

This paper studies the approximation of smooth functionals defined over a reproducing kernel Hilbert space (RKHS) using *tanh neural networks*. A functional maps from a space of functions that has infinite dimensions to $\mathbb R$. In recent years, neural networks have been widely employed in operator learning tasks. We are interested in investigating their capability to approximate nonlinear functionals, a special type of operator.

Neural networks have been known as universal approximators since , i.e., to approximate any continuous function, mapping a finite-dimensional input space into another finite-dimensional output space, to arbitrary accuracy. These days, many interesting tasks entail learning operators, i.e., mappings between an infinite-dimensional input Banach space and (possibly) an infinite-dimensional output space. A prototypical example in scientific computing is to map the initial datum into the (time series of) solution of a nonlinear time-dependent partial differential equation (PDE). A priori, it is unclear if neural networks can be successfully employed to learn such operators from data, given that their universality only pertains to finite-dimensional functions.

One of the first successful uses of neural networks in the context of operator learning was provided by . A shallow neural network-based architecture, termed as *neural operator*, is proposed and is proved to possess an universal approximation property for nonlinear operators from an infinite-dimensional space to a finite-dimensional Euclidean space. Recently, there has been an active line of research developing new methods for learning operators by deep neural networks, such as the *DeepONet* , *physics-informed neural networks* (PINNs) , *Fourier Neural Operator* (FNO) , and *DeepGreen* . Although these methods have shown success through numerical experiments in approximating operators in many interesting examples, theoretical guarantees in this context are relatively scant. Some recent quantitative approximation results are obtained in .

This work focuses on the approximation of functionals by neural networks. Approximating functionals by neural networks is a crucial step in approximating operators by neural networks in the *DeepONet* and similar methods , as used in . In particular, we consider the approximation of functionals on reproducing kernel Hilbert spaces (RKHS’s), where a major advantage of RKHS is that it enables us to replace integration-type basis function expansions by simple point evaluations on a discrete subset of the domain.

We adopt neural networks equipped with tanh activation functions. Most of the recent results on neural network approximation focus on ReLU networks. Although ReLU activation functions are very common in practical applications of deep learning, there are many areas where other activation functions are employed. Among the most popular activations are the tanh activation function and the related sigmoid or logistic function (a scaled and shifted tanh). In the area of operator learning, smooth activation functions such as tanh are preferred over ReLU, especially in PINNs, for solving forward and inverse problems for PDEs (see and references therein). Also, tanh activation functions are the basis of heavily used recurrent neural networks (RNN), such as Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU) .

To establish a theoretical guarantee that neural networks can learn functionals well, we need to show the convergence of *generalization error*, which measures the distance between the ground-truth functional and the estimated functional given by a certain class of neural networks. In this paper, we show that a tanh neural network with two hidden layers suffices to approximate a functional on RKHS to any desired accuracy. Deriving approximation error is the first step of generalization analysis. We also apply our findings to functional data analysis (FDA) and prove that neural networks can approximate regression maps in generalized functional linear models well without any assumptions on the model parameters. Our approximation result will help theoretically justify the use of deep neural networks in functional data analysis. In fact, by following , we are now able to derive generation error and even statistical inferences for generalized functional linear regression models, which will be a future topic. More discussions regarding generalization studies of neural networks can be found in the recent survey paper .

## Related Works

In an earlier work , a shallow neural network with infinitely differentiable activation function is used to approximate nonlinear, continuous functionals on $L^p$ spaces, for $1 \leq p \leq \infty$. Though the approximation rate given there does not seem satisfactory — when measuring the approximation error on the unit ball of a Hölder space, the number of network parameters grows exponentially with a large exponent as the accuracy increases — it is proven that such a rate is, in fact, optimal. Later in , a generalized neural network is introduced to approximate functionals whose domain is a locally convex topological vector space. Based on the aforementioned work, a novel functional multi-layer perception (MLP) is introduced by , which can be directly applied to functional data. Recently in , an optimization algorithm is proposed to implement the functional MLP in practice. The functional MLP is based on, but different from, a classical fully-connected neural network.

To illustrate this special network’s design, we first formulate a standard fully-connected network. For $\mathbf b= (b_1,\ldots, b_r)\in \mathbb R^r$, let $\sigma_{\mathbf b}: \mathbb R^r \rightarrow \mathbb R^r$ be the shifted activation function as $$\sigma_{\mathbf b}\begin{pmatrix}
    x_1\\
    \vdots\\
    x_r
    \end{pmatrix}:= \begin{pmatrix}
    \sigma (x_1-b_1)\\
    \vdots\\
    \sigma (x_r-b_r)
    \end{pmatrix},$$ where $\sigma(x)$ is an activation function such as tanh: $\sigma(x) = \frac{e^x- e^{-x}}{e^x+e^{-x}}$ and ReLU: $\sigma(x) = \max \{x,0\}$. We use the vector $\mathbf d= (d_1,\ldots, d_L)\in \mathbb N^L$ to indicate the width in each hidden layer.

<div class="definition">

**Definition 1** (Standard Fully-connected Network).  * A fully-connected neural network $\widehat{G} : \mathbb R^{N} \rightarrow \mathbb R$ with $L$ hidden layers is a function that takes the form $$\widehat{G}(x) = a^T\sigma_{\mathbf b_L}(W_L \sigma_{\mathbf b_{L-1}}(W_{L-1}\cdots  \sigma_{\mathbf b_1}(W_1 x))),$$ where $x \in \mathbb R^{N}$ is the input, $a\in \mathbb R^{d_L}$ is the outer weight, $d_0 = N$, $W_\ell \in \mathbb R^{d_\ell \times d_{\ell-1}}$ denote the weight matrices, $b_\ell \in \mathbb R^{d_\ell}$ denote the bias vectors over the layers $\ell =1,\ldots, L,$ in the network.*

</div>

The functional MLP introduced in is designed to approximate functionals defined on $L^p[-1,1]^s$ for $p\geq 1$ and $s \in \mathbb N\geq 1$, where the network input is a function covariate $f(x): L^p[-1,1]^s \rightarrow \mathbb R$, $x\in [-1,1]^s$. We give the formulation of functional MLP below.

<div class="definition">

**Definition 2** (Functional MLP). *A functional MLP $\Theta:L^p[-1,1]^s \rightarrow \mathbb R$ with $L$ hidden layers is any functional that takes the form $$\Theta(f) = a^T\sigma_{\mathbf b_L}W_L \sigma_{\mathbf b_{L-1}}W_{L-1}\cdots  \sigma_{\mathbf b_2}W_2 T(f),$$ where $f\in L^p[-1,1]^s$, $T:L^p[-1,1]^s \rightarrow \mathbb R^{d_1}$ is a bounded linear operator with $$\label{firstlayer}
    T(f) = \left(\int_{[-1,1]^s}f(x)g_1(x)dx, \cdots, \int_{[-1,1]^s}f(x)g_{d_1}(x)dx\right)^T,$$ where $\{g_k\}_{k=1}^{d_1}\in L^q[-1,1]^s$ is a set of pre-selected basis functions and $\frac{1}{p}+ \frac{1}{q} = 1$.*

</div>

As we can see in (), the initial layer of this network converts the function $f$ into a vector form that can be fed directly into a standard neural network via basis function expansion. This is referred to as the *discretization step*. One drawback of such a design is that a prior selection of basis functions is required, often without information about the task at hand. A universal approximation theorem of the functional MLP is established in , where a scheme to learn $\{g_k\}_{k=1}^{d_1}$ is introduced by assuming them in a parametric function space.

More recently, two variants of functional MLP have been introduced in and . In , each basis function is parameterized with a micro neural network embedded in the MLP; while in , the basis functions are specifically chosen to be some families of orthogonal polynomials. For approximating functionals on $L^p$ spaces ($p\geq 1$), the network proposed in achieves a similar approximation rate as in . At this point, a question arises: can we adopt a simpler network to approximate nonlinear functionals without integration in the first layer?

All the above-mentioned functional learning methods are based on a set of basis functions defined on a domain in an Euclidean space. In this paper, we study functionals on RKHS’s, and the discretization of functions can be simply done by point evaluations. In other words, we adopt the standard fully-connected neural networks with the input vector given by the discretely observed functional values. Our design eliminates the need for a preprocessing step in the form of integration that typically requires a manually prespecified choice of basis function. Our network speaks directly to the state-of-the-art operator learning methods, as *DeepONet*, PINNs, *DeepGreen* all do not involve the basis expansion of network input.

## Problem Formulation

We first define RKHS. Let $K$ be a Mercer kernel on a compact metric space $(\mathcal{X}, d_\mathcal{X})$, which induces an unique RKHS defined as:

<div class="definition">

**Definition 3** (RKHS). *The RKHS $\mathcal{H}_K$ is the closure of the set $\left\{\sum_{j=1}^p \alpha_j K(\cdot, x_j): p\in \mathbb N, \alpha_j \in \mathbb R, x_j \in \mathcal{X}\right\}$ under the metric induced by the inner product $\langle \cdot, \cdot \rangle_{\mathcal{H}_K}$ given by $$\left\langle \sum_{i=1}^q \beta_i K(\cdot, y_i), \sum_{j=1}^p \alpha_j K(\cdot, x_j) \right\rangle_{\mathcal{H}_K}  = \sum_{i=1}^q \sum_{j=1}^p \beta_i \alpha_j K(y_i, x_j).$$*

</div>

The RKHS $\mathcal{H}_K$ is characterized by the property that the point evaluation functional $L_x$ on $\mathcal{H}_K$ given by $f \mapsto f(x)$ is continuous for every $x \in \mathcal{X}$.

Denote by $\|\cdot\|_{\mathcal{H}_K}$ the RKHS norm induced by the inner product $\langle \cdot, \cdot \rangle_{\mathcal{H}_K}$. We define the unit ball of $\mathcal{H}_K$ given by $\mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}.$ We are interested in approximating the functional $F:\mathcal{K} \rightarrow \mathbb R$, where $\mathcal{K}$ is regarded as a compact subset of $C(\mathcal{X})$, the space of all continuous functions on $\mathcal{X}$ with norm $\|f\|_\infty = \sup_{x\in \mathcal{X}}|f(x)|$. Throughout this paper, we assume that $F$ is $s$-Hölder continuous for $0<s \leq 1$ with constant $C_F \geq 0$: $$|F(f) - F(\Tilde{f})| \leq C_F \|f-\Tilde{f}\|^s_\infty, \qquad \forall f, \Tilde{f} \in \mathcal{K}.$$

In this paper, we adopt the classic fully-connected neural network defined in Definition equipped with the tanh activation function $\sigma(x) = \frac{e^x- e^{-x}}{e^x+e^{-x}}$. We aim to construct tanh neural networks $\widehat{G}: \mathbb R^N \to \mathbb R$ and obtain an upper bound on the following uniform approximation error $$\label{eq:uniapprox}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|,$$ where the network inputs $f(\Bar{t}) = (f(t_1), \ldots, f(t_{N}))\in \mathbb R^{N}$ are function values at sufficient but finitely many discrete locations $\Bar{t} = \{t_1, \ldots, t_N\}\subset \mathcal{X}$. Uniform spacing of locations is not necessary. Our method does not require basis function expansion.

## Main Contributions

We consider fully-connected tanh neural networks with two hidden layers. The novelty of our proposed networks is that we evaluate the input $f \in \mathcal{H}_K$ by simply taking its function values at some discrete points in the domain $\mathcal{X}$, that is, $f(\Bar{t}) = (f(t_1), \ldots, f(t_{N}))\in \mathbb R^{N}$ with $\Bar{t}=\{t_1, \ldots, t_N\} \subset \mathcal{X}$. We do not require uniform partition of these discrete points. Our design is motivated by the practice of representing continuous signals or images by discrete objects in various fields of science and engineering. To represent continuous inputs, such as continuous images and signals defined on a compact domain, it is common to take measurements of values over a discrete subset of the domain as discrete images or signals. Conversely, the integration-type function expansion with a set of pre-specified basis functions employed in previous works is not a typical approach in practice. We adopt a much simpler network by leveraging the interpolating orthogonal projections in RKHS’s.

Our results are summarized below. We establish novel error bounds for the approximation of functionals on the RKHS induced by a general Mercer kernel using tanh neural networks (Theorem ). Prior works on functional approximation study only those defined on $L^p$ spaces, making our results unprecedented in the literature. The approximation error bound exhibits a trade-off associated with $N\in \mathbb N$ (i.e., the number of discrete points used to evaluate our input $f$): when $N$ is small, the power function (which measures the distance between a function and its interpolant) increases; while when $N$ is too large, the network size becomes too big and can results in overfitting. By determining the optimal choices of $N$, we derive explicit rates of approximating functionals on Sobolev space (Theorem ), and on RKHS’s induced by inverse multiquadric kernel (Theorem ) and Gaussian kernel (Theorem ), respectively. These kernels are commonly considered in kernel methods, and their translation-invariance properties facilitate our theoretical analysis. Our results show that neural networks can achieve any desired approximation accuracy with sufficiently many network parameters. Particularly, for the inverse multiquadric and Gaussian kernels, we achieve approximation rates faster than logarithmic rates, w.r.t. the number of network parameters. Our results have shown improvement compared to the previous results on $L^p$ functionals in and .

This is a theoretical work on the approximation theory of RKHS functionals. We leave generalization analysis and numerical studies for future studies.

## Organization of this work

The rest of the paper is organized as follows: In Section , we present applications that require learning nonlinear functionals. We then give our main results in Section and . We broadly divide our main results into two parts; in Section , we give explicit rates of approximating RKHS functionals induced by specific kernels by tanh neural networks, which include Sobolev kernel (Theorem ), inverse multiquadric kernel (Theorem ), and Gaussian kernel (Theorem ); in Section , we give approximation results for general RKHS functionals using tanh networks (Theorem ). In Section , we apply our theoretical findings to approximate regression maps in generalized functional regression models (Corollary ). In Section , we discuss the general approaches and methods for establishing our main results. Concluding remarks are given in Section . Finally, Appendix contains the proof of some remarks, Appendix contains the proof of some supporting lemmas, and Appendix contains the proof of our main results.

# Motivations

Many tasks in statistics and machine learning require learning nonlinear functionals, including functional regression, ODE/PDE learning, and distribution regression. Our work is motivated by the broad applications functionals offer, as well as the abundance of functional data. In the following, we present some related applications.

<u>**Example 1: Functional Regression**</u>

With the progress of technology, we observe more and more data of a functional nature, such as curves, trajectories, time series, and images. The term *functional data* traditionally refers to repeated measurements of an individual over an interval, although it is broader in meaning; for example, it can refer to functions on higher dimensional domains (e.g., images over domains in $\mathbb R^2$ or $\mathbb R^3$) .

As functional data become more common, researchers are increasingly interested in regression models for functional data (i.e., relating functional variables to other variables of interest). A classical functional regression model is the *generalized functional linear model* (FLM) introduced by . In this model, we aim to learn a regression map from a function covariate to a scaler response.

Suppose $X$ is a square-integrable random function defined over a compact interval $\mathcal{X}$ and $Y$ is a scaler response. With the regressor $F:L^2(\mathcal{X})\rightarrow \mathbb R$, the generalized FLM can be formulated as $$Y =  g\left(\int_\mathcal{X} \beta(t) X(t) dt\right) + \varepsilon, \qquad t\in \mathcal{X},$$ where $\beta \in L^2(\mathcal{X})$ is the unknown functional parameter, $g$ is a link function that may possess high-order smoothness (e.g., a logit link), and $\varepsilon$ is a centered noise random variable. A fundamental problem in statistics is to determine the regression map $F$ and to subsequently retrieve the regression mean $\mathbb{E}[Y|X]:= g\left(\int_\mathcal{X} \beta(t) X(t) \right)$.

To solve the functional regression problem, it is crucial to learn the regression map $F:L^2(\mathcal{T})\rightarrow \mathbb R$. While the powerful capabilities of neural networks have been clearly demonstrated for vector/text/graph data, how to adapt these models to functional data remains under-explored. In this paper, we will show that for any given function covariate $X$ residing in a RKHS inside the $L^2$ space, we can approximate the regression map of generalized FLM using fully-connected neural networks. We assume $g$ is Lipschitz continuous. This is a novel result that has not been proven before in statistics literature. Detailed discussions can be found in Section .

<u>**Example 2: ODE/PDE Learning**</u>

Systems of coupled ODEs and PDEs are undoubtedly the most widely used models in science and engineering, as they often capture the dynamical behavior of physical systems. Even simple ODE functions can describe complex dynamical behaviors. Recently, an active line of research has been developing deep learning tools to infer solutions of ODEs/PDEs with lower cost compared to traditional tools (e.g., finite element methods).

In solving ODEs and PDEs, we aim to learn a map from the parametric function space to the ODE/PDE solution space. Consider a very simple initial value problem of an ODE: $$\label{ODE}
    \begin{cases}
        \frac{dh(x)}{dx} = g(x,f(x),h(x)), \qquad x \in [a,b],\\
        h(a) = h_0.
    \end{cases}$$ We aim to learn $h(b)$ for any function $f$, that is, a map $F$ from an input $f$ to an output $h(b)\in \mathbb R$. If we denote the solution to () as $G(f)$, then $$F(f) = G(f)(b),$$ where $G$ is a nonlinear operator being the unique solution to the integral equation $$G(f)(x) = h_0 + \int_{a}^x g(t, f(t), G(f)(t))dt.$$

Our work contributes to the area of ODE/PDE learning by proving that a two-layer standard neural network is sufficient to approximate (to any accuracy) a functional that is a solution map to a problem like ().

<u>**Example 3: Distribution Regression**</u>

We find unique applications that require functionals on RKHS’s. One of the motivations for learning RKHS functionals is *distribution regression*: regressing from probability measures to real-valued responses or, more generally, to vector-valued outputs.

Let $(\mathcal{X}, \tau)$ be a topological space and let $\mathcal{B}(\mathcal{X})$ be the Borel $\sigma$-algebra induced by the topology $\tau$. Let $\mathcal{M}^+(\mathcal{X})$ denote the set of Borel probability measures on $(\mathcal{X}, \tau)$, and $\mathcal{Y}$ be a separable Hilbert space. Consider $z=\{(X_i, y_i)\}_{i=1}^\ell$, where $X_i\in \mathcal{M}^+(\mathcal{X})$, $y_i\in \mathcal{Y}$ is its observable label, each $(X_i, y_i)$ pair is drawn i.i.d. from a joint meta distribution $\mathcal{M}$. We do not observe $X$ directly; rather, we observe a sample $x_{i,1}, \ldots, x_{i,N_i} \overset{i.i.d.}{\sim} X_i$. Thus the observed data are $\tilde z=\{(\{x_{i,n}\}_{n=1}^{N_i}, y_i)\}_{i=1}^\ell$. The goal of distribution regression is to learn the map between the random distribution $X$ and its label $y$ based on the observed data $\tilde z$.

To solve the distribution regression problem, an approach via kernel ridge regression is proposed and a fast convergence is proven. The approach involves kernel mean embedding as an intermediate step. Denote by $\mu(\mathcal{M}^+(\mathcal{X})) = \{\mu_X:X\in \mathcal{M}^+\} \subseteq \mathcal H_K \text{ with } \mu_X = \int_{\mathcal{X}} \textbf{Ker}(\cdot,u)dX(u) \in  \mathcal H_K$ the set of mean embeddings of $X$ to a RKHS $\mathcal H_K$. We also let $H_K$ be the $Y$-valued RKHS of $\mu(\mathcal{M}^+(\mathcal{X})) \rightarrow \mathcal{Y}$ functions with reproducing kernels $\textbf{Ker}: \mu(\mathcal{M}^+(\mathcal{X})) \times \mu(\mathcal{M}^+(\mathcal{X})) \rightarrow \mathcal{Y}$. The distribution $X\in \mathcal{M}^+$ is first mapped to $\mu(\mathcal{M}^+(\mathcal{X})) \subseteq \mathcal H_K$ by the mean embedding $\mu$, then the result is composed with function $f\in H_K$: $$\mathcal{M}^+ \overset{\mu} {\rightarrow} \mu(\mathcal{M}^+(\mathcal{X}))\overset{f\in H_K}{\longrightarrow} \mathcal{Y}.$$ An important part of the method is to learn the functionals that map $\mu(\mathcal{M}^+(\mathcal{X}))$ (a subset of RKHS $\mathcal H_K$) to $\mathcal{Y}$.

In this paper, we give a theoretical guarantee that neural networks can approximate functionals on RKHS well. Our findings suggest that neural networks have the potential to tackle distribution regression problems, as they can be used to find the map from $\mu(\mathcal{M}^+(\mathcal{X}))$ to $\mathcal{Y}$.

# Main Results I: Approximation of RKHS Functionals induced by specific kernels

We consider the domain $\mathcal{X} = [0,1]^d$. We present the first part of our main results here. The second part of our main results is in Section , where we give approximation results for functionals on general RKHS’s. In this section, we give explicit error rates of approximating functionals on RKHS’s— induced by the following three specific reproducing kernels — to demonstrate our methods and approaches:

- Sobolev space $W^r_2(\mathcal{X})$. Although this is not a kernel, it is known that the Sobolev space $W^r_2(\mathbb R^d)$ of order $r$ is a RKHS if $r>d/2$ (see, e.g., ).

- Inverse multiquadric kernel: $K(u,v) = (\sigma^2 + |u-v|^2)^{-\beta}$, where $u,v \in \mathcal{X}$ and $\sigma, \beta >0$. The parameter $\sigma$ is often called the *shape parameter*.

- Gaussian kernel: $K(u,v)= e^{-\frac{|u-v|^2}{2\sigma^2}}$, where $u,v \in \mathcal{X}$ and $\sigma>0$.

We would like to point out that both the inverse multiquadratic kernel and the Gaussian kernel are important examples of *translation-invariant kernels* (also known as *convolution-type kernels*). The definition of a translation-invariant kernel is given below.

<div class="definition">

**Definition 4** (Definition of translation-invariant kernel).  * A kernel $K:\mathbb R^d \times \mathbb R^d \rightarrow \mathbb R$ is called translation-invariant (or convolution-type) if it only depends on the difference between its arguments, that is, $$\label{eq:translation}
        K(u,v) = \phi(u-v), \qquad  u,v \in \mathbb R^d$$ for some function $\phi:\mathbb R^d \rightarrow \mathbb R$. Such a function $\phi$ is considered positive definite if the corresponding kernel $K$ is positive definite.*

</div>

Note that we can represent a translation-invariant kernel both as $K:\mathbb R^d \times \mathbb R^d \rightarrow \mathbb R$ with two arguments and as $\phi:\mathbb R^d \rightarrow \mathbb R$ with only one argument.

The Fourier transform of a function $\phi$ in $\mathbb R^d$ is the function $\widehat{\phi}:  \mathbb R^d \rightarrow \mathbb{C}$ defined by $$\widehat{\phi}(\xi) = \int_{\mathbb R^d} \phi(x) e^{-2\pi i\xi \cdot x} dx, \qquad \xi\in \mathbb R^d.$$ From the inverse form of the Fourier transform, we know $$\label{eq:inversefourier}
    \phi(x) = \int_{\mathbb R^d} \widehat{\phi}(\xi) e^{2\pi i\xi \cdot x} d\xi, \qquad x\in \mathbb R^d.$$ It is sometimes difficult to check if a kernel is positive definite. But if a kernel is translation-invariant, we can do so by easily checking its Fourier transform.

<div class="lemma">

**Lemma 5**.  * A translation-invariant kernel $K$ induced by $\phi: \mathbb R^d \to \mathbb R$ is a reproducing kernel (i.e., positive semi-definite) if its Fourier transform $\widehat{\phi}$ is real-valued and non-negative.*

</div>

In fact, the positivity of the Fourier transform almost everywhere is sufficient for the positive definiteness of a kernel . The proof of Lemma is relegated to Appendix .

In the following, we present explicit error bounds for approximating functionals on Sobolev space and RKHSs induced by inverse multiquadric and Gaussian Kernels. We adopt standard neural networks with two hidden layers. The network input is a function $f$ evaluated at some discrete points in $\mathcal{X}$, that is, $\{f(t_1), \ldots, f(t_N)\}$. We want to highlight that the points $\{t_1,\ldots, t_N\}$ need not be uniformly partitioned in $\mathcal{X}$. Since we only consider RKHS’s induced by translation-invariant kernels in this section, it seems natural to choose $\{t_1,\ldots, t_N\} = \{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ for $m\in \mathbb N$ and $N = (m+1)^d$.

## Approximation of Functionals on Sobolev Space $W^r_2(\mathcal{X})$

We consider the Sobolev Space $W^r_2(\mathcal{X})$ of order $r$ for $r\in \mathbb R_+$ and $d\in \mathbb N$. We allow $r$ to be any arbitrary real number (i.e., $r$ need not be an integer), and the corresponding Sobolev space $W^r_2(\mathcal{X})$ is often referred to as a *fractional Sobolev space*. The following well-known result suggests that when the embedding condition $r>d/2$ holds, $W^r_2(\mathcal{X})$ is a RKHS.

<div class="proposition">

**Proposition 6**. *The standard Sobolev space $W^r_2(\mathbb R^d)$, with arbitrary positive $r$ and $d$ for which $r>d/2$, is a RKHS induced by a translation-invariant kernel $K(u,v) = \phi (u-v)$, where its Fourier transform is the function $\widehat{\phi}(\xi)$ given by $$\label{fourier_Sobolev}
    \widehat \phi (\xi) = (1+|\xi|^2)^{-r}, \qquad  \xi \in \mathbb R^d.$$*

</div>

From Lemma , since $\widehat \phi(\xi) = (1+|\xi|^2)^{-r} >0$ for all $\xi \in \mathbb R^d$, $K$ (or $\phi$) is a reproducing kernel. It is somehow surprising that it is difficult to find explicit formulas for reproducing kernels of Sobolev spaces, except for the univariate case $d=1$. More discussions can be found in and references therein.

The classic Sobolev Embedding Theorem tells us that, for $r>d/2$ and $r-d/2 \notin \mathbb N$, we have $W^r_2(\mathcal{X}) \subset W_\infty^{r-d/2}(\mathcal{X})$, where $W_\infty^{r-d/2}(\mathcal{X})$ represents the Hölder space of order $r-d/2$. Moreover, we have $$\|f\|_{W_\infty^{r-d/2}} \leq C_{r,d}  \|f\|_{W^r_2(\mathbb R^d)},$$ where $C_{r,d}$ is a constant depending on only $r$ and $d$. When $r>d/2$ and $r-d/2 \in \mathbb N$, we can easily extend the above results to get $W^r_2(\mathcal{X}) \subset W_\infty^{r-d/2-\varepsilon}(\mathcal{X})$ and $\|f\|_{W_\infty^{r-d/2 - \varepsilon}} \leq C_{r,d}  \|f\|_{W^r_2(\mathbb R^d)}$, with any $0< \varepsilon < r-d/2$. Hereafter we assume $r-d/2 \notin \mathbb N$.

The following result establishes explicit error bound in approximating functionals on Sobolev Space $W^r_2(\mathcal{X})$ using two-layer tanh neural networks. Its proof is relegated to Appendix .

<div class="theorem">

**Theorem 7** (Approximation of Functionals on Sobolev Space).  * Let $r\in \mathbb R_+, d\in \mathbb N$, $\mathcal{X} = [0,1]^d$, and $r-d/2>1$. We assume that $r-d/2 \notin \mathbb N$. Consider $\mathcal{K} := \{f\in W^r_2(\mathcal{X}): \|f\|_{W^r_2(\mathcal{X})} \leq 1\}$. Suppose $F$ is $s$-Hölder continuous with $s \in (0,1]$. There exists some $M_0\in \mathbb N$ such that for every $M\in \mathbb N$ with $M>M_0$, by taking $m=\left\lceil M^{\frac{1}{2s(2r-1)}}\right\rceil$, $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \subset \mathcal{X}$, and $N= (m+1)^d$, we have a tanh neural network $\widehat{G}: \mathbb R^N \to \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left(\frac{N+1}{2}\right) (5M)^N$ satisfying $$\sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq C_{r, s, d, F} \ d^{s(r+\frac{1}{2})} \left(\frac{1}{M}\right)^{\frac{2r-d}{2(2r-1)}},$$ where $C_{r, s, d, F}$ is a constant depending on $r, s, d$ and the Lipschitz constant of $F$.*

</div>

<div class="remark">

**Remark 8**.  * The maximum total number of parameters in this network is $\mathcal{N} = \mathcal{O}(N(5M)^N)$, where $N=(m+1)^d = \left(\left\lceil M^{\frac{1}{2s(2r-1)}}\right\rceil +1 \right)^d$. As $M$ increases (i.e., the network widths increase), the approximation error converges to $0$. The convergence rate given in the above theorem — w.r.t. the number of parameters $\mathcal{N}$ — achieves a logarithmic rate. We defer the technical proof of this statement to Appendix for conciseness.*

</div>

## Approximation of RKHS Functionals Induced by Inverse Multiquadric Kernel 

Here, we present the result on the approximation of functionals defined over $\mathcal{K}$, an RKHS induced by the inverse multiquadric kernel $$\label{multi}
    K(u,v) = (\sigma^2 + |u-v|^2)^{-\beta}, \qquad  u,v \in \mathcal{X}, \ \sigma, \beta >0$$ using two-layer tanh neural networks. The inverse multiquadric kernel is a reproducing kernel as long as $\sigma,\beta >0$ (see, ). The proof of the following theorem is given in Appendix .

<div class="theorem">

**Theorem 9** (Approximation of RKHS Functionals induced by inverse multiquadric Kernel).  * Let $d\in \mathbb N$, $\sigma, \beta >0$, $\mathcal{X} = [0,1]^d$, and $M_d = 12 \left(\frac{\pi \Gamma^2(\frac{d+2}{2})}{9}\right) \leq 6.38d$. Consider $\mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}$ with $\mathcal{H}_K$ induced by an inverse multiquadric kernel given in (). Suppose $F$ is $s$-Hölder continuous with $s \in (0,1]$. There exists some $M_0\in \mathbb N$ such that for every $M\in \mathbb N$ with $M>M_0$, by taking $m=\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil$, $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \subset \mathcal{X}$, and $N= (m+1)^d$, we have a tanh neural network $\widehat{G}: \mathbb R^N \to \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying $$\sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq C_{\sigma, \beta, s, d, F} (\log (M))^{\max\{0, 2d-s\beta\}}\left(\frac{1}{M}\right)^{\frac{c}{4M_d \sqrt{d}\sigma+c}},$$ where $C_{\sigma, \beta, s, d, F}$ is a constant depending on $\sigma, \beta, s, d$ and the Lipschitz constant of $F$, $c>0$ is a positive constant given for the multiquadric kernel by ().*

</div>

<div class="remark">

**Remark 10**.  * The maximum number of network parameters is $\mathcal{N} = \mathcal{O}\left(N(5M)^N\right)$, where $N = (m+1)^d = \left(\left\lceil\frac{\log (M)}{4M_d\sigma s + \frac{cs}{\sqrt{d}}}\right\rceil +1\right)^d$. As $M$ increases, the approximation error converges to $0$. The convergence rate of the approximation error given in the above theorem — w.r.t. the number of network parameters $\mathcal{N}$ — is always faster than the logarithmic rate but slower than the polynomial rate. We defer the technical proof of this statement to Appendix for conciseness.*

</div>

<div class="remark">

**Remark 11**. *It is indeed quite disappointing that the tanh neural network cannot achieve a polynomial rate for approximating RKHS functionals. The only lower bound of approximation of continuous functionals is established in . They studied the approximation of $F: L^p (\mathcal{X}) \rightarrow \mathbb R$ for $1 \leq p \leq \infty$ on some certain compact domain $K \subset W^\beta_\infty (\mathcal{X})$, the set of functions satisfying a Hölder condition of order $\beta >0$. They obtained a lower bound, with $0 < \lambda \leq 1$, $$\sup_{f\in K}\left|F(f) - \widehat{G}(f)\right|\geq C(\log (\mathcal N))^{-\frac{\beta \lambda}{d}},$$ where $\widehat{G}$ represents neural networks equipped with sigmoid-type infinitely differentiable activation functions (e.g., tanh function), and $\mathcal{N}$ is the total number of network parameters. This is a logarithmic rate of approximation error. We cannot directly compare our results to their lower bound because we study RKHS functionals instead of $L^p$ functionals. However, such a lower bound suggested that the approximation of continuous functionals may be generally slow.*

</div>

## Approximation of RKHS Functionals Induced by Gaussian Kernel

Gaussian kernel is an important translation-invariant kernel given by $$\label{Gaussian}
    K(u,v)= e^{-\frac{|u-v|^2}{2\sigma^2}}, \qquad u,v \in \mathcal{X}, \sigma >0,$$ or $\phi(x) = e^{-\frac{x^2}{2\sigma^2}}$ for $x\in\mathcal{X}$. From Lemma , the Gaussian kernel is a reproducing kernel because its Fourier transform is known to be $\widehat{\phi}(\xi) = (2\sigma^2\pi)^{\frac{d}{2}}e^{-2\sigma^2\pi^2 |\xi|^2}> 0$ for all $\xi \in \mathbb R^d$. The following result gives an explicit error bound of approximation of functional $F$ over $\mathcal{K}$, an RKHS induced by a Gaussian kernel, using two-layer tanh neural networks. Its proof is given in Appendix .

<div class="theorem">

**Theorem 12** (Approximation of RKHS Functionals induced by Gaussian Kernel).  * Let $d\in \mathbb N$, $\sigma >0$, $\mathcal{X} = [0,1]^d$. We also let $c>0$ be some positive constant. Consider $\mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}$ with $\mathcal{H}_K$ induced by a Gaussian kernel given in (). Suppose $F$ is $s$-Hölder continuous with $s \in (0,1]$. There exists some $M_0\in \mathbb N$ such that for every $M\in \mathbb N$ with $M>M_0$, by taking $$\label{m}
        m = \left\lceil \frac{2 \log(M)}{\frac{cs}{\sqrt{d}} + \sqrt{\frac{c^2s^2}{d}+4\sigma^2\pi^2ds \log(M)}}\right \rceil > \sqrt{d},$$ $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \subset \mathcal{X}$, and $N= (m+1)^d$, we have a tanh neural network $\widehat{G}: \mathbb R^N \to \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying $$\label{thm:Gaussian_bound}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq C_{\sigma, \pi, s, d, F} (\log (M))^{d}\left(\frac{1}{M}\right)^{\frac{1}{2(1+ \sigma \pi d)}
    \left(\frac{1}{2}\log\log(M) - \log(cs+ \sigma \pi d\sqrt{s})\right)},$$ where $C_{\sigma, \pi, s, d, F}$ is a constant depending on $\sigma, \pi, s, d$ and the Lipschitz constant of $F$.*

</div>

<div class="remark">

**Remark 13**. *The maximum total number of parameters of the network is $\mathcal{N}=  \mathcal{O}\left(N(5M)^N\right)$, where $N= (m+1)^d$ with $m$ given in . As $M$ increases, the error bound given in () converges to $0$. Similar to Remark for Theorem , we can check that the convergence rate (w.r.t. $\mathcal{N}$) is faster than a logarithmic rate but slower than a polynomial rate.*

</div>

In the next section, we give our result for approximating functionals on general RKHS’s.

# Main Results II: Approximation of general RKHS Functionals

In this section, we give our results on approximating functionals on RKHS’s induced by a general Mercer kernel using tanh neural networks. We consider an RKHS $\mathcal{H}_K$ induced by some Mercer kernel $K: \mathcal{X} \times \mathcal{X}  \rightarrow \mathbb R$. We assume $K$ is $\alpha$-Hölder continuous for $0<\alpha \leq 1$ with constant $C_K \geq 0$, that is, $$\label{reg_K}
    |K(u,v)-K(u,\Tilde{v})| \leq C_K (d_\mathcal{X}(v,\Tilde{v}))^\alpha , \qquad  u,v,\Tilde{v} \in \mathcal{X}.$$

In $\mathcal{X}$, we choose a set of finitely many locations $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$ for $N\in\mathbb N$ such that the corresponding Gram matrix $$\label{gram}
    K[\Bar{t} ] = (K(t_i, t_j))_{i,j=1}^N 
    =  \begin{bmatrix}
    K(t_1, t_1) & K(t_1, t_2) & \cdots & K(t_1, t_N)\\
    K(t_2, t_1) & K(t_2, t_2) & \cdots & K(t_2, t_N)\\
    \vdots & \vdots & \ddots & \vdots\\
    K(t_N, t_1) & K(t_N, t_2) & \cdots & K(t_N, t_N)
      \end{bmatrix}$$ is invertible. We do not need these locations to be uniformly spaced in $\mathcal{X}$. In general, we select $\Bar{t}$ such that $$\max_{x\in \mathcal{X}} \min_{t\in \Bar{t}} \ d_\mathcal{X}(x,t) \rightarrow 0, \qquad \text{as } N \rightarrow \infty.$$ Later, we will show that the norm of the inverse of $K[\Bar{t} ]$ (i.e., $\|(K[\Bar{t}])^{-1 }\|_{op}$) has a significant influence on the approximation rate of RKHS functionals.

Before presenting our last theorem, we introduce two geometric quantities employed in our analysis: the *fill distance* and the *separation radius*. For any $x\in \mathcal{X}$ and $\Bar{t} \subset \mathcal{X}$, let $t_{j_x}$ be the closest point in $\Bar{t}$ to $x$, with $j_x \in \{1,\ldots, N\}$. The fill distance of $\Bar{t}$ is given by $$\begin{aligned}
\label{fill}
    h_{\Bar{t}} := h_{\Bar{t},\mathcal{X}} := \sup_{x\in \mathcal{X}} \min_{1\leq i \leq N}d_\mathcal{X}(x, t_i) = \sup_{x\in \mathcal{X}} d_\mathcal{X}(x,t_{j_x}) , 
\end{aligned}$$ which indicates how well the points in $\Bar{t}$ fill out the domain $\mathcal{X}$. It measures the radius of the largest possible empty ball that can be placed among the points in $\Bar{t}$. The second geometric quantity is the separation radius of $\Bar{t}$, which is half the distance between the two closest points in $\Bar{t}$. It is given by $$\label{separation}
    q_{\Bar{t}} := \frac{1}{2} \min_{u \neq v \in\bar{t}} |u-v|.$$ We can see that $h_{\Bar{t}}$ and $q_{\Bar{t}}$ decreases as $N\in \mathbb N$ increases.

In order to quantify the worst-case (i.e., for any $f$ in ${\mathcal{H}_K}$) uniform error between a function and its interpolant, the so-called *power function* is widely adopted in kernel interpolation literature (see, e.g., ). In our analysis, we use the power function to, in turn, measure the regularity of a Mercer kernel $K$. For a kernel $K$ and $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$, the power function is defined as $$\label{power}
    \epsilon_K(\Bar{t}):= \max_{x\in \mathcal{X}} \min_{c\in\mathbb R^N} \left\|K_x -  \sum_{i=1}^N c_i  K_{t_i}\right\|_{\mathcal{H}_K} 
    = \max_{x\in \mathcal{X}} \min_{c\in\mathbb R^N} \left\{ K(x,x) -2 \sum_{i=1}^N c_i K(x,t_i)+ \sum_{i=1}^N \sum_{j=1}^N c_i  c_j K(t_i, t_j)\right\}^{\frac{1}{2}}.$$

For any $\Bar{t} = \{t_i\}_{i=1}^N \subset \mathcal{X}$, by the Gram matrix given in (), the fill distance given in (), and the power function defined in (), we are able to derive an error bound for the approximation of functional on RKHS induced by a general Mercer kernel.

<div class="theorem">

**Theorem 14** (Approximation of general RKHS Functionals).  * Let $d\in \mathbb N$, $\mathcal{X} = [0,1]^d$. Consider $\mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}$ with $\mathcal{H}_K$ induced by some Mercer kernel $K$ which is $\alpha$-Hölder continuous for $\alpha \in (0, 1]$ with constant $C_K \geq 0$. Suppose $F$ is $s$-Hölder continuous for $s \in (0,1]$ with constant $C_F \geq 0$. There exists some $M_0\in \mathbb N$ such that for every $M\in \mathbb N$ with $M>M_0$, by taking some $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$ with $N\in \mathbb N$, we have a tanh neural network $\widehat{G}$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying $$\label{thm:general_bound}
    \sup_{f\in \mathcal{K}}\left|F(f) - \widehat{G}(f(\Bar{t}))\right|\leq  C_F (\epsilon_K(\Bar{t}))^s + \frac{7N^2C_G}{M},$$ with $$C_G = C_F\left(1+ \|(K[\Bar{t}])^{-1 }\|_{op}\sqrt{N} C_K \left(h_{\Bar{t}}\right)^\alpha\right)^s,$$ where the Gram matrix $K[\Bar{t}]$ is given in (), the fill distance $h_{\Bar{t}}$ is given in (), and the power function $\epsilon_K(\Bar{t})$ is defined in ().*

</div>

Let us look closely at the approximation error bound we obtained in , particularly how the number of locations $N\in \mathbb N$ in $\Bar{t}$ affects the bound. The error bound consists of two components: the former term corresponds to the power function $\epsilon_K(\Bar{t})$ which typically decreases as $N$ increases; the latter term is of $\mathcal{O}(N^{2+\frac{s}{2}})$ which obviously increases with $N$. We can see a trade-off associated with $N$ between these two terms. We shall strike the right balance and find the “sweet spot" of $N$ so neither of these terms blows up. Later, in the proofs of our main results in Appendix , we give detailed derivations of the optimal choices of $N$.

# Approximate Regression Map in Functional Regression Using Neural Network

In this section, we apply our findings to approximate regression maps in generalized functional linear regression models.

Despite their widespread success, the application of neural networks to functional data remains scarce today. Extensively studied in the statistics literature, functional data appear frequently in scientific studies, such as in datasets of air pollution, fMRI scans, and growth curves . Functional regression with observed random functions as predictors coupled with scalar responses is one of the core tools of functional data analysis . The classical model of functional regression is the functional linear model (FLM), where the response $Y\in \mathbb R$ is related to a square-integrable random function $X\in L^2([0,1])$ through $$\label{flm}
    Y= \int_0^1 X(t) \beta(t) dt + \varepsilon  = \langle X, \beta \rangle_{L^2([0,1])} + \varepsilon.$$ Here the unknown functional parameter $\beta \in L^2([0,1])$ is referred to as *slope function*, and $\varepsilon$ is a centered noise random variable. An extension of the above model is a generalized functional linear model given by $$\label{gflm}
    Y= g\left(\int_0^1 X(t) \beta(t) dt\right) + \varepsilon =  g\left(\langle X, \beta \rangle_{L^2([0,1])}\right) + \varepsilon,$$ where $g:\mathbb R\to \mathbb R$ is a link function. We shall assume $g$ is Lipschitz continuous in $\mathbb R$. A fundamental problem in statistics is estimating the slope function $\beta$ in the FLM based on a training sample consisting of $n$ independent copies of $(X, Y)$ (see, e.g., ). An efficient estimator of $\beta$ is crucial for subsequently retrieving the regression mean $\mathbb{E}[Y|X]:= g\left(\int_0^1 X(t) \beta(t) dt\right)$. This paper studies the approximation using neural networks; estimation based on training samples is left for future study. Here, we aim to approximate the regression map of generalized FLM by neural networks and derive the rate of convergence of approximation error.

For a function covariate $X\in L^2([0,1])$, let $F:L^2([0,1]) \to \mathbb R$ be the regression map of generalized FLM given by $$\label{func_gflm}
    F: X \mapsto g\left(\int_0^1 X(t) \beta(t) dt\right), \qquad X\in L^2([0,1]).$$ We shall restrict the domain of $F$ onto a subset of a RKHS $\mathcal{H}_{K_0}$ induced by a Mercer kernel $K_0$ on $[0,1] \times [0,1]$. Consider the unit ball $\mathcal{K}_0 := \{f\in \mathcal{H}_{K_0} \subset L^2([0,1]): \|f\|_{\mathcal{H}_{K_0}} \leq 1\}$. We assume $\mathcal{H}_{K_0}$ is dense in $L^2([0,1])$. It is known that the RKHS induced by an inverse multiquadric kernel or a Gaussian kernel is dense in $L^2$ space, as well as the Sobolev space in $\mathbb R$ with order $r>1/2$. We define a RKHS functional $F|_{\mathcal{H}_{K_0}}:\mathcal{K}_0 \to \mathbb R$ by $$\label{regmap}
    F|_{\mathcal{H}_{K_0}}: X \mapsto g\left(\int_0^1 X(t) \beta(t) dt\right), \qquad X\in \mathcal{K}_0 \subset L^2([0,1]).$$

Applying our previous theoretical findings, we prove that two-layer tanh neural networks can approximate the generalized functional regression map well. Concretely, for any function covariate $X\in \mathcal{K}_0$ and some discrete points $\bar{t} = \{t_1, \cdots, t_N\}\subset [0,1]$ (even spacing is not necessary), there exists a network $\widehat G:\mathbb R^N \to \mathbb R$ that takes $X(\bar{t})=(X(t_1), \cdots, X(t_N))\in\mathbb R^N$ as input such that $\sup_{X\in \mathcal{K}_0}\left|F|_{\mathcal{H}_{K_0}}(X) - \widehat{G}(X(\Bar{t}))\right| \to 0$ as network widths increases.

For a function $f:[-T, T] \to \mathbb R$, we define its Lipschitz seminorm as $$\|f\|_{Lip[-T,T]}:= \sup_{x,y\in [-T, T], x \neq y}\frac{|f(x) - f(y)|}{|x-y|}.$$ With a slight abuse of notation, we use $\|\cdot\|_{L^2}:= \|\cdot\|_{L^2([0,1])}$ denote the $L^2$ norm over $[0,1]$. The following result follows from Theorem and Theorem . Its proof is relegated to Appendix .

<div class="corollary">

**Corollary 15** (Approximation of Regression Map in Generalized FLM).  * Let $d=1$ and $\mathcal{X} = [0,1]$. Assume $\mathcal{H}_{K_0}$ is dense in $L^2([0,1])$, where $\mathcal{H}_{K_0}$ is a RKHS induced by some Mercer kernel $K_0$ which is $\alpha$-Hölder continuous for some $\alpha \in (0, 1]$ with constant $C_{K_0} \geq 0$. Let $\kappa=\sup_{t\in[0,1]}\sqrt{K_0(t,t)}$. Consider the regression map $F|_{\mathcal{H}_{K_0}}$ defined in . There exists some $M_0\in \mathbb N$ such that for every $M\in \mathbb N$ with $M>M_0$, by taking some $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$ with $N\in \mathbb N$, we have a tanh neural network $\widehat{G}: \mathbb R^N \to \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil\frac{N+1}{2}\right\rceil (5M)^N$ satisfying $$\label{thm:general_bound}
    \sup_{X\in \mathcal{K}_0}\left|F|_{\mathcal{H}_{K_0}}(X) - \widehat{G}(X(\Bar{t}))\right|\leq  C_F (\epsilon_{K_0}(\Bar{t})) + \frac{7N^2C_G}{M},$$ with $$C_F= \|g\|_{Lip([-\kappa \|\beta\|_{L^2}, \kappa \|\beta\|_{L^2}])} \|\beta\|_{L^2}\kappa, C_G = C_F\left(1+ \|(K_0[\Bar{t}])^{-1 }\|_{op}\sqrt{N} C_K \left(h_{\Bar{t}}\right)^\alpha\right),$$ where the Gram matrix $K_0[\Bar{t}]$ is defined in (), the fill distance $h_{\Bar{t}}$ is given in (), and the power function $\epsilon_{K_0}(\Bar{t})$ is defined in ().*

*In particular, if we take $K_0$ to be a Gaussian kernel with $\sigma >0$, $m = \left\lceil \frac{2 \log(M)}{c + \sqrt{c^2+4\sigma^2\pi^2 \log(M)}}\right \rceil > 1$, $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}$, and $N= m+1$, we have $$\sup_{X\in \mathcal{K}}\left|F|_{\mathcal{H}_K}(X) - \widehat{G}(X(\Bar{t}))\right|\leq C_{\sigma, \pi, F} \log (M)\left(\frac{1}{M}\right)^{\frac{1}{2(1+ \sigma \pi )}
    \left(\frac{1}{2}\log\log(M) - \log(c+ \sigma \pi)\right)},$$ where $C_{\sigma, \pi, F}$ is a constant depending on $\sigma, \pi$ and $C_F$, $c$ is a positive constant given by $\eqref{f-Pf_Gaussian}$.*

</div>

Note that we can also take $K_0$ to be an inverse multiquadric kernel or a Sobolev kernel in $\mathbb R$. Then, by applying Theorem or Theorem , we can obtain the corresponding approximation error bound.

By the assumption that $\mathcal{H}_{K_0}$ is a dense subset, for any $X\in cl(\mathcal{K})$ (i.e., closure of $\mathcal{K}$), there exists a sequence of functions $\{X_n: \mathcal{K} \to \mathbb R, n\in \mathbb N\}$ uniformly convergent to $X$ (i.e., $\sup_{t\in [0,1]}|X_n(t) - X(t)| \to 0$ as $n\to \infty$). We can see that the functional $F|_{\mathcal{H}_K}$ can be extended to $F$ on $cl(\mathcal{K})$ as $$F(X) = \lim_{n\to \infty} F|_{\mathcal{H}_K} (X_n) = \lim_{n\to \infty} g\left(\int_0^1 X_n(t) \beta(t) dt\right),\qquad X\in cl(\mathcal{K})\subset L^2([0,1])$$ where $\{X_n: \mathcal{K} \to \mathbb R, n\in \mathbb N\}$ is a sequence of functions uniformly convergent to $X$.

In this paper, we only consider approximating functional regression maps using neural networks. Based on our results, we find that the choice of $\Bar{t}$ and the smoothness of the kernel that induces a RKHS play important roles in improving the approximation abilities. To prove that neural networks can learn functional regression maps well, we shall extend our work to a generalization analysis in the future. Mathematically, the generalization error consists of the approximation error and the estimation error (based on training samples of independent copies of $(X,Y)$). We may assume that the slope function $\beta$ resides in RKHS in $L^2$ space to determine the estimation error. This is, in fact, a common assumption imposed in the statistics literature on the estimation and inference of FLM (see, ).

The next section discusses the general approaches and methods for establishing our theorems.

# Proof Sketch of Main Results

This section presents the general methods for approximating RKHS functionals by tanh neural networks.

Our proofs are constructive and rely on three key ideas: (i) the construction of a projection operator on RKHS using a set of nodal functions, (ii) the estimation of power functions, which is widely adopted in the kernel interpolation literature to quantify the worst-case uniform error between a function and its interpolant, and (iii) the approximation of Lipschitz continuous functions by tanh neural networks.

We begin our analysis with a decomposition of the uniform approximation error $\sup_{f\in \mathcal{K}} \left|F(f) - \widehat{G}(f(\Bar{t}))\right|$. Consider the following error decomposition: $$\begin{aligned}
\label{decomposition}
         \sup_{f\in \mathcal{K}}\left| F(f) - \widehat{G}(f(\Bar{t}))\right| &\leq& 
         \underbrace{\sup_{f\in \mathcal{K}}| F(f) - F(Pf)|}_{\text{error term (I)}} +  \underbrace{\sup_{f\in \mathcal{K}}\left|F(Pf) - \widehat{G}(f(\Bar{t}))\right|}_{\text{error term (II)}}, 
\end{aligned}$$ where $Pf: \mathcal{X} \to \mathbb R$ is a function carefully chosen.

We will first introduce our choice of $Pf$ in the Subsection and show that $\|f-Pf\|_\infty:= \max_{x\in \mathcal{X}}|f(x) - Pf(x)|$ can be estimated by the power function given in (). Then, we will estimate the power function for different reproducing kernels of our interest in Subsection . This is how we bound the error term (I). Next, to bound the second error term (II), we bound the operator norm $\|(K[\Bar{t}])^{-1 }\|_{op}$ (in Subsection ) and then apply some existing results in neural network approximation (in Subsection ).

## Approximation of Functions via Nodal functions

Recall that we choose a set of data locations $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$ such that the Gram matrix $K[\Bar{t} ] = (K(t_i, t_j))_{i,j=1}^N$ given in is invertible. We construct a set of *nodal functions* $\{\psi_i\}_{i=1}^N$ on $\mathcal{X}$ associated with $\Bar{t}$ as $$\psi_i(x) := \sum_{j=1}^N c_j K_{t_j}(x),\qquad   c_j\in \mathbb R,i=1,\ldots, N ,$$ such that $$\psi_i(t_j) = \begin{cases}
      1, & \text{if } j=i,\\
      0, & \text{if } j \neq i.
    \end{cases}$$ We can easily check that the set of nodal functions $\{\psi_i\}_{i=1}^N$ are linearly independent.

Next, with the chosen $\Bar{t}=\{t_i\}_{i=1}^N$, we define a projection operator $P: \mathcal{H}_K \rightarrow \mathcal{H}_K$ as $$Pf = \sum_{i=1}^N f(t_i)\psi_i, \qquad f\in \mathcal{H}_K.$$ $Pf$ is the orthogonal projection onto the finite-dimensional space $\text{span}\{K_{t_i}\}_{i=1}^N$, as shown in the following lemma about the interpolation and projection properties of $P$.

<div class="lemma">

**Lemma 16**.  * For any $f \in \mathcal{H}_K$ and $\Bar{t} \subset \mathcal{X}$, we have*

1.  *$f(t_\ell) = Pf(t_\ell), \qquad \ell = 1,\ldots, N,$ *

2.  *$\langle f -Pf, K_{t_\ell}\rangle_{\mathcal{H}_K} = 0, \qquad \ell = 1,\ldots, N.$ *

</div>

<div class="proof">

*Proof of Lemma .* The nodal functions $\{\psi_i\}_{i=1}^N$ can be expressed explicitly as $$\label{psi}
       \begin{bmatrix}
           \psi_1\\
           \vdots\\
           \psi_N
       \end{bmatrix}= ( K[\Bar{t} ])^{-1} \begin{bmatrix}
           K_{t_1}\\
           \vdots\\
           K_{t_N}
       \end{bmatrix}.$$ In this way, we have $$\begin{aligned}
       \begin{bmatrix}
           \psi_1\\
           \vdots\\
           \psi_N
       \end{bmatrix}(t_\ell) 
       &=& ( K[\Bar{t} ])^{-1} \begin{bmatrix}
           K(t_1,t_\ell)\\
           \vdots\\
            K(t_N,t_\ell)
       \end{bmatrix}\\
       &=&\left((K[\Bar{t} ])^{-1} K[\Bar{t} ]\right)_{\cdot , \ell}\qquad (\text{the $\ell$-th column of }(K[\Bar{t} ])^{-1} K[\Bar{t} ])\\
       &=&I_{\cdot , \ell}. \qquad (\text{the $\ell$-th column of identity matrix }I)
     
\end{aligned}$$ We can see that $$\psi_i(t_\ell) = \begin{cases}
      1, & \text{if } i=\ell,\\
      0, & \text{if } i \neq \ell.
    \end{cases}$$ Then we have $$\begin{aligned}
    Pf(t_\ell) = \sum_{i=1}^N f(t_i)\psi_i(t_\ell) = f(t_\ell)\psi_\ell(t_\ell) = f(t_\ell).
\end{aligned}$$ The proof of statement . is complete.

Recall the reproducing property of RKHS: $f(x) = \langle f, K_x \rangle_{\mathcal{H}_K}$. Since $f(t_\ell)- Pf(t_\ell) = 0$, we know $$\langle f,K_{t_\ell} \rangle_{\mathcal{H}_K} - \langle Pf,K_{t_\ell} \rangle_{\mathcal{H}_K}= f(t_\ell) - Pf(t_\ell) =0 \iff  \langle f-Pf,K_{t_\ell} \rangle_{\mathcal{H}_K}=0.$$ The proof of statement . is complete. ◻

</div>

The next lemma presents a uniform bound of $|f (x)- Pf(x)|$ for any $x\in \mathcal{X}$, implying that $Pf$ gives a good approximation of $f$ on $\mathcal{X}$. We use the power function given in () for the measurement: $$\epsilon_K(\Bar{t}):= \max_{x\in \mathcal{X}} \min_{c\in\mathbb R^N} \left\|K_x -  \sum_{i=1}^N c_i  K_{t_i}\right\|_{\mathcal{H}_K} =\max_{x\in \mathcal{X}} \min_{c\in\mathbb R^N} \left\{ K(x,x) -2 \sum_{i=1}^N c_i K(x,t_i)+ \sum_{i=1}^N \sum_{j=1}^N c_i  c_j K(t_i, t_j)\right\}^{\frac{1}{2}}.$$ Lemma is a well-known result in the kernel literature. We provide its proof for completeness in Appendix .

<div class="lemma">

**Lemma 17**.  * For any $\Bar{t} \subset \mathcal{X}$ and $\{\psi_i\}_{i=1}^N$ defined explicitly in (), we have $$\|f- Pf\|_\infty \leq \|f\|_{\mathcal{H}_K}  \epsilon_K(\Bar{t}), \qquad f\in \mathcal{H}_K.$$*

</div>

Recall that we assume, throughout the paper, $F$ is $s$-Hölder continuous for $0<s \leq 1$ with constant $C_F \geq 0$. As a consequence of Lemma , we know $$|F(f) - F(Pf)| \leq C_F \|f-Pf\|^s_\infty \leq C_F \|f\|^s_{\mathcal{H}_K}  (\epsilon_K(\Bar{t}))^s , \qquad \forall f \in \mathcal{H}_K.$$ We consider $\mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}$ with $\mathcal{H}_K$ induced by some Mercer kernel $K$. We then have, $$\label{eq:firstterm}
 \sup_{f\in \mathcal{K}}| F(f) - F(Pf)| \leq  C_F   (\epsilon_K(\Bar{t}))^s.$$ This is an upper bound for the error term (I) in () for a general Mercer kernel $K$. Notice that this is the first term in the error bound presented in in Theorem .

Now the important questions are: how should we choose $\Bar{t}=\{t_i\}_{i=1}^N$, and how do we estimate the power function $\epsilon_K(\Bar{t})$ of a Mercer kernel of interest?

## Estimation of the Power Function

From now on, we take a set of $N=(m+1)^d$ data locations $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X} = [0,1]^d$, for some $m\in\mathbb N$. This is our choice of $\Bar{t}$ in Theorem , , and . We can see that $$\max_{x\in \mathcal{X}} \min_{t\in \Bar{t}} \ d_\mathcal{X}(x,t) \rightarrow 0 \quad \text{ as } N \rightarrow \infty.$$

For any $x\in \mathcal{X}$, let $t_{j_x}$ be the closest point in $\Bar{t}$ to $x$, with $j_x \in \{1,\ldots, N\}$. Then, $$|x_k - (t_{j_x})_k| \leq \frac{1}{m}, \qquad k=1,\ldots, d$$ and $$\begin{aligned}
    d_\mathcal{X}(x, t_{j_x}) \leq \sqrt{d\left(\frac{1}{m}\right)^2} = \frac{\sqrt{d}}{m},
\end{aligned}$$ and thus the fill distance of $\Bar{t}$ is $$\label{fill_tbar}
    h_{\Bar{t}} = \sup_{x\in \mathcal{X}} d_\mathcal{X}(x,t_{j_x})\leq  \frac{\sqrt{d}}{m}.$$

Recall that we assume the Mercer kernel $K$ is $\alpha$-Hölder continuous for $0 < \alpha \leq 1$ with constant $C_K \geq 0$. The following lemma presents an estimate of the power function $\epsilon_K(\Bar{t})$ of a general $\alpha$-Hölder continuous Mercer kernel $K$, and a uniform bound of $|f-Pf|$ for $f\in \mathcal{H}_K$ induced by such a kernel $K$.

<div class="lemma">

**Lemma 18**.  * For $\mathcal{X} = [0,1]^d$, choose $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ for some $m\in \mathbb N$. We have $$\epsilon_K(\Bar{t}) \leq 2C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha \quad
\text { and thus } \quad
  \|f-Pf\|_\infty \leq 2C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.$$*

</div>

The proof of Lemma is given in Appendix . In the following, we give explicit bounds of $\|f-Pf\|_\infty$ for specific kernels of interest.

For an inverse multiquadric kernel given in (), an upper bound of $\epsilon_K(\Bar{t})$ and $\|f-Pf\|_\infty$ are established in for some positive constant $c$: $$\label{f-Pf_multi}
\epsilon_K(\Bar{t}) \leq e^{\frac{-cm}{\sqrt{d}}} \quad \text{and thus }\quad  
    \|f-Pf\|_\infty \leq e^{\frac{-cm}{\sqrt{d}}}\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.$$

For Gaussian kernel given in (), an upper bound of $\epsilon_K(\Bar{t})$ and $\|f-Pf\|_\infty$ are established in for some positive constant $c$: $$\epsilon_K(\Bar{t}) \leq e^{\frac{-cm\left|\log \left(\frac{\sqrt{d}}{m}\right)\right|}{\sqrt{d}}} \quad \text{and thus }\quad
    \|f-Pf\|_\infty \leq e^{\frac{-cm\left|\log \left(\frac{\sqrt{d}}{m}\right)\right|}{\sqrt{d}}}\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.$$ While $d\in \mathbb N$ represents the data dimension, $m\in \mathbb N$ measures the number of locations in $\Bar{t}$ and is a variable of our choice. If we choose $m$ such that $m>\sqrt{d}$, we have$\left|\log \left(\frac{\sqrt{d}}{m}\right)\right| = \log \left(\frac{m}{\sqrt{d}}\right)$ and thus $$\label{f-Pf_Gaussian}
    \|f-Pf\|_\infty \leq e^{\frac{-cm\log \left(\frac{m}{\sqrt{d}}\right)}{\sqrt{d}}}\|f\|_{\mathcal{H}_K}, \qquad f\in  \mathcal{H}_K.$$

We now turn our attention to the Sobolev Space $W^r_2(\mathcal{X})$ as an RKHS $\mathcal{H}_K$. According to , when $r>d/2$ and $r-d/2 \notin \mathbb N$, an upper bound of the power function is derived from the well-known Sobolev Embedding Theorem as $$\epsilon_K(\Bar{t})  \leq C^\prime_{r,d}d^{r-\frac{d}{2}}m^{d-2r},$$ where $C^\prime_{r,d}$ is some positive constant depending on $r$ and $d$. Thus, we have $$\label{f-Pf_Sobolev}
\|f-Pf\|_\infty  \leq C^\prime_{r,d}d^{r-\frac{d}{2}}m^{d-2r}\|f\|_{\mathcal{H}_K}.$$ Later, we will use the above uniform bounds of $\|f-Pf\|_\infty$ to prove our main theorems.

## A New Function $G$ and its Regularity

Now, we move on to bound the error term (II) in (): $\sup_{f\in \mathcal{K}}\left|F(Pf) - \widehat{G}(f(\Bar{t}))\right|$.

In order to approximate $F(Pf) = F\left(\sum_{i=1}^N f(t_i) \psi_i\right)$, we define another function $G:[0,1]^N \rightarrow \mathbb R$ by $$\label{G}
    G(c) = F\left(\sum_{i=1}^N c_i \psi_i\right), \qquad  c\in [0,1]^N.$$ Then, we have $$G(f(\Bar{t})) = F\left(\sum_{i=1}^N f(t_i) \psi_i\right) = F(Pf).$$

The idea of approximating the functional $F$ is by approximating the function $G$ using a fully-connected neural network. For this purpose, we need to figure out the regularity of $G$, presented in the following lemma. Its proof is relegated to Appendix .

<div class="lemma">

**Lemma 19** (Regularity of the function $G$).  * For any $x\in \mathcal{X}$ and $\Bar{t}=\{t_i\}_{i=1}^N \subset \mathcal{X}$, if the functional $F:\mathcal{K} \rightarrow \mathbb R$ is $s$-Hölder continuous for $0<s \leq 1$ with constant $C_F \geq 0$, then the associated function $G$ is $s$-Hölder continuous with constant $$C_G = C_F\left(1+ \|(K[\Bar{t}])^{-1 }\|_{op}\sqrt{N} C_K \left(h_{\Bar{t}}\right)^\alpha\right)^s,$$ where $\|T\|_{op} =\underset{\|x\|=1}{\sup} \|T(x)\|$ denotes the operator norm of $T$, $h_{\Bar{t}}$ denotes the fill distance of $\Bar{t}$ given in ().*

</div>

To know the Hölder constant $C_G$ explicitly, we proceed to compute the operator norm $\|(K[\Bar{t}])^{-1 }\|_{op}$. Since $K[\Bar{t}]$ is a real, symmetric matrix, it is diagonalizable, and so is $(K[\Bar{t}])^{-1 }$. Let $\{\lambda_1, \ldots, \lambda_N\}$ denote the non-increasing sequence of eigenvalues of $K[\Bar{t}]$. We know $$K[\Bar{t}]=V \Lambda V^{-1} \qquad \text{and } \qquad K[\Bar{t}]^{-1}=V \Lambda^{-1} V^{-1},$$ where $\Lambda$ is a $N\times N$ diagonal matrix whose diagonal entries are the eigenvalues $\{\lambda_1, \ldots, \lambda_N\}$, and the columns of $V$ are eigenvectors of $K[\Bar{t}]$. We then have $$\label{operatornorm}
    \|(K[\Bar{t}])^{-1}\|_{op} = \frac{1}{\min_{1 \leq j \leq N} \lambda_j} =: \frac{1}{\lambda_{N}},$$ where $\lambda_{N}$ is the smallest eigenvalue of $K[\Bar{t}]$.

Since $K[\Bar{t}] v = \lambda_{N} v$ for some eignevector $v=(v_1, \cdots, v_N)\in \mathbb R^N$ of norm $1$, we have $$\begin{aligned}
 \label{lambda_N}
    \lambda_{N} = \lambda_{N} v^T v = v^T K[\Bar{t}] v 
    = \sum_{i=1}^N  \sum_{\ell=1}^N v_i v_\ell K(t_i, t_\ell). 
\end{aligned}$$ In other words, to compute $\|(K[\Bar{t}])^{-1}\|_{op}$, it suffices to compute $\lambda_{N} = \sum_{i=1}^N  \sum_{\ell=1}^N v_i v_\ell K(t_i, t_\ell)$.

## Bounding the Operator Norm $\|(K[\Bar{t}])^{-1}\|_{op}$

In general, computing the operator norm $\|(K[\Bar{t}])^{-1}\|_{op}$ can be challenging, especially when we do not have information about the eigenvalues or eigenvectors of the Gram matrix $K[\Bar{t}]$. The lemma below tells us that if kernel $K$ is translation-invariant, we can bound $\|(K[\Bar{t}])^{-1}\|_{op}$ by the Fourier transform of $K$:

<div class="lemma">

**Lemma 20**.  * Let $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X}$ for some $m\in \mathbb N$. For a translation-invariant and reproducing kernel $K$ (or $\phi)$ defined over $\mathbb R^d$, we have $$\|(K[\Bar{t}])^{-1}\|_{op} = \frac{1}{\lambda_{N}} \leq  \frac{1}{m\Gamma_m} \quad \text{with } \Gamma_m :=  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi)> 0,$$ where $\lambda_{N}$ is the smallest eigenvalue of $K[\Bar{t}]$, $\widehat{\phi}$ is the Fourier transform of $\phi$.*

</div>

The proof of Lemma is relegated to Appendix .

Next, we apply Lemma and Lemma to bound $\|(K[\Bar{t}])^{-1}\|_{op}$ and subsequently estimate the regularity of $G$ corresponds to functional $F$ on Sobolev space, and on RKHS’s induced by the inverse multiquadric kernel and Gaussian kernel, respectively.

### Sobolev Space $W^r_2(\mathcal{X})$

From Subsection , we know that $W^r_2(\mathcal{X})$ with order $r>d/2$ is a RKHS induced by a translation-invariant kernel, whose Fourier transform is given by $\widehat \phi (\xi) = (1+|\xi|^2)^{-r}, \xi \in \mathbb R^d$. For $\widehat \phi$ and any $m\in \mathbb N$, $$\label{Gamma_m_Sob}
    \Gamma_m =  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi) \geq \left(1+ \frac{dm^2}{4}\right)^{-r}$$ because $|\xi|^2 \leq \frac{dm^2}{4}$ for $\xi \in [-\frac{m}{2},\frac{m}{2}]^d$. Using this fact, we establish the regularity of the function $G$ corresponding to a functional $F$ on $W^r_2(\mathcal{X})$, presented in the lemma below. Its proof is relegated to Appendix .

<div class="lemma">

**Lemma 21**.  * Let $d\in \mathbb N, r\in \mathbb R_+,  \mathcal{X} = [0,1]^d$, and $r-d/2>1$. Take $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X}$ for some $m\in \mathbb N$, and $N=(m+1)^d$. Consider $\mathcal{K} := \{f\in W^r_2(\mathcal{X}): \|f\|_{W^r_2(\mathcal{X})} \leq 1\}$. If $F: \mathcal{K} \to \mathbb R$ is $s$-Hölder continuous with $s \in (0,1]$ and constant $C_F\geq 0$, the corresponding function $G:[0,1]^N \to \mathbb R$ given by is $s$-Hölder continuous with constant $$C_G =  C_F C_{r,d,s} m^{2rs+\frac{ds}{2}-2s} d^{rs+\frac{s}{2}},$$ where $C_{r,d,s}$ is a positive constant depends only on $r, d$, and $s$.*

</div>

### Inverse Multiquadric Kernel

The inverse multiquadric kernel is a translation-invariant and reproducing kernel. We know $|u-v|\leq \underbrace{\sqrt{1+\cdots+1}}_{d \text{ times}}=\sqrt{d}$ for $u,v \in \mathcal{X}$. By the Mean Value Theorem, we have for $u,v,\Tilde{v} \in \mathcal{X}$, $$\begin{aligned}
\  | K(u,v) - K(u,\Tilde{v})| &= &\left|(\sigma^2 + |u-v|^2)^{-\beta}- (\sigma^2 + |u-\Tilde{v}|^2)^{-\beta}\right| \\
&\leq& \beta \sigma^{-2\beta-2} \left|\sigma^2 +|u-v|^2- \sigma^2 -|u-\Tilde{v}|^2\right|\\
&\leq& \beta   \sigma^{-2\beta-2} (|u-v|+|u-\Tilde{v}|)(|u-v|-|u-\Tilde{v}|)\\
&\leq&2\sqrt{d}\beta  \sigma^{-2\beta-2}|v-\Tilde{v}|.
\end{aligned}$$ We see that this kernel is $\alpha$-Hölder continuous for $\alpha = 1$ with constant $C_K = 2\sqrt{d}\beta  \sigma^{-2\beta-2}$. Based on this information, we can deduce the regularity of function $G$ (given by ) corresponds to the functional $F$ on inverse multiquadric kernel-induced RKHS’s. The proof of the following lemma is given in Appendix .

<div class="lemma">

**Lemma 22**.  * Let $d\in \mathbb N, \sigma,\beta >0, \mathcal{X} = [0,1]^d$, and $M_d = 12 \left(\frac{\pi \Gamma^2(\frac{d+2}{2})}{9}\right) \leq 6.38d$. Take $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \in \mathcal{X}$ for some $m\in \mathbb N$, and $N=(m+1)^d$. Consider $\mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}$ with $\mathcal{H}_K$ induced by a inverse multiquadric kernel. If $F: \mathcal{K}\to \mathbb R$ is $s$-Hölder continuous with $s \in (0,1]$ and constant $C_F\geq 0$, the function $G:[0,1]^N \rightarrow \mathbb R$ given by is $s$-Hölder continuous with constant $$\label{C_G_multi}
    C_G = C_F\left(1+ \frac{4d\beta \sigma^{-2\beta-2}}{C_{\sigma, \beta,d}}(2m)^{-\beta -\frac{1}{2}}e^{4\sigma M_d m}\right)^s,$$ where $C_{\sigma, \beta,d}$ is a constant depending only on $\sigma, \beta,d$.*

</div>

### Gaussian Kernel

The Gaussian kernel $K(u,v)= e^{-\frac{|u-v|^2}{2\sigma^2}}$ for $u,v \in \mathcal{X}$ is translation-invariant and reproducing, corresponding to $\phi(x) = e^{-\frac{x^2}{2\sigma^2}}$. Then, by the Mean Value Theorem, we have for $u,v,\Tilde{v} \in \mathcal{X}$, $$\begin{aligned}
\  | K(u,v) - K(u,\Tilde{v})| &=& \left|e^{-\frac{|u-v|^2}{2\sigma^2}} - e^{-\frac{|u-\Tilde{v}|^2}{2\sigma^2}}\right| \\
&\leq& 1 \cdot \left|\frac{|u-v|^2}{2\sigma^2} - \frac{|u-\Tilde{v}|^2}{2\sigma^2}\right|\\
&\leq&\frac{|u-v|+|u-\Tilde{v}|}{2\sigma^2} (|u-v|-|u-\Tilde{v}|)\\
&\leq& \frac{\sqrt{d}}{\sigma^2}|v-\Tilde{v}|.
\end{aligned}$$ We see that the Gaussian kernel is $\alpha$-Hölder continuous for $\alpha = 1$ with constant $C_K = \frac{\sqrt{d}}{\sigma^2}$. The following lemma presents the regularity of function $G$ corresponding to functional $F$ on Gaussian kernel-induced RKHS’s. Its proof is relegated to Appendix .

<div class="lemma">

**Lemma 23**.  * Let $d\in \mathbb N, \sigma >0,$ and $\mathcal{X} = [0,1]^d$. Take $\Bar{t}=\{ 0, \frac{1}{m},\ldots, \frac{m}{m}\}^d \in \mathcal{X}$ for some $m\in \mathbb N$, and $N=(m+1)^d$. Consider $\mathcal{K} := \{f\in \mathcal{H}_K: \|f\|_{\mathcal{H}_K} \leq 1\}$ with $\mathcal{H}_K$ induced by a Gaussian kernel. If $F: \mathcal{K}\to \mathbb R$ is $s$-Hölder continuous with $s \in (0,1]$ and constant $C_F\geq 0$, the corresponding function $G:[0,1]^N \rightarrow \mathbb R$ given by is $s$-Hölder continuous with constant $$\label{C_G_Gaussian}
    C_G = C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2},$$ where $C_{\sigma,d,s}$ is a constant depending only on $\sigma,d,$ and $s$.*

</div>

Now that we have established the regularity of function $G$, we proceed to approximate $G$ using tanh neural networks.

## Approximation of Function using Tanh Neural Networks

Here, we would like to apply a recent result in approximating Lipschitz continuous functions by tanh neural network from . We will use this existing result to approximate the function $G$ using two-layer tanh neural networks.

<div class="lemma">

**Lemma 24** (Corollary 5.4 in ).  * Let $N\in \mathbb N$ and let $f:[0,1]^N \rightarrow \mathbb R$ be a Lipschitz continuous function with Lipschitz constant $L>0$. For every $M\in \mathbb N$ with $M>5N^2$, there exists a tanh neural network $\widehat{f}$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil    
\begin{pmatrix}
        2N-1\\
        N
    \end{pmatrix}M^N$ (or $M-1$ and $6M$ for $N=1$), such that $$\begin{aligned}
        \left\|\widehat{f}-f\right\|_{L^\infty [0,1]^N} \leq \frac{7N^2L}{M}. 
    
\end{aligned}$$*

</div>

<div class="remark">

**Remark 25**. *It is shown in that $\begin{pmatrix}
        2N-1\\
        N
    \end{pmatrix}< 5^N$. The total number of parameters in the network is at most $\mathcal{O}(N(5M)^N)$ with $M>5N^2$.*

</div>

We first present an approximation result regarding a general translation-invariant Mercer kernel $K$ and $\mathcal{X}=[0,1]^d$.

<div class="theorem">

**Theorem 26**.  * Let $m\in \mathbb N, d\in \mathbb N, \mathcal{X}=[0,1]^d$, and $K$ is a translation-invariant kernel. Also let $\Gamma_m =  \min_{\xi \in [-\frac{m}{2},\frac{m}{2}]^d} \widehat{\phi}(\xi)> 0$. We take a set of $N=(m+1)^d$ points $\Bar{t}=\{0, \frac{1}{m},\ldots, \frac{m}{m}\}^d$ on $\mathcal{X}$. Consider the function $G:[-1,1]^N \rightarrow \mathbb R$ defined in (). It follows from Lemma and Lemma that $G$ is $s$-Hölder continuous for $0 \leq s \leq 1$ with constant $$C_G= C_F\left(1+ \frac{1}{m\Gamma_m}\sqrt{N} C_K \left(\frac{\sqrt{d}}{m}\right)^\alpha\right)^s.$$ From Lemma , for every $M\in \mathbb N$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$, such that $$\begin{aligned}
   \left \|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} \leq \frac{7N^2C_G}{M}. 
\end{aligned}$$*

</div>

As consequences of the above Theorem , we can derive explicit error bounds on the approximation of $G$ functions induced from Reproducing kernel in Sobolev space $W^r_2(\mathcal{X})$ (Corollary ), inverse multiquadric kernel (Corollary ), and Gaussian Kernel (Corollary ), respectively.

<div class="corollary">

**Corollary 27** (Reproducing kernel in Sobolev space $W^r_2(\mathcal{X})$).  * Suppose $K$ is a reproducing kernel in $W^r_2(\mathcal{X})$. We take $\mathcal{X}, N,$ and $\Bar{t}$ as in Theorem . Consider the function $G:[-1,1]^N \rightarrow \mathbb R$ defined in (). We know that the corresponding $G$ function is $s$-Holder with constant $$C_G =  C_F C_{r,d,s} m^{2rs+\frac{ds}{2}-2s} d^{rs+\frac{s}{2}}.$$ For every $M\in \mathbb N$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$ (or $M-1$ and $6M$ for $N=1$), such that $$\begin{aligned}
    \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} =  \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}} \leq \frac{7(m+1)^{2d}}{M}C_F C_{r,d,s} m^{2rs+\frac{ds}{2}-2s} d^{rs+\frac{s}{2}}. 
\end{aligned}$$*

</div>

<div class="corollary">

**Corollary 28** (Inverse multiquadric kernel).  * Suppose $K$ is an inverse multiquadric kernel given by (). We take $\mathcal{X}, N,$ and $\Bar{t}$ as in Theorem . Consider the function $G:[-1,1]^N \rightarrow \mathbb R$ defined in (). From Lemma and (), the function $G$ is $s$-Hölder for $0 \leq s \leq 1$ with constant $$C_G = C_FC_{\sigma, \beta,d,s}(2m)^{-s\beta -\frac{s}{2}}e^{4\sigma M_d sm},$$ where $M_d = 12 \left(\frac{\pi \Gamma^2(\frac{d+2}{2})}{9}\right) \leq 6.38d$. For every $M\in \mathbb N$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$, such that $$\begin{aligned}
    \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} =  \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}} \leq \frac{7(m+1)^{2d}(2m)^{-s\beta-s/2}C_FC_{\sigma, \beta,d,s}e^{4\sigma M_d sm}}{M}. 
\end{aligned}$$*

</div>

<div class="corollary">

**Corollary 29** (Gaussian kernel).  * Suppose $K$ is a Gaussian kernel given in (). We take $\mathcal{X}, N,$ and $\Bar{t}$ as in Theorem . Consider the function $G:[-1,1]^N \rightarrow \mathbb R$ defined in (). It follows from Lemma and () that $G$ is $s$-Hölder continuous for $0 \leq s \leq 1$ with constant $$C_G = C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}.$$ For every $M\in \mathbb N$ with $M>5N^2$, there exists a tanh neural network $\widehat{G}:[-1,1]^N \rightarrow \mathbb R$ with two hidden layers of widths at most $N(M-1)$ and $3\left\lceil \frac{N+1}{2} \right\rceil (5M)^N$, such that $$\begin{aligned}
    \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^N} =  \left\|\widehat{G}-G\right\|_{L^\infty [-1,1]^{(m+1)^{d}}} 
    \leq \frac{7(m+1)^{2d}}{M} C_FC_{\sigma,d,s}e^{\sigma^2\pi^2 dsm^2}. 
\end{aligned}$$*

</div>

# Remarks and Conclusions

While it is widely acknowledged that neural networks are universal function approximators, whether such universality can be extended to infinite-dimensional spaces remains unknown. This paper investigates the approximation theory of functionals, which are mappings from a space of functions to $\mathbb R$.

In summary, we establish explicit error bounds, with respect to the number of network parameters, on the approximations of functionals on Sobolev space and RKHS’s induced by the inverse multiquadric kernel and Gaussian kernel, respectively. We adopt standard neural networks equipped with tanh activation functions, commonly used in operator learning these days . Our main theorems suggest that such networks can achieve any arbitrary approximation accuracy if there are sufficiently many network parameters. We achieve near polynomial approximation rates for approximating RKHS functionals induced by the inverse multiquadric and Gaussian kernels. Our results have shown improvement compared to the previous rates given in and .

Most existing works require a set of handcrafted basis functions to convert functional inputs to vectors via basis function expansion; our design evaluates the input function at some discrete points in the domain where uniform spacing is not necessary. Our method is much simpler than those in the literature.

Moreover, we present a general error bound on approximating functionals on the RKHS induced by some general Mercer kernel. This error bound exhibits a trade-off associated with $N$, which denotes the number of points we use to evaluate our input function. When $N$ is too small, we observe an increase in the power function (measures the distance between a function and its interpolant), while with a very large $N$, the network size becomes too big and results in overfitting. We shall choose an optimal $N$ to strike a balance. In the previously mentioned theorems, we present our choice of $N$; its derivation is given in the proofs. Furthermore, we apply our findings to functional regression, proving that tanh neural networks can accurately approximate the regression maps in generalized functional linear models. This novel result provides an insight that deep learning may be applied successfully in functional data analysis.

To our knowledge, this work is the first to study the approximation of functionals on RKHS’s. All existing works in this regard focus on functionals on $L^p$ spaces. We also want to highlight that no method other than neural networks, such as spline or sieve, has been studied for approximating functionals. By considering RKHS, our methods allow extensions to low-dimensional manifolds embedded in Euclidean spaces. This is beyond the scope of this paper with $\mathcal{X} = [0,1]^d$ and will be discussed in our further study.

# Acknowledgments and Disclosure of Funding

This material is based upon work supported by the National Science Foundation under grant no. 2229876 is partly supported by funds provided by the National Science Foundation, the Department of Homeland Security, and IBM. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or its federal agency and industry partners. Tian-Yi Zhou and Xiaoming Huo are also partially sponsored by NSF grants DMS 2015363 and the A. Russell Chandler III Professorship at Georgia Institute of Technology. Guang Cheng and Namjoon Suh are partially supported by the Office of Naval Research under ONR N00014-22-1-2680 and the National Science Foundation under SCALE MoDL 2134209. Namjoon Suh is also partially supported by the Institute of Digital Research & Education (IDRE) Postdoctoral Fellowship at UCLA.
