\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos]{semdedup}
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari~S. Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication.
\newblock \emph{arXiv}, 2023.

\bibitem[Ansel et~al.(2024)Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao, Bell, Berard, Burovski, Chauhan, Chourdia, Constable, Desmaison, DeVito, Ellison, Feng, Gong, Gschwind, Hirsh, Huang, Kalambarkar, Kirsch, Lazos, Lezcano, Liang, Liang, Lu, Luk, Maher, Pan, Puhrsch, Reso, Saroufim, Siraichi, Suk, Suo, Tillet, Wang, Wang, Wen, Zhang, Zhao, Zhou, Zou, Mathews, Chanan, Wu, and Chintala]{pytorch}
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK~Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos~Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu~Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala.
\newblock {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}.
\newblock \emph{ACM International Conference on Architectural Support for Programming Languages and Operating Systems}, 2024.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock \emph{arXiv}, 2023.

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{openllmleaderboard}
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf, 2023.
\newblock URL \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}.
\newblock Open {LLM} Leaderboard.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal]{pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der Wal.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock \emph{arXiv}, 2023.

\bibitem[BigScience(2023)]{bloom}
BigScience.
\newblock {BLOOM}: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv}, 2023.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{piqa}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock \emph{AAAI}, 2020.

\bibitem[Black(2023)]{black2023}
Sid Black, 2023.
\newblock URL \url{https://huggingface.co/datasets/EleutherAI/lambada_openai}.
\newblock Multilingual LAMBADA.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv}, 2020.

\bibitem[Chen \& Banerjee(2017)Chen and Banerjee]{chen2017robust}
Sheng Chen and Arindam Banerjee.
\newblock Robust structured estimation with single-index models.
\newblock \emph{ICML}, 2017.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? {Try ARC}, the {AI2} reasoning challenge.
\newblock \emph{arXiv}, 2018.

\bibitem[Duchi et~al.(2008)Duchi, Shalev-Shwartz, Singer, and Chandra]{duchi2008efficient}
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.
\newblock Efficient projections onto the l1-ball for learning in high dimensions.
\newblock \emph{ICML}, 2008.

\bibitem[Engstrom et~al.(2024)Engstrom, Feldmann, and Madry]{dsdm}
Logan Engstrom, Axel Feldmann, and Aleksander Madry.
\newblock Dsdm: Model-aware dataset selection with datamodels.
\newblock \emph{arXiv}, 2024.

\bibitem[Everaert \& Potts(2024)Everaert and Potts]{gio}
Dante Everaert and Christopher Potts.
\newblock Gio: Gradient information optimization for training dataset selection.
\newblock \emph{ICLR}, 2024.

\bibitem[Fan et~al.(2019)Fan, Jernite, Perez, Grangier, Weston, and Auli]{eli5}
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.
\newblock {ELI5:} long form question answering.
\newblock \emph{arXiv}, 2019.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv}, 2020.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation.
\newblock \emph{Zenodo}, 2023.

\bibitem[Geng \& Liu(2023)Geng and Liu]{openllama}
Xinyang Geng and Hao Liu.
\newblock Openllama: An open reproduction of llama, 2023.
\newblock URL \url{https://github.com/openlm-research/open_llama}.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu, Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik, Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Strubell, Subramani, Wortsman, Dasigi, Lambert, Richardson, Zettlemoyer, Dodge, Lo, Soldaini, Smith, and Hajishirzi]{olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi~Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah~A. Smith, and Hannaneh Hajishirzi.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv}, 2024.

\bibitem[Gu \& Dao(2024)Gu and Dao]{mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv}, 2024.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, Salim, Shah, Behl, Wang, Bubeck, Eldan, Kalai, Lee, and Li]{phi}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio César~Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat~Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need.
\newblock \emph{arXiv}, 2023.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre]{chinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv}, 2022.

\bibitem[Ilyas et~al.(2022)Ilyas, Park, Engstrom, Leclerc, and Madry]{datamodels}
Andrew Ilyas, Sung~Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry.
\newblock Datamodels: Predicting predictions from training data.
\newblock \emph{ICML}, 2022.

\bibitem[Joulin et~al.(2016)Joulin, Grave, Bojanowski, and Mikolov]{fasttext}
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.
\newblock Bag of tricks for efficient text classification.
\newblock \emph{arXiv}, 2016.

\bibitem[Kalai \& Sastry(2009)Kalai and Sastry]{isotron}
Adam~Tauman Kalai and Ravi Sastry.
\newblock The isotron algorithm: High-dimensional isotonic regression.
\newblock \emph{COLT}, 2009.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv}, 2020.

\bibitem[Laurençon et~al.(2022)Laurençon, Saulnier, Wang, Akiki, del Moral, Scao, Werra, Mou, Ponferrada, Nguyen, Frohberg, Šaško, Lhoest, McMillan-Major, Dupont, Biderman, Rogers, allal, Toni, Pistilli, Nguyen, Nikpoor, Masoud, Colombo, de~la Rosa, Villegas, Thrush, Longpre, Nagel, Weber, Muñoz, Zhu, Strien, Alyafeai, Almubarak, Vu, Gonzalez-Dios, Soroa, Lo, Dey, Suarez, Gokaslan, Bose, Adelani, Phan, Tran, Yu, Pai, Chim, Lepercq, Ilic, Mitchell, Luccioni, and Jernite]{roots}
Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert~Villanova del Moral, Teven~Le Scao, Leandro~Von Werra, Chenghao Mou, Eduardo~González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Šaško, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Loubna~Ben allal, Francesco~De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de~la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Muñoz, Jian Zhu, Daniel~Van Strien, Zaid Alyafeai, Khalid Almubarak, Minh~Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro~Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha~Alexandra Luccioni, and Yacine Jernite.
\newblock The bigscience roots corpus: A 1.6tb composite multilingual dataset.
\newblock \emph{NeurIPS Datasets and Benchmarks}, 2022.

\bibitem[Li et~al.(2024)Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora, Garg, Xin, Muennighoff, Heckel, Mercat, Chen, Gururangan, Wortsman, Albalak, Bitton, Nezhurina, Abbas, Hsieh, Ghosh, Gardner, Kilian, Zhang, Shao, Pratt, Sanyal, Ilharco, Daras, Marathe, Gokaslan, Zhang, Chandu, Nguyen, Vasiljevic, Kakade, Song, Sanghavi, Faghri, Oh, Zettlemoyer, Lo, El-Nouby, Pouransari, Toshev, Wang, Groeneveld, Soldaini, Koh, Jitsev, Kollar, Dimakis, Carmon, Dave, Schmidt, and Shankar]{datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang~Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros~G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock \emph{arXiv}, 2024.

\bibitem[Liu et~al.(2024)Liu, Zheng, Muennighoff, Zeng, Dou, Pang, Jiang, and Lin]{regmix}
Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin.
\newblock Regmix: Data mixture as regression for language model pre-training.
\newblock \emph{arXiv}, 2024.

\bibitem[{Llama Team}(2024)]{llama3}
{Llama Team}.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv}, 2024.

\bibitem[Ng \& Geller(1968)Ng and Geller]{ng1968table}
Edward~W. Ng and Murray Geller.
\newblock A table of integrals of the error functions.
\newblock \emph{Journal of Research of the Natianal Bureau of Standards, Section B: Mathematical Sciences}, 1968.

\bibitem[Owen(2024)]{owen2024predictable}
David Owen.
\newblock How predictable is language model benchmark performance?
\newblock \emph{arXiv}, 2024.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'{a}}ndez]{paperno2016lambada}
Denis Paperno, Germ{\'{a}}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'{a}}ndez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock \emph{ACL}, 2016.

\bibitem[Parmar et~al.(2024)Parmar, Prabhumoye, Jennings, Liu, Jhunjhunwala, Wang, Patwary, Shoeybi, and Catanzaro]{parmar2024datadataeverywhereguide}
Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo~Liu, Aastha Jhunjhunwala, Zhilin Wang, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Data, data everywhere: A guide for pretraining dataset construction.
\newblock \emph{arXiv}, 2024.

\bibitem[Pearson(1901)]{pca}
Karl Pearson.
\newblock On lines and planes of closest fit to systems of points in space.
\newblock \emph{Philosophical Magazine}, 1901.

\bibitem[Penedo et~al.(2024)Penedo, Kydlíček, allal, Lozhkov, Mitchell, Raffel, Werra, and Wolf]{fineweb}
Guilherme Penedo, Hynek Kydlíček, Loubna~Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro~Von Werra, and Thomas Wolf.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.
\newblock \emph{arXiv}, 2024.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Biderman, Cao, Cheng, Chung, Grella, GV, He, Hou, Lin, Kazienko, Kocon, Kong, Koptyra, Lau, Mantri, Mom, Saito, Song, Tang, Wang, Wind, Wozniak, Zhang, Zhang, Zhao, Zhou, Zhou, Zhu, and Zhu]{rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri~Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan~S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock \emph{arXiv}, 2023.

\bibitem[Plan et~al.(2016)Plan, Vershynin, and Yudovina]{plan2016high}
Yaniv Plan, Roman Vershynin, and Elena Yudovina.
\newblock High-dimensional estimation with geometric constraints.
\newblock \emph{Information and Inference: A Journal of the {IMA}}, 2016.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and Ré]{hyena}
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel~Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré.
\newblock Hyena hierarchy: Towards larger convolutional language models.
\newblock \emph{arXiv}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 1--67, 2020.

\bibitem[Ruan et~al.(2024)Ruan, Maddison, and Hashimoto]{ruan2024observationalscalinglawspredictability}
Yangjun Ruan, Chris~J. Maddison, and Tatsunori Hashimoto.
\newblock Observational scaling laws and the predictability of language model performance.
\newblock \emph{arXiv}, 2024.

\bibitem[Shalev-Shwartz \& Singer(2006)Shalev-Shwartz and Singer]{shalev2006efficient}
Shai Shalev-Shwartz and Yoram Singer.
\newblock Efficient learning of label ranking by soft projections onto polyhedra.
\newblock \emph{JMLR}, 2006.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, Hofmann, Jha, Kumar, Lucy, Lyu, Lambert, Magnusson, Morrison, Muennighoff, Naik, Nam, Peters, Ravichander, Richardson, Shen, Strubell, Subramani, Tafjord, Walsh, Zettlemoyer, Smith, Hajishirzi, Beltagy, Groeneveld, Dodge, and Lo]{dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya~Harsh Jha, Sachin Kumar, Li~Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah~A. Smith, Hannaneh Hajishirzi, Iz~Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.
\newblock Dolma: an open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv}, 2024.

\bibitem[Spearman(1904)]{spearmanr}
Charles Spearman.
\newblock \emph{The Proof and Measurement of Association between Two Things}.
\newblock The American Journal of Psychology, 1904.

\bibitem[Teknium(2023)]{oh2.5}
Teknium.
\newblock Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.
\newblock URL \url{https://huggingface.co/datasets/teknium/OpenHermes-2.5}.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[{Together Computer}(2023)]{redpajama}
{Together Computer}, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.
\newblock {RedPajama}: an Open Dataset for Training Large Language Models.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv}, 2023.

\bibitem[van~der Maaten \& Hinton(2008)van~der Maaten and Hinton]{tsne}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using {t-SNE}.
\newblock \emph{JMLR}, 2008.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{TMLR}, 2022.

\bibitem[Welbl et~al.(2017)Welbl, Liu, and Gardner]{sciq}
Johannes Welbl, Nelson~F. Liu, and Matt Gardner.
\newblock Crowdsourcing multiple choice science questions.
\newblock \emph{W-NUT}, 2017.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, and Brew]{transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'{e}}mi Louf, Morgan Funtowicz, and Jamie Brew.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv}, 2019.

\bibitem[Wooldridge(2010)]{ordinalregression}
Jeffrey~M. Wooldridge.
\newblock \emph{Econometric Analysis of Cross Section and Panel Data}.
\newblock MIT Press, 2010.

\bibitem[Xie et~al.(2023{\natexlab{a}})Xie, Pham, Dong, Du, Liu, Lu, Liang, Le, Ma, and Yu]{doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc~V. Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model pretraining.
\newblock \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Xie et~al.(2023{\natexlab{b}})Xie, Santurkar, Ma, and Liang]{dsir}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{NeurIPS}, 2023{\natexlab{b}}.

\end{thebibliography}
