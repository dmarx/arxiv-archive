@article{chen2017robust,
author = {Sheng Chen and Arindam Banerjee},
journal = {ICML},
title = {Robust Structured Estimation with Single-Index Models},
year = {2017}
}

@misc{black2023,
author = {Sid Black},
year = {2023},
note = {Multilingual LAMBADA},
url = {https://huggingface.co/datasets/EleutherAI/lambada_openai},
}

@article{paperno2016lambada,
  author       = {Denis Paperno and
                  Germ{\'{a}}n Kruszewski and
                  Angeliki Lazaridou and
                  Quan Ngoc Pham and
                  Raffaella Bernardi and
                  Sandro Pezzelle and
                  Marco Baroni and
                  Gemma Boleda and
                  Raquel Fern{\'{a}}ndez},
  title        = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
  journal      = {ACL},
  year         = {2016},
}

@article{sciq,
  author       = {Johannes Welbl and
                  Nelson F. Liu and
                  Matt Gardner},
  title        = {Crowdsourcing Multiple Choice Science Questions},
  journal      = {W-NUT},
  year         = {2017},
}

@article{piqa,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  journal      = {AAAI},
  year         = {2020},
}

@article{arc,
  author       = {Peter Clark and
                  Isaac Cowhey and
                  Oren Etzioni and
                  Tushar Khot and
                  Ashish Sabharwal and
                  Carissa Schoenick and
                  Oyvind Tafjord},
  title        = {Think you have Solved Question Answering? {Try ARC}, the {AI2} Reasoning Challenge},
  journal      = {arXiv},
  year         = {2018},
}

@article{pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      journal={arXiv},
}

@misc{openllmleaderboard,
note={Open {LLM} Leaderboard},
author={Edward Beeching and Clémentine Fourrier and Nathan Habib and Sheon Han and Nathan Lambert and Nazneen Rajani and Omar Sanseviero and Lewis Tunstall and Thomas Wolf},
year={2023},
url={https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
}

@misc{redpajama,
  author = {{Together Computer}},
  note = {{RedPajama}: an Open Dataset for Training Large Language Models},
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@article{ruan2024observationalscalinglawspredictability,
      title={Observational Scaling Laws and the Predictability of Language Model Performance}, 
      author={Yangjun Ruan and Chris J. Maddison and Tatsunori Hashimoto},
      year={2024},
      journal={arXiv},
}

@article{dsir,
      title={Data Selection for Language Models via Importance Resampling}, 
      author={Sang Michael Xie and Shibani Santurkar and Tengyu Ma and Percy Liang},
      year={2023},
      journal={NeurIPS},
}

@article{semdedup,
      title={SemDeDup: Data-efficient learning at web-scale through semantic deduplication}, 
      author={Amro Abbas and Kushal Tirumala and Dániel Simig and Surya Ganguli and Ari S. Morcos},
      year={2023},
      journal={arXiv},
}

@article{gio,
 author = {Dante Everaert and Christopher Potts},
 title = {GIO: Gradient information optimization for training dataset selection},
 year = {2024},
 journal = {ICLR},
}

@article{datacomp,
      title={DataComp-LM: In search of the next generation of training sets for language models}, 
      author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar},
      year={2024},
      journal={arXiv},
}

@article{datamodels,
      title={Datamodels: Predicting Predictions from Training Data}, 
      author={Andrew Ilyas and Sung Min Park and Logan Engstrom and Guillaume Leclerc and Aleksander Madry},
      year={2022},
      journal={ICML},
}

@article{doremi,
      title={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining}, 
      author={Sang Michael Xie and Hieu Pham and Xuanyi Dong and Nan Du and Hanxiao Liu and Yifeng Lu and Percy Liang and Quoc V. Le and Tengyu Ma and Adams Wei Yu},
      year={2023},
      journal={NeurIPS},
}

@article{wei2022emergent,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      journal={TMLR},
}

@article{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      journal={arXiv},
}

@article{owen2024predictable,
      title={How predictable is language model benchmark performance?}, 
      author={David Owen},
      year={2024},
      journal={arXiv},
}

@article{bloom,
      title={{BLOOM}: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience},
      year={2023},
      journal={arXiv},
}

@article{tsne,
    title={Visualizing Data using {t-SNE}},
    author={Laurens van der Maaten and Geoffrey Hinton},
    year={2008},
    journal={JMLR},
}

@article{pca,
    title={On Lines and Planes of Closest Fit to Systems of Points in Space},
    author={Karl Pearson},
    year={1901},
    journal={Philosophical Magazine},
}

@article{dolma,
      title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}, 
      author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
      year={2024},
      journal={arXiv},
}

@article{olmo,
      title={OLMo: Accelerating the Science of Language Models}, 
      author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      journal={arXiv},
}

@misc{paloma,
      title={Paloma: A Benchmark for Evaluating Language Model Fit}, 
      author={Ian Magnusson and Akshita Bhagia and Valentin Hofmann and Luca Soldaini and Ananya Harsh Jha and Oyvind Tafjord and Dustin Schwenk and Evan Pete Walsh and Yanai Elazar and Kyle Lo and Dirk Groeneveld and Iz Beltagy and Hannaneh Hajishirzi and Noah A. Smith and Kyle Richardson and Jesse Dodge},
      year={2023},
      journal={arXiv},
}

@article{pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      journal={arXiv},
}

@article{raffel2019exploring,
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	journal = {Journal of Machine Learning Research},
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	volume = {1--67},
	year = {2020}}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Oxford University Press}
}

@article{roots,
      title={The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset}, 
      author={Hugo Laurençon and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo González Ponferrada and Huu Nguyen and Jörg Frohberg and Mario Šaško and Quentin Lhoest and Angelina McMillan-Major and Gerard Dupont and Stella Biderman and Anna Rogers and Loubna Ben allal and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Muñoz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Minh Chien Vu and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Alexandra Luccioni and Yacine Jernite},
      year={2022},
      journal={NeurIPS Datasets and Benchmarks}
}

@article{llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      journal={arXiv},
}

@article{llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      journal={arXiv},
}

@article{mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      journal={arXiv},
}

@article{gpt,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      journal={arXiv},
}

@article{dsdm,
      title={DsDm: Model-Aware Dataset Selection with Datamodels}, 
      author={Logan Engstrom and Axel Feldmann and Aleksander Madry},
      year={2024},
      journal={arXiv},
}

@article{fineweb,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}, 
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      journal={arXiv},
}

@article{datasets,
      title={Datasets: A Community Library for Natural Language Processing}, 
      author={Quentin Lhoest and Albert Villanova del Moral and Yacine Jernite and Abhishek Thakur and Patrick von Platen and Suraj Patil and Julien Chaumond and Mariama Drame and Julien Plu and Lewis Tunstall and Joe Davison and Mario Šaško and Gunjan Chhablani and Bhavitvya Malik and Simon Brandeis and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angelina McMillan-Major and Philipp Schmid and Sylvain Gugger and Clément Delangue and Théo Matussière and Lysandre Debut and Stas Bekman and Pierric Cistac and Thibault Goehringer and Victor Mustar and François Lagunas and Alexander M. Rush and Thomas Wolf},
      year={2021},
      journal={EMNLP System Demos},
}

@article{chinchilla,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      journal={arXiv}
}

@article{regmix,
      title={RegMix: Data Mixture as Regression for Language Model Pre-training}, 
      author={Qian Liu and Xiaosen Zheng and Niklas Muennighoff and Guangtao Zeng and Longxu Dou and Tianyu Pang and Jing Jiang and Min Lin},
      year={2024},
      journal={arXiv},
}

@article{transformers,
  author       = {Thomas Wolf and
                  Lysandre Debut and
                  Victor Sanh and
                  Julien Chaumond and
                  Clement Delangue and
                  Anthony Moi and
                  Pierric Cistac and
                  Tim Rault and
                  R{\'{e}}mi Louf and
                  Morgan Funtowicz and
                  Jamie Brew},
  title        = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal      = {arXiv},
  year         = {2019},
}

@article{plan2016high,
      title={High-dimensional estimation with geometric constraints}, 
      author={Yaniv Plan and Roman Vershynin and Elena Yudovina},
      year={2016},
      journal={Information and Inference: A Journal of the {IMA}} 
}

@article{hyena,
      title={Hyena Hierarchy: Towards Larger Convolutional Language Models}, 
      author={Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher Ré},
      year={2023},
      journal={arXiv}, 
}

@article{rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      journal={arXiv},
}

@article{mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      journal={arXiv},
}

@article{fasttext,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv},
  year={2016}
}

@article{duchi2008efficient,
    title	= {Efficient projections onto the l1-ball for learning in high dimensions},
    author	= {John Duchi and Shai Shalev-Shwartz and Yoram Singer and Tushar Chandra},
    year	= {2008},
    journal	= {ICML}
}

@article{shalev2006efficient,
  title={Efficient learning of label ranking by soft projections onto polyhedra},
  author={Shai Shalev-Shwartz and Yoram Singer},
  journal={JMLR},
  year={2006}
}

@article{ng1968table,
    title={A Table of Integrals of the Error Functions},
    author={Edward W. Ng and Murray Geller},
    journal={Journal of Research of the Natianal Bureau of Standards, Section B: Mathematical Sciences},
    year={1968}
}

@article{pandey2024gzip,
      title={gzip Predicts Data-dependent Scaling Laws}, 
      author={Rohan Pandey},
      year={2024},
      journal={arXiv},
}

@article{llama3,
      title={The Llama 3 Herd of Models}, 
      author={{Llama Team}},
      year={2024},
      journal={arXiv},
}

@book{ordinalregression,
    author={Wooldridge, Jeffrey M.},
    year={2010},
    title={Econometric Analysis of Cross Section and Panel Data},
    publisher={MIT Press},
}

@article{isotron,
    author={Adam Tauman Kalai and Ravi Sastry},
    title={The Isotron Algorithm: High-Dimensional Isotonic Regression},
    year={2009},
    journal={COLT},
}

@book{spearmanr,
    author={Charles Spearman},
    title={The Proof and Measurement of Association between Two Things},
    year={1904},
    publisher={The American Journal of Psychology},
}

@article{pytorch,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
journal = {ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
title = {{PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}},
year = {2024}
}

@article{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  year         = 2023,
  journal    = {Zenodo},
}

@article{qwen,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
      year={2023},
      journal={arXiv},
}

@article{phi,
      title={Textbooks Are All You Need}, 
      author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio César Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Harkirat Singh Behl and Xin Wang and Sébastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuanzhi Li},
      year={2023},
      journal={arXiv},
}

@misc{openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@article{parmar2024datadataeverywhereguide,
      title={Data, Data Everywhere: A Guide for Pretraining Dataset Construction}, 
      author={Jupinder Parmar and Shrimai Prabhumoye and Joseph Jennings and Bo Liu and Aastha Jhunjhunwala and Zhilin Wang and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro},
      year={2024},
      journal={arXiv},
}

@article{eli5,
  author       = {Angela Fan and
                  Yacine Jernite and
                  Ethan Perez and
                  David Grangier and
                  Jason Weston and
                  Michael Auli},
  title        = {{ELI5:} Long Form Question Answering},
  year         = {2019},
  journal    = {arXiv},
}

@misc{oh2.5,
  title = {OpenHermes 2.5: An Open Dataset of Synthetic Data for Generalist LLM Assistants},
  author = {Teknium},
  year = {2023},
  publisher = {HuggingFace},
  url = {https://huggingface.co/datasets/teknium/OpenHermes-2.5}
}