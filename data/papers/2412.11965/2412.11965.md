---
abstract: |
  Attention heads are one of the building blocks of large language models (LLMs). Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks. In this work, we seek a comprehensive mapping of the operations they implement in a model. We propose <span class="smallcaps">MAPS</span> (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference. We showcase the utility of <span class="smallcaps">MAPS</span> for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality. Evaluating <span class="smallcaps">MAPS</span> on 20 operations across 6 popular LLMs shows its estimations correlate with the head’s outputs during inference and are causally linked to the model’s predictions. Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs. Next, we present an automatic pipeline and analysis that leverage <span class="smallcaps">MAPS</span> to characterize the salient operations of a given head. Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations. We release our code and mappings at <https://github.com/amitelhelo/MAPS>.
author:
- |
  Amit Elhelo         Mor Geva  
    
  Blavatnik School of Computer Science, Tel Aviv University  
  `{amitelhelw@mail,morgeva@tauex}.tau.ac.il`
bibliography:
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Inferring Functionality of Attention Heads from their Parameters
---





# Introduction

<figure id="fig:intro">
<span class="image placeholder" data-original-image-src="figures/attn_func_v2.pdf" data-original-image-title=""></span>
<figcaption>Illustration of <span class="smallcaps">MAPS</span>, a framework for inferring the functionality of attention heads in LLMs from their parameters. <span class="smallcaps">MAPS</span> casts the head as a matrix <span class="math inline">\(M\)</span> which assigns a score for every pair of tokens in the model’s vocabulary. Then, it considers groups of token pairs (sub-matrices in <span class="math inline">\(M\)</span>) to measure how strongly the head implements a given operation <strong>(A)</strong> and to inspect the head’s salient operations <strong>(B)</strong>.</figcaption>
</figure>

Attention heads play a key role in modern large language models (LLMs) . A myriad of works have been studying their functionality, typically by analyzing their attention patterns or outputs during inference for certain inputs or tasks.

However, relying on the model’s behavior for certain inputs has drawbacks. First, this approach may overlook some of the functions implemented by the head, as heads can exhibit different behaviors for different inputs . Second, a comprehensive analysis of the head’s operation would require executing the model over numerous inputs, potentially the whole training corpus, which involves a high computational cost and could be impossible when the data is unavailable. Last, analyzing the examples that activate the head is often non-trivial and could be misleading .

In this work, we consider a different approach to this problem, where our goal is to infer the functionality of attention heads *directly from their parameters* and without executing the model. To this end, we leverage the approach of interpreting model parameters in the vocabulary space . Specifically, we build on the formulation by , who cast the attention head as a matrix $M$, where each entry is a mapping score between two tokens. While this approach has been shown effective in identifying heads with certain operations, so far its usage has been limited to studying specific heads in detected circuits or a single operation .

Here, we scale this interpretation approach into a general framework, called <span class="smallcaps">MAPS</span> (Mapping Attention heads ParameterS), which enables answering two types of basic questions: (a) given a predefined operation, mapping how strongly different heads across the model implement it, and (b) given an attention head, inferring its prominent operations. This is done by considering patterns across *groups of mappings* in $M$, as illustrated in . *Predefined relations* signify groups of mappings expressing a certain relation (e.g. city of a country or pronoun resolving). *Salient operations* consist of subsets of mappings for which the head induces the most prominent effect. In addition, analyzing simple statistics of these mappings provides insights into how global or specific its operation is.

We evaluate our framework on 6 popular LLMs and 20 predefined relations of 4 categories – knowledge, language, algorithmic, and translation. Experiments show that estimations by <span class="smallcaps">MAPS</span> strongly correlate with the head outputs during inference. Moreover, causally removing all the heads implementing a certain operation substantially impairs the model’s ability to answer queries requiring this operation, compared to removing other heads.

Analysis of the obtained mappings shows that, across all models, <span class="smallcaps">MAPS</span> detects relation heads mostly in the middle and upper layers, while revealing universality patterns for several relations. Moreover, it demonstrates how the model’s architecture introduces biases in function encoding. Smaller models tend to encode higher numbers of relations on a single head, and in Llama-3.1 models, which use grouped-query attention, grouped attention heads often implement the same or similar relations. Notably, <span class="smallcaps">MAPS</span> successfully detected previously identified heads of specific operations, while discovering additional heads of similar operations not reported before.

Next, we demonstrate the utility of <span class="smallcaps">MAPS</span> for inferring the prominent operations of a given head. We consider the head’s salient mappings in $M$ and use GPT-4o to automatically describe the functionality they exhibit. Applying this procedure to GPT-2 xl and Pythia 6.9B, we map the prominent operations of 62% of their heads and 60%-96% of those in the middle and upper layers. Qualitative analysis shows semantic, linguistic, and algorithmic operations and reveals novel operations, such as the extension of time periods (`day->month;month->year`). A human study shows that our automated pipeline performs reasonably well, and GPT-4o reliably detects observable operations.

To conclude, we introduce <span class="smallcaps">MAPS</span>, an efficient framework for inferring attention heads’ functionality from their parameters. We showcase the utility of <span class="smallcaps">MAPS</span> in systematically mapping a certain functionality across the model and automatically characterizing the salient operations of a given head. Estimations by <span class="smallcaps">MAPS</span> correlate with the head’s outputs and are faithful to the model’s behavior, and provide valuable insights on architecture biases and universality of head operations in LLMs.

# Preliminaries and Notation

We assume a transformer-based LM with a hidden dimension $d$, $L$ layers, $H$ attention heads per layer, a vocabulary $\mathcal{V}$, an embedding matrix $E \in \mathbb{R}^{|\mathcal{V}| \times d}$, and an unembedding matrix $U \in \mathbb{R}^{d \times |\mathcal{V}|}$.

#### Attention heads as interaction matrices

We use the formulation by and view an attention head as two “interaction” matrices $W_{QK},W_{VO}\in\mathbb{R}^{d \times d}$. Given a sequence of $n$ hidden states $X \in \mathbb{R}^{n\times d}$, the matrix $W_{QK}$ computes the query-key scores to produce an attention weights matrix $A \in \mathbb{R}^{n \times n}$: $$A = \text{softmax}\Bigg(\frac{X (W_{QK}) X^T}{\sqrt{d / H}}\Bigg)$$ The matrix $W_{VO}$ operates on the contextualized hidden states according to $A$, namely $\tilde{X} = AX$, and produces the head’s output $Y \in \mathbb{R}^{n\times d}$: $$\label{eq:heads_transformation}
    Y = \tilde{X} W_{VO}$$

The matrix $W_{QK}$ can be viewed as “reading” from the residual stream, and $W_{VO}$ can be viewed as the “writing” component. Notably, this formulation omits the bias terms of the head.

#### Interpreting attention heads in embedding space

Recent works have analyzed the operation of different components in transformers through projection to the model’s vocabulary space . Specifically, interpret each of the attention head matrices — $W_{QK}$ and $W_{VO}$ — as a matrix that maps between pairs of tokens from the vocabulary. Considering $W_{VO}$, it is interpreted via multiplication from both sides with the model’s embedding matrix: ${\tilde{M} = E (W_{VO}) E ^ T  \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}}$. Each entry in $\tilde{M}$ is viewed as a mapping score between source and target tokens ${s, t \in \mathcal{V}}$ based on $W_{VO}$, which signifies how strongly the head promotes it in its outputs. suggested that when the weights of $E$ and $U$ are not tied, a more faithful interpretation can be obtained by: $$M = E (W_{VO}) U$$ Other notable variations include applying the model’s first MLP layer to the embedding matrix $E$ and the final layer norm on rows of $E (W_{VO})$ .

# <span class="smallcaps">MAPS</span>

Based on the above view, we propose a general framework, called <span class="smallcaps">MAPS</span>, for inferring the functionality of attention heads in LLMs directly from their parameters. We focus on analyzing the $W_{VO}$ component of the head, which produces the head’s output to the residual stream, and make the following observations. First, the $i$-th row of $M$ provides the scores for mappings from the $i$-th token to any token in $\mathcal{V}$. Similarly, the $j$-th column of $M$ provides scores for mappings from any token in $\mathcal{V}$ to the $j$-th token. Therefore, considering the scores of certain submatrices of $M$ may reveal how the attention head operates on different sets of inputs. For example, analyzing the rows corresponding to tokens representing countries may reveal general knowledge-related operations implemented by the head, and attention heads that copy certain tokens should have diagonal-like submatrices in $M$.

An important question that arises is which parts of $M$ to consider in order to identify the head’s functionality. In principle, there are $2^{|\mathcal{V}|}$ different subsets of rows that can be considered, which would be infeasible to traverse with $|\mathcal{V}| = \mathcal{O}(10K)$ in typical LLMs. Here, we propose two complementary ways to approach this, described next.

## Predefined Relations

One intuitive approach is to define a set of possible operations that can be realized through pairs of tokens, and then measure the extent to which the head implements each operation. For example, the operation of mapping a country to its capital can be realized through a set of token pairs expressing that relation, e.g. `(France, Paris)` or `(Egypt, Cairo)`. Similarly, mapping between synonyms can be realized via pairs such as `(talk, speak)` and `(fast, quick)`. Such operations can be viewed as an implementation of *relations* between tokens.

Let $R$ be a predefined relation and $\mathcal{D}_R$ a dataset of token pairs expressing $R$. Also, denote by $\mathbf{m}_i \in \mathbb{R}^{|\mathcal{V}|}$ the $i$-th row of $M$ (corresponding to the mapping scores of the $i$-th token), and by $\texttt{topk}(\mathbf{m}_i)$ the $k$ tokens with the highest scores in $\mathbf{m}_i$. The extent to which an attention head, interpreted as the matrix $M$, implements $R$ can be measured as the portion of pairs $(s,t) \in \mathcal{D}_R$ where $t$ is in the top-scoring tokens in $\mathbf{m}_s$: $$\label{eq:relation_score}
    \phi_R(M):=\frac{1}{|\mathcal{D}_R|} \sum_{(s,t)\in \mathcal{D}_R}\mathds{1}[t \in \texttt{topk}(\mathbf{m}_s)]$$ For instance, the score for $R=$`“country to capital”` reflects how often the head promotes the capital city of a country in its output when operating on an input representation of that country.

Notably, our formulation also supports suppression operations observed in previous work , where certain attention heads suppress certain concepts or outputs during inference. Representing a suppressive relation is done by defining the pairs $(s,t)$ as before and considering the top-scoring tokens in $-\mathbf{m}_s$ instead of $\mathbf{m}_s$.

## Salient Operations

The main limitation of the above approach is that it could miss certain relations that heads implement. A complementary approach would be to characterize the head’s functionality from prominent mappings appearing in $M$. tackled this by considering the top-scoring mappings in $M$. However, we recognize two drawbacks in this method: (a) the scores in $M$ are influenced by the token embedding norms, which could bias the top scores towards mappings of tokens with high embedding norms, and (b) the top entries in $M$ may cover mapping from a small number of tokens (e.g., from a single row), thus describing the head’s functionality for only a few tokens.

Here, we propose a more holistic approach to identify salient mappings in $M$, by first identifying *the tokens on which the head’s operation is most prominent*, and then considering the top-scoring mappings for these tokens. We measure how prominent the head’s operation on a token $t \in \mathcal{V}$ via the ratio of the token’s embedding norm after multiplying by $W_{VO}$ to the norm before this transformation: $$\label{eq:saliency_score}
\sigma_t(W_{VO}) :=\frac{||\mathbf{e}_t W_{VO}||}{||\mathbf{e}_t||}$$

Comparing the sets of top versus salient mappings indeed shows substantial differences.[^1] In the next sections, we experiment with both approaches, showing their effectiveness in inferring attention head functionality in multiple LLMs.

# Mapping Predefined Relations

In this section, we utilize <span class="smallcaps">MAPS</span> to map how strongly attention heads implement various operations in multiple LLMs (§). We assess the correctness and generalization of these estimations via correlative and causal experiments (§, §) and analyze prominent trends (§).

## Experimental Setup

#### Datasets

We construct datasets for 20 relations of four categories: algorithmic (e.g., `word to first letter`), knowledge (e.g., `country to capital`), linguistic (e.g., `adjective to comparative`), and translation (`English to French/Spanish`), and 3 vocabularies of widely-used model families. For every relation, we collect pairs of strings expressing it. For instance, possible pairs for the relation word-to-compound are `(hot, hotdog)` and `(wall, wallpaper)`. Data is obtained from previously published datasets and online sources and further augmented by querying ChatGPT to generate example pairs, which we (authors) manually validated. Then, we tokenize the pairs with each of the tokenizers of Llama-3.1 , Pythia   GPT and Phi-2 , keeping only cases where the resulting mapping is between single tokens. Experimenting with different tokenizers is important as <span class="smallcaps">MAPS</span> leverages the model’s vocabulary. Llama-3.1’s vocabulary has $\sim$<!-- -->130k tokens compared to $\sim$<!-- -->50k tokens for GPT-2, Phi-2, and Pythia. For more details on the collection, dataset statistics, and examples, see §.

#### Models

We analyze models of various sizes from different families: Llama-3.1 8B and 70B , Pythia 6.9B and 12B , Phi-2 , and GPT-2 xl . These models have varying numbers of layers and attention heads, from 32 layers and 32 heads in Pythia 6.9B to 80 layers and 64 heads in Llama-3.1 70B. Additionally, Llama-3.1 uses grouped-query attention , versus the other models which use multi-head attention .

#### Measuring predefined relations

For every attention head and relation $R$, we derive the matrix $M$ and calculate the relation score $\phi_R(M)$ (Eq. ). We also compute the score for the suppressive variant $\bar{R}$ of every relation $R$. For example, the suppressive variant of $R = \texttt{country to captial}$ corresponds to the operation of suppressing the capital of a given country.

We follow previous works and set low $k$ values to reflect strong prioritization of the target token in the head’s output. For Pythia, Phi-2 and GPT-2, we use $k=1$ for the copying and name-copying relations and $k=10$ for other relations. For the Llama-3.1 models, we set $k=3$ for copying and name-copying and $k=25$ for other relations. The bigger values for Llama-3.1 are due to their large vocabulary, which allows expressing a concept with more tokens. The smaller values for the copying relations are for measuring them more strictly. For further discussion on this selection, see §.

To classify whether a head “implements” a relation $R$, we apply a threshold $\tau$ to $\phi_R(M)$. Namely, if $t$ appears in the top-$k$ mappings of $s$ for $\tau$ percent of the pairs $(s,t) \in \mathcal{D}_R$, then we consider the head as implementing $R$. We choose a threshold of $\tau = 15\%$ after experimenting with different thresholds and comparing against randomly initialized heads (see § for details).

## Evaluation of Functionality Estimation

We evaluate whether the functionality estimations by <span class="smallcaps">MAPS</span> faithfully describe the operations of the heads during inference. Our experiments show that the estimated operation of a head strongly correlates with its outputs and demonstrates the expected causal effect on the model’s generation.

#### Experiment 1: Correlation with head outputs

For every relation $R$ and source-target pair $(s,t) \in \mathcal{D}_R$, we run the model on the prompt:[^2] $$\mathcal{P}_s := \texttt{``This is a document about $\langle$s$\rangle$''}$$ Where $\langle \texttt{s} \rangle$ is the string of the source token $s$. For example, for the pair `(England, London)`, we will have `“This is a document about England”`. Next, we obtain the output $\mathbf{y}_s \in \mathbb{R}^d$ of every attention head at the last position (corresponding to $s$),[^3] and project it to the model’s vocabulary space, i.e. $\mathbf{y}_s U \in \mathbb{R}^{|\mathcal{V}|}$. The top-scoring tokens in the resulting vector are those promoted by the head given the prompt $\mathcal{P}_s$ . To check whether the head implements the relation $R$, namely promote $t$ when given $s$ in the input, we test for every pair $(s,t)$ whether $t$ appears in the top $k$ tokens in $\mathbf{y}_s U$. We use the same $k$s specified in §. Concretely, for every head $h$ we compute the following score, which represents how strongly the head implements $R$ during inference: $$\phi^*_R(h):=\frac{1}{|\mathcal{D}_R|} \sum_{(s,t)\in \mathcal{D}_R}\mathds{1}[t \in \texttt{topk}(\mathbf{y}_s U)]$$

We check the correlation between the static score $\phi_R(h)$ inferred by our method and the score $\phi^*_R(h)$ observed dynamically. As a baseline, we compute $\phi^*_R(h)$ while restricting the attention in $h$ from $s$ to be only to itself. This emulates an operation of the head as if it fully attends to the representation of $s$.

#### Results

shows the results for Llama-3.1 8B. For the vast majority of relations, we observe a strong to very strong correlation of 0.71-0.95 when the query’s subject is not contextualized. This high correlation often remains or even increases when considering the head’s outputs for contextualized inputs. This shows that <span class="smallcaps">MAPS</span> well-estimates the head’s behavior for task-related inputs. Still, for some relations (e.g. `word to compound` and `word to last letter`) correlation is lower for contextualized inputs, demonstrating that in some cases, the head may switch its operation depending on the context. This agrees with the observation that heads often implement multiple operations (§). Results for other models are in §, generally exhibiting similar trends, though with occasional larger drops in the contextualized setting for Pythia and GPT-2 xl.

<div id="tab:Dynamic results">

|              |                      |      |      |
|:-------------|:---------------------|-----:|-----:|
| Category     | Relation             |      |      |
| w/o context. |                      |      |      |
| w/ context.  |                      |      |      |
| Algorithmic  | Copying              | 0.75 | 0.79 |
|              | Name copying         | 0.95 | 0.94 |
|              | Word to first letter | 0.89 | 0.72 |
|              | Word to last letter  | 0.67 | 0.37 |
| Knowledge    | Country to capital   | 0.81 | 0.84 |
|              | Country to language  | 0.74 | 0.63 |
|              | Object to superclass | 0.74 | 0.66 |
|              | Product by company   | 0.44 | 0.52 |
|              | Work to location     | 0.45 | 0.45 |
| Linguistic   | Word to antonym      | 0.90 | 0.90 |
|              | Adj to comparative   | 0.87 | 0.91 |
|              | Adj to superlative   | 0.87 | 0.89 |
|              | Noun to pronoun      | 0.89 | 0.79 |
|              | Verb to past tense   | 0.91 | 0.89 |
|              | Word to compound     | 0.76 | 0.55 |
|              | Word to homophone    | 0.84 | 0.77 |
|              | Word to synonym      | 0.79 | 0.79 |
| Translation  | English to French    | 0.71 | 0.71 |
|              | English to Spanish   | 0.82 | 0.81 |

Correlation between the relation score of a head and the head’s outputs in Llama-3.1-8b, with and without head contextualization. Results are statistically significant with p-values $< 2.3e^{-35}$ (see §).

</div>

<div id="tab:causal_results_pythia_12b_main">

|                      |          |       |        |           |       |
|:---------------------|:---------|------:|-------:|:----------|------:|
| Relation             | TR Tasks |       |        | CTR Tasks |       |
|                      | Base     | \- TR | \- RND | Base      | \- TR |
| Adj to comparative   | 0.91     |  0.20 |   0.82 | 0.92      |  0.63 |
| Copying              | 0.65     |  0.34 |   0.79 | \-        |    \- |
| Country to capital   | 0.97     |  0.00 |   0.95 | 0.89      |  0.90 |
| Country to language  | 1.00     |  0.08 |   0.96 | 0.89      |  0.89 |
| Name copying         | 1.00     |  0.24 |   1.00 | 0.90      |  0.92 |
| Noun to pronoun      | 0.88     |  0.46 |   0.86 | 0.92      |  0.91 |
| Object to superclass | 0.78     |  0.39 |   0.68 | 0.90      |  0.87 |
| Verb to past tense   | 0.22     |  0.04 |   0.26 | 0.03      |  0.02 |
| Word to first letter | 0.91     |  0.34 |   0.87 | 0.91      |  0.74 |
| Year to following    | 0.92     |  0.00 |   0.87 | 0.83      |  0.79 |

Accuracy of Pythia 12B on tasks for a target relation (TR) versus on control (CTR) tasks, when removing heads implementing the relation compared to when removing random heads (RND). Results for RND heads are averaged over 5 experiments. We omit standard deviation for brevity and report it in §.

</div>

#### Experiment 2: Causal effect on model outputs

For a given relation $R$, we evaluate the model’s performance on queries that require applying $R$, when removing the heads classified by <span class="smallcaps">MAPS</span> as implementing $R$ versus when removing random heads from the model. We choose a diverse set of 13 relations and construct a test set $\tilde{\mathcal{D}}_R$ for every relation $R$ as follows. First, we craft a task prompt that requires the model to apply $R$. For example, a prompt for the `country to capital` relation could be `“The capital of \langle s \rangle is”`, with $\langle s \rangle$ being a placeholder for a country. Then, for each pair $(s,t) \in \mathcal{D}_R$ we instantiate the prompt with $s$ to create an input $\tilde{\mathcal{P}}_s$ and a test example $(\tilde{\mathcal{P}}_s, t) \in \tilde{\mathcal{D}}_R$.

Let $\mathcal{H}_R^i$ be the subset of $i$ attention heads with the highest scores for $\phi_R(M)$. We evaluate the models on $\tilde{\mathcal{D}}_R$ while running each input $n$ times, each time canceling (by setting to zero) the outputs of the attention heads $\mathcal{H}_R^i$ and obtaining the model’s prediction with greedy decoding. We set $n$ as the minimum between the number of heads in the model with $\phi_R(M)>0$ and a fixed boundary: 150 for GPT-2 xl, Pythia 6.9B, Pythia 12B, and Llama-3.1 8B and 250 for Llama-3.1 70B. In cases when the accuracy drops to 0 after ablating $i < n$ heads, we report results obtained up to $i$.

We compare the above intervention against a baseline where $i$ randomly sampled heads that are not in $\mathcal{H}_R^i$ are ablated, repeating this experiment 5 times and reporting the average accuracy. Additionally, to establish that relation heads are important specifically for tasks involving $R$, we remove the relation heads as above and measure the model’s performance on up to five *control tasks* for other relations. We choose the relations such that $<$<!-- -->15% of the target relation heads are also control relation heads, and the absolute difference between the baseline accuracy on the control task and the target task is $\leq$<!-- -->20%.

#### Results

Results for Pythia 12B are presented in , excluding relations where the base accuracy was $<$<!-- -->0.1. For all relations, removing the relation heads identified by <span class="smallcaps">MAPS</span> causes a major accuracy drop of $>$<!-- -->48% compared to $<$<!-- -->13% when removing random heads. Moreover, while the accuracy drop for the control tasks is sometimes considerable (at most 32%), it is significantly smaller than the relative drop on the target relation task. Results for the other models are generally similar (see §). Notable differences are that the accuracy drops in Llama-3.1 are often smaller, but in 9 out of 11 relations they are larger than those obtained for the random and control baselines.

## Generalization to Multi-Token Entities

A natural question that arises is how well the estimations by <span class="smallcaps">MAPS</span> generalize to contextualized inputs representing multiple tokens. Namely, if we infer the head’s ability to perform country-to-capital mappings from country names tokenized as a single token, will we observe the same behavior for countries tokenized as multiple tokens?

To test this, we apply the data collection process from § to create new datasets for three relations — `object to superclass, noun to pronoun, country to language` — of source-target pairs $(s,t)$ where $s$ has multiple tokens. Then, we repeat the correlative experiment in § for GPT-2 xl, Pythia 6.9B and Pythia 12B using this data.

We observe that the estimated operations generalize to multi-token representations. For 6 out of the 9 model-relation combinations, the correlation between the relation score and the head’s output in the multi-token setting is similar ($\leq$<!-- -->0.05 difference) or higher than the single-token setting, both with and without head contextualization. In the remaining cases, there is a bigger drop ($\leq$<!-- -->0.13) but the correlation remains high ($>$<!-- -->0.73). The full results are provided in §.

## Analysis

#### Function distribution

shows category-level classification results of all heads in GPT-2 xl, Phi-2, Pythia 12B, and Llama-3.1 70B. A head is assigned to a certain category if it implements at least one relation from it or its suppressive variant. Considering prominent trends across all models, we first observe that <span class="smallcaps">MAPS</span> identified relations from all categories, with classified heads mostly being located in the middle and upper layers. This may suggest that early layers perform operations that cannot be represented in the model’s output vocabulary space. Interestingly, we observe a “side effect” of the grouped attention structure in Llama-3.1 models, where grouped heads often implement the same relations or their suppressive variants.

In addition, heads often implement multiple relations from the same or different categories. The portion of multi-category heads (out of all classified heads) generally decreases in model size: 38% in GPT-2 xl, 29% in Phi-2, 20% in Pythia 6.9B, Pythia 12B and 11% in Llama-3.1 70B. An exception to this trend is Llama-3.1 8B with 11% of multi-category heads, which may be caused by its grouped query attention structure. Also, 20%-36% of the classified heads implement at least one suppression relation.

<figure id="fig:classified_attn_heads_multiple_models">
<span class="image placeholder" data-original-image-src="figures/classified_heads/classified_attn_heads_multiple_models_threshold_0.15" data-original-image-title=""></span>
<figcaption>Functionality mapping by <span class="smallcaps">MAPS</span> for 20 relations of 4 categories — algorithmic, knowledge, linguistic, translation — across all attention heads in GPT-2 xl, Phi-2, Pythia 12B, Llama-3.1 70B. A head is marked as a specific category if it implements at least one relation from this category.</figcaption>
</figure>

#### Function universality

presents the distributions of relation scores for several representative relations in multiple models showing two interesting trends. First, despite architecture and training data differences, models encode relations in their heads to similar degrees, as observed by the similar highest scores per relation. This observation supports the “universality hypothesis” that different networks learn similar features and circuits and expands recent similar findings about universality in LLMs . Second, the scores for a given relation are diverse, with different heads implementing the relation at varying degrees, as opposed to having a small set of heads with high relation scores. This has implications for research concerning localization and editing; certain concepts or associations are encoded in a large number of model components at varying degrees.

#### Comparison with known head functionalities

identified “Name Mover” and “Anti Name Mover” heads in a circuit for indirect object identification in GPT-2 small, which copy or suppress copying specific names in the context, and identified “Mover” and “Capital” heads in GPT-2 medium. <span class="smallcaps">MAPS</span> successfully identified all these heads as name copiers or country-to-capital mappers . In addition, it discovered 25 heads in GPT-2 small and 46 in GPT-2 medium that implement similar operations but were not recognized in prior analyses. While the additional heads may not participate in the specific circuits discovered, they may be triggered for circuits of similar or related tasks that were overlooked in previous analyses.

Notably, for all the heads identified in previous works, <span class="smallcaps">MAPS</span> reveals various additional functionalities. These observations extend the findings by of heads that implement multiple functionalities.

Taken together, these results demonstrate the effectiveness of <span class="smallcaps">MAPS</span> in comprehensively mapping the implementation of a certain operation by attention heads across the model. A more detailed comparison is in §.

<figure id="fig:relation_scores_across_models">
<span class="image placeholder" data-original-image-src="figures/classified_heads/relation_scores_across_models_0.15.pdf" data-original-image-title=""></span>
<figcaption>Relation scores for all heads of Llama-3.1 70B, Pythia 6.9B, Phi-2, GPT-2 xl for several relations. We observe that heads from all models implement these relations to similar degrees.</figcaption>
</figure>

# Inspecting Salient Operations

We saw that given an operation realized as a relation between pairs of tokens, we can map how strongly it is implemented by attention heads across the model. Here, we use <span class="smallcaps">MAPS</span> to tackle a complementary problem of inferring the prominent operations of a given attention head. We introduce an automatic pipeline for interpreting salient mappings in attention heads (§) and use it to broadly infer the functionalities in Pythia 6.9B and GPT-2 xl (§). In §, we extend our analysis to show that the skewness of saliency scores can indicate how global or specific the head’s functionality is.

## Automatic Functionality Inference

We propose the following steps for inferring the functionality of an attention head:

1.  Using the saliency score (Eq. ) to identify the top $k$ tokens for which the head’s transformation is most prominent.

2.  For each salient token $s$, collecting the top $n$ tokens it is mapped to according to $M$, namely, the tokens corresponding to the top entries in $\mathbf{m}_s$.[^4]

3.  Inferring the head’s salient operations by querying an LLM about prominent patterns in the list of salient tokens and their top mappings. Notably, we ask the model to indicate there is no pattern when no clear pattern is observed across the mappings. For the exact prompt used, see §.

We run this pipeline on a total of 2,224 attention heads in GPT-2 xl and Pythia 6.9B, while setting $k=30$ (step 1) and $n=5$ (step 2) and using GPT-4o (step 3). We analyze how often GPT-4o was able to recognize a prominent functionality and measure the quality of its descriptions compared to human judgment.

## Results

<figure id="fig:identification_rate">
<span class="image placeholder" data-original-image-src="figures/identification_rate.pdf" data-original-image-title=""></span>
<figcaption>Portion of heads where <span>GPT-4o</span> identified a prominent pattern across the head’s <em>salient mappings</em>.</figcaption>
</figure>

shows the percentage of heads per layer in GPT-2 xl and Pythia 6.9B where GPT-4o described a pattern. In both models, we observe a high rate of 60%-96% interpretable heads in the middle and upper layers, compared to a lower rate of 20%-60% in the early and last layers. These trends are consistent with those observed for predefined relations (§), suggesting that early-layer heads are less interpretable in the vocabulary space. Qualitative analysis of 107 heads with identified patterns shows diverse operations: 38% semantic (e.g., extension of time-periods, `day->month; month->year; year->decade`), 36% algorithmic (e.g., capitalization, `water->Water`), and 26% linguistic (e.g., completion of sub-words (`inhib->inhibition; resil->resilience`). Examples of salient mappings and their interpretations are provided in §.

#### Interpretation quality

We conduct a human study to assess the plausibility of the generated descriptions, finding that GPT-4o correctly identifies the presence or absence of a pattern in 80% of the cases and reliably detects observable patterns. This shows that our automatic pipeline is reasonable and demonstrates promising trends in automatically interpreting attention heads with <span class="smallcaps">MAPS</span>. For more details on this study and its results, see §.

# Related Work

Prior studies of attention heads in LLMs mostly focused on analyzing their attention patterns , training probes and sparse auto-encoders , studying head outputs, and performing causal interventions . Unlike these methods, <span class="smallcaps">MAPS</span> infers the functionality of attention heads from their parameters, without any training or inference.

Vocabulary projections of attention head parameters have been used for analyzing certain attention head operations in LLMs . However, they have been used mostly as a validation tool for operations inferred by other methods and were applied to specific relations and heads, typically in the scope of specific circuits. studied a single relation across all heads of multiple LLMs. Our work proposes *a general framework* that uses vocabulary projections as its primary tool for inferring attention head functionality.

utilized an LLM to interpret the vocabulary projections of singular vectors of attention heads and MLP matrices, but their approach does not consider input-output mappings which are essential for estimating head functionality. More recently, used parameter similarities of heads at different layers to study their “communication channels”. Lastly, showed that relation operations of attention heads can be well-approximated by linear functions. Our work further shows that some of these relations are implemented by mappings encoded in head parameters.

# Conclusion

We present <span class="smallcaps">MAPS</span>, an efficient framework for analyzing the functionality of attention heads from their parameters. <span class="smallcaps">MAPS</span> utility is two-fold: it allows mapping how strongly a given operation is implemented across the heads of a model and inferring the salient operations of a given head. Experiments show that estimations by <span class="smallcaps">MAPS</span> correlate with the head outputs during inference and causally relate to the model’s behavior. Moreover, strong LLMs can interpret them automatically, often aligning with human judgment. Our analysis provides insights into architecture biases on function encoding and function universality in LLMs.

# Limitations

<span class="smallcaps">MAPS</span> primarily focuses on analyzing the part of the head’s computation that writes the output to the residual stream, i.e., the matrix $W_{VO}$. In other words, we use single-token mappings to analyze the operation of the output part of the head on contextualized representations $\tilde{X}$. While our experiments in § show that these estimations generalize to multi-token inputs, it is still valuable to examine the head’s computation responsible for contextualization and for creating $\tilde{X}$, i.e., the matrix $W_{QK}$.

Another limitation of <span class="smallcaps">MAPS</span> is that its expressivity is bounded by the model’s vocabulary. Namely, it can only map operations that can be expressed via pairs of tokens. While this formulation can effectively describe and capture various features, as demonstrated by our experiments in § and §, there are likely to be operations that this framework would overlook, such as idioms and positional features. A related challenge is the lower coverage of <span class="smallcaps">MAPS</span> in early layers. Extending <span class="smallcaps">MAPS</span> to support other types of embeddings is a promising direction to overcome these limitations, as well as exploring methods such as linear mappings and patching to improve the performance on early layers.

Lastly, <span class="smallcaps">MAPS</span> relies on the formulation of attention heads as interaction matrices (§), which ignores the bias terms of $W_V,W_O$. While our experiments show there is a strong correlation between the estimations by <span class="smallcaps">MAPS</span> and head outputs, these terms may influence them. Incorporating these bias terms into the analysis is an interesting direction, which we leave for future works to explore.

# Acknowledgments

We thank Guy Dar, Daniela Gottesman, Ohav Barbi, Ori Yoran, Yoav Gur-Arieh and Samuel Amouyal who helped with analysis and provided useful feedback. This research was supported in part by The Israel Science Foundation grant 1083/24.

# Mapping Predefined Relations – Additional Details and Results

In §, we showed how <span class="smallcaps">MAPS</span> can be utilized to map all heads that implement a predefined relation across a language model. Here we offer further details on the datasets and implementation, as well as supplementary results.

## Datasets

<div id="tab:datasets_sources">

| Relation             | Source              | Notes                                            |
|:---------------------|:--------------------|:-------------------------------------------------|
| Country to capital   | Wikidata query      |                                                  |
| Country to language  |                     |                                                  |
| Copying              | Word frequency list | 500 strings randomly sampled from the top 10,000 |
| Year to following    | Python code         |                                                  |
| Word to synonym      | ChatGPT             | Validated using nltk                             |
| Word to homophone    |                     |                                                  |
| Noun to pronoun      | ChatGPT             |                                                  |
| Word to compound     |                     |                                                  |
| Name copying         |                     |                                                  |
| English to Spanish   | ChatGPT             | Validated with Google Translate                  |
| English to French    |                     |                                                  |
| Work to location     |                     | Extended using ChatGPT                           |
| Object to superclass |                     |                                                  |
| Product by company   |                     |                                                  |
| Adj to comparative   |                     |                                                  |
| Adj to superlative   |                     |                                                  |
| Verb to past tense   |                     |                                                  |
| Word to antonym      |                     | Extended using ChatGPT, validated using nltk     |
| Word to first letter |                     | Letters converted to lowercase                   |
| Word to last letter  |                     |                                                  |

Sources for constructing per-relation datasets used in §.

</div>

We display the list of categories and relations used to map predefined relations (§), alongside the sizes of the different datasets and examples for relations pairs in .

#### Data collection

We obtained the relation pairs from the sources: WikiData ; “English Word Frequency List” Kaggle dataset,[^5] which is based on Google Books Ngram Viewer Exports, version 3, exported on Feb 17, 2020,[^6] the datasets used by , which are based on *CounterFact* and WikiData , and ChatGPT.[^7] We also used the *nltk* package to validate several relation datasets. Except for the Translation and `year to following` datasets, all datasets are in English. The details on which source was used to compose which relation are presented in .

In the datasets for the relations `work to location, verb to past tense, product by company, object to superclass, adj to superlative, adj to comparative, word to antonym`, we filter out pairs where the source token appeared as a source token in other pairs. Relation pairs were filtered out from different datasets to assert their correctness.

#### Data processing

For every model, we tokenized the various datasets using the model’s tokenizer. To maximize the number of words mapped to single tokens, we added a leading space before every word. For example, if the relation source word was `"Don"`, we tokenized the string `" Don"` instead. Finally, we filtered out relation pairs where at least one of the words was mapped to more than one token.

## Implementation Details

#### Applying the first MLP

For every model except Llama-3.1 70B, and similarly to , we first applied the model’s first MLP to the tokens embeddings.[^8] To adjust the embeddings to the first MLP’s input distribution, we also applied the layer norm that precedes it. Regarding Llama-3.1 70B, we observed better results when not applying the first MLP.

#### Selection of $k$

To calculate a head’s relation score $\phi_R(M)$, we obtain the top-$k$ tokens in $\mathbf{m}_s$ for every source token $s$. For Pythia, GPT-2 and Phi-2 we set $k=1$ for copying and name-copying relations and $k=10$ for other relations. For the Llama-3.1 models we set $k=3$ for copying and name-copying and $k=25$ for other relations. – which presents the tokenization applied to several base words by the tokenizers of Llama-3.1, GPT-2 and Pythia – demonstrates the need to set larger $k$ values for Llama-3.1. The larger vocabulary size allows Llama-3.1’s tokenizer to express the same concept with more tokens.

## Random Baselines

A concern that may arise from choosing a relatively small relation score threshold, is that the results obtained by <span class="smallcaps">MAPS</span> may capture the similarity of tokens embeddings, rather than a functionality implemented by attention head’s weights. To study this, we applied <span class="smallcaps">MAPS</span> to randomly initialized matrices from the empirical distribution of the model. Concretely, for every layer in the original model, we sampled $H$ random matrices (with the same shape as $W_{VO}$) from a normal distribution, for which the mean and standard deviation are the average and the standard deviation of the $W_{VO}$ matrices in the original layer. We applied our predefined relation analysis (described in §) to those matrices and measured the amounts of “functional attention heads” classified among them.

For models Phi-2, Pythia 6.9B, Pythia 12B, Llama-3.1 8B and Llama-3.1 70B no random matrices were classified as relation heads. For GPT-2 xl, 5 matrices were classified as such, compared to 250 relation heads in the trained model, and out of 1200 heads in the model. This demonstrates that the choice of $\tau=15\%$ is meaningful for separating between functionalities of trained attention heads and random ones. While smaller thresholds could have also been justified by this experiment, we chose $\tau=15\%$ to assert that the heads encode a substantial fraction of the relation pairs.

## Additional Results

In we display all heads classified in Llama-3.1 70B, Llama-3.1 8B, Pythia 12B, Pythia 6.9B, Phi-2 and GPT-2 xl divided to four categories. In Tables and we present the number of relation heads (and suppression relation heads) discovered in the same models, divided into relations. We observe that several relations (`Name copying, Adj to comparative, Word to first letter`) are demonstrated by a relatively large number of heads in at least five out of six models. On the other hand, several relations (e.g., `Word to homophone, Word to last letter`) are demonstrated by a small number of heads across all models.

# Additional Details on Evaluation Experiment

## Correlative Experiment

In § we conducted an experiment which calculates the correlation between <span class="smallcaps">MAPS</span>’s estimations and heads outputs during inference.

#### Implementation details

Recall that the attention head’s formulation that we used: $Y=\tilde{X}W_{VO}$ omits the bias terms of $W_V, W_O$ (§). To account for the bias term of $W_V$ in the correlative experiment, where we compute the attention head’s output dynamically, we use both the original attention head definition and the formulation suggested by , which we have followed so far. First, following , we obtain the head’s intermediate output: $\hat{y}\in \mathbb{R}^{n \times d_\text{head}}$, where $d_\text{head}$ is the inner dimension of the head, often fixed to $\frac{d}{H}$. Notably, this output already considers the bias term of $W_V$.[^9] Then, following , we multiply this intermediate output by $W_O \in \mathbb{R}^{{d_\text{head} \times d}}$ and obtain the head’s final output.

#### Additional results

Tables , , , , present the correlation results between the static score $\phi_R(h)$ inferred by our method and the score $\phi^*_R(h)$ observed dynamically (both when we allow contextualization or not), obtained for Llama-3.1 70B, Llama-3.1 8B, Pythia 12B, Pythia 6.9B, GPT-2 xl. We also present the p-values and the maximum relation score obtained for any head in the model for the required relation. Notably, some of the lower correlations are demonstrated for relations that are not fully implemented by the model’s attention heads, as indicated by the small maximum relation scores. Tables , , , , present the results (following the same format) for the *suppression* relation scores.

## Causal Experiment

In § we measured the causal effect of removing the heads that implement a specific operation on the model’s performance in handling queries that depend on that operation.

#### Implementation details

We evaluate models on tasks for 13 relations. For each model, we filter out relations where (a) the base accuracy is very low ($<$<!-- -->0.1) or (b) there is no dataset for the relation (see §). The task prompts used for the different relations are presented in . Notably, When ablating an attention head, we remove its output only from the last position of the prompt.

#### Additional results

In Tables , , , , we present the extended experiment results for Llama-3.1 70B, Llama-3.1 8B, Pythia 12B, Pythia 6.9B, GPT-2 xl.

# Generalization to Multi-Token Entities – Additional Results

In § we conducted an experiment that evaluates how well the classifications by <span class="smallcaps">MAPS</span> generalize to contextualized inputs. shows the full results of this experiment.

# Comparison to Head Operations Identified in Prior Works

#### Name-mover heads in GPT-2 small

studied the *Indirect Object Identification* circuit in GPT-2 small. Analyzing the operations of the circuit’s heads, they defined heads that copy names as *Name-Mover* heads and heads that suppress names as *Negative Name-Mover* heads. They also classified heads that contribute to these tasks when the original mover heads are ablated as “backup” mover heads.

Using <span class="smallcaps">MAPS</span> we classified all three name-mover heads as implementing the `name copying` relation, and the two negative name-mover heads as implementing the suppression variant of `name copying`. We note that a similar analysis was performed by as well. However, by applying <span class="smallcaps">MAPS</span> to all heads in the model, and not just the heads in the discovered circuit, we were able to identify 21 additional name-copying heads as well, 6 of which were identified by as “backup” heads. One backup mover head and one backup negative mover head that were identified by , were not identified by <span class="smallcaps">MAPS</span>. Moreover, we find that each of the five identified name-mover heads implements a myriad of other relations. In we present the `name copying` relation scores for all heads in GPT-2 small and the heads classified by .

#### Mover heads in GPT-2 medium

studied the Indirect Object Identification (IOI) and Colored Objects circuits in GPT-2 medium. They discovered two sets of attention heads implementing certain functions, both called “Mover” heads. Heads from the first set copy names (in IOI), and heads from the second set copy colors (in the Colored Objects task). The authors also point out a significant overlap between the two sets.

Using <span class="smallcaps">MAPS</span>, we classified all mover heads as implementing the `name copying` relation. We find that many of these heads also implement the relations: `year to following, country to language, country to capital, copying`. Lastly, we identify 31 other name-copying heads. Notably, in our counting, we omit the heads 14.5, 17.10, 16.0, 18.12, and 21.7, which are labeled in Figure 2 of as Mover-heads. This is because, to the best of our understanding, the paper does not provide any explanation for why they are classified as such, while other heads are described as more important than them.

#### Capital heads in GPT-2 medium

have also studied a circuit for resolving the capital city of a country (in Appendix I). <span class="smallcaps">MAPS</span> identified all attention heads classified in that study, along with 15 others. In we present the `name copying, country to capital` relation scores for all heads in GPT-2 medium and the heads classified by .

# Automatic Mapping of Salient Head Operations

## Automatic Functionality Inference

In § we showed that GPT-4o can be utilized to interpret attention heads’ salient operations. Here, we provide additional implementation details and present an evaluation of the interpretation quality.

#### Implementation details

We found that GPT-4o sometimes describes in words that the pattern is unclear, rather than just outputting the word “Unclear”, as requested. To handle these cases, we classify every head for which GPT-4o’s response contained the string “clear” as a head where a pattern was not detected. We view this as an upper bound over the true ratio of heads with undetected patterns. Also, for some heads, GPT-4o would stop generating descriptions mid-generation. We hypothesize that it is because of strings viewed as special GPT-4o tokens that appeared in the salient mappings. We solved this issue by querying GPT-4o again with other random seeds. We note that in several mappings the salient tokens were decoded as an unreadable character. This could be solved by alternating between Transformers package decoding functions.

#### Prompt format

We present the prompt used to query GPT-4o in .

#### Examples

provides examples of salient mappings and the patterns described by GPT-4o for three attention heads in GPT-2 xl and Pythia 6.9B.

## Interpretation Quality

To assess the accuracy and plausibility of the model-generated descriptions, we let human annotators — five graduate students who are fluent English speakers — evaluate its responses in terms of (a) did GPT-4o correctly recognize the existence of a pattern in the mappings, (b) the quality of the generated descriptions, (c) the category of the recognized patterns. We conduct this study for a random sample of 138 (13.5%) heads in Pythia 6.9B and 134 (11.2%) heads in GPT-2 xl.

#### Annotation instructions

We present the instructions given to the human annotators in Figures ,.

#### Human study results

The overall results per question and the distribution of responses across models and layers are presented in (Question 1), (Question 2), (Question 3). In 80% of the cases, GPT-4o correctly identifies the presence or absence of a pattern. In most of the failure cases (87%), the model described a pattern that is not visible in the mappings. We also find that in lower layers there are fewer patterns and they are harder to parse: there are higher rates of *unnatural* patterns and inaccurate descriptions. This agrees with our findings in §. In case of an observable pattern, GPT-4o will identify it: for 95% of heads with observable patterns, GPT-4o described a pattern, and $<$<!-- -->2% of the descriptions were labeled “poor”. Overall, this analysis shows that the quality of our automatic annotation pipeline is reasonable and demonstrates promising trends in automatically interpreting attention heads with <span class="smallcaps">MAPS</span>. We leave further improvements to the pipeline for future work to explore.

<figure id="fig:human_val_q1">
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q1_pie" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 1.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q1_by_layer_gpt" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 1 across layers (GPT-2 xl).</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q1_by_layer_pythia" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 1 across layers (Pythia 6.9B).</figcaption>
</figure>
<figcaption>Quality of GPT-4o interpretation (§<span class="math inline">\(\ref{appendix:automatic_mapping}\)</span>) - Human annotation distribution for Question 1.</figcaption>
</figure>

<figure id="fig:human_val_q2">
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q2_pie" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 2.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q2_by_layer_gpt" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 2 across layers (GPT-2 xl).</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q2_by_layer_pythia" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 2 across layers (Pythia 6.9B).</figcaption>
</figure>
<figcaption>Quality of GPT-4o interpretation (§<span class="math inline">\(\ref{appendix:automatic_mapping}\)</span>) - Human annotation distribution for Question 2.</figcaption>
</figure>

<figure id="fig:human_val_q3">
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q3_pie" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 3.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q3_by_layer_gpt" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 3 across layers (GPT-2 xl).</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/human_validation/q3_by_layer_pythia" data-original-image-title=""></span>
<figcaption>Human annotation distribution for Question 3 across layers (Pythia 6.9B).</figcaption>
</figure>
<figcaption>Quality of GPT-4o interpretation (§<span class="math inline">\(\ref{appendix:automatic_mapping}\)</span>) - Human annotation distribution for Question 3.</figcaption>
</figure>

# Analysis of Global Versus Specific Functionality

We observe that the mappings in $M$ provide a broad view of the head’s functionality, particularly on how *global* the head’s operation is. For example, a head that maps any token to an end-of-sequence token has *global* functionality, whereas heads that map countries to their capitals, colors to their complementary pairs, and so on, demonstrate *specific* operations. In this section, we use properties of $M$ to analyze how global the functionalities of attention heads in LLMs are.

#### Analysis

We estimate how global the functionality of a given head is using two metrics: *input skewness*, which captures the skewness of the head’s operation towards specific inputs, and *output space size*, which estimates the number of tokens the head tends to output. For input skewness, we obtain the saliency scores $\sigma_t (W_{VO}) \;\forall t\in \mathcal{V}$ according to the head (see §), and calculate the skewness of their distribution. For output space size, we compute for every token $s \in \mathcal{V}$ the highest-score token $t$ it is mapped into according to $M$: $t = \arg\max(\mathbf{m}_s)$. Next, we define the output space size to be the portion of unique output tokens over the vocabulary. For instance, we expect the output space of a head that only maps strings to their first letters to be a small set of letter tokens. Similarly to the normalization of the saliency scores by the embedding norms, which we applied in §, here, when calculating $M$, we normalize the *unembeddings* ($U$’s columns).

<figure id="fig:skewness_lineplots">
<span class="image placeholder" data-original-image-src="figures/skewness_vs_output_size" data-original-image-title=""></span>
<figcaption>Input skewness versus output space size for all attention heads per layer in Pythia 6.9B and GPT-2 xl, compared to baseline heads of global and specific functionalities. Lower input skewness indicates a larger input space. </figcaption>
</figure>

Additionally, we present two baselines. The first baseline, dubbed “specific head”, represents the output space size of a head that maps the entire vocabulary to 1 specific token (e.g. a head that always outputs the end of sequence token). The second baseline, called “global head”, represents the output space size of a head that maps the entire vocabulary to capitalized tokens with leading spaces - a subset whose size is 25% of the vocabulary of GPT-2 xl, and 16% of the vocabulary of Pythia 6.9B. An example of such a “global head” is a head that maps every word (or sub-word) in English to its capitalized version, and all other tokens to one specific token.

#### Results

shows the input skewness and output space sizes for all heads in Pythia 6.9B and GPT-2 xl. In both models, the input skewness rises and then sharply decreases in the early layers, after which it stabilizes. This implies that attention heads in shallower layers induce a salient effect into a specific set of inputs compared to later layers. In contrast, the output space size generally decreases across layers with a slight increase in the final layers, suggesting that head outputs across layers converge to smaller token subsets. Taken together, we hypothesize that early layer heads demonstrate their functionality on fewer inputs than deeper heads, which in turn map a larger set of possible inputs to a small set of outputs.

# Resources and Packages

In our experiments, we used models and code from the transformers and TransformerLens packages, and nanoGPT.[^10] All the experiments were conducted using a single A100 80GB or H100 80GB GPU, aside from the experiments studying Llama-3.1 70B, which used nodes with 8 of these GPUs.

[^1]: The average Jaccard similarity of the sets obtained for heads in GPT-2 xl is 0.01.

[^2]: We do not simply feed in $s$ as input to avoid potential biases from the attention sink phenomenon .

[^3]: Here the head outputs include the bias term of $W_V$, see §.

[^4]: This could be extended to suppression for better coverage.

[^5]: <https://www.kaggle.com/datasets/wheelercode/english-word-frequency-list>

[^6]: <https://storage.googleapis.com/books/ngrams/books/datasetsv3.html>

[^7]: <https://chatgpt.com/>

[^8]: Notably, we did not apply the first MLP when we analyzed heads from the models’ first layers (layer 0), since the first attention layer precedes the first MLP in the computation.

[^9]: In , $\hat{y}$ is viewed as the head’s final output.

[^10]: <https://github.com/karpathy/nanoGPT>
