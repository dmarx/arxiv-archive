\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebron, and Sanghai}]{Ainslie2023GQATG}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-main.298} {{GQA}: Training generalized multi-query transformer models from multi-head checkpoints}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4895--4901, Singapore. Association for Computational Linguistics.

\bibitem[{Arditi et~al.(2024)Arditi, Obeso, Syed, Paleka, Panickssery, Gurnee, and Nanda}]{arditi2024refusal}
Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024.
\newblock Refusal in language models is mediated by a single direction.
\newblock \emph{arXiv preprint arXiv:2406.11717}.

\bibitem[{Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal}]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der Wal. 2023.
\newblock \href {https://proceedings.mlr.press/v202/biderman23a.html} {Pythia: {A} suite for analyzing large language models across training and scaling}.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 2397--2430. {PMLR}.

\bibitem[{Bolukbasi et~al.(2021)Bolukbasi, Pearce, Yuan, Coenen, Reif, Vi{\'e}gas, and Wattenberg}]{bolukbasi2021interpretability}
Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Vi{\'e}gas, and Martin Wattenberg. 2021.
\newblock \href {https://arxiv.org/abs/2104.07143} {An interpretability illusion for bert}.
\newblock \emph{ArXiv preprint}, abs/2104.07143.

\bibitem[{Clark et~al.(2019)Clark, Khandelwal, Levy, and Manning}]{clark-etal-2019-bert}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D. Manning. 2019.
\newblock \href {https://doi.org/10.18653/v1/W19-4828} {What does {BERT} look at? an analysis of {BERT}{'}s attention}.
\newblock In \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}, pages 276--286, Florence, Italy. Association for Computational Linguistics.

\bibitem[{Dar et~al.(2023)Dar, Geva, Gupta, and Berant}]{Dar2022AnalyzingTI}
Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.acl-long.893} {Analyzing transformers in embedding space}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 16124--16170, Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.
\newblock \href {https://arxiv.org/abs/2407.21783} {The llama 3 herd of models}.
\newblock \emph{ArXiv preprint}, abs/2407.21783.

\bibitem[{Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly et~al.}]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et~al. 2021.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1(1):12.

\bibitem[{Ferrando et~al.(2024)Ferrando, Sarti, Bisazza, and Costa-juss{\`a}}]{ferrando2024primer}
Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta~R Costa-juss{\`a}. 2024.
\newblock \href {https://arxiv.org/abs/2405.00208} {A primer on the inner workings of transformer-based language models}.
\newblock \emph{ArXiv preprint}, abs/2405.00208.

\bibitem[{Gao et~al.(2024)Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu}]{gao2024scaling}
Leo Gao, Tom~Dupr{\'e} la~Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. 2024.
\newblock \href {https://arxiv.org/abs/2406.04093} {Scaling and evaluating sparse autoencoders}.
\newblock \emph{ArXiv preprint}, abs/2406.04093.

\bibitem[{Garc{\'{\i}}a{-}Carrasco et~al.(2024)Garc{\'{\i}}a{-}Carrasco, Mat{\'{e}}, and Trujillo}]{garcia2024does}
Jorge Garc{\'{\i}}a{-}Carrasco, Alejandro Mat{\'{e}}, and Juan~C. Trujillo. 2024.
\newblock \href {https://proceedings.mlr.press/v238/garcia-carrasco24a.html} {How does {GPT-2} predict acronyms? extracting and understanding a circuit via mechanistic interpretability}.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics, 2-4 May 2024, Palau de Congressos, Valencia, Spain}, volume 238 of \emph{Proceedings of Machine Learning Research}, pages 3322--3330. {PMLR}.

\bibitem[{Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg}]{geva-etal-2022-transformer}
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.emnlp-main.3} {Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 30--45, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Geva et~al.(2021)Geva, Schuster, Berant, and Levy}]{geva-etal-2021-transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.446} {Transformer feed-forward layers are key-value memories}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 5484--5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[{Ghandeharioun et~al.(2024)Ghandeharioun, Caciularu, Pearce, Dixon, and Geva}]{ghandeharioun2024patchscopes}
Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. 2024.
\newblock \href {https://openreview.net/forum?id=5uwBzcn885} {Patchscopes: A unifying framework for inspecting hidden representations of language models}.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[{Gould et~al.(2024)Gould, Ong, Ogden, and Conmy}]{gould2024successor}
Rhys Gould, Euan Ong, George Ogden, and Arthur Conmy. 2024.
\newblock \href {https://openreview.net/forum?id=kvcbV8KQsi} {Successor heads: Recurring, interpretable attention heads in the wild}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Hernandez et~al.(2024)Hernandez, Sharma, Haklay, Meng, Wattenberg, Andreas, Belinkov, and Bau}]{hernandez2024linearity}
Evan Hernandez, Arnab~Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. 2024.
\newblock \href {https://openreview.net/forum?id=w7LU2s14kE} {Linearity of relation decoding in transformer language models}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Hurst et~al.(2024)Hurst, Lerer, Goucher, Perelman, Ramesh, Clark, Ostrow, Welihinda, Hayes, Radford et~al.}]{hurst2024gpt}
Aaron Hurst, Adam Lerer, Adam~P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ~Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et~al. 2024.
\newblock \href {https://arxiv.org/abs/2410.21276} {Gpt-4o system card}.
\newblock \emph{ArXiv preprint}, abs/2410.21276.

\bibitem[{Javaheripi and Bubeck(2023)}]{phi2}
Mojan Javaheripi and SÃ©bastien Bubeck. 2023.
\newblock \href {https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/} {Phi-2: The surprising power of small language models}.

\bibitem[{Katz et~al.(2024)Katz, Belinkov, Geva, and Wolf}]{katz-etal-2024-backward}
Shahar Katz, Yonatan Belinkov, Mor Geva, and Lior Wolf. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.emnlp-main.142} {Backward lens: Projecting language model gradients into the vocabulary space}.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 2390--2422, Miami, Florida, USA. Association for Computational Linguistics.

\bibitem[{Kim et~al.(2024)Kim, Valentino, and Freitas}]{kim2024mechanistic}
Geonhee Kim, Marco Valentino, and Andr{\'e} Freitas. 2024.
\newblock \href {https://arxiv.org/abs/2408.08590} {A mechanistic interpretation of syllogistic reasoning in auto-regressive language models}.
\newblock \emph{ArXiv preprint}, abs/2408.08590.

\bibitem[{Kissane et~al.(2024)Kissane, Krzyzanowski, Bloom, Conmy, and Nanda}]{kissane2024interpreting}
Connor Kissane, Robert Krzyzanowski, Joseph~Isaac Bloom, Arthur Conmy, and Neel Nanda. 2024.
\newblock \href {https://openreview.net/forum?id=fewUBDwjji} {Interpreting attention layer outputs with sparse autoencoders}.
\newblock In \emph{ICML 2024 Workshop on Mechanistic Interpretability}.

\bibitem[{Li et~al.(2015)Li, Yosinski, Clune, Lipson, and Hopcroft}]{pmlr-v44-li15convergent}
Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. 2015.
\newblock \href {https://proceedings.mlr.press/v44/li15convergent.html} {Convergent learning: Do different neural networks learn the same representations?}
\newblock In \emph{Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015}, volume~44 of \emph{Proceedings of Machine Learning Research}, pages 196--212, Montreal, Canada. PMLR.

\bibitem[{Loper and Bird(2002)}]{bird-loper-2004-nltk}
Edward Loper and Steven Bird. 2002.
\newblock \href {https://doi.org/10.3115/1118108.1118117} {{NLTK}: The natural language toolkit}.
\newblock In \emph{Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics}, pages 63--70, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

\bibitem[{McDougall et~al.(2024)McDougall, Conmy, Rushing, McGrath, and Nanda}]{mcdougall-etal-2024-copy}
Callum~Stuart McDougall, Arthur Conmy, Cody Rushing, Thomas McGrath, and Neel Nanda. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.blackboxnlp-1.22} {Copy suppression: Comprehensively understanding a motif in language model attention heads}.
\newblock In \emph{Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP}, pages 337--363, Miami, Florida, US. Association for Computational Linguistics.

\bibitem[{Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov}]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022.
\newblock \href {http://papers.nips.cc/paper\_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html} {Locating and editing factual associations in {GPT}}.
\newblock In \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}.

\bibitem[{Merullo et~al.(2024{\natexlab{a}})Merullo, Eickhoff, and Pavlick}]{merullo2024circuit}
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2024{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=fpoAYV6Wsk} {Circuit component reuse across tasks in transformer language models}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Merullo et~al.(2024{\natexlab{b}})Merullo, Eickhoff, and Pavlick}]{merullo2024talking}
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2024{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=LUsx0chTsL} {Talking heads: Understanding inter-layer communication in transformer language models}.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}.

\bibitem[{Millidge and Black(2022)}]{svd-interpretable}
Beren Millidge and Sid Black. 2022.
\newblock \href {https://www.alignmentforum.org/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight} {The singular value decompositions of transformer weight matrices are highly interpretable}.

\bibitem[{Nanda and Bloom(2022)}]{nanda2022transformerlens}
Neel Nanda and Joseph Bloom. 2022.
\newblock Transformerlens.
\newblock \url{https://github.com/TransformerLensOrg/TransformerLens}.

\bibitem[{nostalgebraist(2020)}]{logitlens}
nostalgebraist. 2020.
\newblock \href {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens} {Interpreting gpt: the logit lens}.

\bibitem[{Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen et~al.}]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al. 2022.
\newblock \href {https://arxiv.org/abs/2209.11895} {In-context learning and induction heads}.
\newblock \emph{ArXiv preprint}, abs/2209.11895.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever et~al.}]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1(8):9.

\bibitem[{Schober et~al.(2018)Schober, Boer, and Schwarte}]{Schober2018CorrelationCA}
Patrick Schober, Christa Boer, and Lothar~A. Schwarte. 2018.
\newblock \href {https://api.semanticscholar.org/CorpusID:13354506} {Correlation coefficients: Appropriate use and interpretation}.
\newblock \emph{Anesthesia \& Analgesia}, 126:1763â1768.

\bibitem[{Tigges et~al.(2024)Tigges, Hanna, Yu, and Biderman}]{tigges2024llm}
Curt Tigges, Michael Hanna, Qinan Yu, and Stella Biderman. 2024.
\newblock \href {https://openreview.net/forum?id=3Ds5vNudIE} {{LLM} circuit analyses are consistent across training and scale}.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}]{Vaswani2017AttentionIA}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock \href {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html} {Attention is all you need}.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pages 5998--6008.

\bibitem[{Vig and Belinkov(2019)}]{vig-belinkov-2019-analyzing}
Jesse Vig and Yonatan Belinkov. 2019.
\newblock \href {https://doi.org/10.18653/v1/W19-4808} {Analyzing the structure of attention in a transformer language model}.
\newblock In \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}, pages 63--76, Florence, Italy. Association for Computational Linguistics.

\bibitem[{Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and Titov}]{voita2019analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1580} {Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 5797--5808, Florence, Italy. Association for Computational Linguistics.

\bibitem[{Vrande\v{c}i\'{c} and Kr\"{o}tzsch(2014)}]{10.1145/2629489}
Denny Vrande\v{c}i\'{c} and Markus Kr\"{o}tzsch. 2014.
\newblock \href {https://doi.org/10.1145/2629489} {Wikidata: a free collaborative knowledgebase}.
\newblock \emph{Commun. ACM}, 57(10):78â85.

\bibitem[{Wang et~al.(2023)Wang, Variengien, Conmy, Shlegeris, and Steinhardt}]{wang2022interpretability}
Kevin~Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2023.
\newblock \href {https://openreview.net/pdf?id=NpsVSN6o4ul} {Interpretability in the wild: a circuit for indirect object identification in {GPT-2} small}.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Le~Scao, Gugger, Drame, Lhoest, and Rush}]{wolf2019transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le~Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-demos.6} {Transformers: State-of-the-art natural language processing}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online. Association for Computational Linguistics.

\bibitem[{Xiao et~al.(2024)Xiao, Tian, Chen, Han, and Lewis}]{xiao2024efficient}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024.
\newblock \href {https://openreview.net/forum?id=NG7sS51zVF} {Efficient streaming language models with attention sinks}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Yom~Din et~al.(2024)Yom~Din, Karidi, Choshen, and Geva}]{yom-din-etal-2024-jump}
Alexander Yom~Din, Taelin Karidi, Leshem Choshen, and Mor Geva. 2024.
\newblock \href {https://aclanthology.org/2024.lrec-main.840} {Jump to conclusions: Short-cutting transformers with linear transformations}.
\newblock In \emph{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 9615--9625, Torino, Italia. ELRA and ICCL.

\bibitem[{Zheng et~al.(2024)Zheng, Wang, Huang, Song, Tang, Xiong, and Li}]{zheng2024attention}
Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Bo~Tang, Feiyu Xiong, and Zhiyu Li. 2024.
\newblock \href {https://arxiv.org/abs/2409.03752} {Attention heads of large language models: A survey}.
\newblock \emph{ArXiv preprint}, abs/2409.03752.

\bibitem[{Zhou et~al.(2024)Zhou, Yu, Zhang, Xu, Huang, Wang, Liu, Fang, and Li}]{zhou2024role}
Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, and Yongbin Li. 2024.
\newblock \href {https://arxiv.org/abs/2410.13708} {On the role of attention heads in large language model safety}.
\newblock \emph{ArXiv preprint}, abs/2410.13708.

\end{thebibliography}
