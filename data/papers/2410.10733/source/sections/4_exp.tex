\section{Experiments}
\label{sec:exp}

\input{tables/ae_main}

\subsection{Setups}

\paragraph{Implementation Details.} We use a mixture of datasets to train autoencoders (baselines and \modelshort), containing ImageNet \citep{deng2009imagenet}, SAM \citep{kirillov2023segment}, MapillaryVistas \citep{neuhold2017mapillary}, and FFHQ \citep{karras2019style}. For ImageNet experiments, we exclusively use the ImageNet training split to train autoencoders and diffusion models. The model architecture is similar to SD-VAE \citep{rombach2022high} except for our new designs discussed in Section~\ref{sec:main_model}. In addition, we use the original autoencoders instead of the variational autoencoders for our models, as they perform the same in our experiments and the original autoencoders are simpler. We also replace transformer blocks with EfficientViT blocks \citep{cai2023efficientvit} to make autoencoders more friendly for handling high-resolution images while maintaining similar accuracy. 

For image generation experiments, we apply autoencoders to diffusion transformer models including DiT \citep{peebles2023scalable} and UViT \citep{bao2023all}. We follow the same training settings as the original papers. 
We consider three settings with different resolutions, including ImageNet \citep{deng2009imagenet} for $512 \times 512$ generation, FFHQ \citep{karras2019style} and MJHQ \citep{li2024playground} for $1024 \times 1024$ generation, and MapillaryVistas \citep{neuhold2017mapillary} for $2048 \times 2048$ generation. 

\input{tables/diffusion_imagenet_main}

\input{tables/diffusion_t2i}

\input{figures/ae_visualization}

\input{figures/diffusion_visualization}

\paragraph{Efficiency Profiling.} We profile the training and inference throughput on the H100 GPU with PyTorch and TensorRT respectively. The latency is measured on the 3090 GPU with batch size 2. The training memory is profiled using PyTorch, assuming a batch size of 256. We use fp16 for all cases. For simplicity, we assume the number of sampling steps is 1.

\subsection{Image Compression and Reconstruction}
Table~\ref{tab:ae_main} summarizes the results of \modelshort and SD-VAE \citep{rombach2022high} under various settings (f represents the spatial compression ratio and c denotes the number of latent channels). \modelshort provides significant reconstruction accuracy improvements than SD-VAE for all cases. For example, on ImageNet $512 \times 512$, \modelshort improves the rFID from 16.84 to 0.22 for the f64c128 autoencoder and 100.74 to 0.23 for the f128c512 autoencoder. 

In addition to the quantitative results, Figure~\ref{fig:ae_visualization} shows image reconstruction samples produced by SD-VAE and \modelshort. Reconstructed images by \modelshort demonstrate a better visual quality than SD-VAE's reconstructed images. In particular, for the f64 and f128 autoencoders,  \modelshort still maintains a good visual quality for small text and the human face. 

\subsection{Latent Diffusion Models}
We compare \modelshort with the widely used SD-VAE-f8 autoencoder \citep{rombach2022high} on various diffusion transformer models. For \modelshort, we always use a patch size of 1 (denoted as p1). For SD-VAE-f8, we follow the common setting and use a patch size of 2 or 4 (denoted as p2, p4). The results are summarized in Table~\ref{tab:diffusion_imagenet_main}, Table~\ref{tab:diffusion_hr_main}, and Table~\ref{tab:diffusion_t2i_main}. 

\vspace{-5pt}
\paragraph{ImageNet 512$\times$512.} As shown in Table~\ref{tab:diffusion_imagenet_main}, \modelshort-f32p1 consistently delivers better FID than SD-VAE-f8p2 on all diffusion transformer models. In addition, it has 4$\times$ fewer tokens than SD-VAE-f8p2, leading to 4.5$\times$ higher H100 training throughput and 4.8$\times$ higher H100 inference 
\input{figures/diffusion_scaling_up}
\!\!\!\! throughput for DiT-XL. We also observe that larger diffusion transformer models seem to benefit more from our \modelshort. For example, \modelshort-f64p1 has a worse FID than SD-VAE-f8p2 on UViT-S but a better FID on UViT-H. We conjecture it is because \modelshort-f64 has a larger latent channel number than SD-VAE-f8, thus needing more model capacity \citep{esser2024scaling}. 

\vspace{-10pt}
\paragraph{Text-to-Image Generation.} Table~\ref{tab:diffusion_t2i_main} reports our text-to-image generation results. All models are trained for 100K iterations from scratch. Similar to prior cases, we observe \modelshort-f32p1 provides a better FID and a better CLIP Score than SD-VAE-f8p2. Figure~\ref{fig:diffusion_visualization} demonstrates samples generated by the diffusion models with our \modelshort, showing the capacity to synthesize high-quality images while being significantly more efficient than prior models.