\section{Related Work}
\label{sec:related}

\paragraph{Autoencoder for Diffusion Models.}
Training and evaluating diffusion models directly in high-resolution pixel space results in prohibitive computational costs. To address this issue, \cite{rombach2022high} proposes latent diffusion models that operate in a compressed latent space produced by pretrained autoencoders. The proposed autoencoder with $8\times$ spatial compression ratio and $4$ latent channels has been widely adopted in subsequent works \citep{peebles2023scalable,bao2023all}. Since then, follow-up works mainly focus on enhancing the reconstruction accuracy of the f8 autoencoder by increasing the number of latent channels \citep{esser2024scaling,dai2023emu,flux2024}. Additionally, to improve the reconstruction quality, \cite{zhu2023designing} leverages a heavier decoder and incorporates task-specific priors. In contrast to prior works, our work focuses on an orthogonal direction, increasing the spatial compression ratio of the autoencoders (e.g., f64). To the best of our knowledge, our work is the first study in this critical but underexplored direction. 

\vspace{-5pt}
\paragraph{Diffusion Model Acceleration.}
Diffusion models have been widely used for image generation and showed impressive results \citep{flux2024,esser2024scaling}. However, diffusion models are computationally intensive, motivating many works to accelerate diffusion models. One representative strategy is reducing the number of inference sampling steps by training-free few-step samplers \citep{songdenoising, lu2022dpm, lu2022dpm2, zheng2023dpm, zhangfast, zhanggddim, zhao2024unipc, shih2024parallel, tang2024accelerating} or distilling-based methods \citep{meng2023distillation, salimans2022progressive, yin2024one, yin2024improved, song2023consistency, luo2023latent, liu2023instaflow}. Another representative strategy is model compression by leveraging sparsity \citep{li2022efficient, ma2024deepcache} or quantization \citep{he2024ptqd, fang2024structural, li2023q, zhao2024vidit}. Designing efficient diffusion model architectures \citep{li2024snapfusion,liu2024linfusion,cai2024condition} or inference systems \citep{li2024distrifusion, wang2024pipefusion} is also an effective approach for boosting efficiency. In addition, improving the data quality \citep{chenpixart,chen2024pixart} can boost the training efficiency of diffusion models. 

All these works focus on diffusion models while the autoencoder remains the same. Our work opens up a new direction for accelerating diffusion models, which can benefit both training and inference.