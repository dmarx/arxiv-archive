\begin{table}[t]
\small\centering\setlength{\tabcolsep}{2pt}
\begin{tabular}{l | g | g | g g | g | g | g g }
\toprule
\rowcolor{white} Diffusion & & Patch & \multicolumn{2}{c|}{Throughput (image/s) $\uparrow$} & Latency & Memory & \multicolumn{2}{c}{FID $\downarrow$} \\
\rowcolor{white} Model & \multirow{-2}{*}{Autoencoder} & Size & Training & Inference & (ms) $\downarrow$ & (GB) $\downarrow$ & w/o CFG & w/ CFG \\
\midrule
\rowcolor{white} & SD3-VAE-f8 \tablecite{esser2024scaling}                   & 2 &  352 &  2984 & 3.8 & 13.8 & 164.34 & 143.82 \\
\rowcolor{white} & Flux-VAE-f8 \tablecite{flux2024}                          & 2 &  352 &  2984 & 3.8 & 13.8 & 106.07 & 84.73 \\
\cmidrule{2-9}
\rowcolor{white} & SDXL-VAE-f8 \tablecite{podell2023sdxl}                    & 2 &  352 &  2991 & 3.8 & 13.8 & 51.03 & 26.38 \\
\rowcolor{white} & Asym-VAE-f8 \tablecite{zhu2023designing}                  & 2 &  352 &  2991 & 3.8 & 13.8 & 51.96 & 24.57 \\
\rowcolor{white} & SD-VAE-f8 \tablecite{rombach2022high}                     & 2 &  352 &  2991 & 3.8 & 13.8 & 51.96 & 24.57 \\
\rowcolor{white} & SD-VAE-f16 \tablecite{rombach2022high}                    & 2 & 1550 & 12881 & 1.3 &  4.0 & 76.86 & 44.22 \\
\rowcolor{white} & SD-VAE-f32 \tablecite{rombach2022high}                    & 1 & 1551 & 12883 & 1.3 &  4.0 & 70.23 & 38.63 \\
\cmidrule{2-9}
& \modelshort-f32                                                            & 1 & 1553 & 12850 & 1.3 &  4.0 & \textbf{46.12} & \textbf{18.08} \\ 
& \modelshort-f64                                                            & 1 & \textbf{6295} & \textbf{53774} & \textbf{0.7} & \textbf{1.5} & 67.30 & 35.96 \\
\multirow{-11}{*}{UViT-S \tablecite{bao2023all}} & \:\:\modelshort-f64$^\dag$ & 1 & \textbf{6295} & \textbf{53774} & \textbf{0.7} & \textbf{1.5} & 61.84 & 30.63 \\
\midrule
\midrule
\rowcolor{white} & Flux-VAE-f8 \tablecite{flux2024}                          & 2 &   54 &  416 & 31.7 & 56.3 & 27.35 & 8.72 \\
\cmidrule{2-9}
\rowcolor{white} & Asym-VAE-f8 \tablecite{zhu2023designing}                  & 2 &   54 &  424 & 31.7 & 56.2 & 11.55 & 2.95 \\
\rowcolor{white} &  SD-VAE-f8 \tablecite{rombach2022high}                    & 2 &   54 &  424 & 31.7 & 56.2 & 12.03 & 3.04 \\
\cmidrule{2-9}
& \modelshort-f32                                                            & 1 &  \textbf{241} &  \textbf{2016} & \textbf{7.8} & \textbf{20.9} &  9.56 & 2.84 \\
\multirow{-5}{*}{DiT-XL \tablecite{peebles2023scalable}} 
& \:\:\modelshort-f32$^\ddag$                                                    & 1 &  \textbf{241} &  \textbf{2016} & \textbf{7.8} & \textbf{20.9} &  \textbf{6.88} & \textbf{2.41} \\


\midrule
\midrule
\rowcolor{white} & Flux-VAE-f8 \tablecite{flux2024}                          & 2 &  55 &  349 & 30.4 & 54.2 & 30.91 & 12.63 \\
\cmidrule{2-9}
\rowcolor{white} & Asym-VAE-f8 \tablecite{zhu2023designing}                  & 2 &  55 &  351 & 30.3 & 54.1 & 11.36 &  3.51 \\
\rowcolor{white}  & SD-VAE-f8 \tablecite{rombach2022high}                    & 2 &   \,\,55\,\,\tikzmark{uvit_h_f8p2:a1} &   \,\,351\,\,\tikzmark{uvit_h_f8p2:a2} & 30.3 & 54.1 & 11.04 & \,\,3.55\,\,\tikzmark{uvit_h_f8p2:a3} \\
\cmidrule{2-9}
& \modelshort-f32                                                            & 1 &  247 &  1622 &  8.2 & 18.6 &  \textbf{9.83} & \textbf{2.53} \\
& \modelshort-f64                                                            & 1 & \,\,\textbf{984}\,\,\tikzmark{uvit_h_f64p1:a1} &  \,\,\textbf{6706}\,\,\tikzmark{uvit_h_f64p1:a2} & \textbf{3.5} & \textbf{10.6} & 13.96 & \,\,3.01\,\,\tikzmark{uvit_h_f64p1:a3} \\
\multirow{-6}{*}{UViT-H \tablecite{bao2023all}} & \:\:\modelshort-f64$^\dag$ & 1 & \textbf{984} &  \textbf{6706} & \textbf{3.5} & \textbf{10.6} & 12.26 & 2.66 \\

\midrule
\midrule
\rowcolor{white} & Asym-VAE-f8 \tablecite{zhu2023designing}                   & 2 &  27 & 157 & 74.8 & OOM & 9.87 & 3.62 \\
\rowcolor{white}  & SD-VAE-f8 \tablecite{rombach2022high}                     & 2 &  27 & 157 & 74.8 & OOM & 9.73 & 3.57 \\
\cmidrule{2-9}
& \modelshort-f32                                                             & 1 & 112 & 665 & 19.7 & 42.0 & 8.13 & 2.30 \\
& \modelshort-f64                                                             & 1 & \textbf{450} & \textbf{2733} & \textbf{8.6} & \textbf{30.2} & 7.78 & 2.47 \\
\multirow{-5}{*}{UViT-2B \tablecite{bao2023all}} & \:\:\modelshort-f64$^\dag$ & 1 & \textbf{450} & \textbf{2733} & \textbf{8.6} & \textbf{30.2} & \textbf{6.50} & \textbf{2.25} \\

\bottomrule
\end{tabular}

\begin{tikzpicture}[overlay, remember picture, shorten >=.5pt, shorten <=.5pt, transform canvas={yshift=.25\baselineskip}]

\draw [->, red] ({pic cs:uvit_h_f8p2:a1}) [bend left] to node [below right] (uvit_h_f8p2:t1) {\hspace{-2pt}\scriptsize \textbf{17.9$\times$}} ({pic cs:uvit_h_f64p1:a1});

\draw [->, red] ({pic cs:uvit_h_f8p2:a2}) [bend left] to node [below right] (uvit_h_f8p2:t2) {\hspace{-2pt}\scriptsize \textbf{19.1$\times$}} ({pic cs:uvit_h_f64p1:a2});

\draw [->, red] ({pic cs:uvit_h_f8p2:a3}) [bend left] to node [below right] (uvit_h_f8p2:t3) {\hspace{-2pt}\scriptsize \textbf{-0.54}} ({pic cs:uvit_h_f64p1:a3});

\end{tikzpicture}
\vspace{-5pt}
\caption{\textbf{Class-Conditional Image Generation Results on ImageNet 512$\times$512.} $^\dag$ represents the model is trained for 4$\times$ training iterations (i.e., 500K $\rightarrow$ 2,000K iterations). $^\ddag$ represents the model is trained with 4$\times$ batch size (i.e., 256 $\rightarrow$ 1024).}
\label{tab:diffusion_imagenet_main}
\end{table}
