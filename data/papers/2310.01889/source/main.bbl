\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anthropic(2023)]{anthropic-claude}
Anthropic.
\newblock Introducing claude, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Bischof(2008)]{bischof2008parallel}
Christian Bischof.
\newblock \emph{Parallel computing: Architectures, algorithms, and
  applications}, volume~15.
\newblock IOS Press, 2008.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, et~al.]{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock \emph{See https://vicuna.lmsys.org}, 2023.

\bibitem[Danalis et~al.(2005)Danalis, Kim, Pollock, and
  Swany]{danalis2005transformations}
Anthony Danalis, Ki-Yong Kim, Lori Pollock, and Martin Swany.
\newblock Transformations to parallel codes for communication-computation
  overlap.
\newblock In \emph{SC'05: Proceedings of the 2005 ACM/IEEE conference on
  Supercomputing}, pages 58--58. IEEE, 2005.

\bibitem[Danalis et~al.(2009)Danalis, Pollock, Swany, and
  Cavazos]{danalis2009mpi}
Anthony Danalis, Lori Pollock, Martin Swany, and John Cavazos.
\newblock Mpi-aware compiler optimizations for improving
  communication-computation overlap.
\newblock In \emph{Proceedings of the 23rd international conference on
  Supercomputing}, pages 316--325, 2009.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16344--16359, 2022.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, et~al.]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang, et~al.
\newblock Large scale distributed deep networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Facebook(2023)]{fbFullySharded}
Facebook.
\newblock {F}ully {S}harded {D}ata {P}arallel: faster {A}{I} training with
  fewer {G}{P}{U}s --- engineering.fb.com.
\newblock \url{https://engineering.fb.com/2021/07/15/open-source/fsdp/}, 2023.

\bibitem[Geng and Liu(2023)]{openllama2023}
Xinyang Geng and Hao Liu.
\newblock Openllama: An open reproduction of llama, may 2023.
\newblock \emph{URL https://github. com/openlm-research/open\_llama}, 2023.

\bibitem[Geng et~al.(2023)Geng, Gudibande, Liu, Wallace, Abbeel, Levine, and
  Song]{geng2023koala}
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey
  Levine, and Dawn Song.
\newblock Koala: A dialogue model for academic research.
\newblock \emph{Blog post, April}, 1, 2023.

\bibitem[Gibiansky(2017)]{gibiansky2017bringing}
Andrew Gibiansky.
\newblock Bringing hpc techniques to deep learning.
\newblock \emph{Baidu Research, Tech. Rep.}, 2017.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, et~al.]{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
  HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Hursey and Graham(2011)]{hursey2011building}
Joshua Hursey and Richard~L Graham.
\newblock Building a fault tolerant mpi application: A ring communication
  example.
\newblock In \emph{2011 IEEE International Symposium on Parallel and
  Distributed Processing Workshops and Phd Forum}, pages 1549--1556. IEEE,
  2011.

\bibitem[Jacobs et~al.(2023)Jacobs, Tanaka, Zhang, Zhang, Song, Rajbhandari,
  and He]{jacobs2023deepspeed}
Sam~Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song,
  Samyam Rajbhandari, and Yuxiong He.
\newblock Deepspeed ulysses: System optimizations for enabling training of
  extreme long sequence transformer models.
\newblock \emph{arXiv preprint arXiv:2309.14509}, 2023.

\bibitem[Korthikanti et~al.(2022)Korthikanti, Casper, Lym, McAfee, Andersch,
  Shoeybi, and Catanzaro]{korthikanti2022reducing}
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael
  Andersch, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Reducing activation recomputation in large transformer models.
\newblock \emph{arXiv preprint arXiv:2205.05198}, 2022.

\bibitem[Laskin et~al.(2021)Laskin, Yarats, Liu, Lee, Zhan, Lu, Cang, Pinto,
  and Abbeel]{laskin2021urlb}
Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu,
  Catherine Cang, Lerrel Pinto, and Pieter Abbeel.
\newblock Urlb: Unsupervised reinforcement learning benchmark.
\newblock \emph{arXiv preprint arXiv:2110.15191}, 2021.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Shao, Xie, Sheng, Zheng, E.~Gonzalez,
  Stoica, Ma, and Zhang]{longchat2023}
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph
  E.~Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.
\newblock How long can open-source llms truly promise on context length?, June
  2023{\natexlab{a}}.
\newblock URL \url{https://lmsys.org/blog/2023-06-29-longchat}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Xue, Baranwal, Li, and
  You]{li2021sequence}
Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.
\newblock Sequence parallelism: Long sequence training from system perspective.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,
  \emph{Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 2391--2404,
  Toronto, Canada, July 2023{\natexlab{b}}. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.134}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.134}.

\bibitem[Liu and Abbeel(2023{\natexlab{a}})]{liu2023agent}
Hao Liu and Pieter Abbeel.
\newblock Emergent agentic transformer from chain of hindsight experience.
\newblock \emph{International Conference on Machine Learning},
  2023{\natexlab{a}}.

\bibitem[Liu and Abbeel(2023{\natexlab{b}})]{liu2023blockwise}
Hao Liu and Pieter Abbeel.
\newblock Blockwise parallel transformer for large context models.
\newblock \emph{Advances in neural information processing systems},
  2023{\natexlab{b}}.

\bibitem[Milakov and Gimelshein(2018)]{milakov2018online}
Maxim Milakov and Natalia Gimelshein.
\newblock Online normalizer calculation for softmax.
\newblock \emph{arXiv preprint arXiv:1805.02867}, 2018.

\bibitem[MosaicML(2023)]{mosaicml-mpt-7b}
MosaicML.
\newblock Introducing mpt-7b: A new standard for open-source, commercially
  usable llms, 2023.
\newblock URL \url{https://www.mosaicml.com/blog/mpt-7b}.

\bibitem[Narang et~al.(2021)Narang, Chung, Tay, Fedus, Fevry, Matena, Malkan,
  Fiedel, Shazeer, Lan, et~al.]{narang2021transformer}
Sharan Narang, Hyung~Won Chung, Yi~Tay, William Fedus, Thibault Fevry, Michael
  Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et~al.
\newblock Do transformer modifications transfer across implementations and
  applications?
\newblock \emph{arXiv preprint arXiv:2102.11972}, 2021.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri,
  Devanur, Ganger, Gibbons, and Zaharia]{narayanan2019pipedream}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R
  Devanur, Gregory~R Ganger, Phillip~B Gibbons, and Matei Zaharia.
\newblock Pipedream: Generalized pipeline parallelism for dnn training.
\newblock In \emph{Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, pages 1--15, 2019.

\bibitem[Narayanan et~al.(2021)Narayanan, Phanishayee, Shi, Chen, and
  Zaharia]{narayanan2021memory}
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
\newblock Memory-efficient pipeline-parallel dnn training.
\newblock In \emph{International Conference on Machine Learning}, pages
  7937--7947. PMLR, 2021.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Rabe and Staats(2021)]{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need o(n2) memory.
\newblock \emph{arXiv preprint arXiv:2112.05682}, 2021.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem[Schulman et~al.(2022)Schulman, Zoph, Kim, Hilton, Menick, Weng, Uribe,
  Fedus, Metz, Pokorny, Lopes, Zhao, Vijayvergiya, Sigler, Perelman, Voss,
  Heaton, Parish, Cummings, Nayak, Balcom, Schnurr, Kaftan, Hallacy, Turley,
  Deutsch, and Goel]{schulman2022chatgpt}
J.~Schulman, B.~Zoph, C.~Kim, J.~Hilton, J.~Menick, J.~Weng, J.~F.~C. Uribe,
  L.~Fedus, L.~Metz, M.~Pokorny, R.~G. Lopes, S.~Zhao, A.~Vijayvergiya,
  E.~Sigler, A.~Perelman, C.~Voss, M.~Heaton, J.~Parish, D.~Cummings, R.~Nayak,
  V.~Balcom, D.~Schnurr, T.~Kaftan, C.~Hallacy, N.~Turley, N.~Deutsch, and
  V.~Goel.
\newblock Chatgpt: Optimizing language models for dialogue.
\newblock \emph{OpenAI Blog}, 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[Sergeev and Del~Balso(2018)]{sergeev2018horovod}
Alexander Sergeev and Mike Del~Balso.
\newblock Horovod: fast and easy distributed deep learning in tensorflow.
\newblock \emph{arXiv preprint arXiv:1802.05799}, 2018.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran,
  Yogatama, and Metzler]{tay2022scaling}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung~Won Chung, William Fedus, Jinfeng
  Rao, Sharan Narang, Vinh~Q Tran, Dani Yogatama, and Donald Metzler.
\newblock Scaling laws vs model architectures: How does inductive bias
  influence scaling?
\newblock \emph{arXiv preprint arXiv:2207.10551}, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Wei, Sabne, Davis, Ilbeyi, Hechtman, Chen,
  Murthy, Maggioni, Zhang, et~al.]{wang2022overlap}
Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake
  Hechtman, Dehao Chen, Karthik~Srinivasa Murthy, Marcello Maggioni, Qiao
  Zhang, et~al.
\newblock Overlap communication with dependent computation via decomposition in
  large deep learning models.
\newblock In \emph{Proceedings of the 28th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems, Volume
  1}, pages 93--106, 2022.

\bibitem[Yarats et~al.(2022)Yarats, Brandfonbrener, Liu, Laskin, Abbeel,
  Lazaric, and Pinto]{yarats2022don}
Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel,
  Alessandro Lazaric, and Lerrel Pinto.
\newblock Don't change the algorithm, change the data: Exploratory data for
  offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2201.13425}, 2022.

\end{thebibliography}
