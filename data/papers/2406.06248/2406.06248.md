---
abstract: |
  Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.
bibliography:
- refs.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
---





# Introduction

Regardless of their architectures, most neural networks consist of interleaved linear layers and simple non-linearities. In large foundation models such as GPT-3 , these linear layers consume the vast majority of the parameters and computation , and are primarily represented by dense matrices. Substituting these dense matrices with structured matrices with fast matrix-vector multiplies (MVMs) has the potential to significantly improve the computational efficiency of these models. Unfortunately, there often isn’t an obvious algebraic structure to exploit in the linear layers of such models, which process end-to-end learned token embeddings rather than objects with clear structures like images .

Structured matrices, however, are not limited to encoding domain-specific inductive biases. They can also offer advantages over dense matrices by enabling different allocations of the same computational budget. For example, a structured layer can be much wider than a dense layer given the same number of parameters and compute. The compute cost $C$ of an MVM is $\order{d^2}$ for a $d \times d$ dense matrix, but only $\order{d^{3/2}}$ for a block diagonal matrix with $\sqrt{d}$ blocks. Consequently, given the same compute $C,$ the width can be at most $\order{C^{1/2}}$ for a dense layer, but $\order{C^{2/3}}$ for such a block diagonal layer. We can replace a dense layer of width $1024$ with a $10\times$ wider block diagonal layer, as illustrated in . Both layers have the same number of parameters and compute costs, but a larger width enables the model to potentially store more information in its activations and use more non-linearities to model complex functions. In this light, structured matrices do not merely approximate dense matrices but enable different ways of scaling up the models with compute that make them potentially more expressive.

To study how structured layers compare against dense layers as a function of compute, we will compare their scaling laws: how compute translates to performance as the models scale up. Across domains such as language, image, and video modeling, the loss or error rate $E$ of a well-trained neural network has shown to be highly predictable as a function of the compute $C$ required by the model, often well-described by a power law $E \propto C^{-\alpha}$ when data is not a bottleneck . If structured layers can achieve better scaling laws, they will outperform dense layers at scale, delivering exponentially better performance per unit compute if they can improve the scaling exponent $\alpha$.

In this work, we systematically study whether structured matrices can have better scaling laws than dense matrices, without relying on domain-specific algebraic structures so that our findings can apply to training foundation models broadly.

  We show that structured layers often require drastically different learning rates and initialization scales compared to their dense counterparts, because their underlying trainable parameter matrices tend to be much smaller in size than the width of the layer (). Naively using dense layer learning rates, structured layers often significantly underperform dense layers, as shown in .

  Leveraging insights from $\mu$P on how to optimally scale the initialization and learning rates for dense layers as a function of width, we show how to automatically determine the appropriate initialization and learning rate scales for structured linear layers. This structure-aware technique enables us to effectively train and scale a wide range of structured layers without additional tuning.

  We measure scaling laws for neural networks employing structured matrices as they scale, showing that structured layers can have better scaling exponents than dense matrices on some tasks. These results suggest that the scaling exponents are not necessarily determined solely by the task as previously hypothesized .

  We identify matching parameter count to FLOPs[^1] as a principle shared by the best-performing structures. Conversely, commonly used structures such as the Kronecker product and Tensor-Train decomposition violate this principle and underperform dense matrices in our experiments. Adhering to this principle can serve as important guidance for future work on designing more efficient linear layers.

  We introduce Block Tensor-Train (BTT) as a new family of expressive structured matrices, containing the Monarch matrices as a special case. The BTT family has better scaling laws than dense matrices on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. On ImageNet-1k, BTT matches dense ViT-S/32 performance with 3.8 times less compute.

  We study divergences in training transformers with BTT layers, showing that weight normalization is required to avoid divergence due to unbounded growth of the activation.

We make our code available available [<u>here</u>](https://github.com/shikaiqiu/compute-better-spent). We use the `Linear Operator` abstractions in CoLA to prototype and compute efficient MVMs for structured matrices.

# Structured Alternatives to Dense Layers

We now introduce the types of structured matrices we consider in this work. We review their computational properties and modeling assumptions, summarized in . Without loss of generality, we consider $d \times d$ square matrices for notational simplicity.

**Low-rank.** A low-rank matrix can be parameterized as ${\boldsymbol{\mathbf{W}}} = {\boldsymbol{\mathbf{U}}} {\boldsymbol{\mathbf{V}}}$ where ${\boldsymbol{\mathbf{U}}} \in \mathbb{R}^{d \times r}$, ${\boldsymbol{\mathbf{V}}} \in \mathbb{R}^{r \times d}$ and $r \leq d$ is its rank. It has $2rd$ parameters and its MVM costs $2rd$ FLOPs. By first performing a dimension reduction on the input via ${\boldsymbol{\mathbf{V}}}$, a low-rank matrix assumes that only a subspace of the input space is relevant to the task and is natural for compression .

**Convolution.** Convolutions, or Toeplitz matrices, naturally model systems with translational symmetries such as images and time-series . A convolution with kernel size $p$ has $p$ parameters and requires $\order{p d}$ FLOPs. Each parameter is used $\order{d}$ times in a convolution to impose translational symmetry. Alternatively, the Fast Fourier transform allows the convolution to be computed in $\order{d \log d}$ FLOPs.

**Kronecker.** Kronecker product structure naturally arises in applications with structured data . A Kroncker product ${\boldsymbol{\mathbf{W}}} ={\boldsymbol{\mathbf{L}}} \otimes {\boldsymbol{\mathbf{R}}}$ with ${\boldsymbol{\mathbf{L}}} \in \mathbb{R}^{d_{1} \times d_{1}}$, ${\boldsymbol{\mathbf{R}}} \in \mathbb{R}^{d_{2} \times d_{2}}$, $d = d_{1} \cdot d_{2},$ specifies a matrix whose MVM ${\boldsymbol{\mathbf{y}}} = {\boldsymbol{\mathbf{W}}} {\boldsymbol{\mathbf{x}}}$ can be efficiently computed as $y_{\alpha\beta} = \sum_{\gamma} L_{\alpha\gamma} \sum_{\delta} R_{\beta\delta} x_{\gamma\delta},$ after reshaping the input ${\boldsymbol{\mathbf{x}}}$ in row-major order into a $d_1 \times d_2$ matrix and followed by flattening ${\boldsymbol{\mathbf{y}}}$ back to a vector. Assuming $d_1 = d_2 = \sqrt{d}$, ${\boldsymbol{\mathbf{W}}}$ has $2d$ parameters and requires $2 d^{3/2}$ FLOPs for an MVM. The Kronecker product uses each parameter $\sqrt{d}$ times, which can be made explicit by interpreting $\sum_{\delta} R_{\beta\delta} x_{\gamma\delta}$ (the same argument applies to the sum involving ${\boldsymbol{\mathbf{L}}})$ as multiplying the vector ${\boldsymbol{\mathbf{x}}}$ by a block-diagonal matrix $\bigoplus_{\gamma=1}^{\sqrt{d}} {\boldsymbol{\mathbf{R}}}_\gamma,$ where all the blocks ${\boldsymbol{\mathbf{R}}}_\gamma \in \mathbb{R}^{\sqrt{d} \times \sqrt{d}}$ are shared: ${\boldsymbol{\mathbf{R}}}_\gamma = {\boldsymbol{\mathbf{R}}}, \gamma = 1, \ldots, \sqrt{d}.$ This parameter-sharing naturally corresponds to the assumption that the input ${\boldsymbol{\mathbf{x}}}$ represents a set of objects of the same kind, such as nodes in a graph , patches of an image , points on a grid , or words in a sentence .

**Monarch.** Introduced in , a Monarch matrix is defined as the product ${\boldsymbol{\mathbf{P}}} {\boldsymbol{\mathbf{L}}} {\boldsymbol{\mathbf{P}}}^{\top} {\boldsymbol{\mathbf{R}}}$ where ${\boldsymbol{\mathbf{P}}}$ is a row-major to column-major permutation and ${\boldsymbol{\mathbf{L}}}, {\boldsymbol{\mathbf{R}}}$ are two block-diagonal matrices: $\bigoplus_{\beta=1}^{\sqrt{d}} {\boldsymbol{\mathbf{L}}}_\beta, \bigoplus_{\gamma=1}^{\sqrt{d}} {\boldsymbol{\mathbf{R}}}_\gamma$. Monarch requires $2d^{3/2}$ FLOPs for an MVM and has $2d^{3/2}$ parameters. The efficient multiply for Monarch can be written as $y_{\alpha\beta} = \sum_{\gamma} L_{\alpha\textcolor{purple}{\beta}\gamma} \sum_{\delta} R_{\beta\textcolor{purple}{\gamma}\delta} x_{\gamma\delta},$ where $R_{\beta\textcolor{purple}{\gamma}\delta} = ({\boldsymbol{\mathbf{R}}}_{\textcolor{purple}{\gamma}})_{\beta\delta}$ and $L_{\alpha\textcolor{purple}{\beta}\gamma} = ({\boldsymbol{\mathbf{L}}}_{\textcolor{purple}{\beta}})_{\alpha\gamma}$ and we have colored the block dimensions $\textcolor{purple}{\beta}, \textcolor{purple}{\gamma}$. Monarch can be viewed as a relaxation of the Kronecker product where parameters that were shared across the block dimensions are now made independent. Monarch matrices do not make strong assumptions about the structure of the input. In practice, the number of blocks $b$ in ${\boldsymbol{\mathbf{L}}}$ and ${\boldsymbol{\mathbf{R}}}$ are often chosen to be much less than $\sqrt{d}$ to reduce sparsity . In this case, Monarch has $2d^2/b$ parameters and requires $2d^2/b$ FLOPs for an MVM.

**Tensor-Train.** The Tensor-Train (TT) decomposition specifies a set of $c$ cores ${\boldsymbol{\mathbf{G}}}^{(i)} \in \mathbb{R}^{r_{i} \times  m_{i} \times  n_{i} \times  r_{i-1}}$ for $i=1, \ldots, c$ where $d = \prod_{i}^{} m_{i} = \prod_{i}^{} n_{i},$ $r_{i} \in \mathbb{N}$ and $r_{0} = r_{c} = 1$. For ease of notation, we will focus on $c=2$ with $m_1=m_2=n_1=n_2=\sqrt{d}, r_{1} = r$, ${\boldsymbol{\mathbf{G}}}^{(1)} = {\boldsymbol{\mathbf{R}}} \in \mathbb{R}^{r \times \sqrt{d} \times \sqrt{d}}, {\boldsymbol{\mathbf{G}}}^{(2)} = {\boldsymbol{\mathbf{L}}} \in \mathbb{R}^{\sqrt{d} \times \sqrt{d} \times r}$, though we present the general case in Appendix . With the input and output as reshaped as $\sqrt{d} \times \sqrt{d}$ matrices, a TT matrix is equivalent to a sum over $r$ Kronecker products indexed by $\sigma=1,\ldots, r$: $$\label{eq:full}
    \begin{split}
      y_{\alpha\beta} = \sum_{\gamma\sigma} L_{\alpha\gamma\sigma} \sum_{\delta} R_{\sigma\beta\delta} x_{\gamma\delta}.
    \end{split}$$

By increasing $r,$ referred to as the TT-rank, TT becomes more expressive relative to the Kronecker product. When $r=d,$ it can represent any $d \times d$ dense matrix. TT has $2rd$ parameters and costs $2rd^{3/2}$ FLOPs for an MVM. Like Kronecker, TT shares parameters along the block dimensions $\beta,\gamma$ and therefore uses each parameter $\sqrt{d}$ times in an MVM. The TT structure is natural for modeling systems that decompose into subsystems with local pairwise interactions, such as quantum spin chains and hidden Markov models .

**Block Tensor-Train.** We propose a novel family of structured matrices called Block Tensor-Train (BTT) matrices, by removing the parameter-sharing along the block dimensions $\textcolor{purple}{\beta},\textcolor{purple}{\gamma}$ in the TT structure. In the two core ($c=2$) case, a BTT matrix of BTT-rank $r$ is defined by two parameter tensors ${\boldsymbol{\mathbf{R}}} \in \mathbb{R}^{r \times \sqrt{d} \times \sqrt{d} \times \sqrt{d}}$ and ${\boldsymbol{\mathbf{L}}} \in \mathbb{R}^{\sqrt{d} \times \sqrt{d} \times \sqrt{d} \times r}.$ Its MVM is given by $$\label{eq:btt}
    \begin{split}
      y_{\alpha\beta} = \sum_{\gamma\sigma} L_{\alpha\textcolor{purple}{\beta}\gamma\sigma} \sum_{\delta} R_{\sigma\beta\textcolor{purple}{\gamma}\delta} x_{\gamma\delta}.
    \end{split}$$

In , we study the expressiveness of BTT, present a simple algorithm for projection onto the BTT family, and show BTT with rank $r=\sqrt{d}$ can represent any dense matrix (in constrast to $r=d$ for TT) when $c=2$ and analogous results for $c > 2$. Therefore, by varying the BTT rank, we effectively interpolate between Monarch matrices and dense matrices.

We use the `Linear Operator` abstractions available in `CoLA` to compute MVMs for these structures efficiently. In , we show the structures we consider have asymptotically the same MVM runtimes as dense matrices as a function of FLOPs because they can be implemented through the same dense matrix multiply primitives, though they introduce non-trivial overhead for small matrix sizes with our current implementation.

# Optimizing Structured Matrices

To study the performance and scaling laws of unconventional layers, we must determine how to optimize them effectively by choosing appropriate initialization and learning rates as the models scale. As illustrates, the optimal settings for structured matrices can differ significantly from dense matrices. We develop a technique based on the Maximal Update Parameterization ($\mu$P) to automatically determine the optimal initialization and learning rate scaling for a generic structured layer given its structure and size, enabling us to train and scale various structured layers with good hyperparameters and minimal tuning. We focus on the Adam optimizer but discuss extensions to other optimizers in .

## Maximal Update Parameterization

The Maximal Update Parameterization ($\mu$P) specifies how to scale the initialization and learning rate of neural networks as their widths increase while maximizing feature learning in every layer . provides an elementary derivation based on the spectral norm, which we now review.

In $\mu$P, initialization and learning rates are chosen so that entries of each layer’s output have size $\Theta(1)$ and are updated at a rate of $\Theta(1)$ per step throughout training. Here, big-$\Theta$ notation denotes scaling in the layer’s width, omitting dependence on other quantities. If these conditions do not hold, the layer’s output or update will either diverge or vanish for sufficiently large widths. For a dense matrix ${\boldsymbol{\mathbf{W}}} \in \mathbb{R}^{{d_\mathrm{out}}\times {d_\mathrm{in}}},$ input ${\boldsymbol{\mathbf{x}}} \in \mathbb{R}^{d_\mathrm{in}},$ output ${\boldsymbol{\mathbf{h}}} = {\boldsymbol{\mathbf{W}}} {\boldsymbol{\mathbf{x}}} \in \mathbb{R}^{d_\mathrm{out}},$ and output update $\Delta {\boldsymbol{\mathbf{h}}} = \Delta{\boldsymbol{\mathbf{Wx}}}$ due to a weight update $\Delta{\boldsymbol{\mathbf{W}}}$, $\mu$P requires $\norm{{\boldsymbol{\mathbf{h}}}}_2 = \Theta(\sqrt{{d_\mathrm{out}}})$ and $\norm{\Delta{\boldsymbol{\mathbf{h}}}}_2 = \Theta(\sqrt{{d_\mathrm{out}}}).$ During training, gradient descent aligns ${\boldsymbol{\mathbf{x}}}$ with the top singular subspace of ${\boldsymbol{\mathbf{W}}}$ and $\Delta{\boldsymbol{\mathbf{W}}}$ , so $\norm{{\boldsymbol{\mathbf{h}}}}_2 = \Theta(\norm{{\boldsymbol{\mathbf{W}}}}_2 \norm{{\boldsymbol{\mathbf{x}}}}_2)$ and $\norm{\Delta{\boldsymbol{\mathbf{h}}}}_2 = \Theta(\norm{\Delta{\boldsymbol{\mathbf{W}}}}_2 \norm{{\boldsymbol{\mathbf{x}}}}_2).$ Assuming ${\boldsymbol{\mathbf{x}}}$ is entry-wise $\Theta(1)$, we want $\norm{{\boldsymbol{\mathbf{W}}}}_2 = \Theta(\sqrt{{d_\mathrm{out}}/{d_\mathrm{in}}})$ and $\norm{\Delta{\boldsymbol{\mathbf{W}}}}_2 = \Theta(\sqrt{{d_\mathrm{out}}/{d_\mathrm{in}}}).$ To ensure the desired spectral norm at initialization, entries of ${\boldsymbol{\mathbf{W}}}$ are drawn from $\mathcal{N}(0, \sigma^2)$ with $\sigma = \Theta(\sqrt{\min({d_\mathrm{in}}, {d_\mathrm{out}}) / d^2_\mathrm{in}})$. For the updates, the gradient $\nabla_{{\boldsymbol{\mathbf{W}}}} \mathcal{L} = \frac{1}{B}\sum_{i=1}^{B} \nabla_{{\boldsymbol{\mathbf{h}}}_i}\mathcal{L} \cdot {\boldsymbol{\mathbf{x}}}_i^\top$ has $\Theta(1)$ stable rank, assuming the batch size $B$ is constant, so its spectral norm scales the same way as its Frobenius norm. Since Adam normalizes the gradient to be entry-wise $\Theta(1),$ the normalized gradient has Frobenius norm $\Theta(\sqrt{{d_\mathrm{in}}{d_\mathrm{out}}})$. Therefore, an Adam learning rate of $\Theta(1 / {d_\mathrm{in}})$ ensures the desired spectral norm.

Once the optimal learning rate $\eta^*$ is found for a particular width ${d_\mathrm{in}},$ it can be transferred to any other width $d'_\mathrm{in}$ by setting the new learning rate as $\eta^* \cdot \frac{{d_\mathrm{in}}}{d'_\mathrm{in}},$ assuming ${d_\mathrm{in}}$ and $d'_\mathrm{in}$ are sufficiently large . For architectures, $\mu$P deviates from conventional initializations mainly in the last layer, where $\sigma = \Theta(1/{d_\mathrm{in}})$ according to $\mu$P but $\sigma = \Theta(\sqrt{1/{d_\mathrm{in}}})$ according to more conventional strategies .

<figure id="fig:ffn">
<p><span class="image placeholder" data-original-image-src="figs/naive_ffn.pdf" data-original-image-title="" width="0.8\linewidth">image</span><br />
</p>
<figcaption> <strong>Structure-aware learning rates improve performance even after tuning the learning with grid search.</strong> Test error of ViT (<span class="math inline">\(d=1024\)</span>) on CIFAR-10 where the feed-forward layers are replaced using BTT. </figcaption>
</figure>

## Identifying $\mu$P for Structured Matrices

The above scaling of learning rate and initialization assume dense matrices and don’t immediately carry over to arbitrarily structured matrices. For example, for a Kronecker product ${\boldsymbol{\mathbf{W}}} = {\boldsymbol{\mathbf{L}}} \otimes {\boldsymbol{\mathbf{R}}}$ where ${\boldsymbol{\mathbf{W}}} \in \mathbb{R}^{d\times d}$ and ${\boldsymbol{\mathbf{L}}}, {\boldsymbol{\mathbf{R}}} \in \mathbb{R}^{\sqrt{d} \times \sqrt{d}},$ one intuitively expects that the optimal learning rates for parameters ${\boldsymbol{\mathbf{L}}}$ and ${\boldsymbol{\mathbf{R}}}$ in this layer to scale as $\Theta(1/\sqrt{d}),$ the size of the actual learnable parameter matrices, rather than naively as $\Theta(1/d)$ based only on the width of the layer.

Since many structured matrices are ultimately compositions of smaller dense matrices and fixed, norm-preserving linear transformations (e.g. reshapes), as exemplified in , we can decompose the problem by applying the same spectral considerations to each dense component separately, effectively treating each structured layer as a deep linear network. Suppose the MVM ${\boldsymbol{\mathbf{W}}}{\boldsymbol{\mathbf{x}}}$ can be computed as ${\boldsymbol{\mathbf{W}}} {\boldsymbol{\mathbf{x}}} = {\boldsymbol{\mathbf{G}}}_k {\boldsymbol{\mathbf{P}}}_{k} \ldots {\boldsymbol{\mathbf{G}}}_1 {\boldsymbol{\mathbf{P}}}_1 {\boldsymbol{\mathbf{x}}}$ where each ${\boldsymbol{\mathbf{P}}}_i$ is a fixed, norm-preserving linear transformation, such as the product of a permutation and a reshape, and multiplication by ${\boldsymbol{\mathbf{G}}}_i$ denotes a batched MVM, i.e., $({\boldsymbol{\mathbf{G}}}_i {\boldsymbol{\mathbf{x}}})_{b \mu} = \sum_{\nu} (G_i)_{b\mu\nu} x_{b\nu}$ for some dense tensor ${\boldsymbol{\mathbf{G}}}_{i} \in \mathbb{R}^{B_i \times d_{\mathrm{out}}^{i} \times d_{\mathrm{in}}^{i}},$ where $b$ is an abstract batch-like dimension. Then to ensure that the activations have size $\Theta(1)$ and all parameters are updated as much as possible to maximize feature learning , we require the initialization and updates to each slice $({\boldsymbol{\mathbf{G}}}_i)_b \in \mathbb{R}^{d_{\mathrm{out}}^{i} \times d_{\mathrm{in}}^{i}}$ of ${\boldsymbol{\mathbf{G}}}_i$ to have $\Theta\Bigl(\sqrt{d_{\mathrm{out}}^{i} / d_{\mathrm{in}}^{i}}\Bigr)$ spectral norm. Thus we initialize each ${\boldsymbol{\mathbf{G}}}_i$ with standard deviation $\Theta\Bigl( \sqrt{\min(d_{\mathrm{in}}^{i}, d_{\mathrm{out}}^{i}) / (d_{\mathrm{in}}^{i})^{2}} \Bigr)$ and set its Adam learning rate as $\Theta\left(1 / d_{\mathrm{in}}^{i}\right)$. When used in the last linear layer in a residual block, we zero-initialize the last component ${\boldsymbol{\mathbf{G}}}_{k}$, which is compatible with $\mu$P by setting the hidden constant in $\Theta(\cdot)$ to 0 .

**Transferring learning rate between structures.** Once the optimal learning rate $\eta^*$ is known for a ${d_\mathrm{out}}\times {d_\mathrm{in}}$ dense layer, we can infer the optimal learning rate $\eta_i^*$ of each component ${\boldsymbol{\mathbf{G}}}_i$ of the corresponding structured layer as $\eta_i^* = \kappa_i \cdot \eta^*,$ where $\kappa_i = \frac{{d_\mathrm{in}}}{d_{\mathrm{in}}^{i}} \cdot \delta_i$ for some constant $\delta_i.$ Here $\frac{{d_\mathrm{in}}}{d_{\mathrm{in}}^{i}}$ accounts for the $\Theta(\mathrm{width}^{-1})$ scaling of optimal learning rate prescribed by $\mu$P, with width identified with ${d_\mathrm{in}}$ and $d_{\mathrm{in}}^{i}$ respectively for the dense matrix and ${\boldsymbol{\mathbf{G}}}_i$, and $\delta_i$ accounts for potential differences in the constants omitted by $\Theta(\cdot)$ for the dense matrix and ${\boldsymbol{\mathbf{G}}}_i$. While the precise value of $\delta_i$ is not theoretically determined by $\mu$P, we adopt the heuristic $\delta_i = 1/k$ where $k$ is the number of learnable dense components so that the overall updates to the output of this layer is roughly preserved, since $\Delta {\boldsymbol{\mathbf{h}}}$ has $k$ leading order terms: $$\begin{aligned}
    \Delta {\boldsymbol{\mathbf{h}}} &= \sum_{i=1}^{k} {\boldsymbol{\mathbf{G}}}_k {\boldsymbol{\mathbf{P}}}_{k} \ldots
\Delta {\boldsymbol{\mathbf{G}}}_i {\boldsymbol{\mathbf{P}}}_{i} \ldots {\boldsymbol{\mathbf{G}}}_1 {\boldsymbol{\mathbf{P}}}_1 {\boldsymbol{\mathbf{x}}} \\
 & + \order{\Delta {\boldsymbol{\mathbf{G}}}^2}.
\end{aligned}$$

$\delta_i$ can be further tuned empirically around $1/k$ to maximize performance, though we will show the $1/k$ heuristic is sufficently good in practice.

As ${\boldsymbol{\mathbf{G}}}_i$’s are often much smaller in size than the matrix ${\boldsymbol{\mathbf{W}}}$ it parameterizes, the required learning rate multiplier $\kappa_i$ is often a large number. For example, suppose we initially represent ${\boldsymbol{\mathbf{W}}} \in \mathbb{R}^{d \times d}$ as a dense matrix and find $\eta$ is an effective learning rate during training. If we now instead represent ${\boldsymbol{\mathbf{W}}} = {\boldsymbol{\mathbf{L}}} \otimes {\boldsymbol{\mathbf{R}}}$ where ${\boldsymbol{\mathbf{L}}}, {\boldsymbol{\mathbf{R}}} \in \mathbb{R}^{\sqrt{d} \times \sqrt{d}},$ we would then need to scale up the learning rate for both ${\boldsymbol{\mathbf{L}}}, {\boldsymbol{\mathbf{R}}}$ by a factor of $\Theta(\sqrt{d}),$ which grows arbitrarily large for large $d.$ We show the Adam learning rate multipliers required for various structures in , adopting our heuristic of $\delta_i = 1/k$.

## Empirical Validation

We now empirically validate the effectiveness of our structure-aware learning rate scaling. We compare it to the naive, structure-agnostic approach that parameterizes the learning rate $\eta_i$ for each parameter tensor ${\boldsymbol{\mathbf{G}}}_i$ in a ${d_\mathrm{out}}\times {d_\mathrm{in}}$ structured layer as $\eta_i = \eta_0 \frac{d_0}{{d_\mathrm{in}}} \propto 1/{d_\mathrm{in}},$ where the base learning rate $\eta_0$ and the base width $d_0$ are constants, corresponding to scaling the learning rate optimally according to $\mu$P if the layer were dense. The structure-aware approach additionally applies the structure-dependent learning rate multipliers $\kappa_i$ in so that $\eta_i = \eta_0 \frac{d_0}{{d_\mathrm{in}}} \kappa_i.$ We use $d_0 = 64$ throughout this section.

**Stable feature learning.** We train an MLP with 2 hidden layers without bias on CIFAR-10 with width $d \in \{16, 64, 256, 1024, 4096\}$ and a base learning rate $\eta_0 = 3 \cdot 10^{-3}$. For a given width, we track the root mean square (RMS) of $\Delta {\boldsymbol{\mathbf{h}}}_{t} = {\boldsymbol{\mathbf{h}}}_{t+1} - {\boldsymbol{\mathbf{h}}}_{t}$ at every step $t$, where ${\boldsymbol{\mathbf{h}}}_{t} \in \mathbb{R}^{d}$ is the activation of the last layer before the classification head. We then plot the the average RMS over $500$ steps for different widths and structures. As seen in Figure , structure-aware learning rate scaling produces consistent feature learning for all structures used with no tuning. In contrast, the naive approach causes much smaller or vanishing updates to the features. The effect is most pronounced for BTT and Kronecker, for which $\kappa_i$ grows without bound for both ${\boldsymbol{\mathbf{L}}}$ and ${\boldsymbol{\mathbf{R}}}$ as the width increases.

**Stable optimal learning rate.** We test if the structure-aware learning scaling preserves the learning rate landscape for all structures so that once an optimal learning rate is found for the dense model with some width, it can be directly transferred to all other structures and widths. We train a 2-layer MLP on CIFAR-10 with augmentation (see for details) for 100 epochs, using a base learning rate of $3 \cdot 10^{-3},$ the optimal value for a dense model at with $d_0=64$. In the first row of Figure , we show the train error as a function of the base learning rate $\eta_0$ when scaled to other widths and structures using the naive approach, which is optimal for the dense model but clearly not for the other structures. By contrast, in the second row, the structure-aware approach approximately stabilizes the learning rate landscape across structures and widths, significantly reducing the cost for exploring different structures. Slight deviation at small widths is expected because the optimality of $\mu$P relies on convergence to the infinite-width limit .

**Improved performance even after tuning.** Finally, we show in the performance of structured models quickly saturate as they are scaled up without structure-aware learning rates. Monarch is an exception, for which the multipliers in are closer to 1 because we use $b=4$. In this case, the learning rate multiplier required for Monarch is only $2$ and independent of scale, which may explain why still achieves good performance with Monarch by reusing the dense learning rates.

Furthermore, the structure-aware approach not only reduces the tuning cost for structured layers, but is necessary for optimal performance if the structures differ across layers, even when we perform a grid search over the base learning rate $\eta_0$. Consider a transformer of hidden dimension $d$ where only the feed-forward layers (FFN) are replaced with BTT and the attention projection matrices are dense. Since the optimal learning rate is $\Theta(1/\sqrt{d})$ for the FFN layer but $\Theta(1/d)$ for the attention projection, the naive approach would have to choose between using a learning rate too large for the attention projection or a learning rate too small for the FFN, whereas the structure-aware approach does not have this problem. In Figure , we show that for a ViT with BTT-structured FFNs, the structure-aware approach indeed achieves much better performance even if we tune the base learning rate.

# Scaling Laws of Structured Matrices

Having developed an effective procedure to automatically scale the initialization and learning rates for structured layers, we now aim to understand how various structures compare in performance.

When data is not a bottleneck, a neural network’s test error or loss on a task follows a power law $E \propto P^{-\alpha_P}$ if trained to (near) convergence, where $P$ is the number of parameters and $\alpha_P$ is a constant . For dense models, compute per forward pass $C \propto P$, so $E \propto C^{-\alpha_C}$ for some constant $\alpha_C$. We explore how different structures change how $E$ scales with $C$, as $P$ does not consistently relate to training or inference cost when varying the structure ().

We train all models for a fixed number of iterations $T$, so the total training compute $C_\mathrm{tot} \propto C$. Thus, the scaling laws in $C$ can differ from compute-optimal scaling laws, which require carefully optimizing the allocation of $C_\mathrm{tot} \propto C T$ between $C$ and $T$ , which we leave to future work.

To compare multiple structures across compute scales, we conduct experiments primarily using MLPs and ViTs on CIFAR-10 and CIFAR-100. In , we present larger-scale experiments on ImageNet and language modeling. With limited training data in CIFAR-10 and CIFAR-100, we apply heavy augmentation to alleviate over-fitting. The augmented training set is sufficiently large, resulting in relatively clean power-law scaling of training error with $C$. We extract these power law parameters, reflecting the expressivity afforded by each structure as a function of $C$, and visualize the scaling of test error with $C$, which is not well-described by a power law due to train-test discrepancy.

**Experimental setup.** We use CIFAR-10 and CIFAR-100 datasets, applying random crop, random flip, MixUp ($\alpha_\mathrm{mixup}=0.8$) augmentations, and label smoothing of $0.3$, following . We use the same MLP architecture as in , but apply a fixed random permutation to the pixels before feeding them to the MLP so our results will more likely generalize to non-image data. We also use ViTs with $8\times8$ patches. We train MLPs for 500 epochs with batch size of 1024, and ViTs for 200 epochs with batch size of 256. To scale up the model, we increase its width while holding the depth constant. For structured models, we replace all except the classification layer with structured layers, though we keep the input layer dense for low rank to avoid an information bottleneck at the first layer. For Monarch, we set the number of blocks $b=4$ following unless stated otherwise. We use BTT with two cores and various BTT-ranks. Further experiment details are in .

**Scaling exponents are structure-dependent.** In and , we find the training error $E$ has an approximate power law relation to the compute $C:$ $E \propto C^{-\alpha_C}$, for both MLPs and ViTs, where the exponent $\alpha_C$ varies significantly among structures. We show the best-fit exponent $\alpha_C$ and its standard error for each structure and plot the fitted power law trends. Monarch ($b=4$) achieves equal or lower train and test error than dense for the same amount of compute, though it does not improve the scaling exponent of training error. BTT has the largest scaling exponent and consistently outperforms all other structures. We use BTT with two cores and rank 1, equivalent to a Monarch with $\sqrt{d}$ blocks, but BTTs with higher ranks also improve scaling as we will soon show.

**Parameters equal FLOPs leads to better scaling laws.** Figure  and Figure  reveal a qualitative difference between the scaling behavior of structures that perform parameter-sharing, i.e. Kronecker and TT, and those that do not, having parameters equal to FLOPs. Structures that do not share parameters are more flexible per unit of compute, and consistently achieve better scaling laws.

Recent works proposing to explain scaling laws from the data manifold dimension can naturally explain worse scaling exponents due to parameter-sharing. This theory predicts the scaling exponent $\alpha_P$ with respect to parameters is determined only by the intrinsic dimension of the data manifold, explaining why architectural details often only have minor impacts on the scaling laws . If changing the matrix structure leaves $\alpha_P$ invariant, then the scaling exponent $\alpha_C$ will depend on the structure in a simple way: if $C \propto P^\beta,$ then $\alpha_C = \alpha_P/\beta,$ that is, the more parameters sharing, the smaller the exponent $\alpha_C$. For example, $\beta = 1$ for dense, low-rank, and BTT, but $\beta = 3/2$ for Kronecker and TT. However, this exact factor underestimates the observed differences in the exponents between Kronecker, TT, and dense, and does not explain why BTT has a larger exponent. A more accurate model is needed to explain the observed structure-dependence of the scaling exponents.

<figure id="fig:compute_per_dim">

<figcaption> <strong>Less compute per dimension is more compute-efficient on CIFAR-10.</strong> (a) BTT with a lower rank achieves lower train error per FLOP. (b) Monarch with more blocks achieves lower train error per FLOP. A lighter color indicates less compute per dimension. </figcaption>
</figure>

<figure id="fig:memory">

<figcaption> <strong>More compute per dimension is more memory-efficient on CIFAR-10.</strong> (a) BTT with a higher rank achieves lower train error per unit width. (b) Monarch with fewer blocks achieves lower train error per unit width. A smaller width means less memory is required to store the activations. A lighter color indicates less compute per dimension. </figcaption>
</figure>

**Optimizing compute spent per dimension.** Both BTT and Monarch have hyperparameters (BTT-rank $r$ and number of blocks $b$) that control how well they can approximate a dense matrix of the same dimension. We can scale up the compute $C$ in a structured layer by increasing either its dimension $d$ or its compute per dimension $\xi:=C/d$ (compute cost for an MVM normalized by $d$), which is controlled by these hyperparameters. From , the compute per dimension is $d$ for dense, $2r\sqrt{d}$ for BTT (with 2 cores), and $2d/b$ for Monarch. To maximize performance as a function of $C,$ we need to optimally allocate it between the dimension $d$ of the layer and the compute spent per dimension $\xi$. In , we show that while higher rank BTTs scale better than dense matrices on CIFAR-10, lower rank BTTs are more compute-efficient. Similarly, in , Monarch matrices with more blocks and higher sparsity are more compute-efficient. These results illustrate that the optimal compute per dimension on this task is much smaller than $d$, and structured matrices beat dense matrices by making a favorable trade-off between dimension and compute per dimension. In , we show that for BTT with $c \geq 3$ cores and different BTT-ranks, smaller ranks lead to better compute-efficiency, and using $c$ greater than 2 does not significantly improve compute efficiency on CIFAR-10, despite compute per dimension scaling as $\order{d^{1/c}}$.

The optimal way to scale $\xi$ with $d$ is likely non-trivial and task-dependent. The extremes are $\xi = d$ for a dense matrix and $\xi=0$ for the identity. The latter is clearly suboptimal, and neither is the former in light of our findings.

**Compute-memory trade-off.** While lowering the compute per dimension can increase compute efficiency, it sacrifices memory efficiency if the memory cost is dominated by storing activations, such as when training with large batch sizes. In this case, the memory for storing activations scales at least as the layer width $d$. Since we can increase the expressivity of BTT and Monarch by increasing the rank or decreasing the number of blocks without increasing $d$, these hyperparameters enable us to trade off compute-efficiency with memory-efficiency, as demonstrated in . While dense matrices are the least compute-efficient, they are the most efficient in terms of activation memory by packing the most parameters and compute into each dimension. The most compute-efficient yet memory-feasible structure will vary depending on the specific memory budget.

# Training Structured Transformers

We now apply structured layers to train larger transformer models for ImageNet classification and language modeling. We also introduce a technique required to prevent training divergence in these experiments.

## Stabilizing Training with Weight Normalization

When training on ImageNet and OpenWebText with BTT layers, we found the activations grow without bound slowly over time as illustrated in for GPT-2, which does not happen in the dense model. We found we can eliminate this behavior without sacrificing expressivity through the following reparameterization: $$\begin{aligned}
    {\boldsymbol{\mathbf{\tilde{M}}}} = \gamma_{\boldsymbol{\mathbf{M}}} \min\qty(1, \frac{\sigma_{\boldsymbol{\mathbf{M}}}}{\mathrm{RMS}({\boldsymbol{\mathbf{M}}})}) {\boldsymbol{\mathbf{M}}},  
\end{aligned}$$ where ${\boldsymbol{\mathbf{M}}} \in \{{\boldsymbol{\mathbf{L}}}, {\boldsymbol{\mathbf{R}}}\}$. It normalizes the BTT cores ${\boldsymbol{\mathbf{L}}}$ and ${\boldsymbol{\mathbf{R}}}$ to have RMS entry sizes no larger than their initialization scales $\sigma_{\boldsymbol{\mathbf{L}}}$ and $\sigma_{\boldsymbol{\mathbf{R}}}$, and scaling them by learnable scalars $\gamma_{\boldsymbol{\mathbf{L}}}$ and $\gamma_{\boldsymbol{\mathbf{R}}}$ to allow the singular values to grow in size if needed, similar to what is proposed in . In , we show a 12-layer GPT-2 model with $d = 128$ using BTT layers trained on OpenWebText with or without normalization. Weight normalization eliminates the unbounded growth of the activations before the last layer normalization, which will eventually lead to NaN. Weight normalization also improves validation loss, which is in contrast to alternatives such as lowering the learning rate and increasing weight decay which we found to only reduce the rate of growth at the cost of worse performance.

<figure id="fig:btt_norm">
<p><span class="image placeholder" data-original-image-src="figs/norm_legend.pdf" data-original-image-title="" width="0.6\linewidth">image</span><br />
 <br />
</p>
<figcaption> <strong>Weight normalization is necessary to stabilize GPT-2 training with BTT.</strong> (a) RMS entry size of the final layer activations stabilizes around 1 with normalization but grows without bound otherwise. (b) Normalization improves validation loss. </figcaption>
</figure>

<figure id="fig:imagenet">
<span class="image placeholder" data-original-image-src="figs/imagenet_val.pdf" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption> <strong>ViTs trained on ImageNet with structured layers are more compute-efficient</strong>. We use ViTs with patch size 32 trained for 300 epochs. BTT reaches the same performance of a dense ViT-S/32 with up to <span class="math inline">\(3.8\times\)</span> fewer FLOPs. </figcaption>
</figure>

## ViT on ImageNet

We train ViTs with patch size $32$ on ImageNet for 300 epochs. We provide full experimental details in . In , we find both BTT with rank $r \in \{1, 2\}$ and Monarch with $b \in \{4, 16\}$ blocks outperform dense for the same amount of compute for training ViTs on ImageNet. BTT reaches the same performance of a dense ViT-S/32 (the larger dense model shown) with up to $3.8\times$ fewer FLOPs. We find Monarch with 16 blocks is more compute-efficient than with 4 blocks, the original version used in and in the Monarch Mixer architecture , consistent with our finding on CIFAR-10 that less compute per dimension is more compute-efficient.

<figure id="fig:gpt2">

<figcaption> <strong>GPT-2 with all BTT layers is more compute-efficient</strong>. (a) When including language modeling head compute, BTT is more efficient than dense. (b) When excluding language modeling head compute, BTT and dense perform similarly. </figcaption>
</figure>

## GPT-2 on OpenWebText

We train GPT-2 models on OpenWebText for $600,000$ steps with a batch size of $245,760$ tokens at a sequence length of $512.$ We provide full experimental details in . We replace all linear layers, including the language modeling head, which accounts for a significant fraction of the compute, with BTT layers. In , we show the resulting GPT-2 model with BTT layers outperforms the original dense GPT-2 as a function of compute. However, in , we find they perform similarly when controlling for non-embedding compute, which excludes the compute spent in the language modeling head . While the improvement is significant, suggests that the improvement primarily comes from reducing the compute spent in the language modeling head and may therefore diminish at larger scales where the fraction of compute spent in the language modeling head becomes negligible.

# Discussion

The exponential growth in the computational cost of training foundation models in recent years has made the development of more compute-efficient architectures and training procedures a critical area of research. While structured matrices have traditionally been used in machine learning to approximate dense matrices or encode constraints such as equivariance, our work shows their promise in serving as general-purpose linear layers, a universal compute bottleneck in current foundation models, while offering improved compute efficiency relative to dense matrices.

Our work uncovers several key insights in designing more compute-efficient linear layers with structured matrices:

- *Careful optimization is crucial:* structure-aware learning rates based on $\mu$P are essential to realize the performance benefits of structured matrices.

- *Better scaling laws than dense are possible:* structured matrices can sometimes exponentially outperform dense matrices as we increase compute.

- *Relaxing parameter sharing produces compute-efficient and general-purpose structures:* By learning more parameters with the same compute, Monarch and BTT can provide better performance as general linear layers than the parameter-sharing Kronecker product and Tensor-Train structures.

- *Compute per dimension is an impactful yet neglected hyperparameter:* dense matrices consume the most compute per dimension, but they can underperform structured matrices that trade less compute per dimension for more dimensions, resulting in wider models.

Extending our evaluation to larger-scale models and datasets, studying the compute-optimal scaling laws, and developing a theoretical understanding of when and why structured matrices can improve scaling laws based on data and model characteristics are exciting directions for future work.

# Acknowledgements

We thank Sanae Lotfi, Alan Amin, and Bayan Bruss for helpful discussions, and Christopher Ferri for HPC assistance. This work is supported by NSF CAREER IIS-2145492, NSF CDS&E-MSS 2134216, NSF HDR-2118310, BigHat Biosciences, Capital One, and an Amazon Research Award.

# Impact Statement

This work aims to improve the performance of MLPs and transformers per unit of compute. Making neural networks more efficient has the potential to reduce energy consumption of training and inference, and more efficient neural networks can also make deep learning accessible where compute resources are scarce. However, we caution that the matrix structures we use should be tested in new domains, at new architectural scales, and within new architectures, to ensure that our results extrapolate for a practitioner’s specific individual needs.

# Related Work

#### Compute-Efficient Alternatives to Dense Layers.

Finding more compute-efficient alternatives to dense layers during training is an under-explored research topic. Convolutional networks and other equivariant models using structured matrices only offer an advantage in specific domains where the assumed symmetries exist . Approaches such as pruning and quantization mainly target reducing the inference cost after a model has been trained. Similarly, introduce a differentiable approach to learn a sparse structure that contain sums of low-rank blocks, but the learned structure can only be made sparse after training. Efficient fine-tuning methods leveraging structured matrices, such as LoRA , only apply in the fine-tuning stage. Recent works have used low-rank structures to reduce the memory usage of training and accelerate the backward pass, but they still use dense matrices in the forward pass . While Tensor-Train decomposition can improve parameter efficiency of neural networks , they have not been shown to improve their compute efficiency.

The recently proposed Monarch matrices are a notable exception, which enable faster training of certain vision and language transformers by training with Monarch matrices for all or most of the training steps followed by only a small amount of dense training.

#### Initialization and Learning Rate for Structured Layers.

The most popular initialization strategies such as Xavier , Kaiming , and Lecun initializations set the initialization scales of the dense matrices so that the forward or backward pass is variance preserving at initialization. extended this analysis to tonsorial convolutional networks where the kernels are structured. In addition to considering only a subset of possible structures (dense and tensorial convolution), these strategies are not optimal because they only consider the initialization and not the training dynamics, as shown by $\mu$P. Specifically, $\mu$P uses an asymptotically smaller initialization variance compared to these methods when a layer’s input dimension is asymptotically larger than its output dimension, such as the last layer.

To the best of our knowledge, there is no prior work that investigates how to scale the learning rate for general structured linear layers. Prior works using Tensor-Train Decomposition , low-rank matrices , and Monarch matrices to replace dense layers simply used global learning rates for all parameters and do not specify how they should be scaled as a function of width. The concurrent work LoRA+ studies the special case for low-rank matrices of the form ${\boldsymbol{\mathbf{W}}} = {\boldsymbol{\mathbf{U}}}{\boldsymbol{\mathbf{V}}}, {\boldsymbol{\mathbf{U}}} \in \mathbb{R}^{d\times r}, {\boldsymbol{\mathbf{V}}} \in \mathbb{R}^{r\times d}, r \ll d,$ and proposes that ${\boldsymbol{\mathbf{U}}}$ should have a higher learning rate compared to ${\boldsymbol{\mathbf{V}}}$, consistent with the more general analysis we present in this work that also applies to other structured matrices.

# Runtime Comparisons

All structures in this work use the same dense matrix multiplication primitive on the GPU, so FLOPs are proportional to their runtimes for large matrix sizes. Only below a certain scale do runtimes vary noticeably between structures as a function of FLOPs. We verify this on an Nvidia A100 GPU in , showing the time for matrix-vector multiplication for different structures vs. FLOPs. For small matrices, runtimes vary between structures and don’t reflect FLOPs due to inefficient tensor core utilization. For large matrices, runtimes converge to the same function in FLOPs. Optimizing structured matrix implementations can reduce their runtime overhead and will be essential to realizing the practical benefits of these structures.

Measuring FLOPs allows incorporating results from smaller experiments without letting the runtime inefficiencies at small scale obscure the scaling laws. and compare BTT with dense MLPs on CIFAR-100 in FLOPs and runtimes on an Nvidia A100. Below $\sim10^7$ FLOPs, increasing FLOPs barely changes runtimes for dense and BTT, obscuring the scaling laws. BTT underperforms dense when controlling for runtime by incurring longer runtime per FLOP at this scale. However, as compute increases, scaling laws in FLOPs translate to scaling laws in runtimes, with BTT outperforming dense significantly.

# General Expression for Tensor-Train and Block Tensor-Train

Here we describe the general expression for Tensor-Train and Block Tensor-Train, with an arbitrary number of cores and ranks. To make the expression more intuitive, we will use superscripts for output indices and subscripts subscripts for input indices. Rank indices appear once as a superscript when first introduced and once as a subscript when summed away.

<div id="tab:structures_mup">

| Structure                                                                                                                |                                Learning rate multiplier $\kappa$                                |
|:-------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------:|
| Low-Rank ${\boldsymbol{\mathbf{U}}} {\boldsymbol{\mathbf{V}}}$                                                           |        $\kappa_{\boldsymbol{\mathbf{U}}} = d/2r, \kappa_{\boldsymbol{\mathbf{V}}} = 1/2$        |
| Kronecker ${\boldsymbol{\mathbf{L}}} \otimes {\boldsymbol{\mathbf{R}}}$                                                  | $\kappa_{\boldsymbol{\mathbf{L}}} = \sqrt{d}/2, \kappa_{\boldsymbol{\mathbf{R}}} = \sqrt{d}/2$  |
| Monarch ${\boldsymbol{\mathbf{P}}} {\boldsymbol{\mathbf{L}}} {\boldsymbol{\mathbf{P}}}^{\top} {\boldsymbol{\mathbf{R}}}$ |        $\kappa_{\boldsymbol{\mathbf{L}}} = b/2, \kappa_{\boldsymbol{\mathbf{R}}} = b/2$         |
| TT$({\boldsymbol{\mathbf{L}}},{\boldsymbol{\mathbf{R}}})$                                                                | $\kappa_{\boldsymbol{\mathbf{L}}} = \sqrt{d}/2r, \kappa_{\boldsymbol{\mathbf{R}}} = \sqrt{d}/2$ |
| BTT$({\boldsymbol{\mathbf{L}}},{\boldsymbol{\mathbf{R}}})$                                                               | $\kappa_{\boldsymbol{\mathbf{L}}} = \sqrt{d}/2r, \kappa_{\boldsymbol{\mathbf{R}}} = \sqrt{d}/2$ |

**Learning rate multipliers for structured matrices.** We show the Adam learning rate multiplier $\kappa$ we use for each parameter tensor of the structure when transferring the learning rate from a dense layer of the same width $d.$ $r$ refers to the rank in low rank, TT, and BTT, while $b$ refers to the number of blocks in Monarch.

</div>

**Tensor-Train.** Tensor-Train (TT) decomposition of a ${d_\mathrm{out}}\times {d_\mathrm{in}}$ matrix ${\boldsymbol{\mathbf{W}}}$ is defined by a set of $c$ cores ${\boldsymbol{\mathbf{G}}}_t \in \mathbb{R}^{r_{t-1} \times  m_{t} \times  n_{t} \times  r_{t}}$ for $t=1, \dots, c,$ where $c \geq 2, {d_\mathrm{out}}= \prod_{t}^{} m_{t}, {d_\mathrm{in}}= \prod_{t}^{} n_{t},$ $r_{0} = r_{c} = 1$ and $\{r_t\}_{t=1}^{c}$ being free integer hyperparameters. These cores specify the elements of an $n_1 \times \ldots \times n_t \times m_1 \times \ldots \times m_t$ tensor ${\boldsymbol{\mathbf{T}}}$ via

$$T^{i_1, \ldots, i_c}_{j_1, \ldots, j_c} = \sum_{\alpha_1, \ldots, \alpha_{t+1}} \prod_{t=1}^{c} (G_t)^{\alpha_{t-1}, i_t}_{ j_t, \alpha_t}.$$

Identifying elements of ${\boldsymbol{\mathbf{T}}}$ with elments of a ${d_\mathrm{out}}\times {d_\mathrm{in}}$ matrix ${\boldsymbol{\mathbf{W}}}$, the efficient matrix-vector multiply against ${\boldsymbol{\mathbf{W}}}$ does not involve materializing ${\boldsymbol{\mathbf{W}}}$ but is simply given by a sequence of contractions against each core ${\boldsymbol{\mathbf{G}}}_t$ from $t=c$ to $t=1:$

$$\label{eq:full}
    \begin{split}
      (z_{t-1})^{\alpha_{t-1}, j_1, \ldots, j_{t-1}, i_t, \ldots, i_c} = \sum_{\alpha_t=1}^{r_t}  \sum_{j_t=1}^{n_t} (G_t)^{\alpha_{t-1}, i_t}_{j_t, \alpha_t} (z_t)^{\alpha_t, j_1, \ldots, j_t, i_{t+1}, \ldots, i_c},
    \end{split}$$

where the initial ${\boldsymbol{\mathbf{z}}}_c$ is obtained by reshaping the input ${\boldsymbol{\mathbf{x}}}$ into an $n_c \times n_{c-1} \ldots \times n_1 \times 1$ tensor and the final ${\boldsymbol{\mathbf{z}}}_0$ is flattened into an output vector. Suppose, for convenience, ${d_\mathrm{in}}={d_\mathrm{out}}=d,$ $n_t = m_t = d^{1/c}$ for all $t,$ and $r_t = r$ for all $t \notin \{0, c\},$ then TT has $P=(2r + (c-2) r^2) d^{2/c}$ parameters, and an MVM costs $C=(2r + (c-2) r^2) d^{1+c^{-1}}$ FLOPs. Note we have $C = P d^{1 - c^{-1}},$ showing each parameter is used for $d^{1 - c^{-1}} \geq \sqrt{d}$ times.

**Block Tensor-Train.** Block Tensor-Train (BTT) is defined simply by appending <span style="color: purple">additional axes</span> to each core in TT via the substitution $$(G_t)^{\alpha_{t-1}, i_t}_{j_t, \alpha_t} \to (G_t)^{\alpha_{t-1}, i_t, \textcolor{purple}{i_{t+1}, \ldots, i_c}}_{\textcolor{purple}{j_1, \ldots, j_{t-1}}, j_t, \alpha_{t}}.$$

As before, multiplying the cores and summing out the rank axes, we have $$T^{i_1, \ldots, i_c}_{j_1, \ldots, j_c} = \sum_{\alpha_1, \ldots, \alpha_{t+1}} \prod_{t=1}^{c} (G_t)^{\alpha_{t-1}, i_t, \textcolor{purple}{i_{t+1}, \ldots, i_c}}_{\textcolor{purple}{j_1, \ldots, j_{t-1}}, j_t, \alpha_{t}}.$$

Efficient multiplication with the corresponding matrix is now given by

$$\begin{split}
      (z_{t-1})^{\alpha_{t-1}, j_1, \ldots, j_{t-1}, i_t, \ldots, i_c} = \sum_{\alpha_t=1}^{r_t}  \sum_{j_t=1}^{n_t} (G_t)^{\alpha_{t-1}, i_t, \textcolor{purple}{i_{t+1}, \ldots, i_c}}_{\textcolor{purple}{j_1, \ldots, j_{t-1}}, j_t, \alpha_{t}} (z_t)^{\alpha_t, j_1, \ldots, j_t, i_{t+1}, \ldots, i_c},
    \end{split}$$

which costs the same FLOPs as for TT, while admitting more learnable parameters. Again we do not need to materialize ${\boldsymbol{\mathbf{T}}}.$ Suppose, for convenience, ${d_\mathrm{in}}={d_\mathrm{out}}=d,$ $n_t = m_t = d^{1/c}$ for all $t,$ and $r_t = r$ for all $t \notin \{0, c\},$ then BTT has $P=(2r + (c-2) r^2) d^{1+c^{-1}}$ parameters, equal in number to the FLOPs for an MVM $C=(2r + (c-2) r^2) d^{1+c^{-1}}$. Thus, for the same amount of compute, BTT can learn a factor of $d^{1 - c^{-1}} \geq \sqrt{d}$ more parameters than TT.

# Expressivity of Block Tensor-Train

We start by providing an algorithm to approximate any existing dense matrix ${\boldsymbol{\mathbf{A}}}$ with a BTT. The algorithm will then illustrate the expressivity of the BTT structure as a function of $c$ and $\{r_t\}_{t=1}^{c}$. For simplicity, we will assume ${\boldsymbol{\mathbf{A}}} \in \mathbb{R}^{d \times d},$ and the cores will be square, having size $d^{1/c}$ in each dimension, except for the rank dimension. Generalization to non-square ${\boldsymbol{\mathbf{A}}}$ and non-square cores is straigtforward.

**Projection onto Block Tensor-Train with $c=2$.** In the case where $c=2,$ we prove a closed-form expression for projecting an arbitrary dense matrix ${\boldsymbol{\mathbf{A}}}$ to the closest rank-$r$ (there is only one rank parameter so we omit the subscript) BTT ${\boldsymbol{\mathbf{B}}}$ that minimizes the squared Frobenius norm $\norm{{\boldsymbol{\mathbf{A}}} - {\boldsymbol{\mathbf{B}}}}^2_F.$ Writing ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$ as $\sqrt{d} \times \sqrt{d} \times \sqrt{d} \times \sqrt{d}$ tensors with $B^{ii'}_{jj'} = \sum_{\alpha=1}^{r} L^{ii'}_{j\alpha} R^{\alpha i'}_{j j'},$ we have $$\begin{aligned}
    & \norm{{\boldsymbol{\mathbf{A}}} - {\boldsymbol{\mathbf{B}}}}^2_F  \\
    =& \sum_{ii'jj'} \qty(A^{ii'}_{jj'} - \sum_{\alpha=1}^{r} L^{ii'}_{j\alpha} R^{\alpha i'}_{j j'})^2 \\
    =& \sum_{\textcolor{purple}{i'j}} \sum_{ij'} \qty(A^{i\textcolor{purple}{i'}}_{\textcolor{purple}{j}j'} - \sum_{\alpha=1}^{r} L^{i\textcolor{purple}{i'}}_{\textcolor{purple}{j}\alpha} R^{\alpha \textcolor{purple}{i'}}_{\textcolor{purple}{j} j'})^2 \\
    =& \sum_{\textcolor{purple}{i'j}}  \norm{{\boldsymbol{\mathbf{A}}}^{(\textcolor{purple}{i'j})} - \sum_{\alpha=1}^{r} {\boldsymbol{\mathbf{\ell}}}^{(\textcolor{purple}{i'j})}_\alpha {\boldsymbol{\mathbf{r}}}^{(\textcolor{purple}{i'j})\top}_\alpha}^2_F, \\
\end{aligned}$$ where we have decomposed the minimization problem into multiple independent minimization problems: for each $i',j,$ we wish to find the best rank-$r$ approximation $\sum_{\alpha=1}^{r} {\boldsymbol{\mathbf{\ell}}}^{(i'j)}_\alpha {\boldsymbol{\mathbf{r}}}^{(i'j)\top}_\alpha$ to the matrix ${\boldsymbol{\mathbf{A}}}^{(i'j)} \in \mathbb{R}^{\sqrt{d} \times \sqrt{d}}.$ Thus, we obtain an optimal solution by finding these best rank-$r$ approximation (e.g. via SVD) for each ${\boldsymbol{\mathbf{A}}}^{(i'j)},$ and reassembling the vectors ${\boldsymbol{\mathbf{\ell}}}^{(i'j)}_\alpha$ and ${\boldsymbol{\mathbf{r}}}^{(i'j)}_\alpha$ into the tensors ${\boldsymbol{\mathbf{L}}}$ and ${\boldsymbol{\mathbf{R}}}.$ This result is a straightforward generalization of the algorithm for projection onto Monarch matrices , which deals with the case where $r=1.$

**Generalization to $c > 2$.** For convenience, let’s relabel ${\boldsymbol{\mathbf{L}}}$ found in the previous algorithm as ${\boldsymbol{\mathbf{\tilde{L}}}},$ and the rank $r$ as $r_2.$ Having found ${\boldsymbol{\mathbf{\tilde{L}}}}$ and ${\boldsymbol{\mathbf{R}}},$ we can recursively apply the above algorithm on ${\boldsymbol{\mathbf{\tilde{L}}}}$ to find its optimal 2-core rank-$r_1$ BTT approximation, with cores ${\boldsymbol{\mathbf{L}}}$ and ${\boldsymbol{\mathbf{M}}}$. Together, ${\boldsymbol{\mathbf{L}}}, {\boldsymbol{\mathbf{M}}},$ and ${\boldsymbol{\mathbf{R}}}$ parameterize a 3-core BTT approximation with ranks $r_1$ and $r_2.$ Similar to the recursive TT-SVD algorithm , the found solution will not necessarily be optimal for $c > 2$ due to its greediness.

It is sufficient to illustrate this algorithm in detail for $c=3.$ Reshaping ${\boldsymbol{\mathbf{A}}}$ into a tensor $A^{i_1 i_2 i_3}_{j_1 j_2 j_3} \in \mathbb{R}^{d^{1/3}\times\ldots\times d^{1/3}},$ we wish to find $B^{i_1 i_2 i_3}_{j_1 j_2 j_3} = \sum_{\alpha=1}^{r} \sum_{\beta=1}^{r} L^{i_1 i_2 i_3}_{j_1 \beta} M^{\beta i_2 i_3}_{j_1 j_2 \alpha} R^{\alpha i_3}_{j_1 j_2 j_3}$ that approximates ${\boldsymbol{\mathbf{A}}}.$ We first group $i_1,i_2$ as a single index $(i_1 i_2)$ and $j_1,j_2$ as a single index $(j_1 j_2),$ and then apply the previous algorithm for the 2-core case to find $\tilde{{\boldsymbol{\mathbf{L}}}}, {\boldsymbol{\mathbf{R}}}$ that minimizes $$\sum_{(i_1 i_2) i_3 (j_1 j_2) j_3} \qty(A^{(i_1 i_2) i_3}_{(j_1 j_2) j_3} - \sum_{\alpha=1}^{r_2} \tilde{L}^{(i_1 i_2) i_3}_{(j_1 j_2) \alpha} R^{\alpha i_3}_{(j_1 j_2) j_3})^2,$$ forming the best following best rank-$r_2$ 2-core approximation: $$\label{eq:2core}
    A^{(i_1 i_2) i_3}_{(j_1 j_2) j_3} \approx \sum_{\alpha=1}^{r_2} \tilde{L}^{(i_1 i_2) i_3}_{(j_1 j_2) \alpha} R^{\alpha i_3}_{(j_1 j_2) j_3}.$$ Setting $r_2 = \min(\#(i_1 i_2), \# j_3) = \sqrt{d}$ will lead to an exact decomposition, where $\# \chi$ denotes the length of the range of the index $\chi$. Then we un-group the indicies to the obtain $\tilde{L}^{i_1 i_2 i_3}_{j_1 j_2 \alpha}, R^{\alpha i_3}_{j_1 j_2 j_3}.$ Now grouping $i_2 i_3$ and $j_2 \alpha$ as single indices, we apply the previous algorithm again to find the best rank-$r_1$ 2-core BTT approximation to $\tilde{{\boldsymbol{\mathbf{L}}}}$ yielding the tensors ${\boldsymbol{\mathbf{L}}}, {\boldsymbol{\mathbf{M}}}$ that minimize $$\sum_{i_1 (i_2 i_3) j_1 (j_2 \alpha)} \qty(\tilde{L}^{i_1 (i_2 i_3)}_{j_1 (j_2 \alpha)} - \sum_{\beta=1}^{r_1} L^{i_1 (i_2 i_3)}_{j_1 \beta} M^{\beta (i_2 i_3)}_{j_1 (j_2\alpha)})^2. \\$$

Setting $r_1 = \min(\# i_1, \# (j_2 \alpha)) = \sqrt{d}$ will again lead to an exact decomposition, Now replacing $\tilde{L}^{i_{12} i_3}_{j_{12} \alpha}$ in by its approximation $\sum_{\beta=1}^{r_1} L^{i_1 i_2 i_3}_{j_1 \beta} M^{\beta i_2 i_3}_{j_1 j_2\alpha},$ we have found the 3-core BTT approximation to ${\boldsymbol{\mathbf{A}}}$ with ranks $(r_1, r_2):$ $$A^{i_1 i_2 i_3}_{j_1 j_2 j_3} \approx B^{i_1 i_2 i_3}_{j_1 j_2 j_3} = \sum_{\beta=1}^{r_1} \sum_{\alpha=1}^{r_2} L^{i_1 i_2 i_3}_{j_1 \beta} M^{\beta i_2 i_3}_{j_1 j_2 \alpha} R^{\alpha i_3}_{j_1 j_2 j_3}.$$

**Quantifying the expressivity of BTT.** By applying the above recursive algorithm and always choosing a high enough rank so that the decomposition is exact at each step, we prove that a $c$-core BTT with sufficiently large ranks $\{r_t\}_{t=1}^{c}$ can represent any $d\times d$ dense matrix exactly. Moreover, the general expression for an upper-bound on $r_t$ to ensure exact decomposition can be deduced as $r_t \leq \min(\# i_1 \times \ldots \times \# i_t, \# j_{t+1} \times r_{t+1}) \leq d^{\min(t, c-t)/c}:$ i.e. $r_1 \leq d^{1/c}, r_2 \leq d^{2/c}, \ldots, r_{c/2} \leq \sqrt{d}, \ldots, r_{c-1} \leq d^{2/c}, r_c \leq d^{1/c}.$ By contrast, TT has a worse bound of $r_1 \leq d^{2/c}, r_2 \leq d^{4/c}, \ldots, r_{c/2} \leq d, \ldots, r_{c-1} \leq d^{4/c}, r_c \leq d^{2/c}$ .

A practical takeaway is that we can monotonically improve the expressivity of BTT by increasing $r_t$ until the bound is reached, and we should never use ranks larger than the bound since it creates unnecessary redundancy in the parameterization.

# Scaling Laws Experiment Details

We provide code for reproducing our experiments [<u>here</u>](https://github.com/shikaiqiu/compute-better-spent).

## Model architectures

**MLP.** Following , we use MLPs consisting of residual blocks of the form $${\boldsymbol{\mathbf{h}}}_{\ell+1}
      =
      {\boldsymbol{\mathbf{h}}}_{\ell} + {\boldsymbol{\mathbf{W}}}_{\ell}^{(2)}g\qty({\boldsymbol{\mathbf{W}}}_{\ell}^{(1)} \text{LN}\left({\boldsymbol{\mathbf{h}}}_{\ell}\right)), \quad {\boldsymbol{\mathbf{W}}}_{\ell}^{(1)} \in \mathbb{R}^{4d \times d}, \quad {\boldsymbol{\mathbf{W}}}_{\ell}^{(2)} \in \mathbb{R}^{d \times 4d},$$ where $g\left(\cdot\right)$ denotes the GELU activation and $\text{LN}\left(\cdot\right)$ stands for layer normalization . In addition, there is an input embedding layer and a classification layer. We refer to $d$ as the width of the model. We use models with $3$ residual blocks and scale them up by increasing $d.$

**ViT.** We use standard ViTs , but with $1/d-$scaled rather $1/\sqrt{d}-$scaled attention as prescribed by $\mu$P and Query-Key Normalization for improved stability. We refer to the embedding dimension, commonly denoted $d_\mathrm{model}$, as the width $d$ of the model. We use models with $3$ transformer blocks and scale them up by increasing $d.$

## Hyperparameters

**Training hyperparameters.** We use random crop, random flip, and MixUp ($\alpha=0.8$) data augmentations, and label smoothing of $0.3.$ We train all MLP models for 500 epochs with batch size 1024, and all ViT models for 200 epochs with batch size 256. At the end of training, the models are close to but not exactly at convergence because fitting the training set is challenging due to strong augmentations and label smoothing. We do not use early stopping as it is not necessary.

We use structure-aware learning rates and initialization described in , with a cosine learning rate decay to $0$. We set the constant in $\Theta(\cdot)$ as $1$ for the initialization standard deviations, with the exception that the last linear layer inside every residual block of the MLP and ViT is zero-initialized, as mentioned in . For a structured layer, zero-initialization is only applied to its last dense component so its output is zero at initialization but all the parameters receive non-zero gradients after the first step. Following , we also zero-initialize the classification layer and the query projection ${\boldsymbol{\mathbf{W}}}_Q$ in transformers. We found zero-initialization generally improves performance.

We use a base learning rate of $\eta_0 = 3e-3$ for a dense MLP at $d_0=64,$ and $\eta_0 = 1e-3$ for a dense ViT at $d_0=64.$ For MLPs, we scale the learning rate of the input layer by a factor of $0.1$ since the input image dimension is much larger than $d_0$. This small multiplier prevents the first layer feature updates from having much larger scales than the other layers , which we found improves performance.

**Structure-specific hyperparameters.** We provide hyperparameters such as ranks we use for each structure and any other design choices we make.

- Low-rank: we set the ranks of low-rank matrices to $\sqrt{\min({d_\mathrm{in}}, {d_\mathrm{out}})}$ for MLP and $0.1 \times \min({d_\mathrm{in}}, {d_\mathrm{out}})$ for ViT. The first choice leads to $\order{d^{3/2}}$ scaling of compute and parameters, same as Kronecker, 2-core BTT, and 2-core TT, but the second choice works significantly better for ViTs. We round the rank to its nearest integer when necessary. We initialize ${\boldsymbol{\mathbf{V}}} \in \mathbb{R}^{r \times d}$ of the low-rank layer as $V_{ij} \sim \mathcal{N}(0, \sqrt{1/{d_\mathrm{in}}}),$ rather than $V_{ij} \sim \mathcal{N}(0, \sqrt{1/(r{d_\mathrm{in}})}).$ While the latter is required for having the desired spectral norm at initialization according to , when we choose a rank of $\sqrt{\min({d_\mathrm{in}}, {d_\mathrm{out}})},$ it is not compatible with our zero-initialization scheme as it led to vanishing gradients for both ${\boldsymbol{\mathbf{U}}}$ and ${\boldsymbol{\mathbf{V}}}$ as the width gets large.

- Kronecker: for any dimension $d$ that is not a perfect square, we factorize it so that the factors are as close as possible. For example, for a $20 \times 30$ matrix, we use the factorization ${\boldsymbol{\mathbf{L}}} \otimes {\boldsymbol{\mathbf{R}}}$ where ${\boldsymbol{\mathbf{L}}} \in \mathbb{R}^{4 \times 5}$ and ${\boldsymbol{\mathbf{R}}} \in \mathbb{R}^{5 \times 6}.$

- TT: we use two cores with TT-rank of $16$ for MLPs and $8$ for ViTs. We deal with non-perfect-square dimensions same as in Kronecker.

- Monarch: unless otherwise specified, we use ${\boldsymbol{\mathbf{L}}}$ and ${\boldsymbol{\mathbf{R}}}$ with 4 blocks, following the ViT and GPT-2 experiments in .

- BTT: we use BTT with various ranks and deal with non-perfect-square dimensions same as in Kronecker.

# Results for BTT with $c > 2$

In , we showed scaling compute per dimension $\xi$ as $\xi=2d^{1/2}$ using BTT with $c=2$ and $r=1$ leads to better scaling laws than other choices of $r$ that increases $\xi$ to $2r^{1/2}.$ The gap between different choices of $r$ closes as the models are scaled up in width, e.g. $d \gg r.$ In , we show a similar trend for $c=3,$ where higher values of $r$ perform worse when controlling for FLOPs, though the gap tends to vanish as the width is scaled up. Each connected line shows the performance of BTT with a fixed $r$ while $d$ is increased.

In , we show the performance of BTT with $r=1$ and $c \in \{2, 3, 4\}.$ Further reducing the scaling of $\xi$ to $3d^{1/3}$ or $4d^{1/4}$ brings no or negligible improvement to performance when controlling for FLOPs.

In summary, choosing $c=2$ and $r=1$ leads to near-optimal performance for BTT on these tasks. In this case, BTT is equivalent to Monarch with $\sqrt{d}$ blocks.

<figure id="fig:3-core-rank">

<figcaption> <strong>Lower BTT-ranks have better compute-efficiency for BTT with <span class="math inline">\(c=3\)</span> cores.</strong> Controlling for FLOPs, increasing the rank often degrades performance, though it reduces memory cost as the width is smaller. </figcaption>
</figure>

<figure id="fig:more-cores">

<figcaption> <strong>BTT with <span class="math inline">\(c=2\)</span> cores achieves near-optimal compute-efficiency.</strong> Controlling for FLOPs, increasing <span class="math inline">\(c\)</span> beyond 2 leads to no or negligible improvement in performance, while incurring higher memory costs as the models are wider. </figcaption>
</figure>

# Transformer experiments

We provide code for reproducing our experiments [<u>here</u>](https://github.com/shikaiqiu/compute-better-spent).

## ViT on ImageNet

We train with a global batch size of 3072 for 300 epochs with random crops, horizontal flip, random augmentations (`rand-m9-mstd0.5-inc1` from the `timm` library ), and Mixup of 0.2. The model has 12 transformer blocks, with width $d_\mathrm{model}$ ranging from $80$ to $384$ for dense. We use BTT with rank 1 or 2 and Monarch with 4 or 16 blocks. All but the classification head is replaced with structured matrices. We use the AdamW optimizer and set the base learning rate to $2e-3$ for the smallest dense model, which is transferred to other models via $\mu$P and our structured-aware learning rate scaling. We apply a cosine learning rate decay to $0.$ The AdamW weight decay is set to $0.05$ for all models and is scaled automatically with width by being multiplied by the learning rate . The architecture is identical to the one in .

## GPT-2 on OpenWebText

We train with a global batch size of 480 and a context length of 512 for 600,000 steps. We report the performance of the following models, all having 12 transformer blocks:

- Structure $=$ Dense, $d_\mathrm{model}=384,$ $n_\mathrm{head} = 6$, $d_\mathrm{head} = 64$

- Structure $=$ Dense, $d_\mathrm{model}=512,$ $n_\mathrm{head} = 12$, $d_\mathrm{head} = 64$

- Structure $=$ Dense, $d_\mathrm{model}=768,$ $n_\mathrm{head} = 12$, $d_\mathrm{head} = 64$ (GPT-2 Small )

- Structure $=$ BTT ($r=4$), $d_\mathrm{model}=1024,$ $n_\mathrm{head} = 6$, $d_\mathrm{head} = 64$

- Structure $=$ BTT ($r=4$), $d_\mathrm{model}=1536,$ $n_\mathrm{head} = 6$, $d_\mathrm{head} = 64$

- Structure $=$ BTT ($r=4$), $d_\mathrm{model}=2048,$ $n_\mathrm{head} = 6$, $d_\mathrm{head} = 64$

- Structure $=$ BTT ($r=4$), $d_\mathrm{model}=2560,$ $n_\mathrm{head} = 12$, $d_\mathrm{head} = 64$

We use BTT with rank 4 in every linear layer, including the language modeling head. We set $n_\mathrm{head}$ to be smaller than the usual $d_\mathrm{model} / d_\mathrm{head}$ for the BTT models since otherwise we would spend too much compute in the attention layers relative to the FFN layers. We use the Adam optimizer and set the base learning rate to $6e-4$ for the dense model at $d_\mathrm{model}=768$, which is transferred to other models via $\mu$P and our structured-aware learning rate scaling.

# Structure-Aware Learning Rate for Other Optimizers

The structure-aware learning rate scaling described in applies to Adam or AdamW. However, we can derive appropriate scaling rules for other optimizers such as SGD. In Section 3.3, we obtain our structure-aware learning rate scaling rule in three steps: 1) decompose the matrix-vector multiplication (MVM) of a structured matrix ${\boldsymbol{\mathbf{W}}} \in \mathbb{R}^{{d_\mathrm{out}}\times {d_\mathrm{in}}}$ as a sequence of batched MVMs involving only dense matrices $\{{\boldsymbol{\mathbf{G}}}_i\}_{i=1}^{k}$, 2) identify the input and output dimensions $d^i_\mathrm{in}$ and $d^i_\mathrm{out}$ of these dense matrices, 3) apply $\mu$P to each of these dense matrices to scale their learning rates based on $d^i_\mathrm{in}$ and $d^i_\mathrm{out}$. Steps 1 and 2 are optimizer-agnostic. While step 3 is optimizer-dependent, it only requires knowing how to set $\mu$P learning rates for regular dense matrices, which has been analyzed in prior works for various optimizers, including SGD, Adam, and SignSGD . For example, instead of having the learning rate $\eta_i$ of ${\boldsymbol{\mathbf{G}}}_i$ be $\Theta(1/d^i_\mathrm{in}),$ which is correct for Adam, SGD would require $\eta_i = \Theta(d^i_\mathrm{out} / d^i_\mathrm{in})$ . Therefore, the structure-aware learning rate multiplier relative to a dense ${\boldsymbol{\mathbf{W}}}$ should now be $\kappa_i = \Theta\qty(\frac{d^i_\mathrm{out} / d^i_\mathrm{in}}{{d_\mathrm{out}}/ {d_\mathrm{in}}})$ instead of $\Theta({d_\mathrm{in}}/d^i_\mathrm{in}),$ which is correct for Adam.

# Limitations and Future Work

We provide a summary of the limitations of this work, and exciting directions for future work:

- Due to affordability constraints, we conducted our evaluation primarily with relatively small-scale models and datasets. Extending our evaluation to much larger-scale models and datasets is an important future direction.

- The scaling laws we study differ from the compute-optimal scaling laws more relevant for large-scale training, which require optimally trading off between training larger models and training for more iterations. We only varied model size while keeping training iterations constant. Similarly, we did not optimize between scaling width v.s. depth, which allowed us to conveniently transfer learning rate through $\mu$P[^2].

- Our comparisons are based on FLOPs rather than runtimes. While the structures we consider have asymptotically the same MVM runtimes as dense matrices per FLOP (), they introduce non-trivial runtime overhead for small matrix sizes, e.g. $\order{10^3}$. Developing highly optimized implementations will be important to realize the benefits of structured matrices in practice.

- Despite our efforts to avoid over-fitting to image data (shuffling pixels for the MLP experiment), our findings that structured matrices can significantly outperform dense matrices may still be highly dataset-dependent, as BTT offers a less significant improvement in language modeling compared to in image classification.

- Our findings are empirical. Theoretically understanding when and why structured matrices can have better scaling laws than dense matrices, depending on model and data characteristics, will enable a prescriptive selection of structure rather than via trial and error alone.

[^1]: Here and elsewhere in the paper we use the more familiar term FLOPs as a stand-in for MACs (Multiply-Accumulate) operations to highlight when they match the number of parameters, even though $1$ MAC is technically $2$ FLOPs.

[^2]: See for a depth extension of $\mu$P and why it doesn’t work for transformers in principle.
