\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016ln}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock {Layer Normalization}.
\newblock \emph{Preprint arXiv 1607.06450}, 2016.

\bibitem[Bachmann et~al.(2023)Bachmann, Anagnostidis, and Hofmann]{bachmann2023scaling}
Bachmann, G., Anagnostidis, S., and Hofmann, T.
\newblock Scaling mlps: A tale of inductive bias.
\newblock \emph{arXiv preprint arXiv:2306.13575}, 2023.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma]{bahri2021explaining}
Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chekalina et~al.(2023)Chekalina, Novikov, Gusak, Oseledets, and Panchenko]{chekalina2023gpttrans}
Chekalina, V., Novikov, G., Gusak, J., Oseledets, I., and Panchenko, A.
\newblock {Efficient GPT Model Pre-training using Tensor Train Matrix Representation}.
\newblock \emph{Preprint arXiv 2306.02697}, 2023.

\bibitem[Critch et~al.(2014)Critch, Morton, et~al.]{critch2014algebraic}
Critch, A., Morton, J., et~al.
\newblock Algebraic geometry of matrix product states.
\newblock \emph{SIGMA. Symmetry, Integrability and Geometry: Methods and Applications}, 10:\penalty0 095, 2014.

\bibitem[Dao et~al.(2022)Dao, Chen, Sohoni, Desai, Poli, Grogan, Liu, Rao, Rudra, and R\'{e}]{dao2022monarch}
Dao, T., Chen, B., Sohoni, N., Desai, A., Poli, M., Grogan, J., Liu, A., Rao, A., Rudra, A., and R\'{e}, C.
\newblock {Monarch: Expressive Structured Matrices for Efficient and Accurate Training}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Diederik P.~Kingma(2015)]{kingma2015adam}
Diederik P.~Kingma, J.~B.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2015.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2022vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}.
\newblock \emph{Preprint arXiv 2010.11929}, 2020.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Fannes et~al.(1992)Fannes, Nachtergaele, and Werner]{fannes1992finitely}
Fannes, M., Nachtergaele, B., and Werner, R.~F.
\newblock Finitely correlated states on quantum spin chains.
\newblock \emph{Communications in mathematical physics}, 144:\penalty0 443--490, 1992.

\bibitem[Finzi et~al.(2020)Finzi, Stanton, Izmailov, and Wilson]{finzi2020generalizing}
Finzi, M., Stanton, S., Izmailov, P., and Wilson, A.~G.
\newblock Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3165--3176. PMLR, 2020.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018ticket}
Frankle, J. and Carbin, M.
\newblock {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks: w}.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2018.

\bibitem[Fu et~al.(2023)Fu, Arora, Grogan, Johnson, Eyuboglu, Thomas, Spector, Poli, Rudra, and R\'{e}]{fu2023mixer}
Fu, D.~Y., Arora, S., Grogan, J., Johnson, I., Eyuboglu, S., Thomas, A.~W., Spector, B., Poli, M., Rudra, A., and R\'{e}, C.
\newblock {Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural networks.
\newblock In \emph{Proceedings of the thirteenth international conference on artificial intelligence and statistics}, pp.\  249--256. JMLR Workshop and Conference Proceedings, 2010.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2016deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}.
\newblock \emph{The 4th International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Hayou et~al.(2024)Hayou, Ghosh, and Yu]{hayou2024lora+}
Hayou, S., Ghosh, N., and Yu, B.
\newblock Lora+: Efficient low rank adaptation of large models.
\newblock \emph{arXiv preprint arXiv:2402.12354}, 2024.

\bibitem[He et~al.(2015{\natexlab{a}})He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  1026--1034, 2015{\natexlab{a}}.

\bibitem[He et~al.(2015{\natexlab{b}})He, Zhang, Ren, and Sun]{he2015resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock \emph{Preprint arXiv 1512.03385}, 2015{\natexlab{b}}.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gelu}
Hendrycks, D. and Gimpel, K.
\newblock {Gaussian Error Linear Units (GELUs)}.
\newblock \emph{Preprint arXiv 1606.08415}, 2016.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scaling}
Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T.~B., Dhariwal, P., Gray, S., et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{arXiv preprint arXiv:2010.14701}, 2020.

\bibitem[Henry et~al.(2020)Henry, Dachapally, Pawar, and Chen]{henry2020query}
Henry, A., Dachapally, P.~R., Pawar, S., and Chen, Y.
\newblock Query-key normalization for transformers.
\newblock \emph{arXiv preprint arXiv:2010.04245}, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock {LoRA: Low-Rank Adaptation of Large Language Models}.
\newblock \emph{Preprint arXiv 2106.09685}, 2021.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:1609.02907}, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, , and Hinton]{krizhevsky2021alexnet}
Krizhevsky, A., Sutskever, I., , and Hinton, G.~E.
\newblock {ImageNet Classification with Deep Convolutional Neural Networks}.
\newblock \emph{Communications of the ACM, Volume 60, Issue 6}, 2012.

\bibitem[LeCun et~al.(1998{\natexlab{a}})LeCun, Bottou, Bengio, , and Haffner]{lecun98grad}
LeCun, Y., Bottou, L., Bengio, Y., , and Haffner, P.
\newblock {Gradient-Based Learning Applied to Document Recognitio}.
\newblock \emph{Proceedings of the IEEE, Volume: 86, Issue: 11}, 1998{\natexlab{a}}.

\bibitem[LeCun et~al.(1998{\natexlab{b}})LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998{\natexlab{b}}.

\bibitem[LeCun et~al.(2002)LeCun, Bottou, Orr, and M{\"u}ller]{lecun2002efficient}
LeCun, Y., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--50. Springer, 2002.

\bibitem[Lee \& Kim(2023)Lee and Kim]{lee2023differentiable}
Lee, C. and Kim, H.-S.
\newblock Differentiable learning of generalized structured matrices for efficient deep neural networks.
\newblock \emph{arXiv preprint arXiv:2310.18882}, 2023.

\bibitem[Lialin et~al.(2023)Lialin, Muckatira, Shivagunde, and Rumshisky]{lialin2023relora}
Lialin, V., Muckatira, S., Shivagunde, N., and Rumshisky, A.
\newblock Relora: High-rank training through low-rank updates.
\newblock In \emph{Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)}, 2023.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017slim}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock {Learning Efficient Convolutional Networks through Network Slimming}.
\newblock \emph{International Conference on Computer Vision (ICCV)}, 2017.

\bibitem[Maron et~al.(2020)Maron, Litany, Chechik, and Fetaya]{maron2020learning}
Maron, H., Litany, O., Chechik, G., and Fetaya, E.
\newblock On learning sets of symmetric elements.
\newblock In \emph{International conference on machine learning}, pp.\  6734--6744. PMLR, 2020.

\bibitem[Michaud et~al.(2023)Michaud, Liu, Girit, and Tegmark]{michaud2023quantization}
Michaud, E.~J., Liu, Z., Girit, U., and Tegmark, M.
\newblock The quantization model of neural scaling.
\newblock \emph{arXiv preprint arXiv:2303.13506}, 2023.

\bibitem[Mishra et~al.(2021)Mishra, Latorre, Pool, Stosic, Stosic, Venkatesh, Yu, and Micikevicius]{mishra2021accelerating}
Mishra, A., Latorre, J.~A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P.
\newblock {Accelerating Sparse Deep Neural Networks}.
\newblock \emph{Preprint arXiv 2104.08378}, 2021.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and Kautz]{molchanov2016prun}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock {Pruning Convolutional Neural Networks for Resource Efficient Inference}.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Novikov et~al.(2015)Novikov, Podoprikhin, Osokin, and Vetrov]{novikov2015tt-nets}
Novikov, A., Podoprikhin, D., Osokin, A., and Vetrov, D.
\newblock {Tensorizing Neural Networks}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2015.

\bibitem[Oseledets(2011)]{oseledets2011tt}
Oseledets, I.~V.
\newblock {Tensor-Train Decomposition}.
\newblock \emph{SIAM Journal on Scientific Computing}, 2011.

\bibitem[Pan et~al.(2022)Pan, Su, Liu, Jingquan, Li, and Xu]{pan2022unified}
Pan, Y., Su, Z., Liu, A., Jingquan, W., Li, N., and Xu, Z.
\newblock A unified weight initialization paradigm for tensorial convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  17238--17257. PMLR, 2022.

\bibitem[Perez et~al.(2017)Perez, Strub, de~Vries, Dumoulin, and Courville]{perez2017film}
Perez, E., Strub, F., de~Vries, H., Dumoulin, V., and Courville, A.
\newblock {FiLM: Visual Reasoning with a General Conditioning Layer}.
\newblock \emph{Association for the Advancement of Artificial Intelligence (AAAI)}, 2017.

\bibitem[Potapczynski et~al.(2024)Potapczynski, Finzi, Pleiss, and Wilson]{potapczynski2024cola}
Potapczynski, A., Finzi, M., Pleiss, G., and Wilson, A.~G.
\newblock Cola: Exploiting compositional structure for automatic and efficient numerical linear algebra.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019gpt2}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock {Language Models are Unsupervised Multitask Learners}.
\newblock \emph{OpenAI}, 2019.

\bibitem[Saat{\c{c}}i(2012)]{saatcci2012scalable}
Saat{\c{c}}i, Y.
\newblock \emph{Scalable inference for structured Gaussian process models}.
\newblock PhD thesis, Citeseer, 2012.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate training of deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Sharma \& Kaplan(2022)Sharma and Kaplan]{sharma2022scaling}
Sharma, U. and Kaplan, J.
\newblock Scaling laws from the data manifold dimension.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (9):\penalty0 1--34, 2022.

\bibitem[Titsias(2009)]{titsias2009variational}
Titsias, M.~K.
\newblock {Variational Learning of Inducing Variables in Sparse Gaussian Processes}.
\newblock \emph{International Conference on Artificial Intelligence and Statistics, pp. 567-574}, 2009.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, Lucic, and Dosovitskiy]{tolstikhin2021mixer}
Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., and Dosovitskiy, A.
\newblock {MLP-Mixer: An all-MLP Architecture for Vision}.
\newblock \emph{Preprint arXiv 2105.01601}, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock {Linformer: Self-Attention with Linear Complexity}.
\newblock \emph{Preprint arXiv 2006.04768}, 2020.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wilson \& Adams(2013)Wilson and Adams]{wilson2013gaussian}
Wilson, A. and Adams, R.
\newblock Gaussian process kernels for pattern discovery and extrapolation.
\newblock In \emph{International conference on machine learning}, pp.\  1067--1075. PMLR, 2013.

\bibitem[Wilson \& Nickisch(2015)Wilson and Nickisch]{wilson2015kernel}
Wilson, A. and Nickisch, H.
\newblock Kernel interpolation for scalable structured gaussian processes (kiss-gp).
\newblock In \emph{International conference on machine learning}, pp.\  1775--1784. PMLR, 2015.

\bibitem[Wortsman et~al.(2023)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
Wortsman, M., Liu, P.~J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J.~D., Gur, I., Kumar, A., Novak, R., et~al.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock \emph{arXiv preprint arXiv:2309.14322}, 2023.

\bibitem[Yang \& Hu(2021)Yang and Hu]{yang2021infty}
Yang, G. and Hu, E.~J.
\newblock {Feature Learning in Infinite-Width Neural Networks}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Yang \& Littwin(2023)Yang and Littwin]{yang2023iv}
Yang, G. and Littwin, E.
\newblock {Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit}.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Yang et~al.(2021)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2021v}
Yang, G., Hu, E.~J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., and Gao, J.
\newblock {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Simon, and Bernstein]{yang2023spectral}
Yang, G., Simon, J.~B., and Bernstein, J.
\newblock {A Spectral Condition for Feature Learning}.
\newblock \emph{Preprint arXiv:2310.17813}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Yu, Zhu, and Hayou]{yang2023tensor}
Yang, G., Yu, D., Zhu, C., and Hayou, S.
\newblock Tensor programs vi: Feature learning in infinite-depth neural networks.
\newblock \emph{arXiv preprint arXiv:2310.02244}, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore}
Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y.
\newblock Galore: Memory-efficient llm training by gradient low-rank projection.
\newblock \emph{arXiv preprint arXiv:2403.03507}, 2024.

\end{thebibliography}
