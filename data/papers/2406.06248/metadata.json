{
  "arxivId": "2406.06248",
  "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
  "authors": "Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon Wilson",
  "abstract": "Dense linear layers are the dominant computational bottleneck in foundation\nmodels. Identifying more efficient alternatives to dense matrices has enormous\npotential for building more compute-efficient models, as exemplified by the\nsuccess of convolutional networks in the image domain. In this work, we\nsystematically explore structured matrices as replacements for dense matrices.\nWe show that different structures often require drastically different\ninitialization scales and learning rates, which are crucial to performance,\nespecially as models scale. Using insights from the Maximal Update\nParameterization, we determine the optimal scaling for initialization and\nlearning rates of these unconventional layers. Finally, we measure the scaling\nlaws of different structures to compare how quickly their performance improves\nwith compute. We propose a novel matrix family containing Monarch matrices, the\nBlock Tensor-Train (BTT), which we show performs better than dense matrices for\nthe same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT\nachieves exponentially lower training loss than dense when training MLPs and\nViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less\ncompute and is more efficient than dense for training small GPT-2 language\nmodels.",
  "url": "https://arxiv.org/abs/2406.06248",
  "issue_number": 130,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/130",
  "created_at": "2024-12-27T10:14:34.797218",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-22T07:34:10.041Z",
  "main_tex_file": null,
  "published_date": "2024-06-10T13:25:43Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}