\begin{thebibliography}{277}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[nan(2024)]{nanotron}
Nanotron.
\newblock \url{https://github.com/huggingface/nanotron}, 2024.

\bibitem[Adelani et~al.(2023)Adelani, Liu, Shen, Vassilyev, Alabi, Mao, Gao, and Lee]{adelani2023sib}
David~Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba~O Alabi, Yanke Mao, Haonan Gao, and Annie En-Shiun Lee.
\newblock Sib-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects.
\newblock \emph{arXiv preprint arXiv:2309.07445}, 2023.

\bibitem[Aghajanyan et~al.(2023)Aghajanyan, Yu, Conneau, Hsu, Hambardzumyan, Zhang, Roller, Goyal, Levy, and Zettlemoyer]{aghajanyan2023scaling}
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer.
\newblock Scaling laws for generative mixed-modal language models.
\newblock In \emph{International Conference on Machine Learning}, pages 265--279. PMLR, 2023.

\bibitem[AI()]{stable-audio-tools}
Stability AI.
\newblock Stable audio tools.
\newblock Github Repo.
\newblock URL \url{https://github.com/Stability-AI/stable-audio-tools}.

\bibitem[Albalak et~al.(2022)Albalak, Tuan, Jandaghi, Pryor, Yoffe, Ramachandran, Getoor, Pujara, and Wang]{albalak-etal-2022-feta}
Alon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor, Jay Pujara, and William~Yang Wang.
\newblock {FETA}: A benchmark for few-sample task transfer in open-domain dialogue.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 10936--10953, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.751}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.751}.

\bibitem[Albalak et~al.(2023)Albalak, Pan, Raffel, and Wang]{albalak2023efficient}
Alon Albalak, Liangming Pan, Colin Raffel, and William~Yang Wang.
\newblock Efficient online data mixing for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2312.02406}, 2023.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang, Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock \emph{arXiv preprint arXiv:2402.16827}, 2024.

\bibitem[Alkemade et~al.(2023)Alkemade, Claeyssens, Colavizza, Freire, Lehmann, Neudeker, Osti, van Strien, et~al.]{alkemade2023datasheets}
Henk Alkemade, Steven Claeyssens, Giovanni Colavizza, Nuno Freire, J{\"o}rg Lehmann, Clemens Neudeker, Giulia Osti, Daniel van Strien, et~al.
\newblock Datasheets for digital cultural heritage datasets.
\newblock \emph{JOURNAL OF OPEN HUMANITIES DATA}, 9\penalty0 (17):\penalty0 1--11, 2023.

\bibitem[Alyafeai et~al.(2022)Alyafeai, Masoud, Ghaleb, and Al-shaibani]{alyafeai2022masader}
Zaid Alyafeai, Maraim Masoud, Mustafa Ghaleb, and Maged~S Al-shaibani.
\newblock Masader: Metadata sourcing for arabic text and speech data resources.
\newblock In \emph{Proceedings of the Thirteenth Language Resources and Evaluation Conference}, pages 6340--6351, 2022.

\bibitem[Andonian et~al.(2021)Andonian, Anthony, Biderman, Black, Gali, Gao, Hallahan, Levy-Kramer, Leahy, Nestler, Parker, Pieler, Purohit, Songz, Phil, and Weinbach]{gpt-neox-library}
Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach.
\newblock {GPT-NeoX}: Large scale autoregressive language modeling in {PyTorch}, 8 2021.
\newblock URL \url{https://www.github.com/eleutherai/gpt-neox}.

\bibitem[Anthony et~al.(2020)Anthony, Kanding, and Selvan]{anthony2020carbontracker}
Lasse F~Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan.
\newblock Carbontracker: Tracking and predicting the carbon footprint of training deep learning models.
\newblock \emph{arXiv preprint arXiv:2007.03051}, 2020.

\bibitem[Anthony et~al.(2023)Anthony, Biderman, and Schoelkopf]{transformer-math-eleutherai}
Quentin Anthony, Stella Biderman, and Hailey Schoelkopf.
\newblock Transformer math 101.
\newblock GitHub Repo, 2023.
\newblock URL \url{blog.eleuther.ai/}.

\bibitem[Anthony et~al.(2024)Anthony, Schoelkopf, and Biderman]{anthony2024cookbook}
Quentin Anthony, Hailey Schoelkopf, and Stella Biderman.
\newblock {The EleutherAI Model Training Cookbook}.
\newblock GitHub Repo, 2024.
\newblock URL \url{https://github.com/EleutherAI/cookbook}.

\bibitem[Anthropic(2023)]{Anthropic2023}
Anthropic.
\newblock Challenges in evaluating ai systems, October 2023.
\newblock URL \url{https://www.anthropic.com/news/evaluating-ai-systems}.

\bibitem[Anthropic(2024{\natexlab{a}})]{anthropic2024claude3}
Anthropic.
\newblock {The Claude 3 Model Family: Opus, Sonnet, Haiku}.
\newblock \url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf}, 3 2024{\natexlab{a}}.
\newblock {URL}: https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf.

\bibitem[Anthropic(2024{\natexlab{b}})]{claude3}
Anthropic.
\newblock The claude 3 model family: Opus, sonnet, haiku.
\newblock 2024{\natexlab{b}}.
\newblock URL \url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf}.

\bibitem[Ardila et~al.(2020)Ardila, Branson, Davis, Kohler, Meyer, Henretty, Morais, Saunders, Tyers, and Weber]{ardila-etal-2020-common}
Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber.
\newblock Common voice: A massively-multilingual speech corpus.
\newblock In Nicoletta Calzolari, Fr{\'e}d{\'e}ric B{\'e}chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H{\'e}l{\`e}ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, \emph{Proceedings of the Twelfth Language Resources and Evaluation Conference}, pages 4218--4222, Marseille, France, May 2020. European Language Resources Association.
\newblock ISBN 979-10-95546-34-4.
\newblock URL \url{https://aclanthology.org/2020.lrec-1.520}.

\bibitem[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu, Marathe, Bitton, Gadre, Sagawa, et~al.]{awadalla2023openflamingo}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et~al.
\newblock Openflamingo: An open-source framework for training large autoregressive vision-language models.
\newblock \emph{arXiv preprint arXiv:2308.01390}, 2023.

\bibitem[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Dos~Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos~Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
\newblock Llemma: An open language model for mathematics.
\newblock In \emph{The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23}, 2023.

\bibitem[Babu et~al.(2022)Babu, Wang, Tjandra, Lakhotia, Xu, Goyal, Singh, {von Platen}, Saraf, Pino, Baevski, Conneau, and Auli]{babu22-interspeech}
Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick {von Platen}, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli.
\newblock {XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale}.
\newblock In \emph{Proc. Interspeech 2022}, pages 2278--2282, 2022.
\newblock \doi{10.21437/Interspeech.2022-143}.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, and Kaplan]{bai-constitutional-2022}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional {AI}: {Harmlessness} from {AI} {Feedback}, December 2022.
\newblock URL \url{http://arxiv.org/abs/2212.08073}.
\newblock arXiv:2212.08073 [cs].

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1728--1738, 2021.

\bibitem[Bandy and Vincent(2021)]{bandy2021addressing}
Jack Bandy and Nicholas Vincent.
\newblock Addressing ``documentation debt'' in machine learning research: A retrospective datasheet for bookcorpus.
\newblock \emph{arXiv preprint arXiv:2105.05241}, 2021.

\bibitem[Banko et~al.(2020)Banko, MacKeen, and Ray]{banko-etal-2020-unified}
Michele Banko, Brendon MacKeen, and Laurie Ray.
\newblock A unified taxonomy of harmful content.
\newblock In Seyi Akiwowo, Bertie Vidgen, Vinodkumar Prabhakaran, and Zeerak Waseem, editors, \emph{Proceedings of the Fourth Workshop on Online Abuse and Harms}, pages 125--137, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.alw-1.16}.
\newblock URL \url{https://aclanthology.org/2020.alw-1.16}.

\bibitem[Beaumont(2022)]{beaumont-2022-clip-retrieval}
Romain Beaumont.
\newblock Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them.
\newblock \url{https://github.com/rom1504/clip-retrieval}, 2022.

\bibitem[Bekman()]{ml-engineering}
Stas Bekman.
\newblock Machine learning engineering open book.
\newblock GitHub Repo.
\newblock URL \url{https://github.com/stas00/ml-engineering}.

\bibitem[Bender and Friedman(2018)]{bender-friedman-2018-data}
Emily~M. Bender and Batya Friedman.
\newblock Data statements for natural language processing: Toward mitigating system bias and enabling better science.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 587--604, 2018.
\newblock \doi{10.1162/tacl\_a\_00041}.
\newblock URL \url{https://aclanthology.org/Q18-1041}.

\bibitem[Bhatt et~al.(2023)Bhatt, Chennabasappa, Nikolaidis, Wan, Evtimov, Gabi, Song, Ahmad, Aschermann, Fontana, et~al.]{bhatt2023purple}
Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, et~al.
\newblock Purple llama cyberseceval: A secure coding benchmark for language models.
\newblock \emph{arXiv preprint arXiv:2312.04724}, 2023.

\bibitem[Biderman et~al.(2022)Biderman, Bicheno, and Gao]{biderman2022datasheet}
Stella Biderman, Kieran Bicheno, and Leo Gao.
\newblock Datasheet for the pile.
\newblock \emph{arXiv preprint arXiv:2201.07311}, 2022.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages 2397--2430. PMLR, 2023.

\bibitem[Biderman et~al.(2024{\natexlab{a}})Biderman, PRASHANTH, Sutawika, Schoelkopf, Anthony, Purohit, and Raff]{biderman2024emergent}
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff.
\newblock Emergent and predictable memorization in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{a}}.

\bibitem[Biderman et~al.(2024{\natexlab{b}})Biderman, Schoelkopf, Sutawika, Gao, Tow, Abbasi, Aji, Ammanamanchi, Black, Clive, et~al.]{biderman2024lessons}
Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham~Fikri Aji, Pawan~Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et~al.
\newblock Lessons from the trenches on reproducible evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2405.14782}, 2024{\natexlab{b}}.

\bibitem[Birhane et~al.(2021)Birhane, Prabhu, and Kahembwe]{birhane2021multimodal}
Abeba Birhane, Vinay~Uday Prabhu, and Emmanuel Kahembwe.
\newblock Multimodal datasets: misogyny, pornography, and malignant stereotypes.
\newblock \emph{arXiv preprint arXiv:2110.01963}, 2021.

\bibitem[Birhane et~al.(2023{\natexlab{a}})Birhane, Prabhu, Han, and Boddeti]{birhane2023hate}
Abeba Birhane, Vinay Prabhu, Sang Han, and Vishnu~Naresh Boddeti.
\newblock On hate scaling laws for data-swamps.
\newblock \emph{arXiv preprint arXiv:2306.13141}, 2023{\natexlab{a}}.

\bibitem[Birhane et~al.(2023{\natexlab{b}})Birhane, Prabhu, Han, Boddeti, and Luccioni]{birhane2023into}
Abeba Birhane, Vinay Prabhu, Sang Han, Vishnu~Naresh Boddeti, and Alexandra~Sasha Luccioni.
\newblock Into the laions den: Investigating hate in multimodal datasets.
\newblock \emph{arXiv preprint arXiv:2311.03449}, 2023{\natexlab{b}}.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, et~al.]{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock \emph{arXiv preprint arXiv:2204.06745}, 2022.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023stable}
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Bommasani et~al.(2023{\natexlab{a}})Bommasani, Klyman, Longpre, Kapoor, Maslej, Xiong, Zhang, and Liang]{bommasani2023foundation}
Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang.
\newblock The foundation model transparency index, 2023{\natexlab{a}}.

\bibitem[Bommasani et~al.(2023{\natexlab{b}})Bommasani, Soylu, Liao, Creel, and Liang]{bommasani2023ecosystem}
Rishi Bommasani, Dilara Soylu, Thomas~I Liao, Kathleen~A Creel, and Percy Liang.
\newblock Ecosystem graphs: The social footprint of foundation models.
\newblock \emph{arXiv preprint arXiv:2303.15772}, 2023{\natexlab{b}}.

\bibitem[Bommasani et~al.(2024)Bommasani, Klyman, Longpre, Xiong, Kapoor, Maslej, Narayanan, and Liang]{bommasani2024foundation}
Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor Maslej, Arvind Narayanan, and Percy Liang.
\newblock Foundation model transparency reports.
\newblock \emph{arXiv preprint arXiv:2402.16268}, 2024.

\bibitem[Brooks et~al.(2024)Brooks, Peebles, Holmes, DePue, Guo, Jing, Schnurr, Taylor, Luhman, Luhman, Ng, Wang, and Ramesh]{sora2024}
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li~Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
\newblock Video generation models as world simulators.
\newblock 2024.
\newblock URL \url{https://openai.com/research/video-generation-models-as-world-simulators}.

\bibitem[Brown et~al.(2002)Brown, Webber, and Koomey]{brown2002status}
R~Brown, C~Webber, and JG~Koomey.
\newblock Status and future directions of the energy star program.
\newblock \emph{Energy}, 5\penalty0 (27):\penalty0 505--520, 2002.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{browngpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper\_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Brundage et~al.(2018)Brundage, Avin, Clark, Toner, Eckersley, Garfinkel, Dafoe, Scharre, Zeitzoff, Filar, Anderson, Roff, Allen, Steinhardt, Flynn, hÉigeartaigh, Beard, Belfield, Farquhar, Lyle, Crootof, Evans, Page, Bryson, Yampolskiy, and Amodei]{brundage2018malicious}
Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory~C. Allen, Jacob Steinhardt, Carrick Flynn, Seán~Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, and Dario Amodei.
\newblock The malicious use of artificial intelligence: Forecasting, prevention, and mitigation, 2018.

\bibitem[Bucknall and Dori-Hacohen(2022)]{10.1145/3514094.3534146}
Benjamin~S. Bucknall and Shiri Dori-Hacohen.
\newblock Current and near-term ai as a potential existential risk factor.
\newblock In \emph{Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society}, AIES '22, page 119–129, New York, NY, USA, 2022. Association for Computing Machinery.
\newblock ISBN 9781450392471.
\newblock \doi{10.1145/3514094.3534146}.
\newblock URL \url{https://doi.org/10.1145/3514094.3534146}.

\bibitem[Burchell et~al.(2023)Burchell, Birch, Bogoychev, and Heafield]{Burchell_2023}
Laurie Burchell, Alexandra Birch, Nikolay Bogoychev, and Kenneth Heafield.
\newblock An open dataset and model for language identification.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}. Association for Computational Linguistics, 2023.
\newblock \doi{10.18653/v1/2023.acl-short.75}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/2023.acl-short.75}.

\bibitem[Cahyawijaya et~al.()Cahyawijaya, Lovenia, Aji, Winata, Wilie, Koto, Mahendra, Wibisono, Romadhony, Vincentio, et~al.]{cahyawijayanusacrowd}
Samuel Cahyawijaya, Holy Lovenia, Alham~Fikri Aji, Genta~Indra Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, et~al.
\newblock Nusacrowd: Open source initiative for indonesian nlp resources.

\bibitem[Carlsmith(2022)]{carlsmith2022powerseeking}
Joseph Carlsmith.
\newblock Is power-seeking ai an existential risk?, 2022.

\bibitem[Chan et~al.(2023)Chan, Bradley, and Rajkumar]{chan2023reclaiming}
Alan Chan, Herbie Bradley, and Nitarshan Rajkumar.
\newblock Reclaiming the digital commons: A public data trust for training data.
\newblock In \emph{Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, AIES '23, page 855–868. Association for Computing Machinery, 2023.
\newblock \doi{10.1145/3600211.3604658}.
\newblock URL \url{https://doi.org/10.1145/3600211.3604658}.

\bibitem[Chang et~al.(2023)Chang, Wang, Wang, Wu, Yang, Zhu, Chen, Yi, Wang, Wang, et~al.]{chang2023survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology}, 2023.

\bibitem[Chen(2022)]{inference-arithmetic}
Carol Chen.
\newblock Transformer inference arithmetic.
\newblock \url{https://kipp.ly/blog/transformer-inference-arithmetic/}, 2022.

\bibitem[Chen et~al.(2021)Chen, Chai, Wang, Du, Zhang, Weng, Su, Povey, Trmal, Zhang, et~al.]{chen2021gigaspeech}
Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et~al.
\newblock Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio.
\newblock \emph{arXiv preprint arXiv:2106.06909}, 2021.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Contractor et~al.(2022)Contractor, McDuff, Haines, Lee, Hines, Hecht, Vincent, and Li]{contractor2022behavioral}
Danish Contractor, Daniel McDuff, Julia~Katherine Haines, Jenny Lee, Christopher Hines, Brent Hecht, Nicholas Vincent, and Hanlin Li.
\newblock Behavioral use licensing for responsible ai.
\newblock In \emph{Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 778--788, 2022.

\bibitem[David(2023)]{David2023AIDatasetCSAM}
Emilia David.
\newblock Ai image training dataset found to include child sexual abuse imagery.
\newblock \emph{The Verge}, December 2023.
\newblock URL \url{https://www.theverge.com/2023/12/20/24009418/generative-ai-image-laion-csam-google-stability-stanford}.
\newblock 7:57 AM PST.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{5206848}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255, 2009.
\newblock \doi{10.1109/CVPR.2009.5206848}.

\bibitem[Deng et~al.(2023)Deng, Cheng, Sun, Zhang, and Huang]{deng2023safer}
Jiawen Deng, Jiale Cheng, Hao Sun, Zhexin Zhang, and Minlie Huang.
\newblock Towards safer generative language models: A survey on safety risks, evaluations, and improvements, 2023.

\bibitem[Derczynski et~al.(2023)Derczynski, Kirk, Balachandran, Kumar, Tsvetkov, Leiser, and Mohammad]{derczynski2023assessing}
Leon Derczynski, Hannah~Rose Kirk, Vidhisha Balachandran, Sachin Kumar, Yulia Tsvetkov, M.~R. Leiser, and Saif Mohammad.
\newblock Assessing language model deployment with risk cards, 2023.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms, 2023.

\bibitem[Dev et~al.(2022)Dev, Sheng, Zhao, Amstutz, Sun, Hou, Sanseverino, Kim, Nishi, Peng, and Chang]{dev-etal-2022-measures}
Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu~Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, and Kai-Wei Chang.
\newblock On measures of biases and harms in {NLP}.
\newblock In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, \emph{Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022}, pages 246--267, Online only, November 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.findings-aacl.24}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dhamala et~al.(2021)Dhamala, Sun, Kumar, Krishna, Pruksachatkun, Chang, and Gupta]{dhamala2021bold}
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta.
\newblock Bold: Dataset and metrics for measuring biases in open-ended language generation.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness, accountability, and transparency}, pages 862--872, 2021.

\bibitem[Dingemanse and Liesenfeld(2022)]{dingemanse2022text}
Mark Dingemanse and Andreas Liesenfeld.
\newblock From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 5614--5633, 2022.

\bibitem[Dobbe(2022)]{dobbe2022system}
Roel Dobbe.
\newblock System safety and artificial intelligence.
\newblock In \emph{Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 1584--1584, 2022.

\bibitem[Dodge et~al.(2021)Dodge, Sap, Marasovi{\'c}, Agnew, Ilharco, Groeneveld, Mitchell, and Gardner]{dodge2021documenting}
Jesse Dodge, Maarten Sap, Ana Marasovi{\'c}, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
\newblock Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 1286--1305, 2021.

\bibitem[Don-Yehiya et~al.(2023)Don-Yehiya, Venezian, Raffel, Slonim, Katz, and Choshen]{donyehiya2023cold}
Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.
\newblock Cold fusion: Collaborative descent for distributed multitask finetuning, 2023.

\bibitem[Downing(2023)]{downing2023licensing}
Kate Downing.
\newblock Ai licensing can’t balance ``open'' with ``responsible''.
\newblock The Law Office of Kate Downing's Blog, 2023.
\newblock URL \url{https://katedowninglaw.com/2023/07/13/ai-licensing-cant-balance-open-with-responsible/}.

\bibitem[Dubois et~al.(2023)Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin, Liang, and Hashimoto]{dubois2023alpacafarm}
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Alpacafarm: A simulation framework for methods that learn from human feedback.
\newblock \emph{arXiv preprint arXiv:2305.14387}, 2023.

\bibitem[Elazar et~al.(2023{\natexlab{a}})Elazar, Bhagia, Magnusson, Ravichander, Schwenk, Suhr, Walsh, Groeneveld, Soldaini, Singh, Hajishirzi, Smith, and Dodge]{elazar2023whats}
Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah~A. Smith, and Jesse Dodge.
\newblock What's in my big data?, 2023{\natexlab{a}}.

\bibitem[Elazar et~al.(2023{\natexlab{b}})Elazar, Bhagia, Magnusson, Ravichander, Schwenk, Suhr, Walsh, Groeneveld, Soldaini, Singh, et~al.]{elazar2023s}
Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et~al.
\newblock What's in my big data?
\newblock \emph{arXiv preprint arXiv:2310.20707}, 2023{\natexlab{b}}.

\bibitem[Foundation(2024)]{FSF2024what}
The Free~Software Foundation.
\newblock What is free software?, 2024.
\newblock URL \url{https://web.archive.org/web/20230306010437/https://www.gnu.org/philosophy/free-sw.en.html}.
\newblock Last accessed on 2024-02-20.

\bibitem[Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Yang, Zheng, Li, Sun, et~al.]{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, et~al.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2306.13394}, 2023.

\bibitem[Gadre et~al.(2023)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Marten, Wortsman, Ghosh, Zhang, Orgad, Entezari, Daras, Pratt, Ramanujan, Bitton, Marathe, Mussmann, Vencu, Cherti, Krishna, Koh, Saukh, Ratner, Song, Hajishirzi, Farhadi, Beaumont, Oh, Dimakis, Jitsev, Carmon, Shankar, and Schmidt]{gadre2023datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang~Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.
\newblock Datacomp: In search of the next generation of multimodal datasets, 2023.

\bibitem[Gadre et~al.(2024{\natexlab{a}})Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Marten, Wortsman, Ghosh, Zhang, et~al.]{gadre2024datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{a}}.

\bibitem[Gadre et~al.(2024{\natexlab{b}})Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao, Mercat, Fang, Li, Keh, et~al.]{gadre2024language}
Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et~al.
\newblock Language models scale reliably with over-training and on downstream tasks.
\newblock \emph{arXiv preprint arXiv:2403.08540}, 2024{\natexlab{b}}.

\bibitem[Galvez et~al.(2021)Galvez, Diamos, Ciro, Cer{\'o}n, Achorn, Gopi, Kanter, Lam, Mazumder, and Reddi]{galvez2021people}
Daniel Galvez, Greg Diamos, Juan Ciro, Juan~Felipe Cer{\'o}n, Keith Achorn, Anjali Gopi, David Kanter, Maximilian Lam, Mark Mazumder, and Vijay~Janapa Reddi.
\newblock The people's speech: A large-scale diverse english speech recognition dataset for commercial usage.
\newblock \emph{arXiv preprint arXiv:2111.09344}, 2021.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gao et~al.(2023{\natexlab{a}})Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023{\natexlab{a}}.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Gao et~al.(2023{\natexlab{b}})Gao, Han, Zhang, Lin, Geng, Zhou, Zhang, Lu, He, Yue, et~al.]{gao2023llama}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et~al.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock \emph{arXiv preprint arXiv:2304.15010}, 2023{\natexlab{b}}.

\bibitem[Gebru et~al.(2021)Gebru, Morgenstern, Vecchione, Vaughan, Wallach, Iii, and Crawford]{gebru2021datasheets}
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer~Wortman Vaughan, Hanna Wallach, Hal~Daum{\'e} Iii, and Kate Crawford.
\newblock Datasheets for datasets.
\newblock \emph{Communications of the ACM}, 64\penalty0 (12):\penalty0 86--92, 2021.

\bibitem[Gehman et~al.(2020{\natexlab{a}})Gehman, Gururangan, Sap, Choi, and Smith]{gehman-etal-2020-realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
\newblock {R}eal{T}oxicity{P}rompts: Evaluating neural toxic degeneration in language models.
\newblock In Trevor Cohn, Yulan He, and Yang Liu, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pages 3356--3369, Online, November 2020{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.301}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.301}.

\bibitem[Gehman et~al.(2020{\natexlab{b}})Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pages 3356--3369, 2020{\natexlab{b}}.

\bibitem[Goddard et~al.(2024)Goddard, Siriwardhana, Ehghaghi, Meyers, Karpukhin, Benedict, McQuade, and Solawetz]{goddard2024arcees}
Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz.
\newblock Arcee's mergekit: A toolkit for merging large language models, 2024.

\bibitem[Gomez(2024)]{gomez2024commandrplus}
Aidan Gomez.
\newblock Introducing command r+: A scalable llm built for business.
\newblock \url{https://txt.cohere.com/command-r-plus-microsoft-azure/}, 4 2024.
\newblock {URL}: https://txt.cohere.com/command-r-plus-microsoft-azure/.

\bibitem[Grave et~al.(2018)Grave, Bojanowski, Gupta, Joulin, and Mikolov]{grave2018learning}
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.
\newblock Learning word vectors for 157 languages.
\newblock In \emph{Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)}, 2018.

\bibitem[Guha et~al.(2024)Guha, Nyarko, Ho, R{\'e}, Chilton, Chohlas-Wood, Peters, Waldon, Rockmore, Zambrano, et~al.]{guha2024legalbench}
Neel Guha, Julian Nyarko, Daniel Ho, Christopher R{\'e}, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et~al.
\newblock Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Gururangan et~al.(2023)Gururangan, Wortsman, Gadre, Dave, Kilian, Shi, Mercat, Smyrnis, Ilharco, Jordan, Heckel, Dimakis, Farhadi, Shankar, and Schmidt]{open_lm}
Suchin Gururangan, Mitchell Wortsman, Samir~Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt.
\newblock {open\_lm}: a minimal but performative language modeling (lm) repository, 2023.
\newblock URL \url{https://github.com/mlfoundations/open\_lm/}.
\newblock GitHub repository.

\bibitem[Hanu and Unitary(2020)]{Hanu_Detoxify_2020}
Laura Hanu and team Unitary.
\newblock {Detoxify}, November 2020.
\newblock URL \url{https://github.com/unitaryai/detoxify}.

\bibitem[Hartvigsen et~al.(2022)Hartvigsen, Gabriel, Palangi, Sap, Ray, and Kamar]{hartvigsen2022toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar.
\newblock Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection.
\newblock \emph{arXiv preprint arXiv:2203.09509}, 2022.

\bibitem[Havrilla et~al.(2023)Havrilla, Zhuravinskyi, Phung, Tiwari, Tow, Biderman, Anthony, and Castricato]{trlx-library}
Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman, Quentin Anthony, and Louis Castricato.
\newblock trl{X}: A framework for large scale reinforcement learning from human feedback.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 8578--8595, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.530}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.530}.

\bibitem[He et~al.(2024)He, Xia, and Henderson]{he2024s}
Luxi He, Mengzhou Xia, and Peter Henderson.
\newblock What's in your" safe" data?: Identifying benign data that breaks safety.
\newblock \emph{arXiv preprint arXiv:2404.01099}, 2024.

\bibitem[Henderson et~al.(2020)Henderson, Hu, Romoff, Brunskill, Jurafsky, and Pineau]{henderson2020towards}
Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau.
\newblock Towards the systematic reporting of the energy and carbon footprints of machine learning.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 10039--10081, 2020.

\bibitem[Henderson et~al.(2022)Henderson, Krass, Zheng, Guha, Manning, Jurafsky, and Ho]{henderson2022pile}
Peter Henderson, Mark Krass, Lucia Zheng, Neel Guha, Christopher~D Manning, Dan Jurafsky, and Daniel Ho.
\newblock Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 29217--29234, 2022.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hendrycks et~al.(2023)Hendrycks, Mazeika, and Woodside]{hendrycks2023overview}
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside.
\newblock An overview of catastrophic ai risks, 2023.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Holland et~al.(2020)Holland, Hosny, Newman, Joseph, and Chmielinski]{holland2020dataset}
Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski.
\newblock The dataset nutrition label.
\newblock \emph{Data Protection and Privacy}, 12\penalty0 (12):\penalty0 1, 2020.

\bibitem[Hosking et~al.(2024)Hosking, Blunsom, and Bartolo]{hosking2024human}
Tom Hosking, Phil Blunsom, and Max Bartolo.
\newblock Human feedback is not gold standard, 2024.

\bibitem[Hughes and Bae(2023)]{Hughes-Vectara-Hallucination-Leaderboard-2023}
Simon Hughes and Minseok Bae.
\newblock {Vectara Hallucination Leaderboard}, November 2023.
\newblock URL \url{https://github.com/vectara/hallucination-leaderboard}.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini, Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and Schmidt]{ilharco_gabriel_2021_5143773}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock Openclip, July 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5143773}.
\newblock If you use this software, please cite it as below.

\bibitem[Inan et~al.(2023)Inan, Upasani, Chi, Rungta, Iyer, Mao, Tontchev, Hu, Fuller, Testuggine, et~al.]{inan2023llama}
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et~al.
\newblock Llama guard: Llm-based input-output safeguard for human-ai conversations.
\newblock \emph{arXiv preprint arXiv:2312.06674}, 2023.

\bibitem[Initiative(2024)]{OSI2024def}
The Open~Source Initiative.
\newblock The open source definition, February 2024.
\newblock URL \url{https://opensource.org/osd/}.

\bibitem[Jagielski(2023)]{jagielski2023note}
Matthew Jagielski.
\newblock A note on interpreting canary exposure.
\newblock \emph{arXiv preprint arXiv:2306.00133}, 2023.

\bibitem[Javed et~al.(2023)Javed, Bhogale, Raman, Kumar, Kunchukuttan, and Khapra]{javed2023indicsuperb}
Tahir Javed, Kaushal Bhogale, Abhigyan Raman, Pratyush Kumar, Anoop Kunchukuttan, and Mitesh~M Khapra.
\newblock Indicsuperb: A speech processing universal performance benchmark for indian languages.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 12942--12950, 2023.

\bibitem[Jernite(2023)]{HuggingFaceCommunityBlog}
Yacine Jernite.
\newblock Training data transparency in ai: Tools, trends, and policy recommendations.
\newblock In \emph{Hugging Face Blog}, 2023.

\bibitem[Jernite et~al.(2022)Jernite, Nguyen, Biderman, Rogers, Masoud, Danchev, Tan, Luccioni, Subramani, Johnson, et~al.]{jernite2022data}
Yacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud, Valentin Danchev, Samson Tan, Alexandra~Sasha Luccioni, Nishant Subramani, Isaac Johnson, et~al.
\newblock Data governance in the age of large-scale data-driven language technology.
\newblock In \emph{Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 2206--2222, 2022.

\bibitem[Jimenez et~al.(2023)Jimenez, Yang, Wettig, Yao, Pei, Press, and Narasimhan]{jimenez2023swe}
Carlos~E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan.
\newblock Swe-bench: Can language models resolve real-world github issues?
\newblock \emph{arXiv preprint arXiv:2310.06770}, 2023.

\bibitem[Kahn et~al.(2020)Kahn, Rivi{\`e}re, Zheng, Kharitonov, Xu, Mazar{\'e}, Karadayi, Liptchinsky, Collobert, Fuegen, et~al.]{kahn2020libri}
Jacob Kahn, Morgane Rivi{\`e}re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar{\'e}, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et~al.
\newblock Libri-light: A benchmark for asr with limited or no supervision.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 7669--7673. IEEE, 2020.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kapoor et~al.(2023)Kapoor, Cantrell, Peng, Pham, Bail, Gundersen, Hofman, Hullman, Lones, Malik, Nanayakkara, Poldrack, Raji, Roberts, Salganik, Serra-Garcia, Stewart, Vandewiele, and Narayanan]{Kapoor2023REFORMSRS}
Sayash Kapoor, Emily~F. Cantrell, Kenny Peng, Thanh~Hien Pham, Christopher~A. Bail, Odd~Erik Gundersen, Jake~M. Hofman, Jessica~R. Hullman, Michael~A. Lones, Momin~M. Malik, Priyanka Nanayakkara, Russel~A. Poldrack, Inioluwa~Deborah Raji, Michael Roberts, Matthew~J. Salganik, Marta Serra-Garcia, Brandon~M Stewart, Gilles Vandewiele, and Arvind Narayanan.
\newblock Reforms: Reporting standards for machine learning based science.
\newblock \emph{ArXiv}, abs/2308.07832, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.07832}.

\bibitem[Kapoor et~al.(2024)Kapoor, Bommasani, Klyman, Longpre, Ramaswami, Cihon, Hopkins, Bankston, Biderman, Bogen, et~al.]{kapoor2024societal}
Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, et~al.
\newblock On the societal impact of open foundation models.
\newblock \emph{arXiv preprint arXiv:2403.07918}, 2024.

\bibitem[Karamcheti et~al.(2024)Karamcheti, Nair, Balakrishna, Liang, Kollar, and Sadigh]{karamcheti2024prismatic}
Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh.
\newblock Prismatic vlms: Investigating the design space of visually-conditioned language models.
\newblock \emph{arXiv preprint arXiv:2402.07865}, 2024.

\bibitem[Kargaran et~al.(2023)Kargaran, Imani, Yvon, and Schütze]{kargaran2023glotlid}
Amir~Hossein Kargaran, Ayyoob Imani, François Yvon, and Hinrich Schütze.
\newblock Glotlid: Language identification for low-resource languages, 2023.

\bibitem[Karpathy(2023)]{nanogpt}
Andrej Karpathy.
\newblock nanogpt.
\newblock GitHub Repo, 2023.
\newblock URL \url{https://github.com/karpathy/nanoGPT}.

\bibitem[Karpov et~al.(2021)Karpov, Denisenko, and Minkin]{karpov2021golos}
Nikolay Karpov, Alexander Denisenko, and Fedor Minkin.
\newblock Golos: Russian dataset for speech research.
\newblock \emph{arXiv preprint arXiv:2106.10161}, 2021.

\bibitem[Kiela et~al.(2021)Kiela, Bartolo, Nie, Kaushik, Geiger, Wu, Vidgen, Prasad, Singh, Ringshia, et~al.]{kiela2021dynabench}
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et~al.
\newblock Dynabench: Rethinking benchmarking in nlp.
\newblock \emph{arXiv preprint arXiv:2104.14337}, 2021.

\bibitem[Kirchenbauer et~al.(2023)Kirchenbauer, Geiping, Wen, Katz, Miers, and Goldstein]{kirchenbauer2023watermark}
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.
\newblock A watermark for large language models.
\newblock \emph{arXiv preprint arXiv:2301.10226}, 2023.

\bibitem[Kl{\"o}pffer(1997)]{klopffer1997life}
Walter Kl{\"o}pffer.
\newblock Life cycle assessment: From the beginning to the current state.
\newblock \emph{Environmental Science and Pollution Research}, 4:\penalty0 223--228, 1997.

\bibitem[Klyman(2024)]{klyman2024aups-for-fms}
Kevin Klyman.
\newblock Acceptable use policies for foundation models: Considerations for policymakers and developers.
\newblock Stanford Center for Research on Foundation Models, apr 2024.
\newblock URL \url{https://crfm.stanford.edu/2024/04/08/aups.html}.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, Jia, Mou, Jernite, Mitchell, Ferrandis, Hughes, Wolf, Bahdanau, et~al.]{kocetkov2022stack}
Denis Kocetkov, Raymond Li, LI~Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos~Mu{\~n}oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et~al.
\newblock The stack: 3 tb of permissively licensed source code.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Korinek and Vipra(2023)]{korinek2023market}
Anton Korinek and Jai Vipra.
\newblock Market concentration implications of foundation models: The invisible hand of chatgpt.
\newblock 2023.

\bibitem[Kreutzer et~al.(2022)Kreutzer, Caswell, Wang, Wahab, van Esch, Ulzii-Orshikh, Tapo, Subramani, Sokolov, Sikasote, et~al.]{kreutzer2022quality}
Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et~al.
\newblock Quality at a glance: An audit of web-crawled multilingual datasets.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 50--72, 2022.

\bibitem[Kudugunta et~al.(2023)Kudugunta, Caswell, Zhang, Garcia, Xin, Kusupati, Stella, Bapna, and Firat]{kudugunta2023madlad}
Sneha Kudugunta, Isaac~Rayburn Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat.
\newblock Madlad-400: A multilingual and document-level large audited dataset.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.

\bibitem[Lacoste et~al.(2019)Lacoste, Luccioni, Schmidt, and Dandres]{lacoste2019quantifying}
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.
\newblock Quantifying the carbon emissions of machine learning.
\newblock \emph{arXiv preprint arXiv:1910.09700}, 2019.

\bibitem[Laippala et~al.(2022)Laippala, Salmela, R{\"o}nnqvist, Aji, Chang, Dhifallah, Goulart, Kortelainen, P{\`a}mies, Dutra, et~al.]{laippala2022towards}
Veronika Laippala, Anna Salmela, Samuel R{\"o}nnqvist, Alham~Fikri Aji, Li-Hsin Chang, Asma Dhifallah, Larissa Goulart, Henna Kortelainen, Marc P{\`a}mies, Deise~Prina Dutra, et~al.
\newblock Towards better structured and less noisy web data: Oscar with register annotations.
\newblock In \emph{Proceedings of the Eighth Workshop on Noisy User-generated Text (W-NUT 2022)}, pages 215--221, 2022.

\bibitem[Lakatos(2023{\natexlab{a}})]{lakatos2023revealing}
S~Lakatos.
\newblock A revealing picture: Ai-generated ‘undressing’images move from niche pornography discussion forums to a scaled and monetized online business.
\newblock Technical report, Technical report, Graphika, Dec 2023. URL https://public-assets. graphika~…, 2023{\natexlab{a}}.

\bibitem[Lakatos(2023{\natexlab{b}})]{lakatos-revealing-2023}
Santiago Lakatos.
\newblock A {Revealing} {Picture}: {AI}-{Generated} ‘{Undressing}’ {Images} {Move} from {Niche} {Pornography} {Discussion} {Forums} to a {Scaled} and {Monetized} {Online} {Business}.
\newblock Technical report, December 2023{\natexlab{b}}.
\newblock URL \url{https://public-assets.graphika.com/reports/graphika-report-a-revealing-picture.pdf}.

\bibitem[Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu, Dziri, Kumar, Zick, Choi, et~al.]{lambert2024rewardbench}
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ~Miranda, Bill~Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et~al.
\newblock Rewardbench: Evaluating reward models for language modeling.
\newblock \emph{arXiv preprint arXiv:2403.13787}, 2024.

\bibitem[Lauren\c{c}on et~al.(2022)Lauren\c{c}on, Saulnier, Wang, Akiki, Villanova~del Moral, Le~Scao, Von~Werra, Mou, Gonz\'{a}lez~Ponferrada, Nguyen, Frohberg, \v{S}a\v{s}ko, Lhoest, McMillan-Major, Dupont, Biderman, Rogers, Ben~allal, De~Toni, Pistilli, Nguyen, Nikpoor, Masoud, Colombo, de~la Rosa, Villegas, Thrush, Longpre, Nagel, Weber, Mu\~{n}oz, Zhu, Van~Strien, Alyafeai, Almubarak, Vu, Gonzalez-Dios, Soroa, Lo, Dey, Ortiz~Suarez, Gokaslan, Bose, Adelani, Phan, Tran, Yu, Pai, Chim, Lepercq, Ilic, Mitchell, Luccioni, and Jernite]{NEURIPS2022_ce9e92e3}
Hugo Lauren\c{c}on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova~del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou, Eduardo Gonz\'{a}lez~Ponferrada, Huu Nguyen, J\"{o}rg Frohberg, Mario \v{S}a\v{s}ko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Loubna Ben~allal, Francesco De~Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de~la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Mu\~{n}oz, Jian Zhu, Daniel Van~Strien, Zaid Alyafeai, Khalid Almubarak, Minh~Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz~Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha~Alexandra Luccioni, and Yacine Jernite.
\newblock The bigscience roots corpus: A 1.6tb composite multilingual dataset.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}, volume~35, pages 31809--31826. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper\_files/paper/2022/file/ce9e92e3de2372a4b93353eb7f3dc0bd-Paper-Datasets\_and\_Benchmarks.pdf}.

\bibitem[Lauren{\c{c}}on et~al.(2022)Lauren{\c{c}}on, Saulnier, Wang, Akiki, Villanova~del Moral, Le~Scao, Von~Werra, Mou, Gonz{\'a}lez~Ponferrada, Nguyen, et~al.]{laurenccon2022bigscience}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova~del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou, Eduardo Gonz{\'a}lez~Ponferrada, Huu Nguyen, et~al.
\newblock The bigscience roots corpus: A 1.6 tb composite multilingual dataset.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 31809--31826, 2022.

\bibitem[Lauren{\c{c}}on et~al.(2023)Lauren{\c{c}}on, Saulnier, Tronchon, Bekman, Singh, Lozhkov, Wang, Karamcheti, Rush, Kiela, et~al.]{laurenccon2023obelisc}
Hugo Lauren{\c{c}}on, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander~M Rush, Douwe Kiela, et~al.
\newblock Obelisc: An open web-scale filtered dataset of interleaved image-text documents.
\newblock \emph{arXiv preprint arXiv:2306.16527}, 2023.

\bibitem[Lauren{\c{c}}on et~al.(2024{\natexlab{a}})Lauren{\c{c}}on, Saulnier, Tronchon, Bekman, Singh, Lozhkov, Wang, Karamcheti, Rush, Kiela, et~al.]{laurenccon2024obelics}
Hugo Lauren{\c{c}}on, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et~al.
\newblock Obelics: An open web-scale filtered dataset of interleaved image-text documents.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{a}}.

\bibitem[Lauren{\c{c}}on et~al.(2024{\natexlab{b}})Lauren{\c{c}}on, Tronchon, Cord, and Sanh]{laurenccon2024matters}
Hugo Lauren{\c{c}}on, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh.
\newblock What matters when building vision-language models?
\newblock \emph{arXiv preprint arXiv:2405.02246}, 2024{\natexlab{b}}.

\bibitem[Le~Ferrand et~al.(2022)Le~Ferrand, Bird, and Besacier]{le2022learning}
{\'E}ric Le~Ferrand, Steven Bird, and Laurent Besacier.
\newblock Learning from failure: Data capture in an australian aboriginal community.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 4988--4998, 2022.

\bibitem[Lee et~al.(2022{\natexlab{a}})Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch, and Carlini]{lee-etal-2022-deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8424--8445, Dublin, Ireland, May 2022{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.577}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.577}.

\bibitem[Lee et~al.(2023{\natexlab{a}})Lee, Srivastava, Hardy, Thickstun, Durmus, Paranjape, Gerard-Ursin, Li, Ladhak, Rong, et~al.]{lee2023evaluating}
Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang~Lisa Li, Faisal Ladhak, Frieda Rong, et~al.
\newblock Evaluating human-language model interaction.
\newblock \emph{Transactions on Machine Learning Research}, 2023{\natexlab{a}}.

\bibitem[Lee et~al.(2022{\natexlab{b}})Lee, Ping, Xu, Patwary, Fung, Shoeybi, and Catanzaro]{lee2022factuality}
Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale~N Fung, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Factuality enhanced language models for open-ended text generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 34586--34599, 2022{\natexlab{b}}.

\bibitem[Lee et~al.(2023{\natexlab{b}})Lee, Yasunaga, Meng, Mai, Park, Gupta, Zhang, Narayanan, Teufel, Bellagente, et~al.]{lee2023holistic}
Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon~Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah~Benita Teufel, Marco Bellagente, et~al.
\newblock Holistic evaluation of text-to-image models.
\newblock \emph{arXiv preprint arXiv:2311.04287}, 2023{\natexlab{b}}.

\bibitem[Lees et~al.(2022)Lees, Tran, Tay, Sorensen, Gupta, Metzler, and Vasserman]{lees2022new}
Alyssa Lees, Vinh~Q Tran, Yi~Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman.
\newblock A new generation of perspective api: Efficient multilingual character-level transformers.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages 3197--3207, 2022.

\bibitem[Lhoest et~al.(2021{\natexlab{a}})Lhoest, del Moral, Jernite, Thakur, von Platen, Patil, Chaumond, Drame, Plu, Tunstall, et~al.]{lhoest2021datasets}
Quentin Lhoest, Albert~Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et~al.
\newblock Datasets: A community library for natural language processing.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 175--184, 2021{\natexlab{a}}.

\bibitem[Lhoest et~al.(2021{\natexlab{b}})Lhoest, Villanova~del Moral, Jernite, Thakur, von Platen, Patil, Chaumond, Drame, Plu, Tunstall, Davison, {\v{S}}a{\v{s}}ko, Chhablani, Malik, Brandeis, Le~Scao, Sanh, Xu, Patry, McMillan-Major, Schmid, Gugger, Delangue, Matussi{\`e}re, Debut, Bekman, Cistac, Goehringer, Mustar, Lagunas, Rush, and Wolf]{lhoest-etal-2021-datasets}
Quentin Lhoest, Albert Villanova~del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario {\v{S}}a{\v{s}}ko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le~Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl{\'e}ment Delangue, Th{\'e}o Matussi{\`e}re, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran{\c{c}}ois Lagunas, Alexander Rush, and Thomas Wolf.
\newblock Datasets: A community library for natural language processing.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 175--184, Online and Punta Cana, Dominican Republic, November 2021{\natexlab{b}}. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2021.emnlp-demo.21}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Yang, Zhang, Pu, and Liu]{li2023otterhd}
Bo~Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu.
\newblock Otterhd: A high-resolution multi-modality model.
\newblock \emph{arXiv preprint arXiv:2311.04219}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhang, Chen, Wang, Yang, and Liu]{li2023otter}
Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
\newblock Otter: A multi-modal model with in-context instruction tuning.
\newblock \emph{arXiv preprint arXiv:2305.03726}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Yin, Li, Chen, Wang, Ren, Li, Yang, Xu, Sun, et~al.]{li2023m}
Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu~Sun, et~al.
\newblock M3it: A large-scale dataset towards multi-modal multilingual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2306.04387}, 2023{\natexlab{c}}.

\bibitem[Li et~al.(2023{\natexlab{d}})Li, Yang, Islam, and Ren]{li2023making}
Pengfei Li, Jianyi Yang, Mohammad~A Islam, and Shaolei Ren.
\newblock Making ai less" thirsty": Uncovering and addressing the secret water footprint of ai models.
\newblock \emph{arXiv preprint arXiv:2304.03271}, 2023{\natexlab{d}}.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2211.09110}, 2022.

\bibitem[Liang et~al.(2023)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar, Newman, Yuan, Yan, Zhang, Cosgrove, Manning, Ré, Acosta-Navas, Hudson, Zelikman, Durmus, Ladhak, Rong, Ren, Yao, Wang, Santhanam, Orr, Zheng, Yuksekgonul, Suzgun, Kim, Guha, Chatterji, Khattab, Henderson, Huang, Chi, Xie, Santurkar, Ganguli, Hashimoto, Icard, Zhang, Chaudhary, Wang, Li, Mai, Zhang, and Koreeda]{liang2023holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce~Zhang, Christian Cosgrove, Christopher~D. Manning, Christopher Ré, Diana Acosta-Navas, Drew~A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang~Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.
\newblock Holistic evaluation of language models, 2023.

\bibitem[Liao and Xiao(2023)]{liao2023rethinking}
Q~Vera Liao and Ziang Xiao.
\newblock Rethinking model evaluation as narrowing the socio-technical gap.
\newblock \emph{arXiv preprint arXiv:2306.03100}, 2023.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{liu2023improved}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2310.03744}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Li, Wu, and Lee]{liu2024visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Min, Zettlemoyer, Choi, and Hajishirzi]{Liu2024InfiniGram}
Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, and Hannaneh Hajishirzi.
\newblock Infini-gram: Scaling unbounded n-gram language models to a trillion tokens.
\newblock \emph{arXiv preprint arXiv:2401.17377}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Wei, Liu, Si, Zhang, Rao, Zheng, Peng, Yang, Zhou, et~al.]{liu2024best}
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et~al.
\newblock Best practices and lessons learned on synthetic data for language models.
\newblock \emph{arXiv preprint arXiv:2404.07503}, 2024{\natexlab{c}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu, et~al.]{liu2023mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock \emph{arXiv preprint arXiv:2307.06281}, 2023{\natexlab{b}}.

\bibitem[Lo et~al.(2020)Lo, Wang, Neumann, Kinney, and Weld]{lo2020s2orc}
Kyle Lo, Lucy~Lu Wang, Mark Neumann, Rodney Kinney, and Daniel~S Weld.
\newblock S2orc: The semantic scholar open research corpus.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 4969--4983, 2020.

\bibitem[Longpre et~al.(2022)Longpre, Storm, and Shah]{longpre2022lethal}
Shayne Longpre, Marcus Storm, and Rishi Shah.
\newblock Lethal autonomous weapons systems \& artificial intelligence: Trends, challenges, and policies.
\newblock \emph{MIT Science Policy Review}, 3\penalty0 (1):\penalty0 47--56, 2022.

\bibitem[Longpre et~al.(2023{\natexlab{a}})Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, et~al.]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}, 2023{\natexlab{a}}.

\bibitem[Longpre et~al.(2023{\natexlab{b}})Longpre, Mahari, Chen, Obeng-Marnu, Sileo, Brannon, Muennighoff, Khazam, Kabbara, Perisetla, et~al.]{longpre2023data}
Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et~al.
\newblock The data provenance initiative: A large scale audit of dataset licensing \& attribution in ai.
\newblock \emph{arXiv preprint arXiv:2310.16787}, 2023{\natexlab{b}}.

\bibitem[Longpre et~al.(2024{\natexlab{a}})Longpre, Kapoor, Klyman, Ramaswami, Bommasani, Blili-Hamelin, Huang, Skowron, Yong, Kotha, et~al.]{longpre2024safe}
Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane Blili-Hamelin, Yangsibo Huang, Aviya Skowron, Zheng-Xin Yong, Suhas Kotha, et~al.
\newblock A safe harbor for ai evaluation and red teaming.
\newblock \emph{arXiv preprint arXiv:2403.04893}, 2024{\natexlab{a}}.

\bibitem[Longpre et~al.(2024{\natexlab{b}})Longpre, Mahari, Obeng-Marnu, Brannon, South, Gero, Pentland, and Kabbara]{longpre2024data}
Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, Katy Gero, Sandy Pentland, and Jad Kabbara.
\newblock Data authenticity, consent, \& provenance for ai are all broken: what will it take to fix them?
\newblock \emph{arXiv preprint arXiv:2404.12691}, 2024{\natexlab{b}}.

\bibitem[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, et~al.]{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al.
\newblock Starcoder 2 and the stack v2: The next generation.
\newblock \emph{arXiv preprint arXiv:2402.19173}, 2024.

\bibitem[Luccioni et~al.(2023{\natexlab{a}})Luccioni, Akiki, Mitchell, and Jernite]{luccioni2023stable}
Alexandra~Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite.
\newblock Stable bias: Analyzing societal representations in diffusion models.
\newblock \emph{arXiv preprint arXiv:2303.11408}, 2023{\natexlab{a}}.

\bibitem[Luccioni et~al.(2023{\natexlab{b}})Luccioni, Viguier, and Ligozat]{luccioni2023estimating}
Alexandra~Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.
\newblock Estimating the carbon footprint of bloom, a 176b parameter language model.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (253), 2023{\natexlab{b}}.

\bibitem[Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, Paul, and Bossan]{peft}
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}, 2022.

\bibitem[Marone and Van~Durme(2023)]{marone2023data}
Marc Marone and Benjamin Van~Durme.
\newblock Data portraits: Recording foundation model training data.
\newblock \emph{arXiv preprint arXiv:2303.03919}, 2023.

\bibitem[Matena and Raffel(2022)]{matena2022merging}
Michael Matena and Colin Raffel.
\newblock Merging models with fisher-weighted averaging, 2022.

\bibitem[Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li, Forsyth, and Hendrycks]{mazeika2024harmbench}
Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, David Forsyth, and Dan Hendrycks.
\newblock Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024.

\bibitem[Mazumder et~al.(2023)Mazumder, Banbury, Yao, Karla{\v{s}}, Rojas, Diamos, Diamos, He, Parrish, Kirk, Quaye, Rastogi, Kiela, Jurado, Kanter, Mosquera, Cukierski, Ciro, Aroyo, Acun, Chen, Raje, Bartolo, Eyuboglu, Ghorbani, Goodman, Howard, Inel, Kane, Kirkpatrick, Sculley, Kuo, Mueller, Thrush, Vanschoren, Warren, Williams, Yeung, Ardalani, Paritosh, Zhang, Zou, Wu, Coleman, Ng, Mattson, and Reddi]{mazumder2023dataperf}
Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karla{\v{s}}, William A~Gaviria Rojas, Sudnya Diamos, Greg Diamos, Lynn He, Alicia Parrish, Hannah~Rose Kirk, Jessica Quaye, Charvi Rastogi, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera, Will Cukierski, Juan Ciro, Lora Aroyo, Bilge Acun, Lingjiao Chen, Mehul~Smriti Raje, Max Bartolo, Sabri Eyuboglu, Amirata Ghorbani, Emmett~Daniel Goodman, Addison Howard, Oana Inel, Tariq Kane, Christine Kirkpatrick, D.~Sculley, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin Vanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen Paritosh, Ce~Zhang, James~Y. Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson, and Vijay~Janapa Reddi.
\newblock Dataperf: Benchmarks for data-centric {AI} development.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.
\newblock URL \url{https://openreview.net/forum?id=LaFKTgrZMG}.

\bibitem[McDuff et~al.(2024)McDuff, Korjakow, Cambo, Benjamin, Lee, Jernite, Ferrandis, Gokaslan, Tarkowski, Lindley, et~al.]{mcduff2024standardization}
Daniel McDuff, Tim Korjakow, Scott Cambo, Jesse~Josua Benjamin, Jenny Lee, Yacine Jernite, Carlos~Mu{\~n}oz Ferrandis, Aaron Gokaslan, Alek Tarkowski, Joseph Lindley, et~al.
\newblock On the standardization of behavioral use clauses and their adoption for responsible licensing of ai.
\newblock \emph{arXiv preprint arXiv:2402.05979}, 2024.

\bibitem[McElheran et~al.(2024)McElheran, Li, Brynjolfsson, Kroff, Dinlersoz, Foster, and Zolas]{mcelheran2024ai}
Kristina McElheran, J~Frank Li, Erik Brynjolfsson, Zachary Kroff, Emin Dinlersoz, Lucia Foster, and Nikolas Zolas.
\newblock Ai adoption in america: Who, what, and where.
\newblock \emph{Journal of Economics \& Management Strategy}, 2024.

\bibitem[McKinzie et~al.(2024)McKinzie, Gan, Fauconnier, Dodge, Zhang, Dufter, Shah, Du, Peng, Weers, et~al.]{mckinzie2024mm1}
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et~al.
\newblock Mm1: Methods, analysis \& insights from multimodal llm pre-training.
\newblock \emph{arXiv preprint arXiv:2403.09611}, 2024.

\bibitem[McMillan-Major et~al.(2022)McMillan-Major, Alyafeai, Biderman, Chen, De~Toni, Dupont, Elsahar, Emezue, Aji, Ili{\'c}, et~al.]{mcmillan2022documenting}
Angelina McMillan-Major, Zaid Alyafeai, Stella Biderman, Kimbo Chen, Francesco De~Toni, G{\'e}rard Dupont, Hady Elsahar, Chris Emezue, Alham~Fikri Aji, Suzana Ili{\'c}, et~al.
\newblock Documenting geographically and contextually diverse data sources: The bigscience catalogue of language data and resources.
\newblock \emph{arXiv preprint arXiv:2201.10066}, 2022.

\bibitem[Min et~al.(2023)Min, Gururangan, Wallace, Shi, Hajishirzi, Smith, and Zettlemoyer]{min2023SILOLM}
Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah~A Smith, and Luke Zettlemoyer.
\newblock Silo language models: Isolating legal risk in a nonparametric datastore.
\newblock In \emph{NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models}, 2023.

\bibitem[Mitchell et~al.(2019)Mitchell, Wu, Zaldivar, Barnes, Vasserman, Hutchinson, Spitzer, Raji, and Gebru]{mitchell2019model}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting.
\newblock In \emph{Proceedings of the conference on fairness, accountability, and transparency}, pages 220--229, 2019.

\bibitem[Muennighoff et~al.(2023{\natexlab{a}})Muennighoff, Liu, Zebaze, Zheng, Hui, Zhuo, Singh, Tang, von Werra, and Longpre]{muennighoff2023octopack}
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry~Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre.
\newblock Octopack: Instruction tuning code large language models.
\newblock \emph{arXiv preprint arXiv:2308.07124}, 2023{\natexlab{a}}.

\bibitem[Muennighoff et~al.(2023{\natexlab{b}})Muennighoff, Rush, Barak, Scao, Piktus, Tazi, Pyysalo, Wolf, and Raffel]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
\newblock Scaling data-constrained language models.
\newblock \emph{arXiv preprint arXiv:2305.16264}, 2023{\natexlab{b}}.

\bibitem[Nagrani et~al.(2017)Nagrani, Chung, and Zisserman]{nagrani17-interspeech}
Arsha Nagrani, Joon~Son Chung, and Andrew Zisserman.
\newblock {VoxCeleb: A Large-Scale Speaker Identification Dataset}.
\newblock In \emph{Proc. Interspeech 2017}, pages 2616--2620, 2017.
\newblock \doi{10.21437/Interspeech.2017-950}.

\bibitem[Narayanan and Kapoor(2024)]{NarayananKapoor2024}
Arvind Narayanan and Sayash Kapoor.
\newblock Ai safety is not a model property.
\newblock \url{https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property}, March 2024.
\newblock Accessed: YYYY-MM-DD.

\bibitem[Nguyen et~al.(2023)Nguyen, Van~Nguyen, Lai, Man, Ngo, Dernoncourt, Rossi, and Nguyen]{nguyen2023culturax}
Thuat Nguyen, Chien Van~Nguyen, Viet~Dac Lai, Hieu Man, Nghia~Trung Ngo, Franck Dernoncourt, Ryan~A Rossi, and Thien~Huu Nguyen.
\newblock Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages.
\newblock \emph{arXiv preprint arXiv:2309.09400}, 2023.

\bibitem[Niklaus et~al.(2023)Niklaus, Matoshi, St{\"u}rmer, Chalkidis, and Ho]{niklaus2023multilegalpile}
Joel Niklaus, Veton Matoshi, Matthias St{\"u}rmer, Ilias Chalkidis, and Daniel~E Ho.
\newblock Multilegalpile: A 689gb multilingual legal corpus.
\newblock \emph{arXiv preprint arXiv:2306.02069}, 2023.

\bibitem[Niu et~al.(2024)Niu, Ren, Gao, Hua, and Jin]{niu2024jailbreaking}
Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin.
\newblock Jailbreaking attack against multimodal large language model.
\newblock \emph{arXiv preprint arXiv:2402.02309}, 2024.

\bibitem[Oladipo et~al.(2023)Oladipo, Adeyemi, Ahia, Owodunni, Ogundepo, Adelani, and Lin]{oladipo-etal-2023-better}
Akintunde Oladipo, Mofetoluwa Adeyemi, Orevaoghene Ahia, Abraham Owodunni, Odunayo Ogundepo, David Adelani, and Jimmy Lin.
\newblock Better quality pre-training data and t5 models for {A}frican languages.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 158--168, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.11}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.11}.

\bibitem[OpenAccess-AI-Collective()]{axolotl}
OpenAccess-AI-Collective.
\newblock Axolotl.
\newblock GitHub Repo.
\newblock URL \url{https://github.com/OpenAccess-AI-Collective/axolotl}.

\bibitem[OpenAI(2023)]{openai2023preparedness}
OpenAI.
\newblock Preparednessframework(beta), 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.02155}.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and Khudanpur]{panayotov2015librispeech}
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In \emph{2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)}, pages 5206--5210. IEEE, 2015.

\bibitem[Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman]{parrish2021bbq}
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel~R Bowman.
\newblock Bbq: A hand-built bias benchmark for question answering.
\newblock \emph{arXiv preprint arXiv:2110.08193}, 2021.

\bibitem[Parrish et~al.(2022)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman]{parrish2022bbq}
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel Bowman.
\newblock Bbq: A hand-built bias benchmark for question answering.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pages 2086--2105, 2022.

\bibitem[Paster et~al.(2023)Paster, Santos, Azerbayev, and Ba]{paster2023openwebmath}
Keiran Paster, Marco~Dos Santos, Zhangir Azerbayev, and Jimmy Ba.
\newblock Openwebmath: An open dataset of high-quality mathematical web text.
\newblock \emph{arXiv preprint arXiv:2310.06786}, 2023.

\bibitem[Patterson et~al.(2021)Patterson, Gonzalez, Le, Liang, Munguia, Rothchild, So, Texier, and Dean]{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock \emph{arXiv preprint arXiv:2104.10350}, 2021.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock \emph{arXiv preprint arXiv:2306.01116}, 2023.

\bibitem[Penedo et~al.(2024)Penedo, Cappelli, Wolf, and Sasko]{penedo2024datatrove}
Guilherme Penedo, Alessandro Cappelli, Thomas Wolf, and Mario Sasko.
\newblock Datatrove: large scale data processing, 2024.
\newblock URL \url{https://github.com/huggingface/datatrove}.

\bibitem[Peng et~al.(2023)Peng, Wang, Dong, Hao, Huang, Ma, and Wei]{peng2023kosmos2}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world, 2023.

\bibitem[Piktus et~al.(2023)Piktus, Akiki, Villegas, Lauren{\c{c}}on, Dupont, Luccioni, Jernite, and Rogers]{piktus2023roots}
Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Lauren{\c{c}}on, G{\'e}rard Dupont, Alexandra~Sasha Luccioni, Yacine Jernite, and Anna Rogers.
\newblock The roots search tool: Data transparency for llms.
\newblock \emph{arXiv preprint arXiv:2302.14035}, 2023.

\bibitem[Pratap et~al.(2020)Pratap, Xu, Sriram, Synnaeve, and Collobert]{pratap20-interspeech}
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert.
\newblock {MLS: A Large-Scale Multilingual Dataset for Speech Research}.
\newblock In \emph{Proc. Interspeech 2020}, pages 2757--2761, 2020.
\newblock \doi{10.21437/Interspeech.2020-2826}.

\bibitem[Pushkarna et~al.(2022)Pushkarna, Zaldivar, and Kjartansson]{pushkarna2022data}
Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson.
\newblock Data cards: Purposeful and transparent dataset documentation for responsible ai.
\newblock In \emph{2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 1776--1826, 2022.

\bibitem[Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2023fine}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.
\newblock Fine-tuning aligned language models compromises safety, even when users do not intend to!
\newblock \emph{arXiv preprint arXiv:2310.03693}, 2023.

\bibitem[Qi et~al.(2024)Qi, Huang, Panda, Henderson, Wang, and Mittal]{qi2024visual}
Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal.
\newblock Visual adversarial examples jailbreak aligned large language models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 21527--21536, 2024.

\bibitem[Radford et~al.(2023)Radford, Kim, Xu, Brockman, McLeavey, and Sutskever]{radford2023robust}
Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In \emph{International Conference on Machine Learning}, pages 28492--28518. PMLR, 2023.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Raji and Dobbe(2023)]{raji2023concrete}
Inioluwa~Deborah Raji and Roel Dobbe.
\newblock Concrete problems in ai safety, revisited.
\newblock \emph{arXiv preprint arXiv:2401.10899}, 2023.

\bibitem[Ravanelli et~al.(2021)Ravanelli, Parcollet, Plantinga, Rouhe, Cornell, Lugosch, Subakan, Dawalatabad, Heba, Zhong, Chou, Yeh, Fu, Liao, Rastorgueva, Grondin, Aris, Na, Gao, Mori, and Bengio]{ravanelli2021speechbrain}
Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, François Grondin, William Aris, Hwidong Na, Yan Gao, Renato~De Mori, and Yoshua Bengio.
\newblock Speechbrain: A general-purpose speech toolkit, 2021.

\bibitem[Rebedea et~al.(2023)Rebedea, Dinu, Sreedhar, Parisien, and Cohen]{rebedea2023nemo}
Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen.
\newblock Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails.
\newblock \emph{arXiv preprint arXiv:2310.10501}, 2023.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115:\penalty0 211--252, 2015.

\bibitem[Röttger et~al.(2024)Röttger, Kirk, Vidgen, Attanasio, Bianchi, and Hovy]{röttger2024xstest}
Paul Röttger, Hannah~Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy.
\newblock Xstest: A test suite for identifying exaggerated safety behaviours in large language models, 2024.

\bibitem[Saberi et~al.(2023)Saberi, Sadasivan, Rezaei, Kumar, Chegini, Wang, and Feizi]{saberi2023robustness}
Mehrdad Saberi, Vinu~Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, and Soheil Feizi.
\newblock Robustness of ai-image detectors: Fundamental limits and practical attacks.
\newblock \emph{arXiv preprint arXiv:2310.00076}, 2023.

\bibitem[Sambasivan et~al.(2021)Sambasivan, Kapania, Highfill, Akrong, Paritosh, and Aroyo]{sambasivan2021everyone}
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora~M Aroyo.
\newblock ``{E}veryone wants to do the model work, not the data work'': Data cascades in high-stakes {AI}.
\newblock In \emph{CHI}, CHI '21, New York, NY, USA, 2021. Association for Computing Machinery.
\newblock ISBN 9781450380966.
\newblock \doi{10.1145/3411764.3445518}.
\newblock URL \url{https://doi.org/10.1145/3411764.3445518}.

\bibitem[Sanabria et~al.(2023)Sanabria, Bogoychev, Markl, Carmantini, Klejch, and Bell]{sanabria2023edinburgh}
Ramon Sanabria, Nikolay Bogoychev, Nina Markl, Andrea Carmantini, Ondrej Klejch, and Peter Bell.
\newblock The edinburgh international accents of english corpus: Towards the democratization of english asr.
\newblock In \emph{ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 1--5. IEEE, 2023.

\bibitem[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2021multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{ICLR 2022}, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.08207}.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Schaeffer et~al.(2024)Schaeffer, Miranda, and Koyejo]{schaeffer2024emergent}
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.
\newblock Are emergent abilities of large language models a mirage?
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Schmidt et~al.(2021)Schmidt, Goyal, Joshi, Feld, Conell, Laskaris, Blank, Wilson, Friedler, and Luccioni]{schmidt2021codecarbon}
Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni.
\newblock Codecarbon: estimate and track carbon emissions from machine learning computing.
\newblock \emph{Cited on}, page~20, 2021.

\bibitem[Schreiber et~al.(2020)Schreiber, Bilmes, and Noble]{JMLR:v21:19-467}
Jacob Schreiber, Jeffrey Bilmes, and William~Stafford Noble.
\newblock apricot: Submodular selection for data summarization in python.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (161):\penalty0 1--6, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/19-467.html}.

\bibitem[Schuhmann et~al.(2022{\natexlab{a}})Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, Schramowski, Kundurthy, Crowson, Schmidt, Kaczmarczyk, and Jitsev]{schuhmann2022laion5b}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models, 2022{\natexlab{a}}.

\bibitem[Schuhmann et~al.(2022{\natexlab{b}})Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25278--25294, 2022{\natexlab{b}}.

\bibitem[Schwartz et~al.(2020)Schwartz, Dodge, Smith, and Etzioni]{schwartz2020green}
Roy Schwartz, Jesse Dodge, Noah~A Smith, and Oren Etzioni.
\newblock Green ai.
\newblock \emph{Communications of the ACM}, 63\penalty0 (12):\penalty0 54--63, 2020.

\bibitem[Shayegani et~al.(2023)Shayegani, Dong, and Abu-Ghazaleh]{shayegani2023jailbreak}
Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh.
\newblock Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Shelby et~al.(2023)Shelby, Rismani, Henne, Moon, Rostamzadeh, Nicholas, Yilla, Gallegos, Smart, Garcia, and Virk]{shelby2023sociotechnical}
Renee Shelby, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar Rostamzadeh, Paul Nicholas, N'Mah Yilla, Jess Gallegos, Andrew Smart, Emilio Garcia, and Gurleen Virk.
\newblock Sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction, 2023.

\bibitem[Shevlane et~al.(2023)Shevlane, Farquhar, Garfinkel, Phuong, Whittlestone, Leung, Kokotajlo, Marchal, Anderljung, Kolt, Ho, Siddarth, Avin, Hawkins, Kim, Gabriel, Bolina, Clark, Bengio, Christiano, and Dafoe]{shevlane2023model}
Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, and Allan Dafoe.
\newblock Model evaluation for extreme risks, 2023.

\bibitem[Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer]{shi2023detecting}
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer.
\newblock Detecting pretraining data from large language models.
\newblock \emph{arXiv preprint arXiv:2310.16789}, 2023.

\bibitem[Shoeybi et~al.(2020)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2020megatronlm}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.

\bibitem[Simmons-Edler et~al.(2024)Simmons-Edler, Badman, Longpre, and Rajan]{simmons2024ai}
Riley Simmons-Edler, Ryan Badman, Shayne Longpre, and Kanaka Rajan.
\newblock Ai-powered autonomous weapons risk geopolitical instability and threaten ai research.
\newblock \emph{arXiv preprint arXiv:2405.01859}, 2024.

\bibitem[Singh et~al.(2024)Singh, Vargus, Dsouza, Karlsson, Mahendiran, Ko, Shandilya, Patel, Mataciunas, OMahony, et~al.]{singh2024aya}
Shivalika Singh, Freddie Vargus, Daniel Dsouza, B{\"o}rje~F Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et~al.
\newblock Aya dataset: An open-access collection for multilingual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2402.06619}, 2024.

\bibitem[Smith et~al.(2022{\natexlab{a}})Smith, Hall, Kambadur, Presani, and Williams]{smith2022m}
Eric~Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams.
\newblock “i’m sorry to hear that”: Finding new biases in language models with a holistic descriptor dataset.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 9180--9211, 2022{\natexlab{a}}.

\bibitem[Smith et~al.(2022{\natexlab{b}})Smith, Patwary, Norick, LeGresley, Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi, Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro]{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza~Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022{\natexlab{b}}.

\bibitem[Soldaini et~al.(2023)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, Hofmann, Jha, Kumar, Lucy, Lyu, Magnusson, Morrison, Muennighoff, Naik, Nam, Peters, Ravichander, Richardson, Shen, Strubell, Subramani, Tafjord, Walsh, Hajishirzi, Smith, Zettlemoyer, Beltagy, Groeneveld, Dodge, and Lo]{dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya~Harsh Jha, Sachin Kumar, Li~Lucy, Xinxi Lyu, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan~Pete Walsh, Hannaneh Hajishirzi, Noah~A. Smith, Luke Zettlemoyer, Iz~Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.
\newblock {Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}.
\newblock \emph{Allen Institute for AI, Tech. Rep}, 2023.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: an open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Srivastava et~al.(2023{\natexlab{a}})Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2023beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{Transactions on Machine Learning Research}, 2023{\natexlab{a}}.

\bibitem[Srivastava et~al.(2023{\natexlab{b}})Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, Kluska, Lewkowycz, Agarwal, Power, Ray, Warstadt, Kocurek, Safaya, Tazarv, Xiang, Parrish, Nie, Hussain, Askell, Dsouza, Slone, Rahane, Iyer, Andreassen, Madotto, Santilli, Stuhlmüller, Dai, La, Lampinen, Zou, Jiang, Chen, Vuong, Gupta, Gottardi, Norelli, Venkatesh, Gholamidavoodi, Tabassum, Menezes, Kirubarajan, Mullokandov, Sabharwal, Herrick, Efrat, Erdem, Karakaş, Roberts, Loe, Zoph, Bojanowski, Özyurt, Hedayatnia, Neyshabur, Inden, Stein, Ekmekci, Lin, Howald, Orinion, Diao, Dour, Stinson, Argueta, Ramírez, Singh, Rathkopf, Meng, Baral, Wu, Callison-Burch, Waites, Voigt, Manning, Potts, Ramirez, Rivera, Siro, Raffel, Ashcraft, Garbacea, Sileo, Garrette, Hendrycks, Kilman, Roth, Freeman, Khashabi, Levy, González, Perszyk, Hernandez, Chen, Ippolito, Gilboa, Dohan, Drakard, Jurgens, Datta, Ganguli, Emelin, Kleyko, Yuret, Chen, Tam, Hupkes, Misra, Buzan, Mollo, Yang, Lee,
  Schrader, Shutova, Cubuk, Segal, Hagerman, Barnes, Donoway, Pavlick, Rodola, Lam, Chu, Tang, Erdem, Chang, Chi, Dyer, Jerzak, Kim, Manyasi, Zheltonozhskii, Xia, Siar, Martínez-Plumed, Happé, Chollet, Rong, Mishra, Winata, de~Melo, Kruszewski, Parascandolo, Mariani, Wang, Jaimovitch-López, Betz, Gur-Ari, Galijasevic, Kim, Rashkin, Hajishirzi, Mehta, Bogar, Shevlin, Schütze, Yakura, Zhang, Wong, Ng, Noble, Jumelet, Geissinger, Kernion, Hilton, Lee, Fisac, Simon, Koppel, Zheng, Zou, Kocoń, Thompson, Wingfield, Kaplan, Radom, Sohl-Dickstein, Phang, Wei, Yosinski, Novikova, Bosscher, Marsh, Kim, Taal, Engel, Alabi, Xu, Song, Tang, Waweru, Burden, Miller, Balis, Batchelder, Berant, Frohberg, Rozen, Hernandez-Orallo, Boudeman, Guerr, Jones, Tenenbaum, Rule, Chua, Kanclerz, Livescu, Krauth, Gopalakrishnan, Ignatyeva, Markert, Dhole, Gimpel, Omondi, Mathewson, Chiafullo, Shkaruta, Shridhar, McDonell, Richardson, Reynolds, Gao, Zhang, Dugan, Qin, Contreras-Ochando, Morency, Moschella, Lam, Noble, Schmidt, He,
  Colón, Metz, Şenel, Bosma, Sap, ter Hoeve, Farooqi, Faruqui, Mazeika, Baturan, Marelli, Maru, Quintana, Tolkiehn, Giulianelli, Lewis, Potthast, Leavitt, Hagen, Schubert, Baitemirova, Arnaud, McElrath, Yee, Cohen, Gu, Ivanitskiy, Starritt, Strube, Swędrowski, Bevilacqua, Yasunaga, Kale, Cain, Xu, Suzgun, Walker, Tiwari, Bansal, Aminnaseri, Geva, Gheini, T, Peng, Chi, Lee, Krakover, Cameron, Roberts, Doiron, Martinez, Nangia, Deckers, Muennighoff, Keskar, Iyer, Constant, Fiedel, Wen, Zhang, Agha, Elbaghdadi, Levy, Evans, Casares, Doshi, Fung, Liang, Vicol, Alipoormolabashi, Liao, Liang, Chang, Eckersley, Htut, Hwang, Miłkowski, Patil, Pezeshkpour, Oli, Mei, Lyu, Chen, Banjade, Rudolph, Gabriel, Habacker, Risco, Millière, Garg, Barnes, Saurous, Arakawa, Raymaekers, Frank, Sikand, Novak, Sitelew, LeBras, Liu, Jacobs, Zhang, Salakhutdinov, Chi, Lee, Stovall, Teehan, Yang, Singh, Mohammad, Anand, Dillavou, Shleifer, Wiseman, Gruetter, Bowman, Schoenholz, Han, Kwatra, Rous, Ghazarian, Ghosh, Casey, Bischoff,
  Gehrmann, Schuster, Sadeghi, Hamdan, Zhou, Srivastava, Shi, Singh, Asaadi, Gu, Pachchigar, Toshniwal, Upadhyay, Shyamolima, Debnath, Shakeri, Thormeyer, Melzi, Reddy, Makini, Lee, Torene, Hatwar, Dehaene, Divic, Ermon, Biderman, Lin, Prasad, Piantadosi, Shieber, Misherghi, Kiritchenko, Mishra, Linzen, Schuster, Li, Yu, Ali, Hashimoto, Wu, Desbordes, Rothschild, Phan, Wang, Nkinyili, Schick, Kornev, Tunduny, Gerstenberg, Chang, Neeraj, Khot, Shultz, Shaham, Misra, Demberg, Nyamai, Raunak, Ramasesh, Prabhu, Padmakumar, Srikumar, Fedus, Saunders, Zhang, Vossen, Ren, Tong, Zhao, Wu, Shen, Yaghoobzadeh, Lakretz, Song, Bahri, Choi, Yang, Hao, Chen, Belinkov, Hou, Hou, Bai, Seid, Zhao, Wang, Wang, Wang, and Wu]{srivastava2023imitation}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander~W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman~S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B.~Ryan Roberts, Bao~Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill~Yuchen Lin, Blake Howald, Bryan
  Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César~Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher~D. Manning, Christopher Potts, Cindy Ramirez, Clara~E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel~Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri~Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin~Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang,
  Ethan~A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice~Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta~Indra Winata, Gerard de~Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh~Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime~Fernández Fisac, James~B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan
  Waweru, John Burden, John Miller, John~U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua~B. Tenenbaum, Joshua~S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh~D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li~Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis~Oliveros Colón, Luke Metz, Lütfi~Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose~Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew~L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina~Orduna
  Baitemirova, Melody Arnaud, Melvin McElrath, Michael~A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo~Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund~Varma T, Nanyun Peng, Nathan~A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish~Shirish Keskar, Niveditha~S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio~Moreno Casares, Parth Doshi, Pascale Fung, Paul~Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu~Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel~Etta Rudolph, Raefer Gabriel, Rahel
  Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif~A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif~M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel~R. Bowman, Samuel~S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah~A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang~Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha~Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen
  Prasad, Steven~T. Piantadosi, Stuart~M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay~Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu~Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie~J. Wang, Zirui Wang, and Ziyi Wu.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023{\natexlab{b}}.

\bibitem[Stoica et~al.(2024)Stoica, Bolya, Bjorner, Ramesh, Hearn, and Hoffman]{stoica2024zipit}
George Stoica, Daniel Bolya, Jakob Bjorner, Pratik Ramesh, Taylor Hearn, and Judy Hoffman.
\newblock Zipit! merging models from different tasks without training, 2024.

\bibitem[Su{\'a}rez et~al.(2019)Su{\'a}rez, Sagot, and Romary]{suarez2019asynchronous}
Pedro Javier~Ortiz Su{\'a}rez, Beno{\^\i}t Sagot, and Laurent Romary.
\newblock Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures.
\newblock In \emph{7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)}. Leibniz-Institut f{\"u}r Deutsche Sprache, 2019.

\bibitem[Sun et~al.(2024)Sun, Huang, Wang, Wu, Zhang, Li, Gao, Huang, Lyu, Zhang, Li, Liu, Liu, Wang, Zhang, Vidgen, Kailkhura, Xiong, Xiao, Li, Xing, Huang, Liu, Ji, Wang, Zhang, Yao, Kellis, Zitnik, Jiang, Bansal, Zou, Pei, Liu, Gao, Han, Zhao, Tang, Wang, Vanschoren, Mitchell, Shu, Xu, Chang, He, Huang, Backes, Gong, Yu, Chen, Gu, Xu, Ying, Ji, Jana, Chen, Liu, Zhou, Wang, Li, Zhang, Wang, Xie, Chen, Wang, Liu, Ye, Cao, Chen, and Zhao]{sun2024trustllm}
Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil~Zhenqiang Gong, Philip~S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao.
\newblock Trustllm: Trustworthiness in large language models, 2024.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Thapliyal et~al.(2022)Thapliyal, Pont-Tuset, Chen, and Soricut]{thapliyal2022crossmodal}
Ashish~V Thapliyal, Jordi Pont-Tuset, Xi~Chen, and Radu Soricut.
\newblock Crossmodal-3600: A massively multilingual multimodal evaluation dataset.
\newblock \emph{arXiv preprint arXiv:2205.12522}, 2022.

\bibitem[Thiel et~al.(2023{\natexlab{a}})Thiel, Stroebel, and Portnoff]{thiel-generative-2023}
David Thiel, Melissa Stroebel, and Rebecca Portnoff.
\newblock Generative {ML} and {CSAM}: {Implications} and {Mitigations}.
\newblock 2023{\natexlab{a}}.
\newblock \doi{10.25740/jv206yg3793}.
\newblock URL \url{https://purl.stanford.edu/jv206yg3793}.

\bibitem[Thiel et~al.(2023{\natexlab{b}})Thiel, Stroebel, and Portnoff]{thiel2023generative}
David Thiel, Melissa Stroebel, and Rebecca Portnoff.
\newblock Generative ml and csam: Implications and mitigations, 2023{\natexlab{b}}.

\bibitem[Tiedemann(2012)]{tiedemann2012parallel}
J{\"o}rg Tiedemann.
\newblock Parallel data, tools and interfaces in opus.
\newblock In \emph{Lrec}, volume 2012, pages 2214--2218. Citeseer, 2012.

\bibitem[{Together AI}(2023)]{together_ai_2023_redpajama}
{Together AI}.
\newblock Redpajama-data-v2: An open dataset with 30 trillion tokens for training large language models.
\newblock Blog post on Together AI, Oct 2023.
\newblock URL \url{https://www.together.ai/blog/redpajama-data-v2}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[{\"U}st{\"u}n et~al.(2024){\"U}st{\"u}n, Aryabumi, Yong, Ko, D'souza, Onilude, Bhandari, Singh, Ooi, Kayid, et~al.]{ustun2024aya}
Ahmet {\"U}st{\"u}n, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et~al.
\newblock Aya model: An instruction finetuned open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2402.07827}, 2024.

\bibitem[Valk and Alum{\"a}e(2021)]{valk2021voxlingua107}
J{\"o}rgen Valk and Tanel Alum{\"a}e.
\newblock Voxlingua107: a dataset for spoken language recognition.
\newblock In \emph{2021 IEEE Spoken Language Technology Workshop (SLT)}, pages 652--658. IEEE, 2021.

\bibitem[Vidgen et~al.(2019)Vidgen, Harris, Nguyen, Tromble, Hale, and Margetts]{vidgen-etal-2019-challenges}
Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts.
\newblock Challenges and frontiers in abusive content detection.
\newblock In Sarah~T. Roberts, Joel Tetreault, Vinodkumar Prabhakaran, and Zeerak Waseem, editors, \emph{Proceedings of the Third Workshop on Abusive Language Online}, pages 80--93, Florence, Italy, August 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-3509}.
\newblock URL \url{https://aclanthology.org/W19-3509}.

\bibitem[Vidgen et~al.(2023)Vidgen, Kirk, Qian, Scherrer, Kannappan, Hale, and R{\"o}ttger]{vidgen2023simplesafetytests}
Bertie Vidgen, Hannah~Rose Kirk, Rebecca Qian, Nino Scherrer, Anand Kannappan, Scott~A Hale, and Paul R{\"o}ttger.
\newblock Simplesafetytests: a test suite for identifying critical safety risks in large language models.
\newblock \emph{arXiv preprint arXiv:2311.08370}, 2023.

\bibitem[Vipra and Korinek(2023)]{vipra2023concentration}
Jai Vipra and Anton Korinek.
\newblock Market concentration implications of foundation models: The invisible hand of chatgpt.
\newblock \emph{The Brookings Institution}, 2023.
\newblock URL \url{https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt}.

\bibitem[Wang et~al.(2024)Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, Truong, Arora, Mazeika, Hendrycks, Lin, Cheng, Koyejo, Song, and Li]{wang2024decodingtrust}
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang~T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu~Cheng, Sanmi Koyejo, Dawn Song, and Bo~Li.
\newblock Decodingtrust: A comprehensive assessment of trustworthiness in gpt models, 2024.

\bibitem[Wang et~al.(2021)Wang, Riviere, Lee, Wu, Talnikar, Haziza, Williamson, Pino, and Dupoux]{wang2021voxpopuli}
Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux.
\newblock Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation.
\newblock \emph{arXiv preprint arXiv:2101.00390}, 2021.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Warstadt et~al.(2023)Warstadt, Mueller, Choshen, Wilcox, Zhuang, Ciro, Mosquera, Paranjabe, Williams, Linzen, and Cotterell]{warstadt-etal-2023-findings}
Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell.
\newblock Findings of the {B}aby{LM} challenge: Sample-efficient pretraining on developmentally plausible corpora.
\newblock In Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell, editors, \emph{Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning}, pages 1--34, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.conll-babylm.1}.
\newblock URL \url{https://aclanthology.org/2023.conll-babylm.1}.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang, Cheng, Glaese, Balle, Kasirzadeh, Kenton, Brown, Hawkins, Stepleton, Biles, Birhane, Haas, Rimell, Hendricks, Isaac, Legassick, Irving, and Gabriel]{weidinger2021ethical}
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa~Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel.
\newblock Ethical and social risks of harm from language models, 2021.

\bibitem[Weidinger et~al.(2022)Weidinger, Uesato, Rauh, Griffin, Huang, Mellor, Glaese, Cheng, Balle, Kasirzadeh, Biles, Brown, Kenton, Hawkins, Stepleton, Birhane, Hendricks, Rimell, Isaac, Haas, Legassick, Irving, and Gabriel]{10.1145/3531146.3533088}
Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa~Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel.
\newblock Taxonomy of risks posed by language models.
\newblock In \emph{Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, FAccT '22, page 214–229, New York, NY, USA, 2022. Association for Computing Machinery.
\newblock ISBN 9781450393522.
\newblock \doi{10.1145/3531146.3533088}.
\newblock URL \url{https://doi.org/10.1145/3531146.3533088}.

\bibitem[Weidinger et~al.(2023)Weidinger, Rauh, Marchal, Manzini, Hendricks, Mateos-Garcia, Bergman, Kay, Griffin, Bariach, Gabriel, Rieser, and Isaac]{weidinger2023sociotechnical}
Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa~Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, and William Isaac.
\newblock Sociotechnical safety evaluation of generative ai systems, 2023.

\bibitem[Wenzek et~al.(2020)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave]{wenzek-etal-2020-ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock {CCN}et: Extracting high quality monolingual datasets from web crawl data.
\newblock In Nicoletta Calzolari, Fr{\'e}d{\'e}ric B{\'e}chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H{\'e}l{\`e}ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, \emph{Proceedings of the Twelfth Language Resources and Evaluation Conference}, pages 4003--4012, Marseille, France, May 2020. European Language Resources Association.
\newblock ISBN 979-10-95546-34-4.
\newblock URL \url{https://aclanthology.org/2020.lrec-1.494}.

\bibitem[Wightman(2019)]{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Woisetschl{\"a}ger et~al.(2024)Woisetschl{\"a}ger, Isenko, Wang, Mayer, and Jacobsen]{woisetschlager2024survey}
Herbert Woisetschl{\"a}ger, Alexander Isenko, Shiqiang Wang, Ruben Mayer, and Hans-Arno Jacobsen.
\newblock A survey on efficient federated learning methods for foundation model training.
\newblock \emph{arXiv preprint arXiv:2401.04472}, 2024.

\bibitem[Wu and Aji(2023)]{wu2023style}
Minghao Wu and Alham~Fikri Aji.
\newblock Style over substance: Evaluation biases for large language models.
\newblock \emph{arXiv preprint arXiv:2307.03025}, 2023.

\bibitem[Xie et~al.(2023{\natexlab{a}})Xie, Pham, Dong, Du, Liu, Lu, Liang, Le, Ma, and Yu]{xie2023doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc~V. Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model pretraining.
\newblock \emph{arXiv preprint arXiv:2305.10429}, 2023{\natexlab{a}}.

\bibitem[Xie et~al.(2023{\natexlab{b}})Xie, Santurkar, Ma, and Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2023)Xu, Song, Iyyer, and Choi]{xu-etal-2023-critical}
Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi.
\newblock A critical evaluation of evaluations for long-form question answering.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 3225--3245, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.181}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.181}.

\bibitem[Xu et~al.(2024)Xu, Wang, Ma, Koh, Xiao, and Chen]{xu2024instructional}
Jiashu Xu, Fei Wang, Mingyu~Derek Ma, Pang~Wei Koh, Chaowei Xiao, and Muhao Chen.
\newblock Instructional fingerprinting of large language models.
\newblock \emph{arXiv preprint arXiv:2401.12255}, 2024.

\bibitem[Yadav et~al.(2023)Yadav, Tam, Choshen, Raffel, and Bansal]{yadav2023tiesmerging}
Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal.
\newblock Ties-merging: Resolving interference when merging models, 2023.

\bibitem[Yong et~al.(2023)Yong, Menghini, and Bach]{yong2023low}
Zheng~Xin Yong, Cristina Menghini, and Stephen Bach.
\newblock Low-resource languages jailbreak gpt-4.
\newblock In \emph{Socially Responsible Language Modelling Research}, 2023.

\bibitem[Yu et~al.(2023)Yu, Kaur, Gupta, Brown-Cohen, Goyal, and Arora]{yu2023skill}
Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora.
\newblock Skill-mix: a flexible and expandable family of evaluations for ai models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, et~al.]{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock \emph{arXiv preprint arXiv:2311.16502}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Lv, Guo, Shao, Yang, Xie, Xu, Bu, Chen, Zeng, et~al.]{zhang2022wenetspeech}
Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et~al.
\newblock Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 6182--6186. IEEE, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Han, Liu, Gao, Zhou, Hu, Yan, Lu, Li, and Qiao]{zhang2023llamaadapter}
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu~Qiao.
\newblock Llama-adapter: Efficient fine-tuning of language models with zero-init attention, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Lei, Wu, Sun, Huang, Long, Liu, Lei, Tang, and Huang]{zhang2023safetybench}
Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang.
\newblock Safetybench: Evaluating the safety of large language models with multiple choice questions, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2023{\natexlab{a}})Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023{\natexlab{a}}.

\bibitem[Zhao et~al.(2023{\natexlab{b}})Zhao, Ren, Hessel, Cardie, Choi, and Deng]{zhao2023inthe}
Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng.
\newblock (inthe) wildchat: 570k chatgpt interaction logs in the wild.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Li, Li, Yu, Liu, Wang, Zhang, Ji, Yan, He, et~al.]{zhou2023comprehensive}
Ce~Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et~al.
\newblock A comprehensive survey on pretrained foundation models: A history from bert to chatgpt.
\newblock \emph{arXiv preprint arXiv:2302.09419}, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Chen, Hong, Chen, Yu, Zhang, Wang, Zhang, and Zheng]{zhou2024training}
Jiahang Zhou, Yanyu Chen, Zicong Hong, Wuhui Chen, Yue Yu, Tao Zhang, Hui Wang, Chuanfu Zhang, and Zibin Zheng.
\newblock Training and serving system of foundation models: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2401.02643}, 2024.

\bibitem[Zhu et~al.(2023)Zhu, Hessel, Awadalla, Gadre, Dodge, Fang, Yu, Schmidt, Wang, and Choi]{zhu2023multimodal}
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir~Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William~Yang Wang, and Yejin Choi.
\newblock Multimodal c4: An open, billion-scale corpus of images interleaved with text.
\newblock \emph{arXiv preprint arXiv:2304.06939}, 2023.

\bibitem[Zhu et~al.(2024)Zhu, Hessel, Awadalla, Gadre, Dodge, Fang, Yu, Schmidt, Wang, and Choi]{zhu2024multimodal}
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir~Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William~Yang Wang, and Yejin Choi.
\newblock Multimodal c4: An open, billion-scale corpus of images interleaved with text.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Żelasko et~al.(2021)Żelasko, Povey, Trmal, and Khudanpur]{lhotse}
Piotr Żelasko, Daniel Povey, Jan~"Yenda" Trmal, and Sanjeev Khudanpur.
\newblock Lhotse: a speech data representation library for the modern deep learning ecosystem, 2021.

\end{thebibliography}
