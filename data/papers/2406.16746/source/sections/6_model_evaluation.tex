\section{Model Evaluation}
\label{sec:model-eval}
\vspace{-2mm}

% https://www.latex4technics.com/?note=MOB
% \begin{tcolorbox}[width=\textwidth,title={Data Preparation Best Practices}]
\begin{tcolorbox}[
    width=\textwidth,
    title={Model Evaluation Best Practices},
    colback=backgroundcol, % Background color of the box
    colframe=darkgray, % Frame color
    colbacktitle=dataprep, % Background color of the title
    coltitle=white, % Title text color
    coltext=black % Text color
]

\begin{itemize}[itemsep=0pt, wide=3pt]
    \item Model evaluation is an essential component of machine learning research. However many machine learning papers use evaluations that are \textbf{not reproducible or comparable to other work}.
    \item One of the biggest causes of irreproducibility is failure to report prompts and other essential components of evaluation protocols. This would not be a problem if researchers released evaluation code and exact prompts, but many prominent labs (OpenAI, Anthropic, Meta) have not done so for model releases. When using evaluation results from a paper that does not release its evaluation code, \textbf{reproduce the evaluations using a public codebase}.
    \item Examples of high-quality documentation practices for model evaluations can be found in \citet{browngpt3} (for bespoke evaluations) and \citet{black2022gpt,scao2022bloom,biderman2023pythia} (for evaluation using a public codebase).
    \item Expect a released model to be used in unexpected ways. Accordingly, try to evaluate the model on benchmarks that are most related to its prescribed use case, but also its failure modes or potential misuses.
    \item All evaluations come with limitations. Be careful to assess and communicate these limitations when reporting results, to avoid overconfidence in model capabilities.
    % \item \citet{touvron2023llama} and \citet{touvron2023llama2} show examples of evaluation practices to be avoided at all costs. They use evaluation protocols that are so irreproducible that two papers released less than six months apart with the same lab and same lead author fail to produce the same results when evaluating the same model.
\end{itemize}
\end{tcolorbox}

\subsection{Capabilities}
\vspace{-2mm}


% \input{tables/general-capability-eval-1}
% \vspace{-2mm}

%\TODO{Kevin/Sayash: expand this to a full survey of the capabilities eval tools and resources in the fmcheatsheet.org. How do they compare and contrast? [2-3 paragraphs]}
Many modern foundation models are released with general conversational abilities, such that their use cases are poorly specified and open-ended.
This poses significant challenges to evaluation benchmarks that are unable to critically evaluate so many tasks, applications, and risks fairly or systematically.
As a result, it is important to carefully scope the original intentions for the model, and to tie evaluations to those intentions.
Even then, the most relevant evaluation benchmarks may not align with real use, and so developers should qualify their results, and carefully supplement them with data from real user/human evaluation settings where feasible.

For language models, common capabilities benchmarks include those that evaluate  models on narrow tasks such as software engineering \citep{jimenez2023swe}, topic classification \citep{adelani2023sib}, and explaining code \citep{muennighoff2023octopack}.  
More comprehensive evaluation suites, such as the Language Model Evaluation Harness ~\citep{eval-harness} and HELM \citep{liang2023holistic}, are also common. 
Leaderboards like LMSys Chatbot Arena \citep{zheng2023judging}  offer another type of capability evaluation based on human feedback.

There are far fewer capability evaluations for other modalities. 
Comprehensive evaluation suites exist for vision models as well \citep{lee2023holistic, awadalla2023openflamingo}, but they are relatively less well developed. 
There are also a number of common benchmarks for evaluating vision models on a large number of tasks \citep{gadre2023datacomp,liu2023mmbench, fu2023mme}.
Evaluations for speech models' capabilities are still nascent, with the OpenASR Leaderboard,\footnote{\url{https://huggingface.co/spaces/hf-audio/open_asr_leaderboard}} which ranks models based on their Word Error Rate and Real-Time Factor, and the Edinburgh International Accents of English Corpus \citep{sanabria2023edinburgh} as leading examples.

The cheatsheet includes common benchmarks as of December 2023, but we caution that each comes with substantial limitations.
For instance, many benchmarks based on multiple choice exams are not indicative of real user questions, and can be gamed with pseudo-data contamination.
Additionally, while leaderboards are exceedingly popular, model responses are often scored by other models, which have implicit biases to model responses that are longer, and look similar to their own \citep{dubois2023alpacafarm}.


% \input{tables/general-capability-eval-2}
% \vspace{-2mm}
\subsection{Harm \& Hazard Taxonomies}

Taxonomies provide a way of categorising, defining and understanding risks and hazards created through the use and deployment of AI systems. Some taxonomies focus primarily on the types of interactions with models that \textit{create} a risk of harm (often called ``hazards'') whereas others focus on the negative effects that they lead to (often called ``harms''). 
Some taxonomies focus on existing issues, such as models that create hate speech or child abuse material, as well as more intangible or indirect forms of harm, such as the risk that models perpetuate biases and stereotypes, and misrepresent social groups. Other taxonomies are focused on the longer-term threats posed by more sophisticated models, such as ultra-personalised disinformation, cybersecurity threats, and military use \citep{brundage2018malicious}. Some work has also focused on categorising catastrophic or ``existential'' risks presented by Artificial General Intelligence, such as rogue AI agents and Chemical, Biological, Radiological, Nuclear and high-yield Explosive weapons \citep{carlsmith2022powerseeking, hendrycks2023overview, 10.1145/3514094.3534146}. Further, a few taxonomies also assess the available evidence for the risks and hazards, discuss their impact, and offer mitigation strategies \citep{deng2023safer, kapoor2024societal, klyman2024aups-for-fms}. 

Many taxonomies are released with an associated dataset, which can be used to either train models to minimise safety risks or to evaluate those risks. Several datasets have been released as benchmarks \citep[e.g.][]{wang2024decodingtrust}, which can be used to track progress across the community. We provide a non-exhaustive list of existing taxonomies with datasets that are available open-source. We also note forthcoming work planned by organisations like ML Commons, which aims to standardise assessment of AI safety risks by introducing a new benchmark, comprising a taxonomy and dataset.\footnote{\url{https://mlcommons.org/working-groups/ai-safety/ai-safety/}}.


\begin{enumerate}
    \item \textbf{TrustLLM} is a benchmark that covers six dimensions in English, including truthfulness, safety, fairness, robustness, privacy, and machine ethics \citep{sun2024trustllm}. The benchmark comprises over 30 datasets from existing research. In the paper, they test 16 open-source and proprietary models, and identify critical safety weaknesses. 
    \item \textbf{SafetyBench} is a benchmark that covers eight categories of safety, in both English and Chinese \citep{zhang2023safetybench}. Categories include Offensiveness; Unfairness and Bias; Physical Health; Mental Health; Illegal Activities; Ethics and Morality; and Privacy and Property. Unlike most safety evaluation datasets, SafetyBench comprises multiple choice questions which makes automated evaluation of models far easier. In the paper, they test 25 models and find that GPT-4 consistently performs best.
    \item \textbf{DecodingTrust} is a benchmark that covers eight dimensions of AI safety and trustworthiness in English \citep{wang2024decodingtrust}. It covers a range of safety criteria such as Toxicity; Stereotypes; Adversarial Robustness; Out-of-Distribution Robustness; Privacy; Machine Ethics; And Fairness. The benchmark has a leaderboard that is hosted on HuggingFace.
    \item \textbf{HarmBench} is a standardized evaluation framework for automated redteaming of LLMs in English \citep{mazeika2024harmbench}. It covers 18 widely used red teaming methods, such as Persona, stochastic-few shot, PEZ and GBDA. The benchmark has been designed with both seven semantic categories (e.g. Cybercrime, Misinformation and Bioweapons) and four “functional categories” (e.g. Standard behaviours). In the paper, 33 LLMs are tested against HarmBench.
    \item \textbf{BigBench} \citep{srivastava2023imitation} and HELM \citep{liang2023holistic} contain tests that are related to safety, such as toxicity, bias and truthfulness in BigBench and toxicity, bias, disinformation, copyright infringement and truthfulness in HELM. Both make use of the widely-used RealToxicityPrompts dataset \citep{gehman-etal-2020-realtoxicityprompts}. Terms such as ``toxicity'' and ``offensiveness'' have been criticised in some papers for being overly broad and easy to misinterpret \citep{vidgen-etal-2019-challenges}, and more recent work has tended to use more fine grained terms. 
    \item Individual datasets have also been released that can be used to assess specific safety risks of models, such as SimpleSafetyTests which tests for clear-cut safety problems \citep{vidgen2023simplesafetytests} and XSTest which tests for model false refusal \citep{röttger2024xstest}.
    \item \textbf{SafetyPrompts} is a website that hosts datasets for evaluating the safety of models \footnote{\url{https://safetyprompts.com/}} It does not aggregate or combine the datasets that it hosts, but has a basic review process to check the quality and integrity of the datasets.
   
\end{enumerate}


Almost all of the taxonomies described here are informed by practitioners' experiences of tackling safety issues in AI models; prior research; and exploratory red teaming. Many have drawn heavily on existing work in social media trust and safety, such as \citet{banko-etal-2020-unified, dev-etal-2022-measures}. They are all top-down in the sense that they define categories of hazard (or harm) and then find, create or curate prompts that match those categories. An alternative way of addressing AI safety is to start from the bottom up and to task red teamers with creating their own categories. Grounded-theory style approaches in linguistics can be used to then standardise the categories and ascribe some structure to the taxonomy. 
Across all methods of creating taxonomies (and datasets), there is a substantial focus on text-only models and future work should pay more attention to multimodal and non-text based models. Similarly, there is a strong bias towards English language and the Western cultural context. This should also be addressed in future work. 


% \vspace{4mm}




% \input{tables/risks-harms-taxonomies-1}
% \vspace{-2mm}

% \clearpage
\subsection{Risks \& Harms}
\vspace{-2mm}

% \input{tables/risks-harms-1}
% \vspace{-2mm}

Model developers have used various techniques to address risks and mitigate harms. Broadly, these attempts fall into three categories, roughly ordered by effectiveness.

\textbf{In-context learning.} In-context learning can be used to add instructions to a model's system prompt to steer the model's outputs. For example, OpenAI's DALL-E 3 model included instructions to avoid outputting copyrighted characters\footnote{See: \url{https://the-decoder.com/dall-e-3s-system-prompt-reveals-openais-rules-for-generative-image-ai/}}. In some sense, this is the easiest model-level intervention: developers do not need to retrain or fine tune the model, neither do they need to rely on external API calls, such as to third-party content moderation endpoints. However, this comes at a cost: system prompts are easy to jailbreak, and as a result, interventions based on in-context learning might be more brittle compared to other guardrails.

\textbf{Model alignment.} One of the most prominent approaches for mitigating risks and harms is aligning models with preference data. This usually involves supervised fine tuning or training reward models that steer the outputs of language models. Popular techniques include reinforcement learning with human (RLHF) and AI (RLAIF) feedback~\citep{bai-constitutional-2022, ouyang2022training}. To help end users understand the performance of reward models, \citet{lambert2024rewardbench} develop a leaderboard for evaluating reward models for various desiderata, including safety. Still, model alignment techniques can be brittle against simple modifications to the model, such as fine tuning~\citep{qi2023fine}, even when the model is fine tuned using benign data~\citep{he2024s}.


\textbf{Guardrails on model inputs and outputs.} The two interventions above rely on changing the model behavior via in-context learning or fine tuning. However, guardrails on model inputs and outputs that lie \textit{outside} the model might be a more robust intervention for preventing harmful content generation. For example, several model developers provide moderation endpoints that can be used to filter inappropriate user requests or model outputs. These can be general-purpose endpoints that can be modified for filtering and moderation (e.g., Cohere's classification endpoint modified for toxicity detection\footnote{See: \url{https://docs.cohere.com/reference/toxicity-detection}} or Anthropic's suggested prompt for content moderation\footnote{See: \url{https://docs.anthropic.com/claude/docs/content-moderation}}) as well as endpoints that are specifically built for content moderation (e.g., Perspective API~\citep{lees2022new} and OpenAI's moderation endpoint \footnote{See: \url{https://platform.openai.com/docs/guides/moderation/quickstart}}). Google's PaLM and Gemini models allow API users to set thresholds based on the safety likelihood and severity of model outputs\footnote{See: \url{https://cloud.google.com/vertex-ai/generative-ai/docs/configure-safety-attributes-palm}, \url{https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes}}).

The examples above are all from closed model developers. Recently, several open models have also been proposed for moderating model inputs and outputs. For example, Llama-Guard by Meta~\citep{inan2023llama} can be used to filter harmful content. Nvidia's NEMO guardrails allow model providers to add programmatic guardrails to filter content~\citep{rebedea2023nemo}.


% ———————— \newline
% Evaluations of risk serve multiple purposes: to identify if there are issues which need mitigation, to track the success of any such mitigations, to document for other users of the model what risks are still present, and to help make decisions related to model access and release.
% Harm is highly contextual \citep{dingemanse2022text, koenecke2020racial}, so developers should consider the context in which their foundation model might be used and evaluate the highest severity and most likely risks.

% To think through the possible risks, many taxonomies of harm have been created and provide good starting points. Determining how to evaluate risk is also challenging, as there are risks and modalities with limited evaluation coverage. The sample included below are a starting point for certain key areas, but we encourage developers to browse the evaluation repository (linked below) to see if there is something more suited to their needs.
% In addition to fixed benchmarks, an emergent approach to evaluation is using one model to evaluate another, as done by \citet{perez2022red} and in Anthropic's Constitutional AI work \citep{bai_constitutional_2022}.
% \vspace{-2mm}
% ———————— \newline

\subsection{Review}
\label{sec:eval-review}

\textbf{A shift away from evaluating models, and towards evaluating systems.}
Existing evaluation practices and reporting often focus on probing individual models, in unconstrained settings.
However, deployed systems often operate in the context of multiple interactive models, moderation endpoints, sophisticated decoding strategies, and rule-based constraints, that make up a ``system''.
More pointedly, for dual-use systems, the context of use \emph{outside the system} is what defines the presence of harm: \citet{NarayananKapoor2024} argue that ``defenses against misuse must primarily be located outside models''.
Prior work has emphasized the importance of the system \citep{dobbe2022system} and context \citep{raji2023concrete} in diagnosing and resolving AI safety challenges.
As a result, efforts to evaluate models alone are inherently limited, both in their findings, and informing effective changes.
Safety problems in particular are better addressed by evaluations that consider the context (e.g. attack vector, deployed use setting), as well as the interactions between elements of a foundation model system (of which only one is a the model itself).
However, these types of evaluations can be difficult in the absence of transparency into the components of deployed systems \citep{bommasani2023foundation}.
Recent work has even shown that independent researchers can face significant obstacles in fairly evaluating proprietary systems \citep{longpre2024safe}.

\textbf{A shift away from evaluating on static toxicity/safety benchmarks, and towards evaluating on real-world, dynamically changing naturalistic observations, as well as human-interaction studies.}
Existing widely used model risk and safety evaluations, such as RealToxicityPrompts \citep{gehman2020realtoxicityprompts}, or bias benchmarks such as BBQ \citep{parrish2022bbq}, BOLD \citep{dhamala2021bold}, and HolisticBias \citep{smith2022m}, have been effective at  testing models' superficial bias tendencies.
However, these evaluations are static, not grounded in real-world contexts, and pay no heed to the human-interaction element---\textit{i.e. the actual system users}.
These limitations suggest several dimensions of improvement for future safety benchmarks:
\begin{itemize}
    \item \textbf{Naturalistic observations} Collecting natural interactions with users enable researchers to identify realistic tasks and prompts, to simulate real harms. For instance, WildChat \citep{zhao2023inthe} collects voluntary user interactions with OpenAI systems through proxy interfaces---however these datasets may be heavily skewed by the types of users that adopt it.
    \item \textbf{Domain expert designed tasks} LegalBench \citep{guha2024legalbench} offers an alternative naturalistic design, whereby evaluations are hand-crafted by the practitioners (in this case, legal professionals) in the field of evaluation. These benchmarks distinguish themselves from existing evaluation suites in their relevance to natural, real-world usage.
    \item \textbf{Human-interaction studies} Most evaluations are non-interactive---they target the model, without real-world users. This form of analysis can identify model flaws, but falls short of investigating the actual affects, harms, and interaction patterns on users. \citet{lee2023evaluating} proposes a framework to evaluate interactive user experience, without which notions of harm and safety are under-developed.
    \citet{le2022learning} illustrates the importance of interactive evaluation, particularly on non-Western users.\footnote{In this study, Australian Aboriginal use of an information retrieval app illuminated false assumptions and expectations in the evaluation procedure.} 
    % \item \textbf{Dynamic benchmarks} Static benchmarks, in particular for jailbreaks, are rapidly overcome, in part because of the 
    % The capabilities and defenses of foundation models rapidly improve, in part due to the cyclical effects of training on datasets developed to target evaluations, and their memorization abilities.
    % Large foundation models' ability to rapidly memorize
    % As the  of foundation models develop,
    
    % 
\end{itemize}
These three evaluation dimensions enable more realistic studies of user interaction with foundation model systems.
Collecting naturalistic or interactive data can also provide \textit{continuous and dynamic} data sources, which are particularly beneficial when there is rapid model development (or the habit of training on prior evaluation sets).
More generally, research into dynamic and evolving benchmarks, that test beyond the training set distribution, are an important research direction \citep{yu2023skill, kiela2021dynabench}.


\textbf{A shift away from reporting evaluations, to releasing reproducible evaluation scripts.} 
There are dozens of choices that affect the results of an evaluation \citep{Anthropic2023}.
Some choices include, but are not limited to:
\begin{itemize}
    \item \textbf{Prompt format} Results vary dramatically depending on if the input prompts are zero-shot, few-shot, chain-of-thought, and also if the prompts were manually refined directly on the test set. Multiple choice benchmarks have both versions with and without the answer choices given in the prompt. (See MMLU as a widely used dataset with inconsistencies in use and standardization.\footnote{\url{https://crfm.stanford.edu/2024/05/01/helm-mmlu.html}}
    \item \textbf{Decoding strategies} The decoding algorithm and its hyperparameter choices, such as the temperature and sampling probabilities, will affect model behavior. For systems, rather than models, responses can be the result of multiple iterations or models, or rely on external tools or sources. Ensembling techniques like self consistency \citep{wang2022self} also improve performance significantly.
    \item \textbf{Evaluation metric} The choice of evaluation metric can impact the apparent magnitude of differences between models, and even their ranking \citep{schaeffer2024emergent}. 
    \item \textbf{Human review setup} For human preference evaluations, several details affect fair evaluation: whether the response selection is sufficiently model-blind, the attentiveness and expertise of the annotators to the given topics, and the chosen rubric. Human preferences can also be skewed by the same factors as model-based evaluations \citep{hosking2024human, xu-etal-2023-critical, wu2023style}
\end{itemize}
Without transparency and reproducibility integrated into the evaluation procedure, the axes of design freedom can allow developers to game results, and prevent fair, apples-to-apples comparisons.
For these reasons, only evaluation scripts that are directly executable by third-parties provide \textbf{verifiable reproducibility}.
Executable evaluation scripts also allow auditors to unpack the choices made in evaluation. 
Auditors can quickly experiment with different prompt formats, decoding parameters, and evaluation metrics, to shed light on the scientific veracity of the claims.
While evaluation documentation is helpful, the required breadth of information inevitably leads to subtle omissions, and even if the information is comprehensive, it does not provide verifiable reproducibility.
For these reasons, we believe the evidence is clear that AI safety reporting is not sufficiently reliable or trustworthy without verifiable execution scripts.

\textbf{Ground harm and hazard taxonomies in empirical observations.}
Existing taxonomies of harm are often created to cluster existing safety benchmarks, which are mostly detached from real observations \citep{sun2024trustllm,zhang2023safetybench,hendrycks2023overview}.
As the taxonomies of harm can guide practitioners priorities, this could lead to neglected or over-emphasized areas of safety research.
It is essential that future harm taxonomies are strongly grounded in empirical or naturalistic observations, through research conducted with \emph{real users}, rather than hypothetical situations envisioned by researchers.

\textbf{Extending risks and harms studies to multimodal and highly sensitive attacks.}
A great deal of research into risks and harms is focused exclusively on text, in English, and on \emph{more conservative} safety risks such as toxicity and bias \citep{gehman-etal-2020-realtoxicityprompts,hartvigsen2022toxigen}.
However, recent work has emphasized the particular risks of generative image, speech, and video models, being used to create deepfakes, NCII or CSAM  \citep{kapoor2024societal, thiel2023generative, lakatos2023revealing}. 
Other work has illustrated the much greater efficacy of jailbreaks and attacks in non-English languages, as compared to English \citep{yong2023low}.
Multimodal jailbreaking is an emerging research area, with some nascent work \citep{qi2024visual, shayegani2023jailbreak, niu2024jailbreaking}.
Research into risks and harms from autonomous weapons systems (AWS) is also highly under-explored, and an emerging risk \citep{simmons2024ai, longpre2022lethal}.
% Another dimension that remains under-explord is sensitivity to prompts. 