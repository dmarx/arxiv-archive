\section{Model Training}
\label{sec:model-training}
\vspace{-2mm}

% https://www.latex4technics.com/?note=MOB
% \begin{tcolorbox}[width=\textwidth,title={Data Preparation Best Practices}]
\begin{tcolorbox}[
    width=\textwidth,
    title={Model Training Best Practices},
    colback=backgroundcol, % Background color of the box
    colframe=darkgray, % Frame color
    colbacktitle=modeltraining, % Background color of the title
    coltitle=white, % Title text color
    coltext=black % Text color
]

\begin{itemize}[itemsep=0pt, wide=3pt] 
    \item The foundation model life-cycle consists of several stages of training, broadly separated into pre-training and fine-tuning. 
    \item Decisions made by developers at any stage of training can have outsized effects on the field and the model's positive and negative impacts, especially decisions made by well-resourced developers during the pre-training stage.
    \item Developers should be thoughtful about the effects of train-time decisions and be aware of the trade-offs and potential downstream effects prior to training.
    \item Due to the large economic and environmental costs incurred during model training, making appropriate use of training best practices and efficiency techniques is important in order to not waste computational or energy resources needlessly.
\end{itemize}
\end{tcolorbox}

% Survey (summarize the contributions of the resources/tools/findings not just in the paper tables, but in the website tables). Don't assume that the tables will be left in, so make sure to add citations appropriately. This part would be broken down into the subsections (e.g. Pretraining Datasets, Finetuning Datasets) that we've already delineated.
% Recommendations (we have those already at the section level, but maybe they can be improved)
% Review (a review for the whole section, weaving the narratives together. what is working? what isn't? what lacks adoption? what needs to be built? etc..)

% \subsection{Survey}

Thousands of tools and resources have been developed for model training. While it is not feasible to comprehensively list them, we focus our efforts on a subset of tools that are well documented, emphasize computational efficiency, or educational resources.
For a more comprehensive list of training tools, we point the reader to surveys, such as the survey of foundation model training and serving systems \citep{zhou2024training}, the survey of efficient federated learning methods \citep{woisetschlager2024survey}, or this ``comprehensive'' survey of foundation models \citep{zhou2023comprehensive}, which can be used to trace their training setup.

Foundation models are by their design frequently reused and applied for numerous diverse downstream uses. This takes the form of a multi-stage training process throughout models' lifestyles, starting with training a strong base from scratch (``pretraining'') and followed by further refinement or adaptation to new use cases (``fine-tuning''). For this reason, all stages of training must be done with care: especially for pretrained models or models that will otherwise be deployed widely or used as a base for further extensions, the decisions made by model trainers at train-time can have outsized impacts on the model's characteristics, both positive and negative, or on the field as a whole. 

Here, we cover a selection of existing resources for model training. We include frequently-used codebases for model training that may be useful entry points for new developers to the field, but note that we cannot cover all existing options. We additionally briefly discuss resources where practitioners can learn about improving the resource-efficiency of their training, as well as educational resources for learning about model training.

\subsection{Pretraining}\label{sec:pretrain-repo}

Model pretraining requires by far the largest amount of resources computationally during model training.

Therefore, practitioners should consider using already-optimized codebases, especially in the pretraining phase, to ensure effective use of computational resources, capital, power, and effort. Existing open-source codebases targeted at foundation model pretraining can make pretraining significantly more accessible to new practitioners and help accumulate techniques for efficiency in model training.

Various codebases may be optimal depending on the scale of model or number of devices used for pretraining. Some codebases seek to be performant while also being lightweight (OpenLM \citep{open_lm}, Nanotron \citep{nanotron}) for effective training at medium scales and up, while others aim for maximal performance at massive scales such as GPT-NeoX \citep{gpt-neox-library}, Megatron-DeepSpeed \citep{smith2022using}, and Megatron-LM \citep{shoeybi2020megatronlm} and have been used in practice to train language models across thousands of GPUs. Depending on the particular vision application or model architecture, good scalable codebases exist for training. For example, OpenCLIP \citep{ilharco_gabriel_2021_5143773} can be used for training CLIP text + vision models, Timm \citep{ rw2019timm} supports a variety of standard vision model architectures and training scripts, and UniLM supports the training of interleaved text-and-image models similar to Kosmos-2 \citep{peng2023kosmos2}. For modalities other than text or vision, tooling exists (Lhotse for audio data processing and data loading \citep{lhotse}, Stable Audio Tools \citep{stable-audio-tools} for training audio generation models) but is less standardized.
Lastly, \citet{karamcheti2024prismatic, mckinzie2024mm1} explore lots of design choices for multimodal models, including training, fine-tuning, image processing strategies, and data mixing.

We emphasize the utility and importance of adopting existing codebases for pretraining, due to the difficulty of debugging and detecting errors in large-scale distributed systems as encountered in pretraining. Existing scalable codebases drastically reduce the chances of silent failures or correctness issues lowering the end model quality.

\subsection{Fine-tuning}
\label{sec:finetune-repo}
% \vspace{-2mm}

Fine-tuning, or other types of adaptation performed on foundation models after pretraining, are an equally important and complex step in model development and have the ability to significantly steer the behaviors and characteristics of the end model. Fine-tuned models are also more frequently deployed than base models, making their safety and usability very important. Here we discuss a subset of finetuning resources that are well documented, flexible, and some of which cater to computational efficiency.

While fine-tuning is significantly less resource-intensive than pretraining, there are still many relevant considerations for practitioners. Use of widely adopted tools such as libraries designed for fine-tuning (Axolotl \citep{axolotl}, trlX \citep{trlx-library}, Levanter (\url{https://github.com/stanford-crfm/levanter})) can ensure greater ecosystem compatibility of resulting models, or reduce the barrier to experimentation by abstracting away common pitfalls or providing guidance on effective hyperparameters. Similarly, the use of techniques such as QLoRA \citep{dettmers2023qlora} or other popular parameter-efficient fine-tuning approaches and libraries (peft \citep{peft} , Otter, LLaMA-Adapter, LLaVA \citep{li2023otter, zhang2023llamaadapter, liu2023improved}) can allow for fine-tuning for lower cost or on more accessible hardware.

\subsection{Efficiency and Resource Allocation}\label{sec:efficiency-training}
% \vspace{-2mm}

Knowledge of training best practices and efficiency techniques can reduce costs to train a desired model significantly. Beyond systems-level optimizations and approaches to increase efficiency, the most important technique is determining the most efficient \textit{allocation} of resources, such as allocating compute between model size and dataset size for a given budget for the best results. The most common approach to solving this problem is to apply \textit{scaling laws} \citep{kaplan2020scaling, hoffmann2022training, muennighoff2023scaling}, a common tool for cheaply extrapolating findings across scales of cost in order to make decisions based on smaller-scale experiments for a final model training run.

Additionally, practitioners seeking to embrace an open approach to model development should consider how their decisions when training a foundation model may have impacts long after that model's creation and release. For instance, a model that is released openly but is too computationally demanding to be run on consumer-grade hardware will be limited in its impact on the field, or a model trained to minimize training compute but not minimize inference cost may result in a greater environmental impact than spending more training compute in the first place for a cheaper-to-infer model \citep{hoffmann2022training, touvron2023llama}. Practitioners should thus be aware of potential second-order effects of their model releases and training choices.

\subsection{Educational Resources}
% \vspace{-2mm}

Training models at any scale can be quite daunting to newer practitioners. However, there are options available for learning about how to train and run foundation models.

Resources which themselves collate and provide reading lists or recommendations for learning about large-scale ML training (EleutherAI Cookbook \citep{anthony2024cookbook}, ML Engineering Open Book \citep{ml-engineering}) can be an especially useful place to begin. Additionally, minimal codebases created as educational examples such as NanoGPT \citep{nanogpt}, or other blog posts detailing fundamental concepts in training or running foundation models may be useful \citep{inference-arithmetic, transformer-math-eleutherai}. 
We recommend that practitioners review these resources and use them to guide further reading about model training and usage. 

\subsection{Recommendations}
\vspace{-2mm}


Regardless of the stage or cost of training being performed, we urge developers to carefully consider the impacts of their training choices, and how they can be improved, as we have described. We hope the linked resources will be a helpful jumping-off point.

We additionally encourage practitioners to use the codebases linked as a foundation for their work, to avoid ``reinventing the wheel'' for every new project. Unifying and collectively improving existing codebases, tooling, and commonly used standards around model training, as with tooling around other parts of the foundation model development process, can allow for the entire community to benefit from others' effort via using codebases that have been well-tested for correctness and scalability. This can leverage the unique advantages of open model development via strength in numbers. 

\subsection{Review}
\vspace{-2mm}


% more resources for multimodality / multimodal RLHF

% more resources for non-english

\textbf{More resources, especially non-English ones, for lowering the barrier to entry for less technical developers are needed}. Current tooling requires at minimum a technically knowledgeable user or proficient programmer, and further may have lacking documentation or be high in complexity. Openly available tools for training, especially fine-tuning, models that do not presume familiarity with training methods or even programming would have the potential to allow many more users not formally educated in computer science to customize and build models suited to their use cases, such as for lower-resource languages. 


\textbf{More standardized and centralized resources, especially cross-modality, should be focused on in the future.} Many current resources are centered around language model training. However, resources for other modalities are often more scattered or rely on custom architectures and implementations. Exploring topics such as multimodal fine-tuning best practices, and multimodal preference learning, is an important future area. 
For instance, there is a lack of efficient tooling for data loading for multimodal training. While WebDataset\footnote{\url{https://github.com/webdataset/webdataset}} offers a resource for images, there are fewer options for videos, which can cause the GPU to stay idling, waiting for data.
Future work should look to unify these techniques and modalities in the same tools and infrastructure to enable more and easier investigation or transfer of techniques that work on language models to the rest of the field. Even within the text modality, an \textit{interoperable} ecosystem with shared, well-tested code building blocks and standards should be pursued, to provide a strong base for further extension and the pooling of developer efforts.

% TODO: from Shayne: E.g. perhaps we need more centralized or more modality agnostic repositories? Is there a disconnect between training and eval resources? Is the community neglecting important training resources? Etc..

\textbf{Other options for massively collaborative development should be pursued.}
To allow for better pooling of resources, both over time and across the community, methods for more collaborative training are an important future frontier. Some of these methods are in their early stages already being provided by tools such as MergeKit \citep{goddard2024arcees} and being explored by the research community \citep{matena2022merging, donyehiya2023cold, stoica2024zipit, yadav2023tiesmerging}. By enabling better re-use of existing artifacts or allowing a greater number of collaborators to contribute to building a model together, these methods could potentially make model training as parallelizable and reusable as work on data improvement, and the merging of efforts and compute expended could allow a wider range of contributors to steer model development.



%What tools/resources are lacking in the current AI ecosystem? What's on your wishlist?!
% What tools/resources have received too much attention, have been misused, or haven't always been useful for responsible development?
% What tools/resources could be improved? How?
% What tools/resources have allowed the community to cut corners / "ethics-wash"?
% Any other points you'd like mentioned in a FM development cheatsheet review?


% \input{tables/model-training-1}

% \subsection{Pretraining Repositories}
% \label{sec:pretrain-repo}
% \vspace{-2mm}

% \input{tables/model-training-1}
% \vspace{-2mm}

% Practitioners should consider using already-optimized codebases, especially in the pre-training phase, to ensure effective use of computational resources, capital, power, and effort. Existing open-source codebases targeted at foundation model pretraining can make pretraining significantly more accessible to new practitioners and help accumulate techniques for efficiency in model training.

% Here, we provide a sample of existing widely-used pre-training codebases or component tools that developers can use as a jumping-off point for pre-training foundation models.




% \subsection{Finetuning Repositories}
% \label{sec:finetune-repo}
% \vspace{-2mm}

% Fine-tuning, or other types of adaptation performed on foundation models after pretraining, are an equally important and complex step in model development. Fine-tuned models are more frequently deployed than base models.

% Here, we also link to some useful and widely-used resources for adapting foundation models or otherwise fine-tuning them. Use of these tools can ensure greater ecosystem compatibility of resulting models, or reduce the barrier to experimentation by abstracting away common pitfalls or providing guidance on effective hyperparameters. 

% \input{tables/model-training-2}
% \vspace{-2mm}

% \subsection{Efficiency and Resource Allocation}\label{sec:efficiency-training}
% \vspace{-2mm}

% Knowledge of training best practices and efficiency techniques can reduce costs to train a desired model significantly. Here, we include a select few readings and resources on effectively using a given resource budget for model training, such as several canonical papers on fitting \textit{scaling laws}, a common tool for extrapolating findings across scales of cost. These are used frequently to determine the most efficient allocation of resources, such as allocating compute between model size and dataset size for a given budget.

% Additionally, practitioners seeking to embrace an open approach to model development should consider how their decisions when training a foundation model may have impacts long after that model's creation and release. For instance, a model that is released openly but is too computationally demanding to be run on consumer-grade hardware will be limited in its impact on the field, or a model trained to minimize training compute but not minimize inference cost may result in a greater environmental impact than spending more training compute in the first place for a cheaper-to-infer model. Practitioners should thus be aware of potential second-order effects of their model releases and training choices.

% \input{tables/model-training-3.tex}
% \vspace{-2mm}

% \subsection{Educational Resources}
% \vspace{-2mm}

% Training models at any scale can be quite daunting to newer practitioners. Here, we include several educational resources that may be useful in learning about the considerations required for successfully and effectively training or fine-tuning foundation models, and recommend that practitioners review these resources and use them to guide further reading about model training and usage. 

% \input{tables/model-training-4.tex}
% \vspace{-2mm}

% \subsection{Barriers to Downstream Usage}