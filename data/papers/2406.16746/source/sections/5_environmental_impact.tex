\section{Environmental Impact}
\label{sec:environmental-impact}
\vspace{-2mm}

% https://www.latex4technics.com/?note=MOB
\begin{tcolorbox}[
    width=\textwidth,
    title={Environmental Impact Best Practices},
    colback=backgroundcol, % Background color of the box
    colframe=darkgray, % Frame color
    colbacktitle=dataprep, % Background color of the title
    coltitle=white, % Title text color
    coltext=black % Text color
]
\begin{itemize}[itemsep=0pt, wide=3pt]
    \item Training and deploying AI models impacts the environment in several ways, from the rare earth minerals used for manufacturing GPUs to the water used for cooling datacenters and the greenhouse gasses (GHG) emitted by generating the energy needed to power training and inference.
    \item Developers should report energy consumption and carbon emissions separately to enable an apples-to-apples comparisons of models trained using different energy sources.
    \item It is important to estimate and report the environmental impact not just of the final training run, but also the many experiments, evaluation, and expected downstream uses.
    \item It is recommended, especially for major model releases, to measure and report their environmental impact, such as carbon footprint, via mechanisms such as model cards (see \cref{sec:documentation}).
\end{itemize}

\end{tcolorbox}


\subsection{Estimating Environmental Impact}
\vspace{-2mm}

The environmental impact of AI is of increasing concern \citep{schwartz2020green}.
Estimating the environmental impact of model development is a challenging task due to the number of relevant, hard to compute, and often unrecorded variables.
For instance, to estimate the Life Cycle Assessment (LCA) of model development \citep{klopffer1997life}, variables include: model size, architecture, duration of training, number of training runs, storing and transferring data, hardware manufacturing, the specific type and setup of the hardware, network configuration, the geographic location of the data center, the carbon intensity of the energy grid, the power usage effectiveness (PUE) of cooling, overhead, and the broader data center infrastructure, as well as a similar set of questions for various deployment/inference settings \citep{patterson2021carbon}.
Many of these variables are also infeasible to precisely estimate, given for instance, that Nvidia does not disclose the carbon footprint of its GPUs \citep{luccioni2023estimating}.
Another challenge includes the variety of relevant environmental outcome measures, from CO2 emissions, to energy footprint, or water use.

Existing tools to measure environmental impact rely on a series of assumptions and estimates based on the available information.
Perhaps the most accurate tools are those built into cloud services, which are able to trace the hardware configurations and geographical locations of data centers but may not otherwise be publicly available to users.
The Azure Emissions Impact Dashboard (\url{https://www.microsoft.com/en-us/sustainability/emissions-impact-dashboard}), AWS carbon footprint tool (\url{https://aws.amazon.com/aws-cost-management/aws-customer-carbon-footprint-tool/}), and Google Cloud Carbon Footprint measurement system (\url{https://cloud.google.com/carbon-footprint?hl=en}) are three such systems, available only when using their cloud services directly.
Independent systems, such as Carbontracker \citep{anthony2020carbontracker}, CodeCarbon \citep{schmidt2021codecarbon}, or the Experimental Impact Tracker \citep{henderson2020towards} offer basic estimate modeling and reporting, based on limited input information.
Similarly, \citet{li2023making} provides an estimate tool for the water usage footprint of language model training and deployment.
ML CO2 Impact (\url{https://mlco2.github.io/impact/}) improves these repositories with a wrapper interface for easier use.
However, these services are forced to trade-off between ease of use and accuracy, as significant input information is required to obtain precise results, which imposes a burden on users and widespread adoption.
 
% Current tools, including the ones mentioned in the table, focus on the latter point by measuring the energy consumed during training or inference and multiplying it by the carbon intensity of the energy source used. While other steps of the model life cycle (e.g. manufacturing hardware, heating/cooling datacenters, storing and transferring data) also come with environmental impacts, we currently lack the information necessary to meaningfully measure these impacts \citep{luccioni2023estimating}.
% The table below outlines resources for back-of-the-envelope estimations of environmental impact, in-code estimation, as well as dashboard for cloud computing platforms to estimate environmental impact \citep{anthony2020carbontracker,lacoste2019quantifying}.

% \input{tables/environment-impact-1}
% \vspace{-2mm}
 
\subsection{Effective use of resources}
\vspace{-2mm}

Several decisions made prior to model training can have significant impacts on the upstream and downstream environmental impact of a given model.
Empirical scaling laws can be used to find the best allocation of resources.
\citet{kaplan2020scaling, hoffmann2022training} estimate the optimal model size and training duration, given a training compute budget.
And \citet{aghajanyan2023scaling} investigates the equivalent efficient compute allocation for multi-modal settings.
When working with text training data that is constrained, recent work explores how to allocate compute efficiently \citep{muennighoff2023scaling}.
For models frequently used downstream, it is important to consider the inference footprint and inference cost during model creation \citep{gadre2024language}, to minimize the environmental impact of inference. For further resources and discussion, see \ref{sec:efficiency-training}.

% \input{tables/environment-impact-2}
% \vspace{-2mm}

\subsection{Review}
\label{sec:environment-review}

In this section we critically review the current state of resources for environmental impact analysis. First, we note a dearth of transparency, from hardware manufacturers, data centers, and corporate model developers stymies environmental impact estimates, and particularly product comparisons for users.
Lastly, scaling laws research needs to expand to cover newer multi-modal modeling efforts, as well as provide intuitive tooling for open developers to adopt.

\textbf{Transparency from consumer-level API providers, into query-level energy and environmental usage measures.}
Currently, some of the most widely adopted generative AI services, including the APIs and playgrounds from OpenAI, Google, Anthropic, Inflection, Midjourney, and others, do not expose any information into the environmental footprint of using their models \citep{bommasani2023foundation}.
As these systems dominate consumer market usage \citep{korinek2023market}, this leaves a wide gap in our knowledge of net effects.
The scientific community relies largely on assumptions and ballpark estimates, both for training and inference impact.
% , for users to make apples-to-apple comparisons
% Better measurements, with fewer assumptions, and ballpark estimates. 

\textbf{Transparency from hardware makers and data centers, into fine-grained energy and environmental usage measures.}
Upstream of AI developers, the data centers and hardware makers expose limited information about environmental and energy measures \citep{luccioni2023estimating}.
These metrics would enable more accurate and real-time estimates of compute, for closed and open developers.


\textbf{``Energy Star'' standards for non-technical users to fairly compare AI services.}
Data center, hardware, and developer transparency are a precursor to accurate estimates and, more importantly, \textit{environmental footprint competition} between corporate services.
Currently, these services compete on model quality and price, but not on environmental impact---a property that many consumers are likely to care about if given the option.
``Energy Star'' standards, from other industries, allow competitors to claim equal or better services, at less energy expenditure \citep{brown2002status}.
These sorts of apples-to-apples comparisons may be necessary to inhibit negative environmental consequences from AI.

\textbf{Scaling laws research currently lacks empirical evidence for the new wave of multi-modal models, and intuitive user interfaces for new developers.}
Scaling laws research has focused predominantly on text \citep{kaplan2020scaling, hoffmann2022training, muennighoff2023scaling, gadre2024language}, with limited work for multi-modal foundation models \citep{aghajanyan2023scaling}. As developers increasingly pursue image, video, and speech models (both in input and output), such as Sora \citep{sora2024}, Stable Video Diffusion \citep{blattmann2023stable}, Claude 3 \citep{claude3}, and Whisper \citep{radford2023robust}, the efficient scaling laws are currently under-investigated.
Secondly, while scaling law research investigates fundamental questions, it can be presented in unapproachable, complex ways.
The ecosystem lacks tools for less technical developers to heed efficient compute estimates, such as a plug-and-play interface.