\section{Data Preparation}
\label{sec:data-prep}

% https://www.latex4technics.com/?note=MOB
% \begin{tcolorbox}[width=\textwidth,title={Data Preparation Best Practices}]
\begin{tcolorbox}[
    width=\textwidth,
    title={Data Preparation Best Practices},
    colback=backgroundcol, % Background color of the box
    colframe=darkgray, % Frame color
    colbacktitle=dataprep, % Background color of the title
    coltitle=white, % Title text color
    coltext=black % Text color
]

\begin{itemize}[itemsep=0pt, wide=3pt]
    \item Tools for \textbf{searching and analysing} can help developers better understand their data, and therefore understand how their model will behave; an important, but often overlooked, step of model development.
    \item Data \textbf{cleaning and filtering} can have an immense impact on the model characteristics, though there is not a one size fits all recommendation. The references provide filtering suggestions based on the application and communities the model is intended to serve.
    \item When training a model on data from multiple sources/domains, the quantity of data seen from each domain (\textbf{data mixing}) can have a significant impact on downstream performance. It is common practice to upweight domains of ``high-quality'' data; data that is known to be written by humans and has likely gone through an editing process such as Wikipedia and books. However, data mixing is an active area of research and best practices are still being developed.
    \item \textbf{Removing duplicated data} can reduce undesirable memorization and can improve training efficiency.
    \item It is important to carefully \textbf{decontaminate training datasets} by removing data from evaluation benchmarks, so their capabilities can be precisely understood.
\end{itemize}
\end{tcolorbox}

% Survey (summarize the contributions of the resources/tools/findings not just in the paper tables, but in the website tables). Don't assume that the tables will be left in, so make sure to add citations appropriately. This part should be broken down into the subsections ((1) data search, analysis and exploration, (2) data cleaning, filtering, and mixing, (3) deduplication, (4) decontamination, and (5) auditing) that we've already delineated. Try to name the resources/tools rather than just cite them, so readers don't have to jump to the references constantly. Ideally we compare and contrast some of the tools at a high level.
\subsection{Data Search, Analysis, and Exploration}

% \paragraph{Data Search, Analysis, and Exploration.}
% motivation
A critical step to understanding a dataset is to explore and analyze what it contains. In particular, exploring training datasets with search and analysis tools can help practitioners develop a nuanced intuition for what exists in the data, and therefore can help to predict what behaviors the model will exhibit.

% List of tools
Tools for search, analysis, and exploration can take a few forms. Some tools are aimed at understanding the high-level statistics of a dataset such as the length of inputs, frequency of specific n-grams, the languages in the corpus, possible biases, or the existence of undesirable content. For example, tools such as WIMBD~\citep{elazar2023whats} and Infini-gram~\citep{Liu2024InfiniGram} allow users to perform n-gram searches through commonly used pretraining datasets, and provide starting points for building a search index over any arbitrary dataset. The ROOTS search tool~\footnote{\url{https://huggingface.co/spaces/bigscience-data/roots-search}}~\citep{piktus2023roots} additionally allows users to search with fuzzy n-grams over the ROOTS corpus, and similarly, the clip-retrieval tool~\footnote{\url{https://github.com/rom1504/clip-retrieval}}\citep{beaumont-2022-clip-retrieval} allows users to search for nearest neighbor images and text from a multimodal corpus (e.g. LAION-5B~\citep{schuhmann2022laion5b}. Furthermore, the HuggingFace Data measurements tool\footnote{~\url{https://huggingface.co/spaces/huggingface/data-measurements-tool}} gives users access to statistics such as the most common n-grams, lengths of data points, and distribution over labels for a number of pretraining and finetuning datasets.
Yet more tools exist to explore the data manually and including the Data Provenance Explorer~\citep{longpre2023data} for text datasets, Google's Know Your Data tool~\footnote{\url{https://knowyourdata.withgoogle.com/}} for vision datasets and NVIDIA's Speech Data Explorer~\footnote{\url{https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tools/speech_data_explorer.html}} for speech datasets.

% high-level notes/takeaway
In most cases, it is highly recommended to explore datasets from the perspective of high-level statistics as well as getting to know the data by looking at individual data points. For instance, text data can have a wide distribution of lengths, topics, tones, formats, licenses, and even diction, and understanding each of these dimensions will require a different tool. We recommend that developers use the many available tools to search and analyze their datasets.


\subsection{Data Cleaning, Filtering, and Mixing}
% Motivation
Once the contents of a dataset are understood, the next step is to clean and filter the dataset to adjust the dataset's distribution towards desirable content. Filtering and cleaning do so by removing unwanted data from the dataset. They can improve training efficiency as well as ensure that data has desirable properties, including: high information content, desired languages, low toxicity, and minimal personally identifiable information. Data mixing is another important component of data preparation, where the mixture proportions of data domains (e.g.\ scientific articles, GitHub, and books) have been shown to dramatically affect downstream performance \citep{gao2020pile, xie2023doremi, albalak2023efficient}.

% List of tools
A first step for filtering text data is by language, where there is a plethora of tools including langdetect~\footnote{\url{https://github.com/Mimino666/langdetect}}, cld3~\footnote{\url{https://github.com/google/cld3}}, OpenLID~\footnote{\url{https://github.com/laurieburchell/open-lid-dataset}}~\citep{Burchell_2023}, GlotLID~\footnote{\url{https://github.com/cisnlp/GlotLID}}~\citep{kargaran2023glotlid}, and FastText~\footnote{\url{https://huggingface.co/facebook/fasttext-language-identification}}~\citep{grave2018learning}. The majority of modern language identification methods have been built on top of the FastText model used in the CCNet pipeline~\citep{wenzek-etal-2020-ccnet}.
In addition to filtering by language, datasets are commonly cleaned using heuristics (e.g. remove documents with fewer than 5 words, or remove lines that start with "sign-in") which have been implemented through a number of different tools including the Dolma toolkit~\footnote{\url{https://github.com/allenai/dolma}}~\citep{dolma}, Lilac~\footnote{\url{https://github.com/lilacai/lilac}}, and DataTrove~\footnote{\url{https://github.com/huggingface/datatrove}}\citep{penedo2024datatrove}, as well as DataComp~\footnote{\url{https://www.datacomp.ai/}}~\citep{gadre2023datacomp} for image-text pairs.
While heuristic filtering methods can remove significant quantities of data, they can be brittle. Model-based methods (e.g. remove data which is dissimilar to a known ``high-quality'' corpus or possibly toxic content) such as DSIR~\footnote{\url{https://github.com/p-lambda/dsir}}~\citep{xie2023data} and Detoxify~\footnote{\url{https://github.com/unitaryai/detoxify}}~\citep{Hanu_Detoxify_2020} can be used as additional filtering that allow for much more flexibility than heuristics.
In addition to cleaning and filtering, mixing is another component of dataset design which requires careful consideration. Many dataset mixing ratios have been determined by heuristics and human judgement. For example, the Pile~\citep{gao2020pile} and Llama~\citep{touvron2023llama} upweight domains that have likely gone through an editing process, such as books and Wikipedia articles. Tools for automated data mixing are, as of writing, very limited. However, academic research projects such as DoReMi~\footnote{\url{https://github.com/sangmichaelxie/doremi}}~\citep{xie2023doremi} and Online Data Mixing~\footnote{\url{https://github.com/alon-albalak/online-data-mixing}}~\citep{albalak2023efficient} provide GitHub repositories that can be repurposed for new datasets.


% high-level notes/takeaway
Cleaning, filtering, and mixing are crucial components of designing an appropriate dataset, but due to the vast space of possible filters and downstream uses of ML models there is no one-size-fits-all recommendation. Practitioners looking to clean and filter their datasets should first use the search, analysis, and exploration tools to determine how to design appropriate filters, and iteratively improve the filtering.
For more details and recommendations on cleaning, filtering, and mixing, see the recent survey by~\citet{albalak2024survey}.


\subsection{Data Deduplication}
% Motivation
Data deduplication is an important preprocessing step where duplicated documents, or chunks within a document, are removed from the dataset. Removing duplicates can reduce the likelihood of memorizing undesirable pieces of information such as boilerplate text, copyrighted data, and personally identifiable information. Additionally, removing duplicated data improves training efficiency by reducing the total dataset size.

% List of tools
Deduplication generally relies on one of four methods: URL matching, hashing methods, string metrics, or model representations, which can classify data as either exact or fuzzy matches. We suggest using the deduplication methods included in DataTrove~\footnote{\url{https://github.com/huggingface/datatrove}}\citep{penedo2024datatrove}, Google's deduplicate-text-datasets library~\footnote{\url{https://github.com/google-research/deduplicate-text-datasets}}~\citep{lee-etal-2022-deduplicating}, or the Dolma toolkit~\footnote{\url{https://github.com/allenai/dolma}}~\citep{dolma} for their ease of use.
% high-level notes/takeaway
In practice, multiple steps of deduplication are performed. First, simple deduplication can be performed (e.g. URL-based filtering), followed by more complicated hashing- and model-based methods.
Practitioners should always determine whether duplicated data will harm or help the model for their use case. While memorization is commonly cast as a bad thing in machine learning, it can be a positive such as when a model ``memorizes'' the answer to a factual question \citep{biderman2024emergent}.

\subsection{Data Decontamination}
% Motivation
Data decontamination is the process of removing evaluation data from the training dataset. This important step in data preprocessing ensures the integrity of model evaluation, ensuring that metrics are reliable and not misleading.

Prior to training a model on a dataset, it is important to decontaminate that dataset from the desired evaluation datasets.
BigCode~\footnote{\url{https://github.com/bigcode-project/bigcode-analysis/tree/main/data_analysis/decontamination}} and Carper AI~\footnote{\url{https://github.com/CarperAI/decontamination/tree/main}} both implement contamination detection through the use of MinHashLSH, which can be used to detect contamination prior to training a model.
One concern with some modern models is that the model developers may not disclose their training data, so methods and tools have been developed to determine whether a model was trained on a specific dataset. For example canary strings, unique sequences of characters, can be included in training datasets, and \citet{jagielski2023note} explain how to interpret canary exposure, which can identify whether a model was trained on the specified dataset. One example of canary strings can be found in the BIG-bench dataset.~\footnote{\href{https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/training_on_test_set/README.md\#training-on-the-test-set}{https://github.com/google/BIG-bench/.../training\_on\_test\_set/README.md\#training-on-the-test-set}}
In addition to canary strings,~\citet{shi2023detecting} propose Min-K\% probability, a method for finding possible contamination.~\footnote{\url{https://github.com/swj0419/detect-pretrain-code}}

One important note for practitioners who are performing decontamination prior to training is to see data deduplication methods for inspiration. For example, data deduplication methods that use exact matching (e.g. Bloom filters from Dolma) are also good candidates for decontamination.

\subsection{Data Auditing}
% Motivation
Auditing datasets is an essential component of dataset design. You should always spend a substantial amount of time reading through your dataset, ideally at many stages of the dataset design process. Many datasets have problems specifically because the authors did not do sufficient auditing before releasing them.
The tools outlined in the data search, analysis, \& exploration section ae typically sufficient to track the evolution of a dataset as it's being created. However, there are also tools that can be used to audit previously created datasets.

% List of tools
The Data Provenance Initiative~\footnote{\url{https://www.dataprovenance.org/}}~\citep{longpre2023data} is a good resource that documents the source, license, creator, and other metadata for over 1,800 text finetuning datasets. The Have I Been Trained?~\footnote{\url{https://haveibeentrained.com/}} tool can assist in finding and detecting data within LAION datasets. See the blog post by~\citet{HuggingFaceCommunityBlog} for more details on auditing datasets.


% high-level notes/takeaway


% Recommendations (we have those already at the section level, but maybe they can be improved)
% \subsection{Recommendations}


% Recommendations

% Data preparation is a step of model development that can require a lot of thought, and purposeful decision-making. Depending on the goal of the dataset, the exact steps taken in each of the above steps may look very different. Therefore, it is crucial to take time to determine the best steps for how to prepare and design a dataset that will satisfy the desired use case.

% Some steps, such as deduplication, will be quite similar for all use cases. Yet others, such as cleaning, filtering, and mixing will be unique to the desired setting. For example, designing a training dataset for a code foundation model will require different filtering strategies than a training dataset for a natural language foundation model, which will be different from a multimodal foundation model.

% One of the most important recommendations from this section is that dataset design is an iterative process. Practitioners should alternate between exploring and analyzing a dataset, followed by cleaning and deduplicating. Iteratively analyzing and cleaning allows for the final dataset to be of the highest quality.


\subsection{Review}

In this section we consider, evaluate, and critically review the current state of resources for data preparation.

% More data exploration tools
\paragraph{The community stands to benefit greatly from increased efforts on open-source data exploration tools.}
% data exploration tools are a critical part of the iterative process discussed above
As we've discussed in this section, tools for data exploration and analysis are a crucial component of the iterative process of creating a dataset, however, the openly available tools for exploring data are limited.
% Improves our understanding of newly created datasets, helps us know the quality of the data for our purpose
Specifically, the existing tools can search for n-grams, show random samples from the dataset, and return high-level statistics of the dataset.
However, there are many additional features that may be useful. For example, after searching for an n-gram in a corpus, it may be helpful to see the containing documents to understand when this n-gram occurs and what the surrounding context is.

% Retrospective analysis of datasets (use LAION 5B as an example?)
\paragraph{Open-source data exploration tools allow for retrospective analysis of datasets.}
Additionally, improving the functionality and ease-of-use for data exploration tools can help to retrospectively analyze existing datasets. For example, these tools can be used to find potential issues such as biases, illegal or copyrighted content, and personally identifiable information, as well as to ensure that they contain the advertised content (e.g. alignment datasets should contain safe text).

% improving infrastructure
\paragraph{Consolidating on a standardized format for data storage and processing will give developers more time to focus on developing infrastructure.}
Furthermore, many of the data preparation tools and methods presented here exist in separate one-off repositories.
This has led to limited open-sourced efforts on large-scale infrastructure.
Conglomerating around a limited number of data formats can reduce friction by allowing developers to make assumptions on the format of data, thus enabling developers to focus efforts on developing scalable tooling for data preparation.
For example, the use of Apache Arrow in HuggingFace Datasets~\citep{lhoest-etal-2021-datasets} and Numpy's \texttt{memmap} in GPT-NeoX~\citep{gpt-neox-library} has reduced the effort required for developers to develop memory-efficient data loading, allowing for developers to focus on building scalable tooling.
Similarly, using a single data format, such as that from Dolma~\citep{soldaini2024dolma}, can allow developers to focus on building better large-scale data infrastructure, and spend less time writing complicated code that considers multiple data formats.

% Need better methods for determining ``high-quality'' data, as well as a more refined understanding of what high-quality data even is
\paragraph{Taking a fine-grained view on ``high-quality'' data.}
Referring to data as ``high-quality'' was originally used in the context of pretraining data, but has since become more widely adopted without a clear definition, leading to significant ambiguity in the community. In the context of cleaning and filtering web data, high-quality has referred to data that is known to have been written by humans, and has likely gone through an editing process, leading to the development of quality filters which aim to find data most similar to domains such as books or Wikipedia~\citep{browngpt3, chowdhery2023palm}. However, the exact definition of quality has since expanded to pretraining domains beyond web data (e.g. code, reasoning), and the phrase has been adopted in other training regimes such as preference fine-tuning. The field of foundation model development does not currently have a clear definition of what data leads to high-quality models, and the development of such definitions is a high-impact direction of research and engineering. While research on data quality for pretraining may be out of reach for smaller institutions and individual researchers, research on data quality for specific downstream use cases (e.g. code, reasoning, math) is more feasible.

For this reason, we advocate for research on data quality to be contextualized within a specific domain or set of evaluations. This will not only allow for research to progress in parallel across many groups and institutions, but we believe will also lead to definitions of quality that are much more concrete, precise, and definitive.
Furthermore, the development of clear definitions lowers the barrier for creating tools that can find additional ``high-quality'' data.

% More data-centric benchmarks
\paragraph{Developing data-centric benchmarks can catalyze progress.}
Data-centric research is currently being done across such a wide variety of settings, and with varying goals, that it has become nearly impossible to compare methods~\citep{albalak2024survey}. Some recent benchmarking works have tried to address this by providing a fixed model training setup, and requiring competitors to improve the data for training.
Specifically, DataComp~\citep{gadre2023datacomp} provides a data-centric benchmark focused on image-text pairs, DataPerf~\citep{mazumder2023dataperf} provide benchmarks for 4 settings: vision, speech, debugging, and language. Additionally for the language domain, FETA~\citep{albalak-etal-2022-feta} provides a benchmark for few-shot task transfer, and the Loose track from BabyLM~\citep{warstadt-etal-2023-findings} provides researchers a benchmark for better data selection.
However, due to the large number of training settings and domains of interest, there are a wide variety of additional benchmarks and challenges that would be useful for the community (e.g. pretraining, instruction tuning, alignment, task-specific fine-tuning). Creating new data-centric benchmarks will not only allow for direct comparison between methods, improving our understanding of the data preparation methods, but also lowers the bar for entry into the field by creating an easy-to-use infrastructure, enabling increased progress.
% Need to consider whether performance on smaller scales will transfew to large scale (model size, dataset size)

% Expand tools to non-English and low-resource languages
\paragraph{Data preparation tools should consider not only English-centric data, but non-English and low-resource languages.}
While some data preparation tools may work out-of-the-box for low-resource languages (e.g. n-gram search), others may require more thought and effort (e.g. heuristic filtering). It is particularly important to include native speakers for low-resource languages throughout the data preparation process.
