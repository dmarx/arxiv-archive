\section{Model Release \& Monitoring}
\label{sec:model-release}
\vspace{-2mm}

% https://www.latex4technics.com/?note=MOB
% \begin{tcolorbox}[width=\textwidth,title={Data Preparation Best Practices}]
\begin{tcolorbox}[
    width=\textwidth,
    title={Model Release \& Monitoring Best Practices},
    colback=backgroundcol, % Background color of the box
    colframe=darkgray, % Frame color
    colbacktitle=dataprep, % Background color of the title
    coltitle=white, % Title text color
    coltext=black % Text color
]

\begin{itemize}[itemsep=0pt, wide=3pt]
    \item Release models with accompanying, easy-to-run code for inference, and ideally training and evaluation.
    \item Document models thoroughly to the extent possible. Model documentation is critical to avoiding misuse and harms, as well as enabling developers to effectively build on your work.
    \item Open source is a technical term and standard with a widely accepted definition that is maintained by the Open Source Initiative (OSI) \citep{OSI2024def}. Not all models that are downloadable or that have publicly available weights and datasets are open-source; open-source models are those that are released under a license that adheres to the OSI standard. 
    \item The extent to which ``responsible use licenses'' are legally enforceable is unclear. While licenses that restrict end use of models may prevent commercial entities from engaging in out-of-scope uses, they are better viewed as tools for establishing norms rather than binding contracts.
    \item Choosing the right license for an open-access model can be difficult. Apache 2.0 is the most common open-source license, while responsible AI licenses with use restrictions have seen growing adoption. Consider using one of the available tools for selecting the right open-source license for your model artifacts.
    \item Frameworks for monitoring and shaping model usage have become more prevalent as policymakers have attempted to constrain certain end uses of foundation models. Several approaches include adverse event reporting, watermarking, and restricting access to models in limited ways. Consider providing guidance to users on how to use your models responsibly and openly stating the norms you hope will shape model use.
\end{itemize}
\end{tcolorbox}

% \subsection{Survey}

\subsection{Model Documentation}

When models, code or applications are released, whether openly or not, it is important that they are documented thoroughly. Documentation should specify how to use the model, recommended and non-recommended use cases, potential harms, state or justify decisions made during training, and more. Documenting models is important not just for responsible development, but also to enable other developers to effectively build on a model. Models are not nearly as useful as artifacts if not properly documented. Model cards~\citep{mitchell2019model} are widely adopted standard for documenting models. Several tools have been developed that support the creation of model cards\footnote{https://huggingface.co/blog/model-cards}. 

\subsection{Reproducibility}

Code to reproduce results are an important complement to other forms of documentation~\cite{Kapoor2023REFORMSRS}. Releasing assets that reproduce results mean that scientific claims can be verified, and that systems can be interrogated, tested and audited  Missing, incomplete, or poorly documented code hinders progress.  There are tools that help make model training, inference and evaluation reproducible. \href{https://www.anaconda.com/}{Anaconda} and \href{https://www.docker.com/}{Docker} make necessary environments and dependencies easier to manage. \href{https://colab.research.google.com/}{Google Colab} and \href{https://jupyter.org/}{Jupyter notebooks} enable easily shareable code snippets and organized tutorials. The Language Model Evaluation Harness provides a framework for prompting and testing generative language models on a large number of different evaluation tasks~\citep{eval-harness}.

\subsection{Licensing}

Licensing is a mechanism creating a legally enforceable agreement that governs use of artifacts. Licenses with use restrictions can be used to limit the ability of certain categories of stakeholders to re-use or adapt the models~\citep{contractor2022behavioral,FSF2024what}. The MIT and Apache 2.0 licenses are the most commonly used. 
Responsible AI Licenses, including \href{}{BigScience's Open RAIL}, have seen growing adoption. However these also face criticism around how they pose challenges for well-intentioned actors and because their enforceability remains an open question \citep{downing2023licensing}. While RAIL licenses that restrict end use of models may prevent commercial entities from engaging in out-of-scope uses, they are better viewed as tools for establishing norms rather than binding contracts.

\subsection{Usage Monitoring}

Some open foundation model developers attempt to monitor the usage of their models, whether by watermarking model outputs or gating access to the model. The cheatsheet provides resources related to usage monitoring, including examples of how to watermark content, guidance on appropriate use, guidance on reporting adverse events associated with model use\footnote{E.g. \url{https://www.microsoft.com/en-us/photodna}}, and ways to limit some forms of access to models. 
Several of these approaches have significant drawbacks: for example, there are no known robust watermarking techniques for language models and there are limits to watermarking for image models \citep{kirchenbauer2023watermark,saberi2023robustness}. As with many of the sections above, usage monitoring remains an area of active research. 

\subsection{Recommendations}
Models should be released with accompanying documentation \emph{and} easy-to-run code for training, evaluation and inference. Document model thoroughly to the extent possible. These are critical to avoiding misuse and harms and enabling developers to effectively build on your work. A well documented environment, code, and versions of the appropriate datasets.

Be thoughtful about the type of license to use for artifacts. Open source is a technical term and standard with a widely accepted definition \citep{OSI2024def}. If there is a risk of misuse then consider behavioral restrictions from a standardized tool \citep{mcduff2024standardization}.

Frameworks for monitoring and shaping model usage have become more prevalent as policymakers have attempted to constrain certain end uses of foundation models. Several approaches include adverse event reporting, watermarking, and restricting access to models in limited ways. Consider providing guidance to users on how to use your models responsibly and openly stating the norms you hope will shape model use.

\subsection{Review}

In this section we critically review the current state of resources for model release and monitoring, from our survey. 

% Model Release and Monitoring Review -- looks way too slim compared to everything else. Could someone flesh it out a little more? I would add the following: discussion of model fingerprinting, PhotoDNA, the overarching challenges in monitoring and watermarking, and the importance not just of paying attention to licensing, but providing thorough transparency/documentation on the legal permissions of the ingredients into the final model: the data licenses, terms of services (if API generated), the base models' license(s), etc.. (someone?)

\textbf{Reproducibility, especially through executable code, benefits all parties.} The research community has benefited substantially from openness and transparency in how foundation model artifacts are documented and released. 
As have proprietary developers who benefit from the rapid prototyping and innovations on their released technical systems, propelled by the open community.
Our review of resources suggests that often the most widely adopted tools are those that are not just well documented/described, but also those that are easy to run with executable code.

\textbf{Usage monitoring remains challenging, and offers both advantages and disadvantages.}
Existing tools such as watermarking for AI outputs \citep{kirchenbauer2023watermark,saberi2023robustness} or model fingerprinting \citep{xu2024instructional} offer some degree of verification regarding the source of data or models.
However, their robustness to adversarial removal, or to detection, remain dubious---this is particularly true for text data.
This can result in over-confidence that these methods provide a panacea, and instead result in false positives.
There are also open questions on privacy as to the right for individuals to produce content without attribution.
We hesitate to prescribe these nascent solutions broadly, without fully understanding the particular context under consideration for the data, the model, and their potential uses and abuses.

\textbf{Permissions and restrictions around model use should be more explicit about their data provenance, and the other upstream ingredients which may impact use intentions, consent, and permissions.}
The ethical, responsible or legal use of a model may depend on a series of upstream factors, beyond the final developers' license. 
This could include the license of the datasets used for training, the terms of service attached to those data sources (e.g. if some data is synthetically generated), or the license of the base model (if finetuned).
The norms, best practices, and legal relevance of each of these factors is evolving, jurisdiction-dependent, and beyond the scope of this cheatsheet.
However, we suggest developers document these factors in their model releases, beyond their own license, such that downstream developers and users can make informed decisions.



%We include frequently-used standards for model documentation as well as tools for easy following of standards and creation of documentation.



% \subsection{Model Documentation}
% \vspace{-2mm}

% It is important to document models that are used and released. Even models and code released openly are important to document thoroughly, in order to specify how to use the model, recommended and non-recommended use cases, potential harms, state or justify decisions made during training, and more. 

% Documenting models is important not just for responsible development, but also to enable other developers to effectively build on a model. Models are not nearly as useful as artifacts if not properly documented.

% We include frequently-used standards for model documentation as well as tools for easy following of standards and creation of documentation. 

% \input{tables/model-documentation-1}
% \vspace{-2mm}

% \subsection{Reproducibility}
% \vspace{-2mm}

% Model releases often go accompanied with claims on evaluation performance, but those results are not always reproducible, or can be misleading \citep{Kapoor2023REFORMSRS}.
% If code is not released, is not comprehensive, is difficult to run, or misses key details, this will cost the scientific community time and effort to replicate and verify the claims.
% Replication time will also slow progress, and discourage developers from adopting that resource over others.

% For these reasons, we strongly recommend carefully curating code, for model training, inference and evaluation.
% Reproducible code begins with clear dependencies, versioning, and setup scripts, that are easy to adopt even if the tools and frameworks are unfamiliar.
% Clear documentation, code legibility and scripts for each entry point also improve ease of adoption.
% Notably, Colab Notebooks provide shareable environment setup and execution tools.
% These measures will significantly improves scientific reproducibility, and transparency.

% \input{tables/code-reproducibility-1}
% \vspace{-2mm}

% \subsection{License Selection}
% \vspace{-2mm}

% Foundation models, like software, are accompanied by licenses that determine how they may be distributed, used, and repurposed. There are a variety of licenses to choose between for open foundation model developers, presenting potential challenges for new developers. The table below includes resources that can help guide developers through the process of selecting a specific license for their model as well as several examples of licenses that include use restrictions. While licenses with use restrictions may be appropriate for certain types of models, in other cases use restrictions can limit the ability of certain categories of stakeholders to re-use or adapt the models \citep{FSF2024what}.

% % One particularly important type of model license to discuss is so-called responsible AI licenses such as BigScience's Open RAIL and AI2's ImpACT Licenses. These, and other licenses like them, have been criticized \citep{downing2023licensing}. It's also worth noting that it is frequently \textit{prima facie} impossible to comply with responsible AI licenses. All OpenRAIL licenses, for example, include the clause that the model cannot be used ``[i]n any way that violates any applicable national, federal, state, local or international law or regulation,'' but global legal systems are not consistent enough for this to be possible.

% Responsible AI Licenses in particular, including BigScience's Open RAIL and AI2's ImpACT Licenses, have seen growing adoption, but also criticism of the difficulties they may pose even for well-intentioned actors seeking to comply with their requirements---especially in commercial applications---and because their enforceability still remains an open question \citep{downing2023licensing}. However standardization of licenses has been identified as something that could be important~\cite{mcduff2024standardization}. While they can provide a convenient way to help a developer express their understanding of their model's limitations, in conjunction with a model card that outlines in-scope and out-of-scope uses, adopters should also consider unintended consequences in limiting the scope of the follow-up research that may be conducted with the licensed artifacts. 
% % Responsible AI licenses can also induce unintuitive requirements, making compliance more challenging. In many U.S. states there are laws prohibiting teachers from disclosing information about LGBTQ people, HIV, or contraception, and in some states teachers are pro-actively required to provide false or misleading information about these topics. This has the net effect of banning any OpenRAIL model from being used in public school settings in Missouri or Florida, as there is no way to ensure that models used in public education settings comply with all state laws and also various anti-discrimination clauses.
% %Proponents of responsible AI licenses typically pitch them not as legally binding documents but rather as ways to \textit{discourage} undesirable use and to document and set forth value statements \textbf{[citation needed]}. According to \textbf{[citation needed]}, RAIL licenses have successfully prevent commercial applications on domains where they would have been inadvisable.
% Responsible AI licenses can act as a useful norm-setting and self-reflection tool, but users should be aware of their limitations and potential downsides, especially compared to established open-source software licenses.

% \input{tables/license-selection-1}
% \vspace{-2mm}

% \subsection{Usage Monitoring}
% \vspace{-2mm}

% Some open foundation model developers attempt to monitor the usage of their models, whether by watermarking model outputs or gating access to the model. 
% The table below includes resources related to usage monitoring, including examples of how to watermark content, provide guidance on appropriate use, report adverse events associated with model use, and limit some forms of access to models. 
% Several of these approaches have significant drawbacks: for example, there are no known robust watermarking techniques for language models and there are limits to watermarking for image models \citep{kirchenbauer2023watermark,saberi2023robustness}. As with many of the sections above, usage monitoring remains an area of active research. %last sentence could essentially say that watermarking is bad as of now but maybe the SOTA for image/audio will improve with time

% \input{tables/usage-monitoring-1}
% \vspace{-2mm}