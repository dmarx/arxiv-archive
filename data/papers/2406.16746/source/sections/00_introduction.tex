\section{Introduction}
\label{sec:introduction}

As the capabilities \citep{ustun2024aya, team2023gemini, gomez2024commandrplus, anthropic2024claude3, radford2023robust, sora2024} and market prospects \citep{vipra2023concentration,mcelheran2024ai} of artificial intelligence have quickly expanded, so have the communities of developers, scientists, and contributors who build foundation models \citep{bommasani2021opportunities}.
The fields' growth has spurred widespread adoption of many tools and resources used to build, deploy, evaluate, and govern large foundation models (FMs).
However, these nascent practices are often immature.
% \pl{My biggest comment is that when reading this, it's hard to anchor this - are we talking about GPT-4 level models, Llama 2, Pythia, random GPT-2 runs that GPU poor researchers train? Maybe give some examples? Because I would say that the top foundation models don't have that many people building them, and the demands for Google/OpenAI are just quite different than everyone else; but then isn't everyone else mainly fine-tuning?  Of course there are other modalities where you might want to train from scratch...anyway, these are the questions that are going through my head...might be good to provide some concrete anchors}
Many outstanding resources are neglected, in part for lack of discoverability, or awareness of good practices.
To address these gaps, we conduct a focused survey, not of scientific literature (which already exists for many FM development topics \citep{albalak2024survey, zhao2023survey, chang2023survey}), but of \emph{resources} for FM development such as datasets, software packages, documentation, guides, frameworks, and practical tools.
In particular, this resource curation is tailored to responsible development practices for newer, smaller, or mid-sized
 development teams.
Large FM development organizations, such as Google, OpenAI, or Meta, with substantial user bases, should adhere to more rigorous and product-specific best practices than outlined in this review.
% This survey of resources is complementary to traditional surveys of scientific literature, which already exist for much of the model development pipeline \citep{albalak2024survey, zhao2023survey, chang2023survey}.
We release the Foundation Model Development Cheatsheet, the repository of annotated tools for text, speech, and vision models, and open it for public contributions.%: \href{http://fmcheatsheet.org}{fmcheatsheet.org}.

For each phase of model development, our contributions are summarized as (i) a survey of relevant tools and resources, (ii) a synthesis of the literature's recommended practices and use of those tools, and (iii) a review of the limitations and omissions of existing resources.
The cheatsheet serves as a succinct guide of the survey and recommended practices, prepared \emph{by} foundation model developers \emph{for} foundation model developers.
The intended audience is a range of foundation model developers, including academic researchers, startup companies, research labs, who are pretraining from scratch, or simply finetuning, big and small.

Our survey and recommendations hope to bring wide attention to tools across several phases of development.
First, we suggest resources that support \emph{informed} data selection, processing, and understanding (\cref{sec:data,sec:data-prep}).
Data prepared without sufficient due diligence can lead to unintended consequences (e.g. risks to privacy, copyright, or generating sensitive content), marginalization (e.g. by inadvertently filtering out certain distributions), or unexpected model behaviors (e.g. train/test overlap or security vulnerabilities). 
Next we survey resources for \emph{precise and limitation-aware} artifact documentation (\cref{sec:documentation}).
When new datasets are released, setting their governance standards early will avoid misuse later.
% Standardizing and structuring documentation will encourage... for its properties to be easily preserved, understood, and respected in downstream data mixes or compositions.
Foundation model training can be financially and environmentally expensive.
We aggregate resources for \emph{efficient} model training (\cref{sec:model-training}) and estimating a model's scaling behavior and environmental impact (\cref{sec:environmental-impact}).
\emph{Advance awareness} of these quantities can inform more efficient training practices.
For once models are trained, we provide evaluation frameworks, taxonomies of risk, and benchmarks for a variety of evaluation criteria (\cref{sec:model-eval}).
Best practices suggest models should be evaluated for their intended uses, as well as some unforeseen misuses or harms.
Developers should design naturalistic evaluations for these settings rather than relying on available but poorly fitting tools \citep{biderman2024lessons,liao2023rethinking}.
And our evaluation frameworks suggest evaluation metrics should be contextualized, to avoid over-claiming or misunderstanding the limitations of the reported numbers.
Lastly, our survey informs \emph{responsible} model release and deployment practices (\cref{sec:model-release}), so developers can make informed selections of licenses and release mechanisms, to address misuse risks.

Beyond the survey of resources, we review the existing ecosystem of tools and resources.
For each segment of model development, we examine the limitations, omissions, and opportunities for improvement of existing tooling and common practices.
Summarized in \cref{tab:review-summary}, we find:
\begin{itemize}[itemsep=1pt] %, wide=5pt]
    \item Tools for data sourcing, model evaluation, and monitoring are critically under-serving responsible development needs and real-world needs. For instance, they often fail to have sufficient documentation, imitate real use cases, or accurately reflect licensing permissions.
    \item Popular model safety, capabilities, and environmental impact evaluation benchmarks lack reproducibility and transparency.
    \item Resources for multilingual and multi-modal development, across every phase of development, continue to receive significantly less attention than English and text-centric equivalents.
    \item Resources to evaluate \emph{systems}, rather than just models, is needed so that capabilities and impact are assessed in the context of real-world deployment.
\end{itemize}

\input{tables/review2}
