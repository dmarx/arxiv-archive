\section{Data Sources}
\label{sec:data}
\vspace{-2mm}

% \citep{liu2024best}

% https://www.latex4technics.com/?note=MOB
% \begin{tcolorbox}[width=\textwidth,title={Data Preparation Best Practices}]
\begin{tcolorbox}[
    width=\textwidth,
    title={Data Sourcing Best Practices},
    colback=backgroundcol, % Background color of the box
    colframe=darkgray, % Frame color
    colbacktitle=dataprep, % Background color of the title
    coltitle=white, % Title text color
    coltext=black % Text color
]

\begin{itemize}[itemsep=0pt, wide=3pt]
    \item Pretraining data provides the fundamental ingredient to foundation models---including their capabilities and flaws. Finetuning data improves the model's performance in specific settings, or in the case of instruction finetuning or alignment training, improves the model's general usability and helpfulness while aiming to reduce potential harms.
    \item More data is not always better. It is essential to carefully source data, and manually inspect it to ensure it \emph{fits the goals of your project.}
    \item Dataset selection includes many relevant considerations, such as language and dialect coverage, topics, tasks, diversity, quality, and representation.
    \item Most datasets come with implicit modifications and augmentations, from their selection, filtering, and formatting. Pay attention to these pre-processing steps, as they will impact your model.
    \item Finetuning data can improve the model's performance in some settings or impair others. Use catalogs to support an informed selection, and prefer well-documented to under-documented datasets.
    \item Crowdsourced data catalogs, including HuggingFace, may contain important omissions and errors in their dataset documentation. Verify information, such as data licensing and critical characteristics, with the original data sources and academic papers, if possible.
    \item The most appropriate datasets may not exist for a given set of tasks. Be aware of the limitations of choosing from what is available. 
\end{itemize}
\end{tcolorbox}

\subsection{Pretraining Data Sources}

% \input{tables/pretraining-1}

Pretraining corpora consist of millions of pieces of content, from documents, images, videos, or speech recordings, often scraped directly from the web.
Model pretraining represents the fundamental step in instilling foundation models with their abilities to represent syntax, grammar, reasoning, and world knowledge \citep{devlin2018bert, browngpt3, chowdhery2023palm}.
Consequently, it is important to carefully curate the data composition, including the mix of sources, characteristics, and preprocessing decisions \citep{longpre2023data}.
However, the vast scale of this content often means its is shallowly documented and understood, despite community efforts to unpack it \citep{dodge2021documenting, elazar2023whats}.

We highlight a few of the most popular pretraining corpora which have accumulated deeper documentation and analysis.
In the text domain, web scrapes from common crawl (\url{commoncrawl.org}), or OSCAR (\url{https://oscar-project.org/}) \citep{suarez2019asynchronous, laippala2022towards} are the base ingredient for most pretraining corpora.
In particular, derivatives of common crawl, such as C4 \citep{raffel2020exploring,dodge2021documenting} or multilingual C4 \citep{kreutzer2022quality} provide 2019 indexes that have been heuristically filtered for well-formed text.
Subsequent pretraining datasets incorporate one or multiple indexes from common crawl.
This includes the Pile \citep{gao2020pile}, RefinedWeb \citep{penedo2023refinedweb}, RedPajama \citep{together_ai_2023_redpajama}, and Dolma \citep{soldaini2024dolma}, which have iterated on basic document quality filtering and deduplication.
These corpora often merge and deduplicate multiple years of common crawl scrapes, or supplement additional sources including biomedical text (PubMed), legal text (Freelaw, USPTO patent documents), code (Github, Stack Exchange), public domain books (Project Gutenberg), among others.

For multilingual text, the Open Parallel Corpus(OPUS) offers a massive collection of translated text document pairs \citep{tiedemann2012parallel}, ROOTS \citep{NEURIPS2022_ce9e92e3} collates and processes diverse multilingual resources, including OSCAR \citep{laippala2022towards} and the Bigscience Catalogue \citep{mcmillan2022documenting}, CulturaX \citep{nguyen2023culturax} covers 167 languages from OSCAR and mC4, and WURA \citep{oladipo-etal-2023-better} centralizes and manually audits documents from 16 African languages.
Most recently, MADLAD-400 \citep{kudugunta2023madlad} provides a 3 trillion token, 2023 processed split of Common Crawl, spanning 419 languages.

Large, specialized corpora of text have also recently emerged, to specialize model abilities, or mitigate risks of possible copyright infringement.
As examples, the Pile of Law \citep{henderson2022pile} and MultiLegalPile \citep{niklaus2023multilegalpile} centralize court opinions, contracts, and legislative records; the Stack and Stack v2 scrape permissively-licensed GitHub repositories \citep{kocetkov2022stack}; peS2o \citep{lo2020s2orc} releases cleaned academic papers from Semantic Scholar; and the Proof Pile 2 \citep{azerbayev2023llemma} and OpenWebMath \citep{paster2023openwebmath} aggregate vast mathematical text resources.
Notably, recent text corpora also \emph{attempt} to isolate permissively-licensed, or copyright free data, such as the Open License Corpus \citep{min2023SILOLM}---though this is a challenging task, and does not guarantee that all non-commercially licensed or copyrighted content has been removed.

In the context of speech, webscrapes of English audiobook data from LibriVox (a website hosting free public domain audiobooks) are commonly used for foundation model architecture development and evaluation. Sourcing data from LibriVox, LibriSpeech \citep{panayotov2015librispeech} is a 960 hour fully supervised dataset (i.e. all audio are paired with transcriptions) and Libri-Light \citep{kahn2020libri} is a 60k hour dataset for benchmarking using no or limited supervision (10h, 1h, and/or 10min). Multilingual models are typically pre-trained on a combination of speech corpora, e.g. for XLS-R \citep{babu22-interspeech} a combination 436k hours from VoxPopuli \citep[400k hours from 23 languages based on European Parliament recordings:][]{wang2021voxpopuli}, Multilingual LibriSpeech \citep[50k hours of audiobooks from 8 languages][]{pratap20-interspeech}, CommonVoice \citep[28k hours of crowd-sourced read speech from 100+ languages][]{ardila-etal-2020-common}, VoxLingua107 \citep[6.6k hours from 107 languages scraped from YouTube][]{valk2021voxlingua107}, and various IARPA BABEL corpora (totaling 1k hours from 17 African and Asian languages).

% In the context of vision, ...
In the context of vision, there are several large pretraining corpora, comprised of text and images found in large web scrapes.
COYO aggregates 700M images with alt-text from the web.\footnote{\url{https://huggingface.co/datasets/kakaobrain/coyo-700m}}
Multimodal C4, or MMC4, \citep{zhu2024multimodal} leverages the original C4 URLs to extract interleaved image and text pairs, totaling 570M images, and 43B tokens.
Similarly, OBELICS also leverages the Common Crawl web collection, filters for multimodal web pages, and extracts images from the HTML \citep{laurenccon2024obelics}, totaling 353M images, with 115B tokens.
Lastly, DataComp-1B and CommonPool-13B \citep{gadre2024datacomp} isolate high quality subsets of image-text pairs on CommonCrawl.
For video specifically, WebVid \citep{bain2021frozen} provides 10M videos and their text, from which image datasets can also be extracted.
For vision or speech, WebDatasets provides a high-performance data streaming tool.\footnote{\url{https://github.com/webdataset/webdataset}}



% \input{tables/pretraining-3}


% \input{tables/pretraining-2}





\subsection{Finetuning Data Catalogs}
\label{sec:finetune}
\vspace{-2mm}

In this section, we survey sources that catalog finetuning datasets---sometimes known more broadly post-training datasets.
Finetuning data, differs from pretraining data in that it is curated for supervised learning, is more specialized to the intended inference-time task, and is typically much smaller in scale.
Finetuning datasets are used for a variety of reasons: to hone specific capabilities, orient the model to a certain task format, improve its responses to general instructions, mitigate harmful or unhelpful response patterns, or generally align its responses to human preferences.
Developers increasingly vary the types of data annotations and loss objectives, depending on the goal.
Notably, after pretraining practitioners commonly use traditional supervised finetuning, DPO \citep{rafailov2023direct} or reinforcement learning objectives from human (or machine) feedback to generated responses \citep{ouyang2022training, bai-constitutional-2022}.

Given the thousands of specialized data sources for finetuning, we recommend using data catalogs that provide well documented datasets, to make for an informed selection.
HuggingFace Datasets (\url{https://huggingface.co/docs/datasets/index}) offers the largest and most popular AI community catalog across modalities and tasks \citep{lhoest2021datasets}.
Many datasets are accompanied by data cards, however \citet{longpre2023data} show there are frequent omissions and errors, as is the nature of crowdsourced catalogs.

There exist an array of other specialized data catalogs with datasets that may not appear directly on HuggingFace.
For instance, the NusaCrowd catalog for South East Asian languages \citep{cahyawijayanusacrowd}, the Masader Arabic data catalogue \citep{alyafeai2022masader}, the AI4Barat Indian data catalog (\url{https://huggingface.co/ai4bharat}), as well as the Maskahane NLP (\url{https://github.com/masakhane-io}) and Zenodo AfricaNLP catalogs (\url{https://zenodo.org/communities/africanlp/}) offer specialized multilingual text and speech resources for the language communities.
Recently, the Aya Collection \citep{singh2024aya} offers curated, multilingual text finetuning datasets across 65 languages.
For more accurate and comprehensive data documentation, particularly for licensing and provenance, the Data Provenance Initiative publishes tools to search and filter popular HuggingFace finetuning datasets (text, speech, and vision) across a variety of criteria \citep{longpre2023data}.
\citet{liu2024best} recommend best practices in developing and using synthetic data, which has become increasingly popular in the text domain.

In the context of speech, OpenSLR (\url{http://openslr.org}) is a large collection of user-contributed datasets for various speech processing tasks. For the task of spoken language identification, VoxLingua107 \citep{valk2021voxlingua107} comprises audio scraped from YouTube using various language-specific keywords and, analogously, for speaker identification/verification VoxCeleb \citep{nagrani17-interspeech} comprises audio of from 1,000 celebrities.

% In the context of vision, ...
In the context of vision, there are a few well known data sources for finetuning.
ImageNet \citep{5206848} provides the historical framework for which image classification models competed on 1.3M samples with 1000 diverse classes.
MS COCO \citep{lin2014microsoft} provides training data for image detection, segmentation, captioning and retrieval.
There exist many options for modern instruction finetuning datasets in the vision domain.
A few notable examples include the Multi-Modal, Multilingual Instruction Tuning (M3IT) dataset \citep{li2023m}, comprising 40 datasets, 2.4 million examples over 400 tasks in 80 languages; The Cauldron, comprising 50 vision-language datasets \citep{laurenccon2024matters}; and LLaVA Visual Insruct 150k, a set of text-image datasets generated by prompting the GPT-4-0314 API \citep{liu2024visual}.
For using web-scraped data, Child and Sexual Abuse Material (CSAM) is an acute concern. Tools such as PhotoDNA can be used to help detect and filter for these images, though they may not be completely accurate or comprehensive.\footnote{\url{https://www.microsoft.com/en-us/photodna}}

% \subsection{Recommendations}

% \input{tables/finetuning-1}
% \vspace{-2mm}

\subsection{Review}
\label{sec:data-review}

In this section we critically review the current state of resources for data sourcing, from our survey. 

\textbf{The community would benefit from more accurate and comprehensive licensing, provenance, and creator consent information for existing datasets.}
Many datasets tend to be under-documented \citep{bandy2021addressing, sambasivan2021everyone}, or erroneously documented \citep{longpre2023data}, with 65\% of HuggingFace dataset licenses either omitted or incorrectly labelled.
This is especially true of large data collections that have re-packaged and sometimes re-licensed hundreds of diverse datasets, each with different documentation standards \citep{longpre2023flan, sanh2021multitask}.
The tools used to discover, select, and verify the dataset properties are under-developed, especially with respect to concerns of creator consent, copyright infringement, and related terms of use.
For creator consent, initial opt-in/opt-out tooling has yet to be widely adopted.
While HuggingFace has integrated Spawning's opt-in database (\url{https://api.spawning.ai/spawning-api}), there remains limited creator and developer adoption.
For copyright information new datasets and catalogs such as the Data Provenance Initiative \citep{longpre2023data}, offer more detailed license tracing tools, but their coverage also remains limited.
Lastly, synthetic data generation (usually using OpenAI APIs) has expanded significantly for finetuning datasets \citep{longpre2023data}, but don't always document the limitations on the data use imposed by the APIs. This is further compounded by the fact that there is substantial uncertainty about the extent to which the terms of service of such platforms bind downstream users.
The absence of infrastructure to trace and verify these types of data documentation lead to uninformed data sourcing and use.

\textbf{The community would benefit from more accurate and comprehensive information on sensitive content, such as CSAM and NCII.}
Prior work highlights the risks of proliferated CSAM and NCII as a fundamental risk from generative AI models \citep{kapoor2024societal, lakatos-revealing-2023, thiel-generative-2023}.
However, significant portions of the AI community have frequently trained on large-scale datasets that contain this sensitive content without widespread awareness---such as LAION-5B \citep{birhane2021multimodal, David2023AIDatasetCSAM}.
With a greater focus on generative, multimodal, and more memorization-prone large models, there is a greater need for resources to help identify and filter for sensitive content.

% \TODO{Gabriel/Nay: for the next two review paragraphs, can you review and add speech/vision examples + citations?}
\textbf{Data \& modeling have focused predominantly on easy-to-scale data formats, neglecting other formats.}
A prominent focus of multimodal modeling has been image-to-caption and caption-to-image tasks.
The ease of sourcing and scaling these caption datasets to tens of billions has enabled these tasks to progress.
However, this has neglected more complex and useful reasoning tasks, that may require text and images with different relationships, interleaved in different ways.
In the context of speech, many automatic speech recognition datasets comprise only of read speech (e.g. as collection involved soliciting crowd-sourced participants to read various text stimuli), which is vastly different from informal, multi-speaker conversations which are the ``primordial home of human language`` \citep{dingemanse2022text}.

\textbf{There is a scarcity of realistic training tasks, and naturalistic observations.}
Many open academic datasets are developed for niche, even artificial purposes (e.g. academic visual question datasets that are often detached from real world use cases).
To collect realistic use cases, however, requires access to volunteered user logs from real products and services.
Some attempts have been made to do this, such as WildChat \citep{zhao2023inthe}, however their participation may still skew to a superficial distribution.
Tools and even policy to scalably source real data, while preserving privacy, is critical to sourcing grounded and relevant training data.

