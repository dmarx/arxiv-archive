\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule

    \multicolumn{4}{c}{\textsc{\emph{Pretraining Repositories}}} \\
    \midrule

\TextCircle\EmptyCircle\EmptyCircle & \textbf{GPT-NeoX} & A library for training large language models, built off Megatron-DeepSpeed and Megatron-LM with an easier user interface. Used at massive scale on a variety of clusters and hardware setups~\citep{gpt-neox-library}. & \emojiblank\emojiblank\href{https://github.com/EleutherAI/gpt-neox}{\egithub}\emojiblank \\

% \TextCircle\EmptyCircle\EmptyCircle & \textbf{Levanter} & A framework for training large language models (LLMs) and other foundation models that strives for legibility, scalability, and reproducibility. & \emojiblank\href{https://huggingface.co/stanford-crfm}{\ehf}\href{https://github.com/stanford-crfm/levanter}{\egithub}\href{https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html	}{\eweb} \\

% \TextCircle\EmptyCircle\EmptyCircle & \textbf{Megatron-LM} & One of the earliest open-source pretraining codebases for large language models. Still updated and has been used for a number of landmark distributed training and parallelism research papers by NVIDIA \citep{shoeybi2020megatronlm}. & \emojiblank\emojiblank\href{https://github.com/NVIDIA/Megatron-LM}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Megatron-DeepSpeed} & A library for training large language models, built off of Megatron-LM but extended by Microsoft to support features of their DeepSpeed library \citep{smith2022using}. & \emojiblank\emojiblank\href{https://github.com/microsoft/Megatron-DeepSpeed}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{OpenLM} & OpenLM is a minimal language modeling repository, aimed to facilitate research on medium sized LMs. They have verified the performance of OpenLM up to 7B parameters and 256 GPUs. They only depend only on PyTorch, XFormers, or Triton ~\citep{open_lm}. & \emojiblank\emojiblank\href{https://github.com/mlfoundations/open_lm}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{Kosmos-2} & For training multimodal models with CLIP backbones \citep{peng2023kosmos2}. & \href{https://arxiv.org/abs/2306.14824}{\earxiv}\href{https://huggingface.co/spaces/ydshieh/Kosmos-2}{\ehf}\href{https://github.com/microsoft/unilm/tree/master/kosmos-2}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{OpenCLIP} & Supports training and inference for over 100 CLIP models \citep{ilharco_gabriel_2021_5143773}. & \emojiblank\emojiblank\href{https://github.com/mlfoundations/open_clip}{\egithub}\emojiblank \\
\EmptyCircle\VisionCircle\EmptyCircle & \textbf{Pytorch Image Models (timm)} & Hub for models, scripts and pre-trained weights for image classification models. & \emojiblank\emojiblank\href{https://github.com/huggingface/pytorch-image-models}{\egithub}\emojiblank \\
\EmptyCircle\EmptyCircle\SpeechCircle & \textbf{Lhotse} & Python library for handling speech data in machine learning projects. & \emojiblank\emojiblank\emojiblank\href{https://github.com/lhotse-speech/lhotse}{\eweb} \\
\EmptyCircle\EmptyCircle\SpeechCircle & \textbf{Stable Audio Tools} & A codebase for distributed training of generative audio models. & \emojiblank\emojiblank\href{https://github.com/Stability-AI/stable-audio-tools}{\egithub}\emojiblank \\
\bottomrule
\end{tabular}
\end{table}