\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule

    \multicolumn{4}{c}{\textsc{\emph{Model Training}}} \\
    \midrule

\TextCircle\EmptyCircle\EmptyCircle & \textbf{Training Compute-Optimal Language Models} & Proposes an optimal allocation of computational budget between model and dataset size, and shows experimental design for fitting scaling laws for compute allocation in a new setting. & \href{https://arxiv.org/abs/2203.15556}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Transformer Inference Arithmetic} & A blog post on the inference costs of transformer-based LMs. Useful for providing more insight into deep learning accelerators and inference-relevant decisions to make when training a model. & \emojiblank\emojiblank\emojiblank\href{https://kipp.ly/transformer-inference-arithmetic/}{\eweb} \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{nanoGPT} & A minimal, stripped-down training codebase for teaching purposes and easily-hackable yet performant small-scale training. & \emojiblank\emojiblank\href{https://github.com/karpathy/nanoGPT}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Axolotl} & A repository for chat- or instruction-tuning language models, including through full fine-tuning, LoRA, QLoRA, and GPTQ. & \emojiblank\emojiblank\href{https://github.com/OpenAccess-AI-Collective/axolotl}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Transformer Math 101} & An introductory blog post on training costs of LLMs, going over useful formulas and considerations from a high to low level & \emojiblank\emojiblank\emojiblank\href{https://blog.eleuther.ai/transformer-math/}{\eweb} \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Scaling Data-Constrained Language Models} & Demonstrates an optimal allocation of compute when dataset size is bounded & \href{https://arxiv.org/abs/2305.16264}{\earxiv}\href{https://huggingface.co/datablations}{\ehf}\href{https://github.com/huggingface/datablations}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{GPT-NeoX} & A library for training large language models & \emojiblank\emojiblank\href{https://github.com/EleutherAI/gpt-neox}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Megatron-DeepSpeed} & A library for training large language models & \emojiblank\emojiblank\href{https://github.com/microsoft/Megatron-DeepSpeed}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{OpenCLIP} & Supports training and inference for over 100 CLIP models & \emojiblank\emojiblank\href{https://github.com/mlfoundations/open_clip}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{BLIP-2} & Fine-tuned LLMs on multimodal data using a projection layer & \href{https://arxiv.org/abs/2301.12597}{\earxiv}\emojiblank\href{https://github.com/salesforce/LAVIS/tree/main/projects/blip2}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{OpenFlamingo} & Open source implementation of Flamingo & \href{https://arxiv.org/abs/2308.01390}{\earxiv}\href{https://huggingface.co/openflamingo}{\ehf}\href{https://github.com/mlfoundations/open_flamingo}{\egithub}\href{https://laion.ai/blog/open-flamingo-v2/}{\eweb} \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{LLaMA-Adapter} & Fine-tuned LLMs on multimodal data using adapters & \href{https://arxiv.org/abs/2304.15010}{\earxiv}\emojiblank\href{https://github.com/OpenGVLab/LLaMA-Adapter}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{Otter} & Multimodal models with Flamingo architecture & \href{https://arxiv.org/abs/2311.04219}{\earxiv}\href{https://huggingface.co/spaces/Otter-AI/OtterHD-Demo}{\ehf}\href{https://github.com/Luodian/Otter}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{MiniGPT4} & Fine-tuned LLMs on multimodal data using a projection layer & \href{https://arxiv.org/abs/2304.10592}{\earxiv}\href{https://huggingface.co/spaces/Vision-CAIR/minigpt4}{\ehf}\href{https://github.com/Vision-CAIR/MiniGPT-4}{\egithub}\href{https://minigpt-4.github.io/}{\eweb} \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{LLaVA} & Fine-tuned LLMs on multimodal data using a projection layer & \href{https://arxiv.org/abs/2310.03744}{\earxiv}\href{https://huggingface.co/spaces/badayvedat/LLaVA}{\ehf}\href{https://github.com/haotian-liu/LLaVA}{\egithub}\href{https://llava-vl.github.io/}{\eweb} \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{Kosmos-2} & Multimodal models with CLIP backbones & \href{https://arxiv.org/abs/2306.14824}{\earxiv}\href{https://huggingface.co/spaces/ydshieh/Kosmos-2}{\ehf}\href{https://github.com/microsoft/unilm/tree/master/kosmos-2}{\egithub}\emojiblank \\
\EmptyCircle\VisionCircle\EmptyCircle & \textbf{Pytorch Image Models (timm)} & Hub for models, scripts and pre-trained weights for image classification & \emojiblank\emojiblank\href{https://github.com/huggingface/pytorch-image-models}{\egithub}\emojiblank \\
\EmptyCircle\EmptyCircle\SpeechCircle & \textbf{Lhotse} & Python library for handling speech data in machine learning projects & \emojiblank\emojiblank\emojiblank\href{https://github.com/lhotse-speech/lhotse}{\eweb} \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{QLoRa} & An efficient finetuning approach that reduces memory usage while training. & \href{https://arxiv.org/abs/2305.14314}{\earxiv}\emojiblank\href{https://github.com/artidoro/qlora}{\egithub}\emojiblank \\

\bottomrule
\end{tabular}
\end{table}

