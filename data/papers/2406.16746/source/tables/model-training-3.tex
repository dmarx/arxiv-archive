\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 

\midrule\multicolumn{4}{c}{\textsc{\emph{Efficiency and Resource Allocation}}}\\\midrule

\TextCircle\VisionCircle\SpeechCircle & \textbf{Cerebras Model Lab} & A calculator to apply compute-optimal scaling laws for a given budget, including factoring expected total inference usage. & \emojiblank\emojiblank\emojiblank\href{https://www.cerebras.net/model-lab/}{\eweb} \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{QLoRa} & An efficient finetuning approach that reduces memory usage while training \citep{dettmers2023qlora}. & \href{https://arxiv.org/abs/2305.14314}{\earxiv}\emojiblank\href{https://github.com/artidoro/qlora}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Scaling Data-Constrained Language Models} & Demonstrates an optimal allocation of compute when dataset size is bounded \citep{muennighoff2023scaling}. & \href{https://arxiv.org/abs/2305.16264}{\earxiv}\href{https://huggingface.co/datablations}{\ehf}\href{https://github.com/huggingface/datablations}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Training Compute-Optimal Language Models} & Proposes an optimal allocation of computational budget between model and dataset size, and shows experimental design for fitting scaling laws for compute allocation in a new setting \citep{hoffmann2022training}. & \href{https://arxiv.org/abs/2203.15556}{\earxiv}\emojiblank\emojiblank\emojiblank \\

\bottomrule
\end{tabular}
\end{table}