\begin{table*}[t!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.5} % Adjusting vertical space between rows
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{m{2.5cm} m{7cm} m{7cm}} % Changed from p to m for vertical centering
\toprule
\textsc{Development Phase} & \textsc{Frequent Status Quo} & \textsc{Review \& Recommendations}\\
\midrule

\multirow{2}{*}{\textbf{All Phases}} & Predominantly English, and text-centric resources. & More multilingual, multi-modal, and flexible resources. \\
% & Limited or performative reproducibility and documentation. & Executable scripts with \emph{verifiable} reproducbility, for data preparation, training, evaluation, and analysis. \\
& A concentration of standalone, flashy, one-off projects, for credit incentives. & More collaborative, large-scale interoperable infrastructure projects that builds on existing tooling. \\
\midrule

\multirow{4}{*}{\textbf{Data Sources}} & Predominantly synthetic, unrealistic finetuning data. & Naturalistic observations and realistic training tasks. \\
& Predominantly English, and text-centric data sources. & More multilingual, multi-modal data sources. \\
& Intended uses, licenses, consent, \& provenance are haphazardly documented. & Prioritizing datasets with standardized, structured, and linked metadata. \\
& Sparse information especially on data's sensitive content, such as CSAM and NCII. & Comprehensive ``data measurements'' as part of large, unstructured data releases. \\
& Data \& modeling focus on easy-to-scale formats, e.g. captioned images. & More attention to collecting neglected data formats, e.g. naturally interspersed text and images. \\

\midrule
\multirow{3}{*}{\textbf{Data Preparation}} & Diverse data infrastructure and processing standards for individual use-cases. & Interoperable data formats and processing for many use-cases. \\
& Mostly one-off and closed-source data exploration tools. & More open data exploration tools. \\
& Loose ideas of ``high-quality'' data that applies to all domains. & Concrete and precise definitions of quality that are unique to a set of domains, tasks, or evaluations.\\
& Data preparation methods compared across models with different training data. & Standardized data-centric benchmarks to fairly compare methods. \\
& English-centric tokenization and processing tools. & More multilingual and low-resource language tokenization and processing tools. \\

% & \cellcolor[gray]{0.9} Open-source data exploration tools allow for retrospective analysis of datasets. \\
% & Consolidating on a standardized format for data storage and processing will give developers more time to focus on developing infrastructure. \\
% & \cellcolor[gray]{0.9} Developing data-centric benchmarks can catalyze progress. \\
% & Data preparation tools should consider not only English-centric data, but non-English and low-resource languages. \\

\midrule
\multirow{4}{*}{\textbf{Data Documentation}} & Data documentation is diversely formatted, often terse, performative, without achieving reproducibility. & Executable, and verifially reproducible scraping and analysis scripts. Standardized data documentation requirements (e.g. from conferences). \\
& Documentation is an after-thought. & Documentation is started early, assembled over the course of collection and processing. \\
& Data stewardship and maintenance are often ignored beyond initial release. & Data governance is proactively organized, with a post-launch maintenance and licensing plan. \\

\midrule

\multirow{2}{*}{\textbf{Model Training}} & Mostly educational resources for technical developers. & More ``last mile'' educational resources for non-technical developers.\\
& Inflexible and disparate tooling. & Standardized and centralized resources, especially cross-modality. \\

\midrule

\multirow{6}{*}{\textbf{Environmental Impact}} & Opaque environmental impact estimate programs from cloud providers. & Query-level energy and environmental usage transparency from . \\
& Opaque information from hardware makers and data centers. & Fine-grained transparency from hardware makers and data centers. \\
& Inability to compare energy standards across systems. & ``Energy Star'' standards for non-technical users to fairly compare AI services. \\
& Scaling laws are text-centric and impractical. & Multimodal scaling laws research. Intuitive interfaces to estimate and model scaling laws for training forecasts. \\

\midrule
\multirow{4}{*}{\textbf{Model Evaluation}} & Evaluating model outputs. & Evaluating systems, within their application contexts. \\
& Evaluating on synthetic toxicity and safety benchmarks. & Evaluating natural observations, real-world settings, and with human-interaction studies.  \\
& Reporting evaluation metrics only. & Releasing evaluation scripts for verifiable reproducibility. \\
\midrule
\multirow{4}{*}{\textbf{Model Release}} & No or uninformed license choices. & License selection guided by the context of data, potential and unforeseen uses, and legal considerations. \\
& Weights release with limited support. & Accompanying documentation, and easy-to-run code for training, evaluation, and inference.  \\
& Limited plans for usage monitoring, or over-claimed benefits from watermarking/monitoring. & A plan that considers gating, watermarking, and misuse reporting (though they are not always beneficial).  \\
& Harm \& hazard taxonomies are based on existing benchmarks. & Harm taxonomies are based on empirical observations, and studies with real users.  \\

\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\textbf{A summary of the reviews \& take-aways from each phase of foundation model development ecosystem.}}
\label{tab:review-summary}
\end{table*}
