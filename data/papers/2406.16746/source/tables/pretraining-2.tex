\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule
\multicolumn{4}{c}{\textsc{\emph{Specialized Text Pretraining Corpora}}} \\\midrule

    \TextCircle\EmptyCircle\EmptyCircle & \textbf{Open License Corpus} & The Open License Corpus is a 228B token corpus of permissively-licensed, English text data for pretraining \citep{min2023SILOLM}. & \href{https://arxiv.org/abs/2308.04430}{\earxiv}\href{https://huggingface.co/datasets/kernelmachine/open-license-corpus}{\ehf}\href{https://github.com/kernelmachine/silo-lm#download-data}{\egithub}\emojiblank \\
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{Pile of Law} & An open-source, English dataset with 256GB of legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records \citep{henderson2022pile}. & \href{https://arxiv.org/abs/2207.00220}{\earxiv}\href{https://huggingface.co/datasets/pile-of-law/pile-of-law}{\ehf}\emojiblank\emojiblank \\
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{The Stack} & A 6TB permissively-licensed corpus from active GitHub repositories (358 programming languages) \citep{kocetkov2022stack}. & \href{https://arxiv.org/abs/2211.15533}{\earxiv}\href{https://huggingface.co/datasets/bigcode/the-stack}{\ehf}\href{https://github.com/bigcode-project/bigcode-dataset}{\egithub}\href{https://www.bigcode-project.org/docs/about/the-stack/#datasets-and-data-governance-tools-released-by-bigcode}{\eweb} \\
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{The Proof Pile 2} & The Proof-Pile-2 is a 55 billion token dataset of mathematical and scientific documents \citep{azerbayev2023llemma}. & \href{https://arxiv.org/abs/2310.10631}{\earxiv}\href{https://huggingface.co/datasets/EleutherAI/proof-pile-2}{\ehf}\href{https://github.com/EleutherAI/math-lm}{\egithub}\href{https://blog.eleuther.ai/llemma/}{\eweb} \\
    
    % \TextCircle\EmptyCircle\EmptyCircle & \textbf{OpenWebMath} & A dataset containing the majority of the high-quality, mathematical text from the internet. It is filtered and extracted from over 200B HTML files on Common Crawl down to a set of 6.3 million documents containing a total of 14.7B tokens \citep{paster2023openwebmath}. & \href{https://arxiv.org/abs/2310.06786}{\earxiv}\href{https://huggingface.co/datasets/open-web-math/open-web-math}{\ehf}\href{https://github.com/keirp/OpenWebMath}{\egithub}\emojiblank \\
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{peS2o} & A collection of ~40M creative open-access academic papers, cleaned, filtered, and formatted for pre-training of language models, originally derived from the Semantic Scholar Open Research Corpus (S2ORC) \citep{lo2020s2orc}. & \href{https://arxiv.org/abs/1911.02782}{\earxiv}\href{https://huggingface.co/datasets/allenai/peS2o}{\ehf}\emojiblank\emojiblank \\

\bottomrule
\end{tabular}
\end{table}