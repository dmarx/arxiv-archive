\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule

    \multicolumn{4}{c}{\textsc{\emph{Risk Evaluation Overviews}}} \\
    \midrule

% \TextCircle\VisionCircle\SpeechCircle & \textbf{Taxonomy of Harms} & Taxonomy of possible harms from generative AI systems to check your model against. & \emojiblank\emojiblank\emojiblank\href{https://arxiv.org/abs/2310.11986}{\eweb} \\
\TextCircle & \textbf{SafetyPrompts website} & Open repository of datasets for LLM evaluation and mitigation. & \emojiblank\emojiblank\emojiblank\href{https://safetyprompts.com/}{\eweb} \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{Safety evaluation repository} & A repository of 200+ safety evaluations, across modalities and harms. Useful for delving deeper into the array of risks. & \emojiblank\emojiblank\emojiblank\href{https://dpmd.ai/46CPd58}{\eweb} \\

    \midrule
    \multicolumn{4}{c}{\textsc{\emph{Bias \& Toxicity Evaluations}}} \\
    \midrule
    
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Bias Benchmark for QA (BBQ)} & A dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine different social dimensions relevant for U.S. English-speaking contexts \citep{parrish2021bbq}. & \href{https://arxiv.org/abs/2110.08193}{\earxiv}\emojiblank\href{https://github.com/nyu-mll/BBQ}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{Crossmodal-3600} & Image captioning evaluation with geographically diverse images in 36 languages \citep{thapliyal2022crossmodal}. & \href{https://arxiv.org/abs/2205.12522}{\earxiv}\emojiblank\href{https://google.github.io/crossmodal-3600/}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{HolisticBias} & A bias and toxicity benchmark using templated sentences, covering nearly 600 descriptor terms across 13 different demographic axes, for a total of 450k examples \citep{smith2022m}. & \href{https://arxiv.org/abs/2205.09209}{\earxiv}\emojiblank\href{https://github.com/facebookresearch/ResponsibleNLP/tree/main/holistic_bias}{\egithub}\href{https://ai.meta.com/research/publications/im-sorry-to-hear-that-finding-new-biases-in-language-models-with-a-holistic-descriptor-dataset/}{\eweb} \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{RealToxicityPrompts} & 100k web sentence snippets for researchers to assess the risk of neural toxic degeneration in models \citep{gehman2020realtoxicityprompts}. & \href{https://arxiv.org/abs/2009.11462}{\earxiv}\emojiblank\href{https://github.com/allenai/real-toxicity-prompts}{\egithub}\href{https://toxicdegeneration.allenai.org/}{\eweb} \\
\EmptyCircle\VisionCircle\EmptyCircle & \textbf{StableBias} & Bias testing benchmark for Image to Text models, based on gender-occupation associations \citep{luccioni2023stable}. & \href{https://arxiv.org/abs/2303.11408}{\earxiv}\href{https://huggingface.co/spaces/society-ethics/StableBias}{\ehf}\emojiblank\emojiblank \\


    \midrule
    \multicolumn{4}{c}{\textsc{\emph{Factuality Evaluations}}} \\
    \midrule

\TextCircle\EmptyCircle\EmptyCircle & \textbf{FactualityPrompt} & A benchmark to measure factuality in language models \citep{lee2022factuality}. & \href{https://arxiv.org/abs/2206.04624}{\earxiv}\emojiblank\href{https://github.com/nayeon7lee/FactualityPrompt}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Hallucinations} & Public LLM leaderboard computed using Vectara's Hallucination Evaluation Model. It evaluates LLM hallucinations when summarizing a document \citep{Hughes-Vectara-Hallucination-Leaderboard-2023}.  & \emojiblank\href{https://huggingface.co/vectara/hallucination_evaluation_model}{\ehf}\emojiblank\href{https://github.com/vectara/hallucination-leaderboard}{\eweb} \\
% \TextCircle\EmptyCircle\EmptyCircle & \textbf{Head-to-Tail} & For knowledge testing: a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity \citep{sun2023head}.  & \href{https://arxiv.org/abs/2308.10168}{\earxiv}\emojiblank\emojiblank\href{https://paperswithcode.com/paper/head-to-tail-how-knowledgeable-are-large}{\eweb} \\

% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[H]
% % \begin{tabular}{@{}lllll@{}}
% \begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
% \toprule
% \textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\
    \midrule
    \multicolumn{4}{c}{\textsc{\emph{Information \& Safety Hazard Evaluations}}} \\
    \midrule

\TextCircle\EmptyCircle\EmptyCircle & \textbf{Purple Llama CyberSecEval} & A benchmark for coding assistants, measuring their propensity to generate insecure code and level of compliance when asked to assist in cyberattacks \citep{bhatt2023purple}. & \href{https://arxiv.org/abs/2312.04724}{\earxiv}\emojiblank\href{https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks}{\egithub}\href{https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/}{\eweb} \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Purple Llama Guard} & A tool to identify and protect against malicious inputs to LLMs \citep{inan2023llama}. & \href{https://arxiv.org/abs/2312.06674}{\earxiv}\emojiblank\href{https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard}{\egithub}\href{https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/}{\eweb} \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{OpenAI content moderation filter} & A moderation filter and released dataset. The endpoint has 5 primary categories (Sexual, Hateful, Violent, Self-harm, Harassment) with sub-categories. & \href{https://arxiv.org/pdf/2208.03274.pdf}{\earxiv}\emojiblank\href{https://github.com/openai/moderation-api-release}{\egithub}\emojiblank \\
%\TextCircle\EmptyCircle\EmptyCircle & \textbf{Red Teaming LMs with LMs} & A method for using one language model to automatically find cases where a target LM behaves in a harmful way, by generating test cases \citep{perez2022red}. & \href{https://arxiv.org/abs/2202.03286}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{SimpleSafetyTests} & Small probe set (100 English text prompts) covering severe harms: child abuse, suicide, self-harm and eating disorders, scams and fraud, illegal items, and physical harm \citep{vidgen2023simplesafetytests}. & \href{https://arxiv.org/abs/2311.08370}{\earxiv}\emojiblank\href{https://github.com/bertiev/SimpleSafetyTests}{\egithub}\emojiblank \\

\bottomrule
\end{tabular}
\end{table}