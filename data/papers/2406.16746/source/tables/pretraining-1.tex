\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule
\multicolumn{4}{c}{\textsc{\emph{English Text}}} \\
    \midrule
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{C4} & An English, cleaned version of \href{https://commoncrawl.org}{Common Crawl}'s web crawl corpus \citep{raffel2020exploring,dodge2021documenting}. & \href{https://arxiv.org/abs/1910.10683}{\earxiv}\href{https://huggingface.co/datasets/allenai/c4}{\ehf}\href{https://github.com/google-research/text-to-text-transfer-transformer#c4}{\egithub}\emojiblank \\
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{Dolma} & An English-only pretraining corpus of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials \citep{soldaini2024dolma}. See the \href{https://github.com/allenai/dolma/blob/main/docs/assets/dolma-datasheet-v0.1.pdf}{datasheet}. & \href{https://arxiv.org/abs/2402.00159}{\earxiv}
    \href{https://huggingface.co/datasets/allenai/dolma}{\ehf}\href{https://github.com/allenai/dolma}{\egithub}\emojiblank \\

    \TextCircle\EmptyCircle\EmptyCircle & \textbf{The Pile} & An 825GB English pretraining corpus that mixes portions of common crawl with 22 smaller, high-quality datasets combined together \citep{gao2020pile,biderman2022datasheet}. & \href{https://arxiv.org/abs/2101.00027}{\earxiv}\href{https://huggingface.co/datasets/EleutherAI/pile}{\ehf}\emojiblank\href{https://pile.eleuther.ai/}{\eweb} \\

    % \TextCircle\EmptyCircle\EmptyCircle & \textbf{The RefinedWeb} & An English-only, web-only, deduplicated pretraining dataset of five trillion tokens \citep{penedo2023refinedweb}. & \href{https://arxiv.org/abs/2306.01116}{\earxiv}\href{https://huggingface.co/datasets/tiiuae/falcon-refinedweb}{\ehf}\emojiblank\emojiblank \\

    \midrule
    \multicolumn{4}{c}{\textsc{\emph{Multilingual Text}}} \\
    \midrule

    % \TextCircle\EmptyCircle\EmptyCircle & \textbf{mC4} & The multilingual cleaned version of \href{https://commoncrawl.org}{Common Crawl}'s web crawl corpus \citep{raffel2020exploring}. See this data analysis \cite{kreutzer2022quality}. & \href{https://arxiv.org/abs/1910.10683}{\earxiv}\href{https://huggingface.co/datasets/mc4}{\ehf}\emojiblank\emojiblank \\

    \TextCircle\EmptyCircle\EmptyCircle & \textbf{ROOTS} & A massive multilingual pretraining corpus from BigScience, comprised of 1.6TB of text spanning 59 languages \citep{laurenccon2022bigscience}. It is a mix of \href{https://oscar-project.org/}{OSCAR} \citep{laippala2022towards} and the datasets found in the BigScience Catalogue \citep{mcmillan2022documenting}. & \href{https://arxiv.org/abs/2303.03915}{\earxiv}\href{https://huggingface.co/bigscience-data}{\ehf}\href{https://github.com/bigscience-workshop/bigscience/tree/master/data}{\egithub}\href{https://bigscience.huggingface.co/}{\eweb} \\

    \TextCircle\EmptyCircle\EmptyCircle & \textbf{MADLAD-400} & An general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages \citep{kudugunta2023madlad}. & \href{https://arxiv.org/abs/2309.04662}{\earxiv}\href{https://huggingface.co/datasets/allenai/MADLAD-400}{\ehf}\href{https://github.com/google-research/google-research/tree/master/madlad_400}{\egithub}\emojiblank \\
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{RedPajama v2} & A pretraining dataset of 30 trillion deduplicated tokens from 84 CommonCrawl dumps and 5 languages \citep{together_ai_2023_redpajama}. & \emojiblank\href{https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2}{\ehf}\href{https://github.com/togethercomputer/RedPajama-Data}{\egithub}\href{https://www.together.ai/blog/redpajama-data-v2}{\eweb} \\
    
    \TextCircle\EmptyCircle\EmptyCircle & \textbf{CulturaX} & A pertaining dataset of 16T tokens, covering 167 languages, cleaned, deduplicated, and refined. Combines mC4 into 2020, with OSCAR project data up to 2023 \citep{nguyen2023culturax}. & \href{https://arxiv.org/abs/2309.09400}{\earxiv}\href{https://huggingface.co/datasets/uonlp/CulturaX}{\ehf}\emojiblank\emojiblank \\

    \TextCircle\EmptyCircle\EmptyCircle & \textbf{OSCAR} & The Open Super-large Crawled Aggregated coRpus provides web-based multilingual datasets across 166 languages \citep{suarez2019asynchronous,laippala2022towards}. & \href{https://aclanthology.org/2022.wnut-1.23/}{\earxiv}\emojiblank\emojiblank\href{https://oscar-project.org/}{\eweb} \\
    
    % \TextCircle\EmptyCircle\EmptyCircle & \textbf{OPUS} & The Open Parallel Corpus is a massive collection of translated text pairs from the web \citep{tiedemann2012parallel}. & \href{https://aclanthology.org/L12-1246/}{\earxiv}\emojiblank\emojiblank\href{https://opus.nlpl.eu/}{\eweb} \\

    \TextCircle\EmptyCircle\EmptyCircle & \textbf{WURA} & A manually audited multilingual pre-training corpus (document-level dataset) for 16 African languages and four  high-resource languages widely spoken in Africa (English, French, Arabic and Portuguese) \citep{oladipo-etal-2023-better}. & \href{https://aclanthology.org/2023.emnlp-main.11/}{\earxiv}\href{https://huggingface.co/datasets/castorini/wura}{\ehf}\emojiblank\emojiblank \\
    
\bottomrule
\end{tabular}
\end{table}