\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule

    \multicolumn{4}{c}{\textsc{\emph{Finetuning Repositories}}} \\
    \midrule

\TextCircle\EmptyCircle\EmptyCircle & \textbf{Axolotl} & A repository for chat- or instruction-tuning language models, including through full fine-tuning, LoRA, QLoRA, and GPTQ. & \emojiblank\emojiblank\href{https://github.com/OpenAccess-AI-Collective/axolotl}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{Levanter} & A framework for training large language models (LLMs) and other foundation models that strives for legibility, scalability, and reproducibility. & \emojiblank\href{https://huggingface.co/stanford-crfm}{\ehf}\href{https://github.com/stanford-crfm/levanter}{\egithub}\href{https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html	}{\eweb} \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{LLaMA-Adapter} & Fine-tuned LLMs on multimodal data using adapters \citep{gao2023llama}. & \href{https://arxiv.org/abs/2304.15010}{\earxiv}\emojiblank\href{https://github.com/OpenGVLab/LLaMA-Adapter}{\egithub}\emojiblank \\
% \TextCircle\VisionCircle\EmptyCircle & \textbf{BLIP-2} & Fine-tuned LLMs on multimodal data using a projection layer \citep{li2023blip2}. & \href{https://arxiv.org/abs/2301.12597}{\earxiv}\emojiblank\href{https://github.com/salesforce/LAVIS/tree/main/projects/blip2}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{LLaVA} & Fine-tuned LLMs on multimodal data using a projection layer \citep{liu2023improved}. & \href{https://arxiv.org/abs/2310.03744}{\earxiv}\href{https://huggingface.co/spaces/badayvedat/LLaVA}{\ehf}\href{https://github.com/haotian-liu/LLaVA}{\egithub}\href{https://llava-vl.github.io/}{\eweb} \\
% \TextCircle\VisionCircle\EmptyCircle & \textbf{MiniGPT4} & Fine-tuned LLMs on multimodal data using a projection layer \citep{zhu2023minigpt}. & \href{https://arxiv.org/abs/2304.10592}{\earxiv}\href{https://huggingface.co/spaces/Vision-CAIR/minigpt4}{\ehf}\href{https://github.com/Vision-CAIR/MiniGPT-4}{\egithub}\href{https://minigpt-4.github.io/}{\eweb} \\
% \TextCircle\VisionCircle\EmptyCircle & \textbf{OpenFlamingo} & Open source implementation of Flamingo \citep{awadalla2023openflamingo}. & \href{https://arxiv.org/abs/2308.01390}{\earxiv}\href{https://huggingface.co/openflamingo}{\ehf}\href{https://github.com/mlfoundations/open_flamingo}{\egithub}\href{https://laion.ai/blog/open-flamingo-v2/}{\eweb} \\
\TextCircle\VisionCircle\EmptyCircle & \textbf{Otter} & Multimodal models with Flamingo architecture \citep{li2023otterhd}. & \href{https://arxiv.org/abs/2311.04219}{\earxiv}\href{https://huggingface.co/spaces/Otter-AI/OtterHD-Demo}{\ehf}\href{https://github.com/Luodian/Otter}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{peft} & A library for doing parameter efficient finetuning. & \emojiblank\emojiblank\href{https://github.com/huggingface/peft}{\egithub}\emojiblank \\
% \TextCircle\VisionCircle\SpeechCircle & \textbf{trl} & A library for doing RLHF. & \emojiblank\emojiblank\href{https://github.com/huggingface/trl}{\egithub}\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{trlX} & A library for doing RLHF at scale \citep{trlx-library}. & \href{https://aclanthology.org/2023.emnlp-main.530/}{\earxiv}\emojiblank\href{https://github.com/CarperAI/trlx}{\egithub}\href{https://trlx.readthedocs.io/en/latest/}{\eweb} \\


\bottomrule
\end{tabular}
\end{table}