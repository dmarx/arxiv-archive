\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule
\multicolumn{4}{c}{\textsc{\emph{Common Benchmarks for Text Capability Evaluation}}} \\
\midrule

\TextCircle\EmptyCircle\EmptyCircle & \textbf{BigBench} & A collaborative benchmark of 100s of tasks, probing LLMs on a wide array of unique capabilities \citep{srivastava2023beyond}. & \href{https://arxiv.org/abs/2206.04615}{\earxiv}\emojiblank\href{https://github.com/google/BIG-bench}{\egithub}\emojiblank \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{BigBench Hard} & A challenging subset of 23 BigBench tasks where 2022 models did not outperform annotator performance \citep{suzgun2022challenging}. & \href{https://arxiv.org/abs/2210.09261}{\earxiv}\emojiblank\href{https://github.com/suzgunmirac/BIG-Bench-Hard}{\egithub}\emojiblank \\

\TextCircle\EmptyCircle\EmptyCircle & \textbf{HELM classic} & A comprehensive suite of scenarios and metrics aimed at holistically evaluating models (including for capabilities) with comparisons to well known models \citep{liang2022holistic}. & \href{https://arxiv.org/abs/2211.09110}{\earxiv}\emojiblank\href{https://github.com/stanford-crfm/helm}{\egithub}\href{https://crfm.stanford.edu/helm/latest/}{\eweb} \\
\TextCircle\EmptyCircle\EmptyCircle & \textbf{HELM lite} & A lightweight subset of capability-centric benchmarks within HELM. Compares prominent open and closed models. & \href{https://crfm.stanford.edu/2023/12/19/helm-lite.html}{\earxiv}\emojiblank\href{https://github.com/stanford-crfm/helm}{\egithub}\href{https://crfm.stanford.edu/helm/lite/latest/#/}{\eweb} \\

\TextCircle\EmptyCircle\EmptyCircle & \textbf{LM Evaluation Harness} & Orchestration framework for standardizing LM prompted evaluation, supporting hundreds of subtasks. & \emojiblank\emojiblank\href{https://github.com/EleutherAI/lm-evaluation-harness}{\egithub}\href{https://github.com/EleutherAI/lm-evaluation-harness}{\eweb} \\

\TextCircle\EmptyCircle\EmptyCircle & \textbf{MMLU} & Evaluation of LM capabilities on multiple-choice college exam questions \citep{hendrycks2020measuring}. &\href{https://arxiv.org/pdf/2009.03300.pdf}{\earxiv}\href{https://huggingface.co/datasets/cais/mmlu}{\ehf}\href{https://github.com/hendrycks/test}{\egithub}\href{https://github.com/hendrycks/test#test-leaderboard}{\eweb} \\

\TextCircle\EmptyCircle\EmptyCircle & \textbf{MTEB} & The Massive Text Embedding Benchmark measures the quality of embeddings across 58 datasets and 112 languages for tasks related to retrieval, classification, clustering or semantic similarity. & \href{https://arxiv.org/abs/2210.07316}{\earxiv}\href{https://huggingface.co/spaces/mteb/leaderboard}{\ehf}\href{https://github.com/embeddings-benchmark/mteb}{\egithub}\emojiblank \\

\TextCircle\EmptyCircle\EmptyCircle & \textbf{SIB-200} & A large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects \citep{adelani2023sib}. & \href{https://arxiv.org/abs/2309.07445}{\earxiv}\href{https://huggingface.co/datasets/Davlan/sib200}{\ehf}\href{https://github.com/dadelani/sib-200	}{\egithub}\emojiblank \\	

\midrule
\multicolumn{4}{c}{\textsc{\emph{Common Benchmarks for Code Capability Evaluation}}} \\
\midrule

\TextCircle\EmptyCircle\EmptyCircle & \textbf{BigCode Evaluation Harness} & A framework for the evaluation of code generation models, compiling many evaluation sets. & \emojiblank\emojiblank\href{https://github.com/bigcode-project/bigcode-evaluation-harness/tree/main}{\egithub}\emojiblank \\

\TextCircle\EmptyCircle\EmptyCircle & \textbf{HumanEvalPack} & A code evaluation benchmark across 6 languages and 3 tasks, extending OpenAI's HumanEval \citep{muennighoff2023octopack}. & \href{https://arxiv.org/abs/2308.07124}{\earxiv}\href{https://huggingface.co/datasets/bigcode/humanevalpack}{\ehf}\href{https://github.com/bigcode-project/octopack}{\egithub}\emojiblank \\

\TextCircle\EmptyCircle\EmptyCircle & \textbf{SWE Bench} & SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a codebase and an issue, a language model is tasked with generating a patch that resolves the described problem \citep{jimenez2023swe}. & \href{https://arxiv.org/abs/2310.06770}{\earxiv}\emojiblank\href{https://github.com/princeton-nlp/SWE-bench}{\egithub}\href{https://www.swebench.com/}{\eweb} \\

\bottomrule
\end{tabular}
\end{table}