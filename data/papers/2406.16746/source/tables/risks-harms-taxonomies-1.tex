\begin{table}[H]
% \begin{tabular}{@{}lllll@{}}
\begin{tabular}{@{}p{\colOneSize}p{\colTwoSize}p{\colThreeSize}p{\colFourSize}@{}}
\toprule
\textsc{Modality} & \textsc{Name} & \textsc{Description} & \textsc{Links} \\ 
\midrule

    \multicolumn{4}{c}{\textsc{\emph{Taxonomies of Risk and Harm}}} \\
    \midrule




\TextCircle\VisionCircle\SpeechCircle & \textbf{Ethical and social risks of harm from language models} & A two-level taxonomy of LLM risks, comprising both classification groups and associated harms \citep{weidinger2021ethical}. The classification groups are: (1) Discrimination, Exclusion and Toxicity, (2) Information Hazards, (3) Misinformation Harms, (4) Malicious Uses, (5) Human-Computer Interaction Harms, and (6) Automation, access, and environmental harms. & \href{https://arxiv.org/pdf/2112.04359.pdf}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{Taxonomy of Risks posed by Language Models} & A taxonomy of language model risks \citep{10.1145/3531146.3533088}. The classification groups are: (1) Discrimination, Hate speech and Exclusion, (2) Information Hazards, (3) Misinformation Harms, (4) Malicious Uses, (5) Human-Computer Interaction Harms, and (6) Environmental and Socioeconomic harms. For each risk area, the authors describe relevant evidence, causal mechanisms, and risk mitigation approaches. & \href{https://arxiv.org/pdf/2112.04359.pdf}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{Sociotechnical Safety Evaluation of Generative AI Systems} & A two-level taxonomy of AI risks, comprising both classification groups and associated harms \citep{weidinger2023sociotechnical}. The classification groups are: (1) Representation and Toxicity Harms, (2) Misinformation Harms, (3) Information \& Society Harms, (4) Malicious Use, (5) Human Autonomy \& Integrity Harms, and (6) Socioeconomic \& Environmental Harms. & \href{https://arxiv.org/pdf/2310.11986.pdf}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{Sociotechnical Harms of Algorithmic Systems} & A taxonomy of AI harms with detailed subcategories, focused on mitigating harm \citep{shelby2023sociotechnical}. The harm categories are: (1) Representational harms, (2) Allocative harms, (3) Quality of Service harms, (4) Interpersonal harms, and (5) Social system harms.  & \href{https://arxiv.org/pdf/2210.05791.pdf}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{Assessing Language Model Deployment with Risk Cards} & A framework for structured assessment and documentation of risks associated with applications of language models \citep{derczynski2023assessing}. Each RiskCard makes clear the routes for the risk to manifest harm, their placement in harm taxonomies, and example prompt-output pairs. 70+ risks are identified, based on a literature survey.& \href{https://arxiv.org/pdf/2303.18190.pdf}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{An Overview of Catastrophic AI Risks} & A taxonomy of 4 catastrophic AI risks, with associated subcategories \citep{hendrycks2023overview}: (1) Malicious use,(2) AI race,(3) Organizational risks, (4) Rogue AIs (Proxy gaming, Goal drift, Power-seeking, Deception). & \href{https://arxiv.org/pdf/2306.12001.pdf}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{OpenAI Preparedness Framework} & Description of 4 catastrophic AI risks \citep{openai2023preparedness}: (1) Cybersecurity, (2) Chemical, Biological, Nuclear and Radiological (CBRN) threats, (3) Persuasion, and (4) Model autonomy. The paper also highlights the risk of "unknown unknowns". & \href{brundage2018malicious}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\TextCircle\VisionCircle\SpeechCircle & \textbf{Model evaluation for extreme risks} & A framework of 9 dangerous capabilities of AI models  \citep{shevlane2023model}: (1) Cyber-offense, (2) Deception, (3) Persuasion \& manipulation, (4) Political strategy, (5) Weapons acquisition, (6) Long-horizon planning, (7) AI development, (8) Situational awareness, (9) Self-proliferation. & \href{https://arxiv.org/pdf/2305.15324.pdf}{\earxiv}\emojiblank\emojiblank\emojiblank \\
\bottomrule
\end{tabular}
\end{table}