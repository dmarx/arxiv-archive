@article{he2024s,
  title={What's in Your" Safe" Data?: Identifying Benign Data that Breaks Safety},
  author={He, Luxi and Xia, Mengzhou and Henderson, Peter},
  journal={arXiv preprint arXiv:2404.01099},
  year={2024}
}

@article{rebedea2023nemo,
  title={Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails},
  author={Rebedea, Traian and Dinu, Razvan and Sreedhar, Makesh and Parisien, Christopher and Cohen, Jonathan},
  journal={arXiv preprint arXiv:2310.10501},
  year={2023}
}

@misc{klyman2024aups-for-fms, 
    author = {Kevin Klyman}, 
    title  = {Acceptable Use Policies for Foundation Models: Considerations for Policymakers and Developers}, 
    howpublished = {Stanford Center for Research on Foundation Models},
    month = {apr},
    year   = 2024, 
    url    = {https://crfm.stanford.edu/2024/04/08/aups.html}
}

@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@inproceedings{lees2022new,
  title={A new generation of perspective api: Efficient multilingual character-level transformers},
  author={Lees, Alyssa and Tran, Vinh Q and Tay, Yi and Sorensen, Jeffrey and Gupta, Jai and Metzler, Donald and Vasserman, Lucy},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3197--3207},
  year={2022}
}


@article{lambert2024rewardbench,
  title={Rewardbench: Evaluating reward models for language modeling},
  author={Lambert, Nathan and Pyatkin, Valentina and Morrison, Jacob and Miranda, LJ and Lin, Bill Yuchen and Chandu, Khyathi and Dziri, Nouha and Kumar, Sachin and Zick, Tom and Choi, Yejin and others},
  journal={arXiv preprint arXiv:2403.13787},
  year={2024}
}

@inproceedings{nagrani17-interspeech,
  author={Arsha Nagrani and Joon Son Chung and Andrew Zisserman},
  title={{VoxCeleb: A Large-Scale Speaker Identification Dataset}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={2616--2620},
  doi={10.21437/Interspeech.2017-950}
}

@inproceedings{ardila-etal-2020-common,
    title = "Common Voice: A Massively-Multilingual Speech Corpus",
    author = "Ardila, Rosana  and
      Branson, Megan  and
      Davis, Kelly  and
      Kohler, Michael  and
      Meyer, Josh  and
      Henretty, Michael  and
      Morais, Reuben  and
      Saunders, Lindsay  and
      Tyers, Francis  and
      Weber, Gregor",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.520",
    pages = "4218--4222",
    abstract = "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\mbox{$\pm$}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}


@inproceedings{pratap20-interspeech,
  author={Vineel Pratap and Qiantong Xu and Anuroop Sriram and Gabriel Synnaeve and Ronan Collobert},
  title={{MLS: A Large-Scale Multilingual Dataset for Speech Research}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={2757--2761},
  doi={10.21437/Interspeech.2020-2826}
}

@inproceedings{babu22-interspeech,
  author={Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick {von Platen} and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},
  title={{XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2278--2282},
  doi={10.21437/Interspeech.2022-143}
}

@misc{röttger2024xstest,
      title={XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models}, 
      author={Paul Röttger and Hannah Rose Kirk and Bertie Vidgen and Giuseppe Attanasio and Federico Bianchi and Dirk Hovy},
      year={2024},
      eprint={2308.01263},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mazeika2024harmbench,
      title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal}, 
      author={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},
      year={2024},
      eprint={2402.04249},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{srivastava2023imitation,
      title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
      author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
      year={2023},
      eprint={2206.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liang2023holistic,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}

@inproceedings{vidgen-etal-2019-challenges,
    title = "Challenges and frontiers in abusive content detection",
    author = "Vidgen, Bertie  and
      Harris, Alex  and
      Nguyen, Dong  and
      Tromble, Rebekah  and
      Hale, Scott  and
      Margetts, Helen",
    editor = "Roberts, Sarah T.  and
      Tetreault, Joel  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3509",
    doi = "10.18653/v1/W19-3509",
    pages = "80--93",
    abstract = "Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, efficiency and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which social scientific insights can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.",
}

@misc{zhang2023safetybench,
      title={SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions}, 
      author={Zhexin Zhang and Leqi Lei and Lindong Wu and Rui Sun and Yongkang Huang and Chong Long and Xiao Liu and Xuanyu Lei and Jie Tang and Minlie Huang},
      year={2023},
      eprint={2309.07045},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sun2024trustllm,
      title={TrustLLM: Trustworthiness in Large Language Models}, 
      author={Lichao Sun and Yue Huang and Haoran Wang and Siyuan Wu and Qihui Zhang and Yuan Li and Chujie Gao and Yixin Huang and Wenhan Lyu and Yixuan Zhang and Xiner Li and Zhengliang Liu and Yixin Liu and Yijue Wang and Zhikun Zhang and Bertie Vidgen and Bhavya Kailkhura and Caiming Xiong and Chaowei Xiao and Chunyuan Li and Eric Xing and Furong Huang and Hao Liu and Heng Ji and Hongyi Wang and Huan Zhang and Huaxiu Yao and Manolis Kellis and Marinka Zitnik and Meng Jiang and Mohit Bansal and James Zou and Jian Pei and Jian Liu and Jianfeng Gao and Jiawei Han and Jieyu Zhao and Jiliang Tang and Jindong Wang and Joaquin Vanschoren and John Mitchell and Kai Shu and Kaidi Xu and Kai-Wei Chang and Lifang He and Lifu Huang and Michael Backes and Neil Zhenqiang Gong and Philip S. Yu and Pin-Yu Chen and Quanquan Gu and Ran Xu and Rex Ying and Shuiwang Ji and Suman Jana and Tianlong Chen and Tianming Liu and Tianyi Zhou and William Wang and Xiang Li and Xiangliang Zhang and Xiao Wang and Xing Xie and Xun Chen and Xuyu Wang and Yan Liu and Yanfang Ye and Yinzhi Cao and Yong Chen and Yue Zhao},
      year={2024},
      eprint={2401.05561},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{wang2024decodingtrust,
      title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models}, 
      author={Boxin Wang and Weixin Chen and Hengzhi Pei and Chulin Xie and Mintong Kang and Chenhui Zhang and Chejian Xu and Zidi Xiong and Ritik Dutta and Rylan Schaeffer and Sang T. Truong and Simran Arora and Mantas Mazeika and Dan Hendrycks and Zinan Lin and Yu Cheng and Sanmi Koyejo and Dawn Song and Bo Li},
      year={2024},
      eprint={2306.11698},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{carlsmith2022powerseeking,
      title={Is Power-Seeking AI an Existential Risk?}, 
      author={Joseph Carlsmith},
      year={2022},
      eprint={2206.13353},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}


@inproceedings{dev-etal-2022-measures,
    title = "On Measures of Biases and Harms in {NLP}",
    author = "Dev, Sunipa  and
      Sheng, Emily  and
      Zhao, Jieyu  and
      Amstutz, Aubrie  and
      Sun, Jiao  and
      Hou, Yu  and
      Sanseverino, Mattie  and
      Kim, Jiin  and
      Nishi, Akihiro  and
      Peng, Nanyun  and
      Chang, Kai-Wei",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-aacl.24",
    pages = "246--267",
    abstract = "Recent studies show that Natural Language Processing (NLP) technologies propagate societal biases about demographic groups associated with attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While existing works propose bias evaluation and mitigation methods for various tasks, there remains a need to cohesively understand the biases and the specific harms they measure, and how different measures compare with each other. To address this gap, this work presents a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. As a validation of our framework and documentation questions, we also present several case studies of how existing bias measures in NLP{---}both intrinsic measures of bias in representations and extrinsic measures of bias of downstream applications{---}can be aligned with different harms and how our proposed documentation questions facilitates more holistic understanding of what bias measures are measuring.",
}


@inproceedings{banko-etal-2020-unified,
    title = "A Unified Taxonomy of Harmful Content",
    author = "Banko, Michele  and
      MacKeen, Brendon  and
      Ray, Laurie",
    editor = "Akiwowo, Seyi  and
      Vidgen, Bertie  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.16",
    doi = "10.18653/v1/2020.alw-1.16",
    pages = "125--137",
    abstract = "The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent years, the community has not standardized around how various harmful behaviors are defined, creating challenges for reliable moderation, modeling and evaluation. As a step towards attaining shared understanding of how online abuse may be modeled, we synthesize the most common types of abuse described by industry, policy, community and health experts into a unified typology of harmful content, with detailed criteria and exceptions for each type of abuse.",
}

@inproceedings{10.1145/3514094.3534146,
author = {Bucknall, Benjamin S. and Dori-Hacohen, Shiri},
title = {Current and Near-Term AI as a Potential Existential Risk Factor},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534146},
doi = {10.1145/3514094.3534146},
abstract = {There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {119–129},
numpages = {11},
keywords = {ai safety, existential risk, societal impacts of ai},
location = {Oxford, United Kingdom},
series = {AIES '22}
}


@misc{weidinger2021ethical,
      title={Ethical and social risks of harm from Language Models}, 
      author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
      year={2021},
      eprint={2112.04359},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{OSI2024def,
      title={The Open Source Definition},
      author={The Open Source Initiative},
      year={2024},
      month = {February},
      url = {https://opensource.org/osd/}
}

@misc{FSF2024what,
      title={What is Free Software?},
      author={The Free Software Foundation},
      url = {https://web.archive.org/web/20230306010437/https://www.gnu.org/philosophy/free-sw.en.html},
     note = {Last accessed on 2024-02-20},
     year = {2024}}

@inproceedings{10.1145/3531146.3533088,
author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
title = {Taxonomy of Risks posed by Language Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533088},
doi = {10.1145/3531146.3533088},
abstract = {Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {214–229},
numpages = {16},
keywords = {technology risks, risk assessment, responsible innovation, responsible AI, language models},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@misc{shevlane2023model,
      title={Model evaluation for extreme risks}, 
      author={Toby Shevlane and Sebastian Farquhar and Ben Garfinkel and Mary Phuong and Jess Whittlestone and Jade Leung and Daniel Kokotajlo and Nahema Marchal and Markus Anderljung and Noam Kolt and Lewis Ho and Divya Siddarth and Shahar Avin and Will Hawkins and Been Kim and Iason Gabriel and Vijay Bolina and Jack Clark and Yoshua Bengio and Paul Christiano and Allan Dafoe},
      year={2023},
      eprint={2305.15324},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@misc{brundage2018malicious,
      title={The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation}, 
      author={Miles Brundage and Shahar Avin and Jack Clark and Helen Toner and Peter Eckersley and Ben Garfinkel and Allan Dafoe and Paul Scharre and Thomas Zeitzoff and Bobby Filar and Hyrum Anderson and Heather Roff and Gregory C. Allen and Jacob Steinhardt and Carrick Flynn and Seán Ó hÉigeartaigh and Simon Beard and Haydn Belfield and Sebastian Farquhar and Clare Lyle and Rebecca Crootof and Owain Evans and Michael Page and Joanna Bryson and Roman Yampolskiy and Dario Amodei},
      year={2018},
      eprint={1802.07228},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{hendrycks2023overview,
      title={An Overview of Catastrophic AI Risks}, 
      author={Dan Hendrycks and Mantas Mazeika and Thomas Woodside},
      year={2023},
      eprint={2306.12001},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{openai2023preparedness,
      title={PreparednessFramework(Beta)}, 
      author={OpenAI},
      year={2023}
}


@misc{derczynski2023assessing,
      title={Assessing Language Model Deployment with Risk Cards}, 
      author={Leon Derczynski and Hannah Rose Kirk and Vidhisha Balachandran and Sachin Kumar and Yulia Tsvetkov and M. R. Leiser and Saif Mohammad},
      year={2023},
      eprint={2303.18190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{deng2023safer,
      title={Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements}, 
      author={Jiawen Deng and Jiale Cheng and Hao Sun and Zhexin Zhang and Minlie Huang},
      year={2023},
      eprint={2302.09270},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{weidinger2023sociotechnical,
      title={Sociotechnical Safety Evaluation of Generative AI Systems}, 
      author={Laura Weidinger and Maribeth Rauh and Nahema Marchal and Arianna Manzini and Lisa Anne Hendricks and Juan Mateos-Garcia and Stevie Bergman and Jackie Kay and Conor Griffin and Ben Bariach and Iason Gabriel and Verena Rieser and William Isaac},
      year={2023},
      eprint={2310.11986},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{shelby2023sociotechnical,
      title={Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction}, 
      author={Renee Shelby and Shalaleh Rismani and Kathryn Henne and AJung Moon and Negar Rostamzadeh and Paul Nicholas and N'Mah Yilla and Jess Gallegos and Andrew Smart and Emilio Garcia and Gurleen Virk},
      year={2023},
      eprint={2210.05791},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}



@article{henderson2020towards,
  title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={10039--10081},
  year={2020},
  publisher={JMLRORG}
}

@article{anthony2020carbontracker,
  title={Carbontracker: Tracking and predicting the carbon footprint of training deep learning models},
  author={Anthony, Lasse F Wolff and Kanding, Benjamin and Selvan, Raghavendra},
  journal={arXiv preprint arXiv:2007.03051},
  year={2020}
}

@article{lacoste2019quantifying,
  title={Quantifying the carbon emissions of machine learning},
  author={Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{luccioni2023estimating,
  title={Estimating the carbon footprint of bloom, a 176b parameter language model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={253},
  year={2023},
}

@misc{bai-constitutional-2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle = {Constitutional {AI}},
	url = {http://arxiv.org/abs/2212.08073},
	doi = {10.48550/arXiv.2212.08073},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08073 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@inproceedings{ahia-etal-2021-low-resource,
    title = "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation",
    author = "Ahia, Orevaoghene  and
      Kreutzer, Julia  and
      Hooker, Sara",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.282",
    doi = "10.18653/v1/2021.findings-emnlp.282",
    pages = "3316--3333",
}

@misc{pozzobon2023challenges,
      title={On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research}, 
      author={Luiza Pozzobon and Beyza Ermis and Patrick Lewis and Sara Hooker},
      year={2023},
      eprint={2304.12397},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yoo2022scalable,
      title={Scalable Training of Language Models using JAX pjit and TPUv4}, 
      author={Joanna Yoo and Kuba Perlin and Siddhartha Rao Kamalakara and João G. M. Araújo},
      year={2022},
      eprint={2204.06514},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{muennighoff2023octopack,
      title={OctoPack: Instruction Tuning Code Large Language Models}, 
      author={Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},
      journal={arXiv preprint arXiv:2308.07124},
      year={2023}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@article{wang2022benchmarking,
  title={Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022},
  url={https://arxiv.org/abs/2204.07705}
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={ICLR 2022},
  year={2021},
  url={https://arxiv.org/abs/2110.08207}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022},
  url={https://arxiv.org/abs/2203.02155}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={TMLR},
  year={2022},
  url={https://openreview.net/forum?id=yzkSU5zdwD},
}

@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford\_alpaca}},
}

@misc{selfinstruct2022,
  doi = {10.48550/ARXIV.2212.10560},
  url = {https://arxiv.org/abs/2212.10560},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{lmsys2023,
  author = {LMSYS-Org},
  title = {LM-SYS: FastChatT5},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/lm-sys/FastChat#FastChat-T5}},
}

@misc{mosailml_jonathan_2023,
  author = {Jonathan Frankle},
  title = {Tweet by Mosaic ML},
  year = {2023},
  publisher = {Twitter},
  journal = {Twitter},
  howpublished = {\url{https://twitter.com/jefrankle/status/1654848529834078208}},
}

@misc{dolly15k_2023,
  author = {Conover, Mike and Hayes, Matt and Mathur, Ankit and Meng, Xiangrui and  Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
  title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year = {2023},
  publisher = {Databricks},
  journal = {Databricks},
  howpublished = {\url{https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}},
}

@misc{oig2023,
  author = {Nguyen, Huu and Suri, Sameer and Tsui, Ken and Schuhmann, Christoph},
  title = {The Open Instruction Generalist (OIG) Dataset},
  year = {2023},
  publisher = {LAION Blog},
  journal = {LAION Blog},
  howpublished = {\url{https://laion.ai/blog/oig-dataset/}},
}

@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{gangulired,
    author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
    title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
    year={2022}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@article{sobel2017artificial,
  title={Artificial Intelligence's Fair Use Crisis},
  author={Sobel, Benjamin LW},
  journal={Columbia Journal of Law \& the Arts},
  volume={41},
  pages={45},
  year={2017},
}

@article{quang2021does,
  title={Does Training AI Violate Copyright Law?},
  author={Quang, Jenny},
  journal={Berkeley Tech. LJ},
  volume={36},
  pages={1407},
  year={2021},
  publisher={HeinOnline}
}

@online{SHP,
  author = {Ethayarajh, Kawin and Zhang, Heidi and Wang, Yizhong and Jurafsky, Dan},
  title = {Stanford Human Preferences Dataset},
  year = {2023},
  url = {https://huggingface.co/datasets/stanfordnlp/SHP}
}

@online{sharegpt,
  author = {Vercel},
  title = {ShareGPT},
  year = {2023},
  url = {https://sharegpt.com/}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@inproceedings{stienon2020learning,
  author = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  title = {Learning to summarize from human feedback},
  booktitle = {NeurIPS},
  year = 2020,
}


@online{arstechnica,
  author = {Arstechnica},
  title = {Stable Diffusion Copyright Lawsuits Could be a Legal Earthquake for AI},
  year = {2023},
  url = {https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/}
}

@misc{koala_blogpost_2023,
  author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},
  title = {Koala: A Dialogue Model for Academic Research},
  howpublished = {Blog post},
  month = {April},
  year = {2023},
  url = {https://bair.berkeley.edu/blog/2023/04/03/koala/},
  urldate = {2023-04-03}
}

@article{anil2023palm,
  title={PaLM 2 Technical Report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@inproceedings{weifinetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  year={2021},
  booktitle={International Conference on Learning Representations}
}

@article{Kinney2023TheSS,
  author = {Kinney, Rodney Michael and Anastasiades, Chloe and Authur, Russell and Beltagy, Iz and Bragg, Jonathan and Buraczynski, Alexandra and Cachola, Isabel and Candra, Stefan and Chandrasekhar, Yoganand and Cohan, Arman and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Etzioni, Oren and Evans, Rob and Feldman, Sergey and Gorney, Joseph and Graham, David W. and Hu, F.Q. and Huff, Regan and King, Daniel and Kohlmeier, Sebastian and Kuehl, Bailey and Langan, Michael and Lin, Daniel and Liu, Haokun and Lo, Kyle and Lochner, Jaron and MacMillan, Kelsey and Murray, Tyler and Newell, Christopher and Rao, Smita and Rohatgi, Shaurya and Sayre, Paul L and Shen, Zejiang and Singh, Amanpreet and Soldaini, Luca and Subramanian, Shivashankar and Tanaka, A. and Wade, Alex D and Wagner, Linda M. and Wang, Lucy Lu and Wilhelm, Christopher and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele and van Zuylen, Madeleine and Weld, Daniel S.},
  journal = {ArXiv},
  year = {2023},
  volume = {abs/2301.10140},
}


@article{sileo2023tasksource,
      title={tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation}, 
      author={Damien Sileo},
      year={2023},
      volume={abs/2301.05948},
      journal={arXiv},
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{scao2022language,
  title={What language model to train if you have one million GPU hours?},
  author={Scao, Teven Le and Wang, Thomas and Hesslow, Daniel and Saulnier, Lucile and Bekman, Stas and Bari, M Saiful and Bideman, Stella and Elsahar, Hady and Muennighoff, Niklas and Phang, Jason and others},
  journal={arXiv preprint arXiv:2210.15424},
  year={2022}
}

@article{epstein2023art,
  title={Art and the science of generative AI},
  author={Epstein, Ziv and Hertzmann, Aaron and Herman, Laura and Mahari, Robert and Frank, Morgan R and Groh, Matthew and Schroeder, Hope and Smith, Amy and Akten, Memo and Fjeld, Jessica and others},
  journal={Science},
  volume={380},
  number={6650},
  pages={1110--1111},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@misc{longpre2023pretrainers,
      title={A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, \& Toxicity}, 
      author={Shayne Longpre and Gregory Yauney and Emily Reif and Katherine Lee and Adam Roberts and Barret Zoph and Denny Zhou and Jason Wei and Kevin Robinson and David Mimno and Daphne Ippolito},
      year={2023},
      eprint={2305.13169},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{dodge2021documenting,
  title={Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus},
  author={Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={1286--1305},
  year={2021}
}


@inproceedings{wang2022super,
  title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@inproceedings{xu2021detoxifying,
  title={Detoxifying Language Models Risks Marginalizing Minority Voices},
  author={Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2390--2397},
  year={2021}
}

@article{VictorEfrati2023,
  title     = {Alphabet's Google and DeepMind Pause Grudges, Join Forces to Chase OpenAI},
  author    = {Jon Victor and Amir Efrati},
  journal   = {The Information},
  year      = {2023},
}

@article{meeker2022beyond,
  title={Beyond Open Data: The Only Good License Is No License},
  author={Meeker, Heather},
  journal={PLI Chronicle: Insights and Perspectives for the Legal Community},
  year={2022},
  month={April},
}

@book{ricketson2022international,
  title={International Copyright and Neighboring Rights: The Berne Convention and Beyond},
  author={Ricketson, Sam and Ginsburg, Jane C.},
  year={2022},
  month={August},
  publisher={Oxford University Press},
}

@article{cohen1986masking,
  title={Masking Copyright Decisionmaking: The Meaninglessness of Substantial Similarity},
  author={Cohen, Amy B},
  journal={UC Davis Law Review},
  volume={20},
  pages={719},
  year={1986},
  publisher={HeinOnline}
}

@article{balganesh2014judging,
  title={Judging similarity},
  author={Balganesh, Shyamkrishna and Manta, Irina D and Wilkinson-Ryan, Tess},
  journal={Iowa Law Review},
  volume={100},
  pages={267},
  year={2014},
  publisher={HeinOnline}
}

@book{marks2023law,
  title={The Law of Business Torts and Unfair Competition: Cases, Materials, and Problems},
  author={Marks, Colin P. and Moll, Douglas K.},
  isbn={9781647084905},
  series={American Casebook Series},
  url={https://books.google.com/books?id=K1fXzwEACAAJ},
  year={2023},
  publisher={West Academic}
}

@article{sag2023copyright,
  title={Copyright safety for generative AI},
  author={Sag, Matthew},
  journal={Forthcoming in the Houston Law Review},
  year={2023}
}

@article{samuelson2023generative,
  title={Generative AI meets copyright},
  author={Samuelson, Pamela},
  journal={Science},
  volume={381},
  number={6654},
  pages={158--161},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}


@article{derclaye2022sui,
  title={Sui generis database protection 2.0: judicial and legislative reforms},
  author={Derclaye, Estelle and Husovec, Martin},
  journal={European Intellectual Property Review},
  volume={44},
  number={6},
  pages={323--331},
  year={2022}
}


@article{margoni2022deeper,
  title={A DEEPER LOOK INTO the EU text and data mining exceptions: harmonisation, data ownership, and the future of technology},
  author={Margoni, Thomas and Kretschmer, Martin},
  journal={GRUR International},
  volume={71},
  number={8},
  pages={685--701},
  year={2022},
  publisher={Oxford University Press UK}
}

@article{ginsburg1992no,
  title={No Sweat Copyright and Other Protection of Works of Information after Feist v. Rural Telephone},
  author={Ginsburg, Jane C},
  journal={Columbia Law Review},
  volume={92},
  pages={338},
  year={1992},
  publisher={HeinOnline}
}

@article{grossman2005sony,
  title={From Sony to Grokster, the failure of the copyright doctrines of contributory infringement and vicarious liability to resolve the war between content and destructive technologies},
  author={Grossman, Craig A},
  journal={Buffalo Law Review},
  volume={53},
  pages={141},
  year={2005},
  publisher={HeinOnline}
}

@article{mohler1999toward,
  title={Toward a better understanding of substantial similarity in copyright infringement cases},
  author={Mohler, Jarrod M},
  journal={U. Cin. L. Rev.},
  volume={68},
  pages={971},
  year={1999},
  publisher={HeinOnline}
}

@article{gervais2021ai,
  title={AI Derivatives: The Application to the Derivative Work Right to Literary and Artistic Productions of AI Machines},
  author={Gervais, Daniel J},
  journal={Seton Hall L. Rev.},
  volume={52},
  pages={1111},
  year={2021},
  publisher={HeinOnline}
}

@online{Suggs2023,
  author = {Neal Suggs and Phil Venables},
  title = {Protecting customers with generative {AI} indemnification},
  year = {2023},
  url = {https://cloud.google.com/blog/products/ai-machine-learning/protecting-customers-with-generative-ai-indemnification},
  organization = {Google Cloud}
}

@article{henderson2023foundation,
  title={Foundation models and fair use},
  author={Henderson, Peter and Li, Xuechen and Jurafsky, Dan and Hashimoto, Tatsunori and Lemley, Mark A and Liang, Percy},
  journal={arXiv preprint arXiv:2303.15715},
  year={2023}
}

@article{vyas2023provable,
  title={Provable copyright protection for generative models},
  author={Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
  journal={arXiv preprint arXiv:2302.10870},
  year={2023}
}

@article{lemley2020fair,
  title={Fair learning},
  author={Lemley, Mark A and Casey, Bryan},
  journal={Texas Law Review},
  volume={99},
  pages={743},
  year={2020},
  publisher={HeinOnline}
}

@inproceedings{carlini2022quantifying,
  title={Quantifying Memorization Across Neural Language Models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{elangovan-etal-2021-memorization,
    title = "Memorization vs. Generalization : Quantifying Data Leakage in {NLP} Performance Evaluation",
    author = "Elangovan, Aparna  and
      He, Jiayuan  and
      Verspoor, Karin",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.113",
    doi = "10.18653/v1/2021.eacl-main.113",
    pages = "1325--1335",
}

@article{luo2023wizardcoder,
  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{bender2011achieving,
  title={On achieving and evaluating language-independence in NLP},
  author={Bender, Emily M},
  journal={Linguistic Issues in Language Technology},
  volume={6},
  year={2011}
}

@misc{Durbin2023Airoboros,
    author={Durbin, Jon},
    title={Airoboros: Using large language models to fine-tune large language models},
    year={2023},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/jondurbin/airoboros}},
}

@misc{xu2023baize,
      title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data}, 
      author={Canwen Xu and Daya Guo and Nan Duan and Julian McAuley},
      year={2023},
      eprint={2304.01196},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kryscinski2022booksum,
      title={BookSum: A Collection of Datasets for Long-form Narrative Summarization}, 
      author={Wojciech Kryściński and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},
      year={2022},
      eprint={2105.08209},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023camel,
      title={CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society}, 
      author={Guohao Li and Hasan Abed Al Kader Hammoud and Hani Itani and Dmitrii Khizbullin and Bernard Ghanem},
      year={2023},
      eprint={2303.17760},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{kim2023cot,
      title={The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning}, 
      author={Seungone Kim and Se June Joo and Doyoung Kim and Joel Jang and Seonghyeon Ye and Jamin Shin and Minjoon Seo},
      year={2023},
      eprint={2305.14045},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@misc{patil2023gorilla,
      title={Gorilla: Large Language Model Connected with Massive APIs}, 
      author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
      year={2023},
      eprint={2305.15334},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{guo2023close,
      title={How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection}, 
      author={Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},
      year={2023},
      eprint={2301.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2023lima,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{koksal2023longform,
      title={LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction}, 
      author={Abdullatif Köksal and Timo Schick and Anna Korhonen and Hinrich Schütze},
      year={2023},
      eprint={2304.08460},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt4all,
  author = {Yuvanesh Anand and Zach Nussbaum and Brandon Duderstadt and Benjamin Schmidt and Andriy Mulyar},
  title = {GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/nomic-ai/gpt4all}},
}

@misc{mukherjee2023orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, 
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qin2023toolllm,
      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2307.16789},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ding2023enhancing,
      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, 
      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
      year={2023},
      eprint={2305.14233},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{honovich2022unnatural,
      title={Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor}, 
      author={Or Honovich and Thomas Scialom and Omer Levy and Timo Schick},
      year={2022},
      eprint={2212.09689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ioannidis2023chatgpt,
  title={Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?},
  author={Ioannidis, Dimitrios and Kepner, Jeremy and Bowne, Andrew and Bryant, Harriet S},
  journal={arXiv preprint arXiv:2306.09267},
  year={2023}
}

##############
# Related Work
##############
@article{Bommasani2023EcosystemGT,
  title={Ecosystem Graphs: The Social Footprint of Foundation Models},
  author={Rishi Bommasani and Dilara Soylu and Thomas Liao and Kathleen A. Creel and Percy Liang},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.15772},
  url={https://arxiv.org/abs/2303.15772}
}

@article{Soh2021BuildingLD,
  title={Building Legal Datasets},
  author={Jerrold Soh},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.02034},
  url={https://arxiv.org/abs/2111.02034}
}

@article{patterson2003copyright,
  title={Copyright in 1791: An Essay Concerning the Founers' View of the Copyright Power Granted to Congress in Article I, Section 8, Clause 8 of the US Constitution},
  author={Patterson, L},
  journal={Emory Law Journal},
  volume={52},
  pages={909},
  year={2003},
  publisher={HeinOnline}
}

@article{burger1988berne,
  title={The Berne Convention: Its history and its key role in the future},
  author={Burger, Peter},
  journal={Journal of Law and Technology},
  volume={3},
  pages={1},
  year={1988},
  publisher={HeinOnline}
}

@misc{von2003special,
  title={Special issue on open source software development},
  author={Von Krogh, Georg and Von Hippel, Eric},
  journal={Research Policy},
  volume={32},
  number={7},
  pages={1149--1157},
  year={2003},
  publisher={Elsevier}
}

@article{petrov2023language,
  title={Language Model Tokenizers Introduce Unfairness Between Languages},
  author={Petrov, Aleksandar and La Malfa, Emanuele and Torr, Philip HS and Bibi, Adel},
  journal={arXiv preprint arXiv:2305.15425},
  year={2023}
}

@article{huson2018copyright,
  title={I, copyright},
  author={Huson, Garrett},
  journal={Santa Clara High Tech. LJ},
  volume={35},
  pages={54},
  year={2018},
  publisher={HeinOnline}
}

@misc{weston2015aicomplete,
      title={Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}, 
      author={Jason Weston and Antoine Bordes and Sumit Chopra and Alexander M. Rush and Bart van Merriënboer and Armand Joulin and Tomas Mikolov},
      year={2015},
      eprint={1502.05698},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{li2023starcoder,
      title={StarCoder: may the source be with you!}, 
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and João Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Muñoz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{eldan2023tinystories,
      title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?}, 
      author={Ronen Eldan and Yuanzhi Li},
      year={2023},
      eprint={2305.07759},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{Meyer2023TheDM,
    author = {Meyer, Anna P. and Albarghouthi, Aws and D'Antoni, Loris},
    title = {The Dataset Multiplicity Problem: How Unreliable Data Impacts Predictions},
    year = {2023},
    isbn = {9798400701924},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3593013.3593988},
    doi = {10.1145/3593013.3593988},
    booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {193–204},
    numpages = {12},
    keywords = {Dataset multiplicity, data bias, procedural fairness, model multiplicity, model robustness},
    location = {Chicago, IL, USA},
    series = {FAccT '23}
}


@inproceedings{Sag2020TheNL,
  title={The New Legal Landscape for Text Mining and Machine Learning},
  author={Sag, Matthew J.},
  booktitle={Journal of the Copyright Society of the USA},
  year={2020},
}

@inproceedings{Sambasivan2021EveryoneWT,
    author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
    title = {“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI},
    year = {2021},
    isbn = {9781450380966},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3411764.3445518},
    doi = {10.1145/3411764.3445518},
    booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
    articleno = {39},
    numpages = {15},
    keywords = {Data, data politics, Ghana, data quality, data cascades, raters, Nigeria, AI, developers, Uganda, data collectors, USA, application-domain experts, high-stakes AI, ML, India, Kenya},
    location = {Yokohama, Japan},
    series = {CHI '21}
}

@inproceedings{rogers-2021-changing,
    title = "Changing the World by Changing the Data",
    author = "Rogers, Anna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.170",
    doi = "10.18653/v1/2021.acl-long.170",
    pages = "2182--2194",
}

@inproceedings{gururangan-etal-2018-annotation,
    title = "Annotation Artifacts in Natural Language Inference Data",
    author = "Gururangan, Suchin  and
      Swayamdipta, Swabha  and
      Levy, Omer  and
      Schwartz, Roy  and
      Bowman, Samuel  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2017",
    doi = "10.18653/v1/N18-2017",
    pages = "107--112",
}


@article{Corry2021ThePO,
  title={The Problem of Zombie Datasets: A Framework For Deprecating Datasets},
  author={Frances Corry and Hamsini Sridharan and Alexandra Sasha Luccioni and Mike Ananny and Jason Schultz and Kate Crawford},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.04424},
  url={https://arxiv.org/abs/2111.04424}
}

@inproceedings{Miceli2022DocumentingDP,
    author = {Miceli, Milagros and Yang, Tianling and Alvarado Garcia, Adriana and Posada, Julian and Wang, Sonja Mei and Pohl, Marc and Hanna, Alex},
    title = {Documenting Data Production Processes: A Participatory Approach for Data Work},
    year = {2022},
    issue_date = {November 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {6},
    number = {CSCW2},
    url = {https://doi.org/10.1145/3555623},
    doi = {10.1145/3555623},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = {nov},
    articleno = {510},
    numpages = {34},
    keywords = {machine learning, data annotation, data production, data labeling, dataset documentation, data work, transparency}
}

@inproceedings{Hutchinson2020TowardsAF,
    author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
    title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3442188.3445918},
    doi = {10.1145/3442188.3445918},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {560–575},
    numpages = {16},
    keywords = {datasets, requirements engineering, machine learning},
    location = {Virtual Event, Canada},
    series = {FAccT '21}
}

@article{Kapoor2023REFORMSRS,
  title={REFORMS: Reporting Standards for Machine Learning Based Science},
  author={Sayash Kapoor and Emily F. Cantrell and Kenny Peng and Thanh Hien Pham and Christopher A. Bail and Odd Erik Gundersen and Jake M. Hofman and Jessica R. Hullman and Michael A. Lones and Momin M. Malik and Priyanka Nanayakkara and Russel A. Poldrack and Inioluwa Deborah Raji and Michael Roberts and Matthew J. Salganik and Marta Serra-Garcia and Brandon M Stewart and Gilles Vandewiele and Arvind Narayanan},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.07832},
  url={https://arxiv.org/abs/2308.07832}
},

@inproceedings{min2023SILOLM,
  title={SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore},
  author={Min, Sewon and Gururangan, Suchin and Wallace, Eric and Shi, Weijia and Hajishirzi, Hannaneh and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models},
  year={2023}
}

@article{wei2023symbol,
  title={Symbol tuning improves in-context learning in language models},
  author={Wei, Jerry and Hou, Le and Lampinen, Andrew and Chen, Xiangning and Huang, Da and Tay, Yi and Chen, Xinyun and Lu, Yifeng and Zhou, Denny and Ma, Tengyu and others},
  journal={arXiv preprint arXiv:2305.08298},
  year={2023}
}

@article{refinedweb,
  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
  journal={arXiv preprint arXiv:2306.01116},
  eprint={2306.01116},
  eprinttype = {arXiv},
  url={https://arxiv.org/abs/2306.01116},
  year={2023}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{dolma,
  title = {{Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}},
  author = {Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Evan Pete Walsh and Hannaneh Hajishirzi and Noah A. Smith and Luke Zettlemoyer and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
  journal={Allen Institute for AI, Tech. Rep},
  year={2023}
}


@article{adelani2021masakhaner,
  title={MasakhaNER: Named entity recognition for African languages},
  author={Adelani, David Ifeoluwa and Abbott, Jade and Neubig, Graham and D’souza, Daniel and Kreutzer, Julia and Lignos, Constantine and Palen-Michel, Chester and Buzaaba, Happy and Rijhwani, Shruti and Ruder, Sebastian and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1116--1131},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{sambasivan2021everyone,
author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
title = {``{E}veryone Wants to Do the Model Work, Not the Data Work'': Data Cascades in High-Stakes {AI}},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445518},
doi = {10.1145/3411764.3445518},
booktitle = {CHI},
location = {Yokohama, Japan},
series = {CHI '21}
}


@article{lee2023talkin,
  title={Talkin''Bout AI Generation: Copyright and the Generative-AI Supply Chain},
  author={Lee, Katherine and Cooper, A Feder and Grimmelmann, James},
  journal={arXiv preprint arXiv:2309.08133},
  year={2023}
}

@article{raji2022actionable,
  title={Actionable Auditing Revisited: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
  author={Raji, Inioluwa Deborah and Buolamwini, Joy},
  journal={Communications of the ACM},
  volume={66},
  number={1},
  pages={101--108},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@article{biderman2022datasheet,
  title={Datasheet for the pile},
  author={Biderman, Stella and Bicheno, Kieran and Gao, Leo},
  journal={arXiv preprint arXiv:2201.07311},
  year={2022}
}

@article{gaia,
  title={GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration},
  author={Piktus, Aleksandra and Ogundepo, Odunayo and Akiki, Christopher and Oladipo, Akintunde and Zhang, Xinyu and Schoelkopf, Hailey and Biderman, Stella and Potthast, Martin and Lin, Jimmy},
  journal={arXiv preprint arXiv:2306.01481},
  year={2023}
}

@article{kreutzer2022quality,
  title={Quality at a glance: An audit of web-crawled multilingual datasets},
  author={Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={50--72},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{mahadev2021understanding,
  title={Understanding Gender and Racial Disparities in Image Recognition Models},
  author={Mahadev, Rohan and Chakravarti, Anindya},
  journal={arXiv preprint arXiv:2107.09211},
  year={2021}
}

@article{parrish2021bbq,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2110.08193},
  year={2021}
}


@inproceedings{talat2022you,
  title={You reap what you sow: On the challenges of bias evaluation under multilingual settings},
  author={Talat, Zeerak and Neveol, Aurelie and Biderman, Stella and Clinciu, Miruna and Dey, Manan and Longpre, Shayne and Luccioni, Sasha and Masoud, Maraim and Mitchell, Margaret and Radev, Dragomir and others},
  booktitle={Proceedings of BigScience Episode\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models},
  pages={26--41},
  year={2022}
}

@inproceedings{de2019does,
  title={Does object recognition work for everyone?},
  author={De Vries, Terrance and Misra, Ishan and Wang, Changhan and Van der Maaten, Laurens},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={52--59},
  year={2019}
}

@article{shankar2017no,
  title={No classification without representation: Assessing geodiversity issues in open data sets for the developing world},
  author={Shankar, Shreya and Halpern, Yoni and Breck, Eric and Atwood, James and Wilson, Jimbo and Sculley, D},
  journal={arXiv preprint arXiv:1711.08536},
  year={2017}
}

@article{mcmillan2022documenting,
  title={Documenting geographically and contextually diverse data sources: The bigscience catalogue of language data and resources},
  author={McMillan-Major, Angelina and Alyafeai, Zaid and Biderman, Stella and Chen, Kimbo and De Toni, Francesco and Dupont, G{\'e}rard and Elsahar, Hady and Emezue, Chris and Aji, Alham Fikri and Ili{\'c}, Suzana and others},
  journal={arXiv preprint arXiv:2201.10066},
  year={2022}
}

@inproceedings{jernite2022data,
  title={Data governance in the age of large-scale data-driven language technology},
  author={Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2206--2222},
  year={2022}
}

@inproceedings{chan2023reclaiming,
author = {Chan, Alan and Bradley, Herbie and Rajkumar, Nitarshan},
title = {Reclaiming the Digital Commons: A Public Data Trust for Training Data},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3600211.3604658},
doi = {10.1145/3600211.3604658},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {855–868},
numpages = {14},
series = {AIES '23}
}

@article{viswanathan2023datafinder,
  title={DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions},
  author={Viswanathan, Vijay and Gao, Luyu and Wu, Tongshuang and Liu, Pengfei and Neubig, Graham},
  journal={arXiv preprint arXiv:2305.16636},
  year={2023}
}

@inproceedings{farber2021datahunter,
  title={Datahunter: A system for finding datasets based on scientific problem descriptions},
  author={F{\"a}rber, Michael and Leisinger, Ann-Kathrin},
  booktitle={Proceedings of the 15th ACM Conference on Recommender Systems},
  pages={749--752},
  year={2021}
}

@article{paullada2021data,
  title={Data and its (dis) contents: A survey of dataset development and use in machine learning research},
  author={Paullada, Amandalynne and Raji, Inioluwa Deborah and Bender, Emily M and Denton, Emily and Hanna, Alex},
  journal={Patterns},
  volume={2},
  number={11},
  year={2021},
  publisher={Elsevier}
}

@online{cen2023aisupply,
    author = "Sarah H. Cen and Aspen Hopkins and Andrew Ilyas and Aleksander Madry and Isabella Struckman and Luis Videgaray",
    title = "AI supply chains (and why they matter)",
    year = "2023",
    month = "April",
    note = "The second post in our series On AI Deployment",
    url = "https://aipolicy.substack.com/p/supply-chains-2"
}

@misc{tremblay2023openai,
    title = {Paul Tremblay, Mona Awad vs. OpenAI, Inc., et al.},
    author = {Saveri, Joseph R. and Zirpoli, Cadio and Young, Christopher K.L. and McMahon, Kathleen J.},
    year = {2023},
    note = {Case 3:23-cv-03223-AMO Document 1 Filed 06/28/23, UNITED STATES DISTRICT COURT, NORTHERN DISTRICT OF CALIFORNIA, SAN FRANCISCO DIVISION},
    url = {https://storage.courtlistener.com/recap/gov.uscourts.cand.414822/gov.uscourts.cand.414822.1.0\_1.pdf}
}

@inproceedings{crisan2022interactive,
  title={Interactive model cards: A human-centered approach to model documentation},
  author={Crisan, Anamaria and Drouhard, Margaret and Vig, Jesse and Rajani, Nazneen},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={427--439},
  year={2022}
}


@misc{bommasani2023foundation,
      title={The Foundation Model Transparency Index}, 
      author={Rishi Bommasani and Kevin Klyman and Shayne Longpre and Sayash Kapoor and Nestor Maslej and Betty Xiong and Daniel Zhang and Percy Liang},
      year={2023},
      eprint={2310.12941},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{bommasani2023ecosystem,
  title={Ecosystem graphs: The social footprint of foundation models},
  author={Bommasani, Rishi and Soylu, Dilara and Liao, Thomas I and Creel, Kathleen A and Liang, Percy},
  journal={arXiv preprint arXiv:2303.15772},
  year={2023}
}

@misc{AI2ImpACTLicenses,
  author = {{Allen Institute}},
  title = {AI2 ImpACT Licenses},
  year = {2023},
  url = {https://allenai.org/impact-license#licenses},
}

@inproceedings{contractor2022behavioral,
  title={Behavioral use licensing for responsible AI},
  author={Contractor, Danish and McDuff, Daniel and Haines, Julia Katherine and Lee, Jenny and Hines, Christopher and Hecht, Brent and Vincent, Nicholas and Li, Hanlin},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={778--788},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{ziems2023can,
  title={Can large language models transform computational social science?},
  author={Ziems, Caleb and Shaikh, Omar and Zhang, Zhehao and Held, William and Chen, Jiaao and Yang, Diyi},
  journal={Computational Linguistics},
  pages={1--53},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{thevergeGettyImages2023,
	author = {James Vincent},
	title = {{G}etty {I}mages sues {A}{I} art generator {S}table {D}iffusion in the {U}{S} for copyright infringement --- theverge.com},
	howpublished = {\url{https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion}},
	year = {2023},
	note = {[Accessed 18-12-2023]},
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@inproceedings{ahia-etal-2021-low-resource,
    title = "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation",
    author = "Ahia, Orevaoghene  and
      Kreutzer, Julia  and
      Hooker, Sara",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.282",
    doi = "10.18653/v1/2021.findings-emnlp.282",
    pages = "3316--3333",
}

@misc{chu2023language,
      title={Language Models Trained on Media Diets Can Predict Public Opinion}, 
      author={Eric Chu and Jacob Andreas and Stephen Ansolabehere and Deb Roy},
      year={2023},
      eprint={2303.16779},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pozzobon2023challenges,
      title={On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research}, 
      author={Luiza Pozzobon and Beyza Ermis and Patrick Lewis and Sara Hooker},
      year={2023},
      eprint={2304.12397},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yoo2022scalable,
      title={Scalable Training of Language Models using JAX pjit and TPUv4}, 
      author={Joanna Yoo and Kuba Perlin and Siddhartha Rao Kamalakara and João G. M. Araújo},
      year={2022},
      eprint={2204.06514},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang2022benchmarking,
  title={Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022},
  url={https://arxiv.org/abs/2204.07705}
}

@inproceedings{lhoest2021datasets,
  title={Datasets: A Community Library for Natural Language Processing},
  author={Lhoest, Quentin and del Moral, Albert Villanova and Jernite, Yacine and Thakur, Abhishek and von Platen, Patrick and Patil, Suraj and Chaumond, Julien and Drame, Mariama and Plu, Julien and Tunstall, Lewis and others},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={175--184},
  year={2021}
}


@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={TMLR},
  year={2022},
  url={https://openreview.net/forum?id=yzkSU5zdwD},
}

@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford\_alpaca}},
}

@misc{selfinstruct2022,
  doi = {10.48550/ARXIV.2212.10560},
  url = {https://arxiv.org/abs/2212.10560},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{lmsys2023,
  author = {LMSYS-Org},
  title = {LM-SYS: FastChatT5},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/lm-sys/FastChat#FastChat-T5}},
}

@misc{mosailml-jonathan-2023,
  author = {Jonathan Frankle},
  title = {Tweet by Mosaic ML},
  year = {2023},
  publisher = {Twitter},
  journal = {Twitter},
  howpublished = {\url{https://twitter.com/jefrankle/status/1654848529834078208}},
}

@misc{dolly15k-2023,
  author = {Conover, Mike and Hayes, Matt and Mathur, Ankit and Meng, Xiangrui and  Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
  title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year = {2023},
  publisher = {Databricks},
  journal = {Databricks},
  howpublished = {\url{https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}},
}

@misc{oig2023,
  author = {Nguyen, Huu and Suri, Sameer and Tsui, Ken and Schuhmann, Christoph},
  title = {The Open Instruction Generalist (OIG) Dataset},
  year = {2023},
  publisher = {LAION Blog},
  journal = {LAION Blog},
  howpublished = {\url{https://laion.ai/blog/oig-dataset/}},
}

@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{gangulired,
    author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
    title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
    year={2022}
}

@article{sobel2017artificial,
  title={Artificial Intelligence's Fair Use Crisis},
  author={Sobel, Benjamin LW},
  journal={Columbia Journal of Law \& the Arts},
  volume={41},
  pages={45},
  year={2017},
}

@article{quang2021does,
  title={Does Training AI Violate Copyright Law?},
  author={Quang, Jenny},
  journal={Berkeley Tech. LJ},
  volume={36},
  pages={1407},
  year={2021},
  publisher={HeinOnline}
}

@online{SHP,
  author = {Ethayarajh, Kawin and Zhang, Heidi and Wang, Yizhong and Jurafsky, Dan},
  title = {Stanford Human Preferences Dataset},
  year = {2023},
  url = {https://huggingface.co/datasets/stanfordnlp/SHP}
}

@online{sharegpt,
  author = {Vercel},
  title = {ShareGPT},
  year = {2023},
  url = {https://sharegpt.com/}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{stienon2020learning,
  author = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  title = {Learning to summarize from human feedback},
  booktitle = {NeurIPS},
  year = 2020,
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pushkarna2022data,
  title={Data cards: Purposeful and transparent dataset documentation for responsible ai},
  author={Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1776--1826},
  year={2022}
}

@online{arstechnica,
  author = {Arstechnica},
  title = {Stable Diffusion Copyright Lawsuits Could be a Legal Earthquake for AI},
  year = {2023},
  url = {https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/}
}

@misc{koala-blogpost-2023,
  author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},
  title = {Koala: A Dialogue Model for Academic Research},
  howpublished = {Blog post},
  month = {April},
  year = {2023},
  url = {https://bair.berkeley.edu/blog/2023/04/03/koala/},
  urldate = {2023-04-03}
}

@article{anil2023palm,
  title={PaLM 2 Technical Report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@inproceedings{weifinetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  year={2021},
  booktitle={International Conference on Learning Representations}
}

@article{Kinney2023TheSS,
  author = {Kinney, Rodney Michael and Anastasiades, Chloe and Authur, Russell and Beltagy, Iz and Bragg, Jonathan and Buraczynski, Alexandra and Cachola, Isabel and Candra, Stefan and Chandrasekhar, Yoganand and Cohan, Arman and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Etzioni, Oren and Evans, Rob and Feldman, Sergey and Gorney, Joseph and Graham, David W. and Hu, F.Q. and Huff, Regan and King, Daniel and Kohlmeier, Sebastian and Kuehl, Bailey and Langan, Michael and Lin, Daniel and Liu, Haokun and Lo, Kyle and Lochner, Jaron and MacMillan, Kelsey and Murray, Tyler and Newell, Christopher and Rao, Smita and Rohatgi, Shaurya and Sayre, Paul L and Shen, Zejiang and Singh, Amanpreet and Soldaini, Luca and Subramanian, Shivashankar and Tanaka, A. and Wade, Alex D and Wagner, Linda M. and Wang, Lucy Lu and Wilhelm, Christopher and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele and van Zuylen, Madeleine and Weld, Daniel S.},
  journal = {ArXiv},
  year = {2023},
  volume = {abs/2301.10140},
}


@article{sileo2023tasksource,
      title={tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation}, 
      author={Damien Sileo},
      year={2023},
      volume={abs/2301.05948},
      journal={arXiv},
}

@article{scao2022language,
  title={What language model to train if you have one million GPU hours?},
  author={Scao, Teven Le and Wang, Thomas and Hesslow, Daniel and Saulnier, Lucile and Bekman, Stas and Bari, M Saiful and Bideman, Stella and Elsahar, Hady and Muennighoff, Niklas and Phang, Jason and others},
  journal={arXiv preprint arXiv:2210.15424},
  year={2022}
}

@article{epstein2023art,
  title={Art and the science of generative AI},
  author={Epstein, Ziv and Hertzmann, Aaron and Herman, Laura and Mahari, Robert and Frank, Morgan R and Groh, Matthew and Schroeder, Hope and Smith, Amy and Akten, Memo and Fjeld, Jessica and others},
  journal={Science},
  volume={380},
  number={6650},
  pages={1110--1111},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{muennighoff2023scaling,
  title={Scaling Data-Constrained Language Models},
  author={Muennighoff, Niklas and Rush, Alexander M and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  journal={arXiv preprint arXiv:2305.16264},
  year={2023}
}



@inproceedings{wang2022super,
  title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@inproceedings{welbl2021challenges,
  title={Challenges in Detoxifying Language Models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2447--2469},
  year={2021}
}

@inproceedings{xu2021detoxifying,
  title={Detoxifying Language Models Risks Marginalizing Minority Voices},
  author={Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2390--2397},
  year={2021}
}

@article{VictorEfrati2023,
  title     = {Alphabet's Google and DeepMind Pause Grudges, Join Forces to Chase OpenAI},
  author    = {Jon Victor and Amir Efrati},
  journal   = {The Information},
  year      = {2023},
}

@article{meeker2022beyond,
  title={Beyond Open Data: The Only Good License Is No License},
  author={Meeker, Heather},
  journal={PLI Chronicle: Insights and Perspectives for the Legal Community},
  year={2022},
  month={April},
}

@book{ricketson2022international,
  title={International Copyright and Neighboring Rights: The Berne Convention and Beyond},
  author={Ricketson, Sam and Ginsburg, Jane C.},
  year={2022},
  month={August},
  publisher={Oxford University Press},
}

@article{cohen1986masking,
  title={Masking Copyright Decisionmaking: The Meaninglessness of Substantial Similarity},
  author={Cohen, Amy B},
  journal={UC Davis Law Review},
  volume={20},
  pages={719},
  year={1986},
  publisher={HeinOnline}
}

@article{balganesh2014judging,
  title={Judging similarity},
  author={Balganesh, Shyamkrishna and Manta, Irina D and Wilkinson-Ryan, Tess},
  journal={Iowa Law Review},
  volume={100},
  pages={267},
  year={2014},
  publisher={HeinOnline}
}

@book{marks2023law,
  title={The Law of Business Torts and Unfair Competition: Cases, Materials, and Problems},
  author={Marks, Colin P. and Moll, Douglas K.},
  isbn={9781647084905},
  series={American Casebook Series},
  url={https://books.google.com/books?id=K1fXzwEACAAJ},
  year={2023},
  publisher={West Academic}
}

@article{sag2023copyright,
  title={Copyright safety for generative AI},
  author={Sag, Matthew},
  journal={Forthcoming in the Houston Law Review},
  year={2023}
}

@article{samuelson2023generative,
  title={Generative AI meets copyright},
  author={Samuelson, Pamela},
  journal={Science},
  volume={381},
  number={6654},
  pages={158--161},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}


@article{derclaye2022sui,
  title={Sui generis database protection 2.0: judicial and legislative reforms},
  author={Derclaye, Estelle and Husovec, Martin},
  journal={European Intellectual Property Review},
  volume={44},
  number={6},
  pages={323--331},
  year={2022}
}


@article{margoni2022deeper,
  title={A DEEPER LOOK INTO the EU text and data mining exceptions: harmonisation, data ownership, and the future of technology},
  author={Margoni, Thomas and Kretschmer, Martin},
  journal={GRUR International},
  volume={71},
  number={8},
  pages={685--701},
  year={2022},
  publisher={Oxford University Press UK}
}

@article{ginsburg1992no,
  title={No Sweat Copyright and Other Protection of Works of Information after Feist v. Rural Telephone},
  author={Ginsburg, Jane C},
  journal={Columbia Law Review},
  volume={92},
  pages={338},
  year={1992},
  publisher={HeinOnline}
}

@article{grossman2005sony,
  title={From Sony to Grokster, the failure of the copyright doctrines of contributory infringement and vicarious liability to resolve the war between content and destructive technologies},
  author={Grossman, Craig A},
  journal={Buffalo Law Review},
  volume={53},
  pages={141},
  year={2005},
  publisher={HeinOnline}
}

@article{mohler1999toward,
  title={Toward a better understanding of substantial similarity in copyright infringement cases},
  author={Mohler, Jarrod M},
  journal={U. Cin. L. Rev.},
  volume={68},
  pages={971},
  year={1999},
  publisher={HeinOnline}
}

@article{gervais2021ai,
  title={AI Derivatives: The Application to the Derivative Work Right to Literary and Artistic Productions of AI Machines},
  author={Gervais, Daniel J},
  journal={Seton Hall L. Rev.},
  volume={52},
  pages={1111},
  year={2021},
  publisher={HeinOnline}
}

@online{Suggs2023,
  author = {Neal Suggs and Phil Venables},
  title = {Protecting customers with generative {AI} indemnification},
  year = {2023},
  url = {https://cloud.google.com/blog/products/ai-machine-learning/protecting-customers-with-generative-ai-indemnification},
  organization = {Google Cloud}
}

@article{henderson2023foundation,
  title={Foundation models and fair use},
  author={Henderson, Peter and Li, Xuechen and Jurafsky, Dan and Hashimoto, Tatsunori and Lemley, Mark A and Liang, Percy},
  journal={arXiv preprint arXiv:2303.15715},
  year={2023}
}

@article{vyas2023provable,
  title={Provable copyright protection for generative models},
  author={Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
  journal={arXiv preprint arXiv:2302.10870},
  year={2023}
}

@article{lemley2020fair,
  title={Fair learning},
  author={Lemley, Mark A and Casey, Bryan},
  journal={Texas Law Review},
  volume={99},
  pages={743},
  year={2020},
  publisher={HeinOnline}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{carlini2022quantifying,
  title={Quantifying Memorization Across Neural Language Models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{elangovan-etal-2021-memorization,
    title = "Memorization vs. Generalization : Quantifying Data Leakage in {NLP} Performance Evaluation",
    author = "Elangovan, Aparna  and
      He, Jiayuan  and
      Verspoor, Karin",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.113",
    doi = "10.18653/v1/2021.eacl-main.113",
    pages = "1325--1335",
}

@article{luo2023wizardcoder,
  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@techreport{McElheran2023,
  title = {AI Adoption in America: Who, What, and Where},
  url = {http://dx.doi.org/10.3386/w31788},
  doi = {10.3386/w31788},
  institution = {National Bureau of Economic Research},
  type = {Working Paper},
  number = 31788,
  author = {McElheran,  Kristina and Li,  J. Frank and Brynjolfsson,  Erik and Kroff,  Zachary and Dinlersoz,  Emin and Foster,  Lucia and Zolas,  Nikolas},
  year = {2023},
  month = oct 
}

@article{holland2020dataset,
  title={The dataset nutrition label},
  author={Holland, Sarah and Hosny, Ahmed and Newman, Sarah and Joseph, Joshua and Chmielinski, Kasia},
  journal={Data Protection and Privacy},
  volume={12},
  number={12},
  pages={1},
  year={2020}
}

@article{li2023otterhd,
  title={OtterHD: A High-Resolution Multi-modality Model},
  author={Li, Bo and Zhang, Peiyuan and Yang, Jingkang and Zhang, Yuanhan and Pu, Fanyi and Liu, Ziwei},
  journal={arXiv preprint arXiv:2311.04219},
  year={2023}
}


@article{bender2011achieving,
  title={On achieving and evaluating language-independence in NLP},
  author={Bender, Emily M},
  journal={Linguistic Issues in Language Technology},
  volume={6},
  year={2011}
}

@misc{Durbin2023Airoboros,
    author={Durbin, Jon},
    title={Airoboros: Using large language models to fine-tune large language models},
    year={2023},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/jondurbin/airoboros}},
}

@misc{xu2023baize,
      title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data}, 
      author={Canwen Xu and Daya Guo and Nan Duan and Julian McAuley},
      year={2023},
      eprint={2304.01196},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kryscinski2022booksum,
      title={BookSum: A Collection of Datasets for Long-form Narrative Summarization}, 
      author={Wojciech Kryściński and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},
      year={2022},
      eprint={2105.08209},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023camel,
      title={CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society}, 
      author={Guohao Li and Hasan Abed Al Kader Hammoud and Hani Itani and Dmitrii Khizbullin and Bernard Ghanem},
      year={2023},
      eprint={2303.17760},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{kim2023cot,
      title={The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning}, 
      author={Seungone Kim and Se June Joo and Doyoung Kim and Joel Jang and Seonghyeon Ye and Jamin Shin and Minjoon Seo},
      year={2023},
      eprint={2305.14045},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@misc{patil2023gorilla,
      title={Gorilla: Large Language Model Connected with Massive APIs}, 
      author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
      year={2023},
      eprint={2305.15334},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{guo2023close,
      title={How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection}, 
      author={Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},
      year={2023},
      eprint={2301.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2023lima,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{koksal2023longform,
      title={LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction}, 
      author={Abdullatif Köksal and Timo Schick and Anna Korhonen and Hinrich Schütze},
      year={2023},
      eprint={2304.08460},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt4all,
  author = {Yuvanesh Anand and Zach Nussbaum and Brandon Duderstadt and Benjamin Schmidt and Andriy Mulyar},
  title = {GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/nomic-ai/gpt4all}},
}

@misc{mukherjee2023orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, 
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qin2023toolllm,
      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2307.16789},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ding2023enhancing,
      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, 
      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
      year={2023},
      eprint={2305.14233},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{honovich2022unnatural,
      title={Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor}, 
      author={Or Honovich and Thomas Scialom and Omer Levy and Timo Schick},
      year={2022},
      eprint={2212.09689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ioannidis2023chatgpt,
  title={Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?},
  author={Ioannidis, Dimitrios and Kepner, Jeremy and Bowne, Andrew and Bryant, Harriet S},
  journal={arXiv preprint arXiv:2306.09267},
  year={2023}
}

@misc{dellacquaNavigatingJaggedTechnological2023,
  author = {Dell'Acqua, Fabrizio and McFowland, Edward and Mollick, Ethan R. and Lifshitz-Assaf, Hila and Kellogg, Katherine and Rajendran, Saran and Krayer, Lisa and Candelon, François and Lakhani, Karim R.},
  title = {Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality},
  year = {2023},
  howpublished = {SSRN Scholarly Paper No. 4573321},
  note = {Available at SSRN: https://ssrn.com/abstract=4573321},
  doi = {10.2139/ssrn.4573321},
}

##############
# Related Work
##############
@article{Bommasani2023EcosystemGT,
  title={Ecosystem Graphs: The Social Footprint of Foundation Models},
  author={Rishi Bommasani and Dilara Soylu and Thomas Liao and Kathleen A. Creel and Percy Liang},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.15772},
  url={https://arxiv.org/abs/2303.15772}
}

@article{Soh2021BuildingLD,
  title={Building Legal Datasets},
  author={Jerrold Soh},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.02034},
  url={https://arxiv.org/abs/2111.02034}
}

@article{patterson2003copyright,
  title={Copyright in 1791: An Essay Concerning the Founers' View of the Copyright Power Granted to Congress in Article I, Section 8, Clause 8 of the US Constitution},
  author={Patterson, L},
  journal={Emory Law Journal},
  volume={52},
  pages={909},
  year={2003},
  publisher={HeinOnline}
}

@article{burger1988berne,
  title={The Berne Convention: Its history and its key role in the future},
  author={Burger, Peter},
  journal={Journal of Law and Technology},
  volume={3},
  pages={1},
  year={1988},
  publisher={HeinOnline}
}

@misc{von2003special,
  title={Special issue on open source software development},
  author={Von Krogh, Georg and Von Hippel, Eric},
  journal={Research Policy},
  volume={32},
  number={7},
  pages={1149--1157},
  year={2003},
  publisher={Elsevier}
}

@article{petrov2023language,
  title={Language Model Tokenizers Introduce Unfairness Between Languages},
  author={Petrov, Aleksandar and La Malfa, Emanuele and Torr, Philip HS and Bibi, Adel},
  journal={arXiv preprint arXiv:2305.15425},
  year={2023}
}

@article{huson2018copyright,
  title={I, copyright},
  author={Huson, Garrett},
  journal={Santa Clara High Tech. LJ},
  volume={35},
  pages={54},
  year={2018},
  publisher={HeinOnline}
}

@misc{weston2015aicomplete,
      title={Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}, 
      author={Jason Weston and Antoine Bordes and Sumit Chopra and Alexander M. Rush and Bart van Merriënboer and Armand Joulin and Tomas Mikolov},
      year={2015},
      eprint={1502.05698},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{li2023starcoder,
      title={StarCoder: may the source be with you!}, 
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and João Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Muñoz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{eldan2023tinystories,
      title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?}, 
      author={Ronen Eldan and Yuanzhi Li},
      year={2023},
      eprint={2305.07759},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{Meyer2023TheDM,
    author = {Meyer, Anna P. and Albarghouthi, Aws and D'Antoni, Loris},
    title = {The Dataset Multiplicity Problem: How Unreliable Data Impacts Predictions},
    year = {2023},
    isbn = {9798400701924},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3593013.3593988},
    doi = {10.1145/3593013.3593988},
    abstract = {We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfactual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets’ factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label errors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predictions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted.},
    booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {193–204},
    numpages = {12},
    keywords = {Dataset multiplicity, data bias, procedural fairness, model multiplicity, model robustness},
    location = {Chicago, IL, USA},
    series = {FAccT '23}
}


@inproceedings{Sag2020TheNL,
  title={The New Legal Landscape for Text Mining and Machine Learning},
  author={Sag, Matthew J.},
  booktitle={Journal of the Copyright Society of the USA},
  year={2020},
}


@misc{elazar2023whats,
      title={What's In My Big Data?}, 
      author={Yanai Elazar and Akshita Bhagia and Ian Magnusson and Abhilasha Ravichander and Dustin Schwenk and Alane Suhr and Pete Walsh and Dirk Groeneveld and Luca Soldaini and Sameer Singh and Hanna Hajishirzi and Noah A. Smith and Jesse Dodge},
      year={2023},
      eprint={2310.20707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{rogers-2021-changing,
    title = "Changing the World by Changing the Data",
    author = "Rogers, Anna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.170",
    doi = "10.18653/v1/2021.acl-long.170",
    pages = "2182--2194",
}


@inproceedings{gururangan-etal-2018-annotation,
    title = "Annotation Artifacts in Natural Language Inference Data",
    author = "Gururangan, Suchin  and
      Swayamdipta, Swabha  and
      Levy, Omer  and
      Schwartz, Roy  and
      Bowman, Samuel  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2017",
    doi = "10.18653/v1/N18-2017",
    pages = "107--112",
    abstract = "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67{\%} of SNLI (Bowman et. al, 2015) and 53{\%} of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.",
}


@article{Corry2021ThePO,
  title={The Problem of Zombie Datasets: A Framework For Deprecating Datasets},
  author={Frances Corry and Hamsini Sridharan and Alexandra Sasha Luccioni and Mike Ananny and Jason Schultz and Kate Crawford},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.04424},
  url={https://arxiv.org/abs/2111.04424}
}

@inproceedings{Miceli2022DocumentingDP,
    author = {Miceli, Milagros and Yang, Tianling and Alvarado Garcia, Adriana and Posada, Julian and Wang, Sonja Mei and Pohl, Marc and Hanna, Alex},
    title = {Documenting Data Production Processes: A Participatory Approach for Data Work},
    year = {2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {6},
    number = {CSCW2},
    url = {https://doi.org/10.1145/3555623},
    doi = {10.1145/3555623},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = {nov},
    articleno = {510},
    numpages = {34},
    keywords = {machine learning, data annotation, data production, data labeling, dataset documentation, data work, transparency}
}

@inproceedings{Hutchinson2020TowardsAF,
    author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
    title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3442188.3445918},
    doi = {10.1145/3442188.3445918},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {560–575},
    numpages = {16},
    keywords = {datasets, requirements engineering, machine learning},
    location = {Virtual Event, Canada},
    series = {FAccT '21}
}

@inproceedings{dingGPT3GoodData2023,
    title = "Is {GPT}-3 a Good Data Annotator?",
    author = "Ding, Bosheng  and
      Qin, Chengwei  and
      Liu, Linlin  and
      Chia, Yew Ken  and
      Li, Boyang  and
      Joty, Shafiq  and
      Bing, Lidong",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.626",
    doi = "10.18653/v1/2023.acl-long.626",
    pages = "11173--11195",
    abstract = "Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.",
}

@article{gilardiChatGPTOutperformsCrowd2023,
author = {Fabrizio Gilardi  and Meysam Alizadeh  and Maël Kubli },
title = {ChatGPT outperforms crowd workers for text-annotation tasks},
journal = {Proceedings of the National Academy of Sciences},
volume = {120},
number = {30},
pages = {e2305016120},
year = {2023},
doi = {10.1073/pnas.2305016120},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2305016120},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2305016120},
}

@inproceedings{suarez2019asynchronous,
  title={Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures},
  author={Su{\'a}rez, Pedro Javier Ortiz and Sagot, Beno{\^\i}t and Romary, Laurent},
  booktitle={7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)},
  year={2019},
  organization={Leibniz-Institut f{\"u}r Deutsche Sprache}
}

@article{galvez2021people,
  title={The people's speech: A large-scale diverse english speech recognition dataset for commercial usage},
  author={Galvez, Daniel and Diamos, Greg and Ciro, Juan and Cer{\'o}n, Juan Felipe and Achorn, Keith and Gopi, Anjali and Kanter, David and Lam, Maximilian and Mazumder, Mark and Reddi, Vijay Janapa},
  journal={arXiv preprint arXiv:2111.09344},
  year={2021}
}

@article{laurenccon2023obelisc,
  title={Obelisc: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander M and Kiela, Douwe and others},
  journal={arXiv preprint arXiv:2306.16527},
  year={2023}
}

@article{zhu2023multimodal,
  title={Multimodal c4: An open, billion-scale corpus of images interleaved with text},
  author={Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
  journal={arXiv preprint arXiv:2304.06939},
  year={2023}
}

@article{wang2021voxpopuli,
  title={VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation},
  author={Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2101.00390},
  year={2021}
}

@article{karpov2021golos,
  title={Golos: Russian dataset for speech research},
  author={Karpov, Nikolay and Denisenko, Alexander and Minkin, Fedor},
  journal={arXiv preprint arXiv:2106.10161},
  year={2021}
}

@article{chen2021gigaspeech,
  title={Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio},
  author={Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and others},
  journal={arXiv preprint arXiv:2106.06909},
  year={2021}
}

@inproceedings{kahn2020libri,
  title={Libri-light: A benchmark for asr with limited or no supervision},
  author={Kahn, Jacob and Rivi{\`e}re, Morgane and Zheng, Weiyi and Kharitonov, Evgeny and Xu, Qiantong and Mazar{\'e}, Pierre-Emmanuel and Karadayi, Julien and Liptchinsky, Vitaliy and Collobert, Ronan and Fuegen, Christian and others},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7669--7673},
  year={2020},
  organization={IEEE}
}

@inproceedings{javed2023indicsuperb,
  title={Indicsuperb: A speech processing universal performance benchmark for indian languages},
  author={Javed, Tahir and Bhogale, Kaushal and Raman, Abhigyan and Kumar, Pratyush and Kunchukuttan, Anoop and Khapra, Mitesh M},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={12942--12950},
  year={2023}
}

@article{piktus2023roots,
  title={The roots search tool: Data transparency for llms},
  author={Piktus, Aleksandra and Akiki, Christopher and Villegas, Paulo and Lauren{\c{c}}on, Hugo and Dupont, G{\'e}rard and Luccioni, Alexandra Sasha and Jernite, Yacine and Rogers, Anna},
  journal={arXiv preprint arXiv:2302.14035},
  year={2023}
}


@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}


@inproceedings{bhogale2023effectiveness,
  title={Effectiveness of mining audio and text pairs from public data for improving ASR systems for low-resource languages},
  author={Bhogale, Kaushal and Raman, Abhigyan and Javed, Tahir and Doddapaneni, Sumanth and Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh M},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{zhang2022wenetspeech,
  title={Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition},
  author={Zhang, Binbin and Lv, Hang and Guo, Pengcheng and Shao, Qijie and Yang, Chao and Xie, Lei and Xu, Xin and Bu, Hui and Chen, Xiaoyu and Zeng, Chenchen and others},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6182--6186},
  year={2022},
  organization={IEEE}
}


@inproceedings{tiedemann2012parallel,
  title={Parallel data, tools and interfaces in OPUS.},
  author={Tiedemann, J{\"o}rg},
  booktitle={Lrec},
  volume={2012},
  pages={2214--2218},
  year={2012},
  organization={Citeseer}
}

@article{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25278--25294},
  year={2022}
}



@article{wei2023symbol,
  title={Symbol tuning improves in-context learning in language models},
  author={Wei, Jerry and Hou, Le and Lampinen, Andrew and Chen, Xiangning and Huang, Da and Tay, Yi and Chen, Xinyun and Lu, Yifeng and Zhou, Denny and Ma, Tengyu and others},
  journal={arXiv preprint arXiv:2305.08298},
  year={2023}
}

@article{refinedweb,
  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
  journal={arXiv preprint arXiv:2306.01116},
  eprint={2306.01116},
  eprinttype = {arXiv},
  url={https://arxiv.org/abs/2306.01116},
  year={2023}
}

@article{bandy2021addressing,
  title={Addressing ``Documentation Debt'' in machine learning research: A retrospective datasheet for bookcorpus},
  author={Bandy, Jack and Vincent, Nicholas},
  journal={arXiv preprint arXiv:2105.05241},
  year={2021}
}

@article{lee2023talkin,
  title={Talkin''Bout AI Generation: Copyright and the Generative-AI Supply Chain},
  author={Lee, Katherine and Cooper, A Feder and Grimmelmann, James},
  journal={arXiv preprint arXiv:2309.08133},
  year={2023}
}

@article{raji2022actionable,
  title={Actionable Auditing Revisited: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
  author={Raji, Inioluwa Deborah and Buolamwini, Joy},
  journal={Communications of the ACM},
  volume={66},
  number={1},
  pages={101--108},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@misc{veselovsky2023artificial,
      title={Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks}, 
      author={Veniamin Veselovsky and Manoel Horta Ribeiro and Robert West},
      year={2023},
      eprint={2306.07899},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{marone2023data,
  title={Data portraits: Recording foundation model training data},
  author={Marone, Marc and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2303.03919},
  year={2023}
}

@article{oren2023proving,
  title={Proving test set contamination in black box language models},
  author={Oren, Yonatan and Meister, Nicole and Chatterji, Niladri and Ladhak, Faisal and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2310.17623},
  year={2023}
}

@article{birhane2023into,
  title={Into the LAIONs Den: Investigating Hate in Multimodal Datasets},
  author={Birhane, Abeba and Prabhu, Vinay and Han, Sang and Boddeti, Vishnu Naresh and Luccioni, Alexandra Sasha},
  journal={arXiv preprint arXiv:2311.03449},
  year={2023}
}

@article{birhane2021multimodal,
  title={Multimodal datasets: misogyny, pornography, and malignant stereotypes},
  author={Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel},
  journal={arXiv preprint arXiv:2110.01963},
  year={2021}
}

@article{birhane2023hate,
  title={On Hate Scaling Laws For Data-Swamps},
  author={Birhane, Abeba and Prabhu, Vinay and Han, Sang and Boddeti, Vishnu Naresh},
  journal={arXiv preprint arXiv:2306.13141},
  year={2023}
}



@article{jagielski2023note,
  title={A Note On Interpreting Canary Exposure},
  author={Jagielski, Matthew},
  journal={arXiv preprint arXiv:2306.00133},
  year={2023}
}


@article{gaia,
  title={GAIA Search Tool},
  author={Spacerini},
  url={https://huggingface.co/spaces/spacerini/gaia},
  year={2021}
}


@article{mahadev2021understanding,
  title={Understanding Gender and Racial Disparities in Image Recognition Models},
  author={Mahadev, Rohan and Chakravarti, Anindya},
  journal={arXiv preprint arXiv:2107.09211},
  year={2021}
}

@inproceedings{talat2022you,
  title={You reap what you sow: On the challenges of bias evaluation under multilingual settings},
  author={Talat, Zeerak and N{\'e}v{\'e}ol, Aur{\'e}lie and Biderman, Stella and Clinciu, Miruna and Dey, Manan and Longpre, Shayne and Luccioni, Sasha and Masoud, Maraim and Mitchell, Margaret and Radev, Dragomir and others},
  booktitle={Proceedings of BigScience Episode\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models},
  pages={26--41},
  year={2022}
}

@inproceedings{de2019does,
  title={Does object recognition work for everyone?},
  author={De Vries, Terrance and Misra, Ishan and Wang, Changhan and Van der Maaten, Laurens},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={52--59},
  year={2019}
}

@article{shankar2017no,
  title={No classification without representation: Assessing geodiversity issues in open data sets for the developing world},
  author={Shankar, Shreya and Halpern, Yoni and Breck, Eric and Atwood, James and Wilson, Jimbo and Sculley, D},
  journal={arXiv preprint arXiv:1711.08536},
  year={2017}
}

@inproceedings{NEURIPS2022_ce9e92e3,
 author = {Lauren\c{c}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz\'{a}lez Ponferrada, Eduardo and Nguyen, Huu and Frohberg, J\"{o}rg and \v{S}a\v{s}ko, Mario and Lhoest, Quentin and McMillan-Major, Angelina and Dupont, Gerard and Biderman, Stella and Rogers, Anna and Ben allal, Loubna and De Toni, Francesco and Pistilli, Giada and Nguyen, Olivier and Nikpoor, Somaieh and Masoud, Maraim and Colombo, Pierre and de la Rosa, Javier and Villegas, Paulo and Thrush, Tristan and Longpre, Shayne and Nagel, Sebastian and Weber, Leon and Mu\~{n}oz, Manuel and Zhu, Jian and Van Strien, Daniel and Alyafeai, Zaid and Almubarak, Khalid and Vu, Minh Chien and Gonzalez-Dios, Itziar and Soroa, Aitor and Lo, Kyle and Dey, Manan and Ortiz Suarez, Pedro and Gokaslan, Aaron and Bose, Shamik and Adelani, David and Phan, Long and Tran, Hieu and Yu, Ian and Pai, Suhas and Chim, Jenny and Lepercq, Violette and Ilic, Suzana and Mitchell, Margaret and Luccioni, Sasha Alexandra and Jernite, Yacine},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {31809--31826},
 publisher = {Curran Associates, Inc.},
 title = {The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset},
 url = {https://proceedings.neurips.cc/paper\_files/paper/2022/file/ce9e92e3de2372a4b93353eb7f3dc0bd-Paper-Datasets\_and\_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{laippala2022towards,
  title={Towards better structured and less noisy Web data: Oscar with Register annotations},
  author={Laippala, Veronika and Salmela, Anna and R{\"o}nnqvist, Samuel and Aji, Alham Fikri and Chang, Li-Hsin and Dhifallah, Asma and Goulart, Larissa and Kortelainen, Henna and P{\`a}mies, Marc and Dutra, Deise Prina and others},
  booktitle={Proceedings of the Eighth Workshop on Noisy User-generated Text (W-NUT 2022)},
  pages={215--221},
  year={2022}
}


@misc{xi2023rise,
      title={The Rise and Potential of Large Language Model Based Agents: A Survey}, 
      author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
      year={2023},
      eprint={2309.07864},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{farber2021datahunter,
  title={Datahunter: A system for finding datasets based on scientific problem descriptions},
  author={F{\"a}rber, Michael and Leisinger, Ann-Kathrin},
  booktitle={Proceedings of the 15th ACM Conference on Recommender Systems},
  pages={749--752},
  year={2021}
}

@article{paullada2021data,
  title={Data and its (dis) contents: A survey of dataset development and use in machine learning research},
  author={Paullada, Amandalynne and Raji, Inioluwa Deborah and Bender, Emily M and Denton, Emily and Hanna, Alex},
  journal={Patterns},
  volume={2},
  number={11},
  year={2021},
  publisher={Elsevier}
}

@online{cen2023aisupply,
    author = "Sarah H. Cen and Aspen Hopkins and Andrew Ilyas and Aleksander Madry and Isabella Struckman and Luis Videgaray",
    title = "AI supply chains (and why they matter)",
    year = "2023",
    month = "April",
    note = "The second post in our series On AI Deployment",
    url = "https://aipolicy.substack.com/p/supply-chains-2"
}

@misc{tremblay2023openai,
    title = {Paul Tremblay, Mona Awad vs. OpenAI, Inc., et al.},
    author = {Saveri, Joseph R. and Zirpoli, Cadio and Young, Christopher K.L. and McMahon, Kathleen J.},
    year = {2023},
    note = {Case 3:23-cv-03223-AMO Document 1 Filed 06/28/23, UNITED STATES DISTRICT COURT, NORTHERN DISTRICT OF CALIFORNIA, SAN FRANCISCO DIVISION},
    url = {https://storage.courtlistener.com/recap/gov.uscourts.cand.414822/gov.uscourts.cand.414822.1.0\_1.pdf}
}

@article{bender-friedman-2018-data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1041",
    doi = "10.1162/tacl\_a\_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}

@inproceedings{crisan2022interactive,
  title={Interactive model cards: A human-centered approach to model documentation},
  author={Crisan, Anamaria and Drouhard, Margaret and Vig, Jesse and Rajani, Nazneen},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={427--439},
  year={2022}
}

@misc{horton2023large,
      title={Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?}, 
      author={John J. Horton},
      year={2023},
      eprint={2301.07543},
      archivePrefix={arXiv},
      primaryClass={econ.GN}
}

@online{pengImpactAIDeveloper2023,
  title = {The {{Impact}} of {{AI}} on {{Developer Productivity}}: {{Evidence}} from {{GitHub Copilot}}},
  shorttitle = {The {{Impact}} of {{AI}} on {{Developer Productivity}}},
  author = {Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  year = {2023},
  eprint = {2302.06590},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.06590},
  pubstate = {preprint}
}

@misc{AI2ImpACTLicenses,
  author = {{Allen Institute}},
  title = {AI2 ImpACT Licenses},
  year = {2023},
  url = {https://allenai.org/impact-license#licenses},
}


@techreport{brynjolfsson2018productivity,
  title        = {The Productivity J-Curve: How Intangibles Complement General Purpose Technologies},
  author       = {Erik Brynjolfsson and Daniel Rock and Chad Syverson},
  year         = {2018},
  month        = {October},
  revision     = {January 2020},
  type         = {NBER Working Paper},
  number       = {25148},
  institution  = {National Bureau of Economic Research},
  address      = {1050 Massachusetts Avenue, Cambridge, MA 02138},
  url          = {https://www.nber.org/system/files/working\_papers/w25148/w25148.pdf}
}


@article{longpre2023data,
  title={The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing \& Attribution in AI},
  author={Longpre, Shayne and Mahari, Robert and Chen, Anthony and Obeng-Marnu, Naana and Sileo, Damien and Brannon, William and Muennighoff, Niklas and Khazam, Nathan and Kabbara, Jad and Perisetla, Kartik and others},
  journal={arXiv preprint arXiv:2310.16787},
  year={2023}
}

@article{ippolitodonottrain,
  title={DONOTTRAIN: A Metadata Standard for Indicating Consent for Machine Learning},
  author={Ippolito, Daphne and Yu, Yun William},
  year=2023
}

@article{zhang2023watermarks,
  title={Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models},
  author={Zhang, Hanlin and Edelman, Benjamin L and Francati, Danilo and Venturi, Daniele and Ateniese, Giuseppe and Barak, Boaz},
  journal={arXiv preprint arXiv:2311.04378},
  year={2023}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{henderson2022pile,
  title={Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset},
  author={Henderson, Peter and Krass, Mark and Zheng, Lucia and Guha, Neel and Manning, Christopher D and Jurafsky, Dan and Ho, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29217--29234},
  year={2022}
}

@article{singhal2023towards,
  title={Towards expert-level medical question answering with large language models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and others},
  journal={arXiv preprint arXiv:2305.09617},
  year={2023}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{shi2023detecting,
  title={Detecting Pretraining Data from Large Language Models},
  author={Shi, Weijia and Ajith, Anirudh and Xia, Mengzhou and Huang, Yangsibo and Liu, Daogao and Blevins, Terra and Chen, Danqi and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2310.16789},
  year={2023}
}

@article{malik2023openai,
  title={OpenAI’s ChatGPT now has 100 million weekly active users},
  author={Malik, Aisha},
  journal={TechCrunch},
  url={https://techcrunch.com/2023/11/06/openais-chatgpt-now-has-100-million-weekly-active-users/},
  year={2023},
  month={Nov},
  day={6}
}

@misc{copyright_alliance_2023,
  title={Artificial Intelligence and Copyright: Comments of the Copyright Alliance},
  author={{Copyright Alliance}},
  howpublished={Before the U.S. Copyright Office},
  year={2023},
  month={Oct},
  url={https://copyrightalliance.org/wp-content/uploads/2023/10/Copyright-Alliance-Response-to-USCO-NOI-FINAL.pdf},
  note={Docket No. 2023-6}
}

@misc{ftc_2023_creative_economy,
  title={Creative Economy and Generative AI},
  author={Federal Trade Commission},
  year={2023},
  month={Oct},
  day={4},
  howpublished={Event held on Wednesday, October 4, 2023, 3:00 PM - 4:40 PM},
  url={https://www.ftc.gov/news-events/events/2023/10/creative-economy-generative-ai}
}

@article{hao2023cleaning,
  title={Cleaning Up ChatGPT Takes Heavy Toll on Human Workers},
  author={Hao, Karen and Seetharaman, Deepa},
  journal={The Wall Street Journal},
  url={https://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483},
  year={2023},
  month={Jul},
  day={24}
}


@article{vipra2023market,
  title={Market concentration implications of foundation models: The Invisible Hand of ChatGPT},
  author={Vipra, Jai and Korinek, Anton},
  journal={Brookings},
  url={https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt/},
  year={2023},
  month={Sep},
  day={7}
}

@article{maiberg2023404,
  title={404 Media Generative AI Market Analysis: People Love to Cum},
  author={Maiberg, Emanuel},
  journal={404 Media},
  url={https://www.404media.co/404-media-generative-ai-sector-analysis-people-love-to-cum/},
  year={2023},
  month={Sep},
  day={19}
}

@article{brynjolfsson2023generative,
  title={Generative AI at Work},
  author={Brynjolfsson, Erik and Li, Danielle and Raymond, Lindsey R.},
  journal={National Bureau of Economic Research},
  year={2023},
  month={Apr},
  note={Working Paper No. 31161},
  doi={10.3386/w31161},
  url={https://www.nber.org/papers/w31161},
  revision={Nov 2023}
}

@misc{together_ai_2023_redpajama,
  title={RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models},
  author={{Together AI}},
  year={2023},
  month={Oct},
  day={30},
  howpublished={Blog post on Together AI},
  url={https://www.together.ai/blog/redpajama-data-v2}
}



@misc{khan2021new,
    title = {A New AI Lexicon: Open},
    author = {Mehtab Khan},
    year = {2021},
    month = {07},
    day = {12},
    howpublished = {\url{https://ainowinstitute.org/publication/a-new-ai-lexicon-open}},
    note = {Contextualizing 'Open Data' and AI}
}

@misc{narayanan2023generative,
    title = {Generative AI Companies Must Publish Transparency Reports},
    author = {Arvind Narayanan and Sayash Kapoor},
    year = {2023},
    month = {06},
    day = {26},
    howpublished = {\url{https://knightcolumbia.org/blog/generative-ai-companies-must-publish-transparency-reports}},
    note = {The debate about AI harms is happening in a data vacuum}
}

@techreport{bommasani2023ai,
    title = {AI Accountability Policy Request for Comment},
    author = {Rishi Bommasani and Sayash Kapoor and Daniel Zhang and Arvind Narayanan and Percy Liang},
    institution = {Stanford University; Princeton University},
    year = {2023},
    month = {06},
    day = {12},
    type = {Department of Commerce, National Telecommunications and Information Administration Docket},
    number = {No. 230407-0093, RIN 0660-XC057},
    howpublished = {\url{https://hai.stanford.edu/sites/default/files/2023-06/Reponse-to-NTIAs-.pdf}},
    note = {Response to NTIA's AI Accountability Policy Request for Comment}
}

@techreport{cmareport2023ai,
    title = {AI Foundation Models: Initial Report},
    author = {{Competition and Markets Autority}},
    year = {2023},
    month = {09},
    day = {18},
    institution = {Competition and Markets Autority},
    type = {Government Report},
    howpublished = {\url{https://www.gov.uk/government/publications/ai-foundation-models-initial-report}},
    note = {A report following the CMA's review into AI Foundation Models, and their impact on competition and consumer protection}
}

@article{lohr2023big,
    title = {Big Companies Find a Way to Identify A.I. Data They Can Trust},
    author = {Steve Lohr},
    journal = {The New York Times},
    year = {2023},
    month = {11},
    day = {30},
    url = {https://www.nytimes.com/2023/11/30/business/ai-data-standards.html}
}

@misc{datanutrition2021,
    title = {The Data Nutrition Project},
    author = {{Data Nutrition Team}},
    year = {2021},
    howpublished = {\url{https://datanutrition.org/}},
    note = {An initiative aimed at providing tools and resources for evaluating the quality of datasets used in AI}
}

@article{werder2022acm,
    title = {Establishing Data Provenance for Responsible Artificial
Intelligence Systems},
    author = {Karl Werder and Balasubramaniam Ramesh and Rongen (Sophia) Zhang},
    journal = {ACM Transactions on Management Information Systems},
    volume = {13},
    number = {2},
    pages = {Article 22},
    year = {2022},
    month = {03},
    doi = {10.1145/3503488},
    url = {https://dl.acm.org/doi/pdf/10.1145/3503488},
    institution = {Cologne Institute for Information Systems, University of Cologne; Computer Information Systems, Georgia State University}
}

@article{kale2023provenance,
    title = {Provenance Documentation to Enable Explainable and Trustworthy AI: A Literature Review},
    author = {Amruta Kale and Tin Nguyen and Frederick C. Harris Jr. and Chenhao Li and Jiyin Zhang and Xiaogang Ma},
    journal = {Data Intelligence},
    volume = {5},
    number = {1},
    pages = {139-162},
    year = {2023},
    doi = {10.1162/dint\_a\_00119},
    url = {The URL of the article, if available},
    keywords = {Explainable AI; Trustworthy AI; Provenance Documentation; Workflow Platforms; Data Science},
    note = {Received October 9, 2021; Revised January 20, 2022; Accepted February 4, 2022}
}


@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{nasr2023scalable,
  title={Scalable Extraction of Training Data from (Production) Language Models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}

@article{mahari2023transparency,
  title={Transparency by Design for Large Language Models},
  author={Mahari, Robert and South, Tobin and Pentland, Alex},
  journal={Computational Legal Futures, Network Law Review.(2023)},
  year={2023}
}

@article{villaronga2018humans,
  title={Humans forget, machines remember: Artificial intelligence and the right to be forgotten},
  author={Villaronga, Eduard Fosch and Kieseberg, Peter and Li, Tiffany},
  journal={Computer Law \& Security Review},
  volume={34},
  number={2},
  pages={304--313},
  year={2018},
  publisher={Elsevier}
}

@misc{C2PA,
  title = {Coalition for Content Provenance and Authenticity},
  url = {https://c2pa.org/},
author = {C2PA},
  year = {2023}
}

@article{shan2023prompt,
  title={Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models},
  author={Shan, Shawn and Ding, Wenxin and Passananti, Josephine and Zheng, Haitao and Zhao, Ben Y},
  journal={arXiv preprint arXiv:2310.13828},
  year={2023}
}

@misc{wga_negotiations_2023,
  title = {WGA Negotiations—Status as of May 1, 2023},
  author = {{Writers Guild of America}},
  year = {2023},
  month = {May},
  day = {1},
  url = {https://www.wga.org/uploadedfiles/members/member\_info/contract-2023/WGA\_proposals.pdf},
}

@article{chayka_aistealing_art_2023,
	title = {Is {A}.{I}. {Art} {Stealing} from {Artists}?},
	issn = {0028-792X},
	url = {https://www.newyorker.com/culture/infinite-scroll/is-ai-art-stealing-from-artists},
	abstract = {According to the lawyer behind a new class-action suit, every image that a generative tool produces “is an infringing, derivative work.”},
	language = {en-US},
	urldate = {2023-12-05},
	journal = {The New Yorker},
	author = {Chayka, Kyle},
	month = feb,
	year = {2023},
	note = {Section: infinite scroll},
	keywords = {artificial intelligence (a.i.), artists, internet, lawsuits, technology},
}

@misc{CRS_Gen_AI_Copyright,
  author = {{Congressional Research Service}},
  title = {Generative Artificial Intelligence and Copyright Law},
  address = {Washington, {DC}, {USA}},
  howpublished = {{CRS} Report No. LSB10922},
  year = {2023},
  url = {https://crsreports.congress.gov/product/pdf/LSB/LSB10922}
}

@article{smee_ai_art_no_threat_2023,
	title = {{AI} is no threat to traditional artists. But it is thrilling.},
	url = {https://www.washingtonpost.com/arts-entertainment/2023/02/15/ai-in-art/},
	language = {en-US},
	journal = {The Washington Post},
	author = {Smee, Sebastian},
	month = feb,
	year = {2023},
}

@article{complete_music_update_ai_act_2023,
	title = {Creator groups call for {EU} {AI} {A}ct to retain strong transparency obligations},
	url = {https://completemusicupdate.com/creator-groups-ai-act/},
	language = {en-US},
	journal = {Complete Music Update},
	author = {Cooke, Chris},
	month = nov,
	year = {2023},
}

@article{the_verge_news_outlets_demand_2023,
	title = {News outlets demand new rules for {AI} training data},
	url = {https://www.theverge.com/2023/8/10/23827316/news-transparency-copyright-generative-ai},
	language = {en-US},
	journal = {The Verge},
	author = {David, Emilia},
	month = aug,
	year = {2023},
}

@misc{theVergeCopilotLawsuit2023,
	author = {James Vincent},
	title = {The lawsuit that could rewrite the rules of {AI} copyright},
	howpublished = {\url{https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data}},
    url={https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data},
	year = {2022},
}

@misc{nytimesSarahSilverman,
	author = {Small, Zachary},
	title = {{S}arah {S}ilverman sues {O}pen{A}{I} and {M}eta Over Copyright Infringement},
	howpublished = {\url{https://www.nytimes.com/2023/07/10/arts/sarah-silverman-lawsuit-openai-meta.html}},
	year = {2023},
	url = {https://www.nytimes.com/2023/07/10/arts/sarah-silverman-lawsuit-openai-meta.html},
}

@misc{reutersAILawsuits2023,
	author = {Brittain, Blake},
	title = {Lawsuits accuse {AI} content creators of misusing copyrighted work},
	howpublished = {\url{https://www.reuters.com/legal/transactional/lawsuits-accuse-ai-content-creators-misusing-copyrighted-work-2023-01-17/}},
	year = {2023},
	url = {https://www.reuters.com/legal/transactional/lawsuits-accuse-ai-content-creators-misusing-copyrighted-work-2023-01-17/},
}

@misc{thevergeGettyLawsuit2023,
	author = {James Vincent},
	title = {{G}etty {I}mages is suing the creators of {AI} art tool {S}table {D}iffusion for scraping its content},
	howpublished = {\url{https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit}},
	year = {2023},
	url = {https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit},
}

@inproceedings{Steed2021,
  series = {FAccT ’21},
  title = {Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases},
  url = {http://dx.doi.org/10.1145/3442188.3445932},
  DOI = {10.1145/3442188.3445932},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness,  Accountability,  and Transparency},
  publisher = {ACM},
  author = {Steed,  Ryan and Caliskan,  Aylin},
  year = {2021},
  month = mar,
  collection = {FAccT ’21}
}


@InProceedings{buolamwiniGenderShades2018,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
}

@misc{whitehouseExecutiveOrder,
	author = {The White House},
	title = {{E}xecutive {O}rder on the {S}afe, {S}ecure, and {T}rustworthy {D}evelopment and {U}se of {A}rtificial {I}ntelligence | {T}he {W}hite {H}ouse --- whitehouse.gov},
	howpublished = {\url{https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/}},
	year = {2023},
	note = {[Accessed 18-12-2023]},
}

@techreport{AutorNewFrontiers2022,
    author      = {Autor,  David and Chin,  Caroline and Salomons,  Anna and Seegmiller,  Bryan},
    title       = {New Frontiers: The Origins and Content of New Work,  1940–2018},
    institution = {National Bureau of Economic Research},
    month       = aug ,
    year        = {2022},
    type        = {{NBER} {W}orking {P}aper},
    number      = {30389},
    url         = {http://dx.doi.org/10.3386/w30389},
    doi         = {10.3386/w30389},
},

@article{GruetzemacherDisplacement2020,
  title = {Forecasting extreme labor displacement: A survey of AI practitioners},
  volume = {161},
  ISSN = {0040-1625},
  url = {http://dx.doi.org/10.1016/j.techfore.2020.120323},
  DOI = {10.1016/j.techfore.2020.120323},
  journal = {Technological Forecasting and Social Change},
  publisher = {Elsevier BV},
  author = {Gruetzemacher,  Ross and Paradice,  David and Lee,  Kang Bok},
  year = {2020},
  month = dec,
  pages = {120323}
}

@inproceedings{RajiSavingFace2020,
  series = {AIES ’20},
  title = {Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing},
  url = {http://dx.doi.org/10.1145/3375627.3375820},
  DOI = {10.1145/3375627.3375820},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI,  Ethics,  and Society},
  publisher = {ACM},
  author = {Raji,  Inioluwa Deborah and Gebru,  Timnit and Mitchell,  Margaret and Buolamwini,  Joy and Lee,  Joonseok and Denton,  Emily},
  year = {2020},
  month = feb,
  collection = {AIES ’20}
}

@misc{KuritaMeasuringBias2019,
	author = {Kurita, Keita and Nidhi Vyas and Ayush Pareek and Alan W Black and Yulia Tsvetkov},
	title = {Measuring Bias in Contextualized Word Representations},
	howpublished = {ArXiv},
	year = {2019},
    url = {https://arxiv.org/abs/1906.07337},
}

@inproceedings{ZhangMitigating2018,
  series = {AIES ’18},
  title = {Mitigating Unwanted Biases with Adversarial Learning},
  url = {http://dx.doi.org/10.1145/3278721.3278779},
  DOI = {10.1145/3278721.3278779},
  booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI,  Ethics,  and Society},
  publisher = {ACM},
  author = {Zhang,  Brian Hu and Lemoine,  Blake and Mitchell,  Margaret},
  year = {2018},
  month = dec,
  collection = {AIES ’18}
}

@Misc{RanaCommonCrawl2010,
  author       = "Ahad Rana",
  year         = "2010",
  title        = "Common Crawl – Building an open web-scale crawl using Hadoop",
  URL          = "https://www.slideshare.net/hadoopusergroup/common-crawlpresentation",
  cc-author-affiliation = "Common Crawl",
  cc-class     = "web-crawling, big data, Hadoop",
}

@article{LupiDataHumanism2017,
  title = {We've reached peak infographics. Are you ready for what comes next?},
  volume = {161},
  url = {https://www.printmag.com/article/data-humanism-future-of-data-visualization/},
  journal = {Print Mag},
  author = {Lupi, Giorgia},
  year = {2017},
  month = jan
}

@book{DIgnazioDataFeminism2023,
  title={Data Feminism},
  author={D'Ignazio, Catherine and Lauren Klein},
  isbn={9780262547185},
  url={https://mitpress.mit.edu/9780262547185/data-feminism/},
  year={2023},
  publisher={MIT Press}
}

@misc{canadaVoluntaryCode2023,
	author = {{Government of Canada}},
	title = {{V}oluntary {C}ode of {C}onduct on the {R}esponsible {D}evelopment and {M}anagement of {A}dvanced {G}enerative {A}{I} {S}ystems},
	howpublished = {\url{https://ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-advanced-generative-ai-systems}},
	year = {2023},
}

@misc{UNGlobalDigitalCompact2023,
    author = {{United Nations}},
    title = {A Global Digital Compact --- an Open, Free and Secure Digital Future for All},
    year = {2023},
    month = may,
    howpublished = {Publisher or Organization},
    note = {Our Common Agenda Policy Brief \#5},
    url = {https://indonesia.un.org/sites/default/files/2023-07/our-common-agenda-policy-brief-gobal-digi-compact-en.pdf},
}

@inproceedings{RosentholC2PA2022,
  title = {C2PA: the world’s first industry standard for content provenance (Conference Presentation)},
  url = {http://dx.doi.org/10.1117/12.2632021},
  DOI = {10.1117/12.2632021},
  booktitle = {Applications of Digital Image Processing XLV},
  publisher = {SPIE},
  author = {Rosenthol,  Leonard},
  editor = {Tescher,  Andrew G. and Ebrahimi,  Touradj},
  year = {2022},
  month = oct 
}

@article{qzShouldCreators2023,
	title = {{H}ow should creators be compensated for their work training {AI} models?},
    author = {Cheng, Michelle},
    url = {https://qz.com/how-should-creators-be-compensated-for-their-work-train-1850932454},
    journal = {Quartz},
	year = {2023},
    month = oct,
}

@misc{C2PA_2023,
  title = {Coalition for Content Provenance and Authenticity (C2PA)},
  url = {https://c2pa.org/},
  year = {2023},
  note = {Accessed: 2023-12-23}
}

@inproceedings{browngpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper\_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{rawte2023survey,
  title={A survey of hallucination in large foundation models},
  author={Rawte, Vipula and Sheth, Amit and Das, Amitava},
  journal={arXiv preprint arXiv:2309.05922},
  year={2023}
}

@misc{IWF2023AIAbuse,
  title        = {How AI is being abused to create child sexual abuse imagery},
  author       = {{Internet Watch Foundation}},
  year         = 2023,
  month        = {October},
  howpublished = {\url{https://www.iwf.org.uk/about-us/why-we-exist/our-research/how-ai-is-being-abused-to-create-child-sexual-abuse-imagery/}}
}


@article{David2023AIDatasetCSAM,
  title        = {AI image training dataset found to include child sexual abuse imagery},
  author       = {David, Emilia},
  year         = 2023,
  month        = {December},
  day          = {20},
  journal      = {The Verge},
  url          = {https://www.theverge.com/2023/12/20/24009418/generative-ai-image-laion-csam-google-stability-stanford},
  note         = {7:57 AM PST}
}

@article{Heath2023ByteDanceOpenAI,
  title        = {ByteDance is secretly using OpenAI's tech to build a competitor},
  author       = {Heath, Alex},
  year         = 2023,
  month        = {December},
  day          = {15},
  journal      = {The Verge},
  url          = {https://www.theverge.com/2023/12/15/24003151/bytedance-china-openai-microsoft-competitor-llm},
  note         = {12:21 PM PST}
}

@article{kapoor2022leakage,
  title={Leakage and the reproducibility crisis in ML-based science},
  author={Kapoor, Sayash and Narayanan, Arvind},
  journal={arXiv preprint arXiv:2207.07048},
  year={2022}
}

@article{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}

@ARTICLE{736686,
  author={Chiou-Ting Hsu and Ja-Ling Wu},
  journal={IEEE Transactions on Image Processing}, 
  title={Hidden digital watermarks in images}, 
  year={1999},
  volume={8},
  number={1},
  pages={58-68},
  doi={10.1109/83.736686}}


@article{sadasivan2023can,
  title={Can ai-generated text be reliably detected?},
  author={Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
  journal={arXiv preprint arXiv:2303.11156},
  year={2023}
}

@misc{DeviantArt2023AIDatasetsOptOut,
  title        = {UPDATE All Deviations Are Opted Out of AI Datasets},
  author       = {{DeviantArt Team}},
  year         = 2023,
  month        = {November},
  day          = {11},
  howpublished = {\url{https://www.deviantart.com/team/journal/Tell-AI-Datasets-If-They-Can-t-Use-Your-Content-934500371}},
  note         = {3 min read}
}

@inproceedings{desai2023archival,
  title={An Archival Perspective on Pretraining Data},
  author={Desai, Meera and Jacobs, Abigail and Card, Dallas},
  booktitle={Socially Responsible Language Modelling Research},
  year={2023}
}

@misc{KellerWarso2023MLTrainingOptOut,
  title        = {Defining Best Practices for Opting Out of ML Training},
  author       = {Keller, Paul and Warso, Zuzanna},
  year         = 2023,
  month        = {September},
  day          = {28},
  howpublished = {\url{https://openfuture.eu/publication/defining-best-practices-for-opting-out-of-ml-training/}},
  publisher    = {Open Future}
}

@misc{PorciunculaChapelle2022Datasphere,
  title        = {Hello Datasphere — Towards a systems approach to data governance},
  author       = {Porciuncula, Lorrayne and De La Chapelle, Bertrand},
  year         = 2022,
  month        = {February},
  day          = {28},
  howpublished = {\url{https://www.thedatasphere.org/news/hello-datasphere-towards-a-systems-approach-to-data-governance/}},
  organization = {The Datasphere}
}

@article{BarrHays2023NYTimesAI,
  title        = {The New York Times got its content removed from one of the biggest AI training datasets. Here's how it did it.},
  author       = {Barr, Alistair and Hays, Kali},
  year         = 2023,
  month        = {November},
  day          = {8},
  journal      = {Business Insider},
  url          = {https://www.businessinsider.com/new-york-times-content-removed-common-crawl-ai-training-dataset-2023-11},
  note         = {2:00 AM PST}
}

@article{boyd2021datasheets,
  title={Datasheets for datasets help ML engineers notice and understand ethical issues in training data},
  author={Boyd, Karen L},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={5},
  number={CSCW2},
  pages={1--27},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pistilli2023stronger,
  title={Stronger Together: on the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML},
  author={Pistilli, Giada and Mu{\~n}oz Ferrandis, Carlos and Jernite, Yacine and Mitchell, Margaret},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={343--354},
  year={2023}
}

@article{mitchell2022measuring,
  title={Measuring data},
  author={Mitchell, Margaret and Luccioni, Alexandra Sasha and Lambert, Nathan and Gerchick, Marissa and McMillan-Major, Angelina and Ozoani, Ezinwanne and Rajani, Nazneen and Thrush, Tristan and Jernite, Yacine and Kiela, Douwe},
  journal={arXiv preprint arXiv:2212.05129},
  year={2022}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@inproceedings{kudugunta2023madlad,
  title={MADLAD-400: A Multilingual And Document-Level Large Audited Dataset},
  author={Kudugunta, Sneha and Caswell, Isaac Rayburn and Zhang, Biao and Garcia, Xavier and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2023}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@article{laurenccon2022bigscience,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}

@article{nguyen2023culturax,
  title={Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages},
  author={Nguyen, Thuat and Van Nguyen, Chien and Lai, Viet Dac and Man, Hieu and Ngo, Nghia Trung and Dernoncourt, Franck and Rossi, Ryan A and Nguyen, Thien Huu},
  journal={arXiv preprint arXiv:2309.09400},
  year={2023}
}

@inproceedings{oladipo-etal-2023-better,
    title = "Better Quality Pre-training Data and T5 Models for {A}frican Languages",
    author = "Oladipo, Akintunde  and
      Adeyemi, Mofetoluwa  and
      Ahia, Orevaoghene  and
      Owodunni, Abraham  and
      Ogundepo, Odunayo  and
      Adelani, David  and
      Lin, Jimmy",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.11",
    doi = "10.18653/v1/2023.emnlp-main.11",
    pages = "158--168",
}

@article{kocetkov2022stack,
  title={The Stack: 3 TB of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Jia, LI and Mou, Chenghao and Jernite, Yacine and Mitchell, Margaret and Ferrandis, Carlos Mu{\~n}oz and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{azerbayev2023llemma,
  title={Llemma: An Open Language Model For Mathematics},
  author={Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Dos Santos, Marco and McAleer, Stephen and Jiang, Albert and Deng, Jia and Biderman, Stella and Welleck, Sean},
  booktitle={The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23},
  year={2023}
}

@article{paster2023openwebmath,
  title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text},
  author={Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy},
  journal={arXiv preprint arXiv:2310.06786},
  year={2023}
}

@inproceedings{lo2020s2orc,
  title={S2ORC: The Semantic Scholar Open Research Corpus},
  author={Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel S},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4969--4983},
  year={2020}
}

@article{piktus2023gaia,
  title={GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration},
  author={Piktus, Aleksandra and Ogundepo, Odunayo and Akiki, Christopher and Oladipo, Akintunde and Zhang, Xinyu and Schoelkopf, Hailey and Biderman, Stella and Potthast, Martin and Lin, Jimmy},
  journal={arXiv preprint arXiv:2306.01481},
  year={2023}
}

@article{elazar2023s,
  title={What's In My Big Data?},
  author={Elazar, Yanai and Bhagia, Akshita and Magnusson, Ian and Ravichander, Abhilasha and Schwenk, Dustin and Suhr, Alane and Walsh, Pete and Groeneveld, Dirk and Soldaini, Luca and Singh, Sameer and others},
  journal={arXiv preprint arXiv:2310.20707},
  year={2023}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@inproceedings{lee-etal-2022-deduplicating,
    title = "Deduplicating Training Data Makes Language Models Better",
    author = "Lee, Katherine  and
      Ippolito, Daphne  and
      Nystrom, Andrew  and
      Zhang, Chiyuan  and
      Eck, Douglas  and
      Callison-Burch, Chris  and
      Carlini, Nicholas",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.577",
    doi = "10.18653/v1/2022.acl-long.577",
    pages = "8424--8445",
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@article{li2023making,
  title={Making AI Less" Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models},
  author={Li, Pengfei and Yang, Jianyi and Islam, Mohammad A and Ren, Shaolei},
  journal={arXiv preprint arXiv:2304.03271},
  year={2023}
}

@misc{peng2023rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{xie2023doremi,
      title={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining}, 
      author={Sang Michael Xie and Hieu Pham and Xuanyi Dong and Nan Du and Hanxiao Liu and Yifeng Lu and Percy Liang and Quoc V. Le and Tengyu Ma and Adams Wei Yu},
      year={2023},
      journal={arXiv preprint arXiv:2305.10429}
}

@article{albalak2023efficient,
      title={Efficient Online Data Mixing For Language Model Pre-Training}, 
      author={Alon Albalak and Liangming Pan and Colin Raffel and William Yang Wang},
      year={2023},
      journal={arXiv preprint arXiv:2312.02406}
}

@misc{gadre2023datacomp,
      title={DataComp: In search of the next generation of multimodal datasets}, 
      author={Samir Yitzhak Gadre and Gabriel Ilharco and Alex Fang and Jonathan Hayase and Georgios Smyrnis and Thao Nguyen and Ryan Marten and Mitchell Wortsman and Dhruba Ghosh and Jieyu Zhang and Eyal Orgad and Rahim Entezari and Giannis Daras and Sarah Pratt and Vivek Ramanujan and Yonatan Bitton and Kalyani Marathe and Stephen Mussmann and Richard Vencu and Mehdi Cherti and Ranjay Krishna and Pang Wei Koh and Olga Saukh and Alexander Ratner and Shuran Song and Hannaneh Hajishirzi and Ali Farhadi and Romain Beaumont and Sewoong Oh and Alex Dimakis and Jenia Jitsev and Yair Carmon and Vaishaal Shankar and Ludwig Schmidt},
      year={2023},
      eprint={2304.14108},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{xie2023data,
      title={Data Selection for Language Models via Importance Resampling}, 
      author={Sang Michael Xie and Shibani Santurkar and Tengyu Ma and Percy Liang},
      year={2023},
      eprint={2302.03169},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{grave2018learning,
  title={Learning Word Vectors for 157 Languages},
  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@inproceedings{Burchell_2023,
   title={An Open Dataset and Model for Language Identification},
   url={http://dx.doi.org/10.18653/v1/2023.acl-short.75},
   DOI={10.18653/v1/2023.acl-short.75},
   booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
   publisher={Association for Computational Linguistics},
   author={Burchell, Laurie and Birch, Alexandra and Bogoychev, Nikolay and Heafield, Kenneth},
   year={2023} }

@misc{kargaran2023glotlid,
      title={GlotLID: Language Identification for Low-Resource Languages}, 
      author={Amir Hossein Kargaran and Ayyoob Imani and François Yvon and Hinrich Schütze},
      year={2023},
      eprint={2310.16248},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Hanu_Detoxify_2020,
    author = {Hanu, Laura and Unitary, team},
    doi = {10.5281/zenodo.7925667},
    month = nov,
    title = {{Detoxify}},
    url = {https://github.com/unitaryai/detoxify},
    version = {0.5.1},
    year = {2020}
}

@misc{ravanelli2021speechbrain,
      title={SpeechBrain: A General-Purpose Speech Toolkit}, 
      author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
      year={2021},
      eprint={2106.04624},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@article{JMLR:v21:19-467,
  author  = {Jacob Schreiber and Jeffrey Bilmes and William Stafford Noble},
  title   = {apricot: Submodular selection for data summarization in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {161},
  pages   = {1--6},
  url     = {http://jmlr.org/papers/v21/19-467.html}
}

@misc{open_lm,
  author = {Gururangan, Suchin and Wortsman, Mitchell and Gadre, Samir Yitzhak and Dave, Achal and Kilian, Maciej and Shi, Weijia and Mercat, Jean and Smyrnis, Georgios and Ilharco, Gabriel and Jordan, Matt and Heckel, Reinhard and Dimakis, Alex and Farhadi, Ali and Shankar, Vaishaal and Schmidt, Ludwig},
  title = {{open\_lm}:  a minimal but performative language modeling (LM) repository},
  year = {2023},
  note = {GitHub repository},
  url = {https://github.com/mlfoundations/open\_lm/}
}

@misc{gpt-neox-library,
  title = {{GPT-NeoX}: Large Scale Autoregressive Language Modeling in {PyTorch}},
  author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Purohit, Shivanshu and Songz, Tri and Phil, Wang and Weinbach, Samuel},
  url = {https://www.github.com/eleutherai/gpt-neox},
  doi = {10.5281/zenodo.5879544},
  month = {8},
  year = {2021},
  Version = {2.0.0},
}

@misc{smith2022using,
      title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model}, 
      author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro},
      year={2022},
      eprint={2201.11990},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}

@misc{peng2023kosmos2,
      title={Kosmos-2: Grounding Multimodal Large Language Models to the World}, 
      author={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Furu Wei},
      year={2023},
      eprint={2306.14824},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@misc{lhotse,
      title={Lhotse: a speech data representation library for the modern deep learning ecosystem}, 
      author={Piotr Żelasko and Daniel Povey and Jan "Yenda" Trmal and Sanjeev Khudanpur},
      year={2021},
      eprint={2110.12561},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{shoeybi2020megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}

@inproceedings{trlx-library,
    title = "trl{X}: A Framework for Large Scale Reinforcement Learning from Human Feedback",
    author = "Havrilla, Alexander  and
      Zhuravinskyi, Maksym  and
      Phung, Duy  and
      Tiwari, Aman  and
      Tow, Jonathan  and
      Biderman, Stella  and
      Anthony, Quentin  and
      Castricato, Louis",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.530",
    doi = "10.18653/v1/2023.emnlp-main.530",
    pages = "8578--8595",
}

@article{schmidt2021codecarbon,
  title={CodeCarbon: estimate and track carbon emissions from machine learning computing},
  author={Schmidt, Victor and Goyal, Kamal and Joshi, Aditya and Feld, Boris and Conell, Liam and Laskaris, Nikolas and Blank, Doug and Wilson, Jonathan and Friedler, Sorelle and Luccioni, Sasha},
  journal={Cited on},
  pages={20},
  year={2021}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{vidgen2023simplesafetytests,
  title={SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models},
  author={Vidgen, Bertie and Kirk, Hannah Rose and Qian, Rebecca and Scherrer, Nino and Kannappan, Anand and Hale, Scott A and R{\"o}ttger, Paul},
  journal={arXiv preprint arXiv:2311.08370},
  year={2023}
}

@article{yang2023sneakyprompt,
  title={SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters},
  author={Yang, Yuchen and Hui, Bo and Yuan, Haolin and Gong, Neil and Cao, Yinzhi},
  journal={arXiv preprint arXiv:2305.12082},
  year={2023}
}

@article{inan2023llama,
  title={Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}
@article{bhatt2023purple,
  title={Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models},
  author={Bhatt, Manish and Chennabasappa, Sahana and Nikolaidis, Cyrus and Wan, Shengye and Evtimov, Ivan and Gabi, Dominik and Song, Daniel and Ahmad, Faizan and Aschermann, Cornelius and Fontana, Lorenzo and others},
  journal={arXiv preprint arXiv:2312.04724},
  year={2023}
}

@misc{anthony2024cookbook,
    title = {{The EleutherAI Model Training Cookbook}},
    author = {Anthony, Quentin and Schoelkopf, Hailey and Biderman, Stella},
    howpublished = {GitHub Repo},
    url = {https://github.com/EleutherAI/cookbook},
    year = 2024
}

@article{koenecke2020racial,
  title={Racial disparities in automated speech recognition},
  author={Koenecke, Allison and Nam, Andrew and Lake, Emily and Nudell, Joe and Quartey, Minnie and Mengesha, Zion and Toups, Connor and Rickford, John R and Jurafsky, Dan and Goel, Sharad},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={14},
  pages={7684--7689},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{dingemanse2022text,
  title={From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology},
  author={Dingemanse, Mark and Liesenfeld, Andreas},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5614--5633},
  year={2022}
}

@article{luccioni2023stable,
  title={Stable bias: Analyzing societal representations in diffusion models},
  author={Luccioni, Alexandra Sasha and Akiki, Christopher and Mitchell, Margaret and Jernite, Yacine},
  journal={arXiv preprint arXiv:2303.11408},
  year={2023}
}

@article{thapliyal2022crossmodal,
  title={Crossmodal-3600: A massively multilingual multimodal evaluation dataset},
  author={Thapliyal, Ashish V and Pont-Tuset, Jordi and Chen, Xi and Soricut, Radu},
  journal={arXiv preprint arXiv:2205.12522},
  year={2022}
}

@misc{Hughes-Vectara-Hallucination-Leaderboard-2023,
author = {Hughes, Simon and Bae, Minseok},
month = nov,
title = {{Vectara Hallucination Leaderboard}},
url = {https://github.com/vectara/hallucination-leaderboard},
year = {2023}
}

@article{sun2023head,
  title={Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? AKA Will LLMs Replace Knowledge Graphs?},
  author={Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
  journal={arXiv preprint arXiv:2308.10168},
  year={2023}
}

@article{lee2022factuality,
  title={Factuality enhanced language models for open-ended text generation},
  author={Lee, Nayeon and Ping, Wei and Xu, Peng and Patwary, Mostofa and Fung, Pascale N and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34586--34599},
  year={2022}
}



@article{saberi2023robustness,
  title={Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks},
  author={Saberi, Mehrdad and Sadasivan, Vinu Sankar and Rezaei, Keivan and Kumar, Aounon and Chegini, Atoosa and Wang, Wenxiao and Feizi, Soheil},
  journal={arXiv preprint arXiv:2310.00076},
  year={2023}
}



@misc{li2023blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2023llamaadapter,
      title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention}, 
      author={Renrui Zhang and Jiaming Han and Chris Liu and Peng Gao and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Yu Qiao},
      year={2023},
      eprint={2303.16199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{li2023otter,
  title={Otter: A Multi-Modal Model with In-Context Instruction Tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@article{gao2023llama,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}



@misc{liu2023visual,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{dettmers2023qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{downing2023licensing,
    title = {AI Licensing Can’t Balance ``Open'' with ``Responsible''},
    author = {Kate Downing},
    howpublished = {The Law Office of Kate Downing's Blog},
    url = {https://katedowninglaw.com/2023/07/13/ai-licensing-cant-balance-open-with-responsible/},
    year={2023}
}

@article{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{lee2023holistic,
  title={Holistic evaluation of text-to-image models},
  author={Lee, Tony and Yasunaga, Michihiro and Meng, Chenlin and Mai, Yifan and Park, Joon Sung and Gupta, Agrim and Zhang, Yunzhi and Narayanan, Deepak and Teufel, Hannah Benita and Bellagente, Marco and others},
  journal={arXiv preprint arXiv:2311.04287},
  year={2023}
}

@article{jimenez2023swe,
  title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@inproceedings{sanabria2023edinburgh,
  title={The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR},
  author={Sanabria, Ramon and Bogoychev, Nikolay and Markl, Nina and Carmantini, Andrea and Klejch, Ondrej and Bell, Peter},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{liu2023mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{fu2023mme,
  title={Mme: A comprehensive evaluation benchmark for multimodal large language models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@article{dubois2023alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2305.14387},
  year={2023}
}

@article{alkemade2023datasheets,
  title={Datasheets for Digital Cultural Heritage Datasets},
  author={Alkemade, Henk and Claeyssens, Steven and Colavizza, Giovanni and Freire, Nuno and Lehmann, J{\"o}rg and Neudeker, Clemens and Osti, Giulia and van Strien, Daniel and others},
  journal={JOURNAL OF OPEN HUMANITIES DATA},
  volume={9},
  number={17},
  pages={1--11},
  year={2023}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{nagrani2017voxceleb,
  title={Voxceleb: a large-scale speaker identification dataset},
  author={Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1706.08612},
  year={2017}
}

@inproceedings{valk2021voxlingua107,
  title={VoxLingua107: a dataset for spoken language recognition},
  author={Valk, J{\"o}rgen and Alum{\"a}e, Tanel},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={652--658},
  year={2021},
  organization={IEEE}
}

@article{yue2023mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  journal={arXiv preprint arXiv:2311.16502},
  year={2023}
}

@article{soldaini2024dolma,
  title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  journal={arXiv preprint arXiv:2402.00159},
  year={2024}
}

@article{singh2024aya,
  title={Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning},
  author={Singh, Shivalika and Vargus, Freddie and Dsouza, Daniel and Karlsson, B{\"o}rje F and Mahendiran, Abinaya and Ko, Wei-Yin and Shandilya, Herumb and Patel, Jay and Mataciunas, Deividas and OMahony, Laura and others},
  journal={arXiv preprint arXiv:2402.06619},
  year={2024}
}

@article{adelani2023sib,
  title={SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects},
  author={Adelani, David Ifeoluwa and Liu, Hannah and Shen, Xiaoyu and Vassilyev, Nikita and Alabi, Jesujoba O and Mao, Yanke and Gao, Haonan and Lee, Annie En-Shiun},
  journal={arXiv preprint arXiv:2309.07445},
  year={2023}
}

@article{albalak2024survey,
  title={A Survey on Data Selection for Language Models},
  author={Albalak, Alon and Elazar, Yanai and Xie, Sang Michael and Longpre, Shayne and Lambert, Nathan and Wang, Xinyi and Muennighoff, Niklas and Hou, Bairu and Pan, Liangming and Jeong, Haewon and others},
  journal={arXiv preprint arXiv:2402.16827},
  year={2024}
}

@article{mcduff2024standardization,
  title={On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI},
  author={McDuff, Daniel and Korjakow, Tim and Cambo, Scott and Benjamin, Jesse Josua and Lee, Jenny and Jernite, Yacine and Ferrandis, Carlos Mu{\~n}oz and Gokaslan, Aaron and Tarkowski, Alek and Lindley, Joseph and others},
  journal={arXiv preprint arXiv:2402.05979},
  year={2024}
}

@misc{transformer-math-eleutherai,
  title = {Transformer Math 101},
  author = {Anthony, Quentin and Biderman, Stella and Schoelkopf, Hailey},
  howpublished = {GitHub Repo},
  url = {blog.eleuther.ai/},
  year = {2023},
}

@misc{inference-arithmetic,
  title = {Transformer Inference Arithmetic},
  author = {Chen, Carol},
  howpublished= {\url{https://kipp.ly/blog/transformer-inference-arithmetic/}},
  year = {2022}
}

@misc{nanogpt,
  author = {Karpathy, Andrej},
  title = {nanoGPT},
  year = {2023},
  howpublished = {GitHub Repo},
  url = {https://github.com/karpathy/nanoGPT},
}

@misc{ml-engineering,
  author = {Bekman, Stas},
  title = {Machine Learning Engineering Open Book},
  howpublished = {GitHub Repo},
  url = {https://github.com/stas00/ml-engineering},
}

@misc{axolotl,
  author = {OpenAccess-AI-Collective},
  title = {Axolotl},
  howpublished= {GitHub Repo},
  url = {https://github.com/OpenAccess-AI-Collective/axolotl}

}

@misc{stable-audio-tools,
    author = {Stability AI},
    title = {Stable Audio Tools},
    howpublished = {Github Repo},
    url = {https://github.com/Stability-AI/stable-audio-tools}
}

@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}


@misc{nanotron,
    title = {Nanotron},
    year = {2024},
    howpublished = {\url{https://github.com/huggingface/nanotron}},
    author = {}
}

@misc{goddard2024arcees,
      title={Arcee's MergeKit: A Toolkit for Merging Large Language Models}, 
      author={Charles Goddard and Shamane Siriwardhana and Malikeh Ehghaghi and Luke Meyers and Vlad Karpukhin and Brian Benedict and Mark McQuade and Jacob Solawetz},
      year={2024},
      eprint={2403.13257},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{matena2022merging,
      title={Merging Models with Fisher-Weighted Averaging}, 
      author={Michael Matena and Colin Raffel},
      year={2022},
      eprint={2111.09832},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{donyehiya2023cold,
      title={ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning}, 
      author={Shachar Don-Yehiya and Elad Venezian and Colin Raffel and Noam Slonim and Yoav Katz and Leshem Choshen},
      year={2023},
      eprint={2212.01378},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{stoica2024zipit,
      title={ZipIt! Merging Models from Different Tasks without Training}, 
      author={George Stoica and Daniel Bolya and Jakob Bjorner and Pratik Ramesh and Taylor Hearn and Judy Hoffman},
      year={2024},
      eprint={2305.03053},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yadav2023tiesmerging,
      title={TIES-Merging: Resolving Interference When Merging Models}, 
      author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
      year={2023},
      eprint={2306.01708},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{niklaus2023multilegalpile,
  title={Multilegalpile: A 689gb multilingual legal corpus},
  author={Niklaus, Joel and Matoshi, Veton and St{\"u}rmer, Matthias and Chalkidis, Ilias and Ho, Daniel E},
  journal={arXiv preprint arXiv:2306.02069},
  year={2023}
}

@article{cahyawijayanusacrowd,
  title={NusaCrowd: Open Source Initiative for Indonesian NLP Resources},
  author={Cahyawijaya, Samuel and Lovenia, Holy and Aji, Alham Fikri and Winata, Genta Indra and Wilie, Bryan and Koto, Fajri and Mahendra, Rahmad and Wibisono, Christian and Romadhony, Ade and Vincentio, Karissa and others}
}

@inproceedings{alyafeai2022masader,
  title={Masader: Metadata Sourcing for Arabic Text and Speech Data Resources},
  author={Alyafeai, Zaid and Masoud, Maraim and Ghaleb, Mustafa and Al-shaibani, Maged S},
  booktitle={Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  pages={6340--6351},
  year={2022}
}

@article{kapoor2024societal,
  title={On the Societal Impact of Open Foundation Models},
  author={Kapoor, Sayash and Bommasani, Rishi and Klyman, Kevin and Longpre, Shayne and Ramaswami, Ashwin and Cihon, Peter and Hopkins, Aspen and Bankston, Kevin and Biderman, Stella and Bogen, Miranda and others},
  journal={arXiv preprint arXiv:2403.07918},
  year={2024}
}

@techreport{lakatos-revealing-2023,
	title = {A {Revealing} {Picture}: {AI}-{Generated} ‘{Undressing}’ {Images} {Move} from {Niche} {Pornography} {Discussion} {Forums} to a {Scaled} and {Monetized} {Online} {Business}},
	url = {https://public-assets.graphika.com/reports/graphika-report-a-revealing-picture.pdf},
	urldate = {2024-01-16},
	author = {Lakatos, Santiago},
	month = dec,
	year = {2023},
}

@article{thiel-generative-2023,
	title = {Generative {ML} and {CSAM}: {Implications} and {Mitigations}},
	shorttitle = {Generative {ML} and {CSAM}},
	url = {https://purl.stanford.edu/jv206yg3793},
	doi = {10.25740/jv206yg3793},
	abstract = {A joint report between the Stanford Internet Observatory and Thorn examines implications of fully realistic child sexual abuse material (CSAM) produced by generative machine learning models. Advanc...},
	language = {en},
	urldate = {2024-01-09},
	author = {Thiel, David and Stroebel, Melissa and Portnoff, Rebecca},
	year = {2023},
}

@inproceedings{zhao2023inthe,
  title={(InThe) WildChat: 570K ChatGPT Interaction Logs In The Wild},
  author={Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{gadre2024language,
  title={Language models scale reliably with over-training and on downstream tasks},
  author={Gadre, Samir Yitzhak and Smyrnis, Georgios and Shankar, Vaishaal and Gururangan, Suchin and Wortsman, Mitchell and Shao, Rulin and Mercat, Jean and Fang, Alex and Li, Jeffrey and Keh, Sedrick and others},
  journal={arXiv preprint arXiv:2403.08540},
  year={2024}
}

@inproceedings{aghajanyan2023scaling,
  title={Scaling laws for generative mixed-modal language models},
  author={Aghajanyan, Armen and Yu, Lili and Conneau, Alexis and Hsu, Wei-Ning and Hambardzumyan, Karen and Zhang, Susan and Roller, Stephen and Goyal, Naman and Levy, Omer and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={265--279},
  year={2023},
  organization={PMLR}
}

@article{klopffer1997life,
  title={Life cycle assessment: From the beginning to the current state},
  author={Kl{\"o}pffer, Walter},
  journal={Environmental Science and Pollution Research},
  volume={4},
  pages={223--228},
  year={1997},
  publisher={Springer}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{brown2002status,
  title={Status and future directions of the Energy Star program},
  author={Brown, R and Webber, C and Koomey, JG},
  journal={Energy},
  volume={5},
  number={27},
  pages={505--520},
  year={2002}
}

@article{sora2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
  year={2024}
}

@article{blattmann2023stable,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}

@article{claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  url={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf},
  year={2024}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@article{korinek2023market,
  title={Market concentration implications of foundation models: The Invisible Hand of ChatGPT},
  author={Korinek, Anton and Vipra, Jai},
  year={2023},
  publisher={Brookings Institution}
}

@article{Liu2024InfiniGram,
  title={Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens},
  author={Liu, Jiacheng and Min, Sewon and Zettlemoyer, Luke and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2401.17377},
  year={2024}
}

@misc{beaumont-2022-clip-retrieval,
  author = {Romain Beaumont},
  title = {Clip Retrieval: Easily compute clip embeddings and build a clip retrieval system with them},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rom1504/clip-retrieval}}
}

@misc{schuhmann2022laion5b,
      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, 
      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
      year={2022},
      eprint={2210.08402},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{wenzek-etal-2020-ccnet,
    title = "{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    author = "Wenzek, Guillaume  and
      Lachaux, Marie-Anne  and
      Conneau, Alexis  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Joulin, Armand  and
      Grave, Edouard",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.494",
    pages = "4003--4012",
    abstract = "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@misc{penedo2024datatrove,
  author = {Penedo, Guilherme and Cappelli, Alessandro and Wolf, Thomas and Sasko, Mario},
  title = {DataTrove: large scale data processing},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/huggingface/datatrove}
}

@inproceedings{HuggingFaceCommunityBlog,
  author    = {Yacine Jernite},
  title     = {Training Data Transparency in AI: Tools, Trends, and Policy Recommendations},
  booktitle = {Hugging Face Blog},
  year      = {2023}
}


@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@inproceedings{yu2023skill,
  title={Skill-Mix: a Flexible and Expandable Family of Evaluations for AI Models},
  author={Yu, Dingli and Kaur, Simran and Gupta, Arushi and Brown-Cohen, Jonah and Goyal, Anirudh and Arora, Sanjeev},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{lee2023evaluating,
  title={Evaluating Human-Language Model Interaction},
  author={Lee, Mina and Srivastava, Megha and Hardy, Amelia and Thickstun, John and Durmus, Esin and Paranjape, Ashwin and Gerard-Ursin, Ines and Li, Xiang Lisa and Ladhak, Faisal and Rong, Frieda and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{le2022learning,
  title={Learning from failure: Data capture in an australian aboriginal community},
  author={Le Ferrand, {\'E}ric and Bird, Steven and Besacier, Laurent},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={4988--4998},
  year={2022}
}

@article{kiela2021dynabench,
  title={Dynabench: Rethinking benchmarking in NLP},
  author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
  journal={arXiv preprint arXiv:2104.14337},
  year={2021}
}

@misc{NarayananKapoor2024,
  author = {Arvind Narayanan and Sayash Kapoor},
  title = {AI Safety is Not a Model Property},
  year = {2024},
  howpublished = {\url{https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property}},
  note = {Accessed: YYYY-MM-DD},
  month = mar,
  day = {12}
}

@inproceedings{dobbe2022system,
  title={System safety and artificial intelligence},
  author={Dobbe, Roel},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1584--1584},
  year={2022}
}

@article{raji2023concrete,
  title={Concrete problems in AI safety, revisited},
  author={Raji, Inioluwa Deborah and Dobbe, Roel},
  journal={arXiv preprint arXiv:2401.10899},
  year={2023}
}

@misc{Anthropic2023,
  author = {Anthropic},
  title = {Challenges in Evaluating AI Systems},
  year = {2023},
  url = {https://www.anthropic.com/news/evaluating-ai-systems},
  month = oct,
  day = {4}
}

@article{guha2024legalbench,
  title={Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models},
  author={Guha, Neel and Nyarko, Julian and Ho, Daniel and R{\'e}, Christopher and Chilton, Adam and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020}
}

@inproceedings{parrish2022bbq,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={2086--2105},
  year={2022}
}

@inproceedings{dhamala2021bold,
  title={Bold: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={862--872},
  year={2021}
}

@inproceedings{smith2022m,
  title={“I’m sorry to hear that”: Finding New Biases in Language Models with a Holistic Descriptor Dataset},
  author={Smith, Eric Michael and Hall, Melissa and Kambadur, Melanie and Presani, Eleonora and Williams, Adina},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={9180--9211},
  year={2022}
}

@article{schaeffer2024emergent,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{thiel2023generative,
  title={Generative ML and CSAM: Implications and Mitigations},
  author={Thiel, David and Stroebel, Melissa and Portnoff, Rebecca},
  year={2023},
  publisher={Stanford Digital Repository. https://doi. org/10.25740/jv206yg3793}
}

@techreport{lakatos2023revealing,
  title={A Revealing Picture: AI-Generated ‘Undressing’Images Move from Niche Pornography Discussion Forums to a Scaled and Monetized Online Business},
  author={Lakatos, S},
  year={2023},
  institution={Technical report, Graphika, Dec 2023. URL https://public-assets. graphika~…}
}

@inproceedings{yong2023low,
  title={Low-Resource Languages Jailbreak GPT-4},
  author={Yong, Zheng Xin and Menghini, Cristina and Bach, Stephen},
  booktitle={Socially Responsible Language Modelling Research},
  year={2023}
}

@article{lozhkov2024starcoder,
  title={StarCoder 2 and The Stack v2: The Next Generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@misc{hosking2024human,
      title={Human Feedback is not Gold Standard}, 
      author={Tom Hosking and Phil Blunsom and Max Bartolo},
      year={2024},
      eprint={2309.16349},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xu-etal-2023-critical,
    title = "A Critical Evaluation of Evaluations for Long-form Question Answering",
    author = "Xu, Fangyuan  and
      Song, Yixiao  and
      Iyyer, Mohit  and
      Choi, Eunsol",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.181",
    doi = "10.18653/v1/2023.acl-long.181",
    pages = "3225--3245",
    abstract = "Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts in seven areas to provide preference judgments over pairs of answers, along with free-form justifications for their choices. We present a careful analysis of experts{'} evaluation, which focuses on new aspects such as the comprehensiveness of the answer. Next, we examine automatic text generation metrics, finding that no existing metrics are predictive of human preference judgments. However, some metrics correlate with fine-grained aspects of answers (e.g., coherence). We encourage future work to move away from a single {``}overall score{''} of the answer and adopt a multi-faceted evaluation, targeting aspects such as factuality and completeness. We publicly release all of our annotations and code to spur future work into LFQA evaluation.",
}

@article{wu2023style,
  title={Style over substance: Evaluation biases for large language models},
  author={Wu, Minghao and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2307.03025},
  year={2023}
}

@inproceedings{mazumder2023dataperf,
    title={DataPerf: Benchmarks for Data-Centric {AI} Development},
    author={Mark Mazumder and Colby Banbury and Xiaozhe Yao and Bojan Karla{\v{s}} and William A Gaviria Rojas and Sudnya Diamos and Greg Diamos and Lynn He and Alicia Parrish and Hannah Rose Kirk and Jessica Quaye and Charvi Rastogi and Douwe Kiela and David Jurado and David Kanter and Rafael Mosquera and Will Cukierski and Juan Ciro and Lora Aroyo and Bilge Acun and Lingjiao Chen and Mehul Smriti Raje and Max Bartolo and Sabri Eyuboglu and Amirata Ghorbani and Emmett Daniel Goodman and Addison Howard and Oana Inel and Tariq Kane and Christine Kirkpatrick and D. Sculley and Tzu-Sheng Kuo and Jonas Mueller and Tristan Thrush and Joaquin Vanschoren and Margaret Warren and Adina Williams and Serena Yeung and Newsha Ardalani and Praveen Paritosh and Ce Zhang and James Y. Zou and Carole-Jean Wu and Cody Coleman and Andrew Ng and Peter Mattson and Vijay Janapa Reddi},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2023},
    url={https://openreview.net/forum?id=LaFKTgrZMG}
}

@inproceedings{albalak-etal-2022-feta,
    title = "{FETA}: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue",
    author = "Albalak, Alon  and
      Tuan, Yi-Lin  and
      Jandaghi, Pegah  and
      Pryor, Connor  and
      Yoffe, Luke  and
      Ramachandran, Deepak  and
      Getoor, Lise  and
      Pujara, Jay  and
      Wang, William Yang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.751",
    doi = "10.18653/v1/2022.emnlp-main.751",
    pages = "10936--10953",
    abstract = "Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This work explores conversational task transfer by introducing FETA: a benchmark for FEw-sample TAsk transfer in open-domain dialogue.FETA contains two underlying sets of conversations upon which there are 10 and 7 tasks annotated, enabling the study of intra-dataset task transfer; task transfer without domain adaptation. We utilize three popular language models and three learning algorithms to analyze the transferability between 132 source-target task pairs and create a baseline for future work. We run experiments in the single- and multi-source settings and report valuable findings, e.g., most performance trends are model-specific, and span extraction and multiple-choice tasks benefit the most from task transfer. In addition to task transfer, FETA can be a valuable resource for future research into the efficiency and generalizability of pre-training datasets and model architectures, as well as for learning settings such as continual and multitask learning.",
}

@inproceedings{warstadt-etal-2023-findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

@misc{anthropic2024claude3,
  title={{The Claude 3 Model Family: Opus, Sonnet, Haiku}},
  author={Anthropic},
  year={2024},
  month={3},
  note={{URL}: https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf},
  howpublished={\url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf}}
}


@misc{gomez2024commandrplus,
  title={Introducing Command R+: A Scalable LLM Built for Business},
  author={Gomez, Aidan},
  year={2024},
  month={4},
  day={4},
  note={{URL}: https://txt.cohere.com/command-r-plus-microsoft-azure/},
  howpublished={\url{https://txt.cohere.com/command-r-plus-microsoft-azure/}}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{ustun2024aya,
  title={Aya model: An instruction finetuned open-access multilingual language model},
  author={{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and others},
  journal={arXiv preprint arXiv:2402.07827},
  year={2024}
}

@article{vipra2023concentration,
  title={Market concentration implications of foundation models: The Invisible Hand of ChatGPT},
  author={Jai Vipra and Anton Korinek},
  journal={The Brookings Institution},
  year={2023},
  url={https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt}
}

@article{mcelheran2024ai,
  title={AI adoption in America: Who, what, and where},
  author={McElheran, Kristina and Li, J Frank and Brynjolfsson, Erik and Kroff, Zachary and Dinlersoz, Emin and Foster, Lucia and Zolas, Nikolas},
  journal={Journal of Economics \& Management Strategy},
  year={2024},
  publisher={Wiley Online Library}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{chang2023survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  year={2023},
  journal={ACM Transactions on Intelligent Systems and Technology},
  publisher={ACM New York, NY}
}

@inproceedings{lhoest-etal-2021-datasets,
    title = "Datasets: A Community Library for Natural Language Processing",
    author = "Lhoest, Quentin  and
      Villanova del Moral, Albert  and
      Jernite, Yacine  and
      Thakur, Abhishek  and
      von Platen, Patrick  and
      Patil, Suraj  and
      Chaumond, Julien  and
      Drame, Mariama  and
      Plu, Julien  and
      Tunstall, Lewis  and
      Davison, Joe  and
      {\v{S}}a{\v{s}}ko, Mario  and
      Chhablani, Gunjan  and
      Malik, Bhavitvya  and
      Brandeis, Simon  and
      Le Scao, Teven  and
      Sanh, Victor  and
      Xu, Canwen  and
      Patry, Nicolas  and
      McMillan-Major, Angelina  and
      Schmid, Philipp  and
      Gugger, Sylvain  and
      Delangue, Cl{\'e}ment  and
      Matussi{\`e}re, Th{\'e}o  and
      Debut, Lysandre  and
      Bekman, Stas  and
      Cistac, Pierric  and
      Goehringer, Thibault  and
      Mustar, Victor  and
      Lagunas, Fran{\c{c}}ois  and
      Rush, Alexander  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-demo.21",
    pages = "175--184",
    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",
    eprint={2109.02846},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}

@article{schwartz2020green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{longpre2024data,
  title={Data Authenticity, Consent, \& Provenance for AI are all broken: what will it take to fix them?},
  author={Longpre, Shayne and Mahari, Robert and Obeng-Marnu, Naana and Brannon, William and South, Tobin and Gero, Katy and Pentland, Sandy and Kabbara, Jad},
  journal={arXiv preprint arXiv:2404.12691},
  year={2024}
}

@article{zhu2024multimodal,
  title={Multimodal c4: An open, billion-scale corpus of images interleaved with text},
  author={Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{laurenccon2024obelics,
  title={Obelics: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gadre2024datacomp,
  title={Datacomp: In search of the next generation of multimodal datasets},
  author={Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1728--1738},
  year={2021}
}

@INPROCEEDINGS{5206848,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{li2023m,
  title={M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning},
  author={Li, Lei and Yin, Yuwei and Li, Shicheng and Chen, Liang and Wang, Peiyi and Ren, Shuhuai and Li, Mukai and Yang, Yazheng and Xu, Jingjing and Sun, Xu and others},
  journal={arXiv preprint arXiv:2306.04387},
  year={2023}
}

@article{laurenccon2024matters,
  title={What matters when building vision-language models?},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal={arXiv preprint arXiv:2405.02246},
  year={2024}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{liu2024best,
  title={Best Practices and Lessons Learned on Synthetic Data for Language Models},
  author={Liu, Ruibo and Wei, Jerry and Liu, Fangyu and Si, Chenglei and Zhang, Yanzhe and Rao, Jinmeng and Zheng, Steven and Peng, Daiyi and Yang, Diyi and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2404.07503},
  year={2024}
}

@article{zhou2023comprehensive,
  title={A comprehensive survey on pretrained foundation models: A history from bert to chatgpt},
  author={Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}

@article{zhou2024training,
  title={Training and Serving System of Foundation Models: A Comprehensive Survey},
  author={Zhou, Jiahang and Chen, Yanyu and Hong, Zicong and Chen, Wuhui and Yu, Yue and Zhang, Tao and Wang, Hui and Zhang, Chuanfu and Zheng, Zibin},
  journal={arXiv preprint arXiv:2401.02643},
  year={2024}
}

@article{woisetschlager2024survey,
  title={A Survey on Efficient Federated Learning Methods for Foundation Model Training},
  author={Woisetschl{\"a}ger, Herbert and Isenko, Alexander and Wang, Shiqiang and Mayer, Ruben and Jacobsen, Hans-Arno},
  journal={arXiv preprint arXiv:2401.04472},
  year={2024}
}

@article{mckinzie2024mm1,
  title={Mm1: Methods, analysis \& insights from multimodal llm pre-training},
  author={McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
  journal={arXiv preprint arXiv:2403.09611},
  year={2024}
}

@article{karamcheti2024prismatic,
  title={Prismatic vlms: Investigating the design space of visually-conditioned language models},
  author={Karamcheti, Siddharth and Nair, Suraj and Balakrishna, Ashwin and Liang, Percy and Kollar, Thomas and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2402.07865},
  year={2024}
}

@article{niu2024jailbreaking,
  title={Jailbreaking attack against multimodal large language model},
  author={Niu, Zhenxing and Ren, Haodong and Gao, Xinbo and Hua, Gang and Jin, Rong},
  journal={arXiv preprint arXiv:2402.02309},
  year={2024}
}

@inproceedings{shayegani2023jailbreak,
  title={Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{qi2024visual,
  title={Visual adversarial examples jailbreak aligned large language models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Henderson, Peter and Wang, Mengdi and Mittal, Prateek},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={21527--21536},
  year={2024}
}

@article{hartvigsen2022toxigen,
  title={Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  journal={arXiv preprint arXiv:2203.09509},
  year={2022}
}

@article{simmons2024ai,
  title={AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research},
  author={Simmons-Edler, Riley and Badman, Ryan and Longpre, Shayne and Rajan, Kanaka},
  journal={arXiv preprint arXiv:2405.01859},
  year={2024}
}

@article{longpre2022lethal,
  title={Lethal autonomous weapons systems \& artificial intelligence: Trends, challenges, and policies},
  author={Longpre, Shayne and Storm, Marcus and Shah, Rishi},
  journal={MIT Science Policy Review},
  volume={3},
  number={1},
  pages={47--56},
  year={2022}
}


@article{bommasani2024foundation,
  title={Foundation Model Transparency Reports},
  author={Bommasani, Rishi and Klyman, Kevin and Longpre, Shayne and Xiong, Betty and Kapoor, Sayash and Maslej, Nestor and Narayanan, Arvind and Liang, Percy},
  journal={arXiv preprint arXiv:2402.16268},
  year={2024}
}

@article{longpre2024safe,
  title={A safe harbor for ai evaluation and red teaming},
  author={Longpre, Shayne and Kapoor, Sayash and Klyman, Kevin and Ramaswami, Ashwin and Bommasani, Rishi and Blili-Hamelin, Borhane and Huang, Yangsibo and Skowron, Aviya and Yong, Zheng-Xin and Kotha, Suhas and others},
  journal={arXiv preprint arXiv:2403.04893},
  year={2024}
}

@article{xu2024instructional,
  title={Instructional fingerprinting of large language models},
  author={Xu, Jiashu and Wang, Fei and Ma, Mingyu Derek and Koh, Pang Wei and Xiao, Chaowei and Chen, Muhao},
  journal={arXiv preprint arXiv:2401.12255},
  year={2024}
}


@article{biderman2024emergent,
  title={Emergent and predictable memorization in large language models},
  author={Biderman, Stella and PRASHANTH, USVSN and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{biderman2024lessons,
  title={Lessons from the Trenches on Reproducible Evaluation of Language Models},
  author={Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and others},
  journal={arXiv preprint arXiv:2405.14782},
  year={2024}
}

@article{liao2023rethinking,
  title={Rethinking model evaluation as narrowing the socio-technical gap},
  author={Liao, Q Vera and Xiao, Ziang},
  journal={arXiv preprint arXiv:2306.03100},
  year={2023}
}