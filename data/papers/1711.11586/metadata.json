{
  "arxivId": "1711.11586",
  "title": "Toward Multimodal Image-to-Image Translation",
  "authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman",
  "abstract": "Many image-to-image translation problems are ambiguous, as a single input\nimage may correspond to multiple possible outputs. In this work, we aim to\nmodel a \\emph{distribution} of possible outputs in a conditional generative\nmodeling setting. The ambiguity of the mapping is distilled in a\nlow-dimensional latent vector, which can be randomly sampled at test time. A\ngenerator learns to map the given input, combined with this latent code, to the\noutput. We explicitly encourage the connection between output and the latent\ncode to be invertible. This helps prevent a many-to-one mapping from the latent\ncode to the output during training, also known as the problem of mode collapse,\nand produces more diverse results. We explore several variants of this approach\nby employing different training objectives, network architectures, and methods\nof injecting the latent code. Our proposed method encourages bijective\nconsistency between the latent encoding and output modes. We present a\nsystematic comparison of our method and other variants on both perceptual\nrealism and diversity.",
  "url": "http://arxiv.org/abs/1711.11586v4",
  "issue_number": 22,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/22",
  "created_at": "2024-12-22T21:39:02.346958",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_minutes": 1,
  "last_read": "2024-12-22T22:12:06.028899"
}