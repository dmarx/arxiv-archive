\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{Anthropic2023Claude3}
Anthropic.
\newblock The claude 3 model family: Opus, sonnet, haiku.
\newblock \url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock {\em arXiv preprint arXiv:2407.21783}, 2024.

\bibitem{prest2020falcon}
Thomas Prest, Pierre-Alain Fouque, Jeffrey Hoffstein, Paul Kirchner, Vadim Lyubashevsky, Thomas Pornin, Thomas Ricosset, Gregor Seiler, William Whyte, and Zhenfei Zhang.
\newblock Falcon.
\newblock {\em Post-Quantum Cryptography Project of NIST}, 2020.

\bibitem{jiang2023mistral7b}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{bommasani2023foundation}
Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang.
\newblock The foundation model transparency index.
\newblock {\em arXiv preprint arXiv:2310.12941}, 2023.

\bibitem{kapoor2024societal}
Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, et~al.
\newblock On the societal impact of open foundation models.
\newblock {\em arXiv preprint arXiv:2403.07918}, 2024.

\bibitem{white2024model}
Matt White, Ibrahim Haddad, Cailean Osborne, Ahmed Abdelmonsef, Sachin Varghese, et~al.
\newblock The model openness framework: Promoting completeness and openness for reproducibility, transparency and usability in ai.
\newblock {\em arXiv preprint arXiv:2403.13784}, 2024.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{yang2024qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et~al.
\newblock Qwen2 technical report.
\newblock {\em arXiv preprint arXiv:2407.10671}, 2024.

\bibitem{lieber2024jamba}
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et~al.
\newblock Jamba: A hybrid transformer-mamba language model.
\newblock {\em arXiv preprint arXiv:2403.19887}, 2024.

\bibitem{team2024jamba}
Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et~al.
\newblock Jamba-1.5: Hybrid transformer-mamba models at scale.
\newblock {\em arXiv preprint arXiv:2408.12570}, 2024.

\bibitem{sennrich2015neural}
Rico Sennrich.
\newblock Neural machine translation of rare words with subword units.
\newblock {\em arXiv preprint arXiv:1508.07909}, 2015.

\bibitem{tiktoken}
OpenAI Team.
\newblock tiktoken, 2022.

\bibitem{kudo2018sentencepiece}
T~Kudo.
\newblock Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock {\em arXiv preprint arXiv:1808.06226}, 2018.

\bibitem{yang2019xlnet}
Zhilin Yang.
\newblock Xlnet: Generalized autoregressive pretraining for language understanding.
\newblock {\em arXiv preprint arXiv:1906.08237}, 2019.

\bibitem{huggingface_tokens}
Hugging~Face Team.
\newblock Summary of the tokenizers.
\newblock 2024.

\bibitem{reid2024gemini}
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock {\em arXiv preprint arXiv:2403.05530}, 2024.

\bibitem{maaz2023video}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision and language models.
\newblock {\em arXiv preprint arXiv:2306.05424}, 2023.

\bibitem{zhang2023video}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for video understanding.
\newblock {\em arXiv preprint arXiv:2306.02858}, 2023.

\bibitem{zhang2024mm}
Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu.
\newblock Mm-llms: Recent advances in multimodal large language models.
\newblock {\em arXiv preprint arXiv:2401.13601}, 2024.

\bibitem{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock {\em arXiv preprint arXiv:2401.04088}, 2024.

\bibitem{mann2020language}
Ben Mann, N~Ryder, M~Subbiah, J~Kaplan, P~Dhariwal, A~Neelakantan, P~Shyam, G~Sastry, A~Askell, S~Agarwal, et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 1, 2020.

\bibitem{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock {\em arXiv preprint arXiv:2306.01116}, 2023.

\bibitem{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock {\em arXiv preprint arXiv:1911.00359}, 2019.

\bibitem{xue2020mt5}
L~Xue.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock {\em arXiv preprint arXiv:2010.11934}, 2020.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{conneau2019cross}
Alexis Conneau and Guillaume Lample.
\newblock Cross-lingual language model pretraining.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{sachdeva2024train}
Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed~H Chi, James Caverlee, Julian McAuley, and Derek~Zhiyuan Cheng.
\newblock How to train data-efficient llms.
\newblock {\em arXiv preprint arXiv:2402.09668}, 2024.

\bibitem{longpre2023pretrainer}
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et~al.
\newblock A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity.
\newblock {\em arXiv preprint arXiv:2305.13169}, 2023.

\bibitem{du2022glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In {\em International Conference on Machine Learning}, pages 5547--5569. PMLR, 2022.

\bibitem{lee2021deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock {\em arXiv preprint arXiv:2107.06499}, 2021.

\bibitem{agarwal2009url}
Amit Agarwal, Hema~Swetha Koppula, Krishna~P Leela, Krishna~Prasad Chitrapura, Sachin Garg, Pavan~Kumar GM, Chittaranjan Haty, Anirban Roy, and Amit Sasturkar.
\newblock Url normalization for de-duplication of web pages.
\newblock In {\em Proceedings of the 18th ACM conference on information and knowledge management}, pages 1987--1990, 2009.

\bibitem{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock {\em arXiv preprint arXiv:2406.11794}, 2024.

\bibitem{albalak2023efficient}
Alon Albalak, Liangming Pan, Colin Raffel, and William~Yang Wang.
\newblock Efficient online data mixing for language model pre-training.
\newblock In {\em R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models}, 2023.

\bibitem{shen2023slimpajama}
Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, et~al.
\newblock Slimpajama-dc: Understanding data combinations for llm training.
\newblock {\em arXiv preprint arXiv:2309.10818}, 2023.

\bibitem{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock {\em arXiv preprint arXiv:2403.08295}, 2024.

\bibitem{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em Journal of Machine Learning Research}, 24(240):1--113, 2023.

\bibitem{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock {\em arXiv preprint arXiv:2402.00159}, 2024.

\bibitem{penedo2024fineweb}
Guilherme Penedo, Hynek Kydl{\'\i}{\v{c}}ek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von~Werra, Thomas Wolf, et~al.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.
\newblock {\em arXiv preprint arXiv:2406.17557}, 2024.

\bibitem{ostendorffllm}
Malte Ostendorff, Pedro~Ortiz Suarez, Lucas~Fonseca Lage, and Georg Rehm.
\newblock Llm-datasets: An open framework for pretraining datasets of large language models.

\bibitem{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al.
\newblock Starcoder 2 and the stack v2: The next generation.
\newblock {\em arXiv preprint arXiv:2402.19173}, 2024.

\bibitem{roziere2023code}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et~al.
\newblock Code llama: Open foundation models for code.
\newblock {\em arXiv preprint arXiv:2308.12950}, 2023.

\bibitem{ainslie2023gqa}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'o}n, and Sumit Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock {\em arXiv preprint arXiv:2305.13245}, 2023.

\bibitem{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{cerebras2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}.
\newblock \url{https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{weber2024redpajama}
Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, et~al.
\newblock Redpajama: an open dataset for training large language models.
\newblock {\em arXiv preprint arXiv:2411.12372}, 2024.

\bibitem{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication.
\newblock {\em arXiv preprint arXiv:2303.09540}, 2023.

\bibitem{face2023}
Large-scale near-deduplication behind bigcode, 2023.

\bibitem{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock {\em arXiv preprint arXiv:1904.09751}, 2019.

\bibitem{leskovec2014mining}
J~Leskovec, A~Rajaraman, and JD~Ullman.
\newblock Mining of massive datasets, cambridge university press, cambridge, 2014.

\bibitem{patel2020introduction}
Jay~M Patel and Jay~M Patel.
\newblock Introduction to common crawl datasets.
\newblock {\em Getting structured data from the internet: running web crawlers/scrapers on a big data production scale}, pages 277--324, 2020.

\bibitem{broder1997resemblance}
Andrei~Z Broder.
\newblock On the resemblance and containment of documents.
\newblock In {\em Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)}, pages 21--29. IEEE, 1997.

\bibitem{fineweb2024}
Fineweb, 2024.

\bibitem{brandfonbrener2024color}
David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan~Richard Schwarz, and Sham Kakade.
\newblock Color-filter: Conditional loss reduction filtering for targeted language model pre-training.
\newblock {\em arXiv preprint arXiv:2406.10670}, 2024.

\bibitem{fang2023data}
Alex Fang, Albin~Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar.
\newblock Data filtering networks.
\newblock {\em arXiv preprint arXiv:2309.17425}, 2023.

\bibitem{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock {\em arXiv preprint arXiv:2402.00838}, 2024.

\bibitem{kocetkov2022stack}
Denis Kocetkov, Raymond Li, Loubna~Ben Allal, Jia Li, Chenghao Mou, Carlos~Mu{\~n}oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et~al.
\newblock The stack: 3 tb of permissively licensed source code.
\newblock {\em arXiv preprint arXiv:2211.15533}, 2022.

\bibitem{zeng2023glmb}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang.
\newblock {GLM}-130b: An open bilingual pre-trained model.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{parmar2024nemotron}
Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et~al.
\newblock Nemotron-4 15b technical report.
\newblock {\em arXiv preprint arXiv:2402.16819}, 2024.

\bibitem{hendrycks2021measuringmassivemultitasklanguage}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding, 2021.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock {\em arXiv preprint arXiv:1905.07830}, 2019.

\bibitem{li2023colossal}
Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and Yang You.
\newblock Colossal-ai: A unified deep learning system for large-scale parallel training.
\newblock In {\em Proceedings of the 52nd International Conference on Parallel Processing}, pages 766--775, 2023.

\bibitem{loshchilov2017decoupled}
I~Loshchilov.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{ivison2023camels}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al.
\newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2.
\newblock {\em arXiv preprint arXiv:2311.10702}, 2023.

\bibitem{lmevaluationharness}
LM~Evaluation~Harness Team.
\newblock Lm evaluation harness, 2024.
\newblock Accessed: Summer 2024.

\bibitem{open_compass}
Open~Compass Team.
\newblock Open compass, 2024.
\newblock Accessed: Summer 2024.

\bibitem{AutoGPTQ}
AutoGPTQ Team.
\newblock Autogptq: An user-friendly llms quantization package, 2024.
\newblock Accessed: Spring 2024.

\bibitem{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock {\em arXiv preprint arXiv:2204.06745}, 2022.

\bibitem{song2023deepspeed4science}
Shuaiwen~Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar~Ahmad Awan, et~al.
\newblock Deepspeed4science initiative: Enabling large-scale scientific discovery through sophisticated ai system technologies.
\newblock {\em arXiv preprint arXiv:2310.04610}, 2023.

\bibitem{allenai:arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock {\em arXiv:1803.05457v1}, 2018.

\bibitem{van2024ai}
Teun van~der Weij, Felix Hofst{\"a}tter, Ollie Jaffe, Samuel~F Brown, and Francis~Rhys Ward.
\newblock Ai sandbagging: Language models can strategically underperform on evaluations.
\newblock {\em arXiv preprint arXiv:2406.07358}, 2024.

\bibitem{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock {\em Communications of the ACM}, 64(9):99--106, 2021.

\bibitem{bisk2019piqareasoningphysicalcommonsense}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language, 2019.

\bibitem{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36:46595--46623, 2023.

\end{thebibliography}
