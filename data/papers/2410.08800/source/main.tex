\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[english]{babel}
\usepackage{xspace}
\usepackage{todonotes}
\usepackage{longtable}
\usepackage{array}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage[normalem]{ulem}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
%\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\newcommand\quoted[1]{``{#1}''}
\newcommand\dataset{data set\xspace}
\newcommand\datasets{data sets\xspace}
\usepackage{url}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Data Processing for the OpenGPT-X Model Family}
\author{
Nicolo' Brandizzi$^1$, 
Hammam Abdelwahab$^1$, 
Anirban Bhowmick$^1$, \\
Lennard Helmer$^1$, 
Benny Jörg Stein$^1$, 
Pavel Denisov$^1$, 
Qasid Saleem$^1$, \\
Michael Fromm$^1$, 
Mehdi Ali$^1$,
Richard Rutmann$^1$, \\
Farzad Naderi$^2$, % (IIS), 
Mohamad Saif Agy$^2$, % (IIS), 
Alexander Schwirjow$^2$, % (IIS)
Fabian Küch$^2$,\\ % (IIS),
Luzian Hahn$^2$, % (IIS),
Malte Ostendorff$^3$, % (DFKI), %###  
Pedro Ortiz Suarez$^3$, % (DFKI), %###
Georg Rehm$^3$,\\ % (DFKI), %###
Dennis Wegener$^1$,
Nicolas Flores-Herr$^1$,
Joachim Köhler$^1$,
Johannes Leveling$^1$\\\\
%
$^1$ Fraunhofer IAIS,  <firstname>.<lastname>@iais.fraunhofer.de\\
%
$^2$ Fraunhofer IIS,  <firstname>.<lastname>@iis.fraunhofer.de\\
%
$^3$ DFKI,  <firstname>.<lastname>@dfki.de
}
\date{}
\begin{document}
\maketitle

\begin{abstract}
This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. 
We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. % (e.g. GDPR, EU AI Act). 
Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs. 
\end{abstract}
\input{Sections/1.introduction}
\input{Sections/2.rel_work}
\input{Sections/3.datasets}
\input{Sections/4.pipelines}
\input{Sections/5.analysis}
\input{Sections/6.insights}
\input{Sections/7.conclusion}


\section{Acknowledgements}
This work was funded by the German Federal Ministry for Economic Affairs 
and Climate Action (BMWK) through the project OpenGPT-X (project
no. 68GX21007D).
%
The authors gratefully acknowledge the Gauss Centre 
for Supercomputing e.V. (\url{http://www.gauss-centre.eu})
for providing compute resources
on the GCS Supercomputer JUWELS at Jülich
Supercomputing Centre (JSC).

The authors gratefully acknowledge the compute resources made available to them on the high-performance computer at the NHR Center of TU Dresden. This center is jointly supported by the Federal Ministry of Education and Research and the state governments participating in the NHR (\url{http://www.nhr-verein.de/unsere-partner}).

Many thanks to the OpenGPT-X project partner IONOS for providing a NVIDIA DGX-2 machine 
to facilitate development of our data pipelines as well as processing
many curated datasets.
%Center
%for Information Services and High Performance
%Computing [Zentrum für Informationsdienste und
%Hochleistungsrechnen (ZIH)] at TU Dresden for
%providing its facilities for high throughput calculations.
%%JL check which is the official partner name



% \todo[inline]{BS: I don't think AI services should be ``acknowledged''. Perhaps write something like ``In the early 
% stages of preparation of this document, ChatGPT-4 \& Claude were used to improve readability and for brainstorming on potential 
% insights in the analysis section.''}
%We acknowledge the use of AI tools for the early stages of the development of this paper. 
%Specifically, we utilized ChatGPT-4/Claude for improving readability, brainstorming useful insights for the analysis section. 

We acknowledge the use of AI tools (i.e. ChatGPT-4/Claude) in the early stages  of this paper
for improving readability and brainstorming useful insights for the analysis section. 

%\bibliographystyle{plain}
\bibliographystyle{plainurl}
\bibliography{references}
\pagebreak
\input{Sections/8.appendix}

\end{document}
