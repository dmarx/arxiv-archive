\section{Insights and Observations}\label{sec:insights}

During the process of preparing the data for LLM training,
several challenges emerged that shaped the development of the data pipeline. 
These challenges are important for understanding both the complexity of 
large-scale multilingual dataset processing and the technical, 
organizational, and legal obstacles one can encounter. By presenting these 
observations, we provide key findings and recommendations to guide future 
projects and to navigate similar challenges.

\subsection{Data Quality, Availability and Management}
\label{sec:insights.data}
Data quality and availability are key criteria in the 
construction of datasets for language models. Ensuring high-quality 
data is essential for model performance, while the availability of 
data across different languages, genres, and domains presents unique challenges.

\paragraph{Quality Assessment}
The definition of \textsl{high-quality} data remains a complex and evolving 
topic in the field of LLMs, which has gained more traction as these models 
scale and diversify across languages and domains. Recent trends in dataset 
curation reflect an increasing emphasis on quality over quantity, with 
techniques such as advanced filtering and deduplication becoming more 
prevalent \cite{penedo_kydlicek_etal2024,wang_mrini_etal2024}.

Traditionally, data quality has been assessed through intrinsic signals 
derived from the data itself, such as average word length, sentence complexity, 
or overall structure. However, as the field has advanced, the focus has 
shifted towards leveraging pretrained LLMs to judge high-quality 
content or even generate synthetic data, a strategy that has proven 
effective in improving model performance on specific benchmarks \cite{gunasekar_zhang_etal2023,kumar_choudhary_etal2020,yoo_park_etal2021}. 
One reason for the success of these methods is the alignment between human 
judgments of quality and the assessments made by 
LLMs \cite{wang_liang_etal2023,kocmi_federmann2023,fu_ng_etal2023}.

However, this approach comes with several drawbacks. First, despite the 
promising results, a significant limitation is the lack of transparency in the LLM's mapping between defined criteria and system output in defining \textsl{high-quality} content. There is often 
no clear link between the modelâ€™s concept of quality and concrete linguistic 
characteristics, such as sentence structure, readability, or word length. 
Additionally, running LLM inference on large datasets substantially increases 
computational demands, making this approach accessible only to those with 
significant resources. Finally, using LLMs to filter or label data may 
introduce licensing challenges, particularly for commercial applications. We therefore recommend prioritizing more interpretable quality signals 
in data filtering. 

These signals are directly mappable to tangible document 
characteristics, making them transparent and easy to justify. While LLM-based 
filtering techniques show promise, we believe there is a strong need for 
further research into how these models define \textit{high-quality} data. At the 
very least, a hybrid approach combining both traditional and AI-based 
filtering could balance between efficiency, interpretability, 
and scalability.

\paragraph{Availability of Multilingual Data}

As seen in Section \ref{sec:analysis}, one of the primary 
challenges is the significant imbalance in data availability across 
languages. This disparity makes it difficult to create training data 
distributions that ensure downstream models perform well in a wide variety 
of languages. High-resource languages like English dominate the dataset, 
while low-resource languages, such as Irish or Maltese, have far fewer 
available documents, thus requiring special consideration such as avoiding 
filtering and/or deduplicating
to not further reduce the number of tokens available for training in these 
languages.


\paragraph{Versioning and Standardization}

% \todo[inline]{BS: In which sense did we do data versioning for the web data for OpenGPT-X? The clear record of changes is missing in my opinion\dots But we could mention the changes that we incorporated later as a learning. NB: we don't say we do it, we imply it's useful}
Data versioning plays a crucial role in maintaining a clear record 
of changes and ensuring reproducibility throughout the pipeline. To 
facilitate reproducibility, we opted for a unified format (JSONL) across all 
datasets which makes it easier to track changes and compare versions, 
as any modifications can be identified and versioning information is represented consistently across changes. Furthermore, each step 
of our pipeline saves intermediate results, allowing for a granular 
approach to version control.

In addition to this, the intermediate results were moved 
to remote servers, separate from the training environment, to safeguard 
against data corruption or loss. While this precaution provides an extra 
layer of security, it does come at the cost of increased storage usage, 
which needs to be carefully managed. Nonetheless, data versioning supports tracking changes to the pipeline by storing the corresponding pipeline version in the metadata.
% \todo[inline]{BS: I'd think we don't have this for the OpenGPT-X data, only for the approaches later\dots }

\subsection{Licensing and Compliance}
\label{sec:insights.license}
A significant issue is the lack of clear licensing documentation. 
Researchers sometimes neglect to accurately represent the licenses under which their data falls, leaving legal ambiguities. While license identification remains a complex challenge \cite{SeneviratneKB09,HabernalZG16}, we strongly advocate for more precise legal reporting in dataset publications. Initiatives like the Dataset Cards by Hugging Face\footnote{\url{https://huggingface.co/docs/hub/en/datasets-cards}} are a positive step toward improving licensing transparency. They allow dataset creators to include clear license information by default, promoting better legal clarity. However, it is important to note that adding a license in these dataset cards is not mandatory, which means some datasets may still be published without explicit legal documentation. 

%%DONE JL again? NB: it's important! %% put the missing licence bit here: which one?
Even when data is available, special care must be taken to comply with the 
licenses under which it is provided. License compliance is crucial from the 
outset, as the licensing terms of any single dataset can impose restrictions 
on the downstream use of merged datasets or models trained on that data. 
Selecting even one dataset with a more restrictive license can constrain 
the licensing flexibility of all derived artifacts, including processed 
datasets and trained models. Ensuring that data sources have compatible and 
permissive licenses is therefore a critical early step in the project.

%DONE \todo[inline]{BS: The next paragraph probably should belong to the insights section, as it seems to be too detailed in this place here.}

\subsection{Technical and Organizational Challenges}
\label{sec:insights.organization}

The successful creation of large-scale multilingual datasets involves 
navigating a range of technical and organizational challenges. Addressing 
these issues is essential for maintaining efficiency, ensuring data integrity, 
and fostering effective collaboration among diverse teams. The following 
sections explore the technical challenges of processing data at scale and 
the organizational aspects necessary for project success.



\paragraph{Processing at Scale}

Processing large-scale datasets presents significant technical challenges, 
both in terms of the associated costs and the complexity of managing software, 
hardware, and distributed systems. Large-scale computations often require access 
to multiple computing resources, which introduces additional complexities, 
such as varying system requirements, the need to transfer large volumes of 
data, and the maintenance of software across different platforms. Despite 
the availability of resources, managing distributed processing at scale 
remains a significant barrier, and exploring distributed training approaches 
may offer potential solutions for mitigating some of these difficulties in 
the future.

At the start of any project of this magnitude, it is crucial to consider the 
size of the datasets, the availability of storage and compute resources in 
the clusters, and the allocation of both CPU and GPU resources. One key 
aspect that is often overlooked is ensuring that the processed data is in 
the same location as the final model training environment, as transferring 
large datasets between different clusters can significantly impact efficiency 
and cost. However, budget or resources constraints may limit the ability to 
carry out both data processing and model training on the same cluster. In 
such cases, it becomes essential to carefully plan and optimize across these 
dimensions, considering factors such as storage, computational availability, 
and network transfer costs.


\paragraph{Deduplication Strategy}

In our project, a global deduplication approach (where deduplication is 
applied across multiple CommonCrawl dumps) was found to be impractical 
for two primary reasons. First, for high-resource languages like English, 
the combined dataset before deduplication is too large to be processed 
within our computational environments. Second, as noted in 
\cite{penedo_kydlicek_etal2024}, global deduplication does not provide 
measurable benefits for downstream model performance.

Instead, a local deduplication strategy, applied per-dump and per-language, 
was adopted. This approach reduced the dataset size by approximately 30\%, 
significantly improving processing efficiency in subsequent stages and 
enhancing the overall quality of the training data by minimizing redundancy 
while preserving diversity.


\paragraph{Organizational aspects}
For future large-scale data projects, it is essential to recognize the 
importance of managing diverse expertise, which spans scientific, technical, 
legal, and linguistic domains. Coordinating these competencies is crucial 
for the successful identification and evaluation of data sources, as well 
as for handling the vast volume of data. However, simply collecting data 
is not enough to ensure smooth operations or the sustainability of data 
pipelines and platforms.

We strongly recommend establishing robust data governance and lineage 
strategies at the outset of the project. Defining clear roles, responsibilities, 
and processes early on can significantly improve communication and collaboration 
across teams, aligning efforts as the project evolves. Implementing governance 
frameworks, such as those outlined in \cite{jernite_nguyen_etal2022}, can 
provide valuable structure, helping to maintain efficiency and ensure 
long-term success. Early planning in these areas will prevent organizational 
bottlenecks and provide a foundation for sustainable data management.


\subsection{Adapting to Rapid Innovation}
\label{sec:insights.innovation}

The rapid pace of innovation in data processing methodologies for language 
models introduces a continual challenge. New tools and software frequently 
emerge, offering solutions to existing bottlenecks such as enhanced processing 
efficiency, improved storage capabilities, and advanced filtering techniques. 
However, integrating these innovations into established systems often necessitates 
workflow adaptations, retraining personnel, and potentially reconfiguring entire 
pipelines or rerunning pipelines on all datasets. This process can disrupt 
system stability while requiring considerable investment of time and resources.

Finding a balance between adopting innovations and maintaining operational 
consistency is essential. While the implementation of new software can result 
in performance improvements, it is critical to evaluate which components of 
the system should remain stable to ensure uninterrupted functionality. 
Furthermore, frequent updates (such as newly released curated datasets or 
CommonCrawl dumps) add to the complexity, as these updates necessitate 
continuous modifications to the data pipeline without undermining the 
overall coherence of the system.

A systematic approach to addressing this challenge involves organizing the 
pipeline into modular components from the outset. By clearly separating and 
labeling each segment of the pipeline, it becomes possible to identify which 
elements are adaptable to future innovations and which should remain static due 
to their critical nature. For instance, components such as deduplication processes
may benefit from frequent optimization, whereas core elements like metadata 
normalization or key filtering mechanisms might need to remain constant to 
maintain reliability. Furthermore, depending on the stage of the project, it 
may be prudent to commit to a particular solution, even if suboptimal, rather 
than continuously reconfiguring the system.

% \begin{itemize}
%     \item 
% move data between compute clusters since most of them are limited. Data needs to be in the same cluster as where you train. HPC not enough storage capacity. Non distributed training. 
% \item 
% Data corruption, how to handle failed downloads?
% \item 
% Use of different frameworks. Fast pace env lead to fast adaptation on frameworks. 
% \item
% keep track of what data is used for what with licenses. Data managment and versioning.
% \item
% First ones to look intro different languages than english. In several processing steps (c4) there were some specifics language processes. Basically, all the multilingual stuff. Encoding of different langs (nordic)
% \item
% Insights in how to judge new tools for data processing. When to use? When to take inspiration? When to rebase?
% \end{itemize}

% \paragraph{Others}

% %- What is Quality (literature search from previous work from old ap2 members??) \\
% there are different aspects/dimensions of quality: quantity (?), diversity, text types/genres/styles
% quality is typically not measured based on the data (as a single metric), but via model performance)


% - distributed data processing in slurm infrastructre (vs spark)\\
% - Scalable data versioning  \\
% everything needs to be versioned, from raw and processed data overs software for the data pipeline, to the trained model
% this allows provenance for the data and traceability

% - deduplication as a huge blocker (back then) \\
% there was no scalable solution for deduplication processing the largest data block (English CommonCrawl) globally
% full global deduplication (across all languages) failed; we split the data by language
% different other implementation were investigated (incl. using Apache Spark)
% different combinations (e.g. 20 CC dumps) were tried; iterative deduplication with shuffling was discussed

% - why we chose to convert everything into one format
% one common format is benefitial for service interoperability and development/testing
% same with common/standard preprocessing steps
% intermediate results can be saved, which is important when switching compute environments
% we chose JSONL after converting from a variety of formats (incl. HTML, XML, CSV, plain text, or SQL dumps)

%  - why we keep local copies of the data (and not rely on HF only)
%  network bandwitdh
%  experimentation and development
%  - streaming vs batch processing of the data
%  same reasons
 
 
%- duration to prepare data for training (not sure about this one, ideally it should be about how long should it take to prepare pretraining data. The reality was it was built up one by one with pressure to handle the prepared version in 2-3 weeks deadline to not waste the compute budget .. sad story but we could mention some good insight/advice from it)\\


