\section{Data selection}
\label{sec:datasets}


The performance of large language models in downstream tasks benefits from large volumes of diverse, high-quality training data. Key properties for effective pretraining include the diversity of knowledge, domains, and tasks, which enhance a model's generalization abilities \cite{gao_biderman_etal2020}, as well as the appropriate size to fully saturate the model's learning capacity \cite{jordan_hoffmann_etal2022}. To mitigate cultural bias from monolingual English datasets, we emphasize the selection of non-English data. Specifically, we aim to balance our data to encompass all European languages. 
This section outlines the data selection requirements that ensure these desirable properties 
and provides an overview of the chosen data sources.

Given the aforementioned properties, 
our approach to data selection involves ensuring a large volume\footnote{The amount of training data is typically measured by the number of tokens or words. However, the choice of a tokenizer affects the number of generated tokens, making absolute numbers for different models not directly comparable. Therefore, we use the \texttt{wc} command line utility to estimate the number of words.} of diverse and \textsl{high-quality} processed data for LLMs training. 

\textbf{Data Quantity and Diversity.}
A significant portion of our data is derived from crawled web pages, complemented with curated datasets to ensure a broad spectrum of languages, genres, text types, and domains. These curated datasets are carefully selected to enhance performance in various downstream tasks such as question answering, machine translation, and summarization.


\textbf{Data Quality.} 
The quality of large language models is highly dependent on the textual training data, often measured in terms of the number of tokens or words. High-quality data should be multilingual (covering as many relevant languages as possible), diverse (sourced from multiple domains and document types), free of toxic or offensive content (to prevent harmful outputs), and unbiased (to ensure fair model behavior). In this context, data quality directly reflects the properties of the text itself. For a detailed discussion of our filtering processes, see Section  \ref{sec:pipelines}.

\subsection{Selected data}
\label{sec:datasets.selection}

Following these requirements, our data sources can be categorized into two groups. The first group consists of curated data from multiple existing datasets, providing a relatively small but diverse set of high-quality knowledge sources. The second group comprises general web data sourced from the CommonCrawl project. 

\subsubsection{Curated data}
\label{sec:datasets.selection.curated}

Curated datasets are essential for providing high-quality, diverse training data for our language models. These datasets have typically undergone a quality review process, either prior to their release or through ongoing public review,
and are publicly available
(see Appendix \ref{sec:appendix.curated} for a list of our curated datasets). This includes data such as Wikipedia, collections of 
scientific articles, books, and source code.

To select appropriate datasets, we employed the following considerations:

%%DONE JL expand on the questions and add implications, NB: what kind of implications? 
\begin{enumerate}
    \item Legal and Licensing
    \begin{itemize}
        \item Is the license unproblematic? (e.g., allows research and commercial use, is permissive, and does not include restrictive copyleft clauses). 
        \item Is the data GDPR-compliant?
    \end{itemize}
    
    \item Linguistic and Relevance
    
    \begin{itemize}
        \item Does the dataset contain documents in a language relevant to our research?
        \item Does the dataset contain a sufficient quantity of tokens/words/documents? 
        \item Do the documents cover relevant topics? 
        (e.g. for downstream applications)
    \end{itemize}
    
    \item Quality and Integrity
    
    \begin{itemize}
        \item Is the content of documents error free? 
        (e.g., no artifacts from Optical Character Recognition, misspellings, or antiquated language)
        \item Are documents from a geographical region of interest? 
        %(e.g. newspaper articles from former colonial territories)
        \item Are the documents from a relevant time period?
        (e.g. no historic documents)
        \item Are the documents human-generated\footnote{While it is not always easy to detect AI-generated content, extra caution should be applied for datasets created after 2021 to avoid including synthetic data.}?
        \item Is the dataset artificial?
        (e.g. no automatically generated content or synthetic data)
        \item Is there no significant overlap with other datasets already in use?
    \end{itemize}
    
    %(e.g. 5M tokens or substantial 
    %    size for underrepresented languages)
    \item Resource Availability
    
    \begin{itemize}
        \item Are there sufficient resources available to preprocess the data? 
        (e.g., adequate disk space and computational resources)
        \item Has the dataset not been fully or partially superseded by a newer version?
        (e.g. no outdated version)
    \end{itemize}

\end{enumerate}

If one or more of these questions were answered negatively, the dataset was excluded from further processing. These criteria ensure that the curated datasets selected for our project are of the highest quality, relevance, and suited for our specific objectives.

% \todo[inline]{DW: In den späteren subsections heiss es Pipeline for web data bzw. einfach web data. Das könnte man für die folgende subsection vereinheitlichen. }
\subsubsection{CommonCrawl data}
\label{sec:datasets.selection.cc}

Unlike curated data, web data is available in a large amount. For this reason, we prioritize processing web data only based on recency and language. Specifically, we use data from CommonCrawl (CC), an organization that maintains a free, open-source repository of web data accessible to anyone since they provide the largest amount of data. They perform monthly crawls of internet webpages, compiling them into data dumps labeled in the format YYYY-WW (e.g., 2024-22, see Section \ref{sec:analysis.web} for a list of selected dumps). 
%Here, Y stands for the year and W stands for the week of the year the data dump was released.
CommonCrawl dumps can be downloaded from URLs\footnote{Following the schema \texttt{https://data.CommonCrawl.org/crawl-data/CC-MAIN-<YEAR-MONTH>/index.html}.} in two main formats: WARC and WET. WARC (Web ARChive format) files contain raw data from the crawl, including the full page HTML and request metadata. WET (WARC Encapsulated Text) files provide a text-only version of the websites, excluding the web metadata.
%%JL we chose CC over various other (processed) sources for web data, due to the overall size of the CC data
In our pipeline, we use the WET files exclusively as they provide the extracted textual content of web pages in a simplified format. This eliminates the need for extensive parsing of raw HTML data found in WARC files, allowing us to focus directly on text analysis. For instance, the CC dump ``2024-22'' includes data crawled from 2.5 billion web pages, totaling 18.35 TiB of compressed HTML data. This particular dump consists of 90,000 WET files, which are grouped into segments for processing. This large-scale web data significantly expands the scope of our collection, providing a robust foundation for pretraining large language models.


