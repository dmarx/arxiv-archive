\section{Related Work}
\label{sec:rel_work}
Training data for LLMs is primarily based on 
web-based data sources like Common Crawl\footnote{\url{https://commoncrawl.org/}}.  
In this section, we present an overview of similar initiatives 
to train LLMs to provide context for the data processing pipeline 
described in this paper. By outlining related datasets and their 
corresponding pipelines, we aim to highlight the key features 
and challenges of building such datasets, offering a comparative 
perspective to the approach taken in our 
work. We provide an overview in Table \ref{tab:datasets} (see Appendix).


\subsection{Monolingual datasets}
\label{sec:rel_work.mono}
The prevalence of English in LLM development is largely driven 
by the availability of English web data and the dominant role 
of English as a global language.
This abundance of English data allows better data quality control,
filtering, and deduplication by setting more aggressive selection criteria
and discarding larger amounts of suspicious data,
resulting in highly optimized datasets for LLM training.
However, this also led to a disproportionate focus on English, potentially limiting the applicability of models trained on these datasets to non-English contexts.

\textbf{Colossal Clean Crawled Corpus (C4)} \cite{raffel_shazeer_etal2020} 
is a widely-used dataset derived from CommonCrawl data and serves as the 
foundation for Google’s T5 model. C4 contains approximately 750GB of 
English text and is built through a filtering process designed to 
extract high-quality language data. The pipeline first filters out 
non-linguistic content, such as source code, placeholders, and documents 
with excessive punctuation or abnormal words per line ratios. A deduplication 
process is applied to spans of three sentences to ensure minimal repetition. 
Afterwards, documents filtered based on the score of language detection on
the textual content, keeping only English documents with a language score higher than 0.99, based
on the \textit{langdetect} tool\footnote{\url{https://github.com/Mimino666/langdetect}}. 
This document filter ensures that C4 remains highly focused on English content, with a 
restrictive approach to both content quality and language identification.

\textbf{The Pile} \cite{gao_biderman_etal2020} is a 800GB English dataset 
comprising a diverse set of 22 specialized text collections, making it 
one of the most comprehensive datasets for LLM training. It includes a 
wide variety of data sources, such as scientific papers, legal documents, 
fiction, patents, subtitles, and online forums, in addition to 
filtered CommonCrawl data. For the CommonCrawl portion, text is extracted 
using the jusText\footnote{\url{https://github.com/miso-belica/jusText}} 
text extractor, and language identification is performed using the \textit{pycld2} tool\footnote{\url{https://github.com/aboSamoor/pycld2}}. 
A \textit{fastText} classifier \cite{joulin_grave_etal2016b}, trained on 
the OpenWebText2 subset \cite{radford_wu_etal2019,gokaslan_cohen2019}, 
%%DONE JL missing reference %%PD: OpenWebText2 is a part of the Pile, %%    but I added WebText and OpenWebTextCorpus references, %%    the Pile paper cites them for %%    the OpenWebText2 generation method
is used to further refine document quality. Deduplication is applied to both the 
CommonCrawl and OpenWebText2 subsets using the MinHashLSH algorithm \cite{broder1997}, 
ensuring minimal redundancy of the textual content while preserving its diversity. 
This pipeline design has made The Pile a key dataset for LLMs, especially due to 
its combination of web-sourced data with specialized collections.

\textbf{RefinedWeb} \cite{penedo_malartic_etal2023} is a 2.8TB English dataset 
created from filtered CommonCrawl data, employing the MacroData refinement 
pipeline. The pipeline begins by extracting text from WARC (Web ARChive) files using the
\textit{trafilatura} tool \cite{barbaresi2021}, which is then passed through 
several stages of filtration and deduplication.
First, the \textit{fastText} language classifier \cite{grave_bojanowski_etal2018}
identifies and retains English documents. The dataset undergoes both 
document-wise and line-wise heuristic filtering to remove low-quality 
or non-linguistic content. Deduplication is then applied in three stages: 
1) MinHashLSH deduplication at the document level, 
2) exact matching on spans of 50 tokens, and 
3) URL-based deduplication. 

\textbf{Dolma} \cite{soldaini_kinney_etal2024} contains about 11TB of English 
text and comprises of the CommonCrawl derived
data combined with code, social media, scientific papers, and fiction from 
the specialized web sites.
The CommonCrawl subset is extracted from the WARC files and is 
filtered by document-wise
and line-wise heuristics, the \textit{fastText} language classifier with 
the cutoff score of 0.5, and the \textit{fastText} toxic content classifier.
Deduplication is performed based on URLs, exact match of raw documents, and
on paragraphs using a Bloom filter \cite{bloom1970}.

\textbf{FineWeb} \cite{penedo_kydlicek_etal2024} contains approximately 43TB of 
English text and is constructed using 
DataTrove\footnote{\url{https://github.com/huggingface/datatrove}}, which 
was released alongside the dataset.
The pipeline builds upon the methodology used in RefinedWeb, beginning with text 
extraction from WARC files using the \textit{trafilatura} tool. 
Language filtering is performed with \textit{fastText}, applying a cutoff score 
of 0.65 to ensure the inclusion of high-quality English documents. After this, the 
dataset undergoes both document-wise and line-wise heuristic filtering to further
enhance quality by removing noise and non-linguistic content. The final deduplication 
step employs the \textit{MinHash} algorithm, ensuring a clean and diverse dataset that 
is well-suited for large-scale language model training.


\subsection{Multilingual datasets}
\label{sec:rel_work.multi}

Creating high-quality multilingual datasets presents distinct challenges compared to 
monolingual English datasets. Language identification becomes significantly more complex, 
as models must accurately differentiate between multiple
of languages, often with limited training data for lower-resource languages. Additionally, 
ensuring consistent quality across languages is difficult due to varying amounts of available 
web data, and some languages are overrepresented while others suffer from a lack of resources. 
High quality multilingual datasets are essential for training models that perform well across 
diverse linguistic and cultural contexts, enabling the training of LLMs that are 
not inherently biased toward dominant languages like English.

\textbf{CCNet} \cite{wenzek_lachaux_etal2019} is among the first datasets built on CommonCrawl data,
containing 3.2TB (1.5 billion documents) in 130 languages. The CCNet pipeline begins 
by extracting paragraphs from WET files and performing SHA-1-based deduplication. The 
extracted text is normalized by converting it to lowercase, replacing all numbers with 
zeros, and removing accents and punctuation.
Language detection is based on \textit{fastText}, with documents with a language score below 0.5  being 
discarded. The final step in the pipeline involves filtering documents based on 
perplexity scores calculated with language-specific 5-gram Kneser-Ney language 
models \cite{heafield2011}. The corresponding models are trained on tokenized Wikipedia 
data, using the sentence piece tokenizer \cite{kudo2018} to ensure consistent text segmentation.


\textbf{Multilingual C4 (mC4)} \cite{xue_constant_etal2021} extends the C4 dataset to 
101 languages and contains approximately 27TB of text. In contrast to C4, which focuses 
on English, mC4 replaces the language classification tool with 
\textit{cld3}\footnote{\url{https://github.com/google/cld3}} to handle the identification of multiple languages.
A significant modification in mC4 is the introduction of a filtering step that requires documents to contain at least three lines of text, each with 200 or more characters. This broader filter replaces C4’s original language-specific rule, which filtered based on English punctuation marks, ensuring a more consistent approach across languages and improving the quality of multilingual data.



\textbf{OSCAR 22.01} \cite{abadji_suarez_etal2022} contains about 8TB of text in 153 languages.
It is produced with the Ungoliant pipeline \cite{abadji_suarez_etal2021} from the CommonCrawl WET (Web Extracted Text) files.
The filtration starts with language detection, which utilizes
the \textit{fastText} model
to identify language of each line in a document.
If the confidence score is less then 0.8,
then the line is assigned with to the \textit{unknown language} class.
The proportion of each language in a document is calculated
as a percentage of bytes in the document assigned with that language.
%% DONE \todo[inline, color=yellow]{BS: I rephrased the following sentence because it was hard to follow. Can you please check that the meaning is still correct? }
A document is classified as multilingual 
if it contains at least 5 lines, not more than 5 languages, the
proportion of each language is at least ${1}/{(m + 1)}$ (where $m$
is the number of languages in the document) and the proportion of 
the unknown language is not larger than ${1}/{(m + 1)}$.
Otherwise, the document is classified as monolingual
and is assigned with the language having the highest weighted confidence
score. The weighted confidence of a language is calculated as a sum of products of
byte size and language confidence of each line classified to that language
divided by the total number of bytes in the document.
A monolingual document is passed to further processing steps
if the weighted confidence of its language is at least 0.6.
Language identification is followed by
the filtering of documents based on the line lengths, proportions
of characters of certain Unicode classes, and the URL based UT1 blocklist.\footnote{\url{https://dsi.ut-capitole.fr/blacklists/index_en.php}}
Deduplication is not applied to non-English data, and only line-wise deduplication
is applied to English data.

\textbf{BigScience ROOTS} \cite{laurenccon_saulnier_etal2022} is a dataset containing about 1.6TB of text in 46 natural languages.
This dataset comprises data from the crowdsourced list of 252 monolingual
and multilingual text collections, accompanied by documents extracted from the
CommonCrawl WARC files according to the list of domains suggested by the community
members. Extraction of text from HTML files is performed by custom code
inspired by the CommonCrawl extractor. Deduplication is performed on the level
of data sources. Additional data is obtained from the OSCAR version
21.09 dataset \cite{suarez_etal2020}. The OSCAR derived documents are passed through heuristic rules
based on word frequencies and are deduplicated in two steps using SimHash \cite{charikar2002,manku_jain_etal2007} and substring deduplication \cite{lee_ippolito_etal2022,manber_myers1993}.

%%DONE JL: why are we citing this? %%PD: HPLT has it in related work, so maybe it's also related to us -- feel free to remove
The \textbf{MADLAD-400} \cite{kudugunta_caswell_etal2024} dataset contains about 30TB of text in 419 languages. The data
is obtained from the CommonCrawl dumps and is initially deduplicated on the line
level, followed by basic prefiltering similar to the C4 rules.
The semi-supervised language identification model
\cite{caswell_breiner_etal2020}, which is trained
with two tasks (supervised language detection task and
unsupervised corrupted input recovery task \cite{raffel_shazeer_etal2020}),
is used to classify the documents after that.
The language identification step is followed by another
set of quality filtering heuristics.
A few language-specific processing and filtering rules are applied based on the
manual inspection of 20 documents per language.

%% DONE JL is this relevant? %%PD: HPLT has it in related work, so maybe it's also related to us -- feel free to remove
%%DONE JL commented out
%The \textbf{Glot500} \cite{imanigooghari_lin_etal2023} dataset contains 600GB of text in 511 languages. The sources of the data are
%some previously published datasets, and web crawling of the manually selected websites.
%The data is filtered using the BigScience ROOTS sentence-level rules applied
%to chunks of the fixed number of tokens (space separated or sentence piece based),
%and using language-script matching on the level of data source. \todo[inline]{BS: What does the previous sentence %mean?}

\textbf{RedPajama-v2}\footnote{\url{https://github.com/togethercomputer/RedPajama-Data}} 
contains about 20 billion documents and 30 trillion tokens
in 5 European languages.
It is produced with the extended CCNet pipeline and utilizes both heuristic based
and classifier based content filtering. The deduplication is performed on the document level using a Bloom filter.
%% DONE %%JL add reference for Bloom filter %PD: there is a reference 9 paragraphs before

The \textbf{HPLT} \cite{degilbert_nail_etal2024} dataset contains 50.1TB of text in 75 languages. The data is sourced from
the Internet Archive and CommonCrawl dumps in WARC format. First, text is
extracted using the \textit{warc2text} tool\footnote{\url{https://github.com/bitextor/warc2text}} and classified by language
using the CLD2 library\footnote{\url{https://github.com/CLD2Owners/cld2}}. Next, text is cleaned from encoding errors,
and another two-stage paragraph-level LID is performed. Its first stage
utilizes the fastText model, and the second stage applies dictionary based
spell checking for several languages related to the fastText result and
selects the language, for which fewer mistakes are detected 
\cite{banon_ramirez_etal2024}. Finally, MinHash based deduplication is 
performed on the document level.

% To conclude, there is a growing number of datasets suitable for LLM pretraining,
% which focus primarily on the amount of data and, more recently, on the number of covered languages,
% but mostly rely on web data and do not target real life use cases \todo{lennard: unclear what real life use cases in languages are} in major European languages.
%%DONE JL commented out as we do not publish the data
%We aim to address fill this with our datasets, which 
The collection of data we processed comprises data from both web and curated sources,
including parts of The Pile and RedPajama-v2, and the content and metadata 
are normalized in a uniform way
by the data processing pipeline described in this work.

%%DONE JL add a summary sentence here %%JL why is all of this relevant? %%JL mention that we used part of the pile and redpajama, but processed it with our own pipeline again, to ensure that normalization of content and metadata was performed consistently


%The OpenGPT-X model family required  a careful data selection strategy and
%looking into additional datasets to provide training data for European languages.
%Data pipelines for other projects only focus on a subset of European languages or are heavily %biased towards English to determine quality criteria. For OpenGPT-X models, 
%CommonCrawl dumps were processed with the Ungoliant pipeline.


% Blog entry C. Perone: 
% https://blog.christianperone.com/2023/06/appreciating-llms-data-pipelines/