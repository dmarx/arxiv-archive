\section{Conclusion and Outlook}\label{sec:conclusion}

In this paper, we presented the entire process of data preparation for the OpenGPT-X model 
family, covering every step from the initial selection of data to delivering the finalized 
datasets for model training. We outlined the requirements that guided our 
data selection, which conceptually split our sources into two categories: curated data and web data. This distinction shaped our approach to the implementation of our data pipelines: one for curated data, which required minimal filtering, and another 
for web data, which focused heavily on filtering and deduplication. In addition to providing 
a thorough description of our pipeline and data preparation methods, we included an 
in-depth analysis of the resulting datasets. 
This analysis ensures transparency, aligning with European data recommendations and 
best practices in scientific research.

Finally, to contribute to the broader research field and promote openness, we 
highlighted the challenges faced during this project and the lessons learned. By sharing these insights, we aim to provide valuable guidance for future projects that undertake large-scale data preparation for multilingual  language models. We utilized a subset of the preprocessed datasets 
detailed in this paper to pretrain large language models for the OpenGPTX project. 

% As part of our future research, we are planning to investigate additional directions such as: synthetically generated data, advance filtering techniques and expanding our collection of dataset. 

As part of our future research, we plan to investigate the use of 
synthetic data generation for the creation of LLM pretraining data 
and for advanced data quality filtering, e.g. based on LLMs as a judge.
Furthermore, we are looking into generating training data that is compliant
with current or upcoming laws and regulations (EU AI Act, GDPR), e.g.
by removing or masking personally identifiable information.
We intend to extend our collection of datasets, also targeting
both datasets for underrepresented domains as well as languages.
With additional processing requirements due to 
advanced processing techniques,
and a growing number of available datasets, 
the demand for compute resources to generate high-quality training data
will also increase.









% \paragraph{Synthtic Data}
% The use of LLMs to generate sythetically generated data is starting to show potential in  classification tasks \cite{penedo_kydlicek_etal2024,josifoski_etal_2023_exploiting,SILVA2024111740}. 
% While generating synthtic data remains a computationally challenging taks, 


% Synthetic data
% Advanced filtering 
% Access to more EU compute
% Access to more relevant content (europaina) 