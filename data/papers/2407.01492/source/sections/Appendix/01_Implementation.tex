\section{Implementation details}
\label{appendix:implement}
We utilize the model architecture proposed by \citet{zhang2024tinyllama} and create various model variants by modifying the number of layers, the number of attention heads, and the dimensions of token embeddings and hidden states, as illustrated in Figure~\ref{tab:model_config}.
For tokenization, we employ the GPTNeoX tokenizer \citep{black2022gpt}, which has a vocabulary size of 50,432.

For models with 1M and 60M parameters, we set the training iterations as 1000 and the batch size as 1M tokens, which means the training budget is 1B tokens.
Similarly, we train the larger model with 1B parameters with 25000 training iterations and the same batch size thus consuming 25B tokens in total.
We set the learning rate as 4e-4 and use the cosine learning rate scheduler.

For linear regression, we employ 5-fold cross-validation with ridge regression to determine the optimal $\ell_2$ regularization weight from the set [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3].
For LightGBM, we manually set the number of iterations to 1000 and the learning rate to 1e-2.
leaving all other hyperparameters at their default values.



\begin{table}[htbp]
    \centering
    \caption{The detailed model configuration for different model sizes.}
    \label{tab:model_config}
    \begin{tabular}{cccc}
        \toprule
        \textbf{Model} & \textbf{1M} & \textbf{60M} & \textbf{1B} \\
        \midrule
        Vocabulary Size & 50432 & 50432 & 50432 \\
        $n_{\rm layers}$ & 2 & 10 & 22 \\
        $n_{\rm heads}$ & 8 & 8 & 16 \\
        $d_{\rm embedding}$ & 256 & 768 & 2048 \\
        $d_{\rm model}$ & 512 & 1536 & 5632 \\
        \bottomrule
    \end{tabular}
\end{table}