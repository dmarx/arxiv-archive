\section{Related work}

\textbf{Data selection and mixture} is concerned with curating data to optimize some goals, usually model performance~\citep{koh2017understanding,albalak2024survey}. Prior methods can be categorized into: \textbf{(1) Token-level} selection is the most fine-grained level of selection dealing with the filtering of tokens~\cite{lin2024rho1}.
\textbf{(2) Sample-level} selection is about choosing individual training examples. It is commonly employed for selecting fine-tuning data~\citep{thakkar2023self, das2023deft,xie2023data,engstrom2024dsdm,xia2024less, liu2023makes, bukharin2023data,kang2024get,mekala2024smaller, sachin2024selectllm,yang2024smalltolarge}. For the pre-training of LLMs, most methods rely on heuristics~\citep{rae2023gopher,sharma2024text,soldaini2024dolma}, but there have been some learned approaches using optimization algorithms~\citep{chen2024take,mindermann2022prioritized,shao2024balanced,yu2024mates}, model perplexity~\citep{marion2023investigating,muennighoff2023scaling}, or LLMs to inform the sample selection process~\citep{alex2024qurating,sachdeva2024train,zhang2024autonomous}. \textbf{(3) Group-level} selection assumes the data can be grouped into pools that are then optimally mixed. While early work again relies on manual mixtures~\citep{the_pile_corpus,gpt3paper}, learned mixtures have become more common~\citep{albalak2024survey}. Learned approaches either leverage proxy models to determine fixed weights for each group (``offline selection'')~\citep{rae2023gopher,xie2023doremi,fan2023doge} or dynamically adjust the weights during training of the final model (``online selection'')~\citep{chen2023skill}. Our approach, \ourmethod, is an offline group-level selection method. Different from the flagship algorithm in this category, DoReMi~\citep{xie2023doremi}, \ourmethod does not require training a single model for hundreds of thousands of steps, but instead a few small models for short durations. As these can be trained in parallel, our approach is more scalable, while also yielding better weights leading to a more performant final model.

\textbf{Data scaling laws} explore interactions of data quantity, quality, and mixing proportions, as LLMs are scaled up. \citet{muennighoff2023scaling} introduce scaling laws for data-constrained scenarios and \citet{goyal2024scaling} try to extend this approach to deal with multiple data pools. Prior research has confirmed that different datasets require different scaling~\citep{hoffmann2022training,pandey2024gzip}, thus \citet{ye2024datamixing} and \citet{ge2024data} propose functional relationships to predict the impact of mixtures on language modeling loss. Some work has investigated optimal mixtures during continued pre-training rather than from scratch training~\citep{que2024dcpt,dou2024sailor}. While most of these works focus on validation loss, others investigate downstream performance and develop predictive relations with loss~\citep{gadre2024language,yang2024fine,xia2022training}.
Different from data scaling work that attempt to find an analytical scaling function~\citep{hoffmann2022training}, \ourmethod directly optimizes the target metric using regression models.
\ourmethod is designed for from-scratch pre-training. In line with previous research, we also find strong correlations between loss and downstream performance, especially for loss on web corpora.

% While early works manually decide the mixture~\citep{the_pile_corpus}, later LLMs try to optimize mixture weights based on downstream performance~\citep{rae2023gopher}. DoReMi~\citep{xie2023doremi} shows that using a small proxy model trained for 200K steps, they can predict a data mixture of a 30x larger model that outperforms a human-selected mixture. Our method, \ourmethod, is a group-level selection algorithm.
% Additionally, some work finds that validation loss and downstream task performance. \citet{gadre2024language,yang2024fine} related language model perplexity to downstream metrics via a power law in the over-trained regime, while \citet{yang2024fine} compared model capabilities across pre-training checkpoints, confirming similar downstream metric dynamics across model sizes.
% Important for our work, because of our rank invariance assumption. These past works generally show that scaling behavior at small scale can be extrapolated to larger scales.
% Our work is different in that we do not require fitting a scaling law which can require lots of runs~\citep{hoffmann2022training} and we can choose any target metric.



% }