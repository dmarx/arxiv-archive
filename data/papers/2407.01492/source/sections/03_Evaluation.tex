\section{Evaluating on regression prediction}

\label{sec:evaluate_regression}

In this section, we evaluate the ability of \ourmethod to predict the effect of unseen data mixtures.
First, we fit the regression model using training artifacts of small (i.e., 1M parameter) models and evaluate the loss prediction performance on small models.
Then, to verify our \textit{rank invariance hypothesis}, we test the learned regression on predicting the rank across model sizes and the number of tokens.


\subsection{Experimental setup}

\paragraph{Datasets and models.}
We conduct our experiments using the domains of the Pile dataset~\citep{the_pile_corpus} depicted in Table~\ref{table:pile_overview}.
Due to copyright concerns, we utilize the 17 subsets available on HuggingFace~\footnote{\url{https://huggingface.co/datasets/monology/pile-uncopyrighted}} that do not violate copyright issues.
We consider both linear regression and LightGBM regression models, where the target variable $y$ is set to be the validation loss of the Pile-CC domain.


\begin{table}[t]
    \centering
    \small
    \caption{We fit the regression model based on the results of the $512 \times$ 1M models trained on 1B tokens, and evaluate it on \textbf{unseen data mixtures} for 1M, 60M, and 1B parameter models depicted below. Pearson's $r$ and MSE measure the loss prediction performance, while the Spearman correlation $\rho$ compares the predicted and actual ranks.}
    \label{tab:linear_vs_lightgbm}
    \scalebox{0.93}{
    \begin{tabular}{lccccccc}
        \toprule
        {Method} & \multicolumn{3}{c}{1M models with 1B tokens} & \multicolumn{2}{c}{60M models with 1B tokens} & \multicolumn{2}{c}{1B models with 25B tokens} \\
        \cmidrule(r){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
         & \textbf{$\rho$ ($\uparrow$)} & {Pearson's $r$ ($\uparrow$)} &
         MSE ($\downarrow$) & $\rho$ ($\uparrow$) & {Pearson's $r$ ($\uparrow$)} & {$\rho$ ($\uparrow$)} & {Pearson's $r$ ($\uparrow$)} \\
        \midrule
        Linear & 90.08 & 87.78 & 0.13  & 89.26 & 86.79 & 88.01 & 72.57 \\
        LightGBM & \textbf{98.45} & \textbf{98.57} & \textbf{0.04} & \textbf{98.64} & \textbf{98.28} & \textbf{97.12} & \textbf{94.36} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{1mm}
\end{table}

\paragraph{Training and evaluation.}
The regression model is fitted using the training artifacts of $512 \times$ 1M models with 1B tokens, and evaluated on $256 \times$ unseen data mixtures for 1M, 60M models (each trained with 1B tokens) and $64 \times$ unseen data mixtures for 1B models (each trained with 25B tokens).

\paragraph{Evaluation metrics.}
We use three different metrics to benchmark our regression models: (1) \textit{Spearman Rank Correlation ($\rho$)} is a non-parametric measure of the strength and direction of the association between two ranked variables. (2) \textit{Pearson's $r$} is a measure of the linear relationship between two variables. (3) \textit{Mean Squared Error (MSE)} is a common metric used to evaluate a regression model by measuring the average squared differences between predicted and actual values. 

\subsection{Experimental results}\label{sec:regression_results}


\paragraph{High correlation across model sizes.}
As shown in Table~\ref{tab:linear_vs_lightgbm}, the LightGBM model demonstrates superior performance over linear regression models across all three metrics, with its advantage becoming increasingly pronounced when evaluating on larger models with more training tokens.
Meanwhile, the fact that 1M models trained with 1B tokens can achieve such a high correlation of $97.12$\% on unseen mixtures of 1B models with 25B tokens directly validates our \textit{rank invariance hypothesis}. \looseness=-1


\paragraph{Proxy model count outweighs training token count.}
Given the same FLOPs budget for small-scale training, we can either increase the token count (i.e., the number of training tokens) or the number of proxy models. Therefore, we study which approach would yield better performance.
As shown in Figure~\ref{fig:1M-to-Test}, increasing the training tokens of the proxy models saturates after approximately 0.25B tokens.
In contrast, increasing the number of proxy models consistently enhances performance, particularly for the LightGBM model.
Notably, the performance of 512 models trained on 0.2B tokens surpasses that of 128 models trained on 0.8B tokens, indicating that increasing the number of proxy models is more effective than increasing the training token count beyond a certain token threshold. \looseness=-1