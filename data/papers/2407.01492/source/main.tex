\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_data_2024

% ready for submission
% \usepackage{neurips_data_2024}

% to compile a preprint version, add the [preprint] option, e.g.:
\usepackage[preprint]{neurips_data_2024}
% This will indicate that the work is currently under review.

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_data_2024}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_data_2024}

% Submissions to the datasets and benchmarks are typically non anonymous,
% but anonymous submissions are allowed. If you feel that you must submit 
% anonymously, you can compile an anonymous version by adding the [anonymous] 
% option, e.g.:
%     \usepackage[anonymous]{neurips_data_2024}
% This will hide all author names.

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors



\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}
\usepackage{xcolor}
\usepackage{colortbl}

\usepackage{graphicx} % Required for inserting images

%\usepackage{natbib}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{todonotes}

\usepackage{bbm}

\usepackage{xcolor}
\usepackage{subfigure}

\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{placeins}
\usepackage{xspace}

\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplots}
\pgfplotsset{width=1.0\columnwidth}

\usepackage{threeparttable}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}


\newcommand{\ourmethod}{\textsc{RegMix}\xspace}
\providecommand{\qian}[1]{{\protect\color{blue}{[Qian: #1]}}}
\providecommand{\niklas}[1]{{\protect\color{purple}{[Niklas: #1]}}}

\title{\ourmethod: Data Mixture as Regression for\\Language Model Pre-training}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
Qian Liu$^{1}$\thanks{The first two authors contributed equally.}\;, Xiaosen Zheng$^{2*}$, Niklas Muennighoff$^3$, Guangtao Zeng$^4$, Longxu Dou$^1$\\
\textbf{Tianyu Pang}$^1$, \textbf{Jing Jiang}$^2$, \textbf{Min Lin}$^1$ \\
    \textsuperscript{1}Sea AI Lab  \quad \textsuperscript{2}SMU \quad \textsuperscript{3}Contextual AI \quad \textsuperscript{4}SUTD \\
    \texttt{liuqian@sea.com; xszheng.2020@phdcs.smu.edu.sg}
}

\begin{document}

\maketitle

\begin{abstract}
The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose \ourmethod to automatically identify a high-performing data mixture by formulating it as a regression task. \ourmethod involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate \ourmethod, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. $1000 \times$ larger and $25 \times$ longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10\% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6\%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like \ourmethod are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at \url{https://github.com/sail-sg/regmix}.

\end{abstract}

\input{sections/00_Intro}
\input{sections/01_Related}
\input{sections/02_Method}
\input{sections/03_Evaluation}
\input{sections/04_Applications}
\input{sections/05_Limitation}

\bibliography{ms}
\bibliographystyle{plainnat}

\clearpage
\appendix
\input{sections/Appendix/00_Additional}
\input{sections/Appendix/01_Implementation}
\input{sections/Appendix/02_All_Model_Results}


\end{document}
