\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AI(2024)]{meta_llama_3_2024}
Meta AI.
\newblock Introducing meta llama 3: The most capable openly available llm to date.
\newblock \url{https://ai.meta.com/blog/meta-llama-3/}, April 2024.

\bibitem[Albalak et~al.(2023)Albalak, Pan, Raffel, and Wang]{albalak2023efficient}
Alon Albalak, Liangming Pan, Colin Raffel, and William~Yang Wang.
\newblock Efficient online data mixing for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2312.02406}, 2023.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang, Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock \emph{arXiv preprint arXiv:2402.16827}, 2024.

\bibitem[Biderman et~al.(2024)Biderman, Schoelkopf, Sutawika, Gao, Tow, Abbasi, Aji, Ammanamanchi, Black, Clive, et~al.]{biderman2024lessons}
Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham~Fikri Aji, Pawan~Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et~al.
\newblock Lessons from the trenches on reproducible evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2405.14782}, 2024.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, 2020.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, et~al.]{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock \emph{arXiv preprint arXiv:2204.06745}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3paper}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Bukharin and Zhao(2023)]{bukharin2023data}
Alexander Bukharin and Tuo Zhao.
\newblock Data diversity matters for robust instruction tuning.
\newblock \emph{arXiv preprint arXiv:2311.14736}, 2023.

\bibitem[Chen et~al.(2023)Chen, Roberts, Bhatia, Wang, Zhang, Sala, and R{\'e}]{chen2023skill}
Mayee~F Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce~Zhang, Frederic Sala, and Christopher R{\'e}.
\newblock Skill-it! a data-driven skills framework for understanding and training language models.
\newblock \emph{arXiv preprint arXiv:2307.14430}, 2023.

\bibitem[Chen et~al.(2024)Chen, Wang, Sow, Yang, Chen, Liang, Zhou, and Wang]{chen2024take}
Xuxi Chen, Zhendong Wang, Daouda Sow, Junjie Yang, Tianlong Chen, Yingbin Liang, Mingyuan Zhou, and Zhangyang Wang.
\newblock Take the bull by the horns: Hard sample-reweighted continual training improves llm generalization.
\newblock \emph{arXiv preprint arXiv:2402.14270}, 2024.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{gsm8k2021}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{CoRR}, abs/2110.14168, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Das and Khetan(2023)]{das2023deft}
Devleena Das and Vivek Khetan.
\newblock Deft: Data efficient fine-tuning for large language models via unsupervised core-set selection.
\newblock \emph{arXiv preprint arXiv:2310.16776}, 2023.

\bibitem[Dou et~al.(2024)Dou, Liu, Zeng, Guo, Zhou, Lu, and Lin]{dou2024sailor}
Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, and Min Lin.
\newblock Sailor: Open language models for south-east asia.
\newblock \emph{CoRR}, abs/2404.03608, 2024.
\newblock \doi{10.48550/ARXIV.2404.03608}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2404.03608}.

\bibitem[Engstrom et~al.(2024)Engstrom, Feldmann, and Madry]{engstrom2024dsdm}
Logan Engstrom, Axel Feldmann, and Aleksander Madry.
\newblock Dsdm: Model-aware dataset selection with datamodels.
\newblock \emph{arXiv preprint arXiv:2401.12926}, 2024.

\bibitem[Fan et~al.(2023)Fan, Pagliardini, and Jaggi]{fan2023doge}
Simin Fan, Matteo Pagliardini, and Martin Jaggi.
\newblock Doge: Domain reweighting with generalization estimation.
\newblock \emph{arXiv preprint arXiv:2310.15393}, 2023.

\bibitem[Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao, Mercat, Fang, Li, Keh, Xin, Nezhurina, Vasiljevic, Jitsev, Dimakis, Ilharco, Song, Kollar, Carmon, Dave, Heckel, Muennighoff, and Schmidt]{gadre2024language}
Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros~G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt.
\newblock Language models scale reliably with over-training and on downstream tasks.
\newblock \emph{CoRR}, abs/2403.08540, 2024.
\newblock \doi{10.48550/ARXIV.2403.08540}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2403.08540}.

\bibitem[Gao et~al.(2021)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{the_pile_corpus}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{CoRR}, abs/2101.00027, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.00027}.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Ge et~al.(2024)Ge, Ma, Chen, Li, and Ding]{ge2024data}
Ce~Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and Bolin Ding.
\newblock Data mixing made efficient: A bivariate scaling law for language model pretraining.
\newblock \emph{arXiv preprint arXiv:2405.14908}, 2024.

\bibitem[Goyal et~al.(2024)Goyal, Maini, Lipton, Raghunathan, and Kolter]{goyal2024scaling}
Sachin Goyal, Pratyush Maini, Zachary~C. Lipton, Aditi Raghunathan, and J.~Zico Kolter.
\newblock Scaling laws for data filtering - data curation cannot be compute agnostic.
\newblock \emph{CoRR}, abs/2404.07177, 2024.
\newblock \doi{10.48550/ARXIV.2404.07177}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2404.07177}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{MMLU2021}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Huang et~al.(2024)Huang, Zhang, Shan, and He]{huang2024compression}
Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He.
\newblock Compression represents intelligence linearly.
\newblock \emph{arXiv preprint arXiv:2404.09937}, 2024.

\bibitem[Kang et~al.(2024)Kang, Just, Sun, Jahagirdar, Zhang, Du, Sahu, and Jia]{kang2024get}
Feiyang Kang, Hoang~Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit~Kumar Sahu, and Ruoxi Jia.
\newblock Get more for less: Principled data selection for warming up fine-tuning in llms.
\newblock \emph{arXiv preprint arXiv:2405.02774}, 2024.

\bibitem[Ke et~al.(2017)Ke, Meng, Finley, Wang, Chen, Ma, Ye, and Liu]{ke2017lightgbm}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Khashabi et~al.(2018)Khashabi, Chaturvedi, Roth, Upadhyay, and Roth]{khashabi2018looking}
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.
\newblock Looking beyond the surface: A challenge set for reading comprehension over multiple sentences.
\newblock In \emph{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages 252--262, 2018.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, Allal, Li, Mou, Ferrandis, Jernite, Mitchell, Hughes, Wolf, Bahdanau, von Werra, and de~Vries]{thestack2022paper}
Denis Kocetkov, Raymond Li, Loubna~Ben Allal, Jia Li, Chenghao Mou, Carlos~Mu{\~{n}}oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de~Vries.
\newblock The stack: 3 {TB} of permissively licensed source code.
\newblock \emph{CoRR}, abs/2211.15533, 2022.
\newblock \doi{10.48550/ARXIV.2211.15533}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2211.15533}.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pages 1885--1894. PMLR, 2017.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock \emph{arXiv preprint arXiv:1704.04683}, 2017.

\bibitem[Lin et~al.(2024)Lin, Gou, Gong, Liu, Shen, Xu, Lin, Yang, Jiao, Duan, and Chen]{lin2024rho1}
Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen.
\newblock Rho-1: Not all tokens are what you need.
\newblock \emph{arXiv preprint arXiv:2404.07965}, 2024.

\bibitem[Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang]{liu2020logiqa}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.
\newblock Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.
\newblock \emph{arXiv preprint arXiv:2007.08124}, 2020.

\bibitem[Liu et~al.(2024)Liu, Zeng, He, Jiang, and He]{liu2023makes}
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
\newblock What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
\newblock In \emph{The International Conference on Learning Representations}, 2024.

\bibitem[Magnusson et~al.(2023)Magnusson, Bhagia, Hofmann, Soldaini, Jha, Tafjord, Schwenk, Walsh, Elazar, Lo, Groeneveld, Beltagy, Hajishirzi, Smith, Richardson, and Dodge]{paloma2023allen}
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya~Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan~Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz~Beltagy, Hannaneh Hajishirzi, Noah~A. Smith, Kyle Richardson, and Jesse Dodge.
\newblock Paloma: {A} benchmark for evaluating language model fit.
\newblock \emph{CoRR}, abs/2312.10523, 2023.
\newblock \doi{10.48550/ARXIV.2312.10523}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2312.10523}.

\bibitem[Marion et~al.(2023)Marion, Üstün, Pozzobon, Wang, Fadaee, and Hooker]{marion2023investigating}
Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker.
\newblock When less is more: Investigating data pruning for pretraining llms at scale, 2023.

\bibitem[Mehta et~al.(2024)Mehta, Sekhavat, Cao, Horton, Jin, Sun, Mirzadeh, Najibi, Belenko, Zatloukal, and Rastegari]{openelm2024}
Sachin Mehta, Mohammad~Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari.
\newblock Openelm: An efficient language model family with open training and inference framework.
\newblock \emph{CoRR}, abs/2404.14619, 2024.
\newblock \doi{10.48550/ARXIV.2404.14619}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2404.14619}.

\bibitem[Mekala et~al.(2024)Mekala, Nguyen, and Shang]{mekala2024smaller}
Dheeraj Mekala, Alex Nguyen, and Jingbo Shang.
\newblock Smaller language models are capable of selecting instruction-tuning training data for larger language models.
\newblock \emph{arXiv preprint arXiv:2402.10430}, 2024.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch, Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, et~al.]{mindermann2022prioritized}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot, Sebastian Farquhar, et~al.
\newblock Prioritized training on points that are learnable, worth learning, and not yet learnt.
\newblock In \emph{International Conference on Machine Learning}, pages 15630--15649. PMLR, 2022.

\bibitem[Muennighoff et~al.(2023)Muennighoff, Rush, Barak, Scao, Tazi, Piktus, Pyysalo, Wolf, and Raffel]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
\newblock Scaling data-constrained language models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=j5BuTrEj35}.

\bibitem[Pandey(2024)]{pandey2024gzip}
Rohan Pandey.
\newblock gzip predicts data-dependent scaling laws.
\newblock \emph{arXiv preprint arXiv:2405.16684}, 2024.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Penedo et~al.(2024)Penedo, Kydlíček, allal, Lozhkov, Mitchell, Raffel, Werra, and Wolf]{penedo2024finewebdatasetsdecantingweb}
Guilherme Penedo, Hynek Kydlíček, Loubna~Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro~Von Werra, and Thomas Wolf.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.17557}.

\bibitem[Que et~al.(2024)Que, Liu, Zhang, Zhang, Qu, Ma, Duan, Bai, Wang, Zhang, Tan, Fu, Su, Wang, Qu, and Zheng]{que2024dcpt}
Haoran Que, Jiaheng Liu, Ge~Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, Xu~Tan, Jie Fu, Wenbo Su, Jiamang Wang, Lin Qu, and Bo~Zheng.
\newblock D-cpt law: Domain-specific continual pre-training scaling law for large language models, 2024.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving]{rae2023gopher}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H.~Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po{-}Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean{-}Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy,
  Chris Jones, James Bradbury, Matthew~J. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis {\&} insights from training gopher.
\newblock \emph{CoRR}, abs/2112.11446, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{arXiv e-prints}, 2019.

\bibitem[Sachdeva et~al.(2024)Sachdeva, Coleman, Kang, Ni, Hong, Chi, Caverlee, McAuley, and Cheng]{sachdeva2024train}
Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed~H Chi, James Caverlee, Julian McAuley, and Derek~Zhiyuan Cheng.
\newblock How to train data-efficient llms.
\newblock \emph{arXiv preprint arXiv:2402.09668}, 2024.

\bibitem[Sachin~Parkar et~al.(2024)Sachin~Parkar, Kim, Inn~Park, and Kang]{sachin2024selectllm}
Ritik Sachin~Parkar, Jaehyung Kim, Jong Inn~Park, and Dongyeop Kang.
\newblock Selectllm: Can llms select important instructions to annotate?
\newblock \emph{arXiv e-prints}, pages arXiv--2401, 2024.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock {SocialIQA}: Commonsense reasoning about social interactions.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Sarlin et~al.(2020)Sarlin, DeTone, Malisiewicz, and Rabinovich]{sarlin2020superglue}
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.
\newblock Superglue: Learning feature matching with graph neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4938--4947, 2020.

\bibitem[Shao et~al.(2024)Shao, Li, Fei, Yan, Lin, and Qiu]{shao2024balanced}
Yunfan Shao, Linyang Li, Zhaoye Fei, Hang Yan, Dahua Lin, and Xipeng Qiu.
\newblock Balanced data sampling for language model training with clustering.
\newblock \emph{arXiv preprint arXiv:2402.14526}, 2024.

\bibitem[Sharma et~al.(2024)Sharma, Padthe, Ardalani, Tirumala, Howes, Xu, Huang, Li, Aghajanyan, and Ghosh]{sharma2024text}
Vasu Sharma, Karthik Padthe, Newsha Ardalani, Kushal Tirumala, Russell Howes, Hu~Xu, Po-Yao Huang, Shang-Wen Li, Armen Aghajanyan, and Gargi Ghosh.
\newblock Text quality-based pruning for efficient training of language models.
\newblock \emph{arXiv preprint arXiv:2405.01582}, 2024.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant]{talmor-etal-2019-commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {C}ommonsense{QA}: A question answering challenge targeting commonsense knowledge.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4149--4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1421}.
\newblock URL \url{https://aclanthology.org/N19-1421}.

\bibitem[Thakkar et~al.(2023)Thakkar, Bolukbasi, Ganapathy, Vashishth, Chandar, and Talukdar]{thakkar2023self}
Megh Thakkar, Tolga Bolukbasi, Sriram Ganapathy, Shikhar Vashishth, Sarath Chandar, and Partha Talukdar.
\newblock Self-influence guided data reweighting for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2311.00913}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2paper}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton{-}Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie{-}Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur{\'{e}}lien Rodriguez, Robert Stojnic, Sergey Edunov,
  and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}, abs/2307.09288, 2023.
\newblock \doi{10.48550/ARXIV.2307.09288}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.09288}.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Welbl et~al.(2017)Welbl, Liu, and Gardner]{welbl2017crowdsourcing}
Johannes Welbl, Nelson~F Liu, and Matt Gardner.
\newblock Crowdsourcing multiple choice science questions.
\newblock \emph{arXiv preprint arXiv:1707.06209}, 2017.

\bibitem[Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen]{alex2024qurating}
Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.
\newblock Qurating: Selecting high-quality data for training language models, 2024.

\bibitem[Xia et~al.(2022)Xia, Artetxe, Zhou, Lin, Pasunuru, Chen, Zettlemoyer, and Stoyanov]{xia2022training}
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi~Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov.
\newblock Training trajectories of language models across scales.
\newblock \emph{arXiv preprint arXiv:2212.09803}, 2022.

\bibitem[Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen]{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
\newblock Less: Selecting influential data for targeted instruction tuning.
\newblock \emph{arXiv preprint arXiv:2402.04333}, 2024.

\bibitem[Xie et~al.(2023{\natexlab{a}})Xie, Pham, Dong, Du, Liu, Lu, Liang, Le, Ma, and Yu]{xie2023doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc~V Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model pretraining.
\newblock \emph{arXiv preprint arXiv:2305.10429}, 2023{\natexlab{a}}.

\bibitem[Xie et~al.(2023{\natexlab{b}})Xie, Santurkar, Ma, and Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{arXiv preprint arXiv:2302.03169}, 2023{\natexlab{b}}.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Li, Niu, Du, Gao, Zhang, Chen, Qu, Yuan, Li, et~al.]{yang2024fine}
Chen Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, et~al.
\newblock The fine line: Navigating large language model pretraining with down-streaming capability analysis.
\newblock \emph{arXiv preprint arXiv:2404.01204}, 2024{\natexlab{a}}.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Mishra, Chiang, and Mirzasoleiman]{yang2024smalltolarge}
Yu~Yang, Siddhartha Mishra, Jeffrey~N Chiang, and Baharan Mirzasoleiman.
\newblock Smalltolarge (s2l): Scalable data selection for fine-tuning large language models by summarizing training trajectories of small models.
\newblock \emph{arXiv preprint arXiv:2403.07384}, 2024{\natexlab{b}}.

\bibitem[Ye et~al.(2024)Ye, Liu, Sun, Zhou, Zhan, and Qiu]{ye2024datamixing}
Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu.
\newblock Data mixing laws: Optimizing data mixtures by predicting language modeling performance.
\newblock \emph{CoRR}, abs/2403.16952, 2024.
\newblock \doi{10.48550/ARXIV.2403.16952}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2403.16952}.

\bibitem[Yu et~al.(2024)Yu, Das, and Xiong]{yu2024mates}
Zichun Yu, Spandan Das, and Chenyan Xiong.
\newblock Mates: Model-aware data selection for efficient pretraining with data influence models, 2024.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Zeng, Wang, and Lu]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
\newblock Tinyllama: An open-source small language model.
\newblock \emph{arXiv preprint arXiv:2401.02385}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Luo, Yuan, and Yao]{zhang2024autonomous}
Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew~C Yao.
\newblock Autonomous data selection with language models for mathematical texts.
\newblock In \emph{ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models}, 2024{\natexlab{b}}.

\end{thebibliography}
