\begin{thebibliography}{}

\bibitem[Abbas et~al., 2023]{abbas2023semdedup}
Abbas, A., Tirumala, K., Simig, D., Ganguli, S., and Morcos, A.~S. (2023).
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication.
\newblock {\em arXiv preprint arXiv:2303.09540}.

\bibitem[Achiam et~al., 2023]{achiam2023gpt}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al. (2023).
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}.

\bibitem[Biderman et~al., 2023]{biderman2023pythia}
Biderman, S., Schoelkopf, H., Anthony, Q.~G., Bradley, H., O’Brien, K., Hallahan, E., Khan, M.~A., Purohit, S., Prashanth, U.~S., Raff, E., et~al. (2023).
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In {\em International Conference on Machine Learning}, pages 2397--2430. PMLR.

\bibitem[Broder, 1997]{broder1997minhash}
Broder, A.~Z. (1997).
\newblock On the resemblance and containment of documents.
\newblock {\em Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)}.

\bibitem[Brown et~al., 2020]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al. (2020).
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901.

\bibitem[Elazar et~al., 2023]{elazar2023whats}
Elazar, Y., Bhagia, A., Magnusson, I., Ravichander, A., Schwenk, D., Suhr, A., Walsh, P., Groeneveld, D., Soldaini, L., Singh, S., Hajishirzi, H., Smith, N.~A., and Dodge, J. (2023).
\newblock What's in my big data?

\bibitem[Gao et~al., 2020]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al. (2020).
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}.

\bibitem[Glorioso et~al., 2024]{glorioso2024zamba}
Glorioso, P., Anthony, Q., Tokpanov, Y., Whittington, J., Pilault, J., Ibrahim, A., and Millidge, B. (2024).
\newblock Zamba: A compact 7b ssm hybrid model.

\bibitem[Hestness et~al., 2017]{hestness2017deep}
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M.~A., Yang, Y., and Zhou, Y. (2017).
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv preprint arXiv:1712.00409}.

\bibitem[Hoffmann et~al., 2022]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al. (2022).
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}.

\bibitem[Ilyas et~al., 2022]{ilyas2022datamodels}
Ilyas, A., Park, S.~M., Engstrom, L., Leclerc, G., and Madry, A. (2022).
\newblock Datamodels: Predicting predictions from training data.
\newblock {\em arXiv preprint arXiv:2202.00622}.

\bibitem[Jiang et~al., 2023]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al. (2023).
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}.

\bibitem[Kaplan et~al., 2020]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020).
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}.

\bibitem[Kenney, 2023]{arxiv-s2orc-parsed}
Kenney, M. (2023).
\newblock arxiv\_s2orc\_parsed.
\newblock https://huggingface.co/datasets/ArtifactAI/arxiv\_s2orc\_parsed.

\bibitem[Kudugunta et~al., 2024]{kudugunta2024madlad}
Kudugunta, S., Caswell, I., Zhang, B., Garcia, X., Xin, D., Kusupati, A., Stella, R., Bapna, A., and Firat, O. (2024).
\newblock Madlad-400: A multilingual and document-level large audited dataset.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Lee et~al., 2021]{lee2021deduplicating}
Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2021).
\newblock Deduplicating training data makes language models better.
\newblock {\em arXiv preprint arXiv:2107.06499}.

\bibitem[Li et~al., 2023]{li2023starcoder}
Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al. (2023).
\newblock Starcoder: may the source be with you!
\newblock {\em arXiv preprint arXiv:2305.06161}.

\bibitem[Maini et~al., 2024]{maini2024rephrasing}
Maini, P., Seto, S., Bai, H., Grangier, D., Zhang, Y., and Jaitly, N. (2024).
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock {\em arXiv preprint arXiv:2401.16380}.

\bibitem[Marion et~al., 2023]{marion2023less}
Marion, M., {\"U}st{\"u}n, A., Pozzobon, L., Wang, A., Fadaee, M., and Hooker, S. (2023).
\newblock When less is more: Investigating data pruning for pretraining llms at scale.
\newblock {\em arXiv preprint arXiv:2309.04564}.

\bibitem[MosaicML, 2024]{mosaic2024dbrx}
MosaicML (2024).
\newblock Introducing dbrx: A new state-of-the-art open llm.
\newblock {\em https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm}.

\bibitem[Penedo et~al., 2024]{penedo2024fineweb}
Penedo, G., Kydlíček, H., von Werra, L., and Wolf, T. (2024).
\newblock Fineweb.
\newblock https://huggingface.co/datasets/HuggingFaceFW/fineweb.

\bibitem[Penedo et~al., 2023]{penedo2023refinedweb}
Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. (2023).
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock {\em arXiv preprint arXiv:2306.01116}.

\bibitem[Radford et~al., 2019]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al. (2019).
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9.

\bibitem[Rae et~al., 2021]{rae2021scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et~al. (2021).
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}.

\bibitem[Raffel et~al., 2020]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J. (2020).
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67.

\bibitem[Sevilla et~al., 2022]{sevilla2022compute}
Sevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn, M., and Villalobos, P. (2022).
\newblock Compute trends across three eras of machine learning.
\newblock In {\em 2022 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8. IEEE.

\bibitem[Shoeybi et~al., 2019]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019).
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock {\em arXiv preprint arXiv:1909.08053}.

\bibitem[Soboleva et~al., 2023]{slimpajama}
Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J., Hestness, J., and Dey, N. (2023).
\newblock Slimpajama: A 627b token cleaned and deduplicated version of redpajama.

\bibitem[Soldaini et~al., 2024]{soldaini2024dolma}
Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., et~al. (2024).
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock {\em arXiv preprint arXiv:2402.00159}.

\bibitem[Soldaini and Lo, 2023]{peS2o}
Soldaini, L. and Lo, K. (2023).
\newblock {peS2o (Pretraining Efficiently on S2ORC) Dataset}.
\newblock Technical report, {Allen Institute for AI}.
\newblock ODC-By, \url{https://github.com/allenai/pes2o}.

\bibitem[Team et~al., 2023]{team2023gemini}
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al. (2023).
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}.

\bibitem[Team et~al., 2024]{team2024gemma}
Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi{\`e}re, M., Kale, M.~S., Love, J., et~al. (2024).
\newblock Gemma: Open models based on gemini research and technology.
\newblock {\em arXiv preprint arXiv:2403.08295}.

\bibitem[Tirumala et~al., 2024]{tirumala2024d4}
Tirumala, K., Simig, D., Aghajanyan, A., and Morcos, A. (2024).
\newblock D4: Improving llm pretraining via document de-duplication and diversification.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[TogetherAI, 2023]{redpajama}
TogetherAI (2023).
\newblock Redpajama-data-v2: An open dataset with 30 trillion tokens for training large language models.

\bibitem[Touvron et~al., 2023]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. (2023).
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Xie et~al., 2024]{xie2024doremi}
Xie, S.~M., Pham, H., Dong, X., Du, N., Liu, H., Lu, Y., Liang, P.~S., Le, Q.~V., Ma, T., and Yu, A.~W. (2024).
\newblock Doremi: Optimizing data mixtures speeds up language model pretraining.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Xie et~al., 2023]{xie2023data}
Xie, S.~M., Santurkar, S., Ma, T., and Liang, P.~S. (2023).
\newblock Data selection for language models via importance resampling.
\newblock {\em Advances in Neural Information Processing Systems}, 36:34201--34227.

\end{thebibliography}
