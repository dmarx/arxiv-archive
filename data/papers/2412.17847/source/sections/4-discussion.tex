\section{Discussion}
\label{sec:discussion}

\vspace{-2mm}
\paragraph{The rise of web-based, social media, and synthetic datasets may pose greater risks to privacy, copyright, and bias.}
\Cref{sec:data-sources} discusses the rise of web-based sources and particularly social media as primary sources for speech and video.
\Cref{fig:temporal-source-categories} shows these sources now exceed more traditional, curated sources such as movies, audiobooks, radio, TV, or content hand-crafted by human participants---by at least one order of magnitude.
These websites made of mostly user-generated content are a natural choice, given that they scale in the quantity, freshness, and heterogeneity that is best suited to train general-purpose models \citep{longpre2023pretrainers,aghajanyan2023scaling}.
However, prior work suggests that crowd-sourced, user-generated web content also introduces more challenges than curated content, particularly for privacy, copyright, bias, harm, and factuality.

Web-based and particularly user-generated content is disproportionately likely to include personally identifiable information (PII) \cite{Luccioni2021-iq,Subramani2023-gj,elazars}, and copyrighted content \citep{meese2019mundane,lee2023talkin}.
These can be reproduced in the outputs of AI models \citep{Carlini2022-nk,Chen2023-lw}, creating privacy and copyright concerns \citep{Zhang2023-ln}. 
Open datasets being used to train GPAI often attempt to filter---but frequently miss---PII and copyrighted data \citep{soldaini2024dolma,Subramani2023-gj} (although not all do \citep{Penedo2023-cr}). 
Social media, in particular, is also known to have bias, toxicity and factuality issues \citep{olteanu2019social}, which can manifest in trained models, even after alignment \citep{kotha2023understanding}.
Lastly, while synthetic data can help reduce the prevalence of PII, copyright, or bias in data, it comes with its own challenges \citep{Kurakin2023-ip,Liu2024-ga}.

\vspace{-2mm}
\paragraph{Social Media websites have become one of the most prominent data sources, but their Terms often restrict crawling or commercial use.}

We find that 71\% of Video data and 69\% of Speech data is from YouTube which has become a prominent source of data, given its scale, freshness, and multimodality (containing videos, speech, images, and text) \citep{abu2016youtube, NEURIPS2018_35309226, NEURIPS2020_2cd4e8a2, NEURIPS2023_5c61452d, coats2023dialect, li2023yodas}.
However, YouTube is a social media platform owned by Google and its Terms of Service\footnote{\href{https://www.youtube.com/static?template=terms}{YouTube Terms of Service}.} prohibit third parties from crawling YouTube.
While content creators maintain their ownership rights in the material they upload to YouTube, the YouTube Terms of Service also grant Google a license to reproduce, modify, display, and use the content for purposes connected to YouTube's ``business'', which may include building machine learning models; even if the copyright holder has selected a permissive license, YouTube's Terms   disallow external parties from crawling that data.
Model developers such as Nvidia and OpenAI have been sued in the U.S. by content creators who allege that they unlawfully trained on YouTube videos \citep{Cole2024, Skolnik2024}. 
Large social media platforms and forums have also adopted restrictive terms in recent years, including Reddit and StackOverflow.\footnote{\href{https://redditinc.com/policies/user-agreement-september-25-2023}{Reddit User Agreement} and \href{https://stackoverflow.com/legal/terms-of-service/public}{StackOverflow Terms of Service}.}
As these data sources become critical to scaling AI systems, access has been made exclusive, which may hamper academic, non-profit, or open source model development---to the extent that social media platforms can enforce their terms against third party developers.\footnote{We treat the enforceability of licenses and terms as an open legal question, beyond the scope of our work.}


\vspace{-2mm}
\paragraph{Ambiguous and poorly documented use restrictions may significantly inhibit model developers adhering to cautious legal and ethical data sourcing standards.}
In \Cref{sec:use-restrictions}. we find that a significant amount of data carry non-commercial restrictions in their sources, rather than on the final dataset, which can contain no license or a permissive one.
For text and video, these restrictions can equate to 99\% of all tokens and hours.
These inconsistencies are the result of datasets being iteratively re-packaged and re-licensed, without carrying on documentation \citep{longpre2023data}.
While not every developer will employ the same filtering standards, our work shows that the challenges to separate and identify appropriate datasets remain difficult across these modalities.
Without continued audits and documentation, practitioners may be forced to forego large collections of partially viable data, hampering data scaling laws \citep{kaplan2020scaling}, or take on avoidable risk.
We hope this released audit will provide greater tools for practitioners to apply their own standards, to make informed decisions on training data use.

\vspace{-2mm}
\paragraph{The limitations of measures of geographical and linguistic representation.}
It is important to note that measures of geographical and linguistic representation are imperfect. We are limited by partial information about the developers' identities (including for privacy reasons), limited transparency into how frequently these datasets are used, and the extent to which proprietary datasets may fill in representation gaps behind closed doors.
Nonetheless, we believe the breadth and rigour of the audit make this the best available empirical measure of representation in \emph{publicly} documented datasets.
Further, we propose the goal of measuring representation in AI data as essential to understanding progress, or its absence, towards AI systems that fairly serve the broader community of users.
\Cref{fig:creator-worldmaps} and \Cref{fig:representation} demonstrate that despite the absolute rise of geographical and linguistic representation, the relative western-centric concentration persists, across thousands of surveyed datasets.
We release all audit materials for transparency and replicability, and for further use by the research community.

\vspace{-2mm}
\paragraph{Conducting representative analyses of an ecosystem comes with assumptions.}
First, an ecosystem for AI is by nature, not centralized or organized.
Widely used datasets for Text are often hosted on Hugging Face, but this is frequently not the case for Speech or Video.
Similarly, while Text data undergoes frequent dataset re-packaging for general-purpose post-training, this is not true to the same extent for other modalities.
As such, the scope and dataset selection process need to be designed for each modality, rather than a single, simple protocol, which inevitably will not accurately represent one modality at its ecosystem-level.
Similarly, we chose a subset of modalities of interest to foundation model development \citep{sora2024, radford2023robust}, but note there are many other left for future work (e.g., images, 3D representations, tabular, time series, graphs, and geospatial data).