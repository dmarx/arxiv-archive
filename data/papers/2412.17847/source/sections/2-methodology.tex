\vspace{-2mm}
\section{Methodology}
\label{sec:methodology}

\input{tables/audit-stats}

\vspace{-2mm}
While many prior works have surveyed the dataset ecosystem \citep{albalak2024survey, liu2024datasets, malik2021automatic, prabhavalkar2023end, 8627985}, few empirically examine data corpora at scale, and those that do focus present a more narrow focus around a specific feature like geographic bias or hate content\citep{birhane2023into,mcmillan2022documenting,shankar2017no} or a single modality \citep{dodge2021documenting, caswell2021quality, elazars, longpre2023data}. The goal of this work is to provide an empirical, ecosystem-level, and multimodal analysis of widely used training datasets \citep{Cen2023}. Our audit focuses on text, speech, and video, as prominent data modalities behind modern multimodal systems, such as Sora, Whisper, Gemini, GPT-4o, and others \citep{sora2024, opensora, radford2023robust, peng2023reproducing, team2023gemini, openAIGPT4o2024}.
Since training data for modalities can often be independent, multimodal models tend to interleave training batches with different combinations of one or two modalities \citep{aghajanyan2023scaling}.
As such, we focus our analysis on datasets that represent one or a pair of these modalities.

\vspace{-2mm}
\paragraph{Annotation Features \& Methodology}
In particular, we analyze data trends for the state of data permissions (licenses and terms), sourcing (the web, human annotation, and synthetic generation), and representation (of tasks, organizations, languages, and countries).
We adopt \citet{longpre2023data}'s methodology, including the license annotation taxonomy and process, to manually audit these features precisely and rigorously.
We go beyond prior work, which considers dataset licenses, by extending the taxonomy to consider the terms of use of the sources of the dataset, either from models used to generate synthetic data (e.g. OpenAI's non-compete clause\footnote{\href{https://openai.com/policies/row-terms-of-use/}{OpenAI Terms of Use}} or Meta's acceptable use policy for Llama 3.1\footnote{\href{https://www.llama.com/llama3_1/use-policy/}{Llama 3.1 Acceptable Use Policy}}), or the source's policy on content restrictions, which can be conveyed in the form of a license, terms of use, or content policy on a website \citep{klyman2024acceptableusepoliciesfoundation}.
For each dataset, the source terms are annotated as Unrestricted, Unspecified, Source Closed or Model Closed, as defined in \Cref{tab:terms-taxonomy}.
For \Cref{fig:license-terms} we combine Source Closed and Model Closed into \emph{Restricted}.

As with prior work \citep{longpre2023data,longpre2024consent}, we engage domain experts for these annotation tasks---AI researchers whose work pertains to the modality and topic.
Because many datasets are iteratively re-packaged before they appear in their final form and often shared on popular dataset marketplaces like HuggingFace, Papers with Code or Github, prior work has found that relevant licensing terms or sourcing information for AI training data is frequently omitted \citep{longpre2023data}.
To ensure we collect this information, we require a full trace of metadata back to their original sources (sometimes a chain of github repositories, websites, or academic papers).
This search can be onerous, especially for terms and licenses, but ensures rigor in the results.
\Cref{tab:audit-stats} enumerates the full statistics of our audit.
All annotations and analysis code will be made publicly available on release.

\vspace{-2mm}
\paragraph{Scope \& Dataset Selection}
For each modality, we define the scope of the audit (detailed separately below), then aggregate resources to distill a list of relevant datasets.
The scope is focused on (a) publicly available datasets, (b) widely used tasks in the context of general-purpose model development, and (c) relevance to generative tasks.
However, we do consider classification-based datasets in text, speech, and video that can and are frequently re-purposed for generative uses (e.g. instruction tuning).
% Dataset Selection
Within the defined audit scope, we use a mix of the HuggingFace Datasets platform, survey papers, survey repositories, workshop proceedings, and expert review to accumulate relevant datasets. More detail about the dataset selection and collection process is given for each modality below.
Each modality requires its own independent process, by virtue of their community dataset ecosystems being unique (discussed in \Cref{sec:discussion}).
Note that text has a wider heterogeneity of published publicly available datasets than speech or video.
Typically those datasets have been aggregated into large, standardized text-to-text collections, and as such we trace both these \emph{Text (Collections)} and their constituent \emph{Text (Datasets)}.
All datasets are described, linked, and attributed in \Cref{app:datasets}.

\vspace{-2mm}
\subsection{Text}

\vspace{-2mm}
\paragraph{Scope} We focus on providing an extensive audit for
\emph{post-training} datasets, used in training language models. We include single and multi-turn formats, encompassing both datasets typically used for instruction finetuning (SFT) and preference alignment \cite{rafailov2023direct}. This scope reflects the prominent role of general-purpose language models, which benefit from multi-task training on heterogeneous collections that span a variety of linguistic, reasoning, and knowledge intensive tasks like question answering, coding, tool use, translation, and classification \citep{weifinetuned, ouyang2022training}.

\vspace{-2mm}
\paragraph{Dataset Selection}
We expand the study conducted by the Data Provenance Collection \citep{longpre2023data}, from $44$ dataset collections (of $1858$ supervised text datasets) to a superset of $108$ collections of $3717$ datasets, prioritizing recent, popular publicly available HuggingFace Datasets introduced between 2022 and April 2024.
Our collection sourced popular datasets from recent survey papers \citep{albalak2024survey, liu2024datasets} and tools \citep{longpre2024responsible}. We additionally reviewed HuggingFace Datasets' most downloaded datasets every month, from April to July 2024, under the Natural Language Processing category, as well as the SFT/DPO datasets associated with popular open model releases.
We also drew from major multilingual data repositories, including the SEACrowd Catalogue \citep{lovenia2024seacrowd}, the Masader Arabic Data Catalogue \citep{alyafeai2022masader}, AI4Bharat \citep{kunchukuttan2020ai4bharat}, and the Aya Collection \citep{singh2024aya}.
% https://ai4bharat.iitm.ac.in/
% https://arbml.github.io/masader/
% https://seacrowd.github.io/seacrowd-catalogue/
% https://cohere.com/research/aya
Lastly, our list of datasets was reviewed and supplemented by language model experts to fill in notable omissions.
In total, we trace the provenance and features of 3713 text datasets from 108 collections, covering 395 popular tasks, spanning from 1994 to 2024.

\vspace{-2mm}
\subsection{Speech}

\vspace{-2mm}
\paragraph{Scope}
We audit speech datasets for which automatic speech recognition (ASR) was noted as a primary task. We focus on ASR datasets because: (1) ASR is fundamental to many speech technologies, including dictation tools, voice assistants, and chatbots~\citep{aksenova-etal-2021-might, Zhang_2022}; (2) large-scale speech datasets are typically designed for ASR~\citep{li2023yodas}; (3) ASR data follows standardized formats, making comparisons easier (e.g., corpus of audio clips paired with text); and (4) ASR data can often be reused for other tasks like text to speech (TTS)~\citep{itoLJSpeechDataset2017} or language identification~\citep{ardila-etal-2020-common}.

\vspace{-2mm}
\paragraph{Dataset Selection}
To curate a representative sample of popular ASR datasets, we relied on a combination of survey repositories\footnote{\href{https://github.com/RevoSpeechTech/speech-datasets-collection}{The Speech Datasets Collection}}, and HuggingFace Datasets using the ``Automatic Speech Recognition'' and ``Text-to-Speech'' task tags.
We expanded coverage to well-documented datasets on the OpenSLR\footnote{\href{https://openslr.org/}{openslr.org}: Open Speech and Language Resources. OpenSLR is a widely used platform in the speech community, dedicated to hosting resources for speech tasks.} platform, even if they were newer or less widely used. We expect this might reflect datasets that could be adopted more widely in the future.
Finally, we included datasets related to low-resource languages and other languages not well-covered by our initial searches. Speech recognition models are increasingly highly multilingual~\cite{babu2021xls,radford2023robust,pratap2024scaling}, and datasets serving different communities of builders and end-users around the world are a priority for making speech recognition technologies more inclusive.
In total, we trace the provenance and features of 95 speech datasets, covering 18 popular ASR tasks, spanning from 1990 to 2024.

\vspace{-2mm}
\subsection{Video} 
\vspace{-2mm}
\paragraph{Scope}
Early video understanding models primarily focused on video classification, detection and action recognition, where short clips were categorized into predefined classes \citep{video_obj_survey,zhu2020comprehensivestudydeepvideo}. More advanced tasks such as temporal action segmentation, video question answering, and video captioning were later introduced to build upon these foundational tasks \citep{moctezuma2022videocaptioningcomparativereview, zhu2023deeplearningvideotextretrieval}. Recently, following the success in the field of image generation, video generation from text has become a new task that has shown promising results \citep{sora2024,opensora, blattmann2023stablevideodiffusionscaling, esser2023structurecontentguidedvideosynthesis}. Given the scarcity of datasets for text-to-video and the often undocumented sources of data used in recent video generation models \citep{MauranOpenAI}, we take a broader approach to our collection of video datasets. We focus on annotating popular video tasks and limit our scope to datasets corresponding to video tasks that are either published, highly cited, or have 100+ downloads on HuggingFace. This approach is justified by three key factors: (1) the usefulness of video data to the research community stems from its collection and presentation in peer-reviewed work, (2) datasets can often be repurposed between different tasks, allowing for applicability to new tasks such as video generation from text, and (3) focusing on highly cited datasets ensures that datasets' quality and relevance has been validated by the research community.

\vspace{-2mm}
\paragraph{Dataset Selection} We include datasets tagged with ``Video Classification'', ``Text-to-Video'', and ``Video-Text-to-Text'' from HuggingFace Datasets.
We augmented this with datasets tagged by \href{https://paperswithcode.com/task/video-understanding}{``Video Understanding''} or \href{https://paperswithcode.com/task/video-generation}{``Video Generation''} in PapersWithCode, as well as datasets listed in a popular \href{https://github.com/xiaobai1217/Awesome-Video-Datasets}{Github survey repository}.
We also consulted the proceedings of recent video workshops: the \href{https://holistic-video-understanding.github.io/}{Large Scale Video Understanding} and \href{https://egovis.github.io/cvpr24/}{Egocentric Vision} workshops.
We separately consulted a committee of non-author video experts to supplement the list with relevant datasets published at CVPR, ICCV, ECCV, and IJCV. 
In total, we trace the provenance and features of 104 video datasets, covering 33 popular video tasks, spanning from 2009 to 2024.