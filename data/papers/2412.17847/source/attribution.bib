
@inproceedings{hernandezmenaSamromurChildrenIcelandic2022,
	location = {Marseille, France},
	title = {Samrómur Children: An Icelandic Speech Corpus},
	url = {https://aclanthology.org/2022.lrec-1.105},
	shorttitle = {Samrómur Children},
	abstract = {Samromur Children is an Icelandic speech corpus intended for the field of automatic speech recognition. It contains 131 hours of read speech from Icelandic children aged between 4 to 17 years. The test portion was meticulously selected to cover a wide range of ages as possible; we aimed to have exactly the same amount of data per age range. The speech was collected with the crowd-sourcing platform Samrómur.is, which is inspired on the “Mozilla's Common Voice Project”. The corpus was developed within the framework of the “Language Technology Programme for Icelandic 2019 − 2023”; the goal of the project is to make Icelandic available in language-technology applications. Samromur Children is the first corpus in Icelandic with children's voices for public use under a Creative Commons license. Additionally, we present baseline experiments and results using Kaldi.},
	eventtitle = {{LREC} 2022},
	pages = {995--1002},
	booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Hernandez Mena, Carlos Daniel and Mollberg, David Erik and Borsky, Michal and Gudnason, Jon},
	editor = {Calzolari, Nicoletta and Bechet, Frederic and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-01},
	date = {2022-06},
}

@inproceedings{mollbergSamromurCrowdsourcingData2020,
	location = {Marseille, France},
	title = {Samrómur: Crowd-sourcing Data Collection for Icelandic Speech Recognition},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.425},
	shorttitle = {Samrómur},
	abstract = {This contribution describes an ongoing project of speech data collection, using the web application Samrómur which is built upon Common Voice, Mozilla Foundation's web platform for open-source voice collection. The goal of the project is to build a large-scale speech corpus for Automatic Speech Recognition ({ASR}) for Icelandic. Upon completion, Samrómur will be the largest open speech corpus for Icelandic collected from the public domain. We discuss the methods used for the crowd-sourcing effort and show the importance of marketing and good media coverage when launching a crowd-sourcing campaign. Preliminary results exceed our expectations, and in one month we collected data that we had estimated would take three months to obtain. Furthermore, our initial dataset of around 45 thousand utterances has good demographic coverage, is gender-balanced and with proper age distribution. We also report on the task of validating the recordings, which we have not promoted, but have had numerous hours invested by volunteers.},
	eventtitle = {{LREC} 2020},
	pages = {3463--3467},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Mollberg, David Erik and Jonsson, Olafur Helgi and Thorsteinsdottir, Sunneva and Steingremsson, Steinthor and Magnusdottir, Eydis Huld and Gudnason, Jon},
	editor = {Calzolari, Nicoletta and Bechet, Frederic and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-01},
	date = {2020-05},
}

@misc{boitoMaSSLargeClean2020,
	title = {{MaSS}: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken Utterances Extracted from the Bible},
	url = {http://arxiv.org/abs/1907.12895},
	shorttitle = {{MaSS}},
	abstract = {The {CMU} Wilderness Multilingual Speech Dataset (Black, 2019) is a newly published multilingual speech dataset based on recorded readings of the New Testament. It provides data to build Automatic Speech Recognition ({ASR}) and Text-to-Speech ({TTS}) models for potentially 700 languages. However, the fact that the source content (the Bible) is the same for all the languages is not exploited to date.Therefore, this article proposes to add multilingual links between speech segments in different languages, and shares a large and clean dataset of 8,130 parallel spoken utterances across 8 languages (56 language pairs). We name this corpus {MaSS} (Multilingual corpus of Sentence-aligned Spoken utterances). The covered languages (Basque, English, Finnish, French, Hungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech alignment as well as on translation for typologically different language pairs. The quality of the final corpus is attested by human evaluation performed on a corpus subset (100 utterances, 8 language pairs). Lastly, we showcase the usefulness of the final product on a bilingual speech retrieval task.},
	number = {{arXiv}:1907.12895},
	publisher = {{arXiv}},
	author = {Boito, Marcely Zanon and Havard, William N. and Garnerin, Mahault and Ferrand, Eric Le and Besacier, Laurent},
	urldate = {2024-05-29},
	date = {2020-02-26},
	eprinttype = {arxiv},
	eprint = {1907.12895 [cs]},
}

@misc{korzinekPolishReadSpeech2017,
	title = {Polish Read Speech Corpus for Speech Tools and Services},
	url = {http://arxiv.org/abs/1706.00245},
	abstract = {This paper describes the speech processing activities conducted at the Polish consortium of the {CLARIN} project. The purpose of this segment of the project was to develop specific tools that would allow for automatic and semi-automatic processing of large quantities of acoustic speech data. The tools include the following: grapheme-to-phoneme conversion, speech-to-text alignment, voice activity detection, speaker diarization, keyword spotting and automatic speech transcription. Furthermore, in order to develop these tools, a large high-quality studio speech corpus was recorded and released under an open license, to encourage development in the area of Polish speech research. Another purpose of the corpus was to serve as a reference for studies in phonetics and pronunciation. All the tools and resources were released on the the Polish {CLARIN} website. This paper discusses the current status and future plans for the project.},
	number = {{arXiv}:1706.00245},
	publisher = {{arXiv}},
	author = {Korzinek, Danijel and Marasek, Krzysztof and Brocki, Lukasz and Wolk, Krzysztof},
	urldate = {2024-05-29},
	date = {2017-06-01},
	eprinttype = {arxiv},
	eprint = {1706.00245 [cs]},
}

@article{bravoExtractionRelationsGenes2015,
	title = {Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research},
	volume = {16},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-015-0472-9},
	doi = {10/f7kn8s},
	shorttitle = {Extraction of relations between genes and diseases from text and large-scale data analysis},
	abstract = {Current biomedical research needs to leverage and exploit the large amount of information reported in scientific publications. Automated text mining approaches, in particular those aimed at finding relationships between entities, are key for identification of actionable knowledge from free text repositories. We present the {BeFree} system aimed at identifying relationships between biomedical entities with a special focus on genes and their associated diseases.},
	pages = {55},
	number = {1},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Bravo, Alex and Pinero, Janet and Queralt-Rosinach, Nuria and Rautschka, Michael and Furlong, Laura I.},
	urldate = {2024-10-02},
	date = {2015-02-21},
}

@misc{mrksicNeuralBeliefTracker2017,
	title = {Neural Belief Tracker: Data-Driven Dialogue State Tracking},
	url = {http://arxiv.org/abs/1606.03777},
	shorttitle = {Neural Belief Tracker},
	abstract = {One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking ({NBT}) framework which overcomes these problems by building on recent advances in representation learning. {NBT} models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.},
	number = {{arXiv}:1606.03777},
	publisher = {{arXiv}},
	author = {Mrksic, Nikola and Seaghdha, Diarmuid O and Wen, Tsung-Hsien and Thomson, Blaise and Young, Steve},
	urldate = {2024-05-01},
	date = {2017-04-21},
	eprinttype = {arxiv},
	eprint = {1606.03777 [cs]},
}

@misc{lightmanLetsVerifyStep2023,
	title = {Let's Verify Step by Step},
	url = {http://arxiv.org/abs/2305.20050},
	abstract = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging {MATH} dataset. Our process-supervised model solves 78\% of problems from a representative subset of the {MATH} test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release {PRM}800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
	number = {{arXiv}:2305.20050},
	publisher = {{arXiv}},
	author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
	urldate = {2024-05-01},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2305.20050 [cs]},
}

@misc{chenSemanticallyConditionedDialog2019,
	title = {Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention},
	url = {http://arxiv.org/abs/1905.12866},
	abstract = {Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-{WOZ} dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.},
	number = {{arXiv}:1905.12866},
	publisher = {{arXiv}},
	author = {Chen, Wenhu and Chen, Jianshu and Qin, Pengda and Yan, Xifeng and Wang, William Yang},
	urldate = {2024-05-01},
	date = {2019-06-09},
	eprinttype = {arxiv},
	eprint = {1905.12866 [cs]},
}

@misc{cobbeTrainingVerifiersSolve2021,
	title = {Training Verifiers to Solve Math Word Problems},
	url = {http://arxiv.org/abs/2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce {GSM}8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on {GSM}8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	number = {{arXiv}:2110.14168},
	publisher = {{arXiv}},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	urldate = {2024-05-29},
	date = {2021-11-17},
	eprinttype = {arxiv},
	eprint = {2110.14168 [cs]},
}

@misc{linTruthfulQAMeasuringHow2022,
	title = {{TruthfulQA}: Measuring How Models Mimic Human Falsehoods},
	url = {http://arxiv.org/abs/2109.07958},
	shorttitle = {{TruthfulQA}},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested {GPT}-3, {GPT}-Neo/J, {GPT}-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other {NLP} tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	number = {{arXiv}:2109.07958},
	publisher = {{arXiv}},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	urldate = {2024-05-29},
	date = {2022-05-07},
	eprinttype = {arxiv},
	eprint = {2109.07958 [cs]},
}

@misc{tandonWIQADatasetWhat2019,
	title = {{WIQA}: A dataset for "What if..." reasoning over procedural text},
	url = {http://arxiv.org/abs/1909.04739},
	shorttitle = {{WIQA}},
	abstract = {We introduce {WIQA}, the first large-scale dataset of "What if..." questions over procedural text. {WIQA} contains three parts: a collection of paragraphs each describing a process, e.g., beach erosion; a set of crowdsourced influence graphs for each paragraph, describing how one change affects another; and a large (40k) collection of "What if...?" multiple-choice questions derived from the graphs. For example, given a paragraph about beach erosion, would stormy weather result in more or less erosion (or have no effect)? The task is to answer the questions, given their associated paragraph. {WIQA} contains three kinds of questions: perturbations to steps mentioned in the paragraph; external (out-of-paragraph) perturbations requiring commonsense knowledge; and irrelevant (no effect) perturbations. We find that state-of-the-art models achieve 73.8\% accuracy, well below the human performance of 96.3\%. We analyze the challenges, in particular tracking chains of influences, and present the dataset as an open challenge to the community.},
	number = {{arXiv}:1909.04739},
	publisher = {{arXiv}},
	author = {Tandon, Niket and Mishra, Bhavana Dalvi and Sakaguchi, Keisuke and Bosselut, Antoine and Clark, Peter},
	urldate = {2024-05-29},
	date = {2019-09-10},
	eprinttype = {arxiv},
	eprint = {1909.04739 [cs]},
}

@misc{sakaguchiWinoGrandeAdversarialWinograd2019,
	title = {{WinoGrande}: An Adversarial Winograd Schema Challenge at Scale},
	url = {http://arxiv.org/abs/1907.10641},
	shorttitle = {{WinoGrande}},
	abstract = {The Winograd Schema Challenge ({WSC}) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of {WSC}. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce {WinoGrande}, a large-scale dataset of 44k problems, inspired by the original {WSC} design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel {AfLite} algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on {WinoGrande} achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - {WSC} (90.1\%), {DPR} (93.1\%), {COPA} (90.6\%), {KnowRef} (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of {WinoGrande} when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
	number = {{arXiv}:1907.10641},
	publisher = {{arXiv}},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2019-11-21},
	eprinttype = {arxiv},
	eprint = {1907.10641 [cs]},
}

@misc{xiongTWEETQASocialMedia2019,
	title = {{TWEETQA}: A Social Media Focused Question Answering Dataset},
	url = {http://arxiv.org/abs/1907.06292},
	shorttitle = {{TWEETQA}},
	abstract = {With social media becoming increasingly pop-ular on which lots of news and real-time eventsare reported, developing automated questionanswering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets haveconcentrated on question answering ({QA}) forformal text like news and Wikipedia, wepresent the first large-scale dataset for {QA} oversocial media data. To ensure that the tweetswe collected are useful, we only gather tweetsused by journalists to write news articles. Wethen ask human annotators to write questionsand answers upon these tweets. Unlike {otherQA} datasets like {SQuAD} in which the answersare extractive, we allow the answers to be ab-stractive. We show that two recently proposedneural models that perform well on formaltexts are limited in their performance when ap-plied to our dataset. In addition, even the fine-tuned {BERT} model is still lagging behind hu-man performance with a large margin. Our re-sults thus point to the need of improved {QAsystems} targeting social media text.},
	number = {{arXiv}:1907.06292},
	publisher = {{arXiv}},
	author = {Xiong, Wenhan and Wu, Jiawei and Wang, Hong and Kulkarni, Vivek and Yu, Mo and Chang, Shiyu and Guo, Xiaoxiao and Wang, William Yang},
	urldate = {2024-05-29},
	date = {2019-07-14},
	eprinttype = {arxiv},
	eprint = {1907.06292 [cs]},
}

@misc{clarkThinkYouHave2018,
	title = {Think you have Solved Question Answering? Try {ARC}, the {AI}2 Reasoning Challenge},
	url = {http://arxiv.org/abs/1803.05457},
	shorttitle = {Think you have Solved Question Answering?},
	abstract = {We present a new question set, text corpus, and baselines assembled to encourage {AI} research in advanced question answering. Together, these constitute the {AI}2 Reasoning Challenge ({ARC}), which requires far more powerful knowledge and reasoning than previous challenges such as {SQuAD} or {SNLI}. The {ARC} question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the {SQuAD} and {SNLI} tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the {ARC} Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose {ARC} as a challenge to the community.},
	number = {{arXiv}:1803.05457},
	publisher = {{arXiv}},
	author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
	urldate = {2024-05-29},
	date = {2018-03-14},
	eprinttype = {arxiv},
	eprint = {1803.05457 [cs]},
}

@misc{novikovaE2EDatasetNew2017,
	title = {The E2E Dataset: New Challenges For End-to-End Generation},
	url = {http://arxiv.org/abs/1706.09254},
	shorttitle = {The E2E Dataset},
	abstract = {This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.},
	number = {{arXiv}:1706.09254},
	publisher = {{arXiv}},
	author = {Novikova, Jekaterina and Dušek, Ondřej and Rieser, Verena},
	urldate = {2024-05-29},
	date = {2017-07-06},
	eprinttype = {arxiv},
	eprint = {1706.09254 [cs]},
}

@misc{joshiTriviaQALargeScale2017,
	title = {{TriviaQA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
	url = {http://arxiv.org/abs/1705.03551},
	shorttitle = {{TriviaQA}},
	abstract = {We present {TriviaQA}, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. {TriviaQA} includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, {TriviaQA} (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on {SQuAD} reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that {TriviaQA} is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/},
	number = {{arXiv}:1705.03551},
	publisher = {{arXiv}},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
	urldate = {2024-05-29},
	date = {2017-05-13},
	eprinttype = {arxiv},
	eprint = {1705.03551 [cs]},
}

@misc{westonAICompleteQuestionAnswering2015,
	title = {Towards {AI}-Complete Question Answering: A Set of Prerequisite Toy Tasks},
	url = {http://arxiv.org/abs/1502.05698},
	shorttitle = {Towards {AI}-Complete Question Answering},
	abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
	number = {{arXiv}:1502.05698},
	publisher = {{arXiv}},
	author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merriënboer, Bart and Joulin, Armand and Mikolov, Tomas},
	urldate = {2024-05-29},
	date = {2015-12-31},
	eprinttype = {arxiv},
	eprint = {1502.05698 [cs, stat]},
}

@misc{zhuXLCoSTBenchmarkDataset2022,
	title = {{XLCoST}: A Benchmark Dataset for Cross-lingual Code Intelligence},
	url = {http://arxiv.org/abs/2206.08474},
	shorttitle = {{XLCoST}},
	abstract = {Recent advances in machine learning have significantly improved the understanding of source code data and achieved good performance on a number of downstream tasks. Open source repositories like {GitHub} enable this process with rich unlabeled code data. However, the lack of high quality labeled data has largely hindered the progress of several code related tasks, such as program translation, summarization, synthesis, and code search. This paper introduces {XLCoST}, Cross-Lingual Code {SnippeT} dataset, a new benchmark dataset for cross-lingual code intelligence. Our dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-lingual code tasks. To the best of our knowledge, it is the largest parallel dataset for source code both in terms of size and the number of languages. We also provide the performance of several state-of-the-art baseline models for each task. We believe this new dataset can be a valuable asset for the research community and facilitate the development and validation of new methods for cross-lingual code intelligence.},
	number = {{arXiv}:2206.08474},
	publisher = {{arXiv}},
	author = {Zhu, Ming and Jain, Aneesh and Suresh, Karthik and Ravindran, Roshan and Tipirneni, Sindhu and Reddy, Chandan K.},
	urldate = {2024-05-29},
	date = {2022-06-16},
	eprinttype = {arxiv},
	eprint = {2206.08474 [cs]},
}

@misc{rashkinEmpatheticOpendomainConversation2019,
	title = {Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset},
	url = {http://arxiv.org/abs/1811.00207},
	shorttitle = {Towards Empathetic Open-domain Conversation Models},
	abstract = {One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others' feelings in a conversation, this is a significant challenge for {AI} systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and {EmpatheticDialogues}, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model.},
	number = {{arXiv}:1811.00207},
	publisher = {{arXiv}},
	author = {Rashkin, Hannah and Smith, Eric Michael and Li, Margaret and Boureau, Y.-Lan},
	urldate = {2024-05-01},
	date = {2019-08-28},
	eprinttype = {arxiv},
	eprint = {1811.00207 [cs]},
}

@misc{byrneTaskmaster1RealisticDiverse2019,
	title = {Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset},
	url = {http://arxiv.org/abs/1909.05358},
	shorttitle = {Taskmaster-1},
	abstract = {A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken "Wizard of Oz" ({WOz}) approach in which trained agents and crowdsourced workers interact to complete the task while the second is "self-dialog" in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with {API} calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider {API} allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.},
	number = {{arXiv}:1909.05358},
	publisher = {{arXiv}},
	author = {Byrne, Bill and Krishnamoorthi, Karthik and Sankar, Chinnadhurai and Neelakantan, Arvind and Duckworth, Daniel and Yavuz, Semih and Goodrich, Ben and Dubey, Amit and Cedilnik, Andy and Kim, Kyu-Young},
	urldate = {2024-05-01},
	date = {2019-09-01},
	eprinttype = {arxiv},
	eprint = {1909.05358 [cs]},
}

@misc{rastogiScalableMultidomainConversational2020,
	title = {Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset},
	url = {http://arxiv.org/abs/1909.05855},
	shorttitle = {Towards Scalable Multi-domain Conversational Agents},
	abstract = {Virtual assistants such as Google Assistant, Alexa and Siri provide a conversational interface to a large number of services and {APIs} spanning multiple domains. Such systems need to support an ever-increasing number of services with possibly overlapping functionality. Furthermore, some of these services have little to no training data available. Existing public datasets for task-oriented dialogue do not sufficiently capture these challenges since they cover few domains and assume a single static ontology per domain. In this work, we introduce the the Schema-Guided Dialogue ({SGD}) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation. Along the same lines, we present a schema-guided paradigm for task-oriented dialogue, in which predictions are made over a dynamic set of intents and slots, provided as input, using their natural language descriptions. This allows a single dialogue system to easily support a large number of services and facilitates simple integration of new services without requiring additional training data. Building upon the proposed paradigm, we release a model for dialogue state tracking capable of zero-shot generalization to new {APIs}, while remaining competitive in the regular setting.},
	number = {{arXiv}:1909.05855},
	publisher = {{arXiv}},
	author = {Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
	urldate = {2024-05-01},
	date = {2020-01-29},
	eprinttype = {arxiv},
	eprint = {1909.05855 [cs]},
}

@misc{baiTrainingHelpfulHarmless2022,
	title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
	url = {http://arxiv.org/abs/2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback ({RLHF}) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all {NLP} evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and {RL} policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of {RLHF} training, and identify a roughly linear relation between the {RL} reward and the square root of the {KL} divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of {OOD} detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	number = {{arXiv}:2204.05862},
	publisher = {{arXiv}},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and {DasSarma}, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and {McCandlish}, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	urldate = {2024-05-01},
	date = {2022-04-12},
	eprinttype = {arxiv},
	eprint = {2204.05862 [cs]},
}

@misc{boratkoProtoQAQuestionAnswering2020,
	title = {{ProtoQA}: A Question Answering Dataset for Prototypical Common-Sense Reasoning},
	url = {http://arxiv.org/abs/2005.00771},
	shorttitle = {{ProtoQA}},
	abstract = {Given questions regarding some prototypical situation such as Name something that people usually do before they leave the house for work? a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international game show {FAMILY}- {FEUD}. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.},
	number = {{arXiv}:2005.00771},
	publisher = {{arXiv}},
	author = {Boratko, Michael and Li, Xiang Lorraine and Das, Rajarshi and O'Gorman, Tim and Le, Dan and {McCallum}, Andrew},
	urldate = {2024-05-29},
	date = {2020-10-27},
	eprinttype = {arxiv},
	eprint = {2005.00771 [cs]},
}

@misc{tafjordQuaRTzOpenDomainDataset2019,
	title = {{QuaRTz}: An Open-Domain Dataset of Qualitative Relationship Questions},
	url = {http://arxiv.org/abs/1909.03553},
	shorttitle = {{QuaRTz}},
	abstract = {We introduce the first open-domain dataset, called {QuaRTz}, for reasoning about textual qualitative relationships. {QuaRTz} contains general qualitative statements, e.g., "A sunscreen with a higher {SPF} protects the skin longer.", twinned with 3864 crowdsourced situated questions, e.g., "Billy is wearing sunscreen with a lower {SPF} than Lucy. Who will be best protected from the sun?", plus annotations of the properties being compared. Unlike previous datasets, the general knowledge is textual and not tied to a fixed set of relationships, and tests a system's ability to comprehend and apply textual qualitative knowledge in a novel setting. We find state-of-the-art results are substantially (20\%) below human performance, presenting an open challenge to the {NLP} community.},
	number = {{arXiv}:1909.03553},
	publisher = {{arXiv}},
	author = {Tafjord, Oyvind and Gardner, Matt and Lin, Kevin and Clark, Peter},
	urldate = {2024-05-29},
	date = {2019-09-08},
	eprinttype = {arxiv},
	eprint = {1909.03553 [cs]},
}

@misc{khotQASCDatasetQuestion2020,
	title = {{QASC}: A Dataset for Question Answering via Sentence Composition},
	url = {http://arxiv.org/abs/1910.11473},
	shorttitle = {{QASC}},
	abstract = {Composing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition({QASC}), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. {QASC} is the first dataset to offer two desirable properties: (a) the facts to be composed are annotated in a large corpus, and (b) the decomposition into these facts is not evident from the question itself. The latter makes retrieval challenging as the system must introduce new concepts or relations in order to discover potential decompositions. Further, the reasoning model must then learn to identify valid compositions of these retrieved facts using common-sense reasoning. To help address these challenges, we provide annotation for supporting facts as well as their composition. Guided by these annotations, we present a two-step approach to mitigate the retrieval challenges. We use other multiple-choice datasets as additional training data to strengthen the reasoning model. Our proposed approach improves over current state-of-the-art language models by 11\% (absolute). The reasoning and retrieval problems, however, remain unsolved as this model still lags by 20\% behind human performance.},
	number = {{arXiv}:1910.11473},
	publisher = {{arXiv}},
	author = {Khot, Tushar and Clark, Peter and Guerquin, Michal and Jansen, Peter and Sabharwal, Ashish},
	urldate = {2024-05-29},
	date = {2020-02-04},
	eprinttype = {arxiv},
	eprint = {1910.11473 [cs]},
}

@misc{dasigiQuorefReadingComprehension2019,
	title = {Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning},
	url = {http://arxiv.org/abs/1908.05803},
	shorttitle = {Quoref},
	abstract = {Machine comprehension of texts longer than a single sentence often requires coreference resolution. However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference. We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning. We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues. We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark---the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.},
	number = {{arXiv}:1908.05803},
	publisher = {{arXiv}},
	author = {Dasigi, Pradeep and Liu, Nelson F. and Marasović, Ana and Smith, Noah A. and Gardner, Matt},
	urldate = {2024-05-29},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1908.05803 [cs]},
}

@misc{linReasoningParagraphEffects2019,
	title = {Reasoning Over Paragraph Effects in Situations},
	url = {http://arxiv.org/abs/1908.05852},
	abstract = {A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation. In order to facilitate progress on this kind of reading, we present {ROPES}, a challenging benchmark for reading comprehension targeting Reasoning Over Paragraph Effects in Situations. We target expository language describing causes and effects (e.g., "animal pollinators increase efficiency of fertilization in flowers"), as they have clear implications for new situations. A system is presented a background passage containing at least one of these relations, a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation. We collect background passages from science textbooks and Wikipedia that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,322 question dataset. We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models. The best model performs only slightly better than randomly guessing an answer of the correct type, at 61.6\% F1, well below the human performance of 89.0\%.},
	number = {{arXiv}:1908.05852},
	publisher = {{arXiv}},
	author = {Lin, Kevin and Tafjord, Oyvind and Clark, Peter and Gardner, Matt},
	urldate = {2024-05-29},
	date = {2019-12-15},
	eprinttype = {arxiv},
	eprint = {1908.05852 [cs]},
}

@misc{sapSocialIQACommonsenseReasoning2019,
	title = {{SocialIQA}: Commonsense Reasoning about Social Interactions},
	url = {http://arxiv.org/abs/1904.09728},
	shorttitle = {{SocialIQA}},
	abstract = {We introduce Social {IQa}, the first largescale benchmark for commonsense reasoning about social situations. Social {IQa} contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?" A: "Make sure no one else could hear"). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20\% gap). Notably, we further establish Social {IQa} as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, {COPA}).},
	number = {{arXiv}:1904.09728},
	publisher = {{arXiv}},
	author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and {LeBras}, Ronan and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2019-09-09},
	eprinttype = {arxiv},
	eprint = {1904.09728 [cs]},
}

@misc{gorrellRumourEval2019Determining2018,
	title = {{RumourEval} 2019: Determining Rumour Veracity and Support for Rumours},
	url = {http://arxiv.org/abs/1809.06683},
	shorttitle = {{RumourEval} 2019},
	abstract = {This is the proposal for {RumourEval}-2019, which will run in early 2019 as part of that year's {SemEval} event. Since the first {RumourEval} shared task in 2017, interest in automated claim validation has greatly increased, as the dangers of "fake news" have become a mainstream concern. Yet automated support for rumour checking remains in its infancy. For this reason, it is important that a shared task in this area continues to provide a focus for effort, which is likely to increase. We therefore propose a continuation in which the veracity of further rumours is determined, and as previously, supportive of this goal, tweets discussing them are classified according to the stance they take regarding the rumour. Scope is extended compared with the first {RumourEval}, in that the dataset is substantially expanded to include Reddit as well as Twitter data, and additional languages are also included.},
	number = {{arXiv}:1809.06683},
	publisher = {{arXiv}},
	author = {Gorrell, Genevieve and Bontcheva, Kalina and Derczynski, Leon and Kochkina, Elena and Liakata, Maria and Zubiaga, Arkaitz},
	urldate = {2024-05-29},
	date = {2018-09-18},
	eprinttype = {arxiv},
	eprint = {1809.06683 [cs]},
}

@misc{laiRACELargescaleReAding2017,
	title = {{RACE}: Large-scale {ReAding} Comprehension Dataset From Examinations},
	url = {http://arxiv.org/abs/1704.04683},
	shorttitle = {{RACE}},
	abstract = {We present {RACE}, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, {RACE} consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in {RACE} than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/{\textasciitilde}glai1/data/race/ and the code is available at https://github.com/qizhex/{RACE}\_AR\_baselines.},
	number = {{arXiv}:1704.04683},
	publisher = {{arXiv}},
	author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
	urldate = {2024-05-29},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1704.04683 [cs]},
}

@misc{pangSeeingStarsExploiting2005,
	title = {Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales},
	url = {http://arxiv.org/abs/cs/0506075},
	shorttitle = {Seeing stars},
	abstract = {We address the rating-inference problem, wherein rather than simply decide whether a review is "thumbs up" or "thumbs down", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five "stars"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, "three stars" is intuitively closer to "four stars" than to "one star". We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of {SVMs} when we employ a novel similarity measure appropriate to the problem.},
	number = {{arXiv}:cs/0506075},
	publisher = {{arXiv}},
	author = {Pang, Bo and Lee, Lillian},
	urldate = {2024-05-29},
	date = {2005-06-17},
	eprinttype = {arxiv},
	eprint = {cs/0506075},
}

@misc{rajpurkarSQuAD100000Questions2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {http://arxiv.org/abs/1606.05250},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	number = {{arXiv}:1606.05250},
	publisher = {{arXiv}},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2024-05-29},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.05250 [cs]},
}

@misc{wangSelfInstructAligningLanguage2023,
	title = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
	url = {http://arxiv.org/abs/2212.10560},
	shorttitle = {Self-Instruct},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla {GPT}3, we demonstrate a 33\% absolute improvement over the original model on Super-{NaturalInstructions}, on par with the performance of {InstructGPT}-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning {GPT}3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind {InstructGPT}-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	number = {{arXiv}:2212.10560},
	publisher = {{arXiv}},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	urldate = {2024-05-29},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2212.10560 [cs]},
}

@misc{jinPubMedQADatasetBiomedical2019,
	title = {{PubMedQA}: A Dataset for Biomedical Research Question Answering},
	url = {http://arxiv.org/abs/1909.06146},
	shorttitle = {{PubMedQA}},
	abstract = {We introduce {PubMedQA}, a novel biomedical question answering ({QA}) dataset collected from {PubMed} abstracts. The task of {PubMedQA} is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. {PubMedQA} has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated {QA} instances. Each {PubMedQA} instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. {PubMedQA} is the first {QA} dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of {BioBERT} with long answer bag-of-word statistics as additional supervision, achieves 68.1\% accuracy, compared to single human performance of 78.0\% accuracy and majority-baseline of 55.2\% accuracy, leaving much room for improvement. {PubMedQA} is publicly available at https://pubmedqa.github.io.},
	number = {{arXiv}:1909.06146},
	publisher = {{arXiv}},
	author = {Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William W. and Lu, Xinghua},
	urldate = {2024-05-01},
	date = {2019-09-13},
	eprinttype = {arxiv},
	eprint = {1909.06146 [cs, q-bio]},
}

@misc{couckeSnipsVoicePlatform2018,
	title = {Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces},
	url = {http://arxiv.org/abs/1805.10190},
	shorttitle = {Snips Voice Platform},
	abstract = {This paper presents the machine learning architecture of the Snips Voice Platform, a software solution to perform Spoken Language Understanding on microprocessors typical of {IoT} devices. The embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. Focusing on Automatic Speech Recognition and Natural Language Understanding, we detail our approach to training high-performance Machine Learning models that are small enough to run in real-time on small devices. Additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy.},
	number = {{arXiv}:1805.10190},
	publisher = {{arXiv}},
	author = {Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Théodore and Caulier, Alexandre and Leroy, David and Doumouro, Clément and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and Primet, Maël and Dureau, Joseph},
	urldate = {2024-05-01},
	date = {2018-12-06},
	eprinttype = {arxiv},
	eprint = {1805.10190 [cs]},
}

@misc{yuSpiderLargeScaleHumanLabeled2019,
	title = {Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task},
	url = {http://arxiv.org/abs/1809.08887},
	shorttitle = {Spider},
	abstract = {We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-{SQL} dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex {SQL} queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-{SQL} task where different complex {SQL} queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new {SQL} queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4\% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.github.io/spider},
	number = {{arXiv}:1809.08887},
	publisher = {{arXiv}},
	author = {Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and Zhang, Zilin and Radev, Dragomir},
	urldate = {2024-05-01},
	date = {2019-02-02},
	eprinttype = {arxiv},
	eprint = {1809.08887 [cs]},
}

@misc{wuMoleculeNetBenchmarkMolecular2018,
	title = {{MoleculeNet}: A Benchmark for Molecular Machine Learning},
	url = {http://arxiv.org/abs/1703.00564},
	shorttitle = {{MoleculeNet}},
	abstract = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces {MoleculeNet}, a large scale benchmark for molecular machine learning. {MoleculeNet} curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the {DeepChem} open source library). {MoleculeNet} benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.},
	number = {{arXiv}:1703.00564},
	publisher = {{arXiv}},
	author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
	urldate = {2024-10-02},
	date = {2018-10-25},
	eprinttype = {arxiv},
	eprint = {1703.00564 [physics, stat]},
}

@misc{sileoProbingNeuralLanguage2023,
	title = {Probing neural language models for understanding of words of estimative probability},
	url = {http://arxiv.org/abs/2211.03358},
	abstract = {Words of estimative probability ({WEP}) are expressions of a statement's plausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...). Multiple surveys demonstrate the agreement of human evaluators when assigning numerical probability levels to {WEP}. For example, highly likely corresponds to a median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this work, we measure the ability of neural language processing models to capture the consensual probability level associated to each {WEP}. Firstly, we use the {UNLI} dataset (Chen et al., 2020) which associates premises and hypotheses with their perceived joint probability p, to construct prompts, e.g. "[{PREMISE}]. [{WEP}], [{HYPOTHESIS}]." and assess whether language models can predict whether the {WEP} consensual probability level is close to p. Secondly, we construct a dataset of {WEP}-based probabilistic reasoning, to test whether language models can reason with {WEP} compositions. When prompted "[{EVENTA}] is likely. [{EVENTB}] is impossible.", a causal language model should not express that [{EVENTA}\&B] is likely. We show that both tasks are unsolved by off-the-shelf English language models, but that fine-tuning leads to transferable improvement.},
	number = {{arXiv}:2211.03358},
	publisher = {{arXiv}},
	author = {Sileo, Damien and Moens, Marie-Francine},
	urldate = {2024-05-29},
	date = {2023-06-25},
	eprinttype = {arxiv},
	eprint = {2211.03358 [cs]},
}

@misc{phamPiCPhraseContextDataset2023,
	title = {{PiC}: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search},
	url = {http://arxiv.org/abs/2207.09068},
	shorttitle = {{PiC}},
	abstract = {While contextualized word embeddings have been a de-facto standard, learning contextualized phrase embeddings is less explored and being hindered by the lack of a human-annotated benchmark that tests machine understanding of phrase semantics given a context sentence or paragraph (instead of phrases alone). To fill this gap, we propose {PiC} -- a dataset of {\textasciitilde}28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings. Training on {PiC} improves ranking models' accuracy and remarkably pushes span-selection ({SS}) models (i.e., predicting the start and end index of the target phrase) near-human accuracy, which is 95\% Exact Match ({EM}) on semantic search given a query phrase and a passage. Interestingly, we find evidence that such impressive performance is because the {SS} models learn to better capture the common meaning of a phrase regardless of its actual context. {SotA} models perform poorly in distinguishing two senses of the same phrase in two contexts ({\textasciitilde}60\% {EM}) and in estimating the similarity between two different phrases in the same context ({\textasciitilde}70\% {EM}).},
	number = {{arXiv}:2207.09068},
	publisher = {{arXiv}},
	author = {Pham, Thang M. and Yoon, Seunghyun and Bui, Trung and Nguyen, Anh},
	urldate = {2024-05-29},
	date = {2023-02-02},
	eprinttype = {arxiv},
	eprint = {2207.09068 [cs]},
}

@misc{emelinMoralStoriesSituated2020,
	title = {Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences},
	url = {http://arxiv.org/abs/2012.15738},
	shorttitle = {Moral Stories},
	abstract = {In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary {NLG} models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce 'Moral Stories', a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.},
	number = {{arXiv}:2012.15738},
	publisher = {{arXiv}},
	author = {Emelin, Denis and Bras, Ronan Le and Hwang, Jena D. and Forbes, Maxwell and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2020-12-31},
	eprinttype = {arxiv},
	eprint = {2012.15738 [cs]},
}

@misc{jiangNeuralCRFModel2021,
	title = {Neural {CRF} Model for Sentence Alignment in Text Simplification},
	url = {http://arxiv.org/abs/2005.02324},
	abstract = {The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia. We propose a novel neural {CRF} alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1. We apply our {CRF} aligner to construct two new text simplification datasets, Newsela-Auto and Wiki-Auto, which are much larger and of better quality compared to the existing datasets. A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation.},
	number = {{arXiv}:2005.02324},
	publisher = {{arXiv}},
	author = {Jiang, Chao and Maddela, Mounica and Lan, Wuwei and Zhong, Yang and Xu, Wei},
	urldate = {2024-05-29},
	date = {2021-08-30},
	eprinttype = {arxiv},
	eprint = {2005.02324 [cs]},
}

@misc{biskPIQAReasoningPhysical2019,
	title = {{PIQA}: Reasoning about Physical Commonsense in Natural Language},
	url = {http://arxiv.org/abs/1911.11641},
	shorttitle = {{PIQA}},
	abstract = {To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as {BERT}) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can {AI} systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or {PIQA}. Though humans find the dataset easy (95\% accuracy), large pretrained models struggle (77\%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.},
	number = {{arXiv}:1911.11641},
	publisher = {{arXiv}},
	author = {Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2019-11-26},
	eprinttype = {arxiv},
	eprint = {1911.11641 [cs]},
}

@misc{fabbriMultiNewsLargeScaleMultiDocument2019,
	title = {Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
	url = {http://arxiv.org/abs/1906.01749},
	shorttitle = {Multi-News},
	abstract = {Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization ({SDS}) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization ({MDS}) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale {MDS} news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard {SDS} model and achieves competitive results on {MDS} datasets. We benchmark several methods on Multi-News and release our data and code in hope that this work will promote advances in summarization in the multi-document setting.},
	number = {{arXiv}:1906.01749},
	publisher = {{arXiv}},
	author = {Fabbri, Alexander R. and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir R.},
	urldate = {2024-05-29},
	date = {2019-06-19},
	eprinttype = {arxiv},
	eprint = {1906.01749 [cs]},
}

@misc{aminiMathQAInterpretableMath2019,
	title = {{MathQA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms},
	url = {http://arxiv.org/abs/1905.13319},
	shorttitle = {{MathQA}},
	abstract = {We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, {MathQA}, significantly enhances the {AQuA} dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our {MathQA} as well as the {AQuA} dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-{QA}/},
	number = {{arXiv}:1905.13319},
	publisher = {{arXiv}},
	author = {Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
	urldate = {2024-05-29},
	date = {2019-05-30},
	eprinttype = {arxiv},
	eprint = {1905.13319 [cs]},
}

@misc{zhangPAWSParaphraseAdversaries2019,
	title = {{PAWS}: Paraphrase Adversaries from Word Scrambling},
	url = {http://arxiv.org/abs/1904.01130},
	shorttitle = {{PAWS}},
	abstract = {Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces {PAWS} (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on {PAWS} ({\textless}40\% accuracy); however, including {PAWS} training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with {PAWS} training examples. As such, {PAWS} provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
	number = {{arXiv}:1904.01130},
	publisher = {{arXiv}},
	author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
	urldate = {2024-05-29},
	date = {2019-04-01},
	eprinttype = {arxiv},
	eprint = {1904.01130 [cs]},
}

@misc{lingProgramInductionRationale2017,
	title = {Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems},
	url = {http://arxiv.org/abs/1705.04146},
	shorttitle = {Program Induction by Rationale Generation},
	abstract = {Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.},
	number = {{arXiv}:1705.04146},
	publisher = {{arXiv}},
	author = {Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
	urldate = {2024-05-29},
	date = {2017-10-23},
	eprinttype = {arxiv},
	eprint = {1705.04146 [cs]},
}

@misc{merityPointerSentinelMixture2016,
	title = {Pointer Sentinel Mixture Models},
	url = {http://arxiv.org/abs/1609.07843},
	abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-{LSTM} model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax {LSTM}. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available {WikiText} corpus.},
	number = {{arXiv}:1609.07843},
	publisher = {{arXiv}},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	urldate = {2024-05-29},
	date = {2016-09-26},
	eprinttype = {arxiv},
	eprint = {1609.07843 [cs]},
}

@misc{lebretNeuralTextGeneration2016,
	title = {Neural Text Generation from Structured Data with Application to the Biography Domain},
	url = {http://arxiv.org/abs/1603.07771},
	abstract = {This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 {BLEU}.},
	number = {{arXiv}:1603.07771},
	publisher = {{arXiv}},
	author = {Lebret, Remi and Grangier, David and Auli, Michael},
	urldate = {2024-05-29},
	date = {2016-09-23},
	eprinttype = {arxiv},
	eprint = {1603.07771 [cs]},
}

@misc{bajajMSMARCOHuman2018,
	title = {{MS} {MARCO}: A Human Generated {MAchine} Reading {COmprehension} Dataset},
	url = {http://arxiv.org/abs/1611.09268},
	shorttitle = {{MS} {MARCO}},
	abstract = {We introduce a large scale {MAchine} Reading {COmprehension} dataset, which we name {MS} {MARCO}. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the {MS} {MARCO} dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes {MS} {MARCO} from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
	number = {{arXiv}:1611.09268},
	publisher = {{arXiv}},
	author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and {McNamara}, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
	urldate = {2024-05-29},
	date = {2018-10-31},
	eprinttype = {arxiv},
	eprint = {1611.09268 [cs]},
}

@misc{mostafazadehGLUCOSEGeneraLizedCOntextualized2020,
	title = {{GLUCOSE}: {GeneraLized} and {COntextualized} Story Explanations},
	url = {http://arxiv.org/abs/2009.07758},
	shorttitle = {{GLUCOSE}},
	abstract = {When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward {AI} systems that can build similar mental models, we introduce {GLUCOSE}, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct {GLUCOSE}, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each {GLUCOSE} entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing {GLUCOSE} data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of {\textasciitilde}670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict {GLUCOSE}'s rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models.},
	number = {{arXiv}:2009.07758},
	publisher = {{arXiv}},
	author = {Mostafazadeh, Nasrin and Kalyanpur, Aditya and Moon, Lori and Buchanan, David and Berkowitz, Lauren and Biran, Or and Chu-Carroll, Jennifer},
	urldate = {2024-05-29},
	date = {2020-10-29},
	eprinttype = {arxiv},
	eprint = {2009.07758 [cs]},
}

@misc{wellerLearningTaskDescriptions2020,
	title = {Learning from Task Descriptions},
	url = {http://arxiv.org/abs/2011.08115},
	abstract = {Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing {NLP} systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, {ZEST}, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12\% on {ZEST}, leaving a significant challenge for {NLP} researchers.},
	number = {{arXiv}:2011.08115},
	publisher = {{arXiv}},
	author = {Weller, Orion and Lourie, Nicholas and Gardner, Matt and Peters, Matthew E.},
	urldate = {2024-05-29},
	date = {2020-11-16},
	eprinttype = {arxiv},
	eprint = {2011.08115 [cs]},
}

@misc{louisIdRatherJust2020,
	title = {"I'd rather just go to bed": Understanding Indirect Answers},
	url = {http://arxiv.org/abs/2010.03450},
	shorttitle = {"I'd rather just go to bed"},
	abstract = {We revisit a pragmatic inference problem in dialog: understanding indirect responses to questions. Humans can interpret 'I'm starving.' in response to 'Hungry?', even without direct cue words such as 'yes' and 'no'. In dialog systems, allowing natural responses rather than closed vocabularies would be similarly beneficial. However, today's systems are only as sensitive to these pragmatic moves as their language model allows. We create and release the first large-scale English language corpus 'Circa' with 34,268 (polar question, indirect answer) pairs to enable progress on this task. The data was collected via elaborate crowdsourcing, and contains utterances with yes/no meaning, as well as uncertain, middle-ground, and conditional responses. We also present {BERT}-based neural models to predict such categories for a question-answer pair. We find that while transfer learning from entailment works reasonably, performance is not yet sufficient for robust dialog. Our models reach 82-88\% accuracy for a 4-class distinction, and 74-85\% for 6 classes.},
	number = {{arXiv}:2010.03450},
	publisher = {{arXiv}},
	author = {Louis, Annie and Roth, Dan and Radlinski, Filip},
	urldate = {2024-05-29},
	date = {2020-10-07},
	eprinttype = {arxiv},
	eprint = {2010.03450 [cs]},
}

@misc{chalkidisLargeScaleMultiLabelText2019,
	title = {Large-Scale Multi-Label Text Classification on {EU} Legislation},
	url = {http://arxiv.org/abs/1906.02192},
	abstract = {We consider Large-Scale Multi-Label Text Classification ({LMTC}) in the legal domain. We release a new dataset of 57k legislative documents from {EURLEX}, annotated with {\textasciitilde}4.3k {EUROVOC} labels, which is suitable for {LMTC}, few- and zero-shot learning. Experimenting with several neural classifiers, we show that {BIGRUs} with label-wise attention perform better than other current state of the art methods. Domain-specific {WORD}2VEC and context-sensitive {ELMO} embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass {BERT}'s maximum text length limit and fine-tune {BERT}, obtaining the best results in all but zero-shot learning cases.},
	number = {{arXiv}:1906.02192},
	publisher = {{arXiv}},
	author = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Androutsopoulos, Ion},
	urldate = {2024-05-29},
	date = {2019-06-05},
	eprinttype = {arxiv},
	eprint = {1906.02192 [cs]},
}

@misc{zellersHellaSwagCanMachine2019,
	title = {{HellaSwag}: Can a Machine Really Finish Your Sentence?},
	url = {http://arxiv.org/abs/1905.07830},
	shorttitle = {{HellaSwag}},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup: "She sets her fingers on the keys." With the introduction of {BERT}, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting {HellaSwag}, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95\% accuracy), state-of-the-art models struggle ({\textless}48\%). We achieve this via Adversarial Filtering ({AF}), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. {AF} proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of {HellaSwag}, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for {NLP} research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	number = {{arXiv}:1905.07830},
	publisher = {{arXiv}},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2019-05-19},
	eprinttype = {arxiv},
	eprint = {1905.07830 [cs]},
}

@misc{yangHotpotQADatasetDiverse2018,
	title = {{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
	url = {http://arxiv.org/abs/1809.09600},
	shorttitle = {{HotpotQA}},
	abstract = {Existing question answering ({QA}) datasets fail to train {QA} systems to perform complex reasoning and provide explanations for answers. We introduce {HotpotQA}, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing {QA} systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test {QA} systems' ability to extract relevant facts and perform necessary comparison. We show that {HotpotQA} is challenging for the latest {QA} systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	number = {{arXiv}:1809.09600},
	publisher = {{arXiv}},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	urldate = {2024-05-29},
	date = {2018-09-25},
	eprinttype = {arxiv},
	eprint = {1809.09600 [cs]},
}

@misc{faruquiIdentifyingWellformedNatural2018,
	title = {Identifying Well-formed Natural Language Questions},
	url = {http://arxiv.org/abs/1808.09419},
	abstract = {Understanding search queries is a hard problem as it involves dealing with "word salad" text ubiquitously issued by users. However, if a query resembles a well-formed question, a natural language processing pipeline is able to perform more accurate interpretation, thus reducing downstream compounding errors. Hence, identifying whether or not a query is well formed can enhance query understanding. Here, we introduce a new task of identifying a well-formed natural language question. We construct and release a dataset of 25,100 publicly available questions classified into well-formed and non-wellformed categories and report an accuracy of 70.7\% on the test set. We also show that our classifier can be used to improve the performance of neural sequence-to-sequence models for generating questions for reading comprehension.},
	number = {{arXiv}:1808.09419},
	publisher = {{arXiv}},
	author = {Faruqui, Manaal and Das, Dipanjan},
	urldate = {2024-05-29},
	date = {2018-08-28},
	eprinttype = {arxiv},
	eprint = {1808.09419 [cs]},
}

@misc{wangGLUEMultiTaskBenchmark2019,
	title = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	url = {http://arxiv.org/abs/1804.07461},
	shorttitle = {{GLUE}},
	abstract = {For natural language understanding ({NLU}) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark ({GLUE}), a tool for evaluating and analyzing the performance of models across a diverse range of existing {NLU} tasks. {GLUE} is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of {NLU} models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust {NLU} systems.},
	number = {{arXiv}:1804.07461},
	publisher = {{arXiv}},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	urldate = {2024-05-29},
	date = {2019-02-22},
	eprinttype = {arxiv},
	eprint = {1804.07461 [cs]},
}

@misc{lakeGeneralizationSystematicityCompositional2018,
	title = {Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
	url = {http://arxiv.org/abs/1711.00350},
	shorttitle = {Generalization without systematicity},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the {SCAN} domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks ({RNNs}) trained on {SCAN} with sequence-to-sequence methods. We find that {RNNs} can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), {RNNs} fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
	number = {{arXiv}:1711.00350},
	publisher = {{arXiv}},
	author = {Lake, Brenden M. and Baroni, Marco},
	urldate = {2024-05-29},
	date = {2018-06-06},
	eprinttype = {arxiv},
	eprint = {1711.00350 [cs]},
}

@misc{seeGetPointSummarization2017,
	title = {Get To The Point: Summarization with Pointer-Generator Networks},
	url = {http://arxiv.org/abs/1704.04368},
	shorttitle = {Get To The Point},
	abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the {CNN} / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 {ROUGE} points.},
	number = {{arXiv}:1704.04368},
	publisher = {{arXiv}},
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	urldate = {2024-05-29},
	date = {2017-04-25},
	eprinttype = {arxiv},
	eprint = {1704.04368 [cs]},
}

@misc{maloGoodDebtBad2013,
	title = {Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts},
	url = {http://arxiv.org/abs/1307.5336},
	shorttitle = {Good Debt or Bad Debt},
	abstract = {The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to investigate how financial sentiments relate to future company performance. However, based on experience from other fields, where sentiment analysis is commonly applied, it is well-known that the overall semantic orientation of a sentence may differ from the prior polarity of individual words. The objective of this article is to investigate how semantic orientations can be better detected in financial and economic news by accommodating the overall phrase-structure information and domain-specific use of language. Our three main contributions are: (1) establishment of a human-annotated finance phrase-bank, which can be used as benchmark for training and evaluating alternative models; (2) presentation of a technique to enhance financial lexicons with attributes that help to identify expected direction of events that affect overall sentiment; (3) development of a linearized phrase-structure model for detecting contextual semantic orientations in financial and economic news texts. The relevance of the newly added lexicon features and the benefit of using the proposed learning-algorithm are demonstrated in a comparative study against previously used general sentiment models as well as the popular word frequency models used in recent financial studies. The proposed framework is parsimonious and avoids the explosion in feature-space caused by the use of conventional n-gram features.},
	number = {{arXiv}:1307.5336},
	publisher = {{arXiv}},
	author = {Malo, Pekka and Sinha, Ankur and Takala, Pyry and Korhonen, Pekka and Wallenius, Jyrki},
	urldate = {2024-05-29},
	date = {2013-07-23},
	eprinttype = {arxiv},
	eprint = {1307.5336 [cs, q-fin]},
}

@misc{guptaDisflQABenchmarkDataset2021,
	title = {Disfl-{QA}: A Benchmark Dataset for Understanding Disfluencies in Question Answering},
	url = {http://arxiv.org/abs/2106.04016},
	shorttitle = {Disfl-{QA}},
	abstract = {Disfluencies is an under-studied topic in {NLP}, even though it is ubiquitous in human conversation. This is largely due to the lack of datasets containing disfluencies. In this paper, we present a new challenge question answering dataset, Disfl-{QA}, a derivative of {SQuAD}, where humans introduce contextual disfluencies in previously fluent questions. Disfl-{QA} contains a variety of challenging disfluencies that require a more comprehensive understanding of the text than what was necessary in prior datasets. Experiments show that the performance of existing state-of-the-art question answering models degrades significantly when tested on Disfl-{QA} in a zero-shot setting.We show data augmentation methods partially recover the loss in performance and also demonstrate the efficacy of using gold data for fine-tuning. We argue that we need large-scale disfluency datasets in order for {NLP} models to be robust to them. The dataset is publicly available at: https://github.com/google-research-datasets/disfl-qa.},
	number = {{arXiv}:2106.04016},
	publisher = {{arXiv}},
	author = {Gupta, Aditya and Xu, Jiacheng and Upadhyay, Shyam and Yang, Diyi and Faruqui, Manaal},
	urldate = {2024-05-29},
	date = {2021-06-07},
	eprinttype = {arxiv},
	eprint = {2106.04016 [cs]},
}

@misc{huangEfficientAttentionsLong2021,
	title = {Efficient Attentions for Long Document Summarization},
	url = {http://arxiv.org/abs/2104.02112},
	abstract = {The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, {GovReport}, with significantly longer documents and summaries. Results show that our models produce significantly higher {ROUGE} scores than competitive comparisons, including new state-of-the-art results on {PubMed}. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.},
	number = {{arXiv}:2104.02112},
	publisher = {{arXiv}},
	author = {Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
	urldate = {2024-05-29},
	date = {2021-04-11},
	eprinttype = {arxiv},
	eprint = {2104.02112 [cs]},
}

@misc{hendrycksCUADExpertAnnotatedNLP2021,
	title = {{CUAD}: An Expert-Annotated {NLP} Dataset for Legal Contract Review},
	url = {http://arxiv.org/abs/2103.06268},
	shorttitle = {{CUAD}},
	abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset ({CUAD}), a new dataset for legal contract review. {CUAD} was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized {NLP} benchmarks annotated by experts, {CUAD} can serve as a challenging research benchmark for the broader {NLP} community.},
	number = {{arXiv}:2103.06268},
	publisher = {{arXiv}},
	author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
	urldate = {2024-05-29},
	date = {2021-11-08},
	eprinttype = {arxiv},
	eprint = {2103.06268 [cs]},
}

@misc{georgeConversationalImplicaturesEnglish2019,
	title = {Conversational implicatures in English dialogue: Annotated dataset},
	url = {http://arxiv.org/abs/1911.10704},
	shorttitle = {Conversational implicatures in English dialogue},
	abstract = {Human dialogue often contains utterances having meanings entirely different from the sentences used and are clearly understood by the interlocutors. But in human-computer interactions, the machine fails to understand the implicated meaning unless it is trained with a dataset containing the implicated meaning of an utterance along with the utterance and the context in which it is uttered. In linguistic terms, conversational implicatures are the meanings of the speaker's utterance that are not part of what is explicitly said. In this paper, we introduce a dataset of dialogue snippets with three constituents, which are the context, the utterance, and the implicated meanings. These implicated meanings are the conversational implicatures. The utterances are collected by transcribing from listening comprehension sections of English tests like {TOEFL} (Test of English as a Foreign Language) as well as scraping dialogues from movie scripts available on {IMSDb} (Internet Movie Script Database). The utterances are manually annotated with implicatures.},
	number = {{arXiv}:1911.10704},
	publisher = {{arXiv}},
	author = {George, Elizabeth Jasmi and Mamidi, Radhika},
	urldate = {2024-05-29},
	date = {2019-11-24},
	eprinttype = {arxiv},
	eprint = {1911.10704 [cs]},
}

@misc{huangCosmosQAMachine2019,
	title = {Cosmos {QA}: Machine Reading Comprehension with Contextual Commonsense Reasoning},
	url = {http://arxiv.org/abs/1909.00277},
	shorttitle = {Cosmos {QA}},
	abstract = {Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos {QA}, a large-scale dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of people's everyday narratives, asking such questions as "what might be the possible reason of ...?", or "what would have happened if ..." that require reasoning beyond the exact text spans in the context. To establish baseline performances on Cosmos {QA}, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4\%) and human performance (94\%), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at https://wilburone.github.io/cosmos.},
	number = {{arXiv}:1909.00277},
	publisher = {{arXiv}},
	author = {Huang, Lifu and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2019-09-06},
	eprinttype = {arxiv},
	eprint = {1909.00277 [cs]},
}

@misc{rajaniExplainYourselfLeveraging2019,
	title = {Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
	url = {http://arxiv.org/abs/1906.02361},
	abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations ({CoS}-E). We use {CoS}-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation ({CAGE}) framework. {CAGE} improves the state-of-the-art by 10\% on the challenging {CommonsenseQA} task. We further study commonsense reasoning in {DNNs} using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
	number = {{arXiv}:1906.02361},
	publisher = {{arXiv}},
	author = {Rajani, Nazneen Fatema and {McCann}, Bryan and Xiong, Caiming and Socher, Richard},
	urldate = {2024-05-29},
	date = {2019-06-05},
	eprinttype = {arxiv},
	eprint = {1906.02361 [cs]},
}

@misc{gevaDiscoFuseLargeScaleDataset2019,
	title = {{DiscoFuse}: A Large-Scale Dataset for Discourse-Based Sentence Fusion},
	url = {http://arxiv.org/abs/1902.10526},
	shorttitle = {{DiscoFuse}},
	abstract = {Sentence fusion is the task of joining several independent sentences into a single coherent text. Current datasets for sentence fusion are small and insufficient for training modern neural models. In this paper, we propose a method for automatically-generating fusion examples from raw text and present {DiscoFuse}, a large scale dataset for discourse-based sentence fusion. We author a set of rules for identifying a diverse set of discourse phenomena in raw text, and decomposing the text into two independent sentences. We apply our approach on two document collections: Wikipedia and Sports articles, yielding 60 million fusion examples annotated with discourse information required to reconstruct the fused text. We develop a sequence-to-sequence model on {DiscoFuse} and thoroughly analyze its strengths and weaknesses with respect to the various discourse phenomena, using both automatic as well as human evaluation. Finally, we conduct transfer learning experiments with {WebSplit}, a recent dataset for text simplification. We show that pretraining on {DiscoFuse} substantially improves performance on {WebSplit} when viewed as a sentence fusion task.},
	number = {{arXiv}:1902.10526},
	publisher = {{arXiv}},
	author = {Geva, Mor and Malmi, Eric and Szpektor, Idan and Berant, Jonathan},
	urldate = {2024-05-29},
	date = {2019-03-18},
	eprinttype = {arxiv},
	eprint = {1902.10526 [cs]},
}

@misc{narayanDontGiveMe2018,
	title = {Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
	url = {http://arxiv.org/abs/1808.08745},
	abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question "What is the article about?". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation ({BBC}). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	number = {{arXiv}:1808.08745},
	publisher = {{arXiv}},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	urldate = {2024-05-29},
	date = {2018-08-27},
	eprinttype = {arxiv},
	eprint = {1808.08745 [cs]},
}

@misc{sahaDuoRCComplexLanguage2018,
	title = {{DuoRC}: Towards Complex Language Understanding with Paraphrased Reading Comprehension},
	url = {http://arxiv.org/abs/1804.07927},
	shorttitle = {{DuoRC}},
	abstract = {We propose {DuoRC}, a novel dataset for Reading Comprehension ({RC}) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing {RC} datasets. {DuoRC} contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from {IMDb} - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of {DuoRC} where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural {RC} models which have achieved near human performance on the {SQuAD} dataset, even when coupled with traditional {NLP} techniques to address the challenges presented in {DuoRC} exhibit very poor performance (F1 score of 37.42\% on {DuoRC} v/s 86\% on {SQuAD} dataset). This opens up several interesting research avenues wherein {DuoRC} could complement other {RC} datasets to explore novel neural approaches for studying language understanding.},
	number = {{arXiv}:1804.07927},
	publisher = {{arXiv}},
	author = {Saha, Amrita and Aralikatte, Rahul and Khapra, Mitesh M. and Sankaranarayanan, Karthik},
	urldate = {2024-05-29},
	date = {2018-10-10},
	eprinttype = {arxiv},
	eprint = {1804.07927 [cs]},
}

@misc{thorneFEVERLargescaleDataset2018,
	title = {{FEVER}: a large-scale dataset for Fact Extraction and {VERification}},
	url = {http://arxiv.org/abs/1803.05355},
	shorttitle = {{FEVER}},
	abstract = {In this paper we introduce a new publicly available dataset for verification against textual sources, {FEVER}: Fact Extraction and {VERification}. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or {NotEnoughInfo} by annotators achieving 0.6841 in Fleiss \${\textbackslash}kappa\$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87\%, while if we ignore the evidence we achieve 50.91\%. Thus we believe that {FEVER} is a challenging testbed that will help stimulate progress on claim verification against textual sources.},
	number = {{arXiv}:1803.05355},
	publisher = {{arXiv}},
	author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
	urldate = {2024-05-29},
	date = {2018-12-18},
	eprinttype = {arxiv},
	eprint = {1803.05355 [cs]},
}

@misc{lewisDealNoDeal2017,
	title = {Deal or No Deal? End-to-End Learning for Negotiation Dialogues},
	url = {http://arxiv.org/abs/1706.05125},
	shorttitle = {Deal or No Deal?},
	abstract = {Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for {AI}. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).},
	number = {{arXiv}:1706.05125},
	publisher = {{arXiv}},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann N. and Parikh, Devi and Batra, Dhruv},
	urldate = {2024-05-29},
	date = {2017-06-15},
	eprinttype = {arxiv},
	eprint = {1706.05125 [cs]},
}

@misc{welblCrowdsourcingMultipleChoice2017,
	title = {Crowdsourcing Multiple Choice Science Questions},
	url = {http://arxiv.org/abs/1707.06209},
	abstract = {We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled {SciQ}, a dataset of 13.7K multiple choice science exam questions (Dataset available at http://allenai.org/data.html). We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using {SciQ} as additional training data to existing questions, we observe accuracy improvements on real science exams.},
	number = {{arXiv}:1707.06209},
	publisher = {{arXiv}},
	author = {Welbl, Johannes and Liu, Nelson F. and Gardner, Matt},
	urldate = {2024-05-29},
	date = {2017-07-19},
	eprinttype = {arxiv},
	eprint = {1707.06209 [cs, stat]},
}

@misc{caoControllableOpenendedQuestion2021,
	title = {Controllable Open-ended Question Generation with A New Question Type Ontology},
	url = {http://arxiv.org/abs/2107.00152},
	abstract = {We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. A new dataset with 4,959 questions is labeled based on the new ontology. We then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and produce the question. Based on this framework, we further use both exemplars and automatically generated templates to improve controllability and diversity. Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics. Human judges also rate our model outputs highly in answerability, coverage of scope, and overall quality. Finally, our model variants with templates can produce questions with enhanced controllability and diversity.},
	number = {{arXiv}:2107.00152},
	publisher = {{arXiv}},
	author = {Cao, Shuyang and Wang, Lu},
	urldate = {2024-05-29},
	date = {2021-06-30},
	eprinttype = {arxiv},
	eprint = {2107.00152 [cs]},
}

@misc{linBirdsHaveFour2020,
	title = {Birds have four legs?! {NumerSense}: Probing Numerical Commonsense Knowledge of Pre-trained Language Models},
	url = {http://arxiv.org/abs/2005.00683},
	shorttitle = {Birds have four legs?},
	abstract = {Recent works show that pre-trained language models ({PTLMs}), such as {BERT}, possess certain commonsense and factual knowledge. They suggest that it is promising to use {PTLMs} as "neural knowledge bases" via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from {PTLMs} as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, {NumerSense}, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) {BERT} and its stronger variant {RoBERTa} perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06\% vs 96.3\% in accuracy).},
	number = {{arXiv}:2005.00683},
	publisher = {{arXiv}},
	author = {Lin, Bill Yuchen and Lee, Seyeon and Khanna, Rahul and Ren, Xiang},
	urldate = {2024-05-29},
	date = {2020-09-17},
	eprinttype = {arxiv},
	eprint = {2005.00683 [cs]},
}

@article{bartoloBeatAIInvestigating2020,
	title = {Beat the {AI}: Investigating Adversarial Human Annotation for Reading Comprehension},
	volume = {8},
	issn = {2307-387X},
	url = {http://arxiv.org/abs/2002.00293},
	doi = {10/gjzgwj},
	shorttitle = {Beat the {AI}},
	abstract = {Innovations in annotation methodology have been a catalyst for Reading Comprehension ({RC}) datasets and models. One recent trend to challenge current {RC} models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model. We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a {BiDAF} model in the loop, {RoBERTa} achieves 39.9F1 on questions that it cannot answer when trained on {SQuAD} - only marginally lower than when trained on data collected using {RoBERTa} itself (41.0F1).},
	pages = {662--678},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Bartolo, Max and Roberts, Alastair and Welbl, Johannes and Riedel, Sebastian and Stenetorp, Pontus},
	urldate = {2024-05-29},
	date = {2020-12},
	eprinttype = {arxiv},
	eprint = {2002.00293 [cs]},
}

@misc{linCommonGenConstrainedText2020,
	title = {{CommonGen}: A Constrained Text Generation Challenge for Generative Commonsense Reasoning},
	url = {http://arxiv.org/abs/1911.03705},
	shorttitle = {{CommonGen}},
	abstract = {Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, {CommonGen} associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., \{dog, frisbee, catch, throw\}); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., "a man throws a frisbee and his dog catches it"). The {CommonGen} task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance. Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as {CommonsenseQA} by generating additional context.},
	number = {{arXiv}:1911.03705},
	publisher = {{arXiv}},
	author = {Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
	urldate = {2024-05-29},
	date = {2020-11-30},
	eprinttype = {arxiv},
	eprint = {1911.03705 [cs]},
}

@misc{warstadtBLiMPBenchmarkLinguistic2023,
	title = {{BLiMP}: The Benchmark of Linguistic Minimal Pairs for English},
	url = {http://arxiv.org/abs/1912.00582},
	shorttitle = {{BLiMP}},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (shortened to {BLiMP}), a challenge set for evaluating what language models ({LMs}) know about major grammatical phenomena in English. {BLiMP} consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4\%. We use it to evaluate n-gram, {LSTM}, and Transformer ({GPT}-2 and Transformer-{XL}) {LMs}. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands.},
	number = {{arXiv}:1912.00582},
	publisher = {{arXiv}},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	urldate = {2024-05-29},
	date = {2023-02-14},
	eprinttype = {arxiv},
	eprint = {1912.00582 [cs]},
}

@misc{nieAdversarialNLINew2020,
	title = {Adversarial {NLI}: A New Benchmark for Natural Language Understanding},
	url = {http://arxiv.org/abs/1910.14599},
	shorttitle = {Adversarial {NLI}},
	abstract = {We introduce a new large-scale {NLI} benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular {NLI} benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for {NLU}, rather than a static benchmark that will quickly saturate.},
	number = {{arXiv}:1910.14599},
	publisher = {{arXiv}},
	author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
	urldate = {2024-05-29},
	date = {2020-05-06},
	eprinttype = {arxiv},
	eprint = {1910.14599 [cs]},
}

@inproceedings{kornilovaBillSumCorpusAutomatic2019,
	title = {{BillSum}: A Corpus for Automatic Summarization of {US} Legislation},
	url = {http://arxiv.org/abs/1910.00523},
	doi = {10/gtwwtd},
	shorttitle = {{BillSum}},
	abstract = {Automatic summarization methods have been studied on a variety of domains, including news and scientific articles. Yet, legislation has not previously been considered for this task, despite {US} Congress and state governments releasing tens of thousands of bills every year. In this paper, we introduce {BillSum}, the first dataset for summarization of {US} Congressional and California state bills (https://github.com/{FiscalNote}/{BillSum}). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congressional bills can be used to summarize California bills, thus, showing that methods developed on this dataset can transfer to states without human-written summaries.},
	pages = {48--56},
	booktitle = {Proceedings of the 2nd Workshop on New Frontiers in Summarization},
	author = {Kornilova, Anastassia and Eidelman, Vlad},
	urldate = {2024-05-29},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1910.00523 [cs]},
}

@misc{clarkBoolQExploringSurprising2019,
	title = {{BoolQ}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
	url = {http://arxiv.org/abs/1905.10044},
	shorttitle = {{BoolQ}},
	abstract = {In this paper we study yes/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, {BoolQ}, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive {QA} data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as {BERT}. Our best method trains {BERT} on {MultiNLI} and then re-trains it on our train set. It achieves 80.4\% accuracy compared to 90\% accuracy of human annotators (and 62\% majority-baseline), leaving a significant gap for future work.},
	number = {{arXiv}:1905.10044},
	publisher = {{arXiv}},
	author = {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
	urldate = {2024-05-29},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1905.10044 [cs]},
}

@misc{talmorCommonsenseQAQuestionAnswering2019,
	title = {{CommonsenseQA}: A Question Answering Challenge Targeting Commonsense Knowledge},
	url = {http://arxiv.org/abs/1811.00937},
	shorttitle = {{CommonsenseQA}},
	abstract = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present {CommonsenseQA}: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from {ConceptNet} (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on {BERT}-large (Devlin et al., 2018) and obtains 56\% accuracy, well below human performance, which is 89\%.},
	number = {{arXiv}:1811.00937},
	publisher = {{arXiv}},
	author = {Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
	urldate = {2024-05-29},
	date = {2019-03-15},
	eprinttype = {arxiv},
	eprint = {1811.00937 [cs]},
}

@misc{mihaylovCanSuitArmor2018,
	title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
	url = {http://arxiv.org/abs/1809.02789},
	shorttitle = {Can a Suit of Armor Conduct Electricity?},
	abstract = {We present a new kind of question answering dataset, {OpenBookQA}, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing {QA} datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, {OpenBookQA} probes a deeper understanding of both the topic---in the context of common knowledge---and the language it is expressed in. Human performance on {OpenBookQA} is close to 92\%, but many state-of-the-art pre-trained {QA} methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.},
	number = {{arXiv}:1809.02789},
	publisher = {{arXiv}},
	author = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
	urldate = {2024-05-29},
	date = {2018-09-08},
	eprinttype = {arxiv},
	eprint = {1809.02789 [cs]},
}

@misc{zhangCharacterlevelConvolutionalNetworks2016,
	title = {Character-level Convolutional Networks for Text Classification},
	url = {http://arxiv.org/abs/1509.01626},
	abstract = {This article offers an empirical exploration on the use of character-level convolutional networks ({ConvNets}) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their {TFIDF} variants, and deep learning models such as word-based {ConvNets} and recurrent neural networks.},
	number = {{arXiv}:1509.01626},
	publisher = {{arXiv}},
	author = {Zhang, Xiang and Zhao, Junbo and {LeCun}, Yann},
	urldate = {2024-05-29},
	date = {2016-04-03},
	eprinttype = {arxiv},
	eprint = {1509.01626 [cs]},
}

@misc{liuBenchmarkingNaturalLanguage2019,
	title = {Benchmarking Natural Language Understanding Services for building Conversational Agents},
	url = {http://arxiv.org/abs/1903.05566},
	abstract = {We have recently seen the emergence of several publicly available Natural Language Understanding ({NLU}) toolkits, which map user utterances to structured, but more abstract, Dialogue Act ({DA}) or Intent specifications, while making this process accessible to the lay developer. In this paper, we present the first wide coverage evaluation and comparison of some of the most popular {NLU} services, on a large, multi-domain (21 domains) dataset of 25K user utterances that we have collected and annotated with Intent and Entity Type specifications and which will be released as part of this submission. The results show that on Intent classification Watson significantly outperforms the other platforms, namely, Dialogflow, {LUIS} and Rasa; though these also perform well. Interestingly, on Entity Type recognition, Watson performs significantly worse due to its low Precision. Again, Dialogflow, {LUIS} and Rasa perform well on this task.},
	number = {{arXiv}:1903.05566},
	publisher = {{arXiv}},
	author = {Liu, Xingkun and Eshghi, Arash and Swietojanski, Pawel and Rieser, Verena},
	urldate = {2024-05-01},
	date = {2019-03-26},
	eprinttype = {arxiv},
	eprint = {1903.05566 [cs]},
}

@misc{rushNeuralAttentionModel2015,
	title = {A Neural Attention Model for Abstractive Sentence Summarization},
	url = {http://arxiv.org/abs/1509.00685},
	abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the {DUC}-2004 shared task compared with several strong baselines.},
	number = {{arXiv}:1509.00685},
	publisher = {{arXiv}},
	author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
	urldate = {2024-05-29},
	date = {2015-09-03},
	eprinttype = {arxiv},
	eprint = {1509.00685 [cs]},
}

@inproceedings{casanuevaEfficientIntentDetection2020,
	location = {Online},
	title = {Efficient Intent Detection with Dual Sentence Encoders},
	url = {https://aclanthology.org/2020.nlp4convai-1.5},
	doi = {10/gjhzzs},
	abstract = {Building conversational systems in new domains and with added functionality requires resource-efficient models that work under low-data regimes (i.e., in few-shot setups). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as {USE} and {ConveRT}. We demonstrate the usefulness and wide applicability of the proposed intent detectors, showing that: 1) they outperform intent detectors based on fine-tuning the full {BERT}-Large model or using {BERT} as a fixed black-box encoder on three diverse intent detection data sets; 2) the gains are especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated examples per intent); 3) our intent detectors can be trained in a matter of minutes on a single {CPU}; and 4) they are stable across different hyperparameter settings. In hope of facilitating and democratizing research focused on intention detection, we release our code, as well as a new challenging single-domain intent detection dataset comprising 13,083 annotated examples over 77 intents.},
	eventtitle = {{NLP}4ConvAI 2020},
	pages = {38--45},
	booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Conversational {AI}},
	publisher = {Association for Computational Linguistics},
	author = {Casanueva, Iñigo and Temčinas, Tadas and Gerz, Daniela and Henderson, Matthew and Vulić, Ivan},
	editor = {Wen, Tsung-Hsien and Celikyilmaz, Asli and Yu, Zhou and Papangelis, Alexandros and Eric, Mihail and Kumar, Anuj and Casanueva, Iñigo and Shah, Rushin},
	urldate = {2024-05-01},
	date = {2020-07},
	eprinttype = {arxiv},
	eprint = {2003.04807 [cs]},
}

@inproceedings{nanDARTOpenDomainStructured2021,
	location = {Online},
	title = {{DART}: Open-Domain Structured Data Record to Text Generation},
	url = {https://aclanthology.org/2021.naacl-main.37},
	doi = {10/gnh49f},
	shorttitle = {{DART}},
	abstract = {We present {DART}, an open domain structured {DAta} Record to Text generation dataset with over 82k instances ({DARTs}). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on {DART} as well as new state-of-the-art results on {WebNLG} 2017 to show that {DART} (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-{LILY}/dart.},
	eventtitle = {{NAACL}-{HLT} 2021},
	pages = {432--447},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Nan, Linyong and Radev, Dragomir and Zhang, Rui and Rau, Amrit and Sivaprasad, Abhinand and Hsieh, Chiachun and Tang, Xiangru and Vyas, Aadit and Verma, Neha and Krishna, Pranav and Liu, Yangxiaokang and Irwanto, Nadia and Pan, Jessica and Rahman, Faiaz and Zaidi, Ahmad and Mutuma, Mutethia and Tarabar, Yasin and Gupta, Ankit and Yu, Tao and Tan, Yi Chern and Lin, Xi Victoria and Xiong, Caiming and Socher, Richard and Rajani, Nazneen Fatema},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	urldate = {2024-05-01},
	date = {2021-06},
	eprinttype = {arxiv},
	eprint = {2007.02871 [cs]},
}

@article{liCompetitionLevelCodeGeneration2022,
	title = {Competition-Level Code Generation with {AlphaCode}},
	volume = {378},
	issn = {0036-8075, 1095-9203},
	url = {http://arxiv.org/abs/2203.07814},
	doi = {10/grggxf},
	abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in {AI} has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce {AlphaCode}, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, {AlphaCode} achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
	pages = {1092--1097},
	number = {6624},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and d'Autume, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
	urldate = {2024-10-02},
	date = {2022-12-09},
	eprinttype = {arxiv},
	eprint = {2203.07814 [cs]},
}

@inproceedings{gliwaSAMSumCorpusHumanannotated2019,
	location = {Hong Kong, China},
	title = {{SAMSum} Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization},
	url = {https://aclanthology.org/D19-5409},
	doi = {10/gmjqgr},
	shorttitle = {{SAMSum} Corpus},
	abstract = {This paper introduces the {SAMSum} Corpus, a new dataset with abstractive dialogue summaries. We investigate the challenges it poses for automated summarization by testing several models and comparing their results with those obtained on a corpus of news articles. We show that model-generated summaries of dialogues achieve higher {ROUGE} scores than the model-generated summaries of news – in contrast with human evaluators' judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated models and non-standard quality measures. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.},
	pages = {70--79},
	booktitle = {Proceedings of the 2nd Workshop on New Frontiers in Summarization},
	publisher = {Association for Computational Linguistics},
	author = {Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},
	editor = {Wang, Lu and Cheung, Jackie Chi Kit and Carenini, Giuseppe and Liu, Fei},
	urldate = {2024-05-01},
	date = {2019-11},
}

@inproceedings{rameshkumarStorytellingDialogueCritical2020,
	location = {Online},
	title = {Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset},
	url = {https://aclanthology.org/2020.acl-main.459},
	doi = {10/gtsqxp},
	shorttitle = {Storytelling with Dialogue},
	abstract = {This paper describes the Critical Role Dungeons and Dragons Dataset ({CRD}3) and related analyses. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game. The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues. In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural {ML} approaches, and we provide an abstractive summarization benchmark and evaluation.},
	eventtitle = {{ACL} 2020},
	pages = {5121--5134},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Rameshkumar, Revanth and Bailey, Peter},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2024-05-01},
	date = {2020-07},
}

@misc{wangVidProMMillionscaleReal2024,
	title = {{VidProM}: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models},
	url = {http://arxiv.org/abs/2403.06098},
	shorttitle = {{VidProM}},
	abstract = {The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce {VidProM}, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how {VidProM} differs from {DiffusionDB}, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models. The project (including the collected dataset {VidProM} and related code) is publicly available at https://vidprom.github.io under the {CC}-{BY}-{NC} 4.0 License.},
	number = {{arXiv}:2403.06098},
	publisher = {{arXiv}},
	author = {Wang, Wenhao and Yang, Yi},
	urldate = {2024-10-02},
	date = {2024-09-30},
	eprinttype = {arxiv},
	eprint = {2403.06098 [cs]},
}

@misc{chenShareGPT4VideoImprovingVideo2024,
	title = {{ShareGPT}4Video: Improving Video Understanding and Generation with Better Captions},
	url = {http://arxiv.org/abs/2406.04325},
	shorttitle = {{ShareGPT}4Video},
	abstract = {We present the {ShareGPT}4Video series, aiming to facilitate the video understanding of large video-language models ({LVLMs}) and the video generation of text-to-video models (T2VMs) via dense and precise captions. The series comprises: 1) {ShareGPT}4Video, 40K {GPT}4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) {ShareCaptioner}-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) {ShareGPT}4Video-8B, a simple yet superb {LVLM} that reached {SOTA} performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using {GPT}4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct {ShareGPT}4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on {ShareGPT}4Video, we further develop {ShareCaptioner}-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos...},
	number = {{arXiv}:2406.04325},
	publisher = {{arXiv}},
	author = {Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and Yuan, Li and Qiao, Yu and Lin, Dahua and Zhao, Feng and Wang, Jiaqi},
	urldate = {2024-10-02},
	date = {2024-06-06},
	eprinttype = {arxiv},
	eprint = {2406.04325 [cs]},
}

@misc{nanOpenVid1MLargeScaleHighQuality2024,
	title = {{OpenVid}-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation},
	url = {http://arxiv.org/abs/2407.02371},
	shorttitle = {{OpenVid}-1M},
	abstract = {Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. {WebVid}-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-quality text-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information from text prompt. To address these issues, we introduce {OpenVid}-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V generation. Furthermore, we curate 433K 1080p videos from {OpenVid}-1M to create {OpenVidHD}-0.4M, advancing high-definition video generation. Additionally, we propose a novel Multi-modal Video Diffusion Transformer ({MVDiT}) capable of mining both structure information from visual tokens and semantic information from text tokens. Extensive experiments and ablation studies verify the superiority of {OpenVid}-1M over previous datasets and the effectiveness of our {MVDiT}.},
	number = {{arXiv}:2407.02371},
	publisher = {{arXiv}},
	author = {Nan, Kepan and Xie, Rui and Zhou, Penghao and Fan, Tiehan and Yang, Zhenheng and Chen, Zhijie and Li, Xiang and Yang, Jian and Tai, Ying},
	urldate = {2024-10-02},
	date = {2024-08-02},
	eprinttype = {arxiv},
	eprint = {2407.02371 [cs]},
}

@misc{liMVBenchComprehensiveMultimodal2024,
	title = {{MVBench}: A Comprehensive Multi-modal Video Understanding Benchmark},
	url = {http://arxiv.org/abs/2311.17005},
	shorttitle = {{MVBench}},
	abstract = {With the rapid development of Multi-modal Large Language Models ({MLLMs}), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely {MVBench}, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel static-to-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiple-choice {QA} to evaluate each task. On one hand, such a distinct paradigm allows us to build {MVBench} efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of {LLMs}. Moreover, we further develop a robust video {MLLM} baseline, i.e., {VideoChat}2, by progressive multi-modal training with diverse instruction-tuning data. The extensive results on our {MVBench} reveal that, the existing {MLLMs} are far from satisfactory in temporal understanding, while our {VideoChat}2 largely surpasses these leading models by over 15\% on {MVBench}. All models and data are available at https://github.com/{OpenGVLab}/Ask-Anything.},
	number = {{arXiv}:2311.17005},
	publisher = {{arXiv}},
	author = {Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and Wang, Limin and Qiao, Yu},
	urldate = {2024-10-02},
	date = {2024-05-23},
	eprinttype = {arxiv},
	eprint = {2311.17005 [cs]},
}

@article{kuryChiaLargeAnnotated2020,
	title = {Chia, a large annotated corpus of clinical trial eligibility criteria},
	volume = {7},
	rights = {2020 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-020-00620-0},
	doi = {10/gr4ftn},
	abstract = {We present Chia, a novel, large annotated corpus of patient eligibility criteria extracted from 1,000 interventional, Phase {IV} clinical trials registered in {ClinicalTrials}.gov. This dataset includes 12,409 annotated eligibility criteria, represented by 41,487 distinctive entities of 15 entity types and 25,017 relationships of 12 relationship types. Each criterion is represented as a directed acyclic graph, which can be easily transformed into Boolean logic to form a database query. Chia can serve as a shared benchmark to develop and test future machine learning, rule-based, or hybrid methods for information extraction from free-text clinical trial eligibility criteria.},
	pages = {281},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Kury, Fabrício and Butler, Alex and Yuan, Chi and Fu, Li-heng and Sun, Yingcheng and Liu, Hao and Sim, Ida and Carini, Simona and Weng, Chunhua},
	urldate = {2024-10-02},
	date = {2020-08-27},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
}

@inproceedings{soleimaniNLQuADNonFactoidLong2021,
	location = {Online},
	title = {{NLQuAD}: A Non-Factoid Long Question Answering Data Set},
	url = {https://aclanthology.org/2021.eacl-main.106},
	doi = {10/g6k6th},
	shorttitle = {{NLQuAD}},
	eventtitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	pages = {1245--1255},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher = {Association for Computational Linguistics},
	author = {Soleimani, Amir and Monz, Christof and Worring, Marcel},
	urldate = {2024-10-02},
	date = {2021},
	langid = {english},
}

@article{gazzolaPredicaoComplexidadeTextual2019,
	title = {Predição da complexidade textual de recursos educacionais abertos em português},
	url = {https://repositorio.usp.br/item/002971271},
	journaltitle = {Symposium in Information and Human Language Technology - {STIL}},
	author = {Gazzola, Murilo Gleyson and Leal, Sidney Evaldo and Aluísio, Sandra Maria},
	urldate = {2024-10-02},
	date = {2019},
	langid = {portuguese},
	keywords = {⛔ No {DOI} found},
}

@article{pakhomovSemanticSimilarityRelatedness2010,
	title = {Semantic Similarity and Relatedness between Clinical Terms: An Experimental Study},
	volume = {2010},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041430/},
	shorttitle = {Semantic Similarity and Relatedness between Clinical Terms},
	abstract = {Automated approaches to measuring semantic similarity and relatedness can provide necessary semantic context information for information retrieval applications and a number of fundamental natural language processing tasks including word sense disambiguation. Challenges for the development of these approaches include the limited availability of validated reference standards and the need for better understanding of the notions of semantic relatedness and similarity in medical vocabulary. We present results of a study in which eight medical residents were asked to judge 724 pairs of medical terms for semantic similarity and relatedness. The results of the study confirm the existence of a measurable mental representation of semantic relatedness between medical terms that is distinct from similarity and independent of the context in which the terms occur. This study produced a validated publicly available dataset for developing automated approaches to measuring semantic relatedness and similarity.},
	pages = {572--576},
	journaltitle = {{AMIA} Annual Symposium Proceedings},
	shortjournal = {{AMIA} Annu Symp Proc},
	author = {Pakhomov, Serguei and {McInnes}, Bridget and Adam, Terrence and Liu, Ying and Pedersen, Ted and Melton, Genevieve B.},
	urldate = {2024-10-02},
	date = {2010},
	pmid = {21347043},
	pmcid = {PMC3041430},
}

@article{khotSciTaiLTextualEntailment2018,
	title = {{SciTaiL}: A Textual Entailment Dataset from Science Question Answering},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12022},
	doi = {10/grm22d},
	shorttitle = {{SciTaiL}},
	abstract = {We present a new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem. {SciTail} is the first entailment set that is created solely from natural sentences that already exist independently ``in the wild'' rather than sentences authored specifically for the entailment task. Different from existing entailment datasets, we create hypotheses from science questions and the corresponding answer candidates, and premises from relevant web sentences retrieved from a large corpus. These sentences are often linguistically challenging. This, combined with the high lexical similarity of premise and hypothesis for both entailed and non-entailed pairs, makes this new entailment task particularly difficult. The resulting challenge is evidenced by state-of-the-art textual entailment systems achieving mediocre performance on {SciTail}, especially in comparison to a simple majority class baseline. As a step forward, we demonstrate that one can improve accuracy on {SciTail} by 5\% using a new neural model that exploits linguistic structure.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Khot, Tushar and Sabharwal, Ashish and Clark, Peter},
	urldate = {2024-10-02},
	date = {2018-04-27},
}

@article{pedersenMeasuresSemanticSimilarity2007,
	title = {Measures of semantic similarity and relatedness in the biomedical domain},
	volume = {40},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046406000645},
	doi = {10/fghjwr},
	abstract = {Measures of semantic similarity between concepts are widely used in Natural Language Processing. In this article, we show how six existing domain-independent measures can be adapted to the biomedical domain. These measures were originally based on {WordNet}, an English lexical database of concepts and relations. In this research, we adapt these measures to the {SNOMED}-{CT}® ontology of medical concepts. The measures include two path-based measures, and three measures that augment path-based measures with information content statistics from corpora. We also derive a context vector measure based on medical corpora that can be used as a measure of semantic relatedness. These six measures are evaluated against a newly created test bed of 30 medical concept pairs scored by three physicians and nine medical coders. We find that the medical coders and physicians differ in their ratings, and that the context vector measure correlates most closely with the physicians, while the path-based measures and one of the information content measures correlates most closely with the medical coders. We conclude that there is a role both for more flexible measures of relatedness based on information derived from corpora, as well as for measures that rely on existing ontological structures.},
	pages = {288--299},
	number = {3},
	journaltitle = {Journal of Biomedical Informatics},
	shortjournal = {Journal of Biomedical Informatics},
	author = {Pedersen, Ted and Pakhomov, Serguei V. S. and Patwardhan, Siddharth and Chute, Christopher G.},
	urldate = {2024-10-02},
	date = {2007-06-01},
}

@article{irwinZINC20AFreeUltralargeScale2020,
	title = {{ZINC}20—A Free Ultralarge-Scale Chemical Database for Ligand Discovery},
	volume = {60},
	issn = {1549-9596},
	url = {https://doi.org/10.1021/acs.jcim.0c00675},
	doi = {10/gmjg8b},
	abstract = {Identifying and purchasing new small molecules to test in biological assays are enabling for ligand discovery, but as purchasable chemical space continues to grow into the tens of billions based on inexpensive make-on-demand compounds, simply searching this space becomes a major challenge. We have therefore developed {ZINC}20, a new version of {ZINC} with two major new features: billions of new molecules and new methods to search them. As a fully enumerated database, {ZINC} can be searched precisely using explicit atomic-level graph-based methods, such as {SmallWorld} for similarity and Arthor for pattern and substructure search, as well as 3D methods such as docking. Analysis of the new make-on-demand compound sets by these and related tools reveals startling features. For instance, over 97\% of the core Bemis–Murcko scaffolds in make-on-demand libraries are unavailable from “in-stock” collections. Correspondingly, the number of new Bemis–Murcko scaffolds is rising almost as a linear fraction of the elaborated molecules. Thus, an 88-fold increase in the number of molecules in the make-on-demand versus the in-stock sets is built upon a 16-fold increase in the number of Bemis–Murcko scaffolds. The make-on-demand library is also more structurally diverse than physical libraries, with a massive increase in disc- and sphere-like shaped molecules. The new system is freely available at zinc20.docking.org.},
	pages = {6065--6073},
	number = {12},
	journaltitle = {Journal of Chemical Information and Modeling},
	shortjournal = {J. Chem. Inf. Model.},
	author = {Irwin, John J. and Tang, Khanh G. and Young, Jennifer and Dandarchuluun, Chinzorig and Wong, Benjamin R. and Khurelbaatar, Munkhzul and Moroz, Yurii S. and Mayfield, John and Sayle, Roger A.},
	urldate = {2024-10-02},
	date = {2020-12-28},
	note = {Publisher: American Chemical Society},
}

@misc{loweUbuntuDialogueCorpus2016,
	title = {The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems},
	url = {http://arxiv.org/abs/1506.08909},
	shorttitle = {The Ubuntu Dialogue Corpus},
	abstract = {This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.},
	number = {{arXiv}:1506.08909},
	publisher = {{arXiv}},
	author = {Lowe, Ryan and Pow, Nissan and Serban, Iulian and Pineau, Joelle},
	urldate = {2024-10-02},
	date = {2016-02-03},
	eprinttype = {arxiv},
	eprint = {1506.08909 [cs]},
}

@article{shribergCanProsodyAid1998,
	title = {Can prosody aid the automatic classification of dialog acts in conversational speech?},
	volume = {41 ( Pt 3-4)},
	issn = {0023-8309},
	doi = {10.1177/002383099804100410},
	abstract = {Identifying whether an utterance is a statement, question, greeting, and so forth is integral to effective automatic understanding of natural dialog. Little is known, however, about how such dialog acts ({DAs}) can be automatically classified in truly natural conversation. This study asks whether current approaches, which use mainly word information, could be improved by adding prosodic information. The study is based on more than 1000 conversations from the Switchboard corpus. {DAs} were hand-annotated, and prosodic features (duration, pause, F0, energy, and speaking rate) were automatically extracted for each {DA}. In training, decision trees based on these features were inferred; trees were then applied to unseen test data to evaluate performance. Performance was evaluated for prosody models alone, and after combining the prosody models with word information--either from true words or from the output of an automatic speech recognizer. For an overall classification task, as well as three subtasks, prosody made significant contributions to classification. Feature-specific analyses further revealed that although canonical features (such as F0 for questions) were important, less obvious features could compensate if canonical features were removed. Finally, in each task, integrating the prosodic model with a {DA}-specific statistical language model improved performance over that of the language model alone, especially for the case of recognized words. Results suggest that {DAs} are redundantly marked in natural conversation, and that a variety of automatically extractable prosodic features could aid dialog processing in speech applications.},
	pages = {443--492},
	journaltitle = {Language and Speech},
	shortjournal = {Lang Speech},
	author = {Shriberg, E. and Bates, R. and Stolcke, A. and Taylor, P. and Jurafsky, D. and Ries, K. and Coccaro, N. and Martin, R. and Meteer, M. and van Ess-Dykema, C.},
	date = {1998},
	pmid = {10746366},
}

@inproceedings{dankersCanTransformerBe2022,
	location = {Dublin, Ireland},
	title = {Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation},
	url = {https://aclanthology.org/2022.acl-long.252},
	doi = {10/g6k6xn},
	shorttitle = {Can Transformer be Too Compositional?},
	abstract = {Unlike literal expressions, idioms' meanings do not directly follow from their parts, posing a challenge for neural machine translation ({NMT}). {NMT} models are often unable to translate idioms accurately and over-generate compositional, literal translations. In this work, we investigate whether the non-compositionality of idioms is reflected in the mechanics of the dominant {NMT} model, Transformer, by analysing the hidden states and attention patterns for models with English as source language and one of seven European languages as target language. When Transformer emits a non-literal translation - i.e. identifies the expression as idiomatic - the encoder processes idioms more strongly as single lexical units compared to literal expressions. This manifests in idioms' parts being grouped through attention and in reduced interaction between idioms and their context. In the decoder's cross-attention, figurative inputs result in reduced attention on source-side tokens. These results suggest that Transformer's tendency to process idioms as compositional expressions contributes to literal translations of idioms.},
	eventtitle = {{ACL} 2022},
	pages = {3608--3626},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Dankers, Verna and Lucas, Christopher and Titov, Ivan},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	urldate = {2024-10-02},
	date = {2022-05},
}

@article{stolckeDialogueActModeling2000,
	title = {Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech},
	volume = {26},
	issn = {0891-2017, 1530-9312},
	url = {http://arxiv.org/abs/cs/0006023},
	doi = {10/dqmv4j},
	abstract = {We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as Statement, Question, Backchannel, Agreement, Disagreement, and Apology. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65\% based on errorful, automatically recognized words and prosody, and 71\% based on word transcripts, compared to a chance baseline accuracy of 35\% and human accuracy of 84\%) and a small reduction in word recognition error.},
	pages = {339--373},
	number = {3},
	journaltitle = {Computational Linguistics},
	shortjournal = {Computational Linguistics},
	author = {Stolcke, A. and Ries, K. and Coccaro, N. and Shriberg, E. and Bates, R. and Jurafsky, D. and Taylor, P. and Martin, R. and Van Ess-Dykema, C. and Meteer, M.},
	urldate = {2024-10-02},
	date = {2000-09},
	eprinttype = {arxiv},
	eprint = {cs/0006023},
}

@inproceedings{gallinaKPTimesLargeScaleDataset2019,
	location = {Tokyo, Japan},
	title = {{KPTimes}: A Large-Scale Dataset for Keyphrase Generation on News Documents},
	url = {https://aclanthology.org/W19-8617},
	doi = {10/g6k64k},
	shorttitle = {{KPTimes}},
	abstract = {Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present {KPTimes}, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents, and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on {KPTimes} to gain insights on how well they perform on the news domain. The dataset is available online at https://github.com/ygorg/{KPTimes}.},
	eventtitle = {{INLG} 2019},
	pages = {130--135},
	booktitle = {Proceedings of the 12th International Conference on Natural Language Generation},
	publisher = {Association for Computational Linguistics},
	author = {Gallina, Ygor and Boudin, Florian and Daille, Beatrice},
	editor = {van Deemter, Kees and Lin, Chenghua and Takamura, Hiroya},
	urldate = {2024-10-02},
	date = {2019-10},
}

@inproceedings{boudinRedefiningAbsentKeyphrases2021,
	location = {Online},
	title = {Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness},
	url = {https://aclanthology.org/2021.naacl-main.330},
	doi = {10/g6k64d},
	abstract = {Neural keyphrase generation models have recently attracted much interest due to their ability to output absent keyphrases, that is, keyphrases that do not appear in the source text. In this paper, we discuss the usefulness of absent keyphrases from an Information Retrieval ({IR}) perspective, and show that the commonly drawn distinction between present and absent keyphrases is not made explicit enough. We introduce a finer-grained categorization scheme that sheds more light on the impact of absent keyphrases on scientific document retrieval. Under this scheme, we find that only a fraction (around 20\%) of the words that make up keyphrases actually serves as document expansion, but that this small fraction of words is behind much of the gains observed in retrieval effectiveness. We also discuss how the proposed scheme can offer a new angle to evaluate the output of neural keyphrase generation models.},
	eventtitle = {{NAACL}-{HLT} 2021},
	pages = {4185--4193},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Boudin, Florian and Gallina, Ygor},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	urldate = {2024-10-02},
	date = {2021-06},
}

@inproceedings{ganesanOpinosisGraphBased2010,
	location = {Beijing, China},
	title = {Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant Opinions},
	url = {https://aclanthology.org/C10-1039},
	shorttitle = {Opinosis},
	eventtitle = {{COLING} 2010},
	pages = {340--348},
	booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
	publisher = {Coling 2010 Organizing Committee},
	author = {Ganesan, Kavita and Zhai, {ChengXiang} and Han, Jiawei},
	editor = {Huang, Chu-Ren and Jurafsky, Dan},
	urldate = {2024-10-02},
	date = {2010-08},
}

@misc{mishraLilaUnifiedBenchmark2023,
	title = {Lila: A Unified Benchmark for Mathematical Reasoning},
	url = {http://arxiv.org/abs/2210.17517},
	shorttitle = {Lila},
	abstract = {Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving {AI} systems in this domain, we propose {LILA}, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce {BHASKARA}, a general-purpose mathematical reasoning model trained on {LILA}. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83\% F1 score vs. single-task models), while the best performing model only obtains 60.40\%, indicating the room for improvement in general mathematical reasoning and understanding.},
	number = {{arXiv}:2210.17517},
	publisher = {{arXiv}},
	author = {Mishra, Swaroop and Finlayson, Matthew and Lu, Pan and Tang, Leonard and Welleck, Sean and Baral, Chitta and Rajpurohit, Tanmay and Tafjord, Oyvind and Sabharwal, Ashish and Clark, Peter and Kalyan, Ashwin},
	urldate = {2024-10-02},
	date = {2023-03-08},
	eprinttype = {arxiv},
	eprint = {2210.17517 [cs]},
}

@inproceedings{tylecekSpatialPatternTemplates2013,
	location = {Berlin, Heidelberg},
	title = {Spatial Pattern Templates for Recognition of Objects with Regular Structure},
	isbn = {978-3-642-40602-7},
	doi = {10/ggwb5g},
	abstract = {We propose a method for semantic parsing of images with regular structure. The structured objects are modeled in a densely connected {CRF}. The paper describes how to embody specific spatial relations in a representation called Spatial Pattern Templates ({SPT}), which allows us to capture regularity constraints of alignment and equal spacing in pairwise and ternary potentials.},
	pages = {364--374},
	booktitle = {Pattern Recognition},
	publisher = {Springer},
	author = {Tyleček, Radim and Šára, Radim},
	editor = {Weickert, Joachim and Hein, Matthias and Schiele, Bernt},
	date = {2013},
	langid = {english},
}

@inproceedings{thawaniNumeracyEnhancesLiteracy2021,
	location = {Online and Punta Cana, Dominican Republic},
	title = {Numeracy enhances the Literacy of Language Models},
	url = {https://aclanthology.org/2021.emnlp-main.557},
	doi = {10/g6k6w5},
	abstract = {Specialized number representations in {NLP} have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your `room' but not 500. Does a better grasp of numbers improve a model's understanding of other concepts and words? This paper studies the effect of using six different number encoders on the task of masked word prediction ({MWP}), as a proxy for evaluating literacy. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in {MWP} for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a {BERT} baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/{JuZXn}.},
	eventtitle = {{EMNLP} 2021},
	pages = {6960--6967},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Thawani, Avijit and Pujara, Jay and Ilievski, Filip},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	urldate = {2024-10-02},
	date = {2021-11},
}

@inproceedings{haagsmaMAGPIELargeCorpus2020,
	location = {Marseille, France},
	title = {{MAGPIE}: A Large Corpus of Potentially Idiomatic Expressions},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.35},
	shorttitle = {{MAGPIE}},
	abstract = {Given the limited size of existing idiom corpora, we aim to enable progress in automatic idiom processing and linguistic analysis by creating the largest-to-date corpus of idioms for English. Using a fixed idiom list, automatic pre-extraction, and a strictly controlled crowdsourced annotation procedure, we show that it is feasible to build a high-quality corpus comprising more than 50K instances, an order of a magnitude larger than previous resources. Crucial ingredients of crowdsourcing were the selection of crowdworkers, clear and comprehensive instructions, and an interface that breaks down the task in small, manageable steps. Analysis of the resulting corpus revealed strong effects of genre on idiom distribution, providing new evidence for existing theories on what influences idiom usage. The corpus also contains rich metadata, and is made publicly available.},
	eventtitle = {{LREC} 2020},
	pages = {279--287},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Haagsma, Hessel and Bos, Johan and Nissim, Malvina},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-10-02},
	date = {2020-05},
}

@article{martinEventRepresentationsAutomated2018,
	title = {Event Representations for Automated Story Generation with Deep Neural Nets},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	url = {http://arxiv.org/abs/1706.01331},
	doi = {10/g6k72p},
	abstract = {Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Martin, Lara J. and Ammanabrolu, Prithviraj and Wang, Xinyu and Hancock, William and Singh, Shruti and Harrison, Brent and Riedl, Mark O.},
	urldate = {2024-10-02},
	date = {2018-04-25},
	eprinttype = {arxiv},
	eprint = {1706.01331 [cs]},
}

@misc{iyerLearningNeuralSemantic2017,
	title = {Learning a Neural Semantic Parser from User Feedback},
	url = {http://arxiv.org/abs/1704.08760},
	abstract = {We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to {SQL} with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of {SQL} facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.},
	number = {{arXiv}:1704.08760},
	publisher = {{arXiv}},
	author = {Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Krishnamurthy, Jayant and Zettlemoyer, Luke},
	urldate = {2024-10-02},
	date = {2017-04-27},
	eprinttype = {arxiv},
	eprint = {1704.08760 [cs]},
}

@misc{haNeuralRepresentationSketch2017,
	title = {A Neural Representation of Sketch Drawings},
	url = {http://arxiv.org/abs/1704.03477},
	abstract = {We present sketch-rnn, a recurrent neural network ({RNN}) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
	number = {{arXiv}:1704.03477},
	publisher = {{arXiv}},
	author = {Ha, David and Eck, Douglas},
	urldate = {2024-10-02},
	date = {2017-05-19},
	eprinttype = {arxiv},
	eprint = {1704.03477 [cs, stat]},
}

@misc{chouldechovaFairPredictionDisparate2017,
	title = {Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
	url = {http://arxiv.org/abs/1703.00056},
	shorttitle = {Fair prediction with disparate impact},
	abstract = {Recidivism prediction instruments ({RPI}'s) provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses several fairness criteria that have recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when a recidivism prediction instrument fails to satisfy the criterion of error rate balance.},
	number = {{arXiv}:1703.00056},
	publisher = {{arXiv}},
	author = {Chouldechova, Alexandra},
	urldate = {2024-10-02},
	date = {2017-02-28},
	eprinttype = {arxiv},
	eprint = {1703.00056 [cs, stat]},
}

@misc{jurczykSelQANewBenchmark2016,
	title = {{SelQA}: A New Benchmark for Selection-based Question Answering},
	url = {http://arxiv.org/abs/1606.08513},
	shorttitle = {{SelQA}},
	abstract = {This paper presents a new selection-based question answering dataset, {SelQA}. The dataset consists of questions generated through crowdsourcing and sentence length answers that are drawn from the ten most prevalent topics in the English Wikipedia. We introduce a corpus annotation scheme that enhances the generation of large, diverse, and challenging datasets by explicitly aiming to reduce word co-occurrences between the question and answers. Our annotation scheme is composed of a series of crowdsourcing tasks with a view to more effectively utilize crowdsourcing in the creation of question answering datasets in various domains. Several systems are compared on the tasks of answer sentence selection and answer triggering, providing strong baseline results for future work to improve upon.},
	number = {{arXiv}:1606.08513},
	publisher = {{arXiv}},
	author = {Jurczyk, Tomasz and Zhai, Michael and Choi, Jinho D.},
	urldate = {2024-10-02},
	date = {2016-10-27},
	eprinttype = {arxiv},
	eprint = {1606.08513 [cs]},
}

@misc{bothaLearningSplitRephrase2018,
	title = {Learning To Split and Rephrase From Wikipedia Edit History},
	url = {http://arxiv.org/abs/1808.09468},
	abstract = {Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia's edit history: {WikiSplit} contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the {WebSplit} corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating {WikiSplit} as training data produces a model with qualitatively better predictions that score 32 {BLEU} points above the prior best result on the {WebSplit} benchmark.},
	number = {{arXiv}:1808.09468},
	publisher = {{arXiv}},
	author = {Botha, Jan A. and Faruqui, Manaal and Alex, John and Baldridge, Jason and Das, Dipanjan},
	urldate = {2024-10-02},
	date = {2018-08-28},
	eprinttype = {arxiv},
	eprint = {1808.09468 [cs]},
}

@misc{kimSemanticSentenceMatching2018,
	title = {Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information},
	url = {http://arxiv.org/abs/1805.11360},
	abstract = {Sentence matching is widely used in various natural language tasks such as natural language inference, paraphrase identification, and question answering. For these tasks, understanding logical and semantic relationship between two sentences is required but it is yet challenging. Although attention mechanism is useful to capture the semantic relationship and to properly align the elements of two sentences, previous methods of attention mechanism simply use a summation operation which does not retain original features enough. Inspired by {DenseNet}, a densely connected convolutional network, we propose a densely-connected co-attentive recurrent neural network, each layer of which uses concatenated information of attentive features as well as hidden features of all the preceding recurrent layers. It enables preserving the original and the co-attentive feature information from the bottommost word embedding layer to the uppermost recurrent layer. To alleviate the problem of an ever-increasing size of feature vectors due to dense concatenation operations, we also propose to use an autoencoder after dense concatenation. We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching. Experimental results show that our architecture, which retains recurrent and attentive features, achieves state-of-the-art performances for most of the tasks.},
	number = {{arXiv}:1805.11360},
	publisher = {{arXiv}},
	author = {Kim, Seonhoon and Kang, Inho and Kwak, Nojun},
	urldate = {2024-10-02},
	date = {2018-11-02},
	eprinttype = {arxiv},
	eprint = {1805.11360 [cs]},
}

@misc{monizMultiSourceSocialFeedback2018,
	title = {Multi-Source Social Feedback of Online News Feeds},
	url = {http://arxiv.org/abs/1801.07055},
	abstract = {The profusion of user generated content caused by the rise of social media platforms has enabled a surge in research relating to fields such as information retrieval, recommender systems, data mining and machine learning. However, the lack of comprehensive baseline data sets to allow a thorough evaluative comparison has become an important issue. In this paper we present a large data set of news items from well-known aggregators such as Google News and Yahoo! News, and their respective social feedback on multiple platforms: Facebook, Google+ and {LinkedIn}. The data collected relates to a period of 8 months, between November 2015 and July 2016, accounting for about 100,000 news items on four different topics: economy, microsoft, obama and palestine. This data set is tailored for evaluative comparisons in predictive analytics tasks, although allowing for tasks in other research areas such as topic detection and tracking, sentiment analysis in short text, first story detection or news recommendation.},
	number = {{arXiv}:1801.07055},
	publisher = {{arXiv}},
	author = {Moniz, Nuno and Torgo, Luís},
	urldate = {2024-10-02},
	date = {2018-01-22},
	eprinttype = {arxiv},
	eprint = {1801.07055 [cs]},
}

@misc{royerXGANUnsupervisedImageImage2018,
	title = {{XGAN}: Unsupervised Image-to-Image Translation for Many-to-Many Mappings},
	url = {http://arxiv.org/abs/1711.05139},
	shorttitle = {{XGAN}},
	abstract = {Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce {XGAN} ("Cross-{GAN}"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions. We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset, {CartoonSet}, we collected for this purpose is publicly available at google.github.io/cartoonset/ as a new benchmark for semantic style transfer.},
	number = {{arXiv}:1711.05139},
	publisher = {{arXiv}},
	author = {Royer, Amélie and Bousmalis, Konstantinos and Gouws, Stephan and Bertsch, Fred and Mosseri, Inbar and Cole, Forrester and Murphy, Kevin},
	urldate = {2024-10-02},
	date = {2018-07-10},
	eprinttype = {arxiv},
	eprint = {1711.05139 [cs]},
}

@misc{welblConstructingDatasetsMultihop2018,
	title = {Constructing Datasets for Multi-hop Reading Comprehension Across Documents},
	url = {http://arxiv.org/abs/1710.06481},
	abstract = {Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence - effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9\% compared to human performance at 74.0\% - leaving ample room for improvement.},
	number = {{arXiv}:1710.06481},
	publisher = {{arXiv}},
	author = {Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
	urldate = {2024-10-02},
	date = {2018-06-11},
	eprinttype = {arxiv},
	eprint = {1710.06481 [cs]},
}

@misc{tafjordQuaRelDatasetModels2018,
	title = {{QuaRel}: A Dataset and Models for Answering Questions about Qualitative Relationships},
	url = {http://arxiv.org/abs/1811.08048},
	shorttitle = {{QuaRel}},
	abstract = {Many natural language questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable challenges. We present {QuaRel}, a dataset of diverse story questions involving qualitative relationships that characterize these challenges, and techniques that begin to address them. The dataset has 2771 questions relating 19 different types of quantities. For example, "Jenny observes that the robot vacuum cleaner moves slower on the living room carpet than on the bedroom carpet. Which carpet has more friction?" We contribute (1) a simple and flexible conceptual framework for representing these kinds of questions; (2) the {QuaRel} dataset, including logical forms, exemplifying the parsing challenges; and (3) two novel models for this task, built as extensions of type-constrained semantic parsing. The first of these models (called {QuaSP}+) significantly outperforms off-the-shelf tools on {QuaRel}. The second ({QuaSP}+Zero) demonstrates zero-shot capability, i.e., the ability to handle new qualitative relationships without requiring additional training data, something not possible with previous models. This work thus makes inroads into answering complex, qualitative questions that require reasoning, and scaling to new relationships at low cost. The dataset and models are available at http://data.allenai.org/quarel.},
	number = {{arXiv}:1811.08048},
	publisher = {{arXiv}},
	author = {Tafjord, Oyvind and Clark, Peter and Gardner, Matt and Yih, Wen-tau and Sabharwal, Ashish},
	urldate = {2024-10-02},
	date = {2018-11-19},
	eprinttype = {arxiv},
	eprint = {1811.08048 [cs]},
}

@incollection{cetoliNamedEntityDisambiguation2019,
	title = {Named Entity Disambiguation using Deep Learning on Graphs},
	volume = {11438},
	url = {http://arxiv.org/abs/1810.09164},
	abstract = {We tackle {\textbackslash}ac\{{NED}\} by comparing entities in short sentences with {\textbackslash}wikidata\{\} graphs. Creating a context vector from graphs through deep learning is a challenging problem that has never been applied to {\textbackslash}ac\{{NED}\}. Our main contribution is to present an experimental study of recent neural techniques, as well as a discussion about which graph features are most important for the disambiguation task. In addition, a new dataset ({\textbackslash}wikidatadisamb\{\}) is created to allow a clean and scalable evaluation of {\textbackslash}ac\{{NED}\} with {\textbackslash}wikidata\{\} entries, and to be used as a reference in future research. In the end our results show that a {\textbackslash}ac\{Bi-{LSTM}\} encoding of the graph triplets performs best, improving upon the baseline models and scoring an {\textbackslash}rm\{F1\} value of \$91.6{\textbackslash}\%\$ on the {\textbackslash}wikidatadisamb\{\} test set},
	pages = {78--86},
	author = {Cetoli, Alberto and Akbari, Mohammad and Bragaglia, Stefano and O'Harney, Andrew D. and Sloan, Marc},
	urldate = {2024-10-02},
	date = {2019},
	doi = {10.1007/978-3-030-15719-7_10},
	eprinttype = {arxiv},
	eprint = {1810.09164 [cs]},
}

@misc{saeidiInterpretationNaturalLanguage2018,
	title = {Interpretation of Natural Language Rules in Conversational Machine Reading},
	url = {http://arxiv.org/abs/1809.01494},
	abstract = {Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. One example is the task of interpreting regulations to answer "Can I...?" or "Do I have to...?" questions such as "I am working in Canada. Do I have to carry on paying {UK} National Insurance?" after reading a {UK} government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as "How long have you been working abroad?" when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.},
	number = {{arXiv}:1809.01494},
	publisher = {{arXiv}},
	author = {Saeidi, Marzieh and Bartolo, Max and Lewis, Patrick and Singh, Sameer and Rocktäschel, Tim and Sheldon, Mike and Bouchard, Guillaume and Riedel, Sebastian},
	urldate = {2024-10-02},
	date = {2018-08-28},
	eprinttype = {arxiv},
	eprint = {1809.01494 [cs, stat]},
}

@misc{liNeuralCodeSearch2019,
	title = {Neural Code Search Evaluation Dataset},
	url = {http://arxiv.org/abs/1908.09804},
	abstract = {There has been an increase of interest in code search using natural language. Assessing the performance of such code search models can be difficult without a readily available evaluation suite. In this paper, we present an evaluation dataset consisting of natural language query and code snippet pairs, with the hope that future work in this area can use this dataset as a common benchmark. We also provide the results of two code search models ([1] and [6]) from recent work. The evaluation dataset is available at https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset},
	number = {{arXiv}:1908.09804},
	publisher = {{arXiv}},
	author = {Li, Hongyu and Kim, Seohyun and Chandra, Satish},
	urldate = {2024-10-02},
	date = {2019-10-01},
	eprinttype = {arxiv},
	eprint = {1908.09804 [cs]},
}

@misc{balakrishnanConstrainedDecodingNeural2019,
	title = {Constrained Decoding for Neural {NLG} from Compositional Representations in Task-Oriented Dialogue},
	url = {http://arxiv.org/abs/1906.07220},
	abstract = {Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E {NLG} Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based {NLG} systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.},
	number = {{arXiv}:1906.07220},
	publisher = {{arXiv}},
	author = {Balakrishnan, Anusha and Rao, Jinfeng and Upasani, Kartikeya and White, Michael and Subba, Rajen},
	urldate = {2024-10-02},
	date = {2019-06-17},
	eprinttype = {arxiv},
	eprint = {1906.07220 [cs]},
}

@misc{perez-beltrachiniGeneratingSummariesTopic2019,
	title = {Generating Summaries with Topic Templates and Structured Convolutional Decoders},
	url = {http://arxiv.org/abs/1906.04687},
	abstract = {Existing neural generation approaches create multi-sentence text as a single sequence. In this paper we propose a structured convolutional decoder that is guided by the content structure of target summaries. We compare our model with existing sequential decoders on three data sets representing different domains. Automatic and human evaluation demonstrate that our summaries have better content coverage.},
	number = {{arXiv}:1906.04687},
	publisher = {{arXiv}},
	author = {Perez-Beltrachini, Laura and Liu, Yang and Lapata, Mirella},
	urldate = {2024-10-02},
	date = {2019-06-11},
	eprinttype = {arxiv},
	eprint = {1906.04687 [cs]},
}

@misc{chalkidisNeuralLegalJudgment2019,
	title = {Neural Legal Judgment Prediction in English},
	url = {http://arxiv.org/abs/1906.02059},
	abstract = {Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case's facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of {BERT}, which bypasses {BERT}'s length limitation.},
	number = {{arXiv}:1906.02059},
	publisher = {{arXiv}},
	author = {Chalkidis, Ilias and Androutsopoulos, Ion and Aletras, Nikolaos},
	urldate = {2024-10-02},
	date = {2019-06-05},
	eprinttype = {arxiv},
	eprint = {1906.02059 [cs]},
}

@misc{sharmaBIGPATENTLargeScaleDataset2019,
	title = {{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization},
	url = {http://arxiv.org/abs/1906.03741},
	shorttitle = {{BIGPATENT}},
	abstract = {Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article's global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, {BIGPATENT}, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, {BIGPATENT} has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on {BIGPATENT} to shed light on new challenges and motivate future directions for summarization research.},
	number = {{arXiv}:1906.03741},
	publisher = {{arXiv}},
	author = {Sharma, Eva and Li, Chen and Wang, Lu},
	urldate = {2024-10-02},
	date = {2019-06-09},
	eprinttype = {arxiv},
	eprint = {1906.03741 [cs]},
}

@inproceedings{christmannLookYouHop2019,
	title = {Look before you Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion},
	url = {http://arxiv.org/abs/1910.03262},
	doi = {10/gkz233},
	shorttitle = {Look before you Hop},
	abstract = {Fact-centric information needs are rarely one-shot; users typically ask follow-up questions to explore a topic. In such a conversational setting, the user's inputs are often incomplete, with entities or predicates left out, and ungrammatical phrases. This poses a huge challenge to question answering ({QA}) systems that typically rely on cues in full-fledged interrogative sentences. As a solution, we develop {CONVEX}: an unsupervised method that can answer incomplete questions over a knowledge graph ({KG}) by maintaining conversation context using entities and predicates seen so far and automatically inferring missing or ambiguous pieces for follow-up questions. The core of our method is a graph exploration algorithm that judiciously expands a frontier to find candidate answers for the current question. To evaluate {CONVEX}, we release {ConvQuestions}, a crowdsourced benchmark with 11,200 distinct conversations from five different domains. We show that {CONVEX}: (i) adds conversational support to any stand-alone {QA} system, and (ii) outperforms state-of-the-art baselines and question completion strategies.},
	pages = {729--738},
	booktitle = {Proceedings of the 28th {ACM} International Conference on Information and Knowledge Management},
	author = {Christmann, Philipp and Roy, Rishiraj Saha and Abujabal, Abdalghani and Singh, Jyotsna and Weikum, Gerhard},
	urldate = {2024-10-02},
	date = {2019-11-03},
	eprinttype = {arxiv},
	eprint = {1910.03262 [cs]},
}

@misc{kaushikLearningDifferenceThat2020,
	title = {Learning the Difference that Makes a Difference with Counterfactually-Augmented Data},
	url = {http://arxiv.org/abs/1909.12434},
	abstract = {Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.},
	number = {{arXiv}:1909.12434},
	publisher = {{arXiv}},
	author = {Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C.},
	urldate = {2024-10-02},
	date = {2020-02-14},
	eprinttype = {arxiv},
	eprint = {1909.12434 [cs, stat]},
}

@misc{juraskaViGGOVideoGame2019,
	title = {{ViGGO}: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation},
	url = {http://arxiv.org/abs/1910.12129},
	shorttitle = {{ViGGO}},
	abstract = {The uptake of deep learning in natural language generation ({NLG}) led to the release of both small and relatively large parallel corpora for training neural models. The existing data-to-text datasets are, however, aimed at task-oriented dialogue systems, and often thus limited in diversity and versatility. They are typically crowdsourced, with much of the noise left in them. Moreover, current neural {NLG} models do not take full advantage of large training data, and due to their strong generalizing properties produce sentences that look template-like regardless. We therefore present a new corpus of 7K samples, which (1) is clean despite being crowdsourced, (2) has utterances of 9 generalizable and conversational dialogue act types, making it more suitable for open-domain dialogue systems, and (3) explores the domain of video games, which is new to dialogue systems despite having excellent potential for supporting rich conversations.},
	number = {{arXiv}:1910.12129},
	publisher = {{arXiv}},
	author = {Juraska, Juraj and Bowden, Kevin K. and Walker, Marilyn},
	urldate = {2024-10-02},
	date = {2019-10-26},
	eprinttype = {arxiv},
	eprint = {1910.12129 [cs]},
}

@misc{zhouDevignEffectiveVulnerability2019,
	title = {Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks},
	url = {http://arxiv.org/abs/1909.03496},
	shorttitle = {Devign},
	abstract = {Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51\% higher accuracy and 8.68{\textbackslash}\% F1 score, increases averagely 4.66\% accuracy and 6.37\% F1 by the Conv module.},
	number = {{arXiv}:1909.03496},
	publisher = {{arXiv}},
	author = {Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
	urldate = {2024-10-02},
	date = {2019-09-08},
	eprinttype = {arxiv},
	eprint = {1909.03496 [cs, stat]},
}

@misc{petroniLanguageModelsKnowledge2019,
	title = {Language Models as Knowledge Bases?},
	url = {http://arxiv.org/abs/1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream {NLP} tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, {BERT} contains relational knowledge competitive with traditional {NLP} methods that have some access to oracle knowledge, (ii) {BERT} also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain {QA} systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/{LAMA}.},
	number = {{arXiv}:1909.01066},
	publisher = {{arXiv}},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	urldate = {2024-10-02},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1909.01066 [cs]},
}

@article{guDomainSpecificLanguageModel2022,
	title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
	volume = {3},
	issn = {2691-1957, 2637-8051},
	url = {http://arxiv.org/abs/2007.15779},
	doi = {10/gnmkjx},
	abstract = {Pretraining large neural language models, such as {BERT}, has led to impressive gains on many natural language processing ({NLP}) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical {NLP} benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical {NLP} tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with {BERT} models, such as using complex tagging schemes in named entity recognition ({NER}). To help accelerate research in biomedical {NLP}, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our {BLURB} benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/{BLURB}.},
	pages = {1--23},
	number = {1},
	journaltitle = {{ACM} Transactions on Computing for Healthcare},
	shortjournal = {{ACM} Trans. Comput. Healthcare},
	author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	urldate = {2024-10-02},
	date = {2022-01-31},
	eprinttype = {arxiv},
	eprint = {2007.15779 [cs]},
}

@misc{caoKQAProDataset2022,
	title = {{KQA} Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base},
	url = {http://arxiv.org/abs/2007.03875},
	shorttitle = {{KQA} Pro},
	abstract = {Complex question answering over knowledge base (Complex {KBQA}) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation. Existing benchmarks have some shortcomings that limit the development of Complex {KBQA}: 1) they only provide {QA} pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce {KQA} Pro, a dataset for Complex {KBQA} including {\textasciitilde}120K diverse natural language questions. We introduce a compositional and interpretable programming language {KoPL} to represent the reasoning process of complex questions. For each question, we provide the corresponding {KoPL} program and {SPARQL} query, so that {KQA} Pro serves for both {KBQA} and semantic parsing tasks. Experimental results show that {SOTA} {KBQA} methods cannot achieve promising results on {KQA} Pro as on current datasets, which suggests that {KQA} Pro is challenging and Complex {KBQA} requires further research efforts. We also treat {KQA} Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex {KBQA}. Our codes and datasets can be obtained from https://github.com/shijx12/{KQAPro}\_Baselines.},
	number = {{arXiv}:2007.03875},
	publisher = {{arXiv}},
	author = {Cao, Shulin and Shi, Jiaxin and Pan, Liangming and Nie, Lunyiu and Xiang, Yutong and Hou, Lei and Li, Juanzi and He, Bin and Zhang, Hanwang},
	urldate = {2024-10-02},
	date = {2022-06-23},
	eprinttype = {arxiv},
	eprint = {2007.03875 [cs]},
}

@misc{brownLanguageModelsAre2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2024-10-02},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165 [cs]},
}

@misc{ardanuyLivingMachinesStudy2020,
	title = {Living Machines: A study of atypical animacy},
	url = {http://arxiv.org/abs/2005.11140},
	shorttitle = {Living Machines},
	abstract = {This paper proposes a new approach to animacy detection, the task of determining whether an entity is represented as animate in a text. In particular, this work is focused on atypical animacy and examines the scenario in which typically inanimate objects, specifically machines, are given animate attributes. To address it, we have created the first dataset for atypical animacy detection, based on nineteenth-century sentences in English, with machines represented as either animate or inanimate. Our method builds on recent innovations in language modeling, specifically {BERT} contextualized word embeddings, to better capture fine-grained contextual properties of words. We present a fully unsupervised pipeline, which can be easily adapted to different contexts, and report its performance on an established animacy dataset and our newly introduced resource. We show that our method provides a substantially more accurate characterization of atypical animacy, especially when applied to highly complex forms of language use.},
	number = {{arXiv}:2005.11140},
	publisher = {{arXiv}},
	author = {Ardanuy, Mariona Coll and Nanni, Federico and Beelen, Kaspar and Hosseini, Kasra and Ahnert, Ruth and Lawrence, Jon and {McDonough}, Katherine and Tolfo, Giorgia and Wilson, Daniel {CS} and {McGillivray}, Barbara},
	urldate = {2024-10-02},
	date = {2020-11-19},
	eprinttype = {arxiv},
	eprint = {2005.11140 [cs]},
}

@misc{wangDetectingCodeClones2020,
	title = {Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract Syntax Tree},
	url = {http://arxiv.org/abs/2002.08653},
	abstract = {Code clones are semantically similar code fragments pairs that are syntactically similar or different. Detection of code clones can help to reduce the cost of software maintenance and prevent bugs. Numerous approaches of detecting code clones have been proposed previously, but most of them focus on detecting syntactic clones and do not work well on semantic clones with different syntactic features. To detect semantic clones, researchers have tried to adopt deep learning for code clone detection to automatically learn latent semantic features from data. Especially, to leverage grammar information, several approaches used abstract syntax trees ({AST}) as input and achieved significant progress on code clone benchmarks in various programming languages. However, these {AST}-based approaches still can not fully leverage the structural information of code fragments, especially semantic information such as control flow and data flow. To leverage control and data flow information, in this paper, we build a graph representation of programs called flow-augmented abstract syntax tree ({FA}-{AST}). We construct {FA}-{AST} by augmenting original {ASTs} with explicit control and data flow edges. Then we apply two different types of graph neural networks ({GNN}) on {FA}-{AST} to measure the similarity of code pairs. As far as we have concerned, we are the first to apply graph neural networks on the domain of code clone detection. We apply our {FA}-{AST} and graph neural networks on two Java datasets: Google Code Jam and {BigCloneBench}. Our approach outperforms the state-of-the-art approaches on both Google Code Jam and {BigCloneBench} tasks.},
	number = {{arXiv}:2002.08653},
	publisher = {{arXiv}},
	author = {Wang, Wenhan and Li, Ge and Ma, Bo and Xia, Xin and Jin, Zhi},
	urldate = {2024-10-02},
	date = {2020-02-20},
	eprinttype = {arxiv},
	eprint = {2002.08653 [cs]},
}

@misc{kanadeLearningEvaluatingContextual2020,
	title = {Learning and Evaluating Contextual Embedding of Source Code},
	url = {http://arxiv.org/abs/2001.00059},
	abstract = {Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as {BERT}, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from {GitHub}, which we use to pre-train {CuBERT}, an open-sourced code-understanding {BERT} model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune {CuBERT} on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, {BiLSTM} and Transformer models, as well as published state-of-the-art models, showing that {CuBERT} outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against {CuBERT} models as a strong baseline.},
	number = {{arXiv}:2001.00059},
	publisher = {{arXiv}},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	urldate = {2024-10-02},
	date = {2020-08-17},
	eprinttype = {arxiv},
	eprint = {2001.00059 [cs]},
}

@misc{tangRapidlyBootstrappingQuestion2020,
	title = {Rapidly Bootstrapping a Question Answering Dataset for {COVID}-19},
	url = {http://arxiv.org/abs/2004.11339},
	abstract = {We present {CovidQA}, the beginnings of a question answering dataset specifically designed for {COVID}-19, built by hand from knowledge gathered from Kaggle's {COVID}-19 Open Research Dataset Challenge. To our knowledge, this is the first publicly available resource of its type, and intended as a stopgap measure for guiding research until more substantial evaluation resources become available. While this dataset, comprising 124 question-article pairs as of the present version 0.1 release, does not have sufficient examples for supervised machine learning, we believe that it can be helpful for evaluating the zero-shot or transfer capabilities of existing models on topics specifically related to {COVID}-19. This paper describes our methodology for constructing the dataset and presents the effectiveness of a number of baselines, including term-based techniques and various transformer-based models. The dataset is available at http://covidqa.ai/},
	number = {{arXiv}:2004.11339},
	publisher = {{arXiv}},
	author = {Tang, Raphael and Nogueira, Rodrigo and Zhang, Edwin and Gupta, Nikhil and Cam, Phuong and Cho, Kyunghyun and Lin, Jimmy},
	urldate = {2024-10-02},
	date = {2020-04-23},
	eprinttype = {arxiv},
	eprint = {2004.11339 [cs]},
}

@misc{devarajParagraphlevelSimplificationMedical2021,
	title = {Paragraph-level Simplification of Medical Texts},
	url = {http://arxiv.org/abs/2104.05767},
	abstract = {We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing "jargon" terms; we find that this yields improvements over baselines in terms of readability.},
	number = {{arXiv}:2104.05767},
	publisher = {{arXiv}},
	author = {Devaraj, Ashwin and Marshall, Iain J. and Wallace, Byron C. and Li, Junyi Jessy},
	urldate = {2024-10-02},
	date = {2021-04-12},
	eprinttype = {arxiv},
	eprint = {2104.05767 [cs]},
}

@misc{chalkidisRegulatoryComplianceDoc2Doc2021,
	title = {Regulatory Compliance through Doc2Doc Information Retrieval: A case study in {EU}/{UK} legislation where text similarity has limitations},
	url = {http://arxiv.org/abs/2101.10726},
	shorttitle = {Regulatory Compliance through Doc2Doc Information Retrieval},
	abstract = {Major scandals in corporate history have urged the need for regulatory compliance, where organizations need to ensure that their controls (processes) comply with relevant laws, regulations, and policies. However, keeping track of the constantly changing legislation is difficult, thus organizations are increasingly adopting Regulatory Technology ({RegTech}) to facilitate the process. To this end, we introduce regulatory information retrieval ({REG}-{IR}), an application of document-to-document information retrieval ({DOC}2DOC {IR}), where the query is an entire document making the task more challenging than traditional {IR} where the queries are short. Furthermore, we compile and release two datasets based on the relationships between {EU} directives and {UK} legislation. We experiment on these datasets using a typical two-step pipeline approach comprising a pre-fetcher and a neural re-ranker. Experimenting with various pre-fetchers from {BM}25 to k nearest neighbors over representations from several {BERT} models, we show that fine-tuning a {BERT} model on an in-domain classification task produces the best representations for {IR}. We also show that neural re-rankers under-perform due to contradicting supervision, i.e., similar query-document pairs with opposite labels. Thus, they are biased towards the pre-fetcher's score. Interestingly, applying a date filter further improves the performance, showcasing the importance of the time dimension.},
	number = {{arXiv}:2101.10726},
	publisher = {{arXiv}},
	author = {Chalkidis, Ilias and Fergadiotis, Manos and Manginas, Nikolaos and Katakalou, Eva and Malakasiotis, Prodromos},
	urldate = {2024-10-02},
	date = {2021-01-26},
	eprinttype = {arxiv},
	eprint = {2101.10726 [cs]},
}

@misc{chenFindingFriendsFlipping2020,
	title = {Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset Augmentation Using Graph Theory},
	url = {http://arxiv.org/abs/2011.01856},
	shorttitle = {Finding Friends and Flipping Frenemies},
	abstract = {Most {NLP} datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from the provided sentence pair labels, and create an augmented dataset by directly inferring labels from the original sentence pairs using a transitivity property. We use structural balance theory to identify likely mislabelings in the graph, and flip their labels. We evaluate our methods on paraphrase models trained using these datasets starting from a pretrained {BERT} model, and find that the automatically-enhanced training sets result in more accurate models.},
	number = {{arXiv}:2011.01856},
	publisher = {{arXiv}},
	author = {Chen, Hannah and Ji, Yangfeng and Evans, David},
	urldate = {2024-10-02},
	date = {2020-11-03},
	eprinttype = {arxiv},
	eprint = {2011.01856 [cs]},
}

@inproceedings{nguyenAdvancedSemanticsCommonsense2021,
	title = {Advanced Semantics for Commonsense Knowledge Extraction},
	url = {http://arxiv.org/abs/2011.00905},
	doi = {10/gnnffn},
	abstract = {Commonsense knowledge ({CSK}) about concepts and their properties is useful for {AI} applications such as robust chatbots. Prior works like {ConceptNet}, {TupleKB} and others compiled large {CSK} collections, but are restricted in their expressiveness to subject-predicate-object ({SPO}) triples with simple concepts for S and monolithic strings for P and O. Also, these projects have either prioritized precision or recall, but hardly reconcile these complementary goals. This paper presents a methodology, called Ascent, to automatically build a large-scale knowledge base ({KB}) of {CSK} assertions, with advanced expressiveness and both better precision and recall than prior works. Ascent goes beyond triples by capturing composite concepts with subgroups and aspects, and by refining assertions with semantic facets. The latter are important to express temporal and spatial validity of assertions and further qualifiers. Ascent combines open information extraction with judicious cleaning using language models. Intrinsic evaluation shows the superior size and quality of the Ascent {KB}, and an extrinsic evaluation for {QA}-support tasks underlines the benefits of Ascent. A web interface, data and code can be found at https://ascent.mpi-inf.mpg.de/.},
	pages = {2636--2647},
	booktitle = {Proceedings of the Web Conference 2021},
	author = {Nguyen, Tuan-Phong and Razniewski, Simon and Weikum, Gerhard},
	urldate = {2024-10-02},
	date = {2021-04-19},
	eprinttype = {arxiv},
	eprint = {2011.00905 [cs]},
}

@misc{schulzBiomedicalConceptRelatedness2020,
	title = {Biomedical Concept Relatedness -- A large {EHR}-based benchmark},
	url = {http://arxiv.org/abs/2010.16218},
	abstract = {A promising application of {AI} to healthcare is the retrieval of information from electronic health records ({EHRs}), e.g. to aid clinicians in finding relevant information for a consultation or to recruit suitable patients for a study. This requires search capabilities far beyond simple string matching, including the retrieval of concepts (diagnoses, symptoms, medications, etc.) related to the one in question. The suitability of {AI} methods for such applications is tested by predicting the relatedness of concepts with known relatedness scores. However, all existing biomedical concept relatedness datasets are notoriously small and consist of hand-picked concept pairs. We open-source a novel concept relatedness benchmark overcoming these issues: it is six times larger than existing datasets and concept pairs are chosen based on co-occurrence in {EHRs}, ensuring their relevance for the application of interest. We present an in-depth analysis of our new dataset and compare it to existing ones, highlighting that it is not only larger but also complements existing datasets in terms of the types of concepts included. Initial experiments with state-of-the-art embedding methods show that our dataset is a challenging new benchmark for testing concept relatedness models.},
	number = {{arXiv}:2010.16218},
	publisher = {{arXiv}},
	author = {Schulz, Claudia and Levy-Kramer, Josh and Van Assel, Camille and Kepes, Miklos and Hammerla, Nils},
	urldate = {2024-10-02},
	date = {2020-10-30},
	eprinttype = {arxiv},
	eprint = {2010.16218 [cs]},
}

@misc{williamsANLIzingAdversarialNatural2020,
	title = {{ANLIzing} the Adversarial Natural Language Inference Dataset},
	url = {http://arxiv.org/abs/2010.12729},
	abstract = {We perform an in-depth error analysis of Adversarial {NLI} ({ANLI}), a recently introduced large-scale human-and-model-in-the-loop natural language inference dataset collected over multiple rounds. We propose a fine-grained annotation scheme of the different aspects of inference that are responsible for the gold classification labels, and use it to hand-code all three of the {ANLI} development sets. We use these annotations to answer a variety of interesting questions: which inference types are most common, which models have the highest performance on each reasoning type, and which types are the most challenging for state of-the-art models? We hope that our annotations will enable more fine-grained evaluation of models trained on {ANLI}, provide us with a deeper understanding of where models fail and succeed, and help us determine how to train better models in future.},
	number = {{arXiv}:2010.12729},
	publisher = {{arXiv}},
	author = {Williams, Adina and Thrush, Tristan and Kiela, Douwe},
	urldate = {2024-10-02},
	date = {2020-10-23},
	eprinttype = {arxiv},
	eprint = {2010.12729 [cs]},
}

@misc{agarwalKnowledgeGraphBased2021,
	title = {Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training},
	url = {http://arxiv.org/abs/2010.12688},
	abstract = {Prior work on Data-To-Text Generation, the task of converting knowledge graph ({KG}) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata {KG}, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic {KG} like Wikidata can be used to integrate structured {KGs} and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the {KG} into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain {QA} and the {LAMA} knowledge probe.},
	number = {{arXiv}:2010.12688},
	publisher = {{arXiv}},
	author = {Agarwal, Oshin and Ge, Heming and Shakeri, Siamak and Al-Rfou, Rami},
	urldate = {2024-10-02},
	date = {2021-03-13},
	eprinttype = {arxiv},
	eprint = {2010.12688 [cs]},
}

@misc{diggelmannCLIMATEFEVERDatasetVerification2021,
	title = {{CLIMATE}-{FEVER}: A Dataset for Verification of Real-World Climate Claims},
	url = {http://arxiv.org/abs/2012.00614},
	shorttitle = {{CLIMATE}-{FEVER}},
	abstract = {We introduce {CLIMATE}-{FEVER}, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of {FEVER} [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the {\textbackslash}textsc\{fever\} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and {AI} community.},
	number = {{arXiv}:2012.00614},
	publisher = {{arXiv}},
	author = {Diggelmann, Thomas and Boyd-Graber, Jordan and Bulian, Jannis and Ciaramita, Massimiliano and Leippold, Markus},
	urldate = {2024-10-02},
	date = {2021-01-02},
	eprinttype = {arxiv},
	eprint = {2012.00614 [cs]},
}

@misc{jiangHoVerDatasetManyHop2020,
	title = {{HoVer}: A Dataset for Many-Hop Fact Extraction And Claim Verification},
	url = {http://arxiv.org/abs/2011.03088},
	shorttitle = {{HoVer}},
	abstract = {We introduce {HoVer} ({HOppy} {VERification}), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is Supported or Not-Supported by the facts. In {HoVer}, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes. Moreover, most of the 3/4-hop claims are written in multiple sentences, which adds to the complexity of understanding long-range dependency relations such as coreference. We show that the performance of an existing state-of-the-art semantic-matching model degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of many-hop reasoning to achieve strong results. We hope that the introduction of this challenging dataset and the accompanying evaluation task will encourage research in many-hop fact retrieval and information verification. We make the {HoVer} dataset publicly available at https://hover-nlp.github.io},
	number = {{arXiv}:2011.03088},
	publisher = {{arXiv}},
	author = {Jiang, Yichen and Bordia, Shikha and Zhong, Zheng and Dognin, Charles and Singh, Maneesh and Bansal, Mohit},
	urldate = {2024-10-02},
	date = {2020-11-15},
	eprinttype = {arxiv},
	eprint = {2011.03088 [cs]},
}

@misc{chapuisHierarchicalPretrainingSequence2021,
	title = {Hierarchical Pre-training for Sequence Labelling in Spoken Dialog},
	url = {http://arxiv.org/abs/2009.11152},
	abstract = {Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new benchmark we call Sequence {labellIng} {evaLuatIon} {benChmark} {fOr} spoken {laNguagE} benchmark ({\textbackslash}texttt\{{SILICONE}\}). {\textbackslash}texttt\{{SILICONE}\} is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pre-training objectives. Pre-training is performed on {OpenSubtitles}: a large corpus of spoken dialog containing over \$2.3\$ billion of tokens. We demonstrate how hierarchical encoders achieve competitive results with consistently fewer parameters compared to state-of-the-art models and we show their importance for both pre-training and fine-tuning.},
	number = {{arXiv}:2009.11152},
	publisher = {{arXiv}},
	author = {Chapuis, Emile and Colombo, Pierre and Manica, Matteo and Labeau, Matthieu and Clavel, Chloe},
	urldate = {2024-10-02},
	date = {2021-02-08},
	eprinttype = {arxiv},
	eprint = {2009.11152 [cs]},
}

@misc{sanhMultitaskPromptedTraining2022,
	title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
	url = {http://arxiv.org/abs/2110.08207},
	abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the {BIG}-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
	number = {{arXiv}:2110.08207},
	publisher = {{arXiv}},
	author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
	urldate = {2024-10-02},
	date = {2022-03-17},
	eprinttype = {arxiv},
	eprint = {2110.08207 [cs]},
}

@misc{paikWorldOctopusHow2021,
	title = {The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color},
	url = {http://arxiv.org/abs/2110.08182},
	shorttitle = {The World of an Octopus},
	abstract = {Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset ({CoDa}), a dataset of human-perceived color distributions for 521 common objects; 2) use {CoDa} to analyze and compare the color distribution found in text, the distribution captured by language models, and a human's perception of color; and 3) investigate the performance differences between text-only and multimodal models on {CoDa}. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.},
	number = {{arXiv}:2110.08182},
	publisher = {{arXiv}},
	author = {Paik, Cory and Aroca-Ouellette, Stéphane and Roncone, Alessandro and Kann, Katharina},
	urldate = {2024-10-02},
	date = {2021-10-15},
	eprinttype = {arxiv},
	eprint = {2110.08182 [cs]},
}

@misc{adlakhaTopiOCQAOpendomainConversational2022,
	title = {{TopiOCQA}: Open-domain Conversational Question Answering with Topic Switching},
	url = {http://arxiv.org/abs/2110.00768},
	shorttitle = {{TopiOCQA}},
	abstract = {In a conversational question answering scenario, a questioner seeks to extract information about a topic through a series of interdependent questions and answers. As the conversation progresses, they may switch to related topics, a phenomenon commonly observed in information-seeking search sessions. However, current datasets for conversational question answering are limiting in two ways: 1) they do not contain topic switches; and 2) they assume the reference text for the conversation is given, i.e., the setting is not open-domain. We introduce {TopiOCQA} (pronounced Tapioca), an open-domain conversational dataset with topic switches on Wikipedia. {TopiOCQA} contains 3,920 conversations with information-seeking questions and free-form answers. On average, a conversation in our dataset spans 13 question-answer turns and involves four topics (documents). {TopiOCQA} poses a challenging test-bed for models, where efficient retrieval is required on multiple turns of the same conversation, in conjunction with constructing valid responses using conversational history. We evaluate several baselines, by combining state-of-the-art document retrieval methods with neural reader models. Our best model achieves F1 of 55.8, falling short of human performance by 14.2 points, indicating the difficulty of our dataset. Our dataset and code is available at https://mcgill-nlp.github.io/topiocqa},
	number = {{arXiv}:2110.00768},
	publisher = {{arXiv}},
	author = {Adlakha, Vaibhav and Dhuliawala, Shehzaad and Suleman, Kaheer and de Vries, Harm and Reddy, Siva},
	urldate = {2024-10-02},
	date = {2022-02-20},
	eprinttype = {arxiv},
	eprint = {2110.00768 [cs]},
}

@inproceedings{fengMultiDoc2DialModelingDialogues2021,
	title = {{MultiDoc}2Dial: Modeling Dialogues Grounded in Multiple Documents},
	url = {http://arxiv.org/abs/2109.12595},
	doi = {10/g6k938},
	shorttitle = {{MultiDoc}2Dial},
	abstract = {We propose {MultiDoc}2Dial, a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as a machine reading comprehension task based on a single given document or passage. In this work, we aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents. To facilitate such a task, we introduce a new dataset that contains dialogues grounded in multiple documents from four different domains. We also explore modeling the dialogue-based and document-based context in the dataset. We present strong baseline approaches and various experimental results, aiming to support further research efforts on such a task.},
	pages = {6162--6176},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	author = {Feng, Song and Patel, Siva Sankalp and Wan, Hui and Joshi, Sachindra},
	urldate = {2024-10-02},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2109.12595 [cs]},
}

@article{ullrichCsFEVERCTKFactsAcquiring2023,
	title = {{CsFEVER} and {CTKFacts}: Acquiring Czech data for fact verification},
	volume = {57},
	issn = {1574-020X, 1574-0218},
	url = {http://arxiv.org/abs/2201.11115},
	doi = {10/g6k95h},
	shorttitle = {{CsFEVER} and {CTKFacts}},
	abstract = {In this paper, we examine several methods of acquiring Czech data for automated fact-checking, which is a task commonly modeled as a classification of textual claim veracity w.r.t. a corpus of trusted ground truths. We attempt to collect sets of data in form of a factual claim, evidence within the ground truth corpus, and its veracity label (supported, refuted or not enough info). As a first attempt, we generate a Czech version of the large-scale {FEVER} dataset built on top of Wikipedia corpus. We take a hybrid approach of machine translation and document alignment; the approach and the tools we provide can be easily applied to other languages. We discuss its weaknesses and inaccuracies, propose a future approach for their cleaning and publish the 127k resulting translations, as well as a version of such dataset reliably applicable for the Natural Language Inference task - the {CsFEVER}-{NLI}. Furthermore, we collect a novel dataset of 3,097 claims, which is annotated using the corpus of 2.2M articles of Czech News Agency. We present its extended annotation methodology based on the {FEVER} approach, and, as the underlying corpus is kept a trade secret, we also publish a standalone version of the dataset for the task of Natural Language Inference we call {CTKFactsNLI}. We analyze both acquired datasets for spurious cues - annotation patterns leading to model overfitting. {CTKFacts} is further examined for inter-annotator agreement, thoroughly cleaned, and a typology of common annotator errors is extracted. Finally, we provide baseline models for all stages of the fact-checking pipeline and publish the {NLI} datasets, as well as our annotation platform and other experimental data.},
	pages = {1571--1605},
	number = {4},
	journaltitle = {Language Resources and Evaluation},
	shortjournal = {Lang Resources \& Evaluation},
	author = {Ullrich, Herbert and Drchal, Jan and Rýpar, Martin and Vincourová, Hana and Moravec, Václav},
	urldate = {2024-10-02},
	date = {2023-12},
	eprinttype = {arxiv},
	eprint = {2201.11115 [cs]},
}

@misc{wangAdversarialGLUEMultiTask2022,
	title = {Adversarial {GLUE}: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
	url = {http://arxiv.org/abs/2111.02840},
	shorttitle = {Adversarial {GLUE}},
	abstract = {Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding ({NLU}) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial {GLUE} ({AdvGLUE}), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to {GLUE} tasks to construct {AdvGLUE}, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90\% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on {AdvGLUE}, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. {AdvGLUE} is available at https://adversarialglue.github.io.},
	number = {{arXiv}:2111.02840},
	publisher = {{arXiv}},
	author = {Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
	urldate = {2024-10-02},
	date = {2022-01-10},
	eprinttype = {arxiv},
	eprint = {2111.02840 [cs]},
}

@misc{austinProgramSynthesisLarge2021,
	title = {Program Synthesis with Large Language Models},
	url = {http://arxiv.org/abs/2108.07732},
	abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, {MBPP} and {MathQA}-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems ({MBPP}) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The {MathQA}-Python dataset, a Python version of the {MathQA} benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from {MBPP} using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the {MathQA}-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
	number = {{arXiv}:2108.07732},
	publisher = {{arXiv}},
	author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
	urldate = {2024-10-02},
	date = {2021-08-15},
	eprinttype = {arxiv},
	eprint = {2108.07732 [cs]},
}

@misc{pavlichenkoCrowdSpeechVoxDIYBenchmark2021,
	title = {{CrowdSpeech} and {VoxDIY}: Benchmark Datasets for Crowdsourced Audio Transcription},
	url = {http://arxiv.org/abs/2107.01091},
	shorttitle = {{CrowdSpeech} and {VoxDIY}},
	abstract = {Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. In simple problems such as image classification, crowdsourcing has become one of the standard tools for cheap and time-efficient data collection: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing aggregation methods for more advanced applications is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release {CrowdSpeech} -- the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing and novel aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of developing the methodology for reliable data collection via crowdsourcing. In that, we design a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing {VoxDIY} -- a counterpart of {CrowdSpeech} for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.},
	number = {{arXiv}:2107.01091},
	publisher = {{arXiv}},
	author = {Pavlichenko, Nikita and Stelmakh, Ivan and Ustalov, Dmitry},
	urldate = {2024-10-02},
	date = {2021-10-20},
	eprinttype = {arxiv},
	eprint = {2107.01091 [cs, eess]},
}

@misc{deyoungMS2MultiDocumentSummarization2021,
	title = {{MS}2: Multi-Document Summarization of Medical Studies},
	url = {http://arxiv.org/abs/2104.06486},
	shorttitle = {{MS}2},
	abstract = {To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and highly manual literature review. {NLP} systems can help to automate or assist in parts of this expensive process. In support of this goal, we release {MS}{\textasciicircum}2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20k summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on {BART}, with promising early results. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system's generated summaries. Data and models are available at https://github.com/allenai/ms2},
	number = {{arXiv}:2104.06486},
	publisher = {{arXiv}},
	author = {{DeYoung}, Jay and Beltagy, Iz and van Zuylen, Madeleine and Kuehl, Bailey and Wang, Lucy Lu},
	urldate = {2024-10-02},
	date = {2021-11-22},
	eprinttype = {arxiv},
	eprint = {2104.06486 [cs]},
}

@misc{chengHiTabHierarchicalTable2022,
	title = {{HiTab}: A Hierarchical Table Dataset for Question Answering and Natural Language Generation},
	url = {http://arxiv.org/abs/2108.06712},
	shorttitle = {{HiTab}},
	abstract = {Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge existing methods by hierarchical indexing, as well as implicit relationships of calculation and semantics. This work presents {HiTab}, a free and open dataset to study question answering ({QA}) and natural language generation ({NLG}) over hierarchical tables. {HiTab} is a cross-domain dataset constructed from a wealth of statistical reports (analyses) and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) both target sentences for {NLG} and questions for {QA} are revised from original, meaningful, and diverse descriptive sentences authored by analysts and professions of reports. (3) to reveal complex numerical reasoning in statistical analyses, we provide fine-grained annotations of entity and quantity alignment. {HiTab} provides 10,686 {QA} pairs and descriptive sentences with well-annotated quantity and entity alignment on 3,597 tables with broad coverage of table hierarchies and numerical reasoning types. Targeting hierarchical structure, we devise a novel hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting complex numerical reasoning, we propose partially supervised training given annotations of entity and quantity alignment, which helps models to largely reduce spurious predictions in the {QA} task. In the {NLG} task, we find that entity and quantity alignment also helps {NLG} models to generate better results in a conditional generation setting. Experiment results of state-of-the-art baselines suggest that this dataset presents a strong challenge and a valuable benchmark for future research.},
	number = {{arXiv}:2108.06712},
	publisher = {{arXiv}},
	author = {Cheng, Zhoujun and Dong, Haoyu and Wang, Zhiruo and Jia, Ran and Guo, Jiaqi and Gao, Yan and Han, Shi and Lou, Jian-Guang and Zhang, Dongmei},
	urldate = {2024-10-02},
	date = {2022-03-26},
	eprinttype = {arxiv},
	eprint = {2108.06712 [cs]},
}

@misc{hazoomTextSQLWildNaturallyOccurring2021,
	title = {Text-to-{SQL} in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data},
	url = {http://arxiv.org/abs/2106.05006},
	shorttitle = {Text-to-{SQL} in the Wild},
	abstract = {Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of natural language understanding systems. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release {SEDE}, a dataset with 12,023 pairs of utterances and {SQL} queries collected from real usage on the Stack Exchange website. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on {SEDE} compared to other common datasets.},
	number = {{arXiv}:2106.05006},
	publisher = {{arXiv}},
	author = {Hazoom, Moshe and Malik, Vibhor and Bogin, Ben},
	urldate = {2024-10-02},
	date = {2021-06-09},
	eprinttype = {arxiv},
	eprint = {2106.05006 [cs]},
}

@misc{chanFewshotAdaptationWorks2022,
	title = {Few-shot Adaptation Works with {UnpredicTable} Data},
	url = {http://arxiv.org/abs/2208.01009},
	abstract = {Prior work on language models ({LMs}) shows that training on a large number of diverse tasks improves few-shot learning ({FSL}) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved {FSL} performance on Natural Language Processing ({NLP}) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of our dataset sometimes outperform more diverse datasets. For example, finetuning on software documentation from support.google.com raises {FSL} performance by a mean of +7.5\% on 52 downstream tasks, which beats training on 40 human-curated {NLP} datasets (+6.7\%). Finetuning on various narrow datasets leads to similar broad improvements across test tasks, suggesting that the gains are not from domain adaptation but adapting to {FSL} in general. We do not observe clear patterns between the datasets that lead to {FSL} gains, leaving open questions about why certain data helps with {FSL}.},
	number = {{arXiv}:2208.01009},
	publisher = {{arXiv}},
	author = {Chan, Jun Shern and Pieler, Michael and Jao, Jonathan and Scheurer, Jérémy and Perez, Ethan},
	urldate = {2024-10-02},
	date = {2022-08-07},
	eprinttype = {arxiv},
	eprint = {2208.01009 [cs]},
}

@misc{ivgiEfficientLongTextUnderstanding2022,
	title = {Efficient Long-Text Understanding with Short-Text Models},
	url = {http://arxiv.org/abs/2208.00748},
	abstract = {Transformer-based pretrained language models ({LMs}) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles and long documents, due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose {SLED}: {SLiding}-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained {LMs}. Specifically, we partition the input into overlapping chunks, encode each with a short-text {LM} encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that {SLED} offers a viable strategy for long text understanding and evaluate our approach on {SCROLLS}, a benchmark with seven datasets across a wide range of language understanding tasks. We find that {SLED} is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.},
	number = {{arXiv}:2208.00748},
	publisher = {{arXiv}},
	author = {Ivgi, Maor and Shaham, Uri and Berant, Jonathan},
	urldate = {2024-10-02},
	date = {2022-12-27},
	eprinttype = {arxiv},
	eprint = {2208.00748 [cs]},
}

@misc{zhouDocPromptingGeneratingCode2023,
	title = {{DocPrompting}: Generating Code by Retrieving the Docs},
	url = {http://arxiv.org/abs/2207.05987},
	shorttitle = {{DocPrompting}},
	abstract = {Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available {APIs} by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce {DocPrompting}: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an {NL} intent, and (2) generating code based on the {NL} intent and the retrieved documentation. {DocPrompting} is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that {DocPrompting} consistently improves {NL}-to-code models: {DocPrompting} improves strong base models such as {CodeT}5 by 2.85\% in pass@1 (52\% relative gain) and 4.39\% in pass@10 (30\% relative gain) in execution-based evaluation on the popular Python {CoNaLa} benchmark; on a new Bash dataset tldr, {DocPrompting} improves {CodeT}5 and {GPT}-Neo1.3B by up to absolute 6.9\% exact match.},
	number = {{arXiv}:2207.05987},
	publisher = {{arXiv}},
	author = {Zhou, Shuyan and Alon, Uri and Xu, Frank F. and Wang, Zhiruo and Jiang, Zhengbao and Neubig, Graham},
	urldate = {2024-10-02},
	date = {2023-02-18},
	eprinttype = {arxiv},
	eprint = {2207.05987 [cs]},
}

@misc{akyurekTracingFactualKnowledge2022,
	title = {Towards Tracing Factual Knowledge in Language Models Back to the Training Data},
	url = {http://arxiv.org/abs/2205.11482},
	abstract = {Language models ({LMs}) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an {LM} generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact tracing: identifying which training examples taught an {LM} to generate a particular factual assertion. Prior work on training data attribution ({TDA}) may offer effective tools for identifying such examples, known as "proponents". We present the first quantitative benchmark to evaluate this. We compare two popular families of {TDA} methods -- gradient-based and embedding-based -- and find that much headroom remains. For example, both methods have lower proponent-retrieval precision than an information retrieval baseline ({BM}25) that does not have access to the {LM} at all. We identify key challenges that may be necessary for further improvement such as overcoming the problem of gradient saturation, and also show how several nuanced implementation details of existing neural {TDA} methods can significantly improve overall fact tracing performance.},
	number = {{arXiv}:2205.11482},
	publisher = {{arXiv}},
	author = {Akyürek, Ekin and Bolukbasi, Tolga and Liu, Frederick and Xiong, Binbin and Tenney, Ian and Andreas, Jacob and Guu, Kelvin},
	urldate = {2024-10-02},
	date = {2022-10-25},
	eprinttype = {arxiv},
	eprint = {2205.11482 [cs]},
}

@misc{rustLanguageModellingPixels2023,
	title = {Language Modelling with Pixels},
	url = {http://arxiv.org/abs/2207.06991},
	abstract = {Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces {PIXEL}, the Pixel-based Encoder of Language, which suffers from neither of these issues. {PIXEL} is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. {PIXEL} is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter {PIXEL} model on the same English data as {BERT} and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that {PIXEL} substantially outperforms {BERT} on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but {PIXEL} is slightly weaker than {BERT} when working with Latin scripts. Furthermore, we find that {PIXEL} is more robust than {BERT} to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.},
	number = {{arXiv}:2207.06991},
	publisher = {{arXiv}},
	author = {Rust, Phillip and Lotz, Jonas F. and Bugliarello, Emanuele and Salesky, Elizabeth and de Lhoneux, Miryam and Elliott, Desmond},
	urldate = {2024-10-02},
	date = {2023-04-26},
	eprinttype = {arxiv},
	eprint = {2207.06991 [cs]},
}

@misc{hendersonPileLawLearning2022,
	title = {Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset},
	url = {http://arxiv.org/abs/2207.00220},
	shorttitle = {Pile of Law},
	abstract = {One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take context into account. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.},
	number = {{arXiv}:2207.00220},
	publisher = {{arXiv}},
	author = {Henderson, Peter and Krass, Mark S. and Zheng, Lucia and Guha, Neel and Manning, Christopher D. and Jurafsky, Dan and Ho, Daniel E.},
	urldate = {2024-10-02},
	date = {2022-11-29},
	eprinttype = {arxiv},
	eprint = {2207.00220 [cs]},
}

@misc{huangTisThyName2022,
	title = {'Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities},
	url = {http://arxiv.org/abs/2202.13581},
	shorttitle = {'Tis but Thy Name},
	abstract = {Classic lexical-matching-based {QA} metrics are slowly being phased out because they punish succinct or informative outputs just because those answers were not provided as ground truth. Recently proposed neural metrics can evaluate semantic similarity but were trained on small textual similarity datasets grafted from foreign domains. We introduce the Wiki Entity Similarity ({WES}) dataset, an 11M example, domain targeted, semantic entity similarity dataset that is generated from link texts in Wikipedia. {WES} is tailored to {QA} evaluation: the examples are entities and phrases and grouped into semantic clusters to simulate multiple ground-truth labels. Human annotators consistently agree with {WES} labels, and a basic cross encoder metric is better than four classic metrics at predicting human judgments of correctness.},
	number = {{arXiv}:2202.13581},
	publisher = {{arXiv}},
	author = {Huang, Albert},
	urldate = {2024-10-02},
	date = {2022-02-28},
	eprinttype = {arxiv},
	eprint = {2202.13581 [cs]},
}

@misc{azerbayevProofNetAutoformalizingFormally2023,
	title = {{ProofNet}: Autoformalizing and Formally Proving Undergraduate-Level Mathematics},
	url = {http://arxiv.org/abs/2302.12433},
	shorttitle = {{ProofNet}},
	abstract = {We introduce {ProofNet}, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The {ProofNet} benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for {ProofNet} to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving. We report baseline results on statement autoformalization via in-context learning. Moreover, we introduce two novel statement autoformalization methods: prompt retrieval and distilled backtranslation.},
	number = {{arXiv}:2302.12433},
	publisher = {{arXiv}},
	author = {Azerbayev, Zhangir and Piotrowski, Bartosz and Schoelkopf, Hailey and Ayers, Edward W. and Radev, Dragomir and Avigad, Jeremy},
	urldate = {2024-10-02},
	date = {2023-02-23},
	eprinttype = {arxiv},
	eprint = {2302.12433 [cs]},
}

@misc{ushioGenerativeLanguageModels2023,
	title = {Generative Language Models for Paragraph-Level Question Generation},
	url = {http://arxiv.org/abs/2210.03992},
	abstract = {Powerful generative models have led to recent progress in question generation ({QG}). However, it is difficult to measure advances in {QG} research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce {QG}-Bench, a multilingual and multidomain benchmark for {QG} that unifies existing question answering datasets by converting them to a standard {QG} setting. It includes general-purpose datasets such as {SQuAD} for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using {QG}-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust {QG} baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating {QG} models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English. {QG}-Bench is released along with the fine-tuned models presented in the paper https://github.com/asahi417/lm-question-generation, which are also available as a demo https://autoqg.net/.},
	number = {{arXiv}:2210.03992},
	publisher = {{arXiv}},
	author = {Ushio, Asahi and Alva-Manchego, Fernando and Camacho-Collados, Jose},
	urldate = {2024-10-02},
	date = {2023-01-02},
	eprinttype = {arxiv},
	eprint = {2210.03992 [cs]},
}

@misc{zhangFengshenbang10Being2023,
	title = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},
	url = {http://arxiv.org/abs/2209.02970},
	shorttitle = {Fengshenbang 1.0},
	abstract = {Nowadays, foundation models become one of fundamental infrastructures in artificial intelligence, paving ways to the general intelligence. However, the reality presents two urgent challenges: existing foundation models are dominated by the English-language community; users are often given limited resources and thus cannot always use foundation models. To support the development of the Chinese-language community, we introduce an open-source project, called Fengshenbang, which leads by the research center for Cognitive Computing and Natural Language ({CCNL}). Our project has comprehensive capabilities, including large pre-trained models, user-friendly {APIs}, benchmarks, datasets, and others. We wrap all these in three sub-projects: the Fengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An open-source roadmap, Fengshenbang, aims to re-evaluate the open-source community of Chinese pre-trained large-scale models, prompting the development of the entire Chinese large-scale model community. We also want to build a user-centered open-source ecosystem to allow individuals to access the desired models to match their computing resources. Furthermore, we invite companies, colleges, and research institutions to collaborate with us to build the large-scale open-source model-based ecosystem. We hope that this project will be the foundation of Chinese cognitive intelligence.},
	number = {{arXiv}:2209.02970},
	publisher = {{arXiv}},
	author = {Zhang, Jiaxing and Gan, Ruyi and Wang, Junjie and Zhang, Yuxiang and Zhang, Lin and Yang, Ping and Gao, Xinyu and Wu, Ziwei and Dong, Xiaoqun and He, Junqing and Zhuo, Jianheng and Yang, Qi and Huang, Yongfeng and Li, Xiayu and Wu, Yanghan and Lu, Junyu and Zhu, Xinyu and Chen, Weifeng and Han, Ting and Pan, Kunhao and Wang, Rui and Wang, Hao and Wu, Xiaojun and Zeng, Zhongshen and Chen, Chongpei},
	urldate = {2024-10-02},
	date = {2023-03-30},
	eprinttype = {arxiv},
	eprint = {2209.02970 [cs]},
}

@misc{abdallahArabicaQAComprehensiveDataset2024,
	title = {{ArabicaQA}: A Comprehensive Dataset for Arabic Question Answering},
	url = {http://arxiv.org/abs/2403.17848},
	shorttitle = {{ArabicaQA}},
	abstract = {In this paper, we address the significant gap in Arabic natural language processing ({NLP}) resources by introducing {ArabicaQA}, the first large-scale dataset for machine reading comprehension and open-domain question answering in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable questions created by crowdworkers to look similar to answerable ones, along with additional labels of open-domain questions marks a crucial advancement in Arabic {NLP} resources. We also present {AraDPR}, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive benchmarking of large language models ({LLMs}) for Arabic question answering, critically evaluating their performance in the Arabic language context. In conclusion, {ArabicaQA}, {AraDPR}, and the benchmarking of {LLMs} in Arabic question answering offer significant advancements in the field of Arabic {NLP}. The dataset and code are publicly accessible for further research https://github.com/{DataScienceUIBK}/{ArabicaQA}.},
	number = {{arXiv}:2403.17848},
	publisher = {{arXiv}},
	author = {Abdallah, Abdelrahman and Kasem, Mahmoud and Abdalla, Mahmoud and Mahmoud, Mohamed and Elkasaby, Mohamed and Elbendary, Yasser and Jatowt, Adam},
	urldate = {2024-10-02},
	date = {2024-03-26},
	eprinttype = {arxiv},
	eprint = {2403.17848 [cs]},
}

@misc{el-khair15BillionWords2016,
	title = {1.5 billion words Arabic Corpus},
	url = {http://arxiv.org/abs/1611.04033},
	abstract = {This study is an attempt to build a contemporary linguistic corpus for Arabic language. The corpus produced, is a text corpus includes more than five million newspaper articles. It contains over a billion and a half words in total, out of which, there is about three million unique words. The data were collected from newspaper articles in ten major news sources from eight Arabic countries, over a period of fourteen years. The corpus was encoded with two types of encoding, namely: {UTF}-8, and Windows {CP}-1256. Also it was marked with two mark-up languages, namely: {SGML}, and {XML}.},
	number = {{arXiv}:1611.04033},
	publisher = {{arXiv}},
	author = {El-khair, Ibrahim Abu},
	urldate = {2024-10-02},
	date = {2016-11-12},
	eprinttype = {arxiv},
	eprint = {1611.04033 [cs]},
}

@misc{aloui101BillionArabic2024,
	title = {101 Billion Arabic Words Dataset},
	url = {http://arxiv.org/abs/2405.01590},
	abstract = {In recent years, Large Language Models have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic {LLMs} capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic {LLMs}, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue -the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale data mining project, extracting a substantial volume of text from the Common Crawl {WET} files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic {LLMs}. This study not only highlights the potential for creating linguistically and culturally accurate Arabic {LLMs} but also sets a precedent for future research in enhancing the authenticity of Arabic language models.},
	number = {{arXiv}:2405.01590},
	publisher = {{arXiv}},
	author = {Aloui, Manel and Chouikhi, Hasna and Chaabane, Ghaith and Kchaou, Haithem and Dhaouadi, Chehir},
	urldate = {2024-10-02},
	date = {2024-04-29},
	eprinttype = {arxiv},
	eprint = {2405.01590 [cs]},
}

@inproceedings{alghamdiArMATHDatasetSolving2022,
	location = {Marseille, France},
	title = {{ArMATH}: a Dataset for Solving Arabic Math Word Problems},
	url = {https://aclanthology.org/2022.lrec-1.37},
	shorttitle = {{ArMATH}},
	abstract = {This paper studies solving Arabic Math Word Problems by deep learning. A Math Word Problem ({MWP}) is a text description of a mathematical problem that can be solved by deriving a math equation to reach the answer. Effective models have been developed for solving {MWPs} in English and Chinese. However, Arabic {MWPs} are rarely studied. This paper contributes the first large-scale dataset for Arabic {MWPs}, which contains 6,000 samples of primary-school math problems, written in Modern Standard Arabic ({MSA}). Arabic {MWP} solvers are then built with deep learning models and evaluated on this dataset. In addition, a transfer learning model is built to let the high-resource Chinese {MWP} solver promote the performance of the low-resource Arabic {MWP} solver. This work is the first to use deep learning methods to solve Arabic {MWP} and the first to use transfer learning to solve {MWP} across different languages. The transfer learning enhanced solver has an accuracy of 74.15\%, which is 3\% higher than the solver without using transfer learning. We make the dataset and solvers available in public for encouraging more research of Arabic {MWPs}: https://github.com/reem-codes/{ArMATH}},
	eventtitle = {{LREC} 2022},
	pages = {351--362},
	booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Alghamdi, Reem and Liang, Zhenwen and Zhang, Xiangliang},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-10-02},
	date = {2022-06},
}

@inproceedings{biltawiArabicReadingComprehension2020,
	location = {Giza, Egypt},
	title = {Arabic Reading Comprehension Benchmarks Created Semiautomatically},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72818-855-3},
	url = {https://ieeexplore.ieee.org/document/9300111/},
	doi = {10/g6k6b8},
	eventtitle = {2020 21st International Arab Conference on Information Technology ({ACIT})},
	pages = {1--6},
	booktitle = {2020 21st International Arab Conference on Information Technology ({ACIT})},
	publisher = {{IEEE}},
	author = {Biltawi, Mariam and Awajan, Arafat and Tedmori, Sara},
	urldate = {2024-10-02},
	date = {2020-11-28},
}

@inproceedings{elsaharBuildingLargeArabic2015,
	location = {Cham},
	title = {Building Large Arabic Multi-domain Resources for Sentiment Analysis},
	isbn = {978-3-319-18117-2},
	doi = {10/g6k58r},
	abstract = {While there has been a recent progress in the area of Arabic Sentiment Analysis, most of the resources in this area are either of limited size, domain specific or not publicly available. In this paper, we address this problem by generating large multi-domain datasets for Sentiment Analysis in Arabic. The datasets were scrapped from different reviewing websites and consist of a total of 33K annotated reviews for movies, hotels, restaurants and products. Moreover we build multi-domain lexicons from the generated datasets. Different experiments have been carried out to validate the usefulness of the datasets and the generated lexicons for the task of sentiment classification. From the experimental results, we highlight some useful insights addressing: the best performing classifiers and feature representation methods, the effect of introducing lexicon based features and factors affecting the accuracy of sentiment classification in general. All the datasets, experiments code and results have been made publicly available for scientific purposes.},
	pages = {23--34},
	booktitle = {Computational Linguistics and Intelligent Text Processing},
	publisher = {Springer International Publishing},
	author = {{ElSahar}, Hady and El-Beltagy, Samhaa R.},
	editor = {Gelbukh, Alexander},
	date = {2015},
	langid = {english},
}

@misc{pieriBiMediXBilingualMedical2024,
	title = {{BiMediX}: Bilingual Medical Mixture of Experts {LLM}},
	url = {http://arxiv.org/abs/2402.13253},
	shorttitle = {{BiMediX}},
	abstract = {In this paper, we introduce {BiMediX}, the first bilingual medical mixture of experts {LLM} designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical {LLMs}. Furthermore, we introduce {BiMed}1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our {BiMed}1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5\% and 4.1\%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our {BiMediX} outperforms the generic Arabic-English bilingual {LLM}, Jais-30B, by average absolute gains of 10\% on our Arabic medical benchmark and 15\% on bilingual evaluations across multiple datasets. Our project page with source code and trained model is available at https://github.com/mbzuai-oryx/{BiMediX} .},
	number = {{arXiv}:2402.13253},
	publisher = {{arXiv}},
	author = {Pieri, Sara and Mullappilly, Sahal Shaji and Khan, Fahad Shahbaz and Anwer, Rao Muhammad and Khan, Salman and Baldwin, Timothy and Cholakkal, Hisham},
	urldate = {2024-10-02},
	date = {2024-02-20},
	eprinttype = {arxiv},
	eprint = {2402.13253 [cs]},
}

@inproceedings{elnagarBRAD10Book2016,
	location = {Agadir, Morocco},
	title = {{BRAD} 1.0: Book reviews in Arabic dataset},
	isbn = {978-1-5090-4320-0},
	url = {http://ieeexplore.ieee.org/document/7945800/},
	doi = {10/g6k6jm},
	shorttitle = {{BRAD} 1.0},
	eventtitle = {2016 {IEEE}/{ACS} 13th International Conference of Computer Systems and Applications ({AICCSA})},
	pages = {1--8},
	booktitle = {2016 {IEEE}/{ACS} 13th International Conference of Computer Systems and Applications ({AICCSA})},
	publisher = {{IEEE}},
	author = {Elnagar, Ashraf and Einea, Omar},
	urldate = {2024-10-02},
	date = {2016-11},
}

@misc{pratapaMultilingualEventLinking2022,
	title = {Multilingual Event Linking to Wikidata},
	url = {http://arxiv.org/abs/2204.06535},
	abstract = {We present a task of multilingual linking of events to a knowledge base. We automatically compile a large-scale dataset for this task, comprising of 1.8M mentions across 44 languages referring to over 10.9K events from Wikidata. We propose two variants of the event linking task: 1) multilingual, where event descriptions are from the same language as the mention, and 2) crosslingual, where all event descriptions are in English. On the two proposed tasks, we compare multiple event linking systems including {BM}25+ (Lv and Zhai, 2011) and multilingual adaptations of the biencoder and crossencoder architectures from {BLINK} (Wu et al., 2020). In our experiments on the two task variants, we find both biencoder and crossencoder models significantly outperform the {BM}25+ baseline. Our results also indicate that the crosslingual task is in general more challenging than the multilingual task. To test the out-of-domain generalization of the proposed linking systems, we additionally create a Wikinews-based evaluation set. We present qualitative analysis highlighting various aspects captured by the proposed dataset, including the need for temporal reasoning over context and tackling diverse event descriptions across languages.},
	number = {{arXiv}:2204.06535},
	publisher = {{arXiv}},
	author = {Pratapa, Adithya and Gupta, Rishubh and Mitamura, Teruko},
	urldate = {2024-10-02},
	date = {2022-07-16},
	eprinttype = {arxiv},
	eprint = {2204.06535 [cs]},
}

@misc{mozannarNeuralArabicQuestion2019,
	title = {Neural Arabic Question Answering},
	url = {http://arxiv.org/abs/1906.05394},
	abstract = {This paper tackles the problem of open domain factual Arabic question answering ({QA}) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain {QA} for Arabic entails three challenges: annotated {QA} datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic {QA} datasets we present the Arabic Reading Comprehension Dataset ({ARCD}) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-{SQuAD}). Our system for open domain question answering in Arabic ({SOQAL}) is based on two components: (1) a document retriever using a hierarchical {TF}-{IDF} approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer {BERT}. Our experiments on {ARCD} indicate the effectiveness of our approach with our {BERT}-based reader achieving a 61.3 F1 score, and our open domain system {SOQAL} achieving a 27.6 F1 score.},
	number = {{arXiv}:1906.05394},
	publisher = {{arXiv}},
	author = {Mozannar, Hussein and Hajal, Karl El and Maamary, Elie and Hajj, Hazem},
	urldate = {2024-10-02},
	date = {2019-06-12},
	eprinttype = {arxiv},
	eprint = {1906.05394 [cs]},
}

@misc{chouikhiGemmArEnhancingLLMs2024,
	title = {{GemmAr}: Enhancing {LLMs} Through Arabic Instruction-Tuning},
	url = {http://arxiv.org/abs/2407.02147},
	shorttitle = {{GemmAr}},
	abstract = {Large language models ({LLMs}) have greatly impacted the natural language processing ({NLP}) field, particularly for the English language. These models have demonstrated capabilities in understanding and generating human-like text. The success of language models largely depends on the availability of high-quality instruction datasets, which consist of detailed task descriptions and corresponding responses that are essential for training the models to address a variety of prompts accurately. However, the availability and quality of these resources vary by language. While models perform well in English, they often need help with languages like Arabic, due to the lack of datasets for fine-tuning Arabic-specific tasks. To address this issue, we introduce {InstAr}-500k, a new Arabic instruction dataset created by generating and collecting content that covers several domains and instruction types. We assess this dataset by fine-tuning an open-source Gemma-7B model on several downstream tasks to improve its functionality. Based on multiple evaluations, our fine-tuned model achieves excellent performance on several Arabic {NLP} benchmarks. These outcomes emphasize the effectiveness of our dataset in elevating the capabilities of language models for Arabic. Our instruction dataset bridges the performance gap between English and Arabic language models by providing resources that amplify Arabic {NLP} development. Building on this foundation, we developed a model, {GemmAr}-7B-V1, specifically tuned to excel at a wide range of Arabic {NLP} tasks.},
	number = {{arXiv}:2407.02147},
	publisher = {{arXiv}},
	author = {Chouikhi, Hasna and Aloui, Manel and Hammou, Cyrine Ben and Chaabane, Ghaith and Kchaou, Haithem and Dhaouadi, Chehir},
	urldate = {2024-10-02},
	date = {2024-07-09},
	eprinttype = {arxiv},
	eprint = {2407.02147 [cs]},
}

@article{abbasEvaluationTopicIdentification2011,
	title = {Evaluation of Topic Identification Methods on Arabic Corpora},
	volume = {9},
	url = {https://www.dline.info/fpaper/jdim/v9i5/1.pdf},
	abstract = {Topic Identification is one of the important keys 
for the success of many applications. Indeed, there are few 
works in this field concerning Arabic language because of 
lack of standard corpora. In this study, we will provide directly 
comparable results of six text categorization methods on a 
new Arabic corpus Alwatan-2004. Hence, Topic Unigram 
Language Model ({TULM}), Term Frequency/Inverse Document 
Frequency ({TFIDF}), Neural Network, {SVM}, M-{SVM} and {TR} 
have been experimented, and showed that {TR}-Classifier is 
the most efficient among the set of classifiers, nevertheless, 
only binary {SVM} outperformed it thanks to its characteristics. 
Moreover, we should note that the size of Alwatan-2004 corpus 
used to achieve our experiments is considered the most 
important compared to any other Arabic corpus which had 
been used for topic identification experiments until now. In 
addition, we aim through using small sizes of vocabularies to 
reduce the time of computation. This is important for adaptive 
language modeling, particularly Topic Adaptation, which is 
required in real time applications such as speech recognition 
and machine translation systems. Our experiments indicate 
that the results are better than other works dealing with Arabic 
text categorization.},
	number = {5},
	journaltitle = {Journal of Digital Information Management},
	author = {Abbas, Mourad and Smaïli, Kamel and Berkani, D.},
	urldate = {2024-10-02},
	date = {2011-10},
	keywords = {⛔ No {DOI} found},
}

@article{eineaSANADSinglelabelArabic2019,
	title = {{SANAD}: Single-label Arabic News Articles Dataset for automatic text categorization},
	volume = {25},
	issn = {23523409},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352340919304305},
	doi = {10/g6k6cn},
	shorttitle = {{SANAD}},
	pages = {104076},
	journaltitle = {Data in Brief},
	shortjournal = {Data in Brief},
	author = {Einea, Omar and Elnagar, Ashraf and Al Debsi, Ridhwan},
	urldate = {2024-10-02},
	date = {2019-08},
	langid = {english},
}

@inproceedings{orabiClassicalArabicPoetry2020,
	location = {Antalya, Turkey},
	title = {Classical Arabic Poetry: Classification based on Era},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72818-577-4},
	url = {https://ieeexplore.ieee.org/document/9316520/},
	doi = {10/g6k6b2},
	shorttitle = {Classical Arabic Poetry},
	eventtitle = {2020 {IEEE}/{ACS} 17th International Conference on Computer Systems and Applications ({AICCSA})},
	pages = {1--6},
	booktitle = {2020 {IEEE}/{ACS} 17th International Conference on Computer Systems and Applications ({AICCSA})},
	publisher = {{IEEE}},
	author = {Orabi, Mariam and Rifai, Hozayfa El and Elnagar, Ashraf},
	urldate = {2024-10-02},
	date = {2020-11},
}

@article{abdelghanyDoc2VecApproachIdentify2020,
	title = {Doc2Vec: An approach to identify Hadith Similarities},
	doi = {10.22587/ajbas.2020.14.12.5},
	shorttitle = {Doc2Vec},
	abstract = {The Islamic religion beliefs and traditions are built on two pillars, Qur’an and Hadith.This is why the Hadith in particular and Sunnah in general are considered to be major resources to understand Islamic teachings. Muslims have learnt Hadith from nine major books in Hadith sciences that are considered the Hadith main study resource since early Islamic eras.This paper discusses the implementation of similarity model detection technique between Hadith books using Doc2vec algorithm. The study was able to build a classification model capable of identifying Hadith with similarities in both Matn and Sanad using different scenarios. The experimental results revealed from that study shows that number of Hadith (62,169 Hadith) doesn’t represent a quiet accurate number},
	pages = {46--53},
	journaltitle = {Australian Journal of Basic and Applied Sciences},
	shortjournal = {Australian Journal of Basic and Applied Sciences},
	author = {Abdelghany, Ahmed and Abdelaal, Hammam and Kamr, Abdulrahman and Elkafrawy, Passent},
	date = {2020-12-01},
	keywords = {⚠️ Invalid {DOI}},
}

@misc{huXTREMEMassivelyMultilingual2020,
	title = {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization},
	url = {http://arxiv.org/abs/2003.11080},
	shorttitle = {{XTREME}},
	abstract = {Much recent progress in applications of machine learning models to {NLP} has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual {TRansfer} Evaluation of Multilingual Encoders {XTREME} benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.},
	number = {{arXiv}:2003.11080},
	publisher = {{arXiv}},
	author = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
	urldate = {2024-10-02},
	date = {2020-09-04},
	eprinttype = {arxiv},
	eprint = {2003.11080 [cs]},
}

@misc{xuMagpieAlignmentData2024,
	title = {Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned {LLMs} with Nothing},
	url = {http://arxiv.org/abs/2406.08464},
	doi = {10.48550/arXiv.2406.08464},
	shorttitle = {Magpie},
	abstract = {High-quality instruction data is critical for aligning large language models ({LLMs}). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of {AI}. High human labor costs and a limited, predefined scope for prompting prevent existing open-source data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets. Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned {LLM}? We present a self-synthesis method for generating large-scale alignment data named Magpie. Our key observation is that aligned {LLMs} like Llama-3-Instruct can generate a user query when we input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature. We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses. We perform a comprehensive analysis of the extracted data and select 300K high-quality instances. To compare Magpie data with other public instruction datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models. Our results indicate that in some tasks, models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning ({SFT}) and subsequent feedback learning. We also show that using Magpie solely for {SFT} can surpass the performance of previous public datasets utilized for both {SFT} and preference optimization, such as direct preference optimization with {UltraFeedback}. This advantage is evident on alignment benchmarks such as {AlpacaEval}, {ArenaHard}, and {WildBench}.},
	number = {{arXiv}:2406.08464},
	publisher = {{arXiv}},
	author = {Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen},
	urldate = {2024-10-02},
	date = {2024-06-12},
	eprinttype = {arxiv},
	eprint = {2406.08464 [cs]},
}

@misc{bainFrozenTimeJoint2022,
	title = {Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval},
	url = {http://arxiv.org/abs/2104.00650},
	doi = {10.48550/arXiv.2104.00650},
	shorttitle = {Frozen in Time},
	abstract = {Our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as {HowTo}100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We address both these challenges in this paper. We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent {ViT} and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset {WebVid}-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including {MSR}-{VTT}, {MSVD}, {DiDeMo} and {LSMDC}.},
	number = {{arXiv}:2104.00650},
	publisher = {{arXiv}},
	author = {Bain, Max and Nagrani, Arsha and Varol, Gül and Zisserman, Andrew},
	urldate = {2024-05-03},
	date = {2022-05-13},
	eprinttype = {arxiv},
	eprint = {2104.00650 [cs]},
}

@incollection{carlettaAMIMeetingCorpus2006,
	location = {Berlin, Heidelberg},
	title = {The {AMI} Meeting Corpus: A Pre-announcement},
	volume = {3869},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-540-32549-9},
	url = {http://link.springer.com/10.1007/11677482_3},
	shorttitle = {The {AMI} Meeting Corpus},
	pages = {28--39},
	booktitle = {Machine Learning for Multimodal Interaction},
	publisher = {Springer Berlin Heidelberg},
	author = {Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and Lisowska, Agnes and {McCowan}, Iain and Post, Wilfried and Reidsma, Dennis and Wellner, Pierre},
	editor = {Renals, Steve and Bengio, Samy},
	urldate = {2024-05-01},
	date = {2006},
	doi = {10.1007/11677482_3},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{al-fetyaniMASCMassiveArabic2023,
	location = {Doha, Qatar},
	title = {{MASC}: Massive Arabic Speech Corpus},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-9690-4},
	url = {https://ieeexplore.ieee.org/document/10022652/},
	doi = {10/gtsqzj},
	shorttitle = {{MASC}},
	eventtitle = {2022 {IEEE} Spoken Language Technology Workshop ({SLT})},
	pages = {1006--1013},
	booktitle = {2022 {IEEE} Spoken Language Technology Workshop ({SLT})},
	publisher = {{IEEE}},
	author = {Al-Fetyani, Mohammad and Al-Barham, Muhammad and Abandah, Gheith and Alsharkawi, Adham and Dawas, Maha},
	urldate = {2024-05-01},
	date = {2023-01-09},
}

@inproceedings{liYodasYoutubeOrientedDataset2023,
	location = {Taipei, Taiwan},
	title = {Yodas: Youtube-Oriented Dataset for Audio and Speech},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0689-7},
	url = {https://ieeexplore.ieee.org/document/10389689/},
	doi = {10/gtsqzc},
	shorttitle = {Yodas},
	eventtitle = {2023 {IEEE} Automatic Speech Recognition and Understanding Workshop ({ASRU})},
	pages = {1--8},
	booktitle = {2023 {IEEE} Automatic Speech Recognition and Understanding Workshop ({ASRU})},
	publisher = {{IEEE}},
	author = {Li, Xinjian and Takamichi, Shinnosuke and Saeki, Takaaki and Chen, William and Shiota, Sayaka and Watanabe, Shinji},
	urldate = {2024-05-01},
	date = {2023-12-16},
}

@inproceedings{kuehneHMDBLargeVideo2011,
	location = {Barcelona, Spain},
	title = {{HMDB}: A large video database for human motion recognition},
	isbn = {978-1-4577-1102-2},
	url = {http://ieeexplore.ieee.org/document/6126543/},
	doi = {10/fxpf8k},
	shorttitle = {{HMDB}},
	eventtitle = {2011 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2556--2563},
	booktitle = {2011 International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Kuehne, H. and Jhuang, H. and Garrote, E. and Poggio, T. and Serre, T.},
	urldate = {2024-05-02},
	date = {2011-11},
}

@incollection{gygliCreatingSummariesUser2014,
	location = {Cham},
	title = {Creating Summaries from User Videos},
	volume = {8695},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-319-10583-3},
	url = {http://link.springer.com/10.1007/978-3-319-10584-0_33},
	pages = {505--520},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Gygli, Michael and Grabner, Helmut and Riemenschneider, Hayko and Van Gool, Luc},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	urldate = {2024-05-02},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10584-0_33},
	note = {Series Title: Lecture Notes in Computer Science},
}

@misc{oneillSPGISpeech000Hours2021,
	title = {{SPGISpeech}: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition},
	url = {http://arxiv.org/abs/2104.02014},
	shorttitle = {{SPGISpeech}},
	abstract = {In the English speech-to-text ({STT}) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models. This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription. Here we propose a new {STT} task: end-to-end neural transcription with fully formatted text for target labels. We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a {CER} of 1.7. As a contribution to the {STT} research community, we release the corpus free for non-commercial use at https://datasets.kensho.com/datasets/scribe.},
	number = {{arXiv}:2104.02014},
	publisher = {{arXiv}},
	author = {O'Neill, Patrick K. and Lavrukhin, Vitaly and Majumdar, Somshubra and Noroozi, Vahid and Zhang, Yuekai and Kuchaiev, Oleksii and Balam, Jagadeesh and Dovzhenko, Yuliya and Freyberg, Keenan and Shulman, Michael D. and Ginsburg, Boris and Watanabe, Shinji and Kucsko, Georg},
	urldate = {2024-05-01},
	date = {2021-04-06},
	eprinttype = {arxiv},
	eprint = {2104.02014 [cs, eess]},
}

@misc{sanabriaEdinburghInternationalAccents2023,
	title = {The Edinburgh International Accents of English Corpus: Towards the Democratization of English {ASR}},
	url = {http://arxiv.org/abs/2303.18110},
	shorttitle = {The Edinburgh International Accents of English Corpus},
	abstract = {English is the most widely spoken language in the world, used daily by millions of people as a first or second language in many different contexts. As a result, there are many varieties of English. Although the great many advances in English automatic speech recognition ({ASR}) over the past decades, results are usually reported based on test datasets which fail to represent the diversity of English as spoken today around the globe. We present the first release of The Edinburgh International Accents of English Corpus ({EdAcc}). This dataset attempts to better represent the wide diversity of English, encompassing almost 40 hours of dyadic video call conversations between friends. Unlike other datasets, {EdAcc} includes a wide range of first and second-language varieties of English and a linguistic background profile of each speaker. Results on latest public, and commercial models show that {EdAcc} highlights shortcomings of current English {ASR} models. The best performing model, trained on 680 thousand hours of transcribed data, obtains an average of 19.7\% word error rate ({WER}) -- in contrast to the 2.7\% {WER} obtained when evaluated on {US} English clean read speech. Across all models, we observe a drop in performance on Indian, Jamaican, and Nigerian English speakers. Recordings, linguistic backgrounds, data statement, and evaluation scripts are released on our website (https://groups.inf.ed.ac.uk/edacc/) under {CC}-{BY}-{SA} license.},
	number = {{arXiv}:2303.18110},
	publisher = {{arXiv}},
	author = {Sanabria, Ramon and Bogoychev, Nikolay and Markl, Nina and Carmantini, Andrea and Klejch, Ondrej and Bell, Peter},
	urldate = {2024-05-01},
	date = {2023-03-31},
	eprinttype = {arxiv},
	eprint = {2303.18110 [cs, eess]},
}

@misc{duanOmnisourcedWeblysupervisedLearning2020,
	title = {Omni-sourced Webly-supervised Learning for Video Recognition},
	url = {http://arxiv.org/abs/2003.13042},
	abstract = {We introduce {OmniSource}, a novel framework for leveraging web data to train video recognition models. {OmniSource} overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, {OmniSource} is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2\% of prior works), our models learned with {OmniSource} improve Top-1 accuracy of 2D- and 3D-{ConvNet} baseline models by 3.0\% and 3.9\%, respectively, on the Kinetics-400 benchmark. With {OmniSource}, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4\%, 80.5\%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, {ImageNet} pre-training and {IG}-65M pre-training.},
	number = {{arXiv}:2003.13042},
	publisher = {{arXiv}},
	author = {Duan, Haodong and Zhao, Yue and Xiong, Yuanjun and Liu, Wentao and Lin, Dahua},
	urldate = {2024-05-02},
	date = {2020-08-25},
	eprinttype = {arxiv},
	eprint = {2003.13042 [cs]},
}

@misc{liuVIOLINLargeScaleDataset2020,
	title = {{VIOLIN}: A Large-Scale Dataset for Video-and-Language Inference},
	url = {http://arxiv.org/abs/2003.11618},
	shorttitle = {{VIOLIN}},
	abstract = {We introduce a new task, Video-and-Language Inference, for joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. A new large-scale dataset, named Violin ({VIdeO}-and-Language {INference}), is introduced for this task, which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular {TV} shows, and (ii) movie clips from {YouTube} channels. In order to address our new multimodal inference task, a model is required to possess sophisticated reasoning skills, from surface-level grounding (e.g., identifying objects and characters in the video) to in-depth commonsense reasoning (e.g., inferring causal relations of events in the video). We present a detailed analysis of the dataset and an extensive evaluation over many strong baselines, providing valuable insights on the challenges of this new task.},
	number = {{arXiv}:2003.11618},
	publisher = {{arXiv}},
	author = {Liu, Jingzhou and Chen, Wenhu and Cheng, Yu and Gan, Zhe and Yu, Licheng and Yang, Yiming and Liu, Jingjing},
	urldate = {2024-05-01},
	date = {2020-03-25},
	eprinttype = {arxiv},
	eprint = {2003.11618 [cs]},
}

@misc{monfortSpokenMomentsLearning2021,
	title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},
	url = {http://arxiv.org/abs/2105.04489},
	shorttitle = {Spoken Moments},
	abstract = {When people observe events, they are able to abstract key information and build concise summaries of what is happening. These summaries include contextual and semantic information describing the important high-level details (what, where, who and how) of the observed event and exclude background information that is deemed unimportant to the observer. With this in mind, the descriptions people generate for videos of different dynamic events can greatly improve our understanding of the key information of interest in each video. These descriptions can be captured in captions that provide expanded attributes for video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing us to gain new insight into what people find important or necessary to summarize specific events. Existing caption datasets for video understanding are either small in scale or restricted to a specific domain. To address this, we present the Spoken Moments (S-{MiT}) dataset of 500k spoken captions each attributed to a unique short video depicting a broad range of different events. We collect our descriptions using audio recordings to ensure that they remain as natural and concise as possible while allowing us to scale the size of a large classification dataset. In order to utilize our proposed dataset, we present a novel Adaptive Mean Margin ({AMM}) approach to contrastive learning and evaluate our models on video/caption retrieval on multiple datasets. We show that our {AMM} approach consistently improves our results and that models trained on our Spoken Moments dataset generalize better than those trained on other video-caption datasets.},
	number = {{arXiv}:2105.04489},
	publisher = {{arXiv}},
	author = {Monfort, Mathew and Jin, {SouYoung} and Liu, Alexander and Harwath, David and Feris, Rogerio and Glass, James and Oliva, Aude},
	urldate = {2024-05-02},
	date = {2021-05-10},
	eprinttype = {arxiv},
	eprint = {2105.04489 [cs, eess]},
}

@misc{zengTitleGenerationUser2016,
	title = {Title Generation for User Generated Videos},
	url = {http://arxiv.org/abs/1608.07068},
	abstract = {A great video title describes the most salient event compactly and captures the viewer's attention. In contrast, video captioning tends to generate sentences that describe the video as a whole. Although generating a video title automatically is a very useful task, it is much less addressed than video captioning. We address video title generation for the first time by proposing two methods that extend state-of-the-art video captioners to this new task. First, we make video captioners highlight sensitive by priming them with a highlight detector. Our framework allows for jointly training a model for title generation and video highlight localization. Second, we induce high sentence diversity in video captioners, so that the generated titles are also diverse and catchy. This means that a large number of sentences might be required to learn the sentence structure of titles. Hence, we propose a novel sentence augmentation method to train a captioner with additional sentence-only examples that come without corresponding videos. We collected a large-scale Video Titles in the Wild ({VTW}) dataset of 18100 automatically crawled user-generated videos and titles. On {VTW}, our methods consistently improve title prediction accuracy, and achieve the best performance in both automatic and human evaluation. Finally, our sentence augmentation method also outperforms the baselines on the M-{VAD} dataset.},
	number = {{arXiv}:1608.07068},
	publisher = {{arXiv}},
	author = {Zeng, Kuo-Hao and Chen, Tseng-Hung and Niebles, Juan Carlos and Sun, Min},
	urldate = {2024-05-01},
	date = {2016-09-08},
	eprinttype = {arxiv},
	eprint = {1608.07068 [cs]},
}

@misc{chenPLACESPromptingLanguage2023,
	title = {{PLACES}: Prompting Language Models for Social Conversation Synthesis},
	url = {http://arxiv.org/abs/2302.03269},
	shorttitle = {{PLACES}},
	abstract = {Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.},
	number = {{arXiv}:2302.03269},
	publisher = {{arXiv}},
	author = {Chen, Maximillian and Papangelis, Alexandros and Tao, Chenyang and Kim, Seokhwan and Rosenbaum, Andy and Liu, Yang and Yu, Zhou and Hakkani-Tur, Dilek},
	urldate = {2024-05-01},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2302.03269 [cs]},
}

@misc{dinanWizardWikipediaKnowledgePowered2019,
	title = {Wizard of Wikipedia: Knowledge-Powered Conversational agents},
	url = {http://arxiv.org/abs/1811.01241},
	shorttitle = {Wizard of Wikipedia},
	abstract = {In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically "generate and hope" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.},
	number = {{arXiv}:1811.01241},
	publisher = {{arXiv}},
	author = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
	urldate = {2024-05-01},
	date = {2019-02-21},
	eprinttype = {arxiv},
	eprint = {1811.01241 [cs]},
}

@misc{goyalSomethingSomethingVideo2017,
	title = {The "something something" video database for learning and evaluating visual common sense},
	url = {http://arxiv.org/abs/1706.04261},
	abstract = {Neural networks trained on datasets such as {ImageNet} have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the "something-something" database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.},
	number = {{arXiv}:1706.04261},
	publisher = {{arXiv}},
	author = {Goyal, Raghav and Kahou, Samira Ebrahimi and Michalski, Vincent and Materzyńska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and Hoppe, Florian and Thurau, Christian and Bax, Ingo and Memisevic, Roland},
	urldate = {2024-05-01},
	date = {2017-06-15},
	eprinttype = {arxiv},
	eprint = {1706.04261 [cs]},
}

@misc{miechHowTo100MLearningTextVideo2019,
	title = {{HowTo}100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},
	url = {http://arxiv.org/abs/1906.03327},
	shorttitle = {{HowTo}100M},
	abstract = {Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce {HowTo}100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as {YouCook}2 or {CrossTask}. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos ({MSR}-{VTT} dataset) and movies ({LSMDC} dataset) outperforms models trained on these datasets alone. Our dataset, code and models will be publicly available at: www.di.ens.fr/willow/research/howto100m/.},
	number = {{arXiv}:1906.03327},
	publisher = {{arXiv}},
	author = {Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
	urldate = {2024-05-01},
	date = {2019-07-31},
	eprinttype = {arxiv},
	eprint = {1906.03327 [cs]},
}

@misc{sunEEVLargeScaleDataset2021,
	title = {{EEV}: A Large-Scale Dataset for Studying Evoked Expressions from Video},
	url = {http://arxiv.org/abs/2001.05488},
	shorttitle = {{EEV}},
	abstract = {Videos can evoke a range of affective responses in viewers. The ability to predict evoked affect from a video, before viewers watch the video, can help in content creation and video recommendation. We introduce the Evoked Expressions from Videos ({EEV}) dataset, a large-scale dataset for studying viewer responses to videos. Each video is annotated at 6 Hz with 15 continuous evoked expression labels, corresponding to the facial expression of viewers who reacted to the video. We use an expression recognition model within our data collection framework to achieve scalability. In total, there are 36.7 million annotations of viewer facial reactions to 23,574 videos (1,700 hours). We use a publicly available video corpus to obtain a diverse set of video content. We establish baseline performance on the {EEV} dataset using an existing multimodal recurrent model. Transfer learning experiments show an improvement in performance on the {LIRIS}-{ACCEDE} video dataset when pre-trained on {EEV}. We hope that the size and diversity of the {EEV} dataset will encourage further explorations in video understanding and affective computing. A subset of {EEV} is released at https://github.com/google-research-datasets/eev.},
	number = {{arXiv}:2001.05488},
	publisher = {{arXiv}},
	author = {Sun, Jennifer J. and Liu, Ting and Cowen, Alan S. and Schroff, Florian and Adam, Hartwig and Prasad, Gautam},
	urldate = {2024-05-01},
	date = {2021-02-22},
	eprinttype = {arxiv},
	eprint = {2001.05488 [cs]},
}

@misc{javedIndicSUPERBSpeechProcessing2022,
	title = {{IndicSUPERB}: A Speech Processing Universal Performance Benchmark for Indian languages},
	url = {http://arxiv.org/abs/2208.11761},
	shorttitle = {{IndicSUPERB}},
	abstract = {A cornerstone in {AI} research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the {GLUE} dataset for training and evaluating Natural Language Understanding ({NLU}) models for English. The large body of research around self-supervised {BERT}-based language models revolved around performance improvements on {NLU} tasks in {GLUE}. To evaluate language models in other languages, several language-specific {GLUE} datasets were created. The area of speech language understanding ({SLU}) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on {SLU} tasks, such as the {SUPERB} benchmark. In this work, we extend this to Indic languages by releasing the {IndicSUPERB} benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline {FBANK}. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76{\textbackslash}\% for the Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope {IndicSUPERB} contributes to the progress of developing speech language understanding models for Indian languages.},
	number = {{arXiv}:2208.11761},
	publisher = {{arXiv}},
	author = {Javed, Tahir and Bhogale, Kaushal Santosh and Raman, Abhigyan and Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh M.},
	urldate = {2024-05-01},
	date = {2022-12-15},
	eprinttype = {arxiv},
	eprint = {2208.11761 [cs, eess]},
}

@misc{fouheyLifestyleVlogsEveryday2017,
	title = {From Lifestyle Vlogs to Everyday Interactions},
	url = {http://arxiv.org/abs/1712.02310},
	abstract = {A major stumbling block to progress in understanding basic human interactions, such as getting out of bed or opening a refrigerator, is lack of good training data. Most past efforts have gathered this data explicitly: starting with a laundry list of action labels, and then querying search engines for videos tagged with each label. In this work, we do the reverse and search implicitly: we start with a large collection of interaction-rich video data and then annotate and analyze it. We use Internet Lifestyle Vlogs as the source of surprisingly large and diverse interaction data. We show that by collecting the data first, we are able to achieve greater scale and far greater diversity in terms of actions and actors. Additionally, our data exposes biases built into common explicitly gathered data. We make sense of our data by analyzing the central component of interaction -- hands. We benchmark two tasks: identifying semantic object contact at the video level and non-semantic contact state at the frame level. We additionally demonstrate future prediction of hands.},
	number = {{arXiv}:1712.02310},
	publisher = {{arXiv}},
	author = {Fouhey, David F. and Kuo, Wei-cheng and Efros, Alexei A. and Malik, Jitendra},
	urldate = {2024-05-01},
	date = {2017-12-06},
	eprinttype = {arxiv},
	eprint = {1712.02310 [cs]},
}

@misc{liTGIFNewDataset2016,
	title = {{TGIF}: A New Dataset and Benchmark on Animated {GIF} Description},
	url = {http://arxiv.org/abs/1604.02748},
	shorttitle = {{TGIF}},
	abstract = {With the recent popularity of animated {GIFs} on social media, there is need for ways to index them with rich metadata. To advance research on animated {GIF} understanding, we collected a new dataset, Tumblr {GIF} ({TGIF}), with 100K animated {GIFs} from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated {GIFs} or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowdworkers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated {GIF} description task, using three representative techniques: nearest neighbor, statistical machine translation, and recurrent neural networks. Finally, we show that models fine-tuned from our animated {GIF} description dataset can be helpful for automatic movie description.},
	number = {{arXiv}:1604.02748},
	publisher = {{arXiv}},
	author = {Li, Yuncheng and Song, Yale and Cao, Liangliang and Tetreault, Joel and Goldberg, Larry and Jaimes, Alejandro and Luo, Jiebo},
	urldate = {2024-05-01},
	date = {2016-04-11},
	eprinttype = {arxiv},
	eprint = {1604.02748 [cs]},
}

@misc{chenKETODKnowledgeEnrichedTaskOriented2022,
	title = {{KETOD}: Knowledge-Enriched Task-Oriented Dialogue},
	url = {http://arxiv.org/abs/2205.05589},
	shorttitle = {{KETOD}},
	abstract = {Existing studies in dialogue system research mostly treat task-oriented dialogue and chit-chat as separate domains. Towards building a human-like assistant that can converse naturally and seamlessly with users, it is important to build a dialogue system that conducts both types of conversations effectively. In this work, we investigate how task-oriented dialogue and knowledge-grounded chit-chat can be effectively integrated into a single model. To this end, we create a new dataset, {KETOD} (Knowledge-Enriched Task-Oriented Dialogue), where we naturally enrich task-oriented dialogues with chit-chat based on relevant entity knowledge. We also propose two new models, {SimpleToDPlus} and Combiner, for the proposed task. Experimental results on both automatic and human evaluations show that the proposed methods can significantly improve the performance in knowledge-enriched response generation while maintaining a competitive task-oriented dialog performance. We believe our new dataset will be a valuable resource for future studies. Our dataset and code are publicly available at {\textbackslash}url\{https://github.com/facebookresearch/ketod\}.},
	number = {{arXiv}:2205.05589},
	publisher = {{arXiv}},
	author = {Chen, Zhiyu and Liu, Bing and Moon, Seungwhan and Sankar, Chinnadhurai and Crook, Paul and Wang, William Yang},
	urldate = {2024-05-01},
	date = {2022-05-11},
	eprinttype = {arxiv},
	eprint = {2205.05589 [cs]},
}

@misc{ericMultiWOZConsolidatedMultiDomain2019,
	title = {{MultiWOZ} 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines},
	url = {http://arxiv.org/abs/1907.01669},
	shorttitle = {{MultiWOZ} 2.1},
	abstract = {{MultiWOZ} 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, {MultiWOZ} 2.0 has a few shortcomings. Firstly, there is substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing {MultiWOZ} 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32\% of state annotations across 40\% of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into {MultiWOZ} 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the {MultiWOZ} 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing {MultiWOZ} 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future.},
	number = {{arXiv}:1907.01669},
	publisher = {{arXiv}},
	author = {Eric, Mihail and Goel, Rahul and Paul, Shachi and Kumar, Adarsh and Sethi, Abhishek and Ku, Peter and Goyal, Anuj Kumar and Agarwal, Sanchit and Gao, Shuyang and Hakkani-Tur, Dilek},
	urldate = {2024-05-01},
	date = {2019-12-03},
	eprinttype = {arxiv},
	eprint = {1907.01669 [cs]},
}

@misc{feigenblatTWEETSUMMDialogSummarization2021,
	title = {{TWEETSUMM} -- A Dialog Summarization Dataset for Customer Service},
	url = {http://arxiv.org/abs/2111.11894},
	abstract = {In a typical customer service chat scenario, customers contact a support center to ask for help or raise complaints, and human agents try to solve the issues. In most cases, at the end of the conversation, agents are asked to write a short summary emphasizing the problem and the proposed solution, usually for the benefit of other agents that may have to deal with the same customer or issue. The goal of the present article is advancing the automation of this task. We introduce the first large scale, high quality, customer care dialog summarization dataset with close to 6500 human annotated summaries. The data is based on real-world customer support dialogs and includes both extractive and abstractive summaries. We also introduce a new unsupervised, extractive summarization method specific to dialogs.},
	number = {{arXiv}:2111.11894},
	publisher = {{arXiv}},
	author = {Feigenblat, Guy and Gunasekara, Chulaka and Sznajder, Benjamin and Joshi, Sachindra and Konopnicki, David and Aharonov, Ranit},
	urldate = {2024-05-01},
	date = {2021-11-23},
	eprinttype = {arxiv},
	eprint = {2111.11894 [cs]},
}

@misc{galaAiravataIntroducingHindi2024,
	title = {Airavata: Introducing Hindi Instruction-tuned {LLM}},
	url = {http://arxiv.org/abs/2401.15006},
	shorttitle = {Airavata},
	abstract = {We announce the initial release of "Airavata," an instruction-tuned {LLM} for Hindi. Airavata was created by fine-tuning {OpenHathi} with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the {IndicInstruct} dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic {LLMs}. Additionally, we present evaluation benchmarks and a framework for assessing {LLM} performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.},
	number = {{arXiv}:2401.15006},
	publisher = {{arXiv}},
	author = {Gala, Jay and Jayakumar, Thanmay and Husain, Jaavid Aktar and M, Aswanth Kumar and Khan, Mohammed Safi Ur Rahman and Kanojia, Diptesh and Puduppully, Ratish and Khapra, Mitesh M. and Dabre, Raj and Murthy, Rudra and Kunchukuttan, Anoop},
	urldate = {2024-05-01},
	date = {2024-02-26},
	eprinttype = {arxiv},
	eprint = {2401.15006 [cs]},
}

@misc{bhogaleEffectivenessMiningAudio2022,
	title = {Effectiveness of Mining Audio and Text Pairs from Public Data for Improving {ASR} Systems for Low-Resource Languages},
	url = {http://arxiv.org/abs/2208.12666},
	abstract = {End-to-end (E2E) models have become the default choice for state-of-the-art speech recognition systems. Such models are trained on large amounts of labelled data, which are often not available for low-resource languages. Techniques such as self-supervised learning and transfer learning hold promise, but have not yet been effective in training accurate models. On the other hand, collecting labelled datasets on a diverse set of domains and speakers is very expensive. In this work, we demonstrate an inexpensive and effective alternative to these approaches by ``mining'' text and audio pairs for Indian languages from public sources, specifically from the public archives of All India Radio. As a key component, we adapt the Needleman-Wunsch algorithm to align sentences with corresponding audio segments given a long audio and a {PDF} of its transcript, while being robust to errors due to {OCR}, extraneous text, and non-transcribed speech. We thus create Shrutilipi, a dataset which contains over 6,400 hours of labelled audio across 12 Indian languages totalling to 4.95M sentences. On average, Shrutilipi results in a 2.3x increase over publicly available labelled data. We establish the quality of Shrutilipi with 21 human evaluators across the 12 languages. We also establish the diversity of Shrutilipi in terms of represented regions, speakers, and mentioned named entities. Significantly, we show that adding Shrutilipi to the training set of Wav2Vec models leads to an average decrease in {WER} of 5.8{\textbackslash}\% for 7 languages on the {IndicSUPERB} benchmark. For Hindi, which has the most benchmarks (7), the average {WER} falls from 18.8\% to 13.5\%. This improvement extends to efficient models: We show a 2.3\% drop in {WER} for a Conformer model (10x smaller than Wav2Vec). Finally, we demonstrate the diversity of Shrutilipi by showing that the model trained with it is more robust to noisy input.},
	number = {{arXiv}:2208.12666},
	publisher = {{arXiv}},
	author = {Bhogale, Kaushal Santosh and Raman, Abhigyan and Javed, Tahir and Doddapaneni, Sumanth and Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh M.},
	urldate = {2024-05-01},
	date = {2022-08-26},
	eprinttype = {arxiv},
	eprint = {2208.12666 [cs, eess]},
}

@misc{conneauFLEURSFewshotLearning2022,
	title = {{FLEURS}: Few-shot Learning Evaluation of Universal Representations of Speech},
	url = {http://arxiv.org/abs/2205.12446},
	shorttitle = {{FLEURS}},
	abstract = {We introduce {FLEURS}, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. {FLEURS} is an n-way parallel speech dataset in 102 languages built on top of the machine translation {FLoRes}-101 benchmark, with approximately 12 hours of speech supervision per language. {FLEURS} can be used for a variety of speech tasks, including Automatic Speech Recognition ({ASR}), Speech Language Identification (Speech {LangID}), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like {mSLAM}. The goal of {FLEURS} is to enable speech technology in more languages and catalyze research in low-resource speech understanding.},
	number = {{arXiv}:2205.12446},
	publisher = {{arXiv}},
	author = {Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur},
	urldate = {2024-05-01},
	date = {2022-05-24},
	eprinttype = {arxiv},
	eprint = {2205.12446 [cs, eess]},
	note = {version: 1},
}

@misc{galvezPeopleSpeechLargeScale2021,
	title = {The People's Speech: A Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage},
	url = {http://arxiv.org/abs/2111.09344},
	shorttitle = {The People's Speech},
	abstract = {The People's Speech is a free-to-download 30,000-hour and growing supervised conversational English speech recognition dataset licensed for academic and commercial usage under {CC}-{BY}-{SA} (with a {CC}-{BY} subset). The data is collected via searching the Internet for appropriately licensed audio data with existing transcriptions. We describe our data collection methodology and release our data collection system under the Apache 2.0 license. We show that a model trained on this dataset achieves a 9.98\% word error rate on Librispeech's test-clean test set.Finally, we discuss the legal and ethical issues surrounding the creation of a sizable machine learning corpora and plans for continued maintenance of the project under {MLCommons}'s sponsorship.},
	number = {{arXiv}:2111.09344},
	publisher = {{arXiv}},
	author = {Galvez, Daniel and Diamos, Greg and Ciro, Juan and Cerón, Juan Felipe and Achorn, Keith and Gopi, Anjali and Kanter, David and Lam, Maximilian and Mazumder, Mark and Reddi, Vijay Janapa},
	urldate = {2024-05-01},
	date = {2021-11-17},
	eprinttype = {arxiv},
	eprint = {2111.09344 [cs, stat]},
}

@misc{wangTHCHS30FreeChinese2015,
	title = {{THCHS}-30 : A Free Chinese Speech Corpus},
	url = {http://arxiv.org/abs/1512.01882},
	shorttitle = {{THCHS}-30},
	abstract = {Speech data is crucially important for speech recognition research. There are quite some speech databases that can be purchased at prices that are reasonable for most research institutes. However, for young people who just start research activities or those who just gain initial interest in this direction, the cost for data is still an annoying barrier. We support the `free data' movement in speech recognition: research institutes (particularly supported by public funds) publish their data freely so that new researchers can obtain sufficient data to kick of their career. In this paper, we follow this trend and release a free Chinese speech database {THCHS}-30 that can be used to build a full- edged Chinese speech recognition system. We report the baseline system established with this database, including the performance under highly noisy conditions.},
	number = {{arXiv}:1512.01882},
	publisher = {{arXiv}},
	author = {Wang, Dong and Zhang, Xuewei},
	urldate = {2024-05-01},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.01882 [cs]},
}

@misc{yeungEveryMomentCounts2017,
	title = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos},
	url = {http://arxiv.org/abs/1507.05738},
	shorttitle = {Every Moment Counts},
	abstract = {Every moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing {THUMOS} dataset and introduce {MultiTHUMOS}, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory ({LSTM}) deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to action prediction.},
	number = {{arXiv}:1507.05738},
	publisher = {{arXiv}},
	author = {Yeung, Serena and Russakovsky, Olga and Jin, Ning and Andriluka, Mykhaylo and Mori, Greg and Fei-Fei, Li},
	urldate = {2024-05-02},
	date = {2017-06-09},
	eprinttype = {arxiv},
	eprint = {1507.05738 [cs]},
}

@misc{monfortMultiMomentsTimeLearning2021,
	title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},
	url = {http://arxiv.org/abs/1911.00232},
	shorttitle = {Multi-Moments in Time},
	abstract = {Videos capture events that typically contain multiple sequential, and simultaneous, actions even in the span of only a few seconds. However, most large-scale datasets built to train models for action recognition in video only provide a single label per video. Consequently, models can be incorrectly penalized for classifying actions that exist in the videos but are not explicitly labeled and do not learn the full spectrum of information present in each video in training. Towards this goal, we present the Multi-Moments in Time dataset (M-{MiT}) which includes over two million action labels for over one million three second videos. This multi-label dataset introduces novel challenges on how to train and analyze models for multi-action detection. Here, we present baseline results for multi-action recognition using loss functions adapted for long tail multi-label learning, provide improved methods for visualizing and interpreting models trained for multi-label action detection and show the strength of transferring models trained on M-{MiT} to smaller datasets.},
	number = {{arXiv}:1911.00232},
	publisher = {{arXiv}},
	author = {Monfort, Mathew and Pan, Bowen and Ramakrishnan, Kandan and Andonian, Alex and {McNamara}, Barry A. and Lascelles, Alex and Fan, Quanfu and Gutfreund, Dan and Feris, Rogerio and Oliva, Aude},
	urldate = {2024-05-02},
	date = {2021-09-27},
	eprinttype = {arxiv},
	eprint = {1911.00232 [cs, eess]},
}

@misc{dettmersQLoRAEfficientFinetuning2023,
	title = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
	url = {http://arxiv.org/abs/2305.14314},
	shorttitle = {{QLoRA}},
	abstract = {We present {QLoRA}, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB {GPU} while preserving full 16-bit finetuning task performance. {QLoRA} backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}({LoRA}). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of {ChatGPT} while only requiring 24 hours of finetuning on a single {GPU}. {QLoRA} introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit {NormalFloat} ({NF}4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use {QLoRA} to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types ({LLaMA}, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that {QLoRA} finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous {SoTA}. We provide a detailed analysis of chatbot performance based on both human and {GPT}-4 evaluations showing that {GPT}-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to {ChatGPT}. We release all of our models and code, including {CUDA} kernels for 4-bit training.},
	number = {{arXiv}:2305.14314},
	publisher = {{arXiv}},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	urldate = {2024-05-01},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14314 [cs]},
}

@misc{liuPKUMMDLargeScale2017,
	title = {{PKU}-{MMD}: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding},
	url = {http://arxiv.org/abs/1703.07475},
	shorttitle = {{PKU}-{MMD}},
	abstract = {Despite the fact that many 3D human activity benchmarks being proposed, most existing action datasets focus on the action recognition tasks for the segmented videos. There is a lack of standard large-scale benchmarks, especially for current popular data-hungry deep learning based methods. In this paper, we introduce a new large scale benchmark ({PKU}-{MMD}) for continuous multi-modality 3D human action understanding and cover a wide range of complex human activities with well annotated information. {PKU}-{MMD} contains 1076 long video sequences in 51 action categories, performed by 66 subjects in three camera views. It contains almost 20,000 action instances and 5.4 million frames in total. Our dataset also provides multi-modality data sources, including {RGB}, depth, Infrared Radiation and Skeleton. With different modalities, we conduct extensive experiments on our dataset in terms of two scenarios and evaluate different methods by various metrics, including a new proposed evaluation protocol 2D-{AP}. We believe this large-scale dataset will benefit future researches on action detection for the community.},
	number = {{arXiv}:1703.07475},
	publisher = {{arXiv}},
	author = {Liu, Chunhui and Hu, Yueyu and Li, Yanghao and Song, Sijie and Liu, Jiaying},
	urldate = {2024-05-01},
	date = {2017-03-27},
	eprinttype = {arxiv},
	eprint = {1703.07475 [cs]},
}

@misc{zhangWenetSpeech10000Hours2022,
	title = {{WenetSpeech}: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition},
	url = {http://arxiv.org/abs/2110.03370},
	shorttitle = {{WenetSpeech}},
	abstract = {In this paper, we present {WenetSpeech}, a multi-domain Mandarin corpus consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in total. We collect the data from {YouTube} and Podcast, which covers a variety of speaking styles, scenarios, domains, topics, and noisy conditions. An optical character recognition ({OCR}) based method is introduced to generate the audio/text segmentation candidates for the {YouTube} data on its corresponding video captions, while a high-quality {ASR} transcription system is used to generate audio/text pair candidates for the Podcast data. Then we propose a novel end-to-end label error detection approach to further validate and filter the candidates. We also provide three manually labelled high-quality test sets along with {WenetSpeech} for evaluation -- Dev for cross-validation purpose in training, Test\_Net, collected from Internet for matched test, and Test{\textbackslash}\_Meeting, recorded from real meetings for more challenging mismatched test. Baseline systems trained with {WenetSpeech} are provided for three popular speech recognition toolkits, namely Kaldi, {ESPnet}, and {WeNet}, and recognition results on the three test sets are also provided as benchmarks. To the best of our knowledge, {WenetSpeech} is the current largest open-sourced Mandarin speech corpus with transcriptions, which benefits research on production-level speech recognition.},
	number = {{arXiv}:2110.03370},
	publisher = {{arXiv}},
	author = {Zhang, Binbin and Lv, Hang and Guo, Pengcheng and Shao, Qijie and Yang, Chao and Xie, Lei and Xu, Xin and Bu, Hui and Chen, Xiaoyu and Zeng, Chenchen and Wu, Di and Peng, Zhendong},
	urldate = {2024-05-01},
	date = {2022-02-23},
	eprinttype = {arxiv},
	eprint = {2110.03370 [cs]},
}

@misc{zhangVideoLTLargescaleLongtailed2021,
	title = {{VideoLT}: Large-scale Long-tailed Video Recognition},
	url = {http://arxiv.org/abs/2105.02668},
	shorttitle = {{VideoLT}},
	abstract = {Label distributions in real-world are oftentimes long-tailed and imbalanced, resulting in biased models towards dominant labels. While long-tailed recognition has been extensively studied for image classification tasks, limited effort has been made for video domain. In this paper, we introduce {VideoLT}, a large-scale long-tailed video recognition dataset, as a step toward real-world video recognition. Our {VideoLT} contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution. Through extensive studies, we demonstrate that state-of-the-art methods used for long-tailed image recognition do not perform well in the video domain due to the additional temporal dimension in video data. This motivates us to propose {FrameStack}, a simple yet effective method for long-tailed video recognition task. In particular, {FrameStack} performs sampling at the frame-level in order to balance class distributions, and the sampling ratio is dynamically determined using knowledge derived from the network during training. Experimental results demonstrate that {FrameStack} can improve classification performance without sacrificing overall accuracy. Code and dataset are available at: https://github.com/17Skye17/{VideoLT}.},
	number = {{arXiv}:2105.02668},
	publisher = {{arXiv}},
	author = {Zhang, Xing and Wu, Zuxuan and Weng, Zejia and Fu, Huazhu and Chen, Jingjing and Jiang, Yu-Gang and Davis, Larry},
	urldate = {2024-05-02},
	date = {2021-08-18},
	eprinttype = {arxiv},
	eprint = {2105.02668 [cs]},
}

@misc{buAISHELL1OpenSourceMandarin2017,
	title = {{AISHELL}-1: An Open-Source Mandarin Speech Corpus and A Speech Recognition Baseline},
	url = {http://arxiv.org/abs/1709.05522},
	shorttitle = {{AISHELL}-1},
	abstract = {An open-source Mandarin speech corpus called {AISHELL}-1 is released. It is by far the largest corpus which is suitable for conducting the speech recognition research and building speech recognition systems for Mandarin. The recording procedure, including audio capturing devices and environments are presented in details. The preparation of the related resources, including transcriptions and lexicon are described. The corpus is released with a Kaldi recipe. Experimental results implies that the quality of audio recordings and transcriptions are promising.},
	number = {{arXiv}:1709.05522},
	publisher = {{arXiv}},
	author = {Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
	urldate = {2024-05-01},
	date = {2017-09-16},
	eprinttype = {arxiv},
	eprint = {1709.05522 [cs]},
}

@misc{ardilaCommonVoiceMassivelyMultilingual2020,
	title = {Common Voice: A Massively-Multilingual Speech Corpus},
	url = {http://arxiv.org/abs/1912.06670},
	shorttitle = {Common Voice},
	abstract = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's {DeepSpeech} Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
	number = {{arXiv}:1912.06670},
	publisher = {{arXiv}},
	author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
	urldate = {2024-05-01},
	date = {2020-03-05},
	eprinttype = {arxiv},
	eprint = {1912.06670 [cs]},
}

@misc{karpovGolosRussianDataset2021,
	title = {Golos: Russian Dataset for Speech Research},
	url = {http://arxiv.org/abs/2106.10161},
	shorttitle = {Golos},
	abstract = {This paper introduces a novel Russian speech dataset called Golos, a large corpus suitable for speech research. The dataset mainly consists of recorded audio files manually annotated on the crowd-sourcing platform. The total duration of the audio is about 1240 hours. We have made the corpus freely available to download, along with the acoustic model with {CTC} loss prepared on this corpus. Additionally, transfer learning was applied to improve the performance of the acoustic model. In order to evaluate the quality of the dataset with the beam-search algorithm, we have built a 3-gram language model on the open Common Crawl dataset. The total word error rate ({WER}) metrics turned out to be about 3.3\% and 11.5\%.},
	number = {{arXiv}:2106.10161},
	publisher = {{arXiv}},
	author = {Karpov, Nikolay and Denisenko, Alexander and Minkin, Fedor},
	urldate = {2024-05-01},
	date = {2021-06-18},
	eprinttype = {arxiv},
	eprint = {2106.10161 [eess]},
}

@misc{barhamMegaWikaMillionsReports2023,
	title = {{MegaWika}: Millions of reports and their sources across 50 diverse languages},
	url = {http://arxiv.org/abs/2307.07049},
	shorttitle = {{MegaWika}},
	abstract = {To foster the development of new models for collaborative {AI}-assisted report generation, we introduce {MegaWika}, consisting of 13 million Wikipedia articles in 50 diverse languages, along with their 71 million referenced source materials. We process this dataset for a myriad of applications, going beyond the initial Wikipedia citation extraction and web scraping of content, including translating non-English articles for cross-lingual applications and providing {FrameNet} parses for automated semantic analysis. {MegaWika} is the largest resource for sentence-level report generation and the only report generation dataset that is multilingual. We manually analyze the quality of this resource through a semantically stratified sample. Finally, we provide baseline results and trained models for crucial steps in automated report generation: cross-lingual question answering and citation retrieval.},
	number = {{arXiv}:2307.07049},
	publisher = {{arXiv}},
	author = {Barham, Samuel and Weller, Orion and Yuan, Michelle and Murray, Kenton and Yarmohammadi, Mahsa and Jiang, Zhengping and Vashishtha, Siddharth and Martin, Alexander and Liu, Anqi and White, Aaron Steven and Boyd-Graber, Jordan and Van Durme, Benjamin},
	urldate = {2024-05-01},
	date = {2023-07-13},
	eprinttype = {arxiv},
	eprint = {2307.07049 [cs]},
}

@misc{abu-el-haijaYouTube8MLargeScaleVideo2016,
	title = {{YouTube}-8M: A Large-Scale Video Classification Benchmark},
	url = {http://arxiv.org/abs/1609.08675},
	shorttitle = {{YouTube}-8M},
	abstract = {Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as {ImageNet}, there are no comparable size video classification datasets. In this paper, we introduce {YouTube}-8M, the largest multi-label video classification dataset, composed of {\textasciitilde}8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a {YouTube} video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep {CNN} pre-trained on {ImageNet} to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using {TensorFlow}. We plan to release code for training a {TensorFlow} model and for computing metrics.},
	number = {{arXiv}:1609.08675},
	publisher = {{arXiv}},
	author = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
	urldate = {2024-05-01},
	date = {2016-09-27},
	eprinttype = {arxiv},
	eprint = {1609.08675 [cs]},
}

@misc{shaoFineGymHierarchicalVideo2020,
	title = {{FineGym}: A Hierarchical Video Dataset for Fine-grained Action Understanding},
	url = {http://arxiv.org/abs/2004.06704},
	shorttitle = {{FineGym}},
	abstract = {On public benchmarks, current action recognition techniques have achieved great success. However, when used in real-world applications, e.g. sport analysis, which requires the capability of parsing an activity into phases and differentiating between subtly different actions, their performances remain far from being satisfactory. To take action recognition to a new level, we develop {FineGym}, a new dataset built on top of gymnastic videos. Compared to existing action recognition datasets, {FineGym} is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a "balance beam" event will be annotated as a sequence of elementary sub-actions derived from five sets: "leap-jump-hop", "beam-turns", "flight-salto", "flight-handspring", and "dismount", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes. We systematically investigate representative methods on this dataset and obtain a number of interesting findings. We hope this dataset could advance research towards action understanding.},
	number = {{arXiv}:2004.06704},
	publisher = {{arXiv}},
	author = {Shao, Dian and Zhao, Yue and Dai, Bo and Lin, Dahua},
	urldate = {2024-05-02},
	date = {2020-04-14},
	eprinttype = {arxiv},
	eprint = {2004.06704 [cs]},
}

@misc{tangCOINLargescaleDataset2019,
	title = {{COIN}: A Large-scale Dataset for Comprehensive Instructional Video Analysis},
	url = {http://arxiv.org/abs/1903.02874},
	shorttitle = {{COIN}},
	abstract = {There are substantial instructional videos on the Internet, which enables us to acquire knowledge for completing various tasks. However, most existing datasets for instructional video analysis have the limitations in diversity and scale,which makes them far from many real-world applications where more diverse activities occur. Moreover, it still remains a great challenge to organize and harness such data. To address these problems, we introduce a large-scale dataset called "{COIN}" for {COmprehensive} {INstructional} video analysis. Organized with a hierarchical structure, the {COIN} dataset contains 11,827 videos of 180 tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. With a new developed toolbox, all the videos are annotated effectively with a series of step descriptions and the corresponding temporal boundaries. Furthermore, we propose a simple yet effective method to capture the dependencies among different steps, which can be easily plugged into conventional proposal-based action detection methods for localizing important steps in instructional videos. In order to provide a benchmark for instructional video analysis, we evaluate plenty of approaches on the {COIN} dataset under different evaluation criteria. We expect the introduction of the {COIN} dataset will promote the future in-depth research on instructional video analysis for the community.},
	number = {{arXiv}:1903.02874},
	publisher = {{arXiv}},
	author = {Tang, Yansong and Ding, Dajun and Rao, Yongming and Zheng, Yu and Zhang, Danyang and Zhao, Lili and Lu, Jiwen and Zhou, Jie},
	urldate = {2024-05-01},
	date = {2019-03-07},
	eprinttype = {arxiv},
	eprint = {1903.02874 [cs]},
}

@misc{goslingPIPPAPartiallySynthetic2023,
	title = {{PIPPA}: A Partially Synthetic Conversational Dataset},
	url = {http://arxiv.org/abs/2308.05884},
	shorttitle = {{PIPPA}},
	abstract = {With the emergence of increasingly powerful large language models, there is a burgeoning interest in leveraging these models for casual conversation and role-play applications. However, existing conversational and role-playing datasets often fail to capture the diverse and nuanced interactions typically exhibited by real-world role-play participants. To address this limitation and contribute to the rapidly growing field, we introduce a partially-synthetic dataset named {PIPPA} (Personal Interaction Pairs between People and {AI}). {PIPPA} is a result of a community-driven crowdsourcing effort involving a group of role-play enthusiasts. The dataset comprises over 1 million utterances that are distributed across 26,000 conversation sessions and provides a rich resource for researchers and {AI} developers to explore and refine conversational {AI} systems in the context of role-play scenarios.},
	number = {{arXiv}:2308.05884},
	publisher = {{arXiv}},
	author = {Gosling, Tear and Dale, Alpin and Zheng, Yinhe},
	urldate = {2024-05-01},
	date = {2023-08-10},
	eprinttype = {arxiv},
	eprint = {2308.05884 [cs]},
}

@misc{kayKineticsHumanAction2017,
	title = {The Kinetics Human Action Video Dataset},
	url = {http://arxiv.org/abs/1705.06950},
	abstract = {We describe the {DeepMind} Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different {YouTube} video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
	number = {{arXiv}:1705.06950},
	publisher = {{arXiv}},
	author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
	urldate = {2024-05-01},
	date = {2017-05-19},
	eprinttype = {arxiv},
	eprint = {1705.06950 [cs]},
}

@misc{soomroUCF101Dataset1012012,
	title = {{UCF}101: A Dataset of 101 Human Actions Classes From Videos in The Wild},
	url = {http://arxiv.org/abs/1212.0402},
	shorttitle = {{UCF}101},
	abstract = {We introduce {UCF}101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5\%. To the best of our knowledge, {UCF}101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.},
	number = {{arXiv}:1212.0402},
	publisher = {{arXiv}},
	author = {Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
	urldate = {2024-05-01},
	date = {2012-12-03},
	eprinttype = {arxiv},
	eprint = {1212.0402 [cs]},
}

@misc{dinanSecondConversationalIntelligence2019,
	title = {The Second Conversational Intelligence Challenge ({ConvAI}2)},
	url = {http://arxiv.org/abs/1902.00098},
	abstract = {We describe the setting and results of the {ConvAI}2 {NeurIPS} competition that aims to further the state-of-the-art in open-domain chatbots. Some key takeaways from the competition are: (i) pretrained Transformer variants are currently the best performing models on this task, (ii) but to improve performance on multi-turn conversations with humans, future systems must go beyond single word metrics like perplexity to measure the performance across sequences of utterances (conversations) -- in terms of repetition, consistency and balance of dialogue acts (e.g. how many questions asked vs. answered).},
	number = {{arXiv}:1902.00098},
	publisher = {{arXiv}},
	author = {Dinan, Emily and Logacheva, Varvara and Malykh, Valentin and Miller, Alexander and Shuster, Kurt and Urbanek, Jack and Kiela, Douwe and Szlam, Arthur and Serban, Iulian and Lowe, Ryan and Prabhumoye, Shrimai and Black, Alan W. and Rudnicky, Alexander and Williams, Jason and Pineau, Joelle and Burtsev, Mikhail and Weston, Jason},
	urldate = {2024-05-01},
	date = {2019-01-31},
	eprinttype = {arxiv},
	eprint = {1902.00098 [cs]},
}

@misc{wangVoxPopuliLargeScaleMultilingual2021,
	title = {{VoxPopuli}: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},
	url = {http://arxiv.org/abs/2101.00390},
	shorttitle = {{VoxPopuli}},
	abstract = {We introduce {VoxPopuli}, a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. {VoxPopuli} also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours. We provide speech recognition baselines and validate the versatility of {VoxPopuli} unlabelled data in semi-supervised learning under challenging out-of-domain settings. We will release the corpus at https://github.com/facebookresearch/voxpopuli under an open license.},
	number = {{arXiv}:2101.00390},
	publisher = {{arXiv}},
	author = {Wang, Changhan and Rivière, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
	urldate = {2024-05-01},
	date = {2021-07-27},
	eprinttype = {arxiv},
	eprint = {2101.00390 [cs, eess]},
}

@misc{chenTheoremQATheoremdrivenQuestion2023,
	title = {{TheoremQA}: A Theorem-driven Question Answering dataset},
	url = {http://arxiv.org/abs/2305.12524},
	shorttitle = {{TheoremQA}},
	abstract = {The recent {LLMs} like {GPT}-4 and {PaLM}-2 have made tremendous progress in solving fundamental math problems like {GSM}8K by achieving over 90\% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce {TheoremQA}, the first theorem-driven question-answering dataset designed to evaluate {AI} models' capabilities to apply theorems to solve challenging science problems. {TheoremQA} is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, {EE}\&{CS}, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that {GPT}-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51\% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15\%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of {TheoremQA}, we believe it can be used as a better benchmark to evaluate {LLMs}' capabilities to solve challenging science problems. The data and code are released in https://github.com/wenhuchen/{TheoremQA}.},
	number = {{arXiv}:2305.12524},
	publisher = {{arXiv}},
	author = {Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
	urldate = {2024-05-01},
	date = {2023-12-05},
	eprinttype = {arxiv},
	eprint = {2305.12524 [cs]},
}

@misc{monfortMomentsTimeDataset2019,
	title = {Moments in Time Dataset: one million videos for event understanding},
	url = {http://arxiv.org/abs/1801.03150},
	shorttitle = {Moments in Time Dataset},
	abstract = {We present the Moments in Time Dataset, a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people, but also objects, animals, and natural phenomena; visual and auditory events can be symmetrical in time ("opening" is "closing" in reverse), and either transient or sustained. We describe the annotation process of our dataset (each video is tagged with one action or activity label among 339 different classes), analyze its scale and diversity in comparison to other large-scale video datasets for action recognition, and report results of several baseline models addressing separately, and jointly, three modalities: spatial, temporal and auditory. The Moments in Time dataset, designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.},
	number = {{arXiv}:1801.03150},
	publisher = {{arXiv}},
	author = {Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Tom and Brown, Lisa and Fan, Quanfu and Gutfruend, Dan and Vondrick, Carl and Oliva, Aude},
	urldate = {2024-05-02},
	date = {2019-02-16},
	eprinttype = {arxiv},
	eprint = {1801.03150 [cs]},
}

@misc{coopeSpanConveRTFewshotSpan2020,
	title = {Span-{ConveRT}: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations},
	url = {http://arxiv.org/abs/2005.08866},
	shorttitle = {Span-{ConveRT}},
	abstract = {We introduce Span-{ConveRT}, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as {ConveRT} (Henderson et al., 2019). We show that leveraging such knowledge in Span-{ConveRT} is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a {BERT}-based span extractor. In order to inspire more work on span extraction for the slot-filling task, we also release {RESTAURANTS}-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain.},
	number = {{arXiv}:2005.08866},
	publisher = {{arXiv}},
	author = {Coope, Sam and Farghly, Tyler and Gerz, Daniela and Vulić, Ivan and Henderson, Matthew},
	urldate = {2024-05-01},
	date = {2020-07-16},
	eprinttype = {arxiv},
	eprint = {2005.08866 [cs]},
}

@misc{carreiraShortNoteKinetics6002018,
	title = {A Short Note about Kinetics-600},
	url = {http://arxiv.org/abs/1808.01340},
	abstract = {We describe an extension of the {DeepMind} Kinetics human action dataset from 400 classes, each with at least 400 video clips, to 600 classes, each with at least 600 video clips. In order to scale up the dataset we changed the data collection process so it uses multiple queries per class, with some of them in a language other than english -- portuguese. This paper details the changes between the two versions of the dataset and includes a comprehensive set of statistics of the new version as well as baseline results using the I3D neural network architecture. The paper is a companion to the release of the ground truth labels for the public test set.},
	number = {{arXiv}:1808.01340},
	publisher = {{arXiv}},
	author = {Carreira, Joao and Noland, Eric and Banki-Horvath, Andras and Hillier, Chloe and Zisserman, Andrew},
	urldate = {2024-05-01},
	date = {2018-08-03},
	eprinttype = {arxiv},
	eprint = {1808.01340 [cs]},
}

@misc{wangVATEXLargeScaleHighQuality2020,
	title = {{VATEX}: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},
	url = {http://arxiv.org/abs/1904.03493},
	shorttitle = {{VATEX}},
	abstract = {We present a new large-scale multilingual video description dataset, {VATEX}, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used {MSR}-{VTT} dataset, {VATEX} is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on {VATEX}: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the {VATEX} dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using {VATEX} for other video-and-language research.},
	number = {{arXiv}:1904.03493},
	publisher = {{arXiv}},
	author = {Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
	urldate = {2024-05-01},
	date = {2020-06-17},
	eprinttype = {arxiv},
	eprint = {1904.03493 [cs]},
}

@misc{baiLongAlignRecipeLong2024,
	title = {{LongAlign}: A Recipe for Long Context Alignment of Large Language Models},
	url = {http://arxiv.org/abs/2401.18058},
	shorttitle = {{LongAlign}},
	abstract = {Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present {LongAlign} -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the {LongBench}-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that {LongAlign} outperforms existing recipes for {LLMs} in long context tasks by up to 30{\textbackslash}\%, while also maintaining their proficiency in handling short, generic tasks. The code, data, and long-aligned models are open-sourced at https://github.com/{THUDM}/{LongAlign}.},
	number = {{arXiv}:2401.18058},
	publisher = {{arXiv}},
	author = {Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi},
	urldate = {2024-05-01},
	date = {2024-01-31},
	eprinttype = {arxiv},
	eprint = {2401.18058 [cs]},
}

@misc{liUAVHumanLargeBenchmark2021,
	title = {{UAV}-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles},
	url = {http://arxiv.org/abs/2104.00946},
	shorttitle = {{UAV}-Human},
	abstract = {Human behavior understanding with unmanned aerial vehicles ({UAVs}) is of great significance for a wide range of applications, which simultaneously brings an urgent demand of large, challenging, and comprehensive benchmarks for the development and evaluation of {UAV}-based models. However, existing benchmarks have limitations in terms of the amount of captured data, types of data modalities, categories of provided tasks, and diversities of subjects and environments. Here we propose a new benchmark - {UAVHuman} - for human behavior understanding with {UAVs}, which contains 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition. Our dataset was collected by a flying {UAV} in multiple urban and rural districts in both daytime and nighttime over three months, hence covering extensive diversities w.r.t subjects, backgrounds, illuminations, weathers, occlusions, camera motions, and {UAV} flying attitudes. Such a comprehensive and challenging benchmark shall be able to promote the research of {UAV}-based human behavior understanding, including action recognition, pose estimation, re-identification, and attribute recognition. Furthermore, we propose a fisheye-based action recognition method that mitigates the distortions in fisheye videos via learning unbounded transformations guided by flat {RGB} videos. Experiments show the efficacy of our method on the {UAV}-Human dataset. The project page: https://github.com/{SUTDCV}/{UAV}-Human},
	number = {{arXiv}:2104.00946},
	publisher = {{arXiv}},
	author = {Li, Tianjiao and Liu, Jun and Zhang, Wei and Ni, Yun and Wang, Wenqian and Li, Zhiheng},
	urldate = {2024-05-02},
	date = {2021-08-15},
	eprinttype = {arxiv},
	eprint = {2104.00946 [cs]},
}

@misc{zhouAutomaticLearningProcedures2017,
	title = {Towards Automatic Learning of Procedures from Web Instructional Videos},
	url = {https://arxiv.org/abs/1703.09788v3},
	abstract = {The potential for agents, whether embodied or software, to learn by observing other agents performing procedures involving objects and actions is rich. Current research on automatic procedure learning heavily relies on action labels or video subtitles, even during the evaluation phase, which makes them infeasible in real-world scenarios. This leads to our question: can the human-consensus structure of a procedure be learned from a large set of long, unconstrained videos (e.g., instructional videos from {YouTube}) with only visual evidence? To answer this question, we introduce the problem of procedure segmentation--to segment a video procedure into category-independent procedure segments. Given that no large-scale dataset is available for this problem, we collect a large-scale procedure segmentation dataset with procedure segments temporally localized and described; we use cooking videos and name the dataset {YouCook}2. We propose a segment-level recurrent network for generating procedure segments by modeling the dependencies across segments. The generated segments can be used as pre-processing for other tasks, such as dense video captioning and event parsing. We show in our experiments that the proposed model outperforms competitive baselines in procedure segmentation.},
	number = {{arXiv}:1703.09788},
	publisher = {{arXiv}},
	author = {Zhou, Luowei and Xu, Chenliang and Corso, Jason J.},
	urldate = {2024-05-02},
	date = {2017-03-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.09788 [cs]},
}

@misc{chungHAA500HumanCentricAtomic2021,
	title = {{HAA}500: Human-Centric Atomic Action Dataset with Curated Videos},
	url = {http://arxiv.org/abs/2009.05224},
	shorttitle = {{HAA}500},
	abstract = {We contribute {HAA}500, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591K labeled frames. To minimize ambiguities in action classification, {HAA}500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g., "Baseball Pitching" vs "Free Throw in Basketball". Thus {HAA}500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as "Throw". {HAA}500 has been carefully curated to capture the precise movement of human figures with little class-irrelevant motions or spatio-temporal label noises. The advantages of {HAA}500 are fourfold: 1) human-centric actions with a high average of 69.7\% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20-60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic action classes. Our extensive experiments including cross-data validation using datasets collected in the wild demonstrate the clear benefits of human-centric and atomic characteristics of {HAA}500, which enable training even a baseline deep learning model to improve prediction by attending to atomic human poses. We detail the {HAA}500 dataset statistics and collection methodology and compare quantitatively with existing action recognition datasets.},
	number = {{arXiv}:2009.05224},
	publisher = {{arXiv}},
	author = {Chung, Jihoon and Wuu, Cheng-hsin and Yang, Hsuan-ru and Tai, Yu-Wing and Tang, Chi-Keung},
	urldate = {2024-05-02},
	date = {2021-08-16},
	eprinttype = {arxiv},
	eprint = {2009.05224 [cs, eess]},
}

@misc{wangFERV39kLargeScaleMultiScene2022,
	title = {{FERV}39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos},
	url = {http://arxiv.org/abs/2203.09463},
	shorttitle = {{FERV}39k},
	abstract = {Current benchmarks for facial expression recognition ({FER}) mainly focus on static images, while there are limited datasets for {FER} in videos. It is still ambiguous to evaluate whether performances of existing methods remain satisfactory in real-world application-oriented scenes. For example, the "Happy" expression with high intensity in Talk-Show is more discriminating than the same expression with low intensity in Official-Event. To fill this gap, we build a large-scale multi-scene dataset, coined as {FERV}39k. We analyze the important ingredients of constructing such a novel dataset in three aspects: (1) multi-scene hierarchy and expression class, (2) generation of candidate video clips, (3) trusted manual labelling process. Based on these guidelines, we select 4 scenarios subdivided into 22 scenes, annotate 86k samples automatically obtained from 4k videos based on the well-designed workflow, and finally build 38,935 video clips labeled with 7 classic expressions. Experiment benchmarks on four kinds of baseline frameworks were also provided and further analysis on their performance across different scenes and some challenges for future research were given. Besides, we systematically investigate key components of {DFER} by ablation studies. The baseline framework and our project will be available.},
	number = {{arXiv}:2203.09463},
	publisher = {{arXiv}},
	author = {Wang, Yan and Sun, Yixuan and Huang, Yiwen and Liu, Zhongying and Gao, Shuyong and Zhang, Wei and Ge, Weifeng and Zhang, Wenqiang},
	urldate = {2024-05-02},
	date = {2022-03-20},
	eprinttype = {arxiv},
	eprint = {2203.09463 [cs]},
}

@misc{yanSocialAdaptiveModule2020,
	title = {Social Adaptive Module for Weakly-supervised Group Activity Recognition},
	url = {http://arxiv.org/abs/2007.09470},
	abstract = {This paper presents a new task named weakly-supervised group activity recognition ({GAR}) which differs from conventional {GAR} tasks in that only video-level labels are available, yet the important persons within each frame are not provided even in the training data. This eases us to collect and annotate a large-scale {NBA} dataset and thus raise new challenges to {GAR}. To mine useful information from weak supervision, we present a key insight that key instances are likely to be related to each other, and thus design a social adaptive module ({SAM}) to reason about key persons and frames from noisy data. Experiments show significant improvement on the {NBA} dataset as well as the popular volleyball dataset. In particular, our model trained on video-level annotation achieves comparable accuracy to prior algorithms which required strong labels.},
	number = {{arXiv}:2007.09470},
	publisher = {{arXiv}},
	author = {Yan, Rui and Xie, Lingxi and Tang, Jinhui and Shu, Xiangbo and Tian, Qi},
	urldate = {2024-05-02},
	date = {2020-07-18},
	eprinttype = {arxiv},
	eprint = {2007.09470 [cs]},
}

@misc{jiaLEMMAMultiviewDataset2020,
	title = {{LEMMA}: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},
	url = {http://arxiv.org/abs/2007.15781},
	shorttitle = {{LEMMA}},
	abstract = {Understanding and interpreting human actions is a long-standing challenge and a critical indicator of perception in artificial intelligence. However, a few imperative components of daily human activities are largely missed in prior literature, including the goal-directed actions, concurrent multi-tasks, and collaborations among multi-agents. We introduce the {LEMMA} dataset to provide a single home to address these missing dimensions with meticulously designed settings, wherein the number of tasks and agents varies to highlight different learning objectives. We densely annotate the atomic-actions with human-object interactions to provide ground-truths of the compositionality, scheduling, and assignment of daily activities. We further devise challenging compositional action recognition and action/task anticipation benchmarks with baseline models to measure the capability of compositional action understanding and temporal reasoning. We hope this effort would drive the machine vision community to examine goal-directed human activities and further study the task scheduling and assignment in the real world.},
	number = {{arXiv}:2007.15781},
	publisher = {{arXiv}},
	author = {Jia, Baoxiong and Chen, Yixin and Huang, Siyuan and Zhu, Yixin and Zhu, Song-chun},
	urldate = {2024-05-02},
	date = {2020-07-30},
	eprinttype = {arxiv},
	eprint = {2007.15781 [cs]},
}

@misc{zhukovCrosstaskWeaklySupervised2019,
	title = {Cross-task weakly supervised learning from instructional videos},
	url = {http://arxiv.org/abs/1903.08225},
	abstract = {In this paper we investigate learning visual models for the steps of ordinary tasks using weak supervision via instructional narrations and an ordered list of steps instead of strong supervision via temporal annotations. At the heart of our approach is the observation that weakly supervised learning may be easier if a model shares components while learning different steps: `pour egg' should be trained jointly with other tasks involving `pour' and `egg'. We formalize this in a component model for recognizing steps and a weakly supervised learning framework that can learn this model under temporal constraints from narration and the list of steps. Past data does not permit systematic studying of sharing and so we also gather a new dataset, {CrossTask}, aimed at assessing cross-task sharing. Our experiments demonstrate that sharing across tasks improves performance, especially when done at the component level and that our component model can parse previously unseen tasks by virtue of its compositionality.},
	number = {{arXiv}:1903.08225},
	publisher = {{arXiv}},
	author = {Zhukov, Dimitri and Alayrac, Jean-Baptiste and Cinbis, Ramazan Gokberk and Fouhey, David and Laptev, Ivan and Sivic, Josef},
	urldate = {2024-05-01},
	date = {2019-04-29},
	eprinttype = {arxiv},
	eprint = {1903.08225 [cs]},
}

@misc{damenScalingEgocentricVision2018,
	title = {Scaling Egocentric Vision: The {EPIC}-{KITCHENS} Dataset},
	url = {http://arxiv.org/abs/1804.02748},
	shorttitle = {Scaling Egocentric Vision},
	abstract = {First-person vision is gaining interest as it offers a unique viewpoint on people's interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce {EPIC}-{KITCHENS}, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict nonscripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labeled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. Dataset and Project page: http://epic-kitchens.github.io},
	number = {{arXiv}:1804.02748},
	publisher = {{arXiv}},
	author = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael},
	urldate = {2024-05-01},
	date = {2018-07-31},
	eprinttype = {arxiv},
	eprint = {1804.02748 [cs]},
}

@misc{zhaoHACSHumanAction2019,
	title = {{HACS}: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},
	url = {http://arxiv.org/abs/1712.09374},
	shorttitle = {{HACS}},
	abstract = {This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as {HACS} (Human Action Clips and Segments). We leverage both consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed {HACS} Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called {HACS} Segments. Overall, {HACS} Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and {HACS} Seg-ments contains 139K action segments densely annotatedin 50K untrimmed videos spanning 200 action categories. {HACS} Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transferlearning experiments on three target datasets, {HACS} Clips outperforms Kinetics-600, Moments-In-Time and Sports1Mas a pretraining source. On {HACS} Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations.},
	number = {{arXiv}:1712.09374},
	publisher = {{arXiv}},
	author = {Zhao, Hang and Torralba, Antonio and Torresani, Lorenzo and Yan, Zhicheng},
	urldate = {2024-05-01},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1712.09374 [cs]},
}

@misc{dibaLargeScaleHolistic2020,
	title = {Large Scale Holistic Video Understanding},
	url = {http://arxiv.org/abs/1904.11451},
	abstract = {Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale "Holistic Video Understanding Dataset"{\textasciitilde}({HVU}). {HVU} is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. {HVU} contains approx.{\textasciitilde}572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. {HVU} encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios. We demonstrate the generalization capability of {HVU} on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called "Holistic Appearance and Temporal Network"{\textasciitilde}({HATNet}) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. {HATNet} focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications.},
	number = {{arXiv}:1904.11451},
	publisher = {{arXiv}},
	author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, Jurgen and Stiefelhagen, Rainer and Van Gool, Luc},
	urldate = {2024-05-01},
	date = {2020-12-15},
	eprinttype = {arxiv},
	eprint = {1904.11451 [cs]},
}

@misc{miechRareActVideoDataset2020,
	title = {{RareAct}: A video dataset of unusual interactions},
	url = {http://arxiv.org/abs/2008.01018},
	shorttitle = {{RareAct}},
	abstract = {This paper introduces a manually annotated video dataset of unusual actions, namely {RareAct}, including actions such as "blend phone", "cut keyboard" and "microwave shoes". {RareAct} aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from {HowTo}100M, but that frequently appear separately. We provide benchmarks using a state-of-the-art {HowTo}100M pretrained video and text model and show that zero-shot and few-shot compositionality of actions remains a challenging and unsolved task.},
	number = {{arXiv}:2008.01018},
	publisher = {{arXiv}},
	author = {Miech, Antoine and Alayrac, Jean-Baptiste and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
	urldate = {2024-05-02},
	date = {2020-08-03},
	eprinttype = {arxiv},
	eprint = {2008.01018 [cs]},
}

@misc{weinzaepfelMimeticsUnderstandingHuman2021,
	title = {Mimetics: Towards Understanding Human Actions Out of Context},
	url = {http://arxiv.org/abs/1912.07249},
	shorttitle = {Mimetics},
	abstract = {Recent methods for video action recognition have reached outstanding performances on existing benchmarks. However, they tend to leverage context such as scenes or objects instead of focusing on understanding the human action itself. For instance, a tennis field leads to the prediction playing tennis irrespectively of the actions performed in the video. In contrast, humans have a more complete understanding of actions and can recognize them without context. The best example of out-of-context actions are mimes, that people can typically recognize despite missing relevant objects and scenes. In this paper, we propose to benchmark action recognition methods in such absence of context and introduce a novel dataset, Mimetics, consisting of mimed actions for a subset of 50 classes from the Kinetics benchmark. Our experiments show that (a) state-of-the-art 3D convolutional neural networks obtain disappointing results on such videos, highlighting the lack of true understanding of the human actions and (b) models leveraging body language via human pose are less prone to context biases. In particular, we show that applying a shallow neural network with a single temporal convolution over body pose features transferred to the action recognition problem performs surprisingly well compared to 3D action recognition methods.},
	number = {{arXiv}:1912.07249},
	publisher = {{arXiv}},
	author = {Weinzaepfel, Philippe and Rogez, Grégory},
	urldate = {2024-05-01},
	date = {2021-02-02},
	eprinttype = {arxiv},
	eprint = {1912.07249 [cs]},
}

@misc{chenGigaSpeechEvolvingMultidomain2021,
	title = {{GigaSpeech}: An Evolving, Multi-domain {ASR} Corpus with 10,000 Hours of Transcribed Audio},
	url = {http://arxiv.org/abs/2106.06909},
	shorttitle = {{GigaSpeech}},
	abstract = {This paper introduces {GigaSpeech}, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and {YouTube}, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription. For system training, {GigaSpeech} provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour {XL} training subset, we cap the word error rate at 4\% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0\%. The {DEV} and {TEST} evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality. Baseline systems are provided for popular speech recognition toolkits, namely Athena, {ESPnet}, Kaldi and Pika.},
	number = {{arXiv}:2106.06909},
	publisher = {{arXiv}},
	author = {Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and Jin, Mingjie and Khudanpur, Sanjeev and Watanabe, Shinji and Zhao, Shuaijiang and Zou, Wei and Li, Xiangang and Yao, Xuchen and Wang, Yongqing and Wang, Yujun and You, Zhao and Yan, Zhiyong},
	urldate = {2024-05-01},
	date = {2021-06-13},
	eprinttype = {arxiv},
	eprint = {2106.06909 [cs, eess]},
}

@misc{sharghiQueryFocusedVideoSummarization2017,
	title = {Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach},
	url = {http://arxiv.org/abs/1707.04960},
	shorttitle = {Query-Focused Video Summarization},
	abstract = {Recent years have witnessed a resurgence of interest in video summarization. However, one of the main obstacles to the research on video summarization is the user subjectivity - users have various preferences over the summaries. The subjectiveness causes at least two problems. First, no single video summarizer fits all users unless it interacts with and adapts to the individual users. Second, it is very challenging to evaluate the performance of a video summarizer. To tackle the first problem, we explore the recently proposed query-focused video summarization which introduces user preferences in the form of text queries about the video into the summarization process. We propose a memory network parameterized sequential determinantal point process in order to attend the user query onto different video frames and shots. To address the second challenge, we contend that a good evaluation metric for video summarization should focus on the semantic information that humans can perceive rather than the visual features or temporal overlaps. To this end, we collect dense per-video-shot concept annotations, compile a new dataset, and suggest an efficient evaluation method defined upon the concept annotations. We conduct extensive experiments contrasting our video summarizer to existing ones and present detailed analyses about the dataset and the new evaluation method.},
	number = {{arXiv}:1707.04960},
	publisher = {{arXiv}},
	author = {Sharghi, Aidean and Laurel, Jacob S. and Gong, Boqing},
	urldate = {2024-05-01},
	date = {2017-07-16},
	eprinttype = {arxiv},
	eprint = {1707.04960 [cs]},
}

@misc{wangCoVoSTMassivelyMultilingual2020,
	title = {{CoVoST} 2 and Massively Multilingual Speech-to-Text Translation},
	url = {http://arxiv.org/abs/2007.10310},
	abstract = {Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release {CoVoST} 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under {CC}0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines with open-source implementation.},
	number = {{arXiv}:2007.10310},
	publisher = {{arXiv}},
	author = {Wang, Changhan and Wu, Anne and Pino, Juan},
	urldate = {2024-05-01},
	date = {2020-10-24},
	eprinttype = {arxiv},
	eprint = {2007.10310 [cs, eess]},
}

@misc{graumanEgo4DWorld0002022,
	title = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},
	url = {http://arxiv.org/abs/2110.07058},
	shorttitle = {Ego4D},
	abstract = {We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/},
	number = {{arXiv}:2110.07058},
	publisher = {{arXiv}},
	author = {Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Gebreselasie, Abrham and Gonzalez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kolar, Jachym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Puentes, Paola Ruiz and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhao, Ziwei and Zhu, Yunyi and Arbelaez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Fuegen, Christian and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra},
	urldate = {2024-05-02},
	date = {2022-03-11},
	eprinttype = {arxiv},
	eprint = {2110.07058 [cs]},
}

@misc{sigurdssonActorObserverJoint2018,
	title = {Actor and Observer: Joint Modeling of First and Third-Person Videos},
	url = {http://arxiv.org/abs/1804.09627},
	shorttitle = {Actor and Observer},
	abstract = {Several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a first-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and first-person (actor). Despite this, learning such models for human action recognition has not been achievable due to the lack of data. This paper takes a step in this direction, with the introduction of Charades-Ego, a large-scale dataset of paired first-person and third-person videos, involving 112 people, with 4000 paired videos. This enables learning the link between the two, actor and observer perspectives. Thereby, we address one of the biggest bottlenecks facing egocentric vision research, providing a link from first-person to the abundant third-person data on the web. We use this data to learn a joint representation of first and third-person videos, with only weak supervision, and show its effectiveness for transferring knowledge from the third-person to the first-person domain.},
	number = {{arXiv}:1804.09627},
	publisher = {{arXiv}},
	author = {Sigurdsson, Gunnar A. and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
	urldate = {2024-05-01},
	date = {2018-04-25},
	eprinttype = {arxiv},
	eprint = {1804.09627 [cs]},
}

@misc{alayracUnsupervisedLearningNarrated2016,
	title = {Unsupervised Learning from Narrated Instruction Videos},
	url = {http://arxiv.org/abs/1506.09215},
	abstract = {We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner, the main steps to achieve the task and locate the steps in the input videos.},
	number = {{arXiv}:1506.09215},
	publisher = {{arXiv}},
	author = {Alayrac, Jean-Baptiste and Bojanowski, Piotr and Agrawal, Nishant and Sivic, Josef and Laptev, Ivan and Lacoste-Julien, Simon},
	urldate = {2024-05-01},
	date = {2016-06-28},
	eprinttype = {arxiv},
	eprint = {1506.09215 [cs]},
}

@misc{demirTinyVIRATLowresolutionVideo2020,
	title = {{TinyVIRAT}: Low-resolution Video Action Recognition},
	url = {http://arxiv.org/abs/2007.07355},
	shorttitle = {{TinyVIRAT}},
	abstract = {The existing research in action recognition is mostly focused on high-quality videos where the action is distinctly visible. In real-world surveillance environments, the actions in videos are captured at a wide range of resolutions. Most activities occur at a distance with a small resolution and recognizing such activities is a challenging problem. In this work, we focus on recognizing tiny actions in videos. We introduce a benchmark dataset, {TinyVIRAT}, which contains natural low-resolution activities. The actions in {TinyVIRAT} videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging. We propose a novel method for recognizing tiny actions in videos which utilizes a progressive generative approach to improve the quality of low-resolution actions. The proposed method also consists of a weakly trained attention mechanism which helps in focusing on the activity regions in the video. We perform extensive experiments to benchmark the proposed {TinyVIRAT} dataset and observe that the proposed method significantly improves the action recognition performance over baselines. We also evaluate the proposed approach on synthetically resized action recognition datasets and achieve state-of-the-art results when compared with existing methods. The dataset and code is publicly available at https://github.com/{UgurDemir}/Tiny-{VIRAT}.},
	number = {{arXiv}:2007.07355},
	publisher = {{arXiv}},
	author = {Demir, Ugur and Rawat, Yogesh S. and Shah, Mubarak},
	urldate = {2024-05-02},
	date = {2020-07-14},
	eprinttype = {arxiv},
	eprint = {2007.07355 [cs, eess]},
}

@misc{ibrahimHierarchicalDeepTemporal2016,
	title = {A Hierarchical Deep Temporal Model for Group Activity Recognition},
	url = {http://arxiv.org/abs/1511.06040},
	abstract = {In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on {LSTM} (long-short term memory) models. To make use of these ob- servations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a {LSTM} model is designed to represent action dynamics of in- dividual people in a sequence and another {LSTM} model is designed to aggregate human-level information for whole activity understanding. We evaluate our model over two datasets: the collective activity dataset and a new volley- ball dataset. Experimental results demonstrate that our proposed model improves group activity recognition perfor- mance with compared to baseline methods.},
	number = {{arXiv}:1511.06040},
	publisher = {{arXiv}},
	author = {Ibrahim, Moustafa and Muralidharan, Srikanth and Deng, Zhiwei and Vahdat, Arash and Mori, Greg},
	urldate = {2024-05-01},
	date = {2016-04-05},
	eprinttype = {arxiv},
	eprint = {1511.06040 [cs]},
}

@misc{sigurdssonHollywoodHomesCrowdsourcing2016,
	title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},
	url = {http://arxiv.org/abs/1604.01753},
	shorttitle = {Hollywood in Homes},
	abstract = {Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on {YouTube}, in movies or {TV} broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 seconds, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and 41,104 labels for 46 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community.},
	number = {{arXiv}:1604.01753},
	publisher = {{arXiv}},
	author = {Sigurdsson, Gunnar A. and Varol, Gül and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
	urldate = {2024-05-01},
	date = {2016-07-26},
	eprinttype = {arxiv},
	eprint = {1604.01753 [cs]},
}

@misc{smairaShortNoteKinetics70020202020,
	title = {A Short Note on the Kinetics-700-2020 Human Action Dataset},
	url = {http://arxiv.org/abs/2010.10864},
	abstract = {We describe the 2020 edition of the {DeepMind} Kinetics human action dataset, which replenishes and extends the Kinetics-700 dataset. In this new version, there are at least 700 video clips from different {YouTube} videos for each of the 700 classes. This paper details the changes introduced for this new release of the dataset and includes a comprehensive set of statistics as well as baseline results using the I3D network.},
	number = {{arXiv}:2010.10864},
	publisher = {{arXiv}},
	author = {Smaira, Lucas and Carreira, João and Noland, Eric and Clancy, Ellen and Wu, Amy and Zisserman, Andrew},
	urldate = {2024-05-02},
	date = {2020-10-21},
	eprinttype = {arxiv},
	eprint = {2010.10864 [cs]},
}

@misc{soldanMADScalableDataset2022,
	title = {{MAD}: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},
	url = {http://arxiv.org/abs/2112.00431},
	shorttitle = {{MAD}},
	abstract = {The recent and increasing interest in video-language research has driven the development of large-scale datasets that enable data-intensive machine learning techniques. In comparison, limited effort has been made at assessing the fitness of these datasets for the video-language grounding task. Recent works have begun to discover significant limitations in these datasets, suggesting that state-of-the-art techniques commonly overfit to hidden dataset biases. In this work, we present {MAD} (Movie Audio Descriptions), a novel benchmark that departs from the paradigm of augmenting existing video datasets with text annotations and focuses on crawling and aligning available audio descriptions of mainstream movies. {MAD} contains over 384,000 natural language sentences grounded in over 1,200 hours of videos and exhibits a significant reduction in the currently diagnosed biases for video-language grounding datasets. {MAD}'s collection strategy enables a novel and more challenging version of video-language grounding, where short temporal moments (typically seconds long) must be accurately grounded in diverse long-form videos that can last up to three hours. We have released {MAD}'s data and baselines code at https://github.com/Soldelli/{MAD}.},
	number = {{arXiv}:2112.00431},
	publisher = {{arXiv}},
	author = {Soldan, Mattia and Pardo, Alejandro and Alcázar, Juan León and Heilbron, Fabian Caba and Zhao, Chen and Giancola, Silvio and Ghanem, Bernard},
	urldate = {2024-05-02},
	date = {2022-03-28},
	eprinttype = {arxiv},
	eprint = {2112.00431 [cs]},
}

@misc{guptaSemanticParsingTask2018,
	title = {Semantic Parsing for Task Oriented Dialog using Hierarchical Representations},
	url = {http://arxiv.org/abs/1810.07942},
	abstract = {Task oriented dialog systems typically first parse user utterances to semantic frames comprised of intents and slots. Previous work on task oriented intent and slot-filling work has been restricted to one intent per query and one slot label per token, and thus cannot model complex compositional requests. Alternative semantic parsing systems have represented queries as logical forms, but these are challenging to annotate and parse. We propose a hierarchical annotation scheme for semantic parsing that allows the representation of compositional queries, and can be efficiently and accurately parsed by standard constituency parsing models. We release a dataset of 44k annotated queries (fb.me/semanticparsingdialog), and show that parsing models outperform sequence-to-sequence approaches on this dataset.},
	number = {{arXiv}:1810.07942},
	publisher = {{arXiv}},
	author = {Gupta, Sonal and Shah, Rushin and Mohit, Mrinal and Kumar, Anuj and Lewis, Mike},
	urldate = {2024-05-01},
	date = {2018-10-18},
	eprinttype = {arxiv},
	eprint = {1810.07942 [cs]},
}

@misc{hanMedAlpacaOpenSourceCollection2023,
	title = {{MedAlpaca} -- An Open-Source Collection of Medical Conversational {AI} Models and Training Data},
	url = {http://arxiv.org/abs/2304.08247},
	abstract = {As large language models ({LLMs}) like {OpenAI}'s {GPT} series continue to make strides, we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine, these {LLMs} hold considerable promise for improving medical workflows, diagnostics, patient care, and education. Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work, we present an innovative dataset consisting of over 160,000 entries, specifically crafted to fine-tune {LLMs} for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained {LLMs}, and subsequently, we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.},
	number = {{arXiv}:2304.08247},
	publisher = {{arXiv}},
	author = {Han, Tianyu and Adams, Lisa C. and Papaioannou, Jens-Michalis and Grundmann, Paul and Oberhauser, Tom and Löser, Alexander and Truhn, Daniel and Bressem, Keno K.},
	urldate = {2024-05-01},
	date = {2023-10-04},
	eprinttype = {arxiv},
	eprint = {2304.08247 [cs]},
}

@misc{heDecouplingStrategyGeneration2018,
	title = {Decoupling Strategy and Generation in Negotiation Dialogues},
	url = {http://arxiv.org/abs/1808.09637},
	abstract = {We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing {\textbackslash}\$50) and the execution of that strategy (e.g., generating "The bike is brand new. Selling for just {\textbackslash}\$50."). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse di- alogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed {DEALORNODEAL} game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.},
	number = {{arXiv}:1808.09637},
	publisher = {{arXiv}},
	author = {He, He and Chen, Derek and Balakrishnan, Anusha and Liang, Percy},
	urldate = {2024-05-01},
	date = {2018-08-29},
	eprinttype = {arxiv},
	eprint = {1808.09637 [cs]},
}

@misc{kimProsocialDialogProsocialBackbone2022,
	title = {{ProsocialDialog}: A Prosocial Backbone for Conversational Agents},
	url = {http://arxiv.org/abs/2205.12688},
	shorttitle = {{ProsocialDialog}},
	abstract = {Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce {ProsocialDialog}, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, {ProsocialDialog} contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, {RoTs}). Created via a human-{AI} collaborative framework, {ProsocialDialog} consists of 58K dialogues, with 331K utterances, 160K unique {RoTs}, and 497K dialogue safety labels accompanied by free-form rationales. With this dataset, we introduce a dialogue safety detection module, Canary, capable of generating {RoTs} given conversational context, and a socially-informed dialogue agent, Prost. Empirical results show that Prost generates more socially acceptable dialogues compared to other state-of-the-art language and dialogue models in both in-domain and out-of-domain settings. Additionally, Canary effectively guides conversational agents and off-the-shelf language models to generate significantly more prosocial responses. Our work highlights the promise and importance of creating and steering conversational {AI} to be socially responsible.},
	number = {{arXiv}:2205.12688},
	publisher = {{arXiv}},
	author = {Kim, Hyunwoo and Yu, Youngjae and Jiang, Liwei and Lu, Ximing and Khashabi, Daniel and Kim, Gunhee and Choi, Yejin and Sap, Maarten},
	urldate = {2024-05-01},
	date = {2022-10-25},
	eprinttype = {arxiv},
	eprint = {2205.12688 [cs]},
}

@misc{kimPrometheusInducingFinegrained2024,
	title = {Prometheus: Inducing Fine-grained Evaluation Capability in Language Models},
	url = {http://arxiv.org/abs/2310.08491},
	shorttitle = {Prometheus},
	abstract = {Recently, using a powerful proprietary Large Language Model ({LLM}) (e.g., {GPT}-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary {LLMs} as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source {LLM} that is on par with {GPT}-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by {GPT}-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator {LLM} that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with {GPT}-4 (0.882), and greatly outperforms {ChatGPT} (0.392). Furthermore, measuring correlation with {GPT}-4 with 1222 customized score rubrics across four benchmarks ({MT} Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator {LLM}. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks ({HHH} Alignment \& {MT} Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at https://kaistai.github.io/prometheus/.},
	number = {{arXiv}:2310.08491},
	publisher = {{arXiv}},
	author = {Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and Shin, Seongjin and Kim, Sungdong and Thorne, James and Seo, Minjoon},
	urldate = {2024-05-01},
	date = {2024-03-09},
	eprinttype = {arxiv},
	eprint = {2310.08491 [cs]},
}

@misc{liEndtoEndTrainableNonCollaborative2019,
	title = {End-to-End Trainable Non-Collaborative Dialog System},
	url = {http://arxiv.org/abs/1911.10742},
	abstract = {End-to-end task-oriented dialog models have achieved promising performance on collaborative tasks where users willingly coordinate with the system to complete a given task. While in non-collaborative settings, for example, negotiation and persuasion, users and systems do not share a common goal. As a result, compared to collaborate tasks, people use social content to build rapport and trust in these non-collaborative settings in order to advance their goals. To handle social content, we introduce a hierarchical intent annotation scheme, which can be generalized to different non-collaborative dialog tasks. Building upon {TransferTransfo} (Wolf et al. 2019), we propose an end-to-end neural network model to generate diverse coherent responses. Our model utilizes intent and semantic slots as the intermediate sentence representation to guide the generation process. In addition, we design a filter to select appropriate responses based on whether these intermediate representations fit the designed task and conversation constraints. Our non-collaborative dialog model guides users to complete the task while simultaneously keeps them engaged. We test our approach on our newly proposed {ANTISCAM} dataset and an existing {PERSUASIONFORGOOD} dataset. Both automatic and human evaluations suggest that our model outperforms multiple baselines in these two non-collaborative tasks.},
	number = {{arXiv}:1911.10742},
	publisher = {{arXiv}},
	author = {Li, Yu and Qian, Kun and Shi, Weiyan and Yu, Zhou},
	urldate = {2024-05-01},
	date = {2019-11-25},
	eprinttype = {arxiv},
	eprint = {1911.10742 [cs]},
}

@misc{linRiddleSenseReasoningRiddle2021,
	title = {{RiddleSense}: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge},
	url = {http://arxiv.org/abs/2101.00376},
	shorttitle = {{RiddleSense}},
	abstract = {Question: I have five fingers but I am not alive. What am I? Answer: a glove. Answering such a riddle-style question is a challenging cognitive process, in that it requires complex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning skills, which are all important abilities for advanced natural language understanding ({NLU}). However, there are currently no dedicated datasets aiming to test these abilities. Herein, we present {RiddleSense}, a new multiple-choice question answering task, which comes with the first large dataset (5.7k examples) for answering riddle-style commonsense questions. We systematically evaluate a wide range of models over the challenge, and point out that there is a large gap between the best-supervised model and human performance -- suggesting intriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards building advanced {NLU} systems.},
	number = {{arXiv}:2101.00376},
	publisher = {{arXiv}},
	author = {Lin, Bill Yuchen and Wu, Ziyi and Yang, Yichi and Lee, Dong-Ho and Ren, Xiang},
	urldate = {2024-05-01},
	date = {2021-07-04},
	eprinttype = {arxiv},
	eprint = {2101.00376 [cs]},
}

@misc{liuWhatMakesGood2024,
	title = {What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
	url = {http://arxiv.org/abs/2312.15685},
	shorttitle = {What Makes Good Data for Alignment?},
	abstract = {Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from {LLaMA} and Mistral models using data samples automatically selected with our proposed approach. Empirically, deita performs better or on par with the state-of-the-art open-source alignment models with only 6K {SFT} training data samples -- over 10x less than the data used in the baselines. When further trained with direct preference optimization ({DPO}), deita-Mistral-7B + {DPO} trained with 6K {SFT} and 10K {DPO} samples achieve 7.55 {MT}-Bench and 90.06\% {AlpacaEval} scores. We anticipate this work to provide tools on automatic data selection, facilitating data-efficient alignment. We release our models as well as the selected datasets for future researches to effectively align models more efficiently.},
	number = {{arXiv}:2312.15685},
	publisher = {{arXiv}},
	author = {Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian},
	urldate = {2024-05-01},
	date = {2024-04-15},
	eprinttype = {arxiv},
	eprint = {2312.15685 [cs]},
}

@misc{luLearnExplainMultimodal2022,
	title = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
	url = {http://arxiv.org/abs/2209.09513},
	shorttitle = {Learn to Explain},
	abstract = {When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought ({CoT}). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an {AI} system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering ({ScienceQA}), a new benchmark that consists of {\textasciitilde}21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought ({CoT}) to mimic the multi-hop reasoning process when answering {ScienceQA} questions. {ScienceQA} demonstrates the utility of {CoT} in language models, as {CoT} improves the question answering performance by 1.20\% in few-shot {GPT}-3 and 3.99\% in fine-tuned {UnifiedQA}. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of {GPT}-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
	number = {{arXiv}:2209.09513},
	publisher = {{arXiv}},
	author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
	urldate = {2024-05-01},
	date = {2022-10-17},
	eprinttype = {arxiv},
	eprint = {2209.09513 [cs]},
}

@misc{macinaMathDialDialogueTutoring2023,
	title = {{MathDial}: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems},
	url = {http://arxiv.org/abs/2305.14536},
	shorttitle = {{MathDial}},
	abstract = {While automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. Collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this, we propose a framework to generate such dialogues by pairing human teachers with a Large Language Model ({LLM}) prompted to represent common student errors. We describe how we use this framework to collect {MathDial}, a dataset of 3k one-to-one teacher-student tutoring dialogues grounded in multi-step math reasoning problems. While models like {GPT}-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early. To overcome this, we let teachers provide learning opportunities to students by guiding them using various scaffolding questions according to a taxonomy of teacher moves. We demonstrate {MathDial} and its extensive annotations can be used to finetune models to be more effective tutors (and not just solvers). We confirm this by automatic and human evaluation, notably in an interactive setting that measures the trade-off between student solving success and telling solutions. The dataset is released publicly.},
	number = {{arXiv}:2305.14536},
	publisher = {{arXiv}},
	author = {Macina, Jakub and Daheim, Nico and Chowdhury, Sankalan Pal and Sinha, Tanmay and Kapur, Manu and Gurevych, Iryna and Sachan, Mrinmaya},
	urldate = {2024-05-01},
	date = {2023-10-23},
	eprinttype = {arxiv},
	eprint = {2305.14536 [cs]},
}

@misc{malaviyaExpertQAExpertCuratedQuestions2024,
	title = {{ExpertQA}: Expert-Curated Questions and Attributed Answers},
	url = {http://arxiv.org/abs/2309.07852},
	shorttitle = {{ExpertQA}},
	abstract = {As language models are adopted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying attribution and factuality has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we conduct human evaluation of responses from a few representative systems along various axes of attribution and factuality, by bringing domain experts in the loop. Specifically, we collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. In addition, we ask experts to improve upon responses from language models. The output of our analysis is {ExpertQA}, a high-quality long-form {QA} dataset with 2177 questions spanning 32 fields, along with verified answers and attributions for claims in the answers.},
	number = {{arXiv}:2309.07852},
	publisher = {{arXiv}},
	author = {Malaviya, Chaitanya and Lee, Subin and Chen, Sihao and Sieber, Elizabeth and Yatskar, Mark and Roth, Dan},
	urldate = {2024-05-01},
	date = {2024-04-01},
	eprinttype = {arxiv},
	eprint = {2309.07852 [cs]},
}

@misc{mitraOrcaMathUnlockingPotential2024,
	title = {Orca-Math: Unlocking the potential of {SLMs} in Grade School Math},
	url = {http://arxiv.org/abs/2402.14830},
	shorttitle = {Orca-Math},
	abstract = {Mathematical word problem-solving has long been recognized as a complex task for small language models ({SLMs}). A recent study hypothesized that the smallest model size, needed to achieve over 80\% accuracy on the {GSM}8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train {SLMs} to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the {SLM}. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-{GSM} uses top-48 to boost the performance from 68.2 to 81.5). In this work, we present Orca-Math, a 7-billion-parameter {SLM} based on the Mistral-7B, which achieves 86.81\% on {GSM}8k without the need for multiple model calls or the use of verifiers, code execution or any other external tools. Our approach has the following key elements: (1) A high quality synthetic dataset of 200K math problems created using a multi-agent setup where agents collaborate to create the data, (2) An iterative learning techniques that enables the {SLM} to practice solving problems, receive feedback on its solutions and learn from preference pairs incorporating the {SLM} solutions and the feedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves 81.50\% on {GSM}8k pass@1 metric. With iterative preference learning, Orca-Math achieves 86.81\% pass@1. Orca-Math surpasses the performance of significantly larger models such as {LLAMA}-2-70B, {WizardMath}-70B, Gemini-Pro, {ChatGPT}-3.5. It also significantly outperforms other smaller models while using much smaller data (hundreds of thousands vs. millions of problems).},
	number = {{arXiv}:2402.14830},
	publisher = {{arXiv}},
	author = {Mitra, Arindam and Khanpour, Hamed and Rosset, Corby and Awadallah, Ahmed},
	urldate = {2024-05-01},
	date = {2024-02-16},
	eprinttype = {arxiv},
	eprint = {2402.14830 [cs]},
}

@misc{mosigSTARSchemaGuidedDialog2020,
	title = {{STAR}: A Schema-Guided Dialog Dataset for Transfer Learning},
	url = {http://arxiv.org/abs/2010.11853},
	shorttitle = {{STAR}},
	abstract = {We present {STAR}, a schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog. Furthermore, we propose a scalable crowd-sourcing paradigm to collect arbitrarily large datasets of the same quality as {STAR}. Moreover, we introduce novel schema-guided dialog models that use an explicit description of the task(s) to generalize from known to unknown tasks. We demonstrate the effectiveness of these models, particularly for zero-shot generalization across tasks and domains.},
	number = {{arXiv}:2010.11853},
	publisher = {{arXiv}},
	author = {Mosig, Johannes E. M. and Mehri, Shikib and Kober, Thomas},
	urldate = {2024-05-01},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.11853 [cs]},
}

@misc{mrksicFullyStatisticalNeural2018,
	title = {Fully Statistical Neural Belief Tracking},
	url = {http://arxiv.org/abs/1805.11350},
	abstract = {This paper proposes an improvement to the existing data-driven Neural Belief Tracking ({NBT}) framework for Dialogue State Tracking ({DST}). The existing {NBT} model uses a hand-crafted belief state update mechanism which involves an expensive manual retuning step whenever the model is deployed to a new dialogue domain. We show that this update mechanism can be learned jointly with the semantic decoding and context modelling parts of the {NBT} model, eliminating the last rule-based module from this {DST} framework. We propose two different statistical update mechanisms and show that dialogue dynamics can be modelled with a very small number of additional model parameters. In our {DST} evaluation over three languages, we show that this model achieves competitive performance and provides a robust framework for building resource-light {DST} models.},
	number = {{arXiv}:1805.11350},
	publisher = {{arXiv}},
	author = {Mrkšić, Nikola and Vulić, Ivan},
	urldate = {2024-05-01},
	date = {2018-05-29},
	eprinttype = {arxiv},
	eprint = {1805.11350 [cs]},
}

@misc{nguyenSeaLLMsLargeLanguage2023,
	title = {{SeaLLMs} -- Large Language Models for Southeast Asia},
	url = {http://arxiv.org/abs/2312.00738},
	abstract = {Despite the remarkable achievements of large language models ({LLMs}) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce {SeaLLMs}, an innovative series of language models that specifically focuses on Southeast Asian ({SEA}) languages. {SeaLLMs} are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that {SeaLLM}-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform {ChatGPT}-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.},
	number = {{arXiv}:2312.00738},
	publisher = {{arXiv}},
	author = {Nguyen, Xuan-Phi and Zhang, Wenxuan and Li, Xin and Aljunied, Mahani and Tan, Qingyu and Cheng, Liying and Chen, Guanzheng and Deng, Yue and Yang, Sen and Liu, Chaoqun and Zhang, Hang and Bing, Lidong},
	urldate = {2024-05-01},
	date = {2023-12-01},
	eprinttype = {arxiv},
	eprint = {2312.00738 [cs]},
}

@misc{reddyCoQAConversationalQuestion2019,
	title = {{CoQA}: A Conversational Question Answering Challenge},
	url = {http://arxiv.org/abs/1808.07042},
	shorttitle = {{CoQA}},
	abstract = {Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce {CoQA}, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze {CoQA} in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on {CoQA}. The best system obtains an F1 score of 65.4\%, which is 23.4 points behind human performance (88.8\%), indicating there is ample room for improvement. We launch {CoQA} as a challenge to the community at http://stanfordnlp.github.io/coqa/},
	number = {{arXiv}:1808.07042},
	publisher = {{arXiv}},
	author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
	urldate = {2024-05-01},
	date = {2019-03-29},
	eprinttype = {arxiv},
	eprint = {1808.07042 [cs]},
}

@misc{rohanianExploringEffectivenessInstruction2023,
	title = {Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing},
	url = {http://arxiv.org/abs/2401.00579},
	abstract = {Large Language Models ({LLMs}), particularly those similar to {ChatGPT}, have significantly influenced the field of Natural Language Processing ({NLP}). While these models excel in general language tasks, their performance in domain-specific downstream tasks such as biomedical and clinical Named Entity Recognition ({NER}), Relation Extraction ({RE}), and Medical Natural Language Inference ({NLI}) is still evolving. In this context, our study investigates the potential of instruction tuning for biomedical language processing, applying this technique to two general {LLMs} of substantial scale. We present a comprehensive, instruction-based model trained on a dataset that consists of approximately \$200,000\$ instruction-focused samples. This dataset represents a carefully curated compilation of existing data, meticulously adapted and reformatted to align with the specific requirements of our instruction-based tasks. This initiative represents an important step in utilising such models to achieve results on par with specialised encoder-only models like {BioBERT} and {BioClinicalBERT} for various classical biomedical {NLP} tasks. Our work includes an analysis of the dataset's composition and its impact on model performance, providing insights into the intricacies of instruction tuning. By sharing our codes, models, and the distinctively assembled instruction-based dataset, we seek to encourage ongoing research and development in this area.},
	number = {{arXiv}:2401.00579},
	publisher = {{arXiv}},
	author = {Rohanian, Omid and Nouriborji, Mohammadmahdi and Clifton, David A.},
	urldate = {2024-05-01},
	date = {2023-12-31},
	eprinttype = {arxiv},
	eprint = {2401.00579 [cs]},
}

@misc{sawadaARBAdvancedReasoning2023,
	title = {{ARB}: Advanced Reasoning Benchmark for Large Language Models},
	url = {http://arxiv.org/abs/2307.13692},
	shorttitle = {{ARB}},
	abstract = {Large Language Models ({LLMs}) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as {LLMs} get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce {ARB}, a novel benchmark composed of advanced reasoning problems in multiple fields. {ARB} presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of {ARB}, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as {GPT}-4 and Claude on {ARB} and demonstrate that current models score well below 50\% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing {GPT}-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of {ARB}, finding promising agreement between annotators and {GPT}-4 rubric evaluation scores.},
	number = {{arXiv}:2307.13692},
	publisher = {{arXiv}},
	author = {Sawada, Tomohiro and Paleka, Daniel and Havrilla, Alexander and Tadepalli, Pranav and Vidas, Paula and Kranias, Alexander and Nay, John J. and Gupta, Kshitij and Komatsuzaki, Aran},
	urldate = {2024-05-01},
	date = {2023-07-27},
	eprinttype = {arxiv},
	eprint = {2307.13692 [cs]},
}

@misc{shahBuildingConversationalAgent2018,
	title = {Building a Conversational Agent Overnight with Dialogue Self-Play},
	url = {http://arxiv.org/abs/1801.04871},
	abstract = {We propose Machines Talking To Machines (M2M), a framework combining automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents for goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with just a task schema and an {API} client from the dialogue system developer, but it is also customizable to cater to task-specific interactions. Compared to the Wizard-of-Oz approach for data collection, M2M achieves greater diversity and coverage of salient dialogue flows while maintaining the naturalness of individual utterances. In the first phase, a simulated user bot and a domain-agnostic system bot converse to exhaustively generate dialogue "outlines", i.e. sequences of template utterances and their semantic parses. In the second phase, crowd workers provide contextual rewrites of the dialogues to make the utterances more natural while preserving their meaning. The entire process can finish within a few hours. We propose a new corpus of 3,000 dialogues spanning 2 domains collected with M2M, and present comparisons with popular dialogue datasets on the quality and diversity of the surface forms and dialogue flows.},
	number = {{arXiv}:1801.04871},
	publisher = {{arXiv}},
	author = {Shah, Pararth and Hakkani-Tür, Dilek and Tür, Gokhan and Rastogi, Abhinav and Bapna, Ankur and Nayak, Neha and Heck, Larry},
	urldate = {2024-05-01},
	date = {2018-01-15},
	eprinttype = {arxiv},
	eprint = {1801.04871 [cs]},
}

@misc{shalyminovFewShotDialogueGeneration2019,
	title = {Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach},
	url = {http://arxiv.org/abs/1908.05854},
	shorttitle = {Few-Shot Dialogue Generation Without Annotated Data},
	abstract = {Learning with minimal data is one of the key challenges in the development of practical, production-ready goal-oriented dialogue systems. In a real-world enterprise setting where dialogue systems are developed rapidly and are expected to work robustly for an ever-growing variety of domains, products, and scenarios, efficient learning from a limited number of examples becomes indispensable. In this paper, we introduce a technique to achieve state-of-the-art dialogue generation performance in a few-shot setup, without using any annotated data. We do this by leveraging background knowledge from a larger, more highly represented dialogue source --- namely, the {MetaLWOz} dataset. We evaluate our model on the Stanford Multi-Domain Dialogue Dataset, consisting of human-human goal-oriented dialogues in in-car navigation, appointment scheduling, and weather information domains. We show that our few-shot approach achieves state-of-the art results on that dataset by consistently outperforming the previous best model in terms of {BLEU} and Entity F1 scores, while being more data-efficient by not requiring any data annotation.},
	number = {{arXiv}:1908.05854},
	publisher = {{arXiv}},
	author = {Shalyminov, Igor and Lee, Sungjin and Eshghi, Arash and Lemon, Oliver},
	urldate = {2024-05-01},
	date = {2019-08-16},
	eprinttype = {arxiv},
	eprint = {1908.05854 [cs]},
}

@misc{shridharALFWorldAligningText2021,
	title = {{ALFWorld}: Aligning Text and Embodied Environments for Interactive Learning},
	url = {http://arxiv.org/abs/2010.03768},
	shorttitle = {{ALFWorld}},
	abstract = {Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing {ALFWorld}, a simulator that enables agents to learn abstract, text based policies in {TextWorld} (C{\textbackslash}{\textasciicircum}ot{\textbackslash}'e et al., 2018) and then execute goals from the {ALFRED} benchmark (Shridhar et al., 2020) in a rich visual environment. {ALFWorld} enables the creation of a new {BUTLER} agent whose abstract knowledge, learned in {TextWorld}, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. {BUTLER}'s simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).},
	number = {{arXiv}:2010.03768},
	publisher = {{arXiv}},
	author = {Shridhar, Mohit and Yuan, Xingdi and Côté, Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and Hausknecht, Matthew},
	urldate = {2024-05-01},
	date = {2021-03-14},
	eprinttype = {arxiv},
	eprint = {2010.03768 [cs]},
}

@misc{singhAyaDatasetOpenAccess2024,
	title = {Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning},
	url = {http://arxiv.org/abs/2402.06619},
	shorttitle = {Aya Dataset},
	abstract = {Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing ({NLP}) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model ({LLM}) to respond to instructions. Instruction fine-tuning ({IFT}) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as a valuable framework for future research collaborations that aim to bridge gaps in resources.},
	number = {{arXiv}:2402.06619},
	publisher = {{arXiv}},
	author = {Singh, Shivalika and Vargus, Freddie and Dsouza, Daniel and Karlsson, Börje F. and Mahendiran, Abinaya and Ko, Wei-Yin and Shandilya, Herumb and Patel, Jay and Mataciunas, Deividas and {OMahony}, Laura and Zhang, Mike and Hettiarachchi, Ramith and Wilson, Joseph and Machado, Marina and Moura, Luisa Souza and Krzemiński, Dominik and Fadaei, Hakimeh and Ergün, Irem and Okoh, Ifeoma and Alaagib, Aisha and Mudannayake, Oshan and Alyafeai, Zaid and Chien, Vu Minh and Ruder, Sebastian and Guthikonda, Surya and Alghamdi, Emad A. and Gehrmann, Sebastian and Muennighoff, Niklas and Bartolo, Max and Kreutzer, Julia and Üstün, Ahmet and Fadaee, Marzieh and Hooker, Sara},
	urldate = {2024-05-01},
	date = {2024-02-09},
	eprinttype = {arxiv},
	eprint = {2402.06619 [cs]},
}

@misc{talmorMultiModalQAComplexQuestion2021,
	title = {{MultiModalQA}: Complex Question Answering over Text, Tables and Images},
	url = {http://arxiv.org/abs/2104.06039},
	shorttitle = {{MultiModalQA}},
	abstract = {When answering complex questions, people can seamlessly combine information from visual, textual and tabular sources. While interest in models that reason over multiple pieces of evidence has surged in recent years, there has been relatively little work on question answering models that reason across multiple modalities. In this paper, we present {MultiModalQA}({MMQA}): a challenging question answering dataset that requires joint reasoning over text, tables and images. We create {MMQA} using a new framework for generating complex multi-modal questions at scale, harvesting tables from Wikipedia, and attaching images and text paragraphs using entities that appear in each table. We then define a formal language that allows us to take questions that can be answered from a single modality, and combine them to generate cross-modal questions. Last, crowdsourcing workers take these automatically-generated questions and rephrase them into more fluent language. We create 29,918 questions through this procedure, and empirically demonstrate the necessity of a multi-modal multi-hop approach to solve our task: our multi-hop model, {ImplicitDecomp}, achieves an average F1of 51.7 over cross-modal questions, substantially outperforming a strong baseline that achieves 38.2 F1, but still lags significantly behind human performance, which is at 90.1 F1},
	number = {{arXiv}:2104.06039},
	publisher = {{arXiv}},
	author = {Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan},
	urldate = {2024-05-01},
	date = {2021-04-13},
	eprinttype = {arxiv},
	eprint = {2104.06039 [cs]},
}

@misc{toshniwalOpenMathInstruct1MillionMath2024,
	title = {{OpenMathInstruct}-1: A 1.8 Million Math Instruction Tuning Dataset},
	url = {http://arxiv.org/abs/2402.10176},
	shorttitle = {{OpenMathInstruct}-1},
	abstract = {Recent work has shown the immense potential of synthetically generated datasets for training large language models ({LLMs}), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as {MetaMathQA} (Yu et al., 2024) and {MAmmoTH} (Yue et al., 2024) are constructed using outputs from closed-source {LLMs} with commercially restrictive licenses. A key reason limiting the use of open-source {LLMs} in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source {LLMs}, such as {GPT}-4, and the best open-source {LLMs}. Building on the recent progress in open-source {LLMs}, our proposed prompting novelty, and some brute-force scaling, we construct {OpenMathInstruct}-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for {GSM}8K and {MATH}, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, {OpenMath}-{CodeLlama}-70B, trained on a subset of {OpenMathInstruct}-1, achieves a score of 84.6\% on {GSM}8K and 50.7\% on {MATH}, which is competitive with the best gpt-distilled models. We release our code, models, and the {OpenMathInstruct}-1 dataset under a commercially permissive license.},
	number = {{arXiv}:2402.10176},
	publisher = {{arXiv}},
	author = {Toshniwal, Shubham and Moshkov, Ivan and Narenthiran, Sean and Gitman, Daria and Jia, Fei and Gitman, Igor},
	urldate = {2024-05-01},
	date = {2024-02-15},
	eprinttype = {arxiv},
	eprint = {2402.10176 [cs]},
}

@misc{wangSciBenchEvaluatingCollegeLevel2024,
	title = {{SciBench}: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models},
	url = {http://arxiv.org/abs/2307.10635},
	shorttitle = {{SciBench}},
	abstract = {Most of the existing Large Language Model ({LLM}) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite {SciBench} for {LLMs}. {SciBench} contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary {LLMs} with various prompting strategies. The results reveal that the current {LLMs} fall short of delivering satisfactory performance, with the best overall score of merely 43.22\%. Furthermore, through a detailed user study, we categorize the errors made by {LLMs} into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that {SciBench} will catalyze further developments in the reasoning abilities of {LLMs}, thereby ultimately contributing to scientific research and discovery.},
	number = {{arXiv}:2307.10635},
	publisher = {{arXiv}},
	author = {Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R. and Zhang, Shichang and Sun, Yizhou and Wang, Wei},
	urldate = {2024-05-01},
	date = {2024-02-08},
	eprinttype = {arxiv},
	eprint = {2307.10635 [cs]},
}

@misc{wangHelpSteerMultiattributeHelpfulness2023,
	title = {{HelpSteer}: Multi-attribute Helpfulness Dataset for {SteerLM}},
	url = {http://arxiv.org/abs/2311.09528},
	shorttitle = {{HelpSteer}},
	abstract = {Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect {HelpSteer}, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the {HelpSteer} dataset with {SteerLM} technique produces a model that scores 7.54 on {MT} Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. {GPT}4). We release this dataset with {CC}-{BY}-4.0 license at https://huggingface.co/datasets/nvidia/{HelpSteer}},
	number = {{arXiv}:2311.09528},
	publisher = {{arXiv}},
	author = {Wang, Zhilin and Dong, Yi and Zeng, Jiaqi and Adams, Virginia and Sreedhar, Makesh Narsimhan and Egert, Daniel and Delalleau, Olivier and Scowcroft, Jane Polak and Kant, Neel and Swope, Aidan and Kuchaiev, Oleksii},
	urldate = {2024-05-01},
	date = {2023-11-15},
	eprinttype = {arxiv},
	eprint = {2311.09528 [cs]},
}

@misc{wuPMCLLaMABuildingOpensource2023,
	title = {{PMC}-{LLaMA}: Towards Building Open-source Language Models for Medicine},
	url = {http://arxiv.org/abs/2304.14454},
	shorttitle = {{PMC}-{LLaMA}},
	abstract = {Recently, Large Language Models ({LLMs}) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as {PMC}-{LLaMA}. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering ({QA}), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight {PMCLLaMA}, which consists of only 13 billion parameters, exhibits superior performance, even surpassing {ChatGPT}. All models, codes, datasets can be found in https://github.com/chaoyi-wu/{PMC}-{LLaMA}.},
	number = {{arXiv}:2304.14454},
	publisher = {{arXiv}},
	author = {Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
	urldate = {2024-05-01},
	date = {2023-08-25},
	eprinttype = {arxiv},
	eprint = {2304.14454 [cs]},
}

@misc{xuKIWIDatasetKnowledgeIntensive2024,
	title = {{KIWI}: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions},
	url = {http://arxiv.org/abs/2403.03866},
	shorttitle = {{KIWI}},
	abstract = {Large language models ({LLMs}) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current {LLMs} on this task, we construct {KIWI}, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art {LLMs}. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that {KIWI} will be a valuable resource to measure progress and improve {LLMs}' instruction-following capabilities for knowledge intensive writing tasks.},
	number = {{arXiv}:2403.03866},
	publisher = {{arXiv}},
	author = {Xu, Fangyuan and Lo, Kyle and Soldaini, Luca and Kuehl, Bailey and Choi, Eunsol and Wadden, David},
	urldate = {2024-05-01},
	date = {2024-03-06},
	eprinttype = {arxiv},
	eprint = {2403.03866 [cs]},
}

@misc{yuMetaMathBootstrapYour2023,
	title = {{MetaMath}: Bootstrap Your Own Mathematical Questions for Large Language Models},
	url = {http://arxiv.org/abs/2309.12284},
	shorttitle = {{MetaMath}},
	abstract = {Large language models ({LLMs}) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source {LLMs} (e.g., {LLaMA}-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose {MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the {LLaMA}-2 models on {MetaMathQA}. Experimental results on two popular benchmarks (i.e., {GSM}8K and {MATH}) for mathematical reasoning demonstrate that {MetaMath} outperforms a suite of open-source {LLMs} by a significant margin. Our {MetaMath}-7B model achieves 66.4\% on {GSM}8K and 19.4\% on {MATH}, exceeding the state-of-the-art models of the same size by 11.5\% and 8.7\%. Particularly, {MetaMath}-70B achieves an accuracy of 82.3\% on {GSM}8K, slightly better than {GPT}-3.5-Turbo. We release all the {MetaMathQA} dataset, the {MetaMath} models with different model sizes and the training code for public use.},
	number = {{arXiv}:2309.12284},
	publisher = {{arXiv}},
	author = {Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T. and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
	urldate = {2024-05-01},
	date = {2023-10-09},
	eprinttype = {arxiv},
	eprint = {2309.12284 [cs]},
}

@misc{yuCoSQLConversationalTexttoSQL2019,
	title = {{CoSQL}: A Conversational Text-to-{SQL} Challenge Towards Cross-Domain Natural Language Interfaces to Databases},
	url = {http://arxiv.org/abs/1909.05378},
	shorttitle = {{CoSQL}},
	abstract = {We present {CoSQL}, a corpus for building cross-domain, general-purpose database ({DB}) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated {SQL} queries, obtained from a Wizard-of-Oz ({WOZ}) collection of 3k dialogues querying 200 complex {DBs} spanning 138 domains. Each dialogue simulates a real-world {DB} query scenario with a crowd worker as a user exploring the {DB} and a {SQL} expert retrieving answers with {SQL}, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by {SQL}, the expert describes the {SQL} and execution results to the user, hence maintaining a natural interaction flow. {CoSQL} introduces new challenges compared to existing task-oriented dialogue datasets:(1) the dialogue states are grounded in {SQL}, a domain-independent executable representation, instead of domain-specific slot-value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. {CoSQL} includes three tasks: {SQL}-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that {CoSQL} presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at https://yale-lily.github.io/cosql.},
	number = {{arXiv}:1909.05378},
	publisher = {{arXiv}},
	author = {Yu, Tao and Zhang, Rui and Er, He Yang and Li, Suyi and Xue, Eric and Pang, Bo and Lin, Xi Victoria and Tan, Yi Chern and Shi, Tianze and Li, Zihan and Jiang, Youxuan and Yasunaga, Michihiro and Shim, Sungrok and Chen, Tao and Fabbri, Alexander and Li, Zifan and Chen, Luyao and Zhang, Yuwen and Dixit, Shreya and Zhang, Vincent and Xiong, Caiming and Socher, Richard and Lasecki, Walter S. and Radev, Dragomir},
	urldate = {2024-05-01},
	date = {2019-09-11},
	eprinttype = {arxiv},
	eprint = {1909.05378 [cs]},
}

@misc{yuReClorReadingComprehension2020,
	title = {{ReClor}: A Reading Comprehension Dataset Requiring Logical Reasoning},
	url = {http://arxiv.org/abs/2002.04326},
	shorttitle = {{ReClor}},
	abstract = {Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning ({ReClor}) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on {ReClor}, we propose to identify biased data points and separate them into {EASY} set while the rest as {HARD} set. Empirical results show that state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on {EASY} set. However, they struggle on {HARD} set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models.},
	number = {{arXiv}:2002.04326},
	publisher = {{arXiv}},
	author = {Yu, Weihao and Jiang, Zihang and Dong, Yanfei and Feng, Jiashi},
	urldate = {2024-05-01},
	date = {2020-08-22},
	eprinttype = {arxiv},
	eprint = {2002.04326 [cs]},
}

@misc{yueMAmmoTHBuildingMath2023,
	title = {{MAmmoTH}: Building Math Generalist Models through Hybrid Instruction Tuning},
	url = {http://arxiv.org/abs/2309.05653},
	shorttitle = {{MAmmoTH}},
	abstract = {We introduce {MAmmoTH}, a series of open-source large language models ({LLMs}) specifically tailored for general math problem-solving. The {MAmmoTH} models are trained on {MathInstruct}, our meticulously curated instruction tuning dataset. {MathInstruct} is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought ({CoT}) and program-of-thought ({PoT}) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of {CoT} and {PoT} not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the {MAmmoTH} series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16\% and 32\%. Remarkably, our {MAmmoTH}-7B model reaches 33\% on {MATH} (a competition-level dataset), which exceeds the best open-source 7B model ({WizardMath}) by 23\%, and the {MAmmoTH}-34B model achieves 44\% accuracy on {MATH}, even surpassing {GPT}-4's {CoT} result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.},
	number = {{arXiv}:2309.05653},
	publisher = {{arXiv}},
	author = {Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
	urldate = {2024-05-01},
	date = {2023-10-02},
	eprinttype = {arxiv},
	eprint = {2309.05653 [cs]},
}

@misc{zangMultiWOZDialogueDataset2020,
	title = {{MultiWOZ} 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines},
	url = {http://arxiv.org/abs/2007.12720},
	shorttitle = {{MultiWOZ} 2.2},
	abstract = {{MultiWOZ} is a well-known task-oriented dialogue dataset containing over 10,000 annotated dialogues spanning 8 domains. It is extensively used as a benchmark for dialogue state tracking. However, recent works have reported presence of substantial noise in the dialogue state annotations. {MultiWOZ} 2.1 identified and fixed many of these erroneous annotations and user utterances, resulting in an improved version of this dataset. This work introduces {MultiWOZ} 2.2, which is a yet another improved version of this dataset. Firstly, we identify and fix dialogue state annotation errors across 17.3\% of the utterances on top of {MultiWOZ} 2.1. Secondly, we redefine the ontology by disallowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking). In addition, we introduce slot span annotations for these slots to standardize them across recent models, which previously used custom string matching heuristics to generate them. We also benchmark a few state of the art dialogue state tracking models on the corrected dataset to facilitate comparison for future work. In the end, we discuss best practices for dialogue data collection that can help avoid annotation errors.},
	number = {{arXiv}:2007.12720},
	publisher = {{arXiv}},
	author = {Zang, Xiaoxue and Rastogi, Abhinav and Sunkara, Srinivas and Gupta, Raghav and Zhang, Jianguo and Chen, Jindong},
	urldate = {2024-05-01},
	date = {2020-07-10},
	eprinttype = {arxiv},
	eprint = {2007.12720 [cs]},
}

@misc{zhangArePretrainedTransformers2022,
	title = {Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection},
	url = {http://arxiv.org/abs/2106.04564},
	shorttitle = {Are Pretrained Transformers Robust in Intent Classification?},
	abstract = {Pre-trained Transformer-based models were reported to be robust in intent classification. In this work, we first point out the importance of in-domain out-of-scope detection in few-shot intent recognition tasks and then illustrate the vulnerability of pre-trained Transformer-based models against samples that are in-domain but out-of-scope ({ID}-{OOS}). We construct two new datasets, and empirically show that pre-trained models do not perform well on both {ID}-{OOS} examples and general out-of-scope examples, especially on fine-grained few-shot intent detection tasks. To figure out how the models mistakenly classify {ID}-{OOS} intents as in-scope intents, we further conduct analysis on confidence scores and the overlapping keywords, as well as point out several prospective directions for future work. Resources are available on https://github.com/jianguoz/Few-Shot-Intent-Detection.},
	number = {{arXiv}:2106.04564},
	publisher = {{arXiv}},
	author = {Zhang, Jianguo and Hashimoto, Kazuma and Wan, Yao and Liu, Zhiwei and Liu, Ye and Xiong, Caiming and Yu, Philip S.},
	urldate = {2024-05-01},
	date = {2022-04-07},
	eprinttype = {arxiv},
	eprint = {2106.04564 [cs]},
}

@misc{zhangAlpaCareInstructiontunedLarge2024,
	title = {{AlpaCare}:Instruction-tuned Large Language Models for Medical Application},
	url = {http://arxiv.org/abs/2310.14558},
	shorttitle = {{AlpaCare}},
	abstract = {Instruction-finetuning ({IFT}) has become crucial in aligning Large Language Models ({LLMs}) with diverse human needs and has shown great potential in medical applications. However, previous studies mainly fine-tune {LLMs} on biomedical datasets with limited diversity, which often rely on benchmarks or narrow task scopes, and hence significantly limit the effectiveness on their medical instruction-following ability and generalizability. To bridge this gap, we propose creating a diverse, machine-generated medical {IFT} dataset, {MedInstruct}-52k, using {GPT}-4 and {ChatGPT} with a high-quality expert-curated seed set. We then fine-tune {LLaMA}-series models on the dataset to develop {AlpaCare}. Despite using a smaller domain-specific dataset than previous medical {LLMs}, {AlpaCare} not only demonstrates superior performance on medical applications, with up to 38.1\% absolute gain over best baselines in medical free-form instruction evaluations, but also achieves 6.7\% absolute gains averaged over multiple general domain benchmarks. Human evaluation further shows that {AlpaCare} consistently outperforms best baselines in terms of both correctness and helpfulness. We offer public access to our data, model, and codebase in https://github.com/{XZhang}97666/{AlpaCare}.},
	number = {{arXiv}:2310.14558},
	publisher = {{arXiv}},
	author = {Zhang, Xinlu and Tian, Chenxin and Yang, Xianjun and Chen, Lichang and Li, Zekun and Petzold, Linda Ruth},
	urldate = {2024-05-01},
	date = {2024-04-03},
	eprinttype = {arxiv},
	eprint = {2310.14558 [cs]},
}

@misc{zhongQMSumNewBenchmark2021,
	title = {{QMSum}: A New Benchmark for Query-based Multi-domain Meeting Summarization},
	url = {http://arxiv.org/abs/2104.05938},
	shorttitle = {{QMSum}},
	abstract = {Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce {QMSum}, a new benchmark for this task. {QMSum} consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that {QMSum} presents significant challenges in long meeting summarization for future research. Dataset is available at {\textbackslash}url\{https://github.com/Yale-{LILY}/{QMSum}\}.},
	number = {{arXiv}:2104.05938},
	publisher = {{arXiv}},
	author = {Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and Radev, Dragomir},
	urldate = {2024-05-01},
	date = {2021-04-13},
	eprinttype = {arxiv},
	eprint = {2104.05938 [cs]},
}

@misc{zhongSeq2SQLGeneratingStructured2017,
	title = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
	url = {http://arxiv.org/abs/1709.00103},
	shorttitle = {Seq2SQL},
	abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as {SQL}. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding {SQL} queries. Our model leverages the structure of {SQL} queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish {WikiSQL}, a dataset of 80654 hand-annotated examples of questions and {SQL} queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to {WikiSQL}, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
	number = {{arXiv}:1709.00103},
	publisher = {{arXiv}},
	author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
	urldate = {2024-05-01},
	date = {2017-11-09},
	eprinttype = {arxiv},
	eprint = {1709.00103 [cs]},
}

@misc{zhuMediaSumLargescaleMedia2021,
	title = {{MediaSum}: A Large-scale Media Interview Dataset for Dialogue Summarization},
	url = {http://arxiv.org/abs/2103.06410},
	shorttitle = {{MediaSum}},
	abstract = {{MediaSum}, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from {NPR} and {CNN} and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that {MediaSum} can be used in transfer learning to improve a model's performance on other dialogue summarization tasks.},
	number = {{arXiv}:2103.06410},
	publisher = {{arXiv}},
	author = {Zhu, Chenguang and Liu, Yang and Mei, Jie and Zeng, Michael},
	urldate = {2024-05-01},
	date = {2021-03-11},
	eprinttype = {arxiv},
	eprint = {2103.06410 [cs]},
}

@misc{alyafeaiCIDARCulturallyRelevant2024,
	title = {{CIDAR}: Culturally Relevant Instruction Dataset For Arabic},
	url = {http://arxiv.org/abs/2402.03177},
	shorttitle = {{CIDAR}},
	abstract = {Instruction tuning has emerged as a prominent methodology for teaching Large Language Models ({LLMs}) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated {LLMs}, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing {CIDAR}: https://hf.co/datasets/arbml/{CIDAR}, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. {CIDAR} contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of {CIDAR} via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that {CIDAR} can help enrich research efforts in aligning {LLMs} with the Arabic culture. All the code is available at https://github.com/{ARBML}/{CIDAR}.},
	number = {{arXiv}:2402.03177},
	publisher = {{arXiv}},
	author = {Alyafeai, Zaid and Almubarak, Khalid and Ashraf, Ahmed and Alnuhait, Deema and Alshahrani, Saied and Abdulrahman, Gubran A. Q. and Ahmed, Gamil and Gawah, Qais and Saleh, Zead and Ghaleb, Mustafa and Ali, Yousef and Al-Shaibani, Maged S.},
	urldate = {2024-05-01},
	date = {2024-02-05},
	eprinttype = {arxiv},
	eprint = {2402.03177 [cs]},
}

@misc{baiCOIGCQIAQualityAll2024,
	title = {{COIG}-{CQIA}: Quality is All You Need for Chinese Instruction Fine-tuning},
	url = {http://arxiv.org/abs/2403.18058},
	shorttitle = {{COIG}-{CQIA}},
	abstract = {Recently, there have been significant advancements in large language models ({LLMs}), particularly focused on the English language. These advancements have enabled these {LLMs} to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric {LLMs} or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce {COIG}-{CQIA}, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q\&A communities, Wikis, examinations, and existing {NLP} datasets. This corpus was rigorously filtered and carefully processed to form the {COIG}-{CQIA} dataset. Furthermore, we train models of various scales on different subsets of {CQIA}, following in-depth evaluation and analyses. The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets. We also find that models trained on {CQIA}-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks. Data are available at https://huggingface.co/datasets/m-a-p/{COIG}-{CQIA}},
	number = {{arXiv}:2403.18058},
	publisher = {{arXiv}},
	author = {Bai, Yuelin and Du, Xinrun and Liang, Yiming and Jin, Yonggang and Liu, Ziqiang and Zhou, Junting and Zheng, Tianyu and Zhang, Xincheng and Ma, Nuo and Wang, Zekun and Yuan, Ruibin and Wu, Haihong and Lin, Hongquan and Huang, Wenhao and Zhang, Jiajun and Chen, Wenhu and Lin, Chenghua and Fu, Jie and Yang, Min and Ni, Shiwen and Zhang, Ge},
	urldate = {2024-05-01},
	date = {2024-03-26},
	eprinttype = {arxiv},
	eprint = {2403.18058 [cs]},
	note = {version: 1},
}

@misc{liBactrianXMultilingualReplicable2023,
	title = {Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation},
	url = {http://arxiv.org/abs/2305.15011},
	shorttitle = {Bactrian-X},
	abstract = {Instruction tuning has shown great promise in improving the performance of large language models. However, research on multilingual instruction tuning has been limited due to the scarcity of high-quality instruction-response datasets across different languages. To bridge this gap, we present Bactrian-X, a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. Leveraging this dataset, we train a set of adapters using low-rank adaptation ({LoRA}), which are lightweight components that seamlessly integrate with large language models. These adapters have a substantially lower parameter count than the base model, making them easily replaceable and usable as plug-ins for different languages or language groups. Extensive experiments in various multilingual evaluation settings demonstrate that models derived from {LoRA}-based training over Bactrian-X outperform both the vanilla models and existing instruction-tuned models. The code and models are publicly available at https://github.com/mbzuai-nlp/bactrian-x},
	number = {{arXiv}:2305.15011},
	publisher = {{arXiv}},
	author = {Li, Haonan and Koto, Fajri and Wu, Minghao and Aji, Alham Fikri and Baldwin, Timothy},
	urldate = {2024-05-01},
	date = {2023-10-10},
	eprinttype = {arxiv},
	eprint = {2305.15011 [cs]},
}

@misc{liChatDoctorMedicalChat2023,
	title = {{ChatDoctor}: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-{AI} ({LLaMA}) Using Medical Domain Knowledge},
	url = {http://arxiv.org/abs/2303.14070},
	shorttitle = {{ChatDoctor}},
	abstract = {The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models ({LLMs}) such as {ChatGPT}, by creating a specialized language model with enhanced accuracy in medical advice. We achieved this by adapting and refining the large language model meta-{AI} ({LLaMA}) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Our proposed {ChatDoctor}, represents a significant advancement in medical {LLMs}, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.},
	number = {{arXiv}:2303.14070},
	publisher = {{arXiv}},
	author = {Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
	urldate = {2024-05-01},
	date = {2023-06-24},
	eprinttype = {arxiv},
	eprint = {2303.14070 [cs]},
}

@misc{sunConiferImprovingComplex2024,
	title = {Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models},
	url = {http://arxiv.org/abs/2404.02823},
	shorttitle = {Conifer},
	abstract = {The ability of large language models ({LLMs}) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that {LLMs} struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance {LLMs} to follow multi-level instructions with complex constraints. Utilizing {GPT}-4, we curate the dataset by a series of {LLM}-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics. All the code and Conifer dataset are available at https://www.github.com/{ConiferLM}/Conifer.},
	number = {{arXiv}:2404.02823},
	publisher = {{arXiv}},
	author = {Sun, Haoran and Liu, Lixin and Li, Junjie and Wang, Fengyu and Dong, Baohua and Lin, Ran and Huang, Ruohui},
	urldate = {2024-05-01},
	date = {2024-04-03},
	eprinttype = {arxiv},
	eprint = {2404.02823 [cs]},
}

@misc{zhangChineseOpenInstruction2023,
	title = {Chinese Open Instruction Generalist: A Preliminary Release},
	url = {http://arxiv.org/abs/2304.07987},
	shorttitle = {Chinese Open Instruction Generalist},
	abstract = {Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of {InstructGPT}{\textasciitilde}{\textbackslash}citep\{ouyang2022training\} and {ChatGPT}{\textbackslash}footnote\{{\textbackslash}url\{https://chat.openai.com/\}\}. Despite impressive progress in English-oriented large-scale language models ({LLMs}), it is still under-explored whether English-based foundation {LLMs} can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning. To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applications of the newly constructed Chinese instruction corpora. The resulting {\textbackslash}textbf\{C\}hinese {\textbackslash}textbf\{O\}pen {\textbackslash}textbf\{I\}nstruction {\textbackslash}textbf\{G\}eneralist ({\textbackslash}textbf\{{COIG}\}) corpora are available in Huggingface{\textbackslash}footnote\{{\textbackslash}url\{https://huggingface.co/datasets/{BAAI}/{COIG}\}\} and Github{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/{BAAI}-Zlab/{COIG}\}\}, and will be continuously updated.},
	number = {{arXiv}:2304.07987},
	publisher = {{arXiv}},
	author = {Zhang, Ge and Shi, Yemin and Liu, Ruibo and Yuan, Ruibin and Li, Yizhi and Dong, Siwei and Shu, Yu and Li, Zhaoqun and Wang, Zekun and Lin, Chenghua and Huang, Wenhao and Fu, Jie},
	urldate = {2024-05-01},
	date = {2023-04-24},
	eprinttype = {arxiv},
	eprint = {2304.07987 [cs]},
}

@misc{zhengJudgingLLMasaJudgeMTBench2023,
	title = {Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
	url = {http://arxiv.org/abs/2306.05685},
	abstract = {Evaluating large language model ({LLM}) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong {LLMs} as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of {LLM}-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between {LLM} judges and human preferences by introducing two benchmarks: {MT}-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong {LLM} judges like {GPT}-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, {LLM}-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of {LLaMA} and Vicuna. The {MT}-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/{FastChat}/tree/main/fastchat/llm\_judge.},
	number = {{arXiv}:2306.05685},
	publisher = {{arXiv}},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	urldate = {2024-05-01},
	date = {2023-12-23},
	eprinttype = {arxiv},
	eprint = {2306.05685 [cs]},
}

@misc{zhengLMSYSChat1MLargeScaleRealWorld2024,
	title = {{LMSYS}-Chat-1M: A Large-Scale Real-World {LLM} Conversation Dataset},
	url = {http://arxiv.org/abs/2309.11998},
	shorttitle = {{LMSYS}-Chat-1M},
	abstract = {Studying how people interact with large language models ({LLMs}) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce {LMSYS}-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art {LLMs}. This dataset is collected from 210K unique {IP} addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to {GPT}-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing {LLM} capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.},
	number = {{arXiv}:2309.11998},
	publisher = {{arXiv}},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric P. and Gonzalez, Joseph E. and Stoica, Ion and Zhang, Hao},
	urldate = {2024-05-01},
	date = {2024-03-10},
	eprinttype = {arxiv},
	eprint = {2309.11998 [cs]},
}

@misc{zhouCOBRAFramesContextual2023,
	title = {{COBRA} Frames: Contextual Reasoning about Effects and Harms of Offensive Statements},
	url = {http://arxiv.org/abs/2306.01985},
	shorttitle = {{COBRA} Frames},
	abstract = {Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance "your English is very good" may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an {ESL} teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce {COBRA} frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create {COBRACORPUS}, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate {COBRA} explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29\% accuracy drop). Our work highlights the importance and feasibility of contextualized {NLP} by modeling social factors.},
	number = {{arXiv}:2306.01985},
	publisher = {{arXiv}},
	author = {Zhou, Xuhui and Zhu, Hao and Yerukola, Akhila and Davidson, Thomas and Hwang, Jena D. and Swayamdipta, Swabha and Sap, Maarten},
	urldate = {2024-05-01},
	date = {2023-06-08},
	eprinttype = {arxiv},
	eprint = {2306.01985 [cs]},
}

@misc{kopfOpenAssistantConversationsDemocratizing2023,
	title = {{OpenAssistant} Conversations -- Democratizing Large Language Model Alignment},
	url = {http://arxiv.org/abs/2304.07327},
	abstract = {Aligning large language models ({LLMs}) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by {ChatGPT}. Alignment techniques such as supervised fine-tuning ({SFT}) and reinforcement learning from human feedback ({RLHF}) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of {LLMs}, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like {RLHF} rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release {OpenAssistant} Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on {OpenAssistant} Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.},
	number = {{arXiv}:2304.07327},
	publisher = {{arXiv}},
	author = {Köpf, Andreas and Kilcher, Yannic and von Rütte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Richárd and {ES}, Shahul and Suri, Sameer and Glushkov, David and Dantuluri, Arnav and Maguire, Andrew and Schuhmann, Christoph and Nguyen, Huu and Mattick, Alexander},
	urldate = {2024-05-02},
	date = {2023-10-31},
	eprinttype = {arxiv},
	eprint = {2304.07327 [cs]},
}

@misc{dingEnhancingChatLanguage2023,
	title = {Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
	url = {http://arxiv.org/abs/2305.14233},
	abstract = {Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like {ChatGPT}. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, {UltraChat}, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an {AI} assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. {UltraChat} contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of {UltraChat} reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon {UltraChat}, we fine-tune a {LLaMA} model to create a powerful conversational model, {UltraLLaMA}. Our evaluations indicate that {UltraLLaMA} consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/thunlp/{UltraChat}\}\}.},
	number = {{arXiv}:2305.14233},
	publisher = {{arXiv}},
	author = {Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
	urldate = {2024-05-02},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14233 [cs]},
}

@misc{yaoWebShopScalableRealWorld2023,
	title = {{WebShop}: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
	url = {http://arxiv.org/abs/2207.01206},
	shorttitle = {{WebShop}},
	abstract = {Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop {WebShop} -- a simulated e-commerce website environment with \$1.18\$ million real-world products and \$12,087\$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. {WebShop} provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over \$1,600\$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of \$29{\textbackslash}\%\$, which outperforms rule-based heuristics (\$9.6{\textbackslash}\%\$) but is far lower than human expert performance (\$59{\textbackslash}\%\$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on {WebShop} exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of {WebShop} in developing practical web-based agents that can operate in the wild.},
	number = {{arXiv}:2207.01206},
	publisher = {{arXiv}},
	author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
	urldate = {2024-05-02},
	date = {2023-02-07},
	eprinttype = {arxiv},
	eprint = {2207.01206 [cs]},
}

@misc{liuAgentBenchEvaluatingLLMs2023,
	title = {{AgentBench}: Evaluating {LLMs} as Agents},
	url = {http://arxiv.org/abs/2308.03688},
	shorttitle = {{AgentBench}},
	abstract = {Large Language Models ({LLMs}) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional {NLP} tasks. As a result, there has been an urgent need to evaluate {LLMs} as agents on challenging tasks in interactive environments. We present {AgentBench}, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess {LLM}-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 {API}-based and open-sourced ({OSS}) {LLMs} shows that, while top commercial {LLMs} present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and {OSS} competitors. We identify the typical reasons of failures in environments and {LLMs}, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable {LLM} agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for {AgentBench} are released at {\textbackslash}url\{https://github.com/{THUDM}/{AgentBench}\}.},
	number = {{arXiv}:2308.03688},
	publisher = {{arXiv}},
	author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
	urldate = {2024-05-02},
	date = {2023-10-25},
	eprinttype = {arxiv},
	eprint = {2308.03688 [cs]},
}

@misc{zengAgentTuningEnablingGeneralized2023,
	title = {{AgentTuning}: Enabling Generalized Agent Abilities for {LLMs}},
	url = {http://arxiv.org/abs/2310.12823},
	shorttitle = {{AgentTuning}},
	abstract = {Open large language models ({LLMs}) with great performance in various tasks have significantly advanced the development of {LLMs}. However, they are far inferior to commercial models such as {ChatGPT} and {GPT}-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ {LLMs} as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust {LLMs} to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of {LLMs} themselves without compromising their general abilities. In this work, we present {AgentTuning}, a simple and general method to enhance the agent abilities of {LLMs} while maintaining their general {LLM} capabilities. We construct {AgentInstruct}, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining {AgentInstruct} with open-source instructions from general domains. {AgentTuning} is used to instruction-tune the Llama 2 series, resulting in {AgentLM}. Our evaluations show that {AgentTuning} enables {LLMs}' agent capabilities without compromising general abilities. The {AgentLM}-70B is comparable to {GPT}-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the {AgentInstruct} and {AgentLM}-7B, 13B, and 70B models at https://github.com/{THUDM}/{AgentTuning}, serving open and powerful alternatives to commercial {LLMs} for agent tasks.},
	number = {{arXiv}:2310.12823},
	publisher = {{arXiv}},
	author = {Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
	urldate = {2024-05-02},
	date = {2023-10-22},
	eprinttype = {arxiv},
	eprint = {2310.12823 [cs]},
}

@misc{dengMind2WebGeneralistAgent2023,
	title = {Mind2Web: Towards a Generalist Agent for the Web},
	url = {http://arxiv.org/abs/2306.06070},
	shorttitle = {Mind2Web},
	abstract = {We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models ({LLMs}) for building generalist web agents. While the raw {HTML} of real-world websites are often too large to be fed to {LLMs}, we show that first filtering it with a small {LM} significantly improves the effectiveness and efficiency of {LLMs}. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.},
	number = {{arXiv}:2306.06070},
	publisher = {{arXiv}},
	author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu},
	urldate = {2024-05-02},
	date = {2023-12-09},
	eprinttype = {arxiv},
	eprint = {2306.06070 [cs]},
}

@misc{wangCORD19COVID19Open2020,
	title = {{CORD}-19: The {COVID}-19 Open Research Dataset},
	url = {http://arxiv.org/abs/2004.10706},
	shorttitle = {{CORD}-19},
	abstract = {The {COVID}-19 Open Research Dataset ({CORD}-19) is a growing resource of scientific papers on {COVID}-19 and related historical coronavirus research. {CORD}-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, {CORD}-19 has been downloaded over 200K times and has served as the basis of many {COVID}-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how {CORD}-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for {COVID}-19.},
	number = {{arXiv}:2004.10706},
	publisher = {{arXiv}},
	author = {Wang, Lucy Lu and Lo, Kyle and Chandrasekhar, Yoganand and Reas, Russell and Yang, Jiangjiang and Burdick, Doug and Eide, Darrin and Funk, Kathryn and Katsis, Yannis and Kinney, Rodney and Li, Yunyao and Liu, Ziyang and Merrill, William and Mooney, Paul and Murdick, Dewey and Rishi, Devvret and Sheehan, Jerry and Shen, Zhihong and Stilson, Brandon and Wade, Alex and Wang, Kuansan and Wang, Nancy Xin Ru and Wilhelm, Chris and Xie, Boya and Raymond, Douglas and Weld, Daniel S. and Etzioni, Oren and Kohlmeier, Sebastian},
	urldate = {2024-05-02},
	date = {2020-07-10},
	eprinttype = {arxiv},
	eprint = {2004.10706 [cs]},
}

@misc{jinWhatDiseaseDoes2020,
	title = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
	url = {http://arxiv.org/abs/2009.13081},
	shorttitle = {What Disease does this Patient Have?},
	abstract = {Open domain question answering ({OpenQA}) tasks have been recently attracting more and more attention from the natural language processing ({NLP}) community. In this work, we present the first free-form multiple-choice {OpenQA} dataset for solving medical problems, {MedQA}, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect {MedQA} to present great challenges to existing {OpenQA} systems and hope that it can serve as a platform to promote much stronger {OpenQA} models from the {NLP} community in the future.},
	number = {{arXiv}:2009.13081},
	publisher = {{arXiv}},
	author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
	urldate = {2024-05-02},
	date = {2020-09-28},
	eprinttype = {arxiv},
	eprint = {2009.13081 [cs]},
}

@misc{saveryQuestionDrivenSummarizationAnswers2020,
	title = {Question-Driven Summarization of Answers to Consumer Health Questions},
	url = {http://arxiv.org/abs/2005.09067},
	abstract = {Automatic summarization of natural language is a widely studied area in computer science, one that is broadly applicable to anyone who routinely needs to understand large quantities of information. For example, in the medical domain, recent developments in deep learning approaches to automatic summarization have the potential to make health information more easily accessible to patients and consumers. However, to evaluate the quality of automatically generated summaries of health information, gold-standard, human generated summaries are required. Using answers provided by the National Library of Medicine's consumer health question answering system, we present the {MEDIQA} Answer Summarization dataset, the first summarization collection containing question-driven summaries of answers to consumer health questions. This dataset can be used to evaluate single or multi-document summaries generated by algorithms using extractive or abstractive approaches. In order to benchmark the dataset, we include results of baseline and state-of-the-art deep learning summarization models, demonstrating that this dataset can be used to effectively evaluate question-driven machine-generated summaries and promote further machine learning research in medical question answering.},
	number = {{arXiv}:2005.09067},
	publisher = {{arXiv}},
	author = {Savery, Max and Abacha, Asma Ben and Gayen, Soumya and Demner-Fushman, Dina},
	urldate = {2024-05-02},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {2005.09067 [cs]},
}

@misc{linBiToDBilingualMultiDomain2021,
	title = {{BiToD}: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue Modeling},
	url = {http://arxiv.org/abs/2106.02787},
	shorttitle = {{BiToD}},
	abstract = {Task-oriented dialogue ({ToD}) benchmarks provide an important avenue to measure progress and develop better conversational agents. However, existing datasets for end-to-end {ToD} modeling are limited to a single language, hindering the development of robust end-to-end {ToD} systems for multilingual countries and regions. Here we introduce {BiToD}, the first bilingual multi-domain dataset for end-to-end task-oriented dialogue modeling. {BiToD} contains over 7k multi-domain dialogues (144k utterances) with a large and realistic bilingual knowledge base. It serves as an effective benchmark for evaluating bilingual {ToD} systems and cross-lingual transfer learning approaches. We provide state-of-the-art baselines under three evaluation settings (monolingual, bilingual, and cross-lingual). The analysis of our baselines in different settings highlights 1) the effectiveness of training a bilingual {ToD} system compared to two independent monolingual {ToD} systems, and 2) the potential of leveraging a bilingual knowledge base and cross-lingual transfer learning to improve the system performance under low resource condition.},
	number = {{arXiv}:2106.02787},
	publisher = {{arXiv}},
	author = {Lin, Zhaojiang and Madotto, Andrea and Winata, Genta Indra and Xu, Peng and Jiang, Feijun and Hu, Yuxiang and Shi, Chen and Fung, Pascale},
	urldate = {2024-05-02},
	date = {2021-06-04},
	eprinttype = {arxiv},
	eprint = {2106.02787 [cs]},
}

@misc{ericKeyValueRetrievalNetworks2017,
	title = {Key-Value Retrieval Networks for Task-Oriented Dialogue},
	url = {http://arxiv.org/abs/1705.05414},
	abstract = {Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.},
	number = {{arXiv}:1705.05414},
	publisher = {{arXiv}},
	author = {Eric, Mihail and Manning, Christopher D.},
	urldate = {2024-05-02},
	date = {2017-07-13},
	eprinttype = {arxiv},
	eprint = {1705.05414 [cs]},
}

@misc{alcazarAPESAudiovisualPerson2021,
	title = {{APES}: Audiovisual Person Search in Untrimmed Video},
	url = {http://arxiv.org/abs/2106.01667},
	shorttitle = {{APES}},
	abstract = {Humans are arguably one of the most important subjects in video streams, many real-world applications such as video summarization or video editing workflows often require the automatic search and retrieval of a person of interest. Despite tremendous efforts in the person reidentification and retrieval domains, few works have developed audiovisual search strategies. In this paper, we present the Audiovisual Person Search dataset ({APES}), a new dataset composed of untrimmed videos whose audio (voices) and visual (faces) streams are densely annotated. {APES} contains over 1.9K identities labeled along 36 hours of video, making it the largest dataset available for untrimmed audiovisual person search. A key property of {APES} is that it includes dense temporal annotations that link faces to speech segments of the same identity. To showcase the potential of our new dataset, we propose an audiovisual baseline and benchmark for person retrieval. Our study shows that modeling audiovisual cues benefits the recognition of people's identities. To enable reproducibility and promote future research, the dataset annotations and baseline code are available at: https://github.com/fuankarion/audiovisual-person-search},
	number = {{arXiv}:2106.01667},
	publisher = {{arXiv}},
	author = {Alcazar, Juan Leon and Mai, Long and Perazzi, Federico and Lee, Joon-Young and Arbelaez, Pablo and Ghanem, Bernard and Heilbron, Fabian Caba},
	urldate = {2024-05-29},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2106.01667 [cs]},
}

@misc{guAVAVideoDataset2018,
	title = {{AVA}: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions},
	url = {http://arxiv.org/abs/1705.08421},
	shorttitle = {{AVA}},
	abstract = {This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions ({AVA}). The {AVA} dataset densely annotates 80 atomic visual actions in 430 15-minute video clips, where actions are localized in space and time, resulting in 1.58M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. We will release the dataset publicly. {AVA}, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on {JHMDB} and {UCF}101-24 categories. While setting a new state of the art on existing datasets, the overall results on {AVA} are low at 15.6\% {mAP}, underscoring the need for developing new approaches for video understanding.},
	number = {{arXiv}:1705.08421},
	publisher = {{arXiv}},
	author = {Gu, Chunhui and Sun, Chen and Ross, David A. and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and Schmid, Cordelia and Malik, Jitendra},
	urldate = {2024-05-29},
	date = {2018-04-30},
	eprinttype = {arxiv},
	eprint = {1705.08421 [cs]},
}

@misc{rothAVAActiveSpeakerAudioVisualDataset2019,
	title = {{AVA}-{ActiveSpeaker}: An Audio-Visual Dataset for Active Speaker Detection},
	url = {http://arxiv.org/abs/1901.01342},
	shorttitle = {{AVA}-{ActiveSpeaker}},
	abstract = {Active speaker detection is an important component in video analysis algorithms for applications such as speaker diarization, video re-targeting for meetings, speech enhancement, and human-robot interaction. The absence of a large, carefully labeled audio-visual dataset for this task has constrained algorithm evaluations with respect to data diversity, environments, and accuracy. This has made comparisons and improvements difficult. In this paper, we present the {AVA} Active Speaker detection dataset ({AVA}-{ActiveSpeaker}) that will be released publicly to facilitate algorithm development and enable comparisons. The dataset contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio. We also present a new audio-visual approach for active speaker detection, and analyze its performance, demonstrating both its strength and the contributions of the dataset.},
	number = {{arXiv}:1901.01342},
	publisher = {{arXiv}},
	author = {Roth, Joseph and Chaudhuri, Sourish and Klejch, Ondrej and Marvin, Radhika and Gallagher, Andrew and Kaver, Liat and Ramaswamy, Sharadh and Stopczynski, Arkadiusz and Schmid, Cordelia and Xi, Zhonghua and Pantofaru, Caroline},
	urldate = {2024-05-29},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1901.01342 [cs, eess]},
}

@misc{bainCondensedMoviesStory2020,
	title = {Condensed Movies: Story Based Retrieval with Contextual Embeddings},
	url = {http://arxiv.org/abs/2005.04208},
	shorttitle = {Condensed Movies},
	abstract = {Our objective in this work is long range understanding of the narrative structure of movies. Instead of considering the entire movie, we propose to learn from the `key scenes' of the movie, providing a condensed look at the full storyline. To this end, we make the following three contributions: (i) We create the Condensed Movies Dataset ({CMD}) consisting of the key scenes from over 3K movies: each key scene is accompanied by a high level semantic description of the scene, character face-tracks, and metadata about the movie. The dataset is scalable, obtained automatically from {YouTube}, and is freely available for anybody to download and use. It is also an order of magnitude larger than existing movie datasets in the number of movies; (ii) We provide a deep network baseline for text-to-video retrieval on our dataset, combining character, speech and visual cues into a single video embedding; and finally (iii) We demonstrate how the addition of context from other video clips improves retrieval performance.},
	number = {{arXiv}:2005.04208},
	publisher = {{arXiv}},
	author = {Bain, Max and Nagrani, Arsha and Brown, Andrew and Zisserman, Andrew},
	urldate = {2024-05-29},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2005.04208 [cs]},
}

@misc{xueAdvancingHighResolutionVideoLanguage2022,
	title = {Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions},
	url = {http://arxiv.org/abs/2111.10337},
	abstract = {We study joint video and language ({VL}) pre-training to enable cross-modality learning and benefit plentiful downstream {VL} tasks. Existing works either extract low-quality video features or learn limited text embedding, while neglecting that high-resolution videos and diversified semantics can significantly improve cross-modality learning. In this paper, we propose a novel High-resolution and Diversified {VIdeo}-{LAnguage} pre-training model ({HD}-{VILA}) for many visual tasks. In particular, we collect a large dataset with two distinct properties: 1) the first high-resolution dataset including 371.5k hours of 720p videos, and 2) the most diversified dataset covering 15 popular {YouTube} categories. To enable {VL} pre-training, we jointly optimize the {HD}-{VILA} model by a hybrid Transformer that learns rich spatiotemporal features, and a multimodal Transformer that enforces interactions of the learned video features with diversified texts. Our pre-training model achieves new state-of-the-art results in 10 {VL} understanding tasks and 2 more novel text-to-visual generation tasks. For example, we outperform {SOTA} models with relative increases of 40.4\% R@1 in zero-shot {MSR}-{VTT} text-to-video retrieval task and 55.4\% in high-resolution dataset {LSMDC}. The learned {VL} embedding is also effective in generating visually pleasing and semantically relevant results in text-to-visual editing and super-resolution tasks.},
	number = {{arXiv}:2111.10337},
	publisher = {{arXiv}},
	author = {Xue, Hongwei and Hang, Tiankai and Zeng, Yanhong and Sun, Yuchong and Liu, Bei and Yang, Huan and Fu, Jianlong and Guo, Baining},
	urldate = {2024-05-29},
	date = {2022-07-08},
	eprinttype = {arxiv},
	eprint = {2111.10337 [cs]},
}

@misc{bojanowskiWeaklySupervisedAction2014,
	title = {Weakly Supervised Action Labeling in Videos Under Ordering Constraints},
	url = {http://arxiv.org/abs/1407.1208},
	abstract = {We are given a set of video clips, each one annotated with an \{{\textbackslash}em ordered\} list of actions, such as "walk" then "sit" then "answer phone" extracted from, for example, the associated text script. We seek to temporally localize the individual actions in each clip as well as to learn a discriminative classifier for each action. We formulate the problem as a weakly supervised temporal assignment with ordering constraints. Each video clip is divided into small time intervals and each time interval of each video clip is assigned one action label, while respecting the order in which the action labels appear in the given annotations. We show that the action label assignment can be determined together with learning a classifier for each action in a discriminative manner. We evaluate the proposed model on a new and challenging dataset of 937 video clips with a total of 787720 frames containing sequences of 16 different actions from 69 Hollywood movies.},
	number = {{arXiv}:1407.1208},
	publisher = {{arXiv}},
	author = {Bojanowski, Piotr and Lajugie, Rémi and Bach, Francis and Laptev, Ivan and Ponce, Jean and Schmid, Cordelia and Sivic, Josef},
	urldate = {2024-05-29},
	date = {2014-07-04},
	eprinttype = {arxiv},
	eprint = {1407.1208 [cs]},
}

@misc{sharmaDeepMultimodalFeature2020,
	title = {Deep Multimodal Feature Encoding for Video Ordering},
	url = {http://arxiv.org/abs/2004.02205},
	abstract = {True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the "Large Scale Movie Description Challenge". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.},
	number = {{arXiv}:2004.02205},
	publisher = {{arXiv}},
	author = {Sharma, Vivek and Tapaswi, Makarand and Stiefelhagen, Rainer},
	urldate = {2024-05-29},
	date = {2020-04-05},
	eprinttype = {arxiv},
	eprint = {2004.02205 [cs]},
}

@misc{rohrbachMovieDescription2016,
	title = {Movie Description},
	url = {http://arxiv.org/abs/1605.03705},
	abstract = {Audio Description ({AD}) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed {ADs}, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. In total the Large Scale Movie Description Challenge ({LSMDC}) contains a parallel corpus of 118,114 sentences and video clips from 202 movies. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing {ADs} to scripts, we find that {ADs} are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in a challenge organized in the context of the workshop "Describing and Understanding Video \& The Large Scale Movie Description Challenge ({LSMDC})", at {ICCV} 2015.},
	number = {{arXiv}:1605.03705},
	publisher = {{arXiv}},
	author = {Rohrbach, Anna and Torabi, Atousa and Rohrbach, Marcus and Tandon, Niket and Pal, Christopher and Larochelle, Hugo and Courville, Aaron and Schiele, Bernt},
	urldate = {2024-05-29},
	date = {2016-05-12},
	eprinttype = {arxiv},
	eprint = {1605.03705 [cs]},
}

@misc{huangMovieNetHolisticDataset2020,
	title = {{MovieNet}: A Holistic Dataset for Movie Understanding},
	url = {http://arxiv.org/abs/2007.10937},
	shorttitle = {{MovieNet}},
	abstract = {Recent years have seen remarkable advances in visual understanding. However, how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce {MovieNet} -- a holistic dataset for movie understanding. {MovieNet} contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc. Besides, different aspects of manual annotations are provided in {MovieNet}, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92K tags of cinematic style. To the best of our knowledge, {MovieNet} is the largest dataset with richest annotations for comprehensive movie understanding. Based on {MovieNet}, we set up several benchmarks for movie understanding from different angles. Extensive experiments are executed on these benchmarks to show the immeasurable value of {MovieNet} and the gap of current approaches towards comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based long video understanding and beyond. {MovieNet} will be published in compliance with regulations at https://movienet.github.io.},
	number = {{arXiv}:2007.10937},
	publisher = {{arXiv}},
	author = {Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
	urldate = {2024-05-29},
	date = {2020-07-21},
	eprinttype = {arxiv},
	eprint = {2007.10937 [cs]},
}

@misc{vicolMovieGraphsUnderstandingHumanCentric2018,
	title = {{MovieGraphs}: Towards Understanding Human-Centric Situations from Videos},
	url = {http://arxiv.org/abs/1712.06761},
	shorttitle = {{MovieGraphs}},
	abstract = {There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to "read" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called {MovieGraphs} which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. {MovieGraphs} is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent {AI} agents.},
	number = {{arXiv}:1712.06761},
	publisher = {{arXiv}},
	author = {Vicol, Paul and Tapaswi, Makarand and Castrejon, Lluis and Fidler, Sanja},
	urldate = {2024-05-29},
	date = {2018-04-15},
	eprinttype = {arxiv},
	eprint = {1712.06761 [cs]},
}

@misc{tapaswiMovieQAUnderstandingStories2016,
	title = {{MovieQA}: Understanding Stories in Movies through Question-Answering},
	url = {http://arxiv.org/abs/1512.02902},
	shorttitle = {{MovieQA}},
	abstract = {We introduce the {MovieQA} dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler "Who" did "What" to "Whom", to "Why" and "How" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- video clips, plots, subtitles, scripts, and {DVS}. We analyze our data through various statistics and methods. We further extend existing {QA} techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain.},
	number = {{arXiv}:1512.02902},
	publisher = {{arXiv}},
	author = {Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	urldate = {2024-05-29},
	date = {2016-09-21},
	eprinttype = {arxiv},
	eprint = {1512.02902 [cs]},
}

@misc{raoLocaltoGlobalApproachMultimodal2020,
	title = {A Local-to-Global Approach to Multi-modal Movie Scene Segmentation},
	url = {http://arxiv.org/abs/2004.02678},
	abstract = {Scene, as the crucial unit of storytelling in movies, contains complex activities of actors and their interactions in a physical environment. Identifying the composition of scenes serves as a critical step towards semantic understanding of movies. This is very challenging -- compared to the videos studied in conventional vision problems, e.g. action recognition, as scenes in movies usually contain much richer temporal structures and more complex semantic information. Towards this goal, we scale up the scene segmentation task by building a large-scale video dataset {MovieScenes}, which contains 21K annotated scene segments from 150 movies. We further propose a local-to-global scene segmentation framework, which integrates multi-modal information across three levels, i.e. clip, segment, and movie. This framework is able to distill complex semantics from hierarchical temporal structures over a long movie, providing top-down guidance for scene segmentation. Our experiments show that the proposed network is able to segment a movie into scenes with high accuracy, consistently outperforming previous methods. We also found that pretraining on our {MovieScenes} can bring significant improvements to the existing approaches.},
	number = {{arXiv}:2004.02678},
	publisher = {{arXiv}},
	author = {Rao, Anyi and Xu, Linning and Xiong, Yu and Xu, Guodong and Huang, Qingqiu and Zhou, Bolei and Lin, Dahua},
	urldate = {2024-05-29},
	date = {2020-04-28},
	eprinttype = {arxiv},
	eprint = {2004.02678 [cs]},
}

@misc{xiongGraphBasedFrameworkBridge2019,
	title = {A Graph-Based Framework to Bridge Movies and Synopses},
	url = {http://arxiv.org/abs/1910.11009},
	abstract = {Inspired by the remarkable advances in video analytics, research teams are stepping towards a greater ambition -- movie understanding. However, compared to those activity videos in conventional datasets, movies are significantly different. Generally, movies are much longer and consist of much richer temporal structures. More importantly, the interactions among characters play a central role in expressing the underlying story. To facilitate the efforts along this direction, we construct a dataset called Movie Synopses Associations ({MSA}) over 327 movies, which provides a synopsis for each movie, together with annotated associations between synopsis paragraphs and movie segments. On top of this dataset, we develop a framework to perform matching between movie segments and synopsis paragraphs. This framework integrates different aspects of a movie, including event dynamics and character interactions, and allows them to be matched with parsed paragraphs, based on a graph-based formulation. Our study shows that the proposed framework remarkably improves the matching accuracy over conventional feature-based methods. It also reveals the importance of narrative structures and character interactions in movie understanding.},
	number = {{arXiv}:1910.11009},
	publisher = {{arXiv}},
	author = {Xiong, Yu and Huang, Qingqiu and Guo, Lingfeng and Zhou, Hang and Zhou, Bolei and Lin, Dahua},
	urldate = {2024-05-29},
	date = {2019-10-24},
	eprinttype = {arxiv},
	eprint = {1910.11009 [cs]},
}

@misc{lvAriaEverydayActivities2024,
	title = {Aria Everyday Activities Dataset},
	url = {http://arxiv.org/abs/2402.13349},
	abstract = {We present Aria Everyday Activities ({AEA}) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses. {AEA} contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains multimodal sensor data recorded through the Project Aria glasses. In addition, {AEA} provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. {AEA} is an open source dataset that can be downloaded from https://www.projectaria.com/datasets/aea/. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools https://github.com/facebookresearch/projectaria\_tools.},
	number = {{arXiv}:2402.13349},
	publisher = {{arXiv}},
	author = {Lv, Zhaoyang and Charron, Nicholas and Moulon, Pierre and Gamino, Alexander and Peng, Cheng and Sweeney, Chris and Miller, Edward and Tang, Huixuan and Meissner, Jeff and Dong, Jing and Somasundaram, Kiran and Pesqueira, Luis and Schwesinger, Mark and Parkhi, Omkar and Gu, Qiao and De Nardi, Renzo and Cheng, Shangyi and Saarinen, Steve and Baiyya, Vijay and Zou, Yuyang and Newcombe, Richard and Engel, Jakob Julian and Pan, Xiaqing and Ren, Carl},
	urldate = {2024-05-29},
	date = {2024-02-21},
	eprinttype = {arxiv},
	eprint = {2402.13349 [cs]},
}

@misc{oncescuQuerYDVideoDataset2021,
	title = {{QuerYD}: A video dataset with high-quality text and audio narrations},
	url = {http://arxiv.org/abs/2011.11071},
	shorttitle = {{QuerYD}},
	abstract = {We introduce {QuerYD}, a new large-scale dataset for retrieval and event localisation in video. A unique feature of our dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content. The dataset is based on {YouDescribe}, a volunteer project that assists visually-impaired people by attaching voiced narrations to existing {YouTube} videos. This ever-growing collection of videos contains highly detailed, temporally aligned audio and text annotations. The content descriptions are more relevant than dialogue, and more detailed than previous description attempts, which can be observed to contain many superficial or uninformative descriptions. To demonstrate the utility of the {QuerYD} dataset, we show that it can be used to train and benchmark strong models for retrieval and event localisation. Data, code and models are made publicly available, and we hope that {QuerYD} inspires further research on video understanding with written and spoken natural language.},
	number = {{arXiv}:2011.11071},
	publisher = {{arXiv}},
	author = {Oncescu, Andreea-Maria and Henriques, João F. and Liu, Yang and Zisserman, Andrew and Albanie, Samuel},
	urldate = {2024-05-01},
	date = {2021-02-17},
	eprinttype = {arxiv},
	eprint = {2011.11071 [cs]},
}

@misc{raiHomeActionGenome2021,
	title = {Home Action Genome: Cooperative Compositional Action Understanding},
	url = {http://arxiv.org/abs/2105.05226},
	shorttitle = {Home Action Genome},
	abstract = {Existing research on action recognition treats activities as monolithic events occurring in videos. Recently, the benefits of formulating actions as a combination of atomic-actions have shown promise in improving action understanding with the emergence of datasets containing such annotations, allowing us to learn representations capturing this information. However, there remains a lack of studies that extend action composition and leverage multiple viewpoints and multiple modalities of data for representation learning. To promote research in this direction, we introduce Home Action Genome ({HOMAGE}): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels. Leveraging rich multi-modal and multi-view settings, we propose Cooperative Compositional Action Understanding ({CCAU}), a cooperative learning framework for hierarchical action recognition that is aware of compositional action elements. {CCAU} shows consistent performance improvements across all modalities. Furthermore, we demonstrate the utility of co-learning compositions in few-shot action recognition by achieving 28.6\% {mAP} with just a single sample.},
	number = {{arXiv}:2105.05226},
	publisher = {{arXiv}},
	author = {Rai, Nishant and Chen, Haofeng and Ji, Jingwei and Desai, Rishi and Kozuka, Kazuki and Ishizaka, Shun and Adeli, Ehsan and Niebles, Juan Carlos},
	urldate = {2024-05-02},
	date = {2021-05-11},
	eprinttype = {arxiv},
	eprint = {2105.05226 [cs]},
}

@misc{rohrbachDatasetMovieDescription2015,
	title = {A Dataset for Movie Description},
	url = {http://arxiv.org/abs/1501.02530},
	abstract = {Descriptive video service ({DVS}) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed {DVS}, which is temporally aligned to full length {HD} movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions. In total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72 {HD} movies. We characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing {DVS} to scripts, we find that {DVS} is far more visual and describes precisely what is shown rather than what should happen according to the scripts created prior to movie production.},
	number = {{arXiv}:1501.02530},
	publisher = {{arXiv}},
	author = {Rohrbach, Anna and Rohrbach, Marcus and Tandon, Niket and Schiele, Bernt},
	urldate = {2024-05-01},
	date = {2015-01-11},
	eprinttype = {arxiv},
	eprint = {1501.02530 [cs]},
}

@misc{sanabriaHow2LargescaleDataset2018,
	title = {How2: A Large-scale Dataset for Multimodal Language Understanding},
	url = {http://arxiv.org/abs/1811.00347},
	shorttitle = {How2},
	abstract = {In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks, we hope to stimulate more research on these and similar challenges, to obtain a deeper understanding of multimodality in language processing.},
	number = {{arXiv}:1811.00347},
	publisher = {{arXiv}},
	author = {Sanabria, Ramon and Caglayan, Ozan and Palaskar, Shruti and Elliott, Desmond and Barrault, Loïc and Specia, Lucia and Metze, Florian},
	urldate = {2024-05-01},
	date = {2018-12-07},
	eprinttype = {arxiv},
	eprint = {1811.00347 [cs]},
}

@misc{shahroudyNTURGBLarge2016,
	title = {{NTU} {RGB}+D: A Large Scale Dataset for 3D Human Activity Analysis},
	url = {http://arxiv.org/abs/1604.02808},
	shorttitle = {{NTU} {RGB}+D},
	abstract = {Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and {RGB}+D-based action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for {RGB}+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art hand-crafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and {RGB}+D-based human activity analysis.},
	number = {{arXiv}:1604.02808},
	publisher = {{arXiv}},
	author = {Shahroudy, Amir and Liu, Jun and Ng, Tian-Tsong and Wang, Gang},
	urldate = {2024-05-02},
	date = {2016-04-11},
	eprinttype = {arxiv},
	eprint = {1604.02808 [cs]},
}

@misc{shanUnderstandingHumanHands2020,
	title = {Understanding Human Hands in Contact at Internet Scale},
	url = {http://arxiv.org/abs/2006.06669},
	abstract = {Hands are the central means by which humans manipulate their world and being able to reliably extract hand state information from Internet videos of humans engaged in their hands has the potential to pave the way to systems that can learn from petabytes of video data. This paper proposes steps towards this by inferring a rich representation of hands engaged in interaction method that includes: hand location, side, contact state, and a box around the object in contact. To support this effort, we gather a large-scale dataset of hands in contact with objects consisting of 131 days of footage as well as a 100K annotated hand-contact video frame dataset. The learned model on this dataset can serve as a foundation for hand-contact understanding in videos. We quantitatively evaluate it both on its own and in service of predicting and learning from 3D meshes of human hands.},
	number = {{arXiv}:2006.06669},
	publisher = {{arXiv}},
	author = {Shan, Dandan and Geng, Jiaqi and Shu, Michelle and Fouhey, David F.},
	urldate = {2024-05-02},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2006.06669 [cs]},
}

@misc{panAriaDigitalTwin2023,
	title = {Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception},
	url = {http://arxiv.org/abs/2306.06362},
	shorttitle = {Aria Digital Twin},
	abstract = {We introduce the Aria Digital Twin ({ADT}) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This {ADT} release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one {RGB} camera stream, two {IMU} streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to {ADT}. By contributing {ADT} to the research community, our mission is to set a new standard for evaluation in the egocentric machine perception domain, which includes very challenging research problems such as 3D object detection and tracking, scene reconstruction and understanding, sim-to-real learning, human pose prediction - while also inspiring new machine perception tasks for augmented reality ({AR}) applications. To kick start exploration of the {ADT} research use cases, we evaluated several existing state-of-the-art methods for object detection, segmentation and image translation tasks that demonstrate the usefulness of {ADT} as a benchmarking dataset.},
	number = {{arXiv}:2306.06362},
	publisher = {{arXiv}},
	author = {Pan, Xiaqing and Charron, Nicholas and Yang, Yongqian and Peters, Scott and Whelan, Thomas and Kong, Chen and Parkhi, Omkar and Newcombe, Richard and Ren, Carl Yuheng},
	urldate = {2024-05-29},
	date = {2023-06-13},
	eprinttype = {arxiv},
	eprint = {2306.06362 [cs]},
}

@misc{awadTRECVID2019Evaluation2020,
	title = {{TRECVID} 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search \& Retrieval},
	url = {http://arxiv.org/abs/2009.09984},
	shorttitle = {{TRECVID} 2019},
	abstract = {The {TREC} Video Retrieval Evaluation ({TRECVID}) 2019 was a {TREC}-style video analysis and retrieval evaluation, the goal of which remains to promote progress in research and development of content-based exploitation and retrieval of information from digital video via open, metrics-based evaluation. Over the last nineteen years this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. {TRECVID} has been funded by {NIST} (National Institute of Standards and Technology) and other {US} government agencies. In addition, many organizations and individuals worldwide contribute significant time and effort. {TRECVID} 2019 represented a continuation of four tasks from {TRECVID} 2018. In total, 27 teams from various research organizations worldwide completed one or more of the following four tasks: 1. Ad-hoc Video Search ({AVS}) 2. Instance Search ({INS}) 3. Activities in Extended Video ({ActEV}) 4. Video to Text Description ({VTT}) This paper is an introduction to the evaluation framework, tasks, data, and measures used in the workshop.},
	number = {{arXiv}:2009.09984},
	publisher = {{arXiv}},
	author = {Awad, George and Butt, Asad A. and Curtis, Keith and Lee, Yooyoung and Fiscus, Jonathan and Godil, Afzal and Delgado, Andrew and Zhang, Jesse and Godard, Eliot and Diduch, Lukas and Smeaton, Alan F. and Graham, Yvette and Kraaij, Wessel and Quenot, Georges},
	urldate = {2024-05-29},
	date = {2020-09-21},
	eprinttype = {arxiv},
	eprint = {2009.09984 [cs]},
}

@misc{olatunjiAfriSpeech200PanAfricanAccented2023,
	title = {{AfriSpeech}-200: Pan-African Accented Speech Dataset for Clinical and General Domain {ASR}},
	url = {http://arxiv.org/abs/2310.00274},
	shorttitle = {{AfriSpeech}-200},
	abstract = {Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors could see 30+ patients per day -- a heavy patient burden compared with developed countries -- but productivity tools such as clinical automatic speech recognition ({ASR}) are lacking for these overworked clinicians. However, clinical {ASR} is mature, even ubiquitous, in developed nations, and clinician-reported performance of commercial clinical {ASR} systems is generally satisfactory. Furthermore, the recent performance of general domain {ASR} is approaching human accuracy. However, several gaps exist. Several publications have highlighted racial bias with speech-to-text algorithms and performance on minority accents lags significantly. To our knowledge, there is no publicly available research or benchmark on accented African clinical {ASR}, and speech data is non-existent for the majority of African accents. We release {AfriSpeech}, 200hrs of Pan-African English speech, 67,577 clips from 2,463 unique speakers across 120 indigenous accents from 13 countries for clinical and general domain {ASR}, a benchmark test set, with publicly available pre-trained models with {SOTA} performance on the {AfriSpeech} benchmark.},
	number = {{arXiv}:2310.00274},
	publisher = {{arXiv}},
	author = {Olatunji, Tobi and Afonja, Tejumade and Yadavalli, Aditya and Emezue, Chris Chinenye and Singh, Sahib and Dossou, Bonaventure F. P. and Osuchukwu, Joanne and Osei, Salomey and Tonja, Atnafu Lambebo and Etori, Naome and Mbataku, Clinton},
	urldate = {2024-05-29},
	date = {2023-09-30},
	eprinttype = {arxiv},
	eprint = {2310.00274 [cs]},
}

@misc{fuAISHELL4OpenSource2021,
	title = {{AISHELL}-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario},
	url = {http://arxiv.org/abs/2104.03603},
	shorttitle = {{AISHELL}-4},
	abstract = {In this paper, we present {AISHELL}-4, a sizable real-recorded Mandarin speech dataset collected by 8-channel circular microphone array for speech processing in conference scenario. The dataset consists of 211 recorded meeting sessions, each containing 4 to 8 speakers, with a total length of 120 hours. This dataset aims to bridge the advanced research on multi-speaker processing and the practical application scenario in three aspects. With real recorded meetings, {AISHELL}-4 provides realistic acoustics and rich natural speech characteristics in conversation such as short pause, speech overlap, quick speaker turn, noise, etc. Meanwhile, accurate transcription and speaker voice activity are provided for each meeting in {AISHELL}-4. This allows the researchers to explore different aspects in meeting processing, ranging from individual tasks such as speech front-end processing, speech recognition and speaker diarization, to multi-modality modeling and joint optimization of relevant tasks. Given most open source dataset for multi-speaker tasks are in English, {AISHELL}-4 is the only Mandarin dataset for conversation speech, providing additional value for data diversity in speech community. We also release a {PyTorch}-based training and evaluation framework as baseline system to promote reproducible research in this field.},
	number = {{arXiv}:2104.03603},
	publisher = {{arXiv}},
	author = {Fu, Yihui and Cheng, Luyao and Lv, Shubo and Jv, Yukai and Kong, Yuxiang and Chen, Zhuo and Hu, Yanxin and Xie, Lei and Wu, Jian and Bu, Hui and Xu, Xin and Du, Jun and Chen, Jingdong},
	urldate = {2024-05-29},
	date = {2021-08-10},
	eprinttype = {arxiv},
	eprint = {2104.03603 [cs, eess]},
}

@misc{virkkunenFinnishParliamentASR2022,
	title = {Finnish Parliament {ASR} corpus - Analysis, benchmarks and statistics},
	url = {http://arxiv.org/abs/2203.14876},
	abstract = {Public sources like parliament meeting recordings and transcripts provide ever-growing material for the training and evaluation of automatic speech recognition ({ASR}) systems. In this paper, we publish and analyse the Finnish parliament {ASR} corpus, the largest publicly available collection of manually transcribed speech data for Finnish with over 3000 hours of speech and 449 speakers for which it provides rich demographic metadata. This corpus builds on earlier initial work, and as a result the corpus has a natural split into two training subsets from two periods of time. Similarly, there are two official, corrected test sets covering different times, setting an {ASR} task with longitudinal distribution-shift characteristics. An official development set is also provided. We develop a complete Kaldi-based data preparation pipeline, and hidden Markov model ({HMM}), hybrid deep neural network ({HMM}-{DNN}) and attention-based encoder-decoder ({AED}) {ASR} recipes. We set benchmarks on the official test sets, as well as multiple other recently used test sets. Both temporal corpus subsets are already large, and we observe that beyond their scale, {ASR} performance on the official test sets plateaus, whereas other domains benefit from added data. The {HMM}-{DNN} and {AED} approaches are compared in a carefully matched equal data setting, with the {HMM}-{DNN} system consistently performing better. Finally, the variation of the {ASR} accuracy is compared between the speaker categories available in the parliament metadata to detect potential biases based on factors such as gender, age, and education.},
	number = {{arXiv}:2203.14876},
	publisher = {{arXiv}},
	author = {Virkkunen, Anja and Rouhe, Aku and Phan, Nhan and Kurimo, Mikko},
	urldate = {2024-05-29},
	date = {2022-03-28},
	eprinttype = {arxiv},
	eprint = {2203.14876 [cs, eess]},
}

@misc{leongBloomLibraryMultimodal2022,
	title = {Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks},
	url = {http://arxiv.org/abs/2210.14712},
	shorttitle = {Bloom Library},
	abstract = {We present Bloom Library, a linguistically diverse set of multimodal and multilingual datasets for language modeling, image captioning, visual storytelling, and speech synthesis/recognition. These datasets represent either the most, or among the most, multilingual datasets for each of the included downstream tasks. In total, the initial release of the Bloom Library datasets covers 363 languages across 32 language families. We train downstream task models for various languages represented in the data, showing the viability of the data for future work in low-resource, multimodal {NLP} and establishing the first known baselines for these downstream tasks in certain languages (e.g., Bisu [bzi], with an estimated population of 700 users). Some of these first-of-their-kind baselines are comparable to state-of-the-art performance for higher-resourced languages. The Bloom Library datasets are released under Creative Commons licenses on the Hugging Face datasets hub to catalyze more linguistically diverse research in the included downstream tasks.},
	number = {{arXiv}:2210.14712},
	publisher = {{arXiv}},
	author = {Leong, Colin and Nemecek, Joshua and Mansdorfer, Jacob and Filighera, Anna and Owodunni, Abraham and Whitenack, Daniel},
	urldate = {2024-05-29},
	date = {2022-10-26},
	eprinttype = {arxiv},
	eprint = {2210.14712 [cs]},
}

@misc{guoDiDiSpeechLargeScale2021,
	title = {{DiDiSpeech}: A Large Scale Mandarin Speech Corpus},
	url = {http://arxiv.org/abs/2010.09275},
	shorttitle = {{DiDiSpeech}},
	abstract = {This paper introduces a new open-sourced Mandarin speech corpus, called {DiDiSpeech}. It consists of about 800 hours of speech data at 48kHz sampling rate from 6000 speakers and the corresponding texts. All speech data in the corpus is recorded in quiet environment and is suitable for various speech processing tasks, such as voice conversion, multi-speaker text-to-speech and automatic speech recognition. We conduct experiments with multiple speech tasks and evaluate the performance, showing that it is promising to use the corpus for both academic research and practical application. The corpus is available at https://outreach.didichuxing.com/research/opendata/.},
	number = {{arXiv}:2010.09275},
	publisher = {{arXiv}},
	author = {Guo, Tingwei and Wen, Cheng and Jiang, Dongwei and Luo, Ne and Zhang, Ruixiong and Zhao, Shuaijiang and Li, Wubo and Gong, Cheng and Zou, Wei and Han, Kun and Li, Xiangang},
	urldate = {2024-05-29},
	date = {2021-02-08},
	eprinttype = {arxiv},
	eprint = {2010.09275 [eess]},
}

@misc{delrioEarnings22PracticalBenchmark2022,
	title = {Earnings-22: A Practical Benchmark for Accents in the Wild},
	url = {http://arxiv.org/abs/2203.15591},
	shorttitle = {Earnings-22},
	abstract = {Modern automatic speech recognition ({ASR}) systems have achieved superhuman Word Error Rate ({WER}) on many common corpora despite lacking adequate performance on speech in the wild. Beyond that, there is a lack of real-world, accented corpora to properly benchmark academic and commercial models. To ensure this type of speech is represented in {ASR} benchmarking, we present Earnings-22, a 125 file, 119 hour corpus of English-language earnings calls gathered from global companies. We run a comparison across 4 commercial models showing the variation in performance when taking country of origin into consideration. Looking at hypothesis transcriptions, we explore errors common to all {ASR} systems tested. By examining Individual Word Error Rate ({IWER}), we find that key speech features impact model performance more for certain accents than others. Earnings-22 provides a free-to-use benchmark of real-world, accented audio to bridge academic and industrial research.},
	number = {{arXiv}:2203.15591},
	publisher = {{arXiv}},
	author = {Del Rio, Miguel and Ha, Peter and {McNamara}, Quinten and Miller, Corey and Chandra, Shipra},
	urldate = {2024-05-29},
	date = {2022-03-29},
	eprinttype = {arxiv},
	eprint = {2203.15591 [cs]},
}

@misc{nguyenHighQualityLargeScaleDataset2022,
	title = {A High-Quality and Large-Scale Dataset for English-Vietnamese Speech Translation},
	url = {http://arxiv.org/abs/2208.04243},
	abstract = {In this paper, we introduce a high-quality and large-scale benchmark dataset for English-Vietnamese speech translation with 508 audio hours, consisting of 331K triplets of (sentence-lengthed audio, English source transcript sentence, Vietnamese target subtitle sentence). We also conduct empirical experiments using strong baselines and find that the traditional "Cascaded" approach still outperforms the modern "End-to-End" approach. To the best of our knowledge, this is the first large-scale English-Vietnamese speech translation study. We hope both our publicly available dataset and study can serve as a starting point for future research and applications on English-Vietnamese speech translation. Our dataset is available at https://github.com/{VinAIResearch}/{PhoST}},
	number = {{arXiv}:2208.04243},
	publisher = {{arXiv}},
	author = {Nguyen, Linh The and Tran, Nguyen Luong and Doan, Long and Luong, Manh and Nguyen, Dat Quoc},
	urldate = {2024-05-29},
	date = {2022-08-08},
	eprinttype = {arxiv},
	eprint = {2208.04243 [cs]},
}

@misc{takamichiJTubeSpeechCorpusJapanese2021,
	title = {{JTubeSpeech}: corpus of Japanese speech collected from {YouTube} for speech recognition and speaker verification},
	url = {http://arxiv.org/abs/2112.09323},
	shorttitle = {{JTubeSpeech}},
	abstract = {In this paper, we construct a new Japanese speech corpus called "{JTubeSpeech}." Although recent end-to-end learning requires large-size speech corpora, open-sourced such corpora for languages other than English have not yet been established. In this paper, we describe the construction of a corpus from {YouTube} videos and subtitles for speech recognition and speaker verification. Our method can automatically filter the videos and subtitles with almost no language-dependent processes. We consistently employ Connectionist Temporal Classification ({CTC})-based techniques for automatic speech recognition ({ASR}) and a speaker variation-based method for automatic speaker verification ({ASV}). We build 1) a large-scale Japanese {ASR} benchmark with more than 1,300 hours of data and 2) 900 hours of data for Japanese {ASV}.},
	number = {{arXiv}:2112.09323},
	publisher = {{arXiv}},
	author = {Takamichi, Shinnosuke and Kürzinger, Ludwig and Saeki, Takaaki and Shiota, Sayaka and Watanabe, Shinji},
	urldate = {2024-05-29},
	date = {2021-12-17},
	eprinttype = {arxiv},
	eprint = {2112.09323 [cs, eess]},
}

@misc{khassanovCrowdsourcedOpenSourceKazakh2021,
	title = {A Crowdsourced Open-Source Kazakh Speech Corpus and Initial Speech Recognition Baseline},
	url = {http://arxiv.org/abs/2009.10334},
	abstract = {We present an open-source speech corpus for the Kazakh language. The Kazakh speech corpus ({KSC}) contains around 332 hours of transcribed audio comprising over 153,000 utterances spoken by participants from different regions and age groups, as well as both genders. It was carefully inspected by native Kazakh speakers to ensure high quality. The {KSC} is the largest publicly available database developed to advance various Kazakh speech and language processing applications. In this paper, we first describe the data collection and preprocessing procedures followed by a description of the database specifications. We also share our experience and challenges faced during the database construction, which might benefit other researchers planning to build a speech corpus for a low-resource language. To demonstrate the reliability of the database, we performed preliminary speech recognition experiments. The experimental results imply that the quality of audio and transcripts is promising (2.8\% character error rate and 8.7\% word error rate on the test set). To enable experiment reproducibility and ease the corpus usage, we also released an {ESPnet} recipe for our speech recognition models.},
	number = {{arXiv}:2009.10334},
	publisher = {{arXiv}},
	author = {Khassanov, Yerbolat and Mussakhojayeva, Saida and Mirzakhmetov, Almas and Adiyev, Alen and Nurpeiissov, Mukhamet and Varol, Huseyin Atakan},
	urldate = {2024-05-29},
	date = {2021-01-13},
	eprinttype = {arxiv},
	eprint = {2009.10334 [cs, eess]},
}

@misc{andoConstructionLargescaleJapanese2021,
	title = {Construction of a Large-scale Japanese {ASR} Corpus on {TV} Recordings},
	url = {http://arxiv.org/abs/2103.14736},
	abstract = {This paper presents a new large-scale Japanese speech corpus for training automatic speech recognition ({ASR}) systems. This corpus contains over 2,000 hours of speech with transcripts built on Japanese {TV} recordings and their subtitles. We develop herein an iterative workflow to extract matching audio and subtitle segments from {TV} recordings based on a conventional method for lightly-supervised audio-to-text alignment. We evaluate a model trained with our corpus using an evaluation dataset built on Japanese {TEDx} presentation videos and confirm that the performance is better than that trained with the Corpus of Spontaneous Japanese ({CSJ}). The experiment results show the usefulness of our corpus for training {ASR} systems. This corpus is made public for the research community along with Kaldi scripts for training the models reported in this paper.},
	number = {{arXiv}:2103.14736},
	publisher = {{arXiv}},
	author = {Ando, Shintaro and Fujihara, Hiromasa},
	urldate = {2024-05-29},
	date = {2021-03-26},
	eprinttype = {arxiv},
	eprint = {2103.14736 [cs, eess]},
}

@misc{yangOpenSourceMagicDataRAMC2022,
	title = {Open Source {MagicData}-{RAMC}: A Rich Annotated Mandarin Conversational({RAMC}) Speech Dataset},
	url = {http://arxiv.org/abs/2203.16844},
	shorttitle = {Open Source {MagicData}-{RAMC}},
	abstract = {This paper introduces a high-quality rich annotated Mandarin conversational ({RAMC}) speech dataset called {MagicData}-{RAMC}. The {MagicData}-{RAMC} corpus contains 180 hours of conversational speech data recorded from native speakers of Mandarin Chinese over mobile phones with a sampling rate of 16 {kHz}. The dialogs in {MagicData}-{RAMC} are classified into 15 diversified domains and tagged with topic labels, ranging from science and technology to ordinary life. Accurate transcription and precise speaker voice activity timestamps are manually labeled for each sample. Speakers' detailed information is also provided. As a Mandarin speech dataset designed for dialog scenarios with high quality and rich annotations, {MagicData}-{RAMC} enriches the data diversity in the Mandarin speech community and allows extensive research on a series of speech-related tasks, including automatic speech recognition, speaker diarization, topic detection, keyword search, text-to-speech, etc. We also conduct several relevant tasks and provide experimental results to help evaluate the dataset.},
	number = {{arXiv}:2203.16844},
	publisher = {{arXiv}},
	author = {Yang, Zehui and Chen, Yifan and Luo, Lei and Yang, Runyan and Ye, Lingxuan and Cheng, Gaofeng and Xu, Ji and Jin, Yaohui and Zhang, Qingqing and Zhang, Pengyuan and Xie, Lei and Yan, Yonghong},
	urldate = {2024-05-29},
	date = {2022-03-31},
	eprinttype = {arxiv},
	eprint = {2203.16844 [cs, eess]},
}

@misc{kolobovMediaSpeechMultilanguageASR2021,
	title = {{MediaSpeech}: Multilanguage {ASR} Benchmark and Dataset},
	url = {http://arxiv.org/abs/2103.16193},
	shorttitle = {{MediaSpeech}},
	abstract = {The performance of automated speech recognition ({ASR}) systems is well known to differ for varied application domains. At the same time, vendors and research groups typically report {ASR} quality results either for limited use simplistic domains (audiobooks, {TED} talks), or proprietary datasets. To fill this gap, we provide an open-source 10-hour {ASR} system evaluation dataset {NTR} {MediaSpeech} for 4 languages: Spanish, French, Turkish and Arabic. The dataset was collected from the official youtube channels of media in the respective languages, and manually transcribed. We estimate that the {WER} of the dataset is under 5\%. We have benchmarked many {ASR} systems available both commercially and freely, and provide the benchmark results. We also open-source baseline {QuartzNet} models for each language.},
	number = {{arXiv}:2103.16193},
	publisher = {{arXiv}},
	author = {Kolobov, Rostislav and Okhapkina, Olga and Omelchishina, Olga and Platunov, Andrey and Bedyakin, Roman and Moshkin, Vyacheslav and Menshikov, Dmitry and Mikhaylovskiy, Nikolay},
	urldate = {2024-05-29},
	date = {2021-03-30},
	eprinttype = {arxiv},
	eprint = {2103.16193 [cs, eess]},
}

@misc{gerzMultilingualCrossLingualIntent2021,
	title = {Multilingual and Cross-Lingual Intent Detection from Spoken Data},
	url = {http://arxiv.org/abs/2104.08524},
	abstract = {We present a systematic study on multilingual and cross-lingual intent detection from spoken data. The study leverages a new resource put forth in this work, termed {MInDS}-14, a first training and evaluation resource for the intent detection task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., {LaBSE}) can yield strong intent detectors in the majority of target languages covered in {MInDS}-14, and offer comparative analyses across different axes: e.g., zero-shot versus few-shot learning, translation direction, and impact of speech recognition. We see this work as an important step towards more inclusive development and evaluation of multilingual intent detectors from spoken data, in a much wider spectrum of languages compared to prior work.},
	number = {{arXiv}:2104.08524},
	publisher = {{arXiv}},
	author = {Gerz, Daniela and Su, Pei-Hao and Kusztos, Razvan and Mondal, Avishek and Lis, Michał and Singhal, Eshan and Mrkšić, Nikola and Wen, Tsung-Hsien and Vulić, Ivan},
	urldate = {2024-05-29},
	date = {2021-04-17},
	eprinttype = {arxiv},
	eprint = {2104.08524 [cs]},
}

@misc{saleskyMultilingualTEDxCorpus2021,
	title = {The Multilingual {TEDx} Corpus for Speech Recognition and Translation},
	url = {http://arxiv.org/abs/2102.01757},
	abstract = {We present the Multilingual {TEDx} corpus, built to support speech recognition ({ASR}) and speech translation ({ST}) research across many non-English source languages. The corpus is a collection of audio recordings from {TEDx} talks in 8 source languages. We segment transcripts into sentences and align them to the source-language audio and target-language translations. The corpus is released along with open-sourced code enabling extension to new talks and languages as they become available. Our corpus creation methodology can be applied to more languages than previous work, and creates multi-way parallel evaluation sets. We provide baselines in multiple {ASR} and {ST} settings, including multilingual models to improve translation performance for low-resource language pairs.},
	number = {{arXiv}:2102.01757},
	publisher = {{arXiv}},
	author = {Salesky, Elizabeth and Wiesner, Matthew and Bremerman, Jacob and Cattoni, Roldano and Negri, Matteo and Turchi, Marco and Oard, Douglas W. and Post, Matt},
	urldate = {2024-05-29},
	date = {2021-06-14},
	eprinttype = {arxiv},
	eprint = {2102.01757 [cs]},
}

@misc{parkOLKAVSOpenLargeScale2023,
	title = {{OLKAVS}: An Open Large-Scale Korean Audio-Visual Speech Dataset},
	url = {http://arxiv.org/abs/2301.06375},
	shorttitle = {{OLKAVS}},
	abstract = {Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech ({OLKAVS}) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the {OLKAVS} dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis.},
	number = {{arXiv}:2301.06375},
	publisher = {{arXiv}},
	author = {Park, Jeongkyun and Hwang, Jung-Wook and Choi, Kwanghee and Lee, Seung-Hyun and Ahn, Jun Hwan and Park, Rae-Hong and Park, Hyung-Min},
	urldate = {2024-05-29},
	date = {2023-01-16},
	eprinttype = {arxiv},
	eprint = {2301.06375 [cs]},
}

@misc{mubarakQASRQCRIAljazeera2021,
	title = {{QASR}: {QCRI} Aljazeera Speech Resource -- A Large Scale Annotated Arabic Speech Corpus},
	url = {http://arxiv.org/abs/2106.13000},
	shorttitle = {{QASR}},
	abstract = {We introduce the largest transcribed Arabic speech corpus, {QASR}, collected from the broadcast domain. This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, {QASR} contains linguistically motivated segmentation, punctuation, speaker information among others. {QASR} is suitable for training and evaluating speech recognition systems, acoustics- and/or linguistics- based Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other {NLP} modules for spoken data. In addition to {QASR} transcription, we release a dataset of 130M words to aid in designing and training a better language model. We show that end-to-end automatic speech recognition trained on {QASR} reports a competitive word error rate compared to the previous {MGB}-2 corpus. We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. We also report the first baseline for Arabic punctuation restoration. We make the corpus available for the research community.},
	number = {{arXiv}:2106.13000},
	publisher = {{arXiv}},
	author = {Mubarak, Hamdy and Hussein, Amir and Chowdhury, Shammur Absar and Ali, Ahmed},
	urldate = {2024-05-29},
	date = {2021-06-24},
	eprinttype = {arxiv},
	eprint = {2106.13000 [cs, eess]},
}

@misc{plussSDS200SwissGerman2022,
	title = {{SDS}-200: A Swiss German Speech to Standard German Text Corpus},
	url = {http://arxiv.org/abs/2205.09501},
	shorttitle = {{SDS}-200},
	abstract = {We present {SDS}-200, a corpus of Swiss German dialectal speech with Standard German text translations, annotated with dialect, age, and gender information of the speakers. The dataset allows for training speech translation, dialect recognition, and speech synthesis systems, among others. The data was collected using a web recording tool that is open to the public. Each participant was given a text in Standard German and asked to translate it to their Swiss German dialect before recording it. To increase the corpus quality, recordings were validated by other participants. The data consists of 200 hours of speech by around 4000 different speakers and covers a large part of the Swiss-German dialect landscape. We release {SDS}-200 alongside a baseline speech translation model, which achieves a word error rate ({WER}) of 30.3 and a {BLEU} score of 53.1 on the {SDS}-200 test set. Furthermore, we use {SDS}-200 to fine-tune a pre-trained {XLS}-R model, achieving 21.6 {WER} and 64.0 {BLEU}.},
	number = {{arXiv}:2205.09501},
	publisher = {{arXiv}},
	author = {Plüss, Michel and Hürlimann, Manuela and Cuny, Marc and Stöckli, Alla and Kapotis, Nikolaos and Hartmann, Julia and Ulasik, Malgorzata Anna and Scheller, Christian and Schraner, Yanick and Jain, Amit and Deriu, Jan and Cieliebak, Mark and Vogel, Manfred},
	urldate = {2024-05-29},
	date = {2022-05-19},
	eprinttype = {arxiv},
	eprint = {2205.09501 [cs]},
}

@misc{rajuSnowMountainDataset2023,
	title = {Snow Mountain: Dataset of Audio Recordings of The Bible in Low Resource Languages},
	url = {http://arxiv.org/abs/2206.01205},
	shorttitle = {Snow Mountain},
	abstract = {Automatic Speech Recognition ({ASR}) has increasing utility in the modern world. There are a many {ASR} models available for languages with large amounts of training data like English. However, low-resource languages are poorly represented. In response we create and release an open-licensed and formatted dataset of audio recordings of the Bible in low-resource northern Indian languages. We setup multiple experimental splits and train and analyze two competitive {ASR} models to serve as the baseline for future research using this data.},
	number = {{arXiv}:2206.01205},
	publisher = {{arXiv}},
	author = {Raju, Kavitha and V, Anjaly and Lish, Ryan and Mathew, Joel},
	urldate = {2024-05-29},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2206.01205 [cs, eess]},
}

@misc{doumbouyaUsingRadioArchives2021,
	title = {Using Radio Archives for Low-Resource Speech Recognition: Towards an Intelligent Virtual Assistant for Illiterate Users},
	url = {http://arxiv.org/abs/2104.13083},
	shorttitle = {Using Radio Archives for Low-Resource Speech Recognition},
	abstract = {For many of the 700 million illiterate people around the world, speech recognition technology could provide a bridge to valuable information and services. Yet, those most in need of this technology are often the most underserved by it. In many countries, illiterate people tend to speak only low-resource languages, for which the datasets necessary for speech technology development are scarce. In this paper, we investigate the effectiveness of unsupervised speech representation learning on noisy radio broadcasting archives, which are abundant even in low-resource languages. We make three core contributions. First, we release two datasets to the research community. The first, West African Radio Corpus, contains 142 hours of audio in more than 10 languages with a labeled validation subset. The second, West African Virtual Assistant Speech Recognition Corpus, consists of 10K labeled audio clips in four languages. Next, we share West African wav2vec, a speech encoder trained on the noisy radio corpus, and compare it with the baseline Facebook speech encoder trained on six times more data of higher quality. We show that West African wav2vec performs similarly to the baseline on a multilingual speech recognition task, and significantly outperforms the baseline on a West African language identification task. Finally, we share the first-ever speech recognition models for Maninka, Pular and Susu, languages spoken by a combined 10 million people in over seven countries, including six where the majority of the adult population is illiterate. Our contributions offer a path forward for ethical {AI} research to serve the needs of those most disadvantaged by the digital divide.},
	number = {{arXiv}:2104.13083},
	publisher = {{arXiv}},
	author = {Doumbouya, Moussa and Einstein, Lisa and Piech, Chris},
	urldate = {2024-05-29},
	date = {2021-04-27},
	eprinttype = {arxiv},
	eprint = {2104.13083 [cs]},
}

@misc{xuBaizeOpenSourceChat2023,
	title = {Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
	url = {http://arxiv.org/abs/2304.01196},
	shorttitle = {Baize},
	abstract = {Chat models, such as {ChatGPT}, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted {API}, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging {ChatGPT} to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance {LLaMA}, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from {ChatGPT}. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. An online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize.},
	number = {{arXiv}:2304.01196},
	publisher = {{arXiv}},
	author = {Xu, Canwen and Guo, Daya and Duan, Nan and {McAuley}, Julian},
	urldate = {2024-05-29},
	date = {2023-12-02},
	eprinttype = {arxiv},
	eprint = {2304.01196 [cs]},
}

@misc{kryscinskiBookSumCollectionDatasets2022,
	title = {{BookSum}: A Collection of Datasets for Long-form Narrative Summarization},
	url = {http://arxiv.org/abs/2105.08209},
	shorttitle = {{BookSum}},
	abstract = {The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future generations of text summarization systems. We address these issues by introducing {BookSum}, a collection of datasets for long-form narrative summarization. Our dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset.},
	number = {{arXiv}:2105.08209},
	publisher = {{arXiv}},
	author = {Kryściński, Wojciech and Rajani, Nazneen and Agarwal, Divyansh and Xiong, Caiming and Radev, Dragomir},
	urldate = {2024-05-29},
	date = {2022-12-06},
	eprinttype = {arxiv},
	eprint = {2105.08209 [cs]},
	note = {version: 2},
}

@misc{liCAMELCommunicativeAgents2023,
	title = {{CAMEL}: Communicative Agents for "Mind" Exploration of Large Language Model Society},
	url = {http://arxiv.org/abs/2303.17760},
	shorttitle = {{CAMEL}},
	abstract = {The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.},
	number = {{arXiv}:2303.17760},
	publisher = {{arXiv}},
	author = {Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
	urldate = {2024-05-29},
	date = {2023-11-02},
	eprinttype = {arxiv},
	eprint = {2303.17760 [cs]},
}

@misc{kimCoTCollectionImproving2023,
	title = {The {CoT} Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning},
	url = {http://arxiv.org/abs/2305.14045},
	shorttitle = {The {CoT} Collection},
	abstract = {Language models ({LMs}) with less than 100B parameters are known to perform poorly on chain-of-thought ({CoT}) reasoning in contrast to large {LMs} when solving unseen tasks. In this work, we aim to equip smaller {LMs} with the step-by-step reasoning capability by instruction tuning with {CoT} rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the {CoT} Collection, which augments the existing Flan Collection (including only 9 {CoT} tasks) with additional 1.84 million rationales across 1,060 tasks. We show that {CoT} fine-tuning Flan-T5 (3B \& 11B) with {CoT} Collection enables smaller {LMs} to have better {CoT} capabilities on unseen tasks. On the {BIG}-Bench-Hard ({BBH}) benchmark, we report an average improvement of +4.34\% (Flan-T5 3B) and +2.60\% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with {CoT} Collection allows {LMs} to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24\% (Flan-T5 3B) and +2.37\% (Flan-T5 11B), even outperforming {ChatGPT} utilizing demonstrations until the max length by a +13.98\% margin. Our code, the {CoT} Collection data, and model checkpoints are publicly available.},
	number = {{arXiv}:2305.14045},
	publisher = {{arXiv}},
	author = {Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},
	urldate = {2024-05-29},
	date = {2023-10-14},
	eprinttype = {arxiv},
	eprint = {2305.14045 [cs]},
}

@misc{muennighoffOctoPackInstructionTuning2024,
	title = {{OctoPack}: Instruction Tuning Code Large Language Models},
	url = {http://arxiv.org/abs/2308.07124},
	shorttitle = {{OctoPack}},
	abstract = {Finetuning large language models ({LLMs}) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile {CommitPack}: 4 terabytes of Git commits across 350 programming languages. We benchmark {CommitPack} against other natural and synthetic code instructions ({xP}3x, Self-Instruct, {OASST}) on the 16B parameter {StarCoder} model, and achieve state-of-the-art performance among models not trained on {OpenAI} outputs, on the {HumanEval} Python benchmark (46.2\% pass@1). We further introduce {HumanEvalPack}, expanding the {HumanEval} benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, {JavaScript}, Java, Go, C++, Rust). Our models, {OctoCoder} and {OctoGeeX}, achieve the best performance across {HumanEvalPack} among all permissive models, demonstrating {CommitPack}'s benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.},
	number = {{arXiv}:2308.07124},
	publisher = {{arXiv}},
	author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and von Werra, Leandro and Longpre, Shayne},
	urldate = {2024-05-29},
	date = {2024-02-18},
	eprinttype = {arxiv},
	eprint = {2308.07124 [cs]},
}

@misc{pengInstructionTuningGPT42023,
	title = {Instruction Tuning with {GPT}-4},
	url = {http://arxiv.org/abs/2304.03277},
	abstract = {Prior work has shown that finetuning large language models ({LLMs}) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use {GPT}-4 to generate instruction-following data for {LLM} finetuning. Our early experiments on instruction-tuned {LLaMA} models show that the 52K English and Chinese instruction-following data generated by {GPT}-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from {GPT}-4 to enable a comprehensive evaluation and reward model training. We make our data generated using {GPT}-4 as well as our codebase publicly available.},
	number = {{arXiv}:2304.03277},
	publisher = {{arXiv}},
	author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	urldate = {2024-05-29},
	date = {2023-04-06},
	eprinttype = {arxiv},
	eprint = {2304.03277 [cs]},
}

@misc{patilGorillaLargeLanguage2023,
	title = {Gorilla: Large Language Model Connected with Massive {APIs}},
	url = {http://arxiv.org/abs/2305.15334},
	shorttitle = {Gorilla},
	abstract = {Large Language Models ({LLMs}) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via {API} calls remains unfulfilled. This is a challenging task even for today's state-of-the-art {LLMs} such as {GPT}-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an {API} call. We release Gorilla, a finetuned {LLaMA}-based model that surpasses the performance of {GPT}-4 on writing {API} calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting {LLMs} directly. To evaluate the model's ability, we introduce {APIBench}, a comprehensive dataset consisting of {HuggingFace}, {TorchHub}, and {TensorHub} {APIs}. The successful integration of the retrieval system with Gorilla demonstrates the potential for {LLMs} to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
	number = {{arXiv}:2305.15334},
	publisher = {{arXiv}},
	author = {Patil, Shishir G. and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E.},
	urldate = {2024-05-29},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15334 [cs]},
}

@misc{guoHowCloseChatGPT2023,
	title = {How Close is {ChatGPT} to Human Experts? Comparison Corpus, Evaluation, and Detection},
	url = {http://arxiv.org/abs/2301.07597},
	shorttitle = {How Close is {ChatGPT} to Human Experts?},
	abstract = {The introduction of {ChatGPT} has garnered widespread attention in both academic and industrial communities. {ChatGPT} is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how {ChatGPT} is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models ({LLMs}) like {ChatGPT} could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and {ChatGPT}, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human {ChatGPT} Comparison Corpus ({HC}3). Based on the {HC}3 dataset, we study the characteristics of {ChatGPT}'s responses, the differences and gaps from human experts, and future directions for {LLMs}. We conducted comprehensive human evaluations and linguistic analyses of {ChatGPT}-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by {ChatGPT} or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-{SimpleAI}/chatgpt-comparison-detection.},
	number = {{arXiv}:2301.07597},
	publisher = {{arXiv}},
	author = {Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},
	urldate = {2024-05-29},
	date = {2023-01-18},
	eprinttype = {arxiv},
	eprint = {2301.07597 [cs]},
}

@misc{zhouLIMALessMore2023,
	title = {{LIMA}: Less Is More for Alignment},
	url = {http://arxiv.org/abs/2305.11206},
	shorttitle = {{LIMA}},
	abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training {LIMA}, a 65B parameter {LLaMa} language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. {LIMA} demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from {LIMA} are either equivalent or strictly preferred to {GPT}-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus {DaVinci}003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
	number = {{arXiv}:2305.11206},
	publisher = {{arXiv}},
	author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
	urldate = {2024-05-29},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2305.11206 [cs]},
}

@misc{koksalLongFormEffectiveInstruction2024,
	title = {{LongForm}: Effective Instruction Tuning with Reverse Instructions},
	url = {http://arxiv.org/abs/2304.08460},
	shorttitle = {{LongForm}},
	abstract = {Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via {LLMs}. We introduce the {LongForm}-C dataset, which is created by reverse instructions. We generate instructions via {LLMs} for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via {LLMs}. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover, {LongForm} models outperform prior instruction-tuned models such as {FLAN}-T5 and Alpaca by a large margin, and improve language understanding capabilities further. Finally, our models can effectively follow and answer multilingual instructions; we demonstrate this for news generation. We publicly release our data and models: https://github.com/akoksal/{LongForm}.},
	number = {{arXiv}:2304.08460},
	publisher = {{arXiv}},
	author = {Köksal, Abdullatif and Schick, Timo and Korhonen, Anna and Schütze, Hinrich},
	urldate = {2024-05-29},
	date = {2024-02-14},
	eprinttype = {arxiv},
	eprint = {2304.08460 [cs]},
}

@misc{mukherjeeOrcaProgressiveLearning2023,
	title = {Orca: Progressive Learning from Complex Explanation Traces of {GPT}-4},
	url = {http://arxiv.org/abs/2306.02707},
	shorttitle = {Orca},
	abstract = {Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models ({LFMs}). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow {LFM} outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of {LFMs}. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with {LLaMA}'s release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of {LFMs}. Orca learns from rich signals from {GPT}-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from {ChatGPT}. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100\% in complex zero-shot reasoning benchmarks like Big-Bench Hard ({BBH}) and 42\% on {AGIEval}. Moreover, Orca reaches parity with {ChatGPT} on the {BBH} benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the {SAT}, {LSAT}, {GRE}, and {GMAT}, both in zero-shot settings without {CoT}; while trailing behind {GPT}-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced {AI} models, is a promising direction to improve model capabilities and skills.},
	number = {{arXiv}:2306.02707},
	publisher = {{arXiv}},
	author = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
	urldate = {2024-05-29},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.02707 [cs]},
}

@misc{stiennonLearningSummarizeHuman2022,
	title = {Learning to summarize from human feedback},
	url = {http://arxiv.org/abs/2009.01325},
	abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using {ROUGE}, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the {TL};{DR} dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to {CNN}/{DM} news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing {ROUGE} according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
	number = {{arXiv}:2009.01325},
	publisher = {{arXiv}},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
	urldate = {2024-05-29},
	date = {2022-02-15},
	eprinttype = {arxiv},
	eprint = {2009.01325 [cs]},
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022,
	title = {{WebGPT}: Browser-assisted question-answering with human feedback},
	url = {http://arxiv.org/abs/2112.09332},
	shorttitle = {{WebGPT}},
	abstract = {We fine-tune {GPT}-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on {ELI}5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning {GPT}-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
	number = {{arXiv}:2112.09332},
	publisher = {{arXiv}},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	urldate = {2024-05-29},
	date = {2022-06-01},
	eprinttype = {arxiv},
	eprint = {2112.09332 [cs]},
}

@misc{ethayarajhUnderstandingDatasetDifficulty2022,
	title = {Understanding Dataset Difficulty with \${\textbackslash}mathcal\{V\}\$-Usable Information},
	url = {http://arxiv.org/abs/2110.08420},
	abstract = {Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model \${\textbackslash}mathcal\{V\}\$ -- as the lack of \${\textbackslash}mathcal\{V\}\$-\${\textbackslash}textit\{usable information\}\$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for \${\textbackslash}mathcal\{V\}\$. We further introduce \${\textbackslash}textit\{pointwise \${\textbackslash}mathcal\{V\}\$-information\}\$ ({PVI}) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, \${\textbackslash}mathcal\{V\}\$-\${\textbackslash}textit\{usable information\}\$ and {PVI} also permit the converse: for a given model \${\textbackslash}mathcal\{V\}\$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used {NLP} benchmarks.},
	number = {{arXiv}:2110.08420},
	publisher = {{arXiv}},
	author = {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
	urldate = {2024-05-29},
	date = {2022-06-14},
	eprinttype = {arxiv},
	eprint = {2110.08420 [cs]},
}

@misc{liStarCoderMaySource2023,
	title = {{StarCoder}: may the source be with you!},
	url = {http://arxiv.org/abs/2305.06161},
	shorttitle = {{StarCoder}},
	abstract = {The {BigCode} community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code {LLMs}), introduces {StarCoder} and {StarCoderBase}: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. {StarCoderBase} is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed {GitHub} repositories with inspection tools and an opt-out process. We fine-tuned {StarCoderBase} on 35B Python tokens, resulting in the creation of {StarCoder}. We perform the most comprehensive evaluation of Code {LLMs} to date and show that {StarCoderBase} outperforms every open Code {LLM} that supports multiple programming languages and matches or outperforms the {OpenAI} code-cushman-001 model. Furthermore, {StarCoder} outperforms every model that is fine-tuned on Python, can be prompted to achieve 40{\textbackslash}\% pass@1 on {HumanEval}, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved {PII} redaction pipeline and a novel attribution tracing tool, and make the {StarCoder} models publicly available under a more commercially viable version of the Open Responsible {AI} Model license.},
	number = {{arXiv}:2305.06161},
	publisher = {{arXiv}},
	author = {Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and Liu, Qian and Zheltonozhskii, Evgenii and Zhuo, Terry Yue and Wang, Thomas and Dehaene, Olivier and Davaadorj, Mishig and Lamy-Poirier, Joel and Monteiro, João and Shliazhko, Oleh and Gontier, Nicolas and Meade, Nicholas and Zebaze, Armel and Yee, Ming-Ho and Umapathi, Logesh Kumar and Zhu, Jian and Lipkin, Benjamin and Oblokulov, Muhtasham and Wang, Zhiruo and Murthy, Rudra and Stillerman, Jason and Patel, Siva Sankalp and Abulkhanov, Dmitry and Zocca, Marco and Dey, Manan and Zhang, Zhihan and Fahmy, Nour and Bhattacharyya, Urvashi and Yu, Wenhao and Singh, Swayam and Luccioni, Sasha and Villegas, Paulo and Kunakov, Maxim and Zhdanov, Fedor and Romero, Manuel and Lee, Tony and Timor, Nadav and Ding, Jennifer and Schlesinger, Claire and Schoelkopf, Hailey and Ebert, Jan and Dao, Tri and Mishra, Mayank and Gu, Alex and Robinson, Jennifer and Anderson, Carolyn Jane and Dolan-Gavitt, Brendan and Contractor, Danish and Reddy, Siva and Fried, Daniel and Bahdanau, Dzmitry and Jernite, Yacine and Ferrandis, Carlos Muñoz and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm},
	urldate = {2024-05-29},
	date = {2023-12-13},
	eprinttype = {arxiv},
	eprint = {2305.06161 [cs]},
}

@misc{eldanTinyStoriesHowSmall2023,
	title = {{TinyStories}: How Small Can Language Models Be and Still Speak Coherent English?},
	url = {http://arxiv.org/abs/2305.07759},
	shorttitle = {{TinyStories}},
	abstract = {Language models ({LMs}) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as {GPT}-Neo (small) or {GPT}-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce {TinyStories}, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by {GPT}-3.5 and {GPT}-4. We show that {TinyStories} can be used to train and evaluate {LMs} that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses {GPT}-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that {TinyStories} can facilitate the development, analysis and research of {LMs}, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in {LMs}.},
	number = {{arXiv}:2305.07759},
	publisher = {{arXiv}},
	author = {Eldan, Ronen and Li, Yuanzhi},
	urldate = {2024-05-29},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.07759 [cs]},
}

@misc{qinToolLLMFacilitatingLarge2023,
	title = {{ToolLLM}: Facilitating Large Language Models to Master 16000+ Real-world {APIs}},
	url = {http://arxiv.org/abs/2307.16789},
	shorttitle = {{ToolLLM}},
	abstract = {Despite the advancements of open-source large language models ({LLMs}), e.g., {LLaMA}, they remain significantly limited in tool-use capabilities, i.e., using external tools ({APIs}) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art ({SOTA}) closed-source {LLMs}, e.g., {ChatGPT}. To bridge this gap, we introduce {ToolLLM}, a general tool-use framework encompassing data construction, model training, and evaluation. We first present {ToolBench}, an instruction-tuning dataset for tool use, which is constructed automatically using {ChatGPT}. Specifically, the construction can be divided into three stages: (i) {API} collection: we collect 16,464 real-world {RESTful} {APIs} spanning 49 categories from {RapidAPI} Hub; (ii) instruction generation: we prompt {ChatGPT} to generate diverse instructions involving these {APIs}, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use {ChatGPT} to search for a valid solution path (chain of {API} calls) for each instruction. To enhance the reasoning capabilities of {LLMs}, we develop a novel depth-first search-based decision tree algorithm. It enables {LLMs} to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of {LLMs}, we develop an automatic evaluator: {ToolEval}. Based on {ToolBench}, we fine-tune {LLaMA} to obtain an {LLM} {ToolLLaMA}, and equip it with a neural {API} retriever to recommend appropriate {APIs} for each instruction. Experiments show that {ToolLLaMA} demonstrates a remarkable ability to execute complex instructions and generalize to unseen {APIs}, and exhibits comparable performance to {ChatGPT}. Our {ToolLLaMA} also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: {APIBench}.},
	number = {{arXiv}:2307.16789},
	publisher = {{arXiv}},
	author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	urldate = {2024-05-29},
	date = {2023-10-03},
	eprinttype = {arxiv},
	eprint = {2307.16789 [cs]},
}

@misc{honovichUnnaturalInstructionsTuning2022,
	title = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
	url = {http://arxiv.org/abs/2212.09689},
	shorttitle = {Unnatural Instructions},
	abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
	number = {{arXiv}:2212.09689},
	publisher = {{arXiv}},
	author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
	urldate = {2024-05-29},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2212.09689 [cs]},
}

@misc{xuWizardLMEmpoweringLarge2023,
	title = {{WizardLM}: Empowering Large Language Models to Follow Complex Instructions},
	url = {http://arxiv.org/abs/2304.12244},
	shorttitle = {{WizardLM}},
	abstract = {Training large language models ({LLMs}) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using {LLM} instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune {LLaMA}. We call the resulting model {WizardLM}. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our {WizardLM} are preferred to outputs from {OpenAI} {ChatGPT}. In {GPT}-4 automatic evaluation, {WizardLM} achieves more than 90{\textbackslash}\% capacity of {ChatGPT} on 17 out of 29 skills. Even though {WizardLM} still lags behind {ChatGPT} in some aspects, our findings suggest that fine-tuning with {AI}-evolved instructions is a promising direction for enhancing {LLMs}. Our code and data are public at https://github.com/nlpxucan/{WizardLM}},
	number = {{arXiv}:2304.12244},
	publisher = {{arXiv}},
	author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
	urldate = {2024-05-29},
	date = {2023-06-10},
	eprinttype = {arxiv},
	eprint = {2304.12244 [cs]},
}

@misc{baroneParallelCorpusPython2017,
	title = {A parallel corpus of Python functions and documentation strings for automated code documentation and code generation},
	url = {http://arxiv.org/abs/1707.02275},
	abstract = {Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains. In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings ("docstrings") generated by scraping open source repositories on {GitHub}. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with data augmentation techniques to further increase the amount of training data. We release our datasets and processing scripts in order to stimulate research in these areas.},
	number = {{arXiv}:1707.02275},
	publisher = {{arXiv}},
	author = {Barone, Antonio Valerio Miceli and Sennrich, Rico},
	urldate = {2024-05-29},
	date = {2017-07-07},
	eprinttype = {arxiv},
	eprint = {1707.02275 [cs]},
}

@misc{yangPAWSXCrosslingualAdversarial2019,
	title = {{PAWS}-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification},
	url = {http://arxiv.org/abs/1908.11828},
	shorttitle = {{PAWS}-X},
	abstract = {Most existing work on adversarial data generation focuses on English. For example, {PAWS} (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with {PAWS}-X, a new dataset of 23,659 human translated {PAWS} evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual {BERT} fine-tuned on {PAWS} English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23\% over the next best model. {PAWS}-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.},
	number = {{arXiv}:1908.11828},
	publisher = {{arXiv}},
	author = {Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},
	urldate = {2024-05-29},
	date = {2019-08-30},
	eprinttype = {arxiv},
	eprint = {1908.11828 [cs]},
}

@misc{lewisMLQAEvaluatingCrosslingual2020,
	title = {{MLQA}: Evaluating Cross-lingual Extractive Question Answering},
	url = {http://arxiv.org/abs/1910.07475},
	shorttitle = {{MLQA}},
	abstract = {Question answering ({QA}) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making training {QA} systems in other languages challenging. An alternative to building large monolingual training datasets is to develop cross-lingual systems which can transfer to a target language without requiring training data in that language. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present {MLQA}, a multi-way aligned extractive {QA} evaluation benchmark intended to spur research in this area. {MLQA} contains {QA} instances in 7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. It consists of over 12K {QA} instances in English and 5K in each other language, with each {QA} instance being parallel between 4 languages on average. {MLQA} is built using a novel alignment context strategy on Wikipedia articles, and serves as a cross-lingual extension to existing extractive {QA} datasets. We evaluate current state-of-the-art cross-lingual representations on {MLQA}, and also provide machine-translation-based baselines. In all cases, transfer results are shown to be significantly behind training-language performance.},
	number = {{arXiv}:1910.07475},
	publisher = {{arXiv}},
	author = {Lewis, Patrick and Oğuz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
	urldate = {2024-05-29},
	date = {2020-05-03},
	eprinttype = {arxiv},
	eprint = {1910.07475 [cs]},
}

@misc{clarkTyDiQABenchmark2020,
	title = {{TyDi} {QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
	url = {http://arxiv.org/abs/2003.05002},
	shorttitle = {{TyDi} {QA}},
	abstract = {Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present {TyDi} {QA}---a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of {TyDi} {QA} are diverse with regard to their typology---the set of linguistic features each language expresses---such that we expect models performing well on this set to generalize across a large number of the world's languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don't know the answer yet, and the data is collected directly in each language without the use of translation.},
	number = {{arXiv}:2003.05002},
	publisher = {{arXiv}},
	author = {Clark, Jonathan H. and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and Nikolaev, Vitaly and Palomaki, Jennimaria},
	urldate = {2024-05-29},
	date = {2020-03-10},
	eprinttype = {arxiv},
	eprint = {2003.05002 [cs]},
}

@misc{xuCLUEChineseLanguage2020,
	title = {{CLUE}: A Chinese Language Understanding Evaluation Benchmark},
	url = {http://arxiv.org/abs/2004.05986},
	shorttitle = {{CLUE}},
	abstract = {The advent of natural language understanding ({NLU}) benchmarks for English, such as {GLUE} and {SuperGLUE} allows new {NLU} models to be evaluated across a diverse set of tasks. These comprehensive benchmarks have facilitated a broad range of research and applications in natural language processing ({NLP}). The problem, however, is that most such benchmarks are limited to English, which has made it difficult to replicate many of the successes in English {NLU} for other languages. To help remedy this issue, we introduce the first large-scale Chinese Language Understanding Evaluation ({CLUE}) benchmark. {CLUE} is an open-ended, community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text. To establish results on these tasks, we report scores using an exhaustive set of current state-of-the-art pre-trained Chinese models (9 in total). We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on Chinese {NLU}. Our benchmark is released at https://www.{CLUEbenchmarks}.com},
	number = {{arXiv}:2004.05986},
	publisher = {{arXiv}},
	author = {Xu, Liang and Hu, Hai and Zhang, Xuanwei and Li, Lu and Cao, Chenjie and Li, Yudong and Xu, Yechen and Sun, Kai and Yu, Dian and Yu, Cong and Tian, Yin and Dong, Qianqian and Liu, Weitang and Shi, Bo and Cui, Yiming and Li, Junyi and Zeng, Jun and Wang, Rongzhao and Xie, Weijian and Li, Yanting and Patterson, Yina and Tian, Zuoyu and Zhang, Yiwen and Zhou, He and Liu, Shaoweihua and Zhao, Zhe and Zhao, Qipeng and Yue, Cong and Zhang, Xinrui and Yang, Zhengliang and Richardson, Kyle and Lan, Zhenzhong},
	urldate = {2024-05-29},
	date = {2020-11-05},
	eprinttype = {arxiv},
	eprint = {2004.05986 [cs]},
}

@misc{ladhakWikiLinguaNewBenchmark2020,
	title = {{WikiLingua}: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization},
	url = {http://arxiv.org/abs/2010.03093},
	shorttitle = {{WikiLingua}},
	abstract = {We introduce {WikiLingua}, a large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. We extract article and summary pairs in 18 languages from {WikiHow}, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct crosslingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference.},
	number = {{arXiv}:2010.03093},
	publisher = {{arXiv}},
	author = {Ladhak, Faisal and Durmus, Esin and Cardie, Claire and {McKeown}, Kathleen},
	urldate = {2024-05-29},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.03093 [cs]},
}

@misc{tiedemannTatoebaTranslationChallenge2020,
	title = {The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual {MT}},
	url = {http://arxiv.org/abs/2010.06354},
	abstract = {This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World's languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups.},
	number = {{arXiv}:2010.06354},
	publisher = {{arXiv}},
	author = {Tiedemann, Jörg},
	urldate = {2024-05-29},
	date = {2020-10-13},
	eprinttype = {arxiv},
	eprint = {2010.06354 [cs]},
}

@misc{raganatoXLWiCMultilingualBenchmark2020,
	title = {{XL}-{WiC}: A Multilingual Benchmark for Evaluating Semantic Contextualization},
	url = {http://arxiv.org/abs/2010.06478},
	shorttitle = {{XL}-{WiC}},
	abstract = {The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually {WordNet}), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset ({WiC}) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, {XL}-{WiC}, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. {XL}-{WiC} is available at https://pilehvar.github.io/xlwic/.},
	number = {{arXiv}:2010.06478},
	publisher = {{arXiv}},
	author = {Raganato, Alessandro and Pasini, Tommaso and Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
	urldate = {2024-05-29},
	date = {2020-10-13},
	eprinttype = {arxiv},
	eprint = {2010.06478 [cs]},
}

@misc{hendrycksMeasuringCodingChallenge2021,
	title = {Measuring Coding Challenge Competence With {APPS}},
	url = {http://arxiv.org/abs/2105.09938},
	abstract = {While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce {APPS}, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both {GitHub} and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as {GPT}-Neo can pass approximately 20\% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.},
	number = {{arXiv}:2105.09938},
	publisher = {{arXiv}},
	author = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
	urldate = {2024-05-29},
	date = {2021-11-08},
	eprinttype = {arxiv},
	eprint = {2105.09938 [cs]},
}

@misc{goyalFLORES101EvaluationBenchmark2021,
	title = {The {FLORES}-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
	url = {http://arxiv.org/abs/2106.03193},
	abstract = {One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the {FLORES}-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.},
	number = {{arXiv}:2106.03193},
	publisher = {{arXiv}},
	author = {Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzman, Francisco and Fan, Angela},
	urldate = {2024-05-29},
	date = {2021-06-06},
	eprinttype = {arxiv},
	eprint = {2106.03193 [cs]},
}

@misc{hasanXLSumLargeScaleMultilingual2021,
	title = {{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages},
	url = {http://arxiv.org/abs/2106.13822},
	shorttitle = {{XL}-Sum},
	abstract = {Contemporary works on abstractive text summarization have focused primarily on high-resource languages like English, mostly due to the limited availability of datasets for low/mid-resource ones. In this work, we present {XL}-Sum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from {BBC}, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. {XL}-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune {mT}5, a state-of-the-art pretrained multilingual model, with {XL}-Sum and experiment on multilingual and low-resource summarization tasks. {XL}-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 {ROUGE}-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, {XL}-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at {\textbackslash}url\{https://github.com/csebuetnlp/xl-sum\}.},
	number = {{arXiv}:2106.13822},
	publisher = {{arXiv}},
	author = {Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md Saiful and Samin, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M. Sohel and Shahriyar, Rifat},
	urldate = {2024-05-29},
	date = {2021-06-25},
	eprinttype = {arxiv},
	eprint = {2106.13822 [cs]},
}

@misc{chalkidisMultiEURLEXMultilingualMultilabel2021,
	title = {{MultiEURLEX} -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer},
	url = {http://arxiv.org/abs/2109.00904},
	abstract = {We introduce {MULTI}-{EURLEX}, a new multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union ({EU}) laws, officially translated in 23 languages, annotated with multiple labels from the {EUROVOC} taxonomy. We highlight the effect of temporal concept drift and the importance of chronological, instead of random splits. We use the dataset as a testbed for zero-shot cross-lingual transfer, where we exploit annotated training documents in one language (source) to classify documents in another language (target). We find that fine-tuning a multilingually pretrained model ({XLM}-{ROBERTA}, {MT}5) in a single source language leads to catastrophic forgetting of multilingual knowledge and, consequently, poor zero-shot transfer to other languages. Adaptation strategies, namely partial fine-tuning, adapters, {BITFIT}, {LNFIT}, originally proposed to accelerate fine-tuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially improving zero-shot cross-lingual transfer, but their impact also depends on the pretrained model used and the size of the label set.},
	number = {{arXiv}:2109.00904},
	publisher = {{arXiv}},
	author = {Chalkidis, Ilias and Fergadiotis, Manos and Androutsopoulos, Ion},
	urldate = {2024-05-29},
	date = {2021-09-06},
	eprinttype = {arxiv},
	eprint = {2109.00904 [cs]},
}

@misc{kimBiSECTLearningSplit2021,
	title = {{BiSECT}: Learning to Split and Rephrase Sentences with Bitexts},
	url = {http://arxiv.org/abs/2109.05006},
	shorttitle = {{BiSECT}},
	abstract = {An important task in {NLP} applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this `split and rephrase' task. Our {BiSECT} training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language. {BiSECT} contains higher quality training examples than previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our corpus, and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on {BiSECT} can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.},
	number = {{arXiv}:2109.05006},
	publisher = {{arXiv}},
	author = {Kim, Joongwon and Maddela, Mounica and Kriz, Reno and Xu, Wei and Callison-Burch, Chris},
	urldate = {2024-05-29},
	date = {2021-09-10},
	eprinttype = {arxiv},
	eprint = {2109.05006 [cs]},
}

@misc{sangIntroductionCoNLL2000Shared2000,
	title = {Introduction to the {CoNLL}-2000 Shared Task: Chunking},
	url = {http://arxiv.org/abs/cs/0009008},
	shorttitle = {Introduction to the {CoNLL}-2000 Shared Task},
	abstract = {We describe the {CoNLL}-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.},
	number = {{arXiv}:cs/0009008},
	publisher = {{arXiv}},
	author = {Sang, Erik F. Tjong Kim and Buchholz, Sabine},
	urldate = {2024-05-29},
	date = {2000-09-18},
	eprinttype = {arxiv},
	eprint = {cs/0009008},
}

@misc{sangIntroductionCoNLL2002Shared2002,
	title = {Introduction to the {CoNLL}-2002 Shared Task: Language-Independent Named Entity Recognition},
	url = {http://arxiv.org/abs/cs/0209010},
	shorttitle = {Introduction to the {CoNLL}-2002 Shared Task},
	abstract = {We describe the {CoNLL}-2002 shared task: language-independent named entity recognition. We give background information on the data sets and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
	number = {{arXiv}:cs/0209010},
	publisher = {{arXiv}},
	author = {Sang, Erik F. Tjong Kim},
	urldate = {2024-05-29},
	date = {2002-09-05},
	eprinttype = {arxiv},
	eprint = {cs/0209010},
}

@misc{sangIntroductionCoNLL2003Shared2003,
	title = {Introduction to the {CoNLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
	url = {http://arxiv.org/abs/cs/0306050},
	shorttitle = {Introduction to the {CoNLL}-2003 Shared Task},
	abstract = {We describe the {CoNLL}-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
	number = {{arXiv}:cs/0306050},
	publisher = {{arXiv}},
	author = {Sang, Erik F. Tjong Kim and De Meulder, Fien},
	urldate = {2024-05-29},
	date = {2003-06-12},
	eprinttype = {arxiv},
	eprint = {cs/0306050},
}

@misc{duAISHELL2TransformingMandarin2018,
	title = {{AISHELL}-2: Transforming Mandarin {ASR} Research Into Industrial Scale},
	url = {http://arxiv.org/abs/1808.10583},
	shorttitle = {{AISHELL}-2},
	abstract = {{AISHELL}-1 is by far the largest open-source speech corpus available for Mandarin speech recognition research. It was released with a baseline system containing solid training and testing pipelines for Mandarin {ASR}. In {AISHELL}-2, 1000 hours of clean read-speech data from {iOS} is published, which is free for academic usage. On top of {AISHELL}-2 corpus, an improved recipe is developed and released, containing key components for industrial applications, such as Chinese word segmentation, flexible vocabulary expension and phone set transformation etc. Pipelines support various state-of-the-art techniques, such as time-delayed neural networks and Lattic-Free {MMI} objective funciton. In addition, we also release dev and test data from other channels(Android and Mic). For research community, we hope that {AISHELL}-2 corpus can be a solid resource for topics like transfer learning and robust {ASR}. For industry, we hope {AISHELL}-2 recipe can be a helpful reference for building meaningful industrial systems and products.},
	number = {{arXiv}:1808.10583},
	publisher = {{arXiv}},
	author = {Du, Jiayu and Na, Xingyu and Liu, Xuechen and Bu, Hui},
	urldate = {2024-05-31},
	date = {2018-09-12},
	eprinttype = {arxiv},
	eprint = {1808.10583 [cs]},
}

@inproceedings{maekawaCorpusSpontaneousJapanese2003,
	title = {Corpus of spontaneous Japanese: its design and evaluation},
	url = {https://www.isca-archive.org/sspr_2003/maekawa03_sspr.html},
	shorttitle = {Corpus of spontaneous Japanese},
	pages = {paper MMO2},
	booktitle = {Proceedings of the {ISCA}/{IEEE} Workshop on Spontaneous Speech Processing and Recognition},
	author = {Maekawa, Kikuo},
	urldate = {2024-05-29},
	date = {2003},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{coatsCorpusRegionalAmerican2019,
	title = {A Corpus of Regional American Language from {YouTube}},
	url = {https://www.semanticscholar.org/paper/A-Corpus-of-Regional-American-Language-from-YouTube-Coats/bc428db824d261794a7e081a53c4315b8e02f855},
	abstract = {Recent years have seen an increase in the number of corpora of regional language variation for English, allowing new types of aggregate analysis to be conducted. While the creation of a corpus from written language material is relatively straightforward, transcribing speech is time-consuming, and thus there are no large corpora of transcribed American speech with broad geographic coverage. This paper describes the creation of a new corpus of regional American English from the automatically generated captions of videos from {YouTube} channels with a local American focus – mainly channels of regional and local government entities or civic organizations. The corpus, which consists of transcripts of over 29,267 hours of spoken language, will enable the analysis of regional patterns of lexical, morphosyntactic, and other types of variation in spoken American English. Exploratory analysis and mapping of the corpus data indicates regional variation in spoken language is evident.},
	booktitle = {Proceedings of the Digital Humanities in the Nordic Countries 5th Conference},
	author = {Coats, Steven},
	urldate = {2024-05-29},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@misc{yinReazonSpeechFreeMassive2023,
	title = {{ReazonSpeech}: A Free and Massive Corpus for Japanese {ASR}},
	url = {https://research.reazon.jp/_static/reazonspeech_nlp2023.pdf},
	abstract = {{ReazonSpeech} is a 15,000-hour and continuously growing corpus collected from Japanese {TV} shows free for commercial usage. The automatic speech recognition ({ASR}) model trained on {ReazonSpeech} achieves state-of-the-art results with 8.23\% character error rate ({CER}) on {JSUT} basic5000[1] and 9.93\% on Common Voice[2] v8.0 test set, on par with the recently released Whisper[3] largev2 model. We released the dataset creation toolkit under Apache License 2.0 and made both the corpus and the pretrained {ASR} model freely available1）.},
	author = {Yin, Yue and Mori, Daijiro and Fujimoto, Seiji},
	date = {2023},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@incollection{hernandezTEDLIUMTwiceMuch2018,
	title = {{TED}-{LIUM} 3: twice as much data and corpus repartition for experiments on speaker adaptation},
	volume = {11096},
	url = {http://arxiv.org/abs/1805.04699},
	shorttitle = {{TED}-{LIUM} 3},
	abstract = {In this paper, we present {TED}-{LIUM} release 3 corpus dedicated to speech recognition in English, that multiplies by more than two the available data to train acoustic models in comparison with {TED}-{LIUM} 2. We present the recent development on Automatic Speech Recognition ({ASR}) systems in comparison with the two previous releases of the {TED}-{LIUM} Corpus from 2012 and 2014. We demonstrate that, passing from 207 to 452 hours of transcribed speech training data is really more useful for end-to-end {ASR} systems than for {HMM}-based state-of-the-art ones, even if the {HMM}-based {ASR} system still outperforms end-to-end {ASR} system when the size of audio training data is 452 hours, with respectively a Word Error Rate ({WER}) of 6.6\% and 13.7\%. Last, we propose two repartitions of the {TED}-{LIUM} release 3 corpus: the legacy one that is the same as the one existing in release 2, and a new one, calibrated and designed to make experiments on speaker adaptation. Like the two first releases, {TED}-{LIUM} 3 corpus will be freely available for the research community.},
	pages = {198--208},
	booktitle = {Lecture Notes in Computer Science},
	publisher = {Springer, Cham},
	author = {Hernandez, François and Nguyen, Vincent and Ghannay, Sahar and Tomashenko, Natalia and Estève, Yannick},
	urldate = {2024-05-01},
	date = {2018},
	doi = {10.1007/978-3-319-99579-3_21},
	eprinttype = {arxiv},
	eprint = {1805.04699 [cs]},
}

@inproceedings{tapaswiStoryGraphsVisualizingCharacter2014,
	title = {{StoryGraphs}: Visualizing Character Interactions as a Timeline},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.html},
	shorttitle = {{StoryGraphs}},
	pages = {827--834},
	booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Tapaswi, Makarand and Bauml, Martin and Stiefelhagen, Rainer},
	urldate = {2024-05-29},
	date = {2014},
}

@misc{bradlowALLSSTARArchiveL12010,
	title = {{ALLSSTAR}: Archive of L1 and L2 Scripted and Spontaneous Transcripts And Recordings},
	url = {https://speechbox.linguistics.northwestern.edu/#!/?goto=allsstar},
	abstract = {A continuously expanding multi-lingual speech corpus:
* 120+ talkers
* 20+ languages
* simple and complex sentences, paragraphs, spontaneous monologues
* recordings from each talker in both L1 and L2 (English)

Ongoing processing and testing:
* intelligibility testing
* transcription and text-to-audio alignment},
	author = {Bradlow, A.R.},
	urldate = {2024-05-01},
	date = {2010},
}

@incollection{andrusenkoExplorationEndtoEndASR2020,
	title = {Exploration of End-to-End {ASR} for {OpenSTT} -- Russian Open Speech-to-Text Dataset},
	volume = {12335},
	url = {http://arxiv.org/abs/2006.08274},
	abstract = {This paper presents an exploration of end-to-end automatic speech recognition systems ({ASR}) for the largest open-source Russian language data set -- {OpenSTT}. We evaluate different existing end-to-end approaches such as joint {CTC}/Attention, {RNN}-Transducer, and Transformer. All of them are compared with the strong hybrid {ASR} system based on {LF}-{MMI} {TDNN}-F acoustic model. For the three available validation sets (phone calls, {YouTube}, and books), our best end-to-end model achieves word error rate ({WER}) of 34.8\%, 19.1\%, and 18.1\%, respectively. Under the same conditions, the {hybridASR} system demonstrates 33.5\%, 20.9\%, and 18.6\% {WER}.},
	pages = {35--44},
	booktitle = {Lecture Notes in Computer Science},
	publisher = {Springer, Cham},
	author = {Andrusenko, Andrei and Laptev, Aleksandr and Medennikov, Ivan},
	urldate = {2024-05-29},
	date = {2020},
	doi = {10.1007/978-3-030-60276-5_4},
	eprinttype = {arxiv},
	eprint = {2006.08274 [cs, eess]},
}

@inproceedings{dolanAutomaticallyConstructingCorpus2005,
	title = {Automatically Constructing a Corpus of Sentential Paraphrases},
	url = {https://aclanthology.org/I05-5002},
	eventtitle = {{IJCNLP} 2005},
	booktitle = {Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)},
	author = {Dolan, William B. and Brockett, Chris},
	urldate = {2024-05-29},
	date = {2005},
	keywords = {⛔ No {DOI} found},
}

@article{baumannSpokenWikipediaCorpus2019,
	title = {The Spoken Wikipedia Corpus collection: Harvesting, alignment and an application to hyperlistening},
	volume = {53},
	issn = {1574-0218},
	url = {https://doi.org/10.1007/s10579-017-9410-y},
	doi = {10/gq5xdf},
	shorttitle = {The Spoken Wikipedia Corpus collection},
	abstract = {Spoken corpora are important for speech research, but are expensive to create and do not necessarily reflect (read or spontaneous) speech ‘in the wild’. We report on our conversion of the preexisting and freely available Spoken Wikipedia into a speech resource. The Spoken Wikipedia project unites volunteer readers of Wikipedia articles. There are initiatives to create and sustain Spoken Wikipedia versions in many languages and hence the available data grows over time. Thousands of spoken articles are available to users who prefer a spoken over the written version. We turn these semi-structured collections into structured and time-aligned corpora, keeping the exact correspondence with the original hypertext as well as all available metadata. Thus, we make the Spoken Wikipedia accessible for sustainable research. We present our open-source software pipeline that downloads, extracts, normalizes and text–speech aligns the Spoken Wikipedia. Additional language versions can be exploited by adapting configuration files or extending the software if necessary for language peculiarities. We also present and analyze the resulting corpora for German, English, and Dutch, which presently total 1005 h and grow at an estimated 87 h per year. The corpora, together with our software, are available via http://islrn.org/resources/684-927-624-257-3/. As a prototype usage of the time-aligned corpus, we describe an experiment about the preferred modalities for interacting with information-rich read-out hypertext. We find alignments to help improve user experience and factual information access by enabling targeted interaction.},
	pages = {303--329},
	number = {2},
	journaltitle = {Language Resources and Evaluation},
	shortjournal = {Lang Resources \& Evaluation},
	author = {Baumann, Timo and Köhn, Arne and Hennig, Felix},
	urldate = {2024-05-29},
	date = {2019-06-01},
	langid = {english},
}

@inproceedings{shiFreeKazakhSpeech2017,
	location = {Kuala Lumpur},
	title = {A free Kazakh speech database and a speech recognition baseline},
	isbn = {978-1-5386-1542-3},
	url = {http://ieeexplore.ieee.org/document/8282133/},
	doi = {10/gtsqzf},
	eventtitle = {2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	pages = {745--748},
	booktitle = {2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	publisher = {{IEEE}},
	author = {Shi, Ying and Hamdullah, Askar and Tang, Zhiyuan and Wang, Dong and Zheng, Thomas Fang},
	urldate = {2024-05-01},
	date = {2017-12},
}

@inproceedings{roziOpenFreeDatabase2015,
	location = {Shanghai, China},
	title = {An open/free database and Benchmark for Uyghur speaker recognition},
	isbn = {978-1-4673-8279-3},
	url = {http://ieeexplore.ieee.org/document/7357869/},
	doi = {10/grh5rd},
	eventtitle = {2015 International Conference Oriental {COCOSDA} held jointly with 2015 Conference on Asian Spoken Language Research and Evaluation (O-{COCOSDA}/{CASLRE})},
	pages = {81--85},
	booktitle = {2015 International Conference Oriental {COCOSDA} held jointly with 2015 Conference on Asian Spoken Language Research and Evaluation (O-{COCOSDA}/{CASLRE})},
	publisher = {{IEEE}},
	author = {Rozi, Askar and {Dong Wang} and {Zhiyong Zhang} and Zheng, Thomas Fang},
	urldate = {2024-05-01},
	date = {2015-10},
}

@misc{kirkedalFTSpeechDanish2020,
	title = {{FT} Speech: Danish Parliament Speech Corpus},
	url = {http://arxiv.org/abs/2005.12368},
	doi = {10.21437/Interspeech.2020-3164},
	shorttitle = {{FT} Speech},
	abstract = {This paper introduces {FT} Speech, a new speech corpus created from the recorded meetings of the Danish Parliament, otherwise known as the Folketing ({FT}). The corpus contains over 1,800 hours of transcribed speech by a total of 434 speakers. It is significantly larger in duration, vocabulary, and amount of spontaneous speech than the existing public speech corpora for Danish, which are largely limited to read-aloud and dictation data. We outline design considerations, including the preprocessing methods and the alignment procedure. To evaluate the quality of the corpus, we train automatic speech recognition systems on the new resource and compare them to the systems trained on the Danish part of Spr{\textbackslash}r\{a\}kbanken, the largest public {ASR} corpus for Danish to date. Our baseline results show that we achieve a 14.01 {WER} on the new corpus. A combination of {FT} Speech with in-domain language data provides comparable results to models trained specifically on Spr{\textbackslash}r\{a\}kbanken, showing that {FT} Speech transfers well to this data set. Interestingly, our results demonstrate that the opposite is not the case. This shows that {FT} Speech provides a valuable resource for promoting research on Danish {ASR} with more spontaneous speech.},
	author = {Kirkedal, Andreas and Stepanović, Marija and Plank, Barbara},
	urldate = {2024-05-29},
	date = {2020-10-28},
	eprinttype = {arxiv},
	eprint = {2005.12368 [cs, eess]},
}

@inproceedings{kjartanssonCrowdSourcedSpeechCorpora2018,
	title = {Crowd-Sourced Speech Corpora for Javanese, Sundanese, Sinhala, Nepali, and Bangladeshi Bengali},
	url = {https://www.isca-archive.org/sltu_2018/kjartansson18_sltu.html},
	doi = {10/gtwwbs},
	eventtitle = {6th Workshop on Spoken Language Technologies for Under-Resourced Languages ({SLTU} 2018)},
	pages = {52--55},
	booktitle = {6th Workshop on Spoken Language Technologies for Under-Resourced Languages ({SLTU} 2018)},
	publisher = {{ISCA}},
	author = {Kjartansson, Oddur and Sarin, Supheakmungkol and Pipatsrisawat, Knot and Jansche, Martin and Ha, Linne},
	urldate = {2024-05-29},
	date = {2018-08-29},
	langid = {english},
}

@misc{landertCSLU22Languages2005,
	title = {{CSLU}: 22 Languages Corpus},
	url = {https://catalog.ldc.upenn.edu/LDC2005S26},
	doi = {10.35111/ZKN2-5X88},
	shorttitle = {{CSLU}},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}This file contains documentation on the {CSLU}: 22 Languages v 1.2, Linguistic Data Consortium ({LDC}) catalog number {LDC}2005S26 and {ISBN} 1-58563-361-5.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Produced by Center for Spoken Language Understanding and distributed by the Linguistic Data Consortium, the 22 Languages corpus consists of telephone speech from 21 languages: Eastern Arabic, Cantonese, Czech, Farsi, German, Hindi, Hungarian, Japanese, Korean, Malay, Mandarin, Italian, Polish, Portuguese, Russian, Spanish, Swedish, Swahili, Tamil, Vietnamese, and English. The corpus contains fixed vocabulary utterances (e.g. days of the week) as well as fluent continuous speech. Each of the 50,191 utterances is verified by a native speaker to determine if the caller followed instructions when answering the prompts. For this release, approximately 19,758 utterances have corresponding orthographic transcriptions in all the above languages except\&nbsp;Eastern Arabic,\&nbsp;Farsi,\&nbsp;Korean,\&nbsp;Russian,\&nbsp;Italian.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}For an exampe of this corpus, please listen to these {\textless}a href="desc/addenda/{LDC}2005S26\_AR.wav" rel="nofollow"{\textgreater}Arabic{\textless}/a{\textgreater} and {\textless}a href="desc/addenda/{LDC}2005S26\_EN.wav" rel="nofollow"{\textgreater}English{\textless}/a{\textgreater} audio samples.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Updates and Contact{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Questions regarding this corpus and about the {\textless}a href="http://www.cslu.ogi.edu/" rel="nofollow"{\textgreater}Center for Spoken Language Understanding{\textless}/a{\textgreater} should be directed to {\textless}a rel="nofollow"{\textgreater}Jan van Santen{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}/br{\textgreater} 
Portions © 1998-2002 Center for Spoken Language Understanding Oregon Health \& Science University, © 2005 Trustees of the University of Pennsylvania},
	publisher = {Linguistic Data Consortium},
	author = {{Lander, T}},
	urldate = {2024-05-29},
	date = {2005-11-29},
}

@inproceedings{liFreeLinguisticSpeech2017,
	location = {Kuala Lumpur},
	title = {Free linguistic and speech resources for Tibetan},
	isbn = {978-1-5386-1542-3},
	url = {http://ieeexplore.ieee.org/document/8282130/},
	doi = {10/gtsqzh},
	eventtitle = {2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	pages = {733--736},
	booktitle = {2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	publisher = {{IEEE}},
	author = {Li, Guanyu and Yu, Hongzhi and Zheng, Thomas Fang and Yan, Jinghao and Xu, Shipeng},
	urldate = {2024-05-01},
	date = {2017-12},
}

@misc{landertCSLUForeignAccented2007,
	title = {{CSLU}:  Foreign Accented English Release 1.2},
	url = {https://catalog.ldc.upenn.edu/LDC2007S08},
	doi = {10.35111/0VWP-XN48},
	shorttitle = {{CSLU}},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater} {\textless}p{\textgreater}This file contains documentation on {CSLU}: Foreign Accented English Release 1.2, Linguistic Data Consortium ({LDC}) catalog number {LDC}2006S38 and isbn 1-58563-392-5.{\textless}/p{\textgreater} {\textless}p{\textgreater}{CSLU}: Foreign Accented English Release 1.2 consists of continuous speech in English by native speakers of 22 different languages: Arabic, Cantonese, Czech, Farsi, French, German, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Mandarin Chinese, Malay, Polish, Portuguese (Brazilian and Iberian), Russian, Swedish, Spanish, Swahili, Tamil and Vietnamese. The corpus contains 4925 telephone-quality utterances, information about the speakers' linguistic backgrounds and perceptual judgments about the accents in the utterances. The speakers were asked to speak about themselves in English for 20 seconds. Three native speakers of American English independently listened to each utterance and judged the speakers' accents on a 4-point scale: negligible/no accent, mild accent, strong accent and very strong accent. This corpus is intended to support the study of the underlying characteristics of foreign accent and to enable research, development and evaluation of algorithms for the identification and understanding of accented speech. Some of the files in this corpus are also contained in {\textless}a href="http://catalog.ldc.upenn.edu/{LDC}2005S26" rel="nofollow"{\textgreater}{CSLU}: 22 Languages Corpus, {LDC}2005S26.{\textless}/a{\textgreater} {\textless}/p{\textgreater} {\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater} {\textless}p{\textgreater}For an example of the data in this corpus, please listen to this {\textless}a href="./desc/addenda/{LDC}2007S08.wav" rel="nofollow"{\textgreater}audio sample{\textless}/a{\textgreater}. {\textless}/p{\textgreater} {\textless}/br{\textgreater} 
Portions © 2000-2002 Center for Spoken Language Understanding, Oregon Health \&amp; Science University, © 2007 Trustees of the University of Pennsylvania},
	publisher = {[object Object]},
	author = {{Lander, T}},
	urldate = {2024-05-01},
	date = {2007-05-17},
	note = {Artwork Size: 1468006 {KB}
Pages: 1468006 {KB}},
}

@inproceedings{korvasFreeEnglishCzech2014,
	location = {Reykjavik, Iceland},
	title = {Free English and Czech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/535_Paper.pdf},
	abstract = {We present a dataset of telephone conversations in English and Czech, developed for training acoustic models for automatic speech recognition ({ASR}) in spoken dialogue systems ({SDSs}). The data comprise 45 hours of speech in English and over 18 hours in Czech. Large part of the data, both audio and transcriptions, was collected using crowdsourcing, the rest are transcriptions by hired transcribers. We release the data together with scripts for data pre-processing and building acoustic models using the {HTK} and Kaldi {ASR} toolkits. We publish also the trained models described in this paper. The data are released under the {CC}-{BY}-{SA} 3.0 license, the scripts are licensed under Apache 2.0. In the paper, we report on the methodology of collecting the data, on the size and properties of the data, and on the scripts and their use. We verify the usability of the datasets by training and evaluating acoustic models using the presented data and scripts.},
	eventtitle = {{LREC} 2014},
	pages = {4423--4428},
	booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Korvas, Matěj and Plátek, Ondřej and Dušek, Ondřej and Žilka, Lukáš and Jurčíček, Filip},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-01},
	date = {2014-05},
}

@article{bangKsponSpeechKoreanSpontaneous2020,
	title = {{KsponSpeech}: Korean Spontaneous Speech Corpus for Automatic Speech Recognition},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/19/6936},
	doi = {10/gtwwck},
	shorttitle = {{KsponSpeech}},
	abstract = {This paper introduces a large-scale spontaneous speech corpus of Korean, named {KsponSpeech}. This corpus contains 969 h of general open-domain dialog utterances, spoken by about 2000 native Korean speakers in a clean environment. All data were constructed by recording the dialogue of two people freely conversing on a variety of topics and manually transcribing the utterances. The transcription provides a dual transcription consisting of orthography and pronunciation, and disfluency tags for spontaneity of speech, such as filler words, repeated words, and word fragments. This paper also presents the baseline performance of an end-to-end speech recognition model trained with {KsponSpeech}. In addition, we investigated the performance of standard end-to-end architectures and the number of sub-word units suitable for Korean. We investigated issues that should be considered in spontaneous speech recognition in Korean. {KsponSpeech} is publicly available on an open data hub site of the Korea government.},
	pages = {6936},
	number = {19},
	journaltitle = {Applied Sciences},
	author = {Bang, Jeong-Uk and Yun, Seung and Kim, Seung-Hi and Choi, Mu-Yeol and Lee, Min-Kyu and Kim, Yeo-Jeong and Kim, Dong-Hyun and Park, Jun and Lee, Young-Jik and Kim, Sang-Hun},
	urldate = {2024-05-29},
	date = {2020-01},
	langid = {english},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
}

@inproceedings{shiHighlandPueblaNahuatl2021,
	location = {Online},
	title = {Highland Puebla Nahuatl Speech Translation Corpus for Endangered Language Documentation},
	url = {https://aclanthology.org/2021.americasnlp-1.7},
	doi = {10/gtwwcd},
	abstract = {Documentation of endangered languages ({ELs}) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of {EL} audio via automatic speech recognition ({ASR}), machine translation ({MT}), or speech translation ({ST}). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an {EL} spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end {ST} models could outperform a cascaded {ST} ({ASR} {\textbackslash}textgreater {MT}) pipeline when translating endangered language documentation materials.},
	eventtitle = {{AmericasNLP} 2021},
	pages = {53--63},
	booktitle = {Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas},
	publisher = {Association for Computational Linguistics},
	author = {Shi, Jiatong and Amith, Jonathan D. and Chang, Xuankai and Dalmia, Siddharth and Yan, Brian and Watanabe, Shinji},
	editor = {Mager, Manuel and Oncevay, Arturo and Rios, Annette and Ruiz, Ivan Vladimir Meza and Palmer, Alexis and Neubig, Graham and Kann, Katharina},
	urldate = {2024-05-29},
	date = {2021-06},
}

@inproceedings{tangKeSpeechOpenSource2021,
	title = {{KeSpeech}: An Open Source Speech Dataset of Mandarin and Its Eight Subdialects},
	volume = {1},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract-round2.html},
	shorttitle = {{KeSpeech}},
	booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	author = {Tang, Zhiyuan and Wang, Dong and Xu, Yanguang and Sun, Jianwei and Lei, Xiaoning and Zhao, Shuaijiang and Wen, Cheng and Tan, Xingjun and Xie, Chuandong and Zhou, Shuran and Yan, Rui and Lv, Chenjia and Han, Yang and Zou, Wei and Li, Xiangang},
	urldate = {2024-05-29},
	date = {2021-12-06},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{kratochvilLargeCorpusCzech2020,
	location = {Marseille, France},
	title = {Large Corpus of Czech Parliament Plenary Hearings},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.781},
	abstract = {We present a large corpus of Czech parliament plenary sessions. The corpus consists of approximately 1200 hours of speech data and corresponding text transcriptions. The whole corpus has been segmented to short audio segments making it suitable for both training and evaluation of automatic speech recognition ({ASR}) systems. The source language of the corpus is Czech, which makes it a valuable resource for future research as only a few public datasets are available in the Czech language. We complement the data release with experiments of two baseline {ASR} systems trained on the presented data: the more traditional approach implemented in the Kaldi {ASRtoolkit} which combines hidden Markov models and deep neural networks ({NN}) and a modern {ASR} architecture implemented in Jaspertoolkit which uses deep {NNs} in an end-to-end fashion.},
	eventtitle = {{LREC} 2020},
	pages = {6363--6367},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Kratochvil, Jonáš and Polák, Peter and Bojar, Ondřej},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2020-05},
}

@inproceedings{panayotovLibrispeechASRCorpus2015,
	location = {South Brisbane, Queensland, Australia},
	title = {Librispeech: An {ASR} corpus based on public domain audio books},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178964/},
	doi = {10/gfv84w},
	shorttitle = {Librispeech},
	eventtitle = {{ICASSP} 2015 - 2015 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {5206--5210},
	booktitle = {2015 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	urldate = {2024-05-01},
	date = {2015-04},
}

@inproceedings{pratapMLSLargeScaleMultilingual2020,
	title = {{MLS}: A Large-Scale Multilingual Dataset for Speech Research},
	url = {http://arxiv.org/abs/2012.03411},
	doi = {10/grk6mp},
	shorttitle = {{MLS}},
	abstract = {This paper introduces Multilingual {LibriSpeech} ({MLS}) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from {LibriVox} and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models ({LM}) and baseline Automatic Speech Recognition ({ASR}) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in {ASR} and Text-To-Speech ({TTS}) research. The dataset will be made freely available for anyone at http://www.openslr.org.},
	pages = {2757--2761},
	booktitle = {Interspeech 2020},
	author = {Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
	urldate = {2024-05-01},
	date = {2020-10-25},
	eprinttype = {arxiv},
	eprint = {2012.03411 [cs, eess]},
}

@inproceedings{zhiM2ASRMONGOFreeMongolian2021,
	location = {Singapore, Singapore},
	title = {M2ASR-{MONGO}: A Free Mongolian Speech Database and Accompanied Baselines},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-66540-870-7},
	url = {https://ieeexplore.ieee.org/document/9660401/},
	doi = {10/gtsqzg},
	shorttitle = {M2ASR-{MONGO}},
	eventtitle = {2021 24th Conference of the Oriental {COCOSDA} International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-{COCOSDA})},
	pages = {140--145},
	booktitle = {2021 24th Conference of the Oriental {COCOSDA} International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-{COCOSDA})},
	publisher = {{IEEE}},
	author = {Zhi, Tiankai and Shi, Ying and Du, Wenqiang and Li, Guanyu and Wang, Dong},
	urldate = {2024-05-01},
	date = {2021-11-18},
}

@article{mamtiminM2ASRKIRGHIZFreeKirghiz2023,
	title = {M2ASR-{KIRGHIZ}: A Free Kirghiz Speech Database and Accompanied Baselines},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/14/1/55},
	doi = {10/gtsqzm},
	shorttitle = {M2ASR-{KIRGHIZ}},
	abstract = {Deep learning has significantly boosted the performance improvement of automatic speech recognition ({ASR}) with the cooperation of large amounts of data resources. For minority languages, however, there are almost no large-scale data resources, limiting the development of {ASR} technologies in these languages. In this paper, we publish a free Kirghiz speech database accompanied by associated language resources. The entire database involves 128 h of speech data from 163 speakers and corresponding transcriptions. To our knowledge, this is the largest Kirghiz speech database that is dedicated to the {ASR} task and is publicly free so far. In addition, we also provide several baseline systems based on Kaldi and {WeNet} to demonstrate how these public data resources can be used to facilitate the Kirghiz {ASR} research. This publication is a part of the M2ASR project, and all the resources can be downloaded at the project webpage.},
	pages = {55},
	number = {1},
	journaltitle = {Information},
	shortjournal = {Information},
	author = {Mamtimin, Ikram and Du, Wenqiang and Hamdulla, Askar},
	urldate = {2024-05-01},
	date = {2023-01-16},
	langid = {english},
}

@inproceedings{bhanushaliGramVaaniASR2022,
	title = {Gram Vaani {ASR} Challenge on spontaneous telephone speech recordings in regional variations of Hindi},
	url = {https://www.isca-archive.org/interspeech_2022/bhanushali22_interspeech.html},
	doi = {10/gtsqzn},
	eventtitle = {Interspeech 2022},
	pages = {3548--3552},
	booktitle = {Interspeech 2022},
	publisher = {{ISCA}},
	author = {Bhanushali, Anish and Bridgman, Grant and G, Deekshitha and Ghosh, Prasanta and Kumar, Pratik and Kumar, Saurabh and Raj Kolladath, Adithya and Ravi, Nithya and Seth, Aaditeshwar and Seth, Ashish and Singh, Abhayjeet and Sukhadia, Vrunda and S, Umesh and Udupa, Sathvik and Prasad, Lodagala V. S. V. Durga},
	urldate = {2024-05-01},
	date = {2022-09-18},
	langid = {english},
}

@inproceedings{demirsahinOpensourceMultispeakerCorpora2020,
	location = {Marseille, France},
	title = {Open-source Multi-speaker Corpora of the English Accents in the British Isles},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.804},
	abstract = {This paper presents a dataset of transcribed high-quality audio of English sentences recorded by volunteers speaking with different accents of the British Isles. The dataset is intended for linguistic analysis as well as use for speech technologies. The recording scripts were curated specifically for accent elicitation, covering a variety of phonological phenomena and providing a high phoneme coverage. The scripts include pronunciations of global locations, major airlines and common personal names in different accents; and native speaker pronunciations of local words. Overlapping lines for all speakers were included for idiolect elicitation, which include the same or similar lines with other existing resources such as the {CSTR} {VCTK} corpus and the Speech Accent Archive to allow for easy comparison of personal and regional accents. The resulting corpora include over 31 hours of recordings from 120 volunteers who self-identify as native speakers of Southern England, Midlands, Northern England, Welsh, Scottish and Irish varieties of English.},
	eventtitle = {{LREC} 2020},
	pages = {6532--6541},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Demirsahin, Isin and Kjartansson, Oddur and Gutkin, Alexander and Rivera, Clara},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2020-05},
}

@inproceedings{kjartanssonOpenSourceHighQuality2020,
	location = {Marseille, France},
	title = {Open-Source High Quality Speech Datasets for Basque, Catalan and Galician},
	isbn = {979-10-95546-35-1},
	url = {https://aclanthology.org/2020.sltu-1.3},
	abstract = {This paper introduces new open speech datasets for three of the languages of Spain: Basque, Catalan and Galician. Catalan is furthermore the official language of the Principality of Andorra. The datasets consist of high-quality multi-speaker recordings of the three languages along with the associated transcriptions. The resulting corpora include over 33 hours of crowd-sourced recordings of 132 male and female native speakers. The recording scripts also include material for elicitation of global and local place names, personal and business names. The datasets are released under a permissive license and are available for free download for commercial, academic and personal use. The high-quality annotated speech datasets described in this paper can be used to, among other things, build text-to-speech systems, serve as adaptation data in automatic speech recognition and provide useful phonetic and phonological insights in corpus linguistics.},
	eventtitle = {{SLTU} 2020},
	pages = {21--27},
	booktitle = {Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages ({SLTU}) and Collaboration and Computing for Under-Resourced Languages ({CCURL})},
	publisher = {European Language Resources association},
	author = {Kjartansson, Oddur and Gutkin, Alexander and Butryna, Alena and Demirsahin, Isin and Rivera, Clara},
	editor = {Beermann, Dorothee and Besacier, Laurent and Sakti, Sakriani and Soria, Claudia},
	urldate = {2024-05-29},
	date = {2020-05},
}

@inproceedings{digangiMuSTCMultilingualSpeech2019,
	location = {Minneapolis, Minnesota},
	title = {{MuST}-C: a Multilingual Speech Translation Corpus},
	url = {https://aclanthology.org/N19-1202},
	doi = {10/gtsqzk},
	shorttitle = {{MuST}-C},
	abstract = {Current research on spoken language translation ({SLT}) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of {SLT}: automatic speech recognition and machine translation. To fill this gap, we created {MuST}-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for {SLT} from English into 8 languages. For each target language, {MuST}-C comprises at least 385 hours of audio recordings from English {TED} Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and {SLT} results computed with a state-of-the-art approach on each language direction.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {2012--2017},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Di Gangi, Mattia A. and Cattoni, Roldano and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	urldate = {2024-05-01},
	date = {2019-06},
}

@inproceedings{solbergNorwegianParliamentarySpeech2022,
	location = {Marseille, France},
	title = {The Norwegian Parliamentary Speech Corpus},
	url = {https://aclanthology.org/2022.lrec-1.106},
	abstract = {The Norwegian Parliamentary Speech Corpus ({NPSC}) is a speech dataset with recordings of meetings from Stortinget, the Norwegian parliament. It is the first, publicly available dataset containing unscripted, Norwegian speech designed for training of automatic speech recognition ({ASR}) systems. The recordings are manually transcribed and annotated with language codes and speakers, and there are detailed metadata about the speakers. The transcriptions exist in both normalized and non-normalized form, and non-standardized words are explicitly marked and annotated with standardized equivalents. To test the usefulness of this dataset, we have compared an {ASR} system trained on the {NPSC} with a baseline system trained on only manuscript-read speech. These systems were tested on an independent dataset containing spontaneous, dialectal speech. The {NPSC}-trained system performed significantly better, with a 22.9\% relative improvement in word error rate ({WER}). Moreover, training on the {NPSC} is shown to have a “democratizing” effects in terms of dialects, as improvements are generally larger for dialects with higher {WER} from the baseline system.},
	eventtitle = {{LREC} 2022},
	pages = {1003--1008},
	booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Solberg, Per Erik and Ortiz, Pablo},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2022-06},
}

@inproceedings{godfreySWITCHBOARDTelephoneSpeech1992,
	location = {San Francisco, {CA}, {USA}},
	title = {{SWITCHBOARD}: telephone speech corpus for research and development},
	isbn = {978-0-7803-0532-8},
	url = {http://ieeexplore.ieee.org/document/225858/},
	doi = {10/fp48kw},
	shorttitle = {{SWITCHBOARD}},
	eventtitle = {[Proceedings] {ICASSP}-92: 1992 {IEEE} International Conference on Acoustics, Speech, and Signal Processing},
	pages = {517--520 vol.1},
	booktitle = {[Proceedings] {ICASSP}-92: 1992 {IEEE} International Conference on Acoustics, Speech, and Signal Processing},
	publisher = {{IEEE}},
	author = {Godfrey, J.J. and Holliman, E.C. and {McDaniel}, J.},
	urldate = {2024-05-01},
	date = {1992},
}

@inproceedings{cieriFisherCorpusResource2004,
	location = {Lisbon, Portugal},
	title = {The Fisher Corpus: a Resource for the Next Generations of Speech-to-Text},
	url = {http://www.lrec-conf.org/proceedings/lrec2004/pdf/767.pdf},
	shorttitle = {The Fisher Corpus},
	eventtitle = {{LREC} 2004},
	booktitle = {Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}'04)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Cieri, Christopher and Miller, David and Walker, Kevin},
	editor = {Lino, Maria Teresa and Xavier, Maria Francisca and Ferreira, Fátima and Costa, Rute and Silva, Raquel},
	urldate = {2024-05-01},
	date = {2004-05},
}

@misc{itoLJSpeechDataset2017,
	title = {The {LJ} Speech Dataset},
	url = {https://keithito.com/LJ-Speech-Dataset},
	abstract = {A public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip.},
	author = {Ito, Keith and Johnson, Linda},
	urldate = {2024-05-01},
	date = {2017},
}

@misc{solakMAILABSSpeechDataset2024,
	title = {The M-{AILABS} Speech Dataset – caito},
	url = {https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/},
	abstract = {The M-{AILABS} Speech Dataset is the first large dataset that we are providing free-of-charge, freely usable as training data for speech recognition and speech synthesis.

Most of the data is based on {LibriVox} and Project Gutenberg. The training data consist of nearly thousand hours of audio and the text-files in prepared format.

A transcription is provided for each clip. Clips vary in length from 1 to 20 seconds and have a total length of approximately shown in the list (and in the respective info.txt-files) below.

The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded by the {LibriVox} project and is also in the public domain – except for Ukrainian.},
	author = {Solak, Imdat},
	urldate = {2024-05-01},
	date = {2024-01-05},
}

@misc{garofolojohns.TIMITAcousticPhoneticContinuous1993,
	title = {{TIMIT} Acoustic-Phonetic Continuous Speech Corpus},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	doi = {10.35111/17GK-BN40},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The {TIMIT} corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. {TIMIT} contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The {TIMIT} corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology ({MIT}), {SRI} International ({SRI}) and Texas Instruments, Inc. ({TI}). The speech was recorded at {TI}, transcribed at {MIT} and verified and prepared for {CD}-{ROM} production by the National Institute of Standards and Technology ({NIST}).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The {TIMIT} corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}ul{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/{LDC}93S1.phn" rel="nofollow"{\textgreater}phonemes{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/{LDC}93S1.txt" rel="nofollow"{\textgreater}transcripts{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/{LDC}93S1.wav" rel="nofollow"{\textgreater}audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/{LDC}93S1.wrd" rel="nofollow"{\textgreater}word list{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}/ul{\textgreater}{\textless}/br{\textgreater} 
Portions © 1993 Trustees of the University of Pennsylvania},
	publisher = {[object Object]},
	author = {{Garofolo, John S.} and {Lamel, Lori F.} and {Fisher, William M.} and {Pallett, David S.} and {Dahlgren, Nancy L.} and {Zue, Victor} and {Fiscus, Jonathan G.}},
	urldate = {2024-05-01},
	date = {1993},
	note = {Artwork Size: 715776 {KB}
Pages: 715776 {KB}},
}

@misc{levyZeroShotRelationExtraction2017,
	title = {Zero-Shot Relation Extraction via Reading Comprehension},
	url = {http://arxiv.org/abs/1706.04115},
	abstract = {We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.},
	number = {{arXiv}:1706.04115},
	publisher = {{arXiv}},
	author = {Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
	urldate = {2024-05-29},
	date = {2017-06-13},
	eprinttype = {arxiv},
	eprint = {1706.04115 [cs]},
}

@misc{pontiXCOPAMultilingualDataset2020,
	title = {{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning},
	url = {http://arxiv.org/abs/2005.00333},
	shorttitle = {{XCOPA}},
	abstract = {In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives ({XCOPA}), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur{\textbackslash}'imac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The {XCOPA} dataset is freely available at github.com/cambridgeltl/xcopa.},
	number = {{arXiv}:2005.00333},
	publisher = {{arXiv}},
	author = {Ponti, Edoardo Maria and Glavaš, Goran and Majewska, Olga and Liu, Qianchu and Vulić, Ivan and Korhonen, Anna},
	urldate = {2024-05-29},
	date = {2020-10-26},
	eprinttype = {arxiv},
	eprint = {2005.00333 [cs]},
}

@misc{zhangWinoWhyDeepDiagnosis2020,
	title = {{WinoWhy}: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge},
	url = {http://arxiv.org/abs/2005.05763},
	shorttitle = {{WinoWhy}},
	abstract = {In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge ({WSC}). For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories. By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning. Moreover, to investigate whether current {WSC} models can understand the commonsense or they simply solve the {WSC} questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called {WinoWhy}, which requires models to distinguish plausible reasons from very similar but wrong reasons for all {WSC} questions. Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original {WSC} dataset, they are still struggling at {WinoWhy}. Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution. {WinoWhy} and all codes are available at: https://github.com/{HKUST}-{KnowComp}/{WinoWhy}.},
	number = {{arXiv}:2005.05763},
	publisher = {{arXiv}},
	author = {Zhang, Hongming and Zhao, Xinran and Song, Yangqiu},
	urldate = {2024-05-29},
	date = {2020-05-12},
	eprinttype = {arxiv},
	eprint = {2005.05763 [cs]},
}

@inproceedings{yangWikiQAChallengeDataset2015,
	location = {Lisbon, Portugal},
	title = {{WikiQA}: A Challenge Dataset for Open-Domain Question Answering},
	url = {https://aclanthology.org/D15-1237},
	doi = {10/gfsqqb},
	shorttitle = {{WikiQA}},
	eventtitle = {{EMNLP} 2015},
	pages = {2013--2018},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Yi and Yih, Wen-tau and Meek, Christopher},
	editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
	urldate = {2024-05-29},
	date = {2015-09},
}

@misc{zhengWhenDoesPretraining2021,
	title = {When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the {CaseHOLD} Dataset},
	url = {http://arxiv.org/abs/2104.08671},
	shorttitle = {When Does Pretraining Help?},
	abstract = {While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal {NLP} tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present {CaseHOLD} (Case Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an {NLP} perspective (F1 of 0.4 with a {BiLSTM} baseline). Second, we assess performance gains on {CaseHOLD} and existing legal {NLP} datasets. While a Transformer architecture ({BERT}) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (using corpus of approximately 3.5M decisions across all courts in the U.S. that is larger than {BERT}'s) with a custom legal vocabulary exhibits the most substantial performance gains with {CaseHOLD} (gain of 7.2\% on F1, representing a 12\% improvement on {BERT}) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.},
	number = {{arXiv}:2104.08671},
	publisher = {{arXiv}},
	author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
	urldate = {2024-05-29},
	date = {2021-07-05},
	eprinttype = {arxiv},
	eprint = {2104.08671 [cs]},
}

@misc{veysehWhatDoesThis2020,
	title = {What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation},
	url = {http://arxiv.org/abs/2010.14678},
	shorttitle = {What Does This Acronym Mean?},
	abstract = {Acronyms are the short forms of phrases that facilitate conveying lengthy sentences in documents and serve as one of the mainstays of writing. Due to their importance, identifying acronyms and corresponding phrases (i.e., acronym identification ({AI})) and finding the correct meaning of each acronym (i.e., acronym disambiguation ({AD})) are crucial for text understanding. Despite the recent progress on this task, there are some limitations in the existing datasets which hinder further improvement. More specifically, limited size of manually annotated {AI} datasets or noises in the automatically created acronym identification datasets obstruct designing advanced high-performing acronym identification models. Moreover, the existing datasets are mostly limited to the medical domain and ignore other domains. In order to address these two limitations, we first create a manually annotated large {AI} dataset for scientific domain. This dataset contains 17,506 sentences which is substantially larger than previous scientific {AI} datasets. Next, we prepare an {AD} dataset for scientific domain with 62,441 samples which is significantly larger than the previous scientific {AD} dataset. Our experiments show that the existing state-of-the-art models fall far behind human-level performance on both datasets proposed by this work. In addition, we propose a new deep learning model that utilizes the syntactical structure of the sentence to expand an ambiguous acronym in a sentence. The proposed model outperforms the state-of-the-art models on the new {AD} dataset, providing a strong baseline for future research on this dataset.},
	number = {{arXiv}:2010.14678},
	publisher = {{arXiv}},
	author = {Veyseh, Amir Pouran Ben and Dernoncourt, Franck and Tran, Quan Hung and Nguyen, Thien Huu},
	urldate = {2024-05-29},
	date = {2020-10-27},
	eprinttype = {arxiv},
	eprint = {2010.14678 [cs]},
}

@misc{kavumbaWhenChoosingPlausible2019,
	title = {When Choosing Plausible Alternatives, Clever Hans can be Clever},
	url = {http://arxiv.org/abs/1911.00225},
	abstract = {Pretrained language models, such as {BERT} and {RoBERTa}, have shown large improvements in the commonsense reasoning benchmark {COPA}. However, recent work found that many improvements in benchmarks of natural language understanding are not due to models learning the task, but due to their increasing ability to exploit superficial cues, such as tokens that occur more often in the correct answer than the wrong one. Are {BERT}'s and {RoBERTa}'s good performance on {COPA} also caused by this? We find superficial cues in {COPA}, as well as evidence that {BERT} exploits these cues. To remedy this problem, we introduce Balanced {COPA}, an extension of {COPA} that does not suffer from easy-to-exploit single token cues. We analyze {BERT}'s and {RoBERTa}'s performance on original and Balanced {COPA}, finding that {BERT} relies on superficial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that {BERT} learns the task to a certain degree when forced to. In contrast, {RoBERTa} does not appear to rely on superficial cues.},
	number = {{arXiv}:1911.00225},
	publisher = {{arXiv}},
	author = {Kavumba, Pride and Inoue, Naoya and Heinzerling, Benjamin and Singh, Keshav and Reisert, Paul and Inui, Kentaro},
	urldate = {2024-05-29},
	date = {2019-11-01},
	eprinttype = {arxiv},
	eprint = {1911.00225 [cs]},
}

@misc{qiWhenWhyAre2018,
	title = {When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?},
	url = {http://arxiv.org/abs/1804.06323},
	abstract = {The performance of Neural Machine Translation ({NMT}) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for {NMT} has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in {NMT} tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 {BLEU} points in the most favorable setting.},
	number = {{arXiv}:1804.06323},
	publisher = {{arXiv}},
	author = {Qi, Ye and Sachan, Devendra Singh and Felix, Matthieu and Padmanabhan, Sarguna Janani and Neubig, Graham},
	urldate = {2024-05-29},
	date = {2018-04-18},
	eprinttype = {arxiv},
	eprint = {1804.06323 [cs]},
}

@misc{pilehvarWiCWordinContextDataset2019,
	title = {{WiC}: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
	url = {http://arxiv.org/abs/1808.09121},
	shorttitle = {{WiC}},
	abstract = {By design, word embeddings are unable to model the dynamic nature of words' semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called {WiC}, based on annotations curated by experts, for generic evaluation of context-sensitive representations. {WiC} is released in https://pilehvar.github.io/wic/.},
	number = {{arXiv}:1808.09121},
	publisher = {{arXiv}},
	author = {Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
	urldate = {2024-05-29},
	date = {2019-04-27},
	eprinttype = {arxiv},
	eprint = {1808.09121 [cs]},
}

@misc{liuWeReAfraid2023,
	title = {We're Afraid Language Models Aren't Modeling Ambiguity},
	url = {http://arxiv.org/abs/2304.14399},
	abstract = {Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models ({LMs}) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect {AmbiEnt}, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on {AmbiEnt}, presenting the first evaluation of pretrained {LMs} to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for {GPT}-4, whose generated disambiguations are considered correct only 32\% of the time in human evaluation, compared to 90\% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel {NLI} model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for {NLP}.},
	number = {{arXiv}:2304.14399},
	publisher = {{arXiv}},
	author = {Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr, Alane and West, Peter and Koller, Alexander and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2023-10-20},
	eprinttype = {arxiv},
	eprint = {2304.14399 [cs]},
}

@misc{liuWANLIWorkerAI2022,
	title = {{WANLI}: Worker and {AI} Collaboration for Natural Language Inference Dataset Creation},
	url = {http://arxiv.org/abs/2201.05955},
	shorttitle = {{WANLI}},
	abstract = {A recurring challenge of crowdsourcing {NLP} datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and {AI} collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, {MultiNLI} for natural language inference ({NLI}), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs {GPT}-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, {WANLI}, consists of 107,885 {NLI} examples and presents unique empirical strengths over existing {NLI} datasets. Remarkably, training a model on {WANLI} improves performance on eight out-of-domain test sets we consider, including by 11\% on {HANS} and 9\% on Adversarial {NLI}, compared to training on the 4x larger {MultiNLI}. Moreover, it continues to be more effective than {MultiNLI} augmented with other {NLI} datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.},
	number = {{arXiv}:2201.05955},
	publisher = {{arXiv}},
	author = {Liu, Alisa and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2022-11-14},
	eprinttype = {arxiv},
	eprint = {2201.05955 [cs]},
}

@inproceedings{karadzhovWeBuiltFake2017,
	title = {We Built a Fake News \& Click-bait Filter: What Happened Next Will Blow Your Mind!},
	url = {http://arxiv.org/abs/1803.03786},
	doi = {10/gg6q82},
	shorttitle = {We Built a Fake News \& Click-bait Filter},
	abstract = {It is completely amazing! Fake news and click-baits have totally invaded the cyber space. Let us face it: everybody hates them for three simple reasons. Reason \#2 will absolutely amaze you. What these can achieve at the time of election will completely blow your mind! Now, we all agree, this cannot go on, you know, somebody has to stop it. So, we did this research on fake news/click-bait detection and trust us, it is totally great research, it really is! Make no mistake. This is the best research ever! Seriously, come have a look, we have it all: neural networks, attention mechanism, sentiment lexicons, author profiling, you name it. Lexical features, semantic features, we absolutely have it all. And we have totally tested it, trust us! We have results, and numbers, really big numbers. The best numbers ever! Oh, and analysis, absolutely top notch analysis. Interested? Come read the shocking truth about fake news and click-bait in the Bulgarian cyber space. You won't believe what we have found!},
	pages = {334--343},
	booktitle = {{RANLP} 2017 - Recent Advances in Natural Language Processing Meet Deep Learning},
	author = {Karadzhov, Georgi and Gencheva, Pepa and Nakov, Preslav and Koychev, Ivan},
	urldate = {2024-05-29},
	date = {2017-11-10},
	eprinttype = {arxiv},
	eprint = {1803.03786 [cs]},
}

@misc{nagraniVoxCelebLargescaleSpeaker2018,
	title = {{VoxCeleb}: a large-scale speaker identification dataset},
	url = {http://arxiv.org/abs/1706.08612},
	doi = {10.21437/Interspeech.2017-950},
	shorttitle = {{VoxCeleb}},
	abstract = {Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from {YouTube}; performing active speaker verification using a two-stream synchronization Convolutional Neural Network ({CNN}), and confirming the identity of the speaker using {CNN} based facial recognition. We use this pipeline to curate {VoxCeleb} which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a {CNN} based architecture obtains the best performance for both identification and verification.},
	author = {Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew},
	urldate = {2024-05-29},
	date = {2018-05-30},
	eprinttype = {arxiv},
	eprint = {1706.08612 [cs]},
}

@misc{xieUnifiedSKGUnifyingMultiTasking2022,
	title = {{UnifiedSKG}: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models},
	url = {http://arxiv.org/abs/2201.05966},
	shorttitle = {{UnifiedSKG}},
	abstract = {Structured knowledge grounding ({SKG}) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of {SKG} tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on {SKG}. In this paper, we overcome this limitation by proposing the {UnifiedSKG} framework, which unifies 21 {SKG} tasks into a text-to-text format, aiming to promote systematic {SKG} research, instead of being exclusive to a single task, domain, or dataset. We use {UnifiedSKG} to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. {UnifiedSKG} also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, {GPT}-3, and Codex struggle in zero-shot and few-shot learning for {SKG}. We also use {UnifiedSKG} to conduct a series of controlled experiments on structured knowledge encoding variants across {SKG} tasks. {UnifiedSKG} is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.},
	number = {{arXiv}:2201.05966},
	publisher = {{arXiv}},
	author = {Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I. and Zhong, Victor and Wang, Bailin and Li, Chengzu and Boyle, Connor and Ni, Ansong and Yao, Ziyu and Radev, Dragomir and Xiong, Caiming and Kong, Lingpeng and Zhang, Rui and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
	urldate = {2024-05-29},
	date = {2022-10-18},
	eprinttype = {arxiv},
	eprint = {2201.05966 [cs]},
}

@misc{lebanoffUnderstandingPointsCorrespondence2020,
	title = {Understanding Points of Correspondence between Sentences for Abstractive Summarization},
	url = {http://arxiv.org/abs/2006.05621},
	abstract = {Fusing sentences containing disparate content is a remarkable human ability that helps create informative and succinct summaries. Such a simple task for humans has remained challenging for modern abstractive summarizers, substantially restricting their applicability in real-world scenarios. In this paper, we present an investigation into fusing sentences drawn from a document by introducing the notion of points of correspondence, which are cohesive devices that tie any two sentences together into a coherent text. The types of points of correspondence are delineated by text cohesion theory, covering pronominal and nominal referencing, repetition and beyond. We create a dataset containing the documents, source and fusion sentences, and human annotations of points of correspondence between sentences. Our dataset bridges the gap between coreference resolution and summarization. It is publicly shared to serve as a basis for future work to measure the success of sentence fusion systems. (https://github.com/ucfnlp/points-of-correspondence)},
	number = {{arXiv}:2006.05621},
	publisher = {{arXiv}},
	author = {Lebanoff, Logan and Muchovej, John and Dernoncourt, Franck and Kim, Doo Soon and Wang, Lidan and Chang, Walter and Liu, Fei},
	urldate = {2024-05-29},
	date = {2020-06-09},
	eprinttype = {arxiv},
	eprint = {2006.05621 [cs]},
}

@inproceedings{nivreUniversalDependenciesV12016,
	location = {Portorož, Slovenia},
	title = {Universal Dependencies v1: A Multilingual Treebank Collection},
	url = {https://aclanthology.org/L16-1262},
	shorttitle = {Universal Dependencies v1},
	abstract = {Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.},
	eventtitle = {{LREC} 2016},
	pages = {1659--1666},
	booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajič, Jan and Manning, Christopher D. and {McDonald}, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2016-05},
}

@inproceedings{kobbeUnsupervisedStanceDetection2020,
	location = {Online},
	title = {Unsupervised stance detection for arguments from consequences},
	url = {https://aclanthology.org/2020.emnlp-main.4},
	doi = {10/gm3nhq},
	abstract = {Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic. To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences. We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence. Our experiments provide promising results that are comparable to, and in particular regards even outperform {BERT}. Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.},
	eventtitle = {{EMNLP} 2020},
	pages = {50--60},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kobbe, Jonathan and Hulpu\{{\textbackslash}textbackslash\}textcommabelows, Ioana and Stuckenschmidt, Heiner},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-05-29},
	date = {2020-11},
}

@misc{uchenduTURINGBENCHBenchmarkEnvironment2021,
	title = {{TURINGBENCH}: A Benchmark Environment for Turing Test in the Age of Neural Text Generation},
	url = {http://arxiv.org/abs/2109.13296},
	shorttitle = {{TURINGBENCH}},
	abstract = {Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called "Turing Test" problem for neural text generation methods. In this work, we present the {TuringBench} benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels \{Human, {GPT}-1, {GPT}-2\_small, {GPT}-2\_medium, {GPT}-2\_large, {GPT}-2\_xl, {GPT}-2\_PyTorch, {GPT}-3, {GROVER}\_base, {GROVER}\_large, {GROVER}\_mega, {CTRL}, {XLM}, {XLNET}\_base, {XLNET}\_large, {FAIR}\_wmt19, {FAIR}\_wmt20, {TRANSFORMER}\_XL, {PPLM}\_distil, {PPLM}\_gpt2\}, (2) two benchmark tasks -- i.e., Turing Test ({TT}) and Authorship Attribution ({AA}), and (3) a website with leaderboards. Our preliminary experimental results using {TuringBench} show that {FAIR}\_wmt20 and {GPT}-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art {TT} detection models. The {TuringBench} is available at: https://turingbench.ist.psu.edu/},
	number = {{arXiv}:2109.13296},
	publisher = {{arXiv}},
	author = {Uchendu, Adaku and Ma, Zeyu and Le, Thai and Zhang, Rui and Lee, Dongwon},
	urldate = {2024-05-29},
	date = {2021-09-27},
	eprinttype = {arxiv},
	eprint = {2109.13296 [cs]},
}

@misc{taruneshTrustingRoBERTaBERT2021,
	title = {Trusting {RoBERTa} over {BERT}: Insights from {CheckListing} the Natural Language Inference Task},
	url = {http://arxiv.org/abs/2107.07229},
	shorttitle = {Trusting {RoBERTa} over {BERT}},
	abstract = {The recent state-of-the-art natural language understanding ({NLU}) systems often behave unpredictably, failing on simpler reasoning examples. Despite this, there has been limited focus on quantifying progress towards systems with more predictable behavior. We think that reasoning capability-wise behavioral summary is a step towards bridging this gap. We create a {CheckList} test-suite (184K examples) for the Natural Language Inference ({NLI}) task, a representative {NLU} task. We benchmark state-of-the-art {NLI} systems on this test-suite, which reveals fine-grained insights into the reasoning abilities of {BERT} and {RoBERTa}. Our analysis further reveals inconsistencies of the models on examples derived from the same template or distinct templates but pertaining to same reasoning capability, indicating that generalizing the models' behavior through observations made on a {CheckList} is non-trivial. Through an user-study, we find that users were able to utilize behavioral information to generalize much better for examples predicted from {RoBERTa}, compared to that of {BERT}.},
	number = {{arXiv}:2107.07229},
	publisher = {{arXiv}},
	author = {Tarunesh, Ishan and Aditya, Somak and Choudhury, Monojit},
	urldate = {2024-05-29},
	date = {2021-07-15},
	eprinttype = {arxiv},
	eprint = {2107.07229 [cs]},
}

@misc{barbieriTweetEvalUnifiedBenchmark2020,
	title = {{TweetEval}: Unified Benchmark and Comparative Evaluation for Tweet Classification},
	url = {http://arxiv.org/abs/2010.12421},
	shorttitle = {{TweetEval}},
	abstract = {The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework ({TweetEval}) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.},
	number = {{arXiv}:2010.12421},
	publisher = {{arXiv}},
	author = {Barbieri, Francesco and Camacho-Collados, Jose and Neves, Leonardo and Espinosa-Anke, Luis},
	urldate = {2024-05-29},
	date = {2020-10-26},
	eprinttype = {arxiv},
	eprint = {2010.12421 [cs]},
}

@misc{hedderichTransferLearningDistant2020,
	title = {Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages},
	url = {http://arxiv.org/abs/2010.03179},
	shorttitle = {Transfer Learning and Distant Supervision for Multilingual Transformer Models},
	abstract = {Multilingual transformer models like {mBERT} and {XLM}-{RoBERTa} have obtained great improvements for many {NLP} tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, {isiXhosa} and Yor{\textbackslash}`ub{\textbackslash}'a on both {NER} and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.},
	number = {{arXiv}:2010.03179},
	publisher = {{arXiv}},
	author = {Hedderich, Michael A. and Adelani, David and Zhu, Dawei and Alabi, Jesujoba and Markus, Udia and Klakow, Dietrich},
	urldate = {2024-05-29},
	date = {2020-10-07},
	eprinttype = {arxiv},
	eprint = {2010.03179 [cs]},
}

@misc{clarkTransformersSoftReasoners2020,
	title = {Transformers as Soft Reasoners over Language},
	url = {http://arxiv.org/abs/2002.05867},
	abstract = {Beginning with {McCarthy}'s Advice Taker (1959), {AI} has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call {RuleTakers}, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99\%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.},
	number = {{arXiv}:2002.05867},
	publisher = {{arXiv}},
	author = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
	urldate = {2024-05-29},
	date = {2020-05-05},
	eprinttype = {arxiv},
	eprint = {2002.05867 [cs]},
}

@misc{mishraTrackingStateChanges2018,
	title = {Tracking State Changes in Procedural Text: A Challenge Dataset and Models for Process Paragraph Comprehension},
	url = {http://arxiv.org/abs/1805.06975},
	shorttitle = {Tracking State Changes in Procedural Text},
	abstract = {We present a new dataset and models for comprehending paragraphs about processes (e.g., photosynthesis), an important genre of text describing a dynamic world. The new dataset, {ProPara}, is the first to contain natural (rather than machine-generated) text about a changing world along with a full annotation of entity states (location and existence) during those changes (81k datapoints). The end-task, tracking the location and existence of entities through the text, is challenging because the causal effects of actions are often implicit and need to be inferred. We find that previous models that have worked well on synthetic data achieve only mediocre performance on {ProPara}, and introduce two new neural models that exploit alternative mechanisms for state prediction, in particular using {LSTM} input encoding and span prediction. The new models improve accuracy by up to 19\%. The dataset and models are available to the community at http://data.allenai.org/propara.},
	number = {{arXiv}:1805.06975},
	publisher = {{arXiv}},
	author = {Mishra, Bhavana Dalvi and Huang, Lifu and Tandon, Niket and Yih, Wen-tau and Clark, Peter},
	urldate = {2024-05-29},
	date = {2018-05-17},
	eprinttype = {arxiv},
	eprint = {1805.06975 [cs]},
}

@inproceedings{pradhanRobustLinguisticAnalysis2013,
	location = {Sofia, Bulgaria},
	title = {Towards Robust Linguistic Analysis using {OntoNotes}},
	url = {https://aclanthology.org/W13-3516},
	eventtitle = {{CoNLL} 2013},
	pages = {143--152},
	booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Ng, Hwee Tou and Björkelund, Anders and Uryupina, Olga and Zhang, Yuchen and Zhong, Zhi},
	editor = {Hockenmaier, Julia and Riedel, Sebastian},
	urldate = {2024-05-29},
	date = {2013-08},
}

@misc{ningTORQUEReadingComprehension2020,
	title = {{TORQUE}: A Reading Comprehension Dataset of Temporal Ordering Questions},
	url = {http://arxiv.org/abs/2005.00242},
	shorttitle = {{TORQUE}},
	abstract = {A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as "what happened before/after [some event]?" We introduce {TORQUE}, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that {RoBERTa}-large achieves an exact-match score of 51\% on the test set of {TORQUE}, about 30\% behind human performance.},
	number = {{arXiv}:2005.00242},
	publisher = {{arXiv}},
	author = {Ning, Qiang and Wu, Hao and Han, Rujun and Peng, Nanyun and Gardner, Matt and Roth, Dan},
	urldate = {2024-05-29},
	date = {2020-10-05},
	eprinttype = {arxiv},
	eprint = {2005.00242 [cs]},
}

@misc{cacholaTLDRExtremeSummarization2020,
	title = {{TLDR}: Extreme Summarization of Scientific Documents},
	url = {http://arxiv.org/abs/2004.15011},
	shorttitle = {{TLDR}},
	abstract = {We introduce {TLDR} generation, a new form of extreme summarization, for scientific papers. {TLDR} generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce {SciTLDR}, a new multi-target dataset of 5.4K {TLDRs} over 3.2K papers. {SciTLDR} contains both author-written and expert-derived {TLDRs}, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose {CATTS}, a simple yet effective learning strategy for generating {TLDRs} that exploits titles as an auxiliary training signal. {CATTS} improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.},
	number = {{arXiv}:2004.15011},
	publisher = {{arXiv}},
	author = {Cachola, Isabel and Lo, Kyle and Cohan, Arman and Weld, Daniel S.},
	urldate = {2024-05-29},
	date = {2020-10-08},
	eprinttype = {arxiv},
	eprint = {2004.15011 [cs]},
}

@misc{zhongPersonaBasedEmpatheticConversational2020,
	title = {Towards Persona-Based Empathetic Conversational Models},
	url = {http://arxiv.org/abs/2004.12316},
	abstract = {Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose {CoBERT}, an efficient {BERT}-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when {CoBERT} is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.},
	number = {{arXiv}:2004.12316},
	publisher = {{arXiv}},
	author = {Zhong, Peixiang and Zhang, Chen and Wang, Hao and Liu, Yong and Miao, Chunyan},
	urldate = {2024-05-29},
	date = {2020-11-19},
	eprinttype = {arxiv},
	eprint = {2004.12316 [cs]},
}

@misc{zhangThisEmailCould2019,
	title = {This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation},
	url = {http://arxiv.org/abs/1906.03497},
	shorttitle = {This Email Could Save Your Life},
	abstract = {Given the overwhelming number of emails, an effective subject line becomes essential to better inform the recipient of the email's content. In this paper, we propose and study the task of email subject line generation: automatically generating an email subject line from the email body. We create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation.},
	number = {{arXiv}:1906.03497},
	publisher = {{arXiv}},
	author = {Zhang, Rui and Tetreault, Joel},
	urldate = {2024-05-29},
	date = {2019-06-08},
	eprinttype = {arxiv},
	eprint = {1906.03497 [cs]},
}

@misc{mirzakhmedovaTouchE23ValueEvalDataset2023,
	title = {The Touch{\textbackslash}'e23-{ValueEval} Dataset for Identifying Human Values behind Arguments},
	url = {http://arxiv.org/abs/2301.13771},
	abstract = {We present the Touch{\textbackslash}'e23-{ValueEval} Dataset for Identifying Human Values behind Arguments. To investigate approaches for the automated detection of human values behind arguments, we collected 9324 arguments from 6 diverse sources, covering religious texts, political discussions, free-text arguments, newspaper editorials, and online democracy platforms. Each argument was annotated by 3 crowdworkers for 54 values. The Touch{\textbackslash}'e23-{ValueEval} dataset extends the Webis-{ArgValues}-22. In comparison to the previous dataset, the effectiveness of a 1-Baseline decreases, but that of an out-of-the-box {BERT} model increases. Therefore, though the classification difficulty increased as per the label distribution, the larger dataset allows for training better models.},
	number = {{arXiv}:2301.13771},
	publisher = {{arXiv}},
	author = {Mirzakhmedova, Nailia and Kiesel, Johannes and Alshomary, Milad and Heinrich, Maximilian and Handke, Nicolas and Cai, Xiaoni and Valentin, Barriere and Dastgheib, Doratossadat and Ghahroodi, Omid and Sadraei, Mohammad Ali and Asgari, Ehsaneddin and Kawaletz, Lea and Wachsmuth, Henning and Stein, Benno},
	urldate = {2024-05-29},
	date = {2023-01-31},
	eprinttype = {arxiv},
	eprint = {2301.13771 [cs]},
}

@misc{abdouSensitivityLanguageModels2020,
	title = {The Sensitivity of Language Models and Humans to Winograd Schema Perturbations},
	url = {http://arxiv.org/abs/2005.01348},
	abstract = {Large-scale pretrained language models are the major driving force behind recent improvements in performance on the Winograd Schema Challenge, a widely employed test of common sense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones. Overall, humans are correct more often than out-of-the-box models, and the models are sometimes right for the wrong reasons. Finally, we show that fine-tuning on a large, task-specific dataset can offer a solution to these issues.},
	number = {{arXiv}:2005.01348},
	publisher = {{arXiv}},
	author = {Abdou, Mostafa and Ravishankar, Vinit and Barrett, Maria and Belinkov, Yonatan and Elliott, Desmond and Søgaard, Anders},
	urldate = {2024-05-29},
	date = {2020-05-07},
	eprinttype = {arxiv},
	eprint = {2005.01348 [cs]},
}

@misc{shengWomanWorkedBabysitter2019,
	title = {The Woman Worked as a Babysitter: On Biases in Language Generation},
	url = {http://arxiv.org/abs/1909.01326},
	shorttitle = {The Woman Worked as a Babysitter},
	abstract = {We present a systematic study of biases in natural language generation ({NLG}) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in {NLG}, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in {NLG}, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.},
	number = {{arXiv}:1909.01326},
	publisher = {{arXiv}},
	author = {Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
	urldate = {2024-05-29},
	date = {2019-10-23},
	eprinttype = {arxiv},
	eprint = {1909.01326 [cs]},
}

@inproceedings{rudingerThinkingSkepticDefeasible2020,
	location = {Online},
	title = {Thinking Like a Skeptic: Defeasible Inference in Natural Language},
	url = {https://aclanthology.org/2020.findings-emnlp.418},
	doi = {10/gm7592},
	shorttitle = {Thinking Like a Skeptic},
	abstract = {Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical {AI} and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible {NLI} (abbreviated δ-{NLI}), a dataset for defeasible inference in natural language. Defeasible {NLI} contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible {NLI}, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68\% of the time.},
	eventtitle = {Findings 2020},
	pages = {4661--4675},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Rudinger, Rachel and Shwartz, Vered and Hwang, Jena D. and Bhagavatula, Chandra and Forbes, Maxwell and Le Bras, Ronan and Smith, Noah A. and Choi, Yejin},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-05-29},
	date = {2020-11},
}

@misc{mccannNaturalLanguageDecathlon2018,
	title = {The Natural Language Decathlon: Multitask Learning as Question Answering},
	url = {http://arxiv.org/abs/1806.08730},
	shorttitle = {The Natural Language Decathlon},
	abstract = {Deep learning has improved performance on many natural language processing ({NLP}) tasks individually. However, general {NLP} models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon ({decaNLP}), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network ({MQAN}) jointly learns all tasks in {decaNLP} without any task-specific modules or parameters in the multitask setting. {MQAN} shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the {MQAN}'s multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for {decaNLP}, {MQAN} also achieves state of the art results on the {WikiSQL} semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for {decaNLP}.},
	number = {{arXiv}:1806.08730},
	publisher = {{arXiv}},
	author = {{McCann}, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	urldate = {2024-05-29},
	date = {2018-06-20},
	eprinttype = {arxiv},
	eprint = {1806.08730 [cs, stat]},
}

@misc{kociskyNarrativeQAReadingComprehension2017,
	title = {The {NarrativeQA} Reading Comprehension Challenge},
	url = {http://arxiv.org/abs/1712.07040},
	abstract = {Reading comprehension ({RC})---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess {RC} ability, in both artificial agents and children learning to read. However, existing {RC} datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of {RC}. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard {RC} models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.},
	number = {{arXiv}:1712.07040},
	publisher = {{arXiv}},
	author = {Kočiský, Tomáš and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, Gábor and Grefenstette, Edward},
	urldate = {2024-05-29},
	date = {2017-12-19},
	eprinttype = {arxiv},
	eprint = {1712.07040 [cs]},
}

@misc{keungMultilingualAmazonReviews2020,
	title = {The Multilingual Amazon Reviews Corpus},
	url = {http://arxiv.org/abs/2010.02573},
	abstract = {We present the Multilingual Amazon Reviews Corpus ({MARC}), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer {ID}, an anonymized product {ID}, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20\% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual {BERT} model on reviews data. We propose the use of mean absolute error ({MAE}) instead of classification accuracy for this task, since {MAE} accounts for the ordinal nature of the ratings.},
	number = {{arXiv}:2010.02573},
	publisher = {{arXiv}},
	author = {Keung, Phillip and Lu, Yichao and Szarvas, György and Smith, Noah A.},
	urldate = {2024-05-29},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02573 [cs]},
}

@misc{papernoLAMBADADatasetWord2016,
	title = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
	url = {http://arxiv.org/abs/1606.06031},
	shorttitle = {The {LAMBADA} dataset},
	abstract = {We introduce {LAMBADA}, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. {LAMBADA} is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on {LAMBADA}, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that {LAMBADA} exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1\% on this novel benchmark. We thus propose {LAMBADA} as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.},
	number = {{arXiv}:1606.06031},
	publisher = {{arXiv}},
	author = {Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernández, Raquel},
	urldate = {2024-05-29},
	date = {2016-06-20},
	eprinttype = {arxiv},
	eprint = {1606.06031 [cs]},
}

@misc{adelaniEffectDomainDiacritics2021,
	title = {The Effect of Domain and Diacritics in Yor{\textbackslash}`ub{\textbackslash}'a-English Neural Machine Translation},
	url = {http://arxiv.org/abs/2103.08647},
	abstract = {Massively multilingual machine translation ({MT}) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating {MT} models on low-resource pairs is often due to lack of standardized evaluation datasets. In this paper, we present {MENYO}-20k, the first multi-domain parallel corpus with a special focus on clean orthography for Yor{\textbackslash}`ub{\textbackslash}'a--English with standardized train-test splits for benchmarking. We provide several neural {MT} benchmarks and compare them to the performance of popular pre-trained (massively multilingual) {MT} models both for the heterogeneous test set and its subdomains. Since these pre-trained models use huge amounts of data with uncertain quality, we also analyze the effect of diacritics, a major characteristic of Yor{\textbackslash}`ub{\textbackslash}'a, in the training data. We investigate how and when this training condition affects the final quality and intelligibility of a translation. Our models outperform massively multilingual models such as Google (\$+8.7\$ {BLEU}) and Facebook M2M (\$+9.1\$ {BLEU}) when translating to Yor{\textbackslash}`ub{\textbackslash}'a, setting a high quality benchmark for future research.},
	number = {{arXiv}:2103.08647},
	publisher = {{arXiv}},
	author = {Adelani, David I. and Ruiter, Dana and Alabi, Jesujoba O. and Adebonojo, Damilola and Ayeni, Adesina and Adeyemi, Mofe and Awokoya, Ayodele and España-Bonet, Cristina},
	urldate = {2024-05-29},
	date = {2021-08-14},
	eprinttype = {arxiv},
	eprint = {2103.08647 [cs]},
}

@misc{guzmanFLoResEvaluationDatasets2019,
	title = {The {FLoRes} Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},
	url = {http://arxiv.org/abs/1902.01382},
	shorttitle = {The {FLoRes} Evaluation Datasets for Low-Resource Machine Translation},
	abstract = {For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the {FLoRes} evaluation datasets for Nepali-English and Sinhala-English, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource {MT}. Data and code to reproduce our experiments are available at https://github.com/facebookresearch/flores.},
	number = {{arXiv}:1902.01382},
	publisher = {{arXiv}},
	author = {Guzmán, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc'Aurelio},
	urldate = {2024-05-29},
	date = {2019-09-14},
	eprinttype = {arxiv},
	eprint = {1902.01382 [cs]},
}

@misc{sileoTasksourceDatasetHarmonization2023,
	title = {tasksource: A Dataset Harmonization Framework for Streamlined {NLP} Multi-Task Learning and Evaluation},
	url = {http://arxiv.org/abs/2301.05948},
	shorttitle = {tasksource},
	abstract = {The {HuggingFace} Datasets Hub hosts thousands of datasets, offering exciting opportunities for language model training and evaluation. However, datasets for a specific task type often have different schemas, making harmonization challenging. Multi-task training or evaluation necessitates manual work to fit data into task templates. Several initiatives independently tackle this issue by releasing harmonized datasets or providing harmonization codes to preprocess datasets into a consistent format. We identify patterns across previous preprocessing efforts, such as column name mapping and extracting specific sub-fields from structured data in a column. We then propose a structured annotation framework that ensures our annotations are fully exposed and not hidden within unstructured code. We release a dataset annotation framework and dataset annotations for more than 500 English tasks{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/sileod/tasksource\}\}. These annotations include metadata, such as the names of columns to be used as input or labels for all datasets, which can save time for future dataset preprocessing, regardless of whether our framework is utilized. We fine-tune a multi-task text encoder on all tasksource tasks, outperforming every publicly available text encoder of comparable size in an external evaluation.},
	number = {{arXiv}:2301.05948},
	publisher = {{arXiv}},
	author = {Sileo, Damien},
	urldate = {2024-05-29},
	date = {2023-05-16},
	eprinttype = {arxiv},
	eprint = {2301.05948 [cs]},
}

@misc{liuTestingAbilityLanguage2022,
	title = {Testing the Ability of Language Models to Interpret Figurative Language},
	url = {http://arxiv.org/abs/2204.12632},
	abstract = {Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in {NLP}, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-{QA}, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.},
	number = {{arXiv}:2204.12632},
	publisher = {{arXiv}},
	author = {Liu, Emmy and Cui, Chen and Zheng, Kenneth and Neubig, Graham},
	urldate = {2024-05-29},
	date = {2022-05-15},
	eprinttype = {arxiv},
	eprint = {2204.12632 [cs]},
}

@inproceedings{lalTellMeWhyDatasetAnswering2021,
	title = {{TellMeWhy}: A Dataset for Answering Why-Questions in Narratives},
	url = {http://arxiv.org/abs/2106.06132},
	doi = {10/gtwwvn},
	shorttitle = {{TellMeWhy}},
	abstract = {Answering questions about why characters perform certain actions is central to understanding and reasoning about narratives. Despite recent progress in {QA}, it is not clear if existing models have the ability to answer "why" questions that may require commonsense knowledge external to the input narrative. In this work, we introduce {TellMeWhy}, a new crowd-sourced dataset that consists of more than 30k questions and free-form answers concerning why characters in short narratives perform the actions described. For a third of this dataset, the answers are not present within the narrative. Given the limitations of automated evaluation for this task, we also present a systematized human evaluation interface for this dataset. Our evaluation of state-of-the-art models show that they are far below human performance on answering such questions. They are especially worse on questions whose answers are external to the narrative, thus providing a challenge for future {QA} and narrative understanding research.},
	pages = {596--610},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL}-{IJCNLP} 2021},
	author = {Lal, Yash Kumar and Chambers, Nathanael and Mooney, Raymond and Balasubramanian, Niranjan},
	urldate = {2024-05-29},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2106.06132 [cs]},
}

@article{machinesTaskOrientedDialogueDataflow2020,
	title = {Task-Oriented Dialogue as Dataflow Synthesis},
	volume = {8},
	issn = {2307-387X},
	url = {http://arxiv.org/abs/2009.11423},
	doi = {10/ghf3c9},
	abstract = {We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, {SMCalFlow}, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the {MultiWOZ} dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The {SMCalFlow} dataset and code for replicating experiments are available at https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.},
	pages = {556--571},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Machines, Semantic and Andreas, Jacob and Bufe, John and Burkett, David and Chen, Charles and Clausman, Josh and Crawford, Jean and Crim, Kate and {DeLoach}, Jordan and Dorner, Leah and Eisner, Jason and Fang, Hao and Guo, Alan and Hall, David and Hayes, Kristin and Hill, Kellie and Ho, Diana and Iwaszuk, Wendy and Jha, Smriti and Klein, Dan and Krishnamurthy, Jayant and Lanman, Theo and Liang, Percy and Lin, Christopher H. and Lintsbakh, Ilya and {McGovern}, Andy and Nisnevich, Aleksandr and Pauls, Adam and Petters, Dmitrij and Read, Brent and Roth, Dan and Roy, Subhro and Rusak, Jesse and Short, Beth and Slomin, Div and Snyder, Ben and Striplin, Stephon and Su, Yu and Tellman, Zachary and Thomson, Sam and Vorobev, Andrei and Witoszko, Izabela and Wolfe, Jason and Wray, Abby and Zhang, Yuchen and Zotov, Alexander},
	urldate = {2024-05-29},
	date = {2020-12},
	eprinttype = {arxiv},
	eprint = {2009.11423 [cs]},
}

@misc{zhouTemporalReasoningImplicit2021,
	title = {Temporal Reasoning on Implicit Events from Distant Supervision},
	url = {http://arxiv.org/abs/2010.12753},
	abstract = {We propose {TRACIE}, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events -- events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, {SYMTIME}, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. {SYMTIME} outperforms strong baseline systems on {TRACIE} by 5\%, and by 11\% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1\%-9\% on {MATRES}, an explicit event benchmark.},
	number = {{arXiv}:2010.12753},
	publisher = {{arXiv}},
	author = {Zhou, Ben and Richardson, Kyle and Ning, Qiang and Khot, Tushar and Sabharwal, Ashish and Roth, Dan},
	urldate = {2024-05-29},
	date = {2021-05-07},
	eprinttype = {arxiv},
	eprint = {2010.12753 [cs]},
}

@inproceedings{abdelaliAMARACorpusBuilding2014,
	location = {Reykjavik, Iceland},
	title = {The {AMARA} Corpus: Building Parallel Language Resources for the Educational Domain},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/877_Paper.pdf},
	shorttitle = {The {AMARA} Corpus},
	abstract = {This paper presents the {AMARA} corpus of on-line educational content: a new parallel corpus of educational video subtitles, multilingually aligned for 20 languages, i.e. 20 monolingual corpora and 190 parallel corpora. This corpus includes both resource-rich languages such as English and Arabic, and resource-poor languages such as Hindi and Thai. In this paper, we describe the gathering, validation, and preprocessing of a large collection of parallel, community-generated subtitles. Furthermore, we describe the methodology used to prepare the data for Machine Translation tasks. Additionally, we provide a document-level, jointly aligned development and test sets for 14 language pairs, designed for tuning and testing Machine Translation systems. We provide baseline results for these tasks, and highlight some of the challenges we face when building machine translation systems for educational content.},
	eventtitle = {{LREC} 2014},
	pages = {1856--1862},
	booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Abdelali, Ahmed and Guzman, Francisco and Sajjad, Hassan and Vogel, Stephan},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2014-05},
}

@inproceedings{habernalArgumentReasoningComprehension2018,
	title = {The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants},
	url = {http://arxiv.org/abs/1708.01425},
	doi = {10/ggv3dv},
	shorttitle = {The Argument Reasoning Comprehension Task},
	abstract = {Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.},
	pages = {1930--1940},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)},
	author = {Habernal, Ivan and Wachsmuth, Henning and Gurevych, Iryna and Stein, Benno},
	urldate = {2024-05-29},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1708.01425 [cs]},
}

@misc{minSyntacticDataAugmentation2020,
	title = {Syntactic Data Augmentation Increases Robustness to Inference Heuristics},
	url = {http://arxiv.org/abs/2004.11999},
	abstract = {Pretrained neural models such as {BERT}, when fine-tuned to perform natural language inference ({NLI}), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model's limitations, but rather by the paucity of crowdsourced {NLI} examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the {MNLI} corpus. The best-performing augmentation method, subject/object inversion, improved {BERT}'s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the {MNLI} test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes {BERT} to recruit abstract syntactic representations.},
	number = {{arXiv}:2004.11999},
	publisher = {{arXiv}},
	author = {Min, Junghyun and {McCoy}, R. Thomas and Das, Dipanjan and Pitler, Emily and Linzen, Tal},
	urldate = {2024-05-29},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {2004.11999 [cs]},
}

@misc{zellersSWAGLargeScaleAdversarial2018,
	title = {{SWAG}: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference},
	url = {http://arxiv.org/abs/1808.05326},
	shorttitle = {{SWAG}},
	abstract = {Given a partial description like "she opened the hood of the car," humans can reason about the situation and anticipate what might come next ("then, she examined the engine"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present {SWAG}, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering ({AF}), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88\%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.},
	number = {{arXiv}:1808.05326},
	publisher = {{arXiv}},
	author = {Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2018-08-15},
	eprinttype = {arxiv},
	eprint = {1808.05326 [cs]},
}

@misc{bjervaSubjQADatasetSubjectivity2020,
	title = {{SubjQA}: A Dataset for Subjectivity and Review Comprehension},
	url = {http://arxiv.org/abs/2004.14283},
	shorttitle = {{SubjQA}},
	abstract = {Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering ({QA}). We therefore investigate the relationship between subjectivity and {QA}, while developing a new dataset. We compare and contrast with analyses from previous work, and verify that findings regarding subjectivity still hold when using recently developed {NLP} architectures. We find that subjectivity is also an important feature in the case of {QA}, albeit with more intricate interactions between subjectivity and {QA} performance. For instance, a subjective question may or may not be associated with a subjective answer. We release an English {QA} dataset ({SubjQA}) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 distinct domains.},
	number = {{arXiv}:2004.14283},
	publisher = {{arXiv}},
	author = {Bjerva, Johannes and Bhutani, Nikita and Golshan, Behzad and Tan, Wang-Chiew and Augenstein, Isabelle},
	urldate = {2024-05-29},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2004.14283 [cs]},
}

@misc{hossainStimulatingCreativityFunLines2020,
	title = {Stimulating Creativity with {FunLines}: A Case Study of Humor Generation in Headlines},
	url = {http://arxiv.org/abs/2002.02031},
	shorttitle = {Stimulating Creativity with {FunLines}},
	abstract = {Building datasets of creative text, such as humor, is quite challenging. We introduce {FunLines}, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. {FunLines} makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. {FunLines} offers useful performance feedback, assisting players in getting better over time at generating and assessing humor, as our analysis shows. This helps to further increase the quality of the generated dataset. We show the effectiveness of this data by training humor classification models that outperform a previous benchmark, and we release this dataset to the public.},
	number = {{arXiv}:2002.02031},
	publisher = {{arXiv}},
	author = {Hossain, Nabil and Krumm, John and Sajed, Tanvir and Kautz, Henry},
	urldate = {2024-05-29},
	date = {2020-02-05},
	eprinttype = {arxiv},
	eprint = {2002.02031 [cs]},
}

@misc{nadeemStereoSetMeasuringStereotypical2020,
	title = {{StereoSet}: Measuring stereotypical bias in pretrained language models},
	url = {http://arxiv.org/abs/2004.09456},
	shorttitle = {{StereoSet}},
	abstract = {A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or Asians are bad drivers. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real world data, they are known to capture stereotypical biases. In order to assess the adverse effects of these models, it is important to quantify the bias captured in them. Existing literature on quantifying bias evaluates pretrained language models on a small set of artificially constructed bias-assessing sentences. We present {StereoSet}, a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion. We evaluate popular models like {BERT}, {GPT}-2, {RoBERTa}, and {XLNet} on our dataset and show that these models exhibit strong stereotypical biases. We also present a leaderboard with a hidden test set to track the bias of future language models at https://stereoset.mit.edu},
	number = {{arXiv}:2004.09456},
	publisher = {{arXiv}},
	author = {Nadeem, Moin and Bethke, Anna and Reddy, Siva},
	urldate = {2024-05-29},
	date = {2020-04-20},
	eprinttype = {arxiv},
	eprint = {2004.09456 [cs]},
}

@misc{cohanStructuralScaffoldsCitation2019,
	title = {Structural Scaffolds for Citation Intent Classification in Scientific Publications},
	url = {http://arxiv.org/abs/1904.01608},
	abstract = {Identifying the intent of a citation in scientific papers (e.g., background information, use of methods, comparing results) is critical for machine reading of individual publications and automated analysis of the scientific literature. We propose structural scaffolds, a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents. Our model achieves a new state-of-the-art on an existing {ACL} anthology dataset ({ACL}-{ARC}) with a 13.3\% absolute increase in F1 score, without relying on external linguistic resources or hand-engineered features as done in existing methods. In addition, we introduce a new dataset of citation intents ({SciCite}) which is more than five times larger and covers multiple scientific domains compared with existing datasets. Our code and data are available at: https://github.com/allenai/scicite.},
	number = {{arXiv}:1904.01608},
	publisher = {{arXiv}},
	author = {Cohan, Arman and Ammar, Waleed and van Zuylen, Madeleine and Cady, Field},
	urldate = {2024-05-29},
	date = {2019-09-30},
	eprinttype = {arxiv},
	eprint = {1904.01608 [cs]},
}

@misc{berzakSTARCStructuredAnnotations2020,
	title = {{STARC}: Structured Annotations for Reading Comprehension},
	url = {http://arxiv.org/abs/2004.14797},
	shorttitle = {{STARC}},
	abstract = {We present {STARC} (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions. Our framework introduces a principled structure for the answer choices and ties them to textual span annotations. The framework is implemented in {OneStopQA}, a new high-quality dataset for evaluation and analysis of reading comprehension in English. We use this dataset to demonstrate that {STARC} can be leveraged for a key new application for the development of {SAT}-like reading comprehension materials: automatic annotation quality probing via span ablation experiments. We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability. Our experiments also reveal that the standard multiple choice dataset in {NLP}, {RACE}, is limited in its ability to measure reading comprehension. 47\% of its questions can be guessed by machines without accessing the passage, and 18\% are unanimously judged by humans as not having a unique correct answer. {OneStopQA} provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.},
	number = {{arXiv}:2004.14797},
	publisher = {{arXiv}},
	author = {Berzak, Yevgeni and Malmaud, Jonathan and Levy, Roger},
	urldate = {2024-05-29},
	date = {2020-04-30},
	eprinttype = {arxiv},
	eprint = {2004.14797 [cs]},
}

@inproceedings{stanovskySpotOddMan2018,
	location = {Brussels, Belgium},
	title = {Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources},
	url = {https://aclanthology.org/D18-1182},
	doi = {10/gtwwfw},
	shorttitle = {Spot the Odd Man Out},
	abstract = {We propose Odd-Man-Out, a novel task which aims to test different properties of word representations. An Odd-Man-Out puzzle is composed of 5 (or more) words, and requires the system to choose the one which does not belong with the others. We show that this simple setup is capable of teasing out various properties of different popular lexical resources (like {WordNet} and pre-trained word embeddings), while being intuitive enough to annotate on a large scale. In addition, we propose a novel technique for training multi-prototype word representations, based on unsupervised clustering of {ELMo} embeddings, and show that it surpasses all other representations on all Odd-Man-Out collections.},
	eventtitle = {{EMNLP} 2018},
	pages = {1533--1542},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Stanovsky, Gabriel and Hopkins, Mark},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	urldate = {2024-05-29},
	date = {2018-10},
}

@inproceedings{maniatiSOMOSSamsungOpen2022,
	title = {{SOMOS}: The Samsung Open {MOS} Dataset for the Evaluation of Neural Text-to-Speech Synthesis},
	url = {http://arxiv.org/abs/2204.03040},
	doi = {10/gtwwvt},
	shorttitle = {{SOMOS}},
	abstract = {In this work, we present the {SOMOS} dataset, the first large-scale mean opinion scores ({MOS}) dataset consisting of solely neural text-to-speech ({TTS}) samples. It can be employed to train automatic {MOS} prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the {LJ} Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 {TTS} systems including vanilla neural acoustic models as well as models which allow prosodic variations. An {LPCNet} vocoder is used for all systems, so that the samples' variation depends only on the acoustic models. The synthesized utterances provide balanced and adequate domain and length coverage. We collect {MOS} naturalness evaluations on 3 English Amazon Mechanical Turk locales and share practices leading to reliable crowdsourced annotations for this task. We provide baseline results of state-of-the-art {MOS} prediction models on the {SOMOS} dataset and show the limitations that such models face when assigned to evaluate {TTS} utterances.},
	pages = {2388--2392},
	booktitle = {Interspeech 2022},
	author = {Maniati, Georgia and Vioni, Alexandra and Ellinas, Nikolaos and Nikitaras, Karolos and Klapsas, Konstantinos and Sung, June Sig and Jho, Gunu and Chalamandaris, Aimilios and Tsiakoulis, Pirros},
	urldate = {2024-05-29},
	date = {2022-09-18},
	eprinttype = {arxiv},
	eprint = {2204.03040 [cs, eess]},
}

@misc{mirzaeeSpartQATextualQuestion2021,
	title = {{SpartQA}: : A Textual Question Answering Benchmark for Spatial Reasoning},
	url = {http://arxiv.org/abs/2104.05832},
	shorttitle = {{SpartQA}},
	abstract = {This paper proposes a question-answering ({QA}) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models ({LM}). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding {QA} pairs. Experiments show that further pretraining {LMs} on these automatically generated data significantly improves {LMs}' capability on spatial understanding, which in turn helps to better solve two external datasets, {bAbI}, and {boolQ}. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.},
	number = {{arXiv}:2104.05832},
	publisher = {{arXiv}},
	author = {Mirzaee, Roshanak and Faghihi, Hossein Rajaby and Ning, Qiang and Kordjmashidi, Parisa},
	urldate = {2024-05-29},
	date = {2021-04-12},
	eprinttype = {arxiv},
	eprint = {2104.05832 [cs]},
}

@misc{elgoharySpeakYourParser2020,
	title = {Speak to your Parser: Interactive Text-to-{SQL} with Natural Language Feedback},
	url = {http://arxiv.org/abs/2005.02539},
	shorttitle = {Speak to your Parser},
	abstract = {We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to {SQL} systems and construct, {SPLASH}, a dataset of utterances, incorrect {SQL} interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5\%, our best model achieves only 25.1\%, which leaves a large gap for improvement in future research. {SPLASH} is publicly available at https://aka.ms/Splash\_dataset.},
	number = {{arXiv}:2005.02539},
	publisher = {{arXiv}},
	author = {Elgohary, Ahmed and Hosseini, Saghar and Awadallah, Ahmed Hassan},
	urldate = {2024-05-29},
	date = {2020-06-01},
	eprinttype = {arxiv},
	eprint = {2005.02539 [cs]},
}

@misc{kimSODAMillionscaleDialogue2023,
	title = {{SODA}: Million-scale Dialogue Distillation with Social Commonsense Contextualization},
	url = {http://arxiv.org/abs/2212.10465},
	shorttitle = {{SODA}},
	abstract = {Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present {SODA}: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in {SODA} are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using {SODA}, we train {COSMO}: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., {GODEL}, {BlenderBot}-1, Koala, Vicuna). Experiments reveal {COSMO} is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.},
	number = {{arXiv}:2212.10465},
	publisher = {{arXiv}},
	author = {Kim, Hyunwoo and Hessel, Jack and Jiang, Liwei and West, Peter and Lu, Ximing and Yu, Youngjae and Zhou, Pei and Bras, Ronan Le and Alikhani, Malihe and Kim, Gunhee and Sap, Maarten and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2023-10-23},
	eprinttype = {arxiv},
	eprint = {2212.10465 [cs]},
}

@misc{wrightSemiSupervisedExaggerationDetection2021,
	title = {Semi-Supervised Exaggeration Detection of Health Science Press Releases},
	url = {http://arxiv.org/abs/2108.13493},
	abstract = {Public trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of exaggeration detection in science communication. While there are an abundance of scientific papers and popular media articles written about them, very rarely do the articles include a direct link to the original paper, making data collection challenging. We address this by curating a set of labeled press release/abstract pairs from existing expert annotated studies on exaggeration in press releases of scientific papers suitable for benchmarking the performance of machine learning models on the task. Using limited data from this and previous studies on exaggeration detection in science, we introduce {MT}-{PET}, a multi-task version of Pattern Exploiting Training ({PET}), which leverages knowledge from complementary cloze-style {QA} tasks to improve few-shot learning. We demonstrate that {MT}-{PET} outperforms {PET} and supervised learning both when data is limited, as well as when there is an abundance of data for the main task.},
	number = {{arXiv}:2108.13493},
	publisher = {{arXiv}},
	author = {Wright, Dustin and Augenstein, Isabelle},
	urldate = {2024-05-29},
	date = {2021-08-30},
	eprinttype = {arxiv},
	eprint = {2108.13493 [cs]},
}

@misc{sapSocialBiasFrames2020,
	title = {Social Bias Frames: Reasoning about Social and Power Implications of Language},
	url = {http://arxiv.org/abs/1911.03891},
	shorttitle = {Social Bias Frames},
	abstract = {Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that "we shouldn't lower our standards to hire more women," most listeners will infer the implicature intended by the speaker -- that "women (candidates) are less qualified." Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80\% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.},
	number = {{arXiv}:1911.03891},
	publisher = {{arXiv}},
	author = {Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A. and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2020-04-23},
	eprinttype = {arxiv},
	eprint = {1911.03891 [cs]},
}

@misc{schmittSherLIiCTypedEventFocused2019,
	title = {{SherLIiC}: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference},
	url = {http://arxiv.org/abs/1906.01393},
	shorttitle = {{SherLIiC}},
	abstract = {We present {SherLIiC}, a testbed for lexical inference in context ({LIiC}), consisting of 3985 manually annotated inference rule candidates ({InfCands}), accompanied by (i) {\textasciitilde}960k unlabeled {InfCands}, and (ii) {\textasciitilde}190k typed textual relations between Freebase entities extracted from the large entity-linked corpus {ClueWeb}09. Each {InfCand} consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types. Due to our candidate selection process based on strong distributional evidence, {SherLIiC} is much harder than existing testbeds because distributional evidence is of little utility in the classification of {InfCands}. We also show that, due to its construction, many of {SherLIiC}'s correct {InfCands} are novel and missing from existing rule bases. We evaluate a number of strong baselines on {SherLIiC}, ranging from semantic vector space models to state of the art neural models of natural language inference ({NLI}). We show that {SherLIiC} poses a tough challenge to existing {NLI} systems.},
	number = {{arXiv}:1906.01393},
	publisher = {{arXiv}},
	author = {Schmitt, Martin and Schütze, Hinrich},
	urldate = {2024-05-29},
	date = {2019-06-04},
	eprinttype = {arxiv},
	eprint = {1906.01393 [cs]},
}

@misc{conneauSentEvalEvaluationToolkit2018,
	title = {{SentEval}: An Evaluation Toolkit for Universal Sentence Representations},
	url = {http://arxiv.org/abs/1803.05449},
	shorttitle = {{SentEval}},
	abstract = {We introduce {SentEval}, a toolkit for evaluating the quality of universal sentence representations. {SentEval} encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to provide a fairer, less cumbersome and more centralized way for evaluating sentence representations.},
	number = {{arXiv}:1803.05449},
	publisher = {{arXiv}},
	author = {Conneau, Alexis and Kiela, Douwe},
	urldate = {2024-05-29},
	date = {2018-03-14},
	eprinttype = {arxiv},
	eprint = {1803.05449 [cs]},
}

@inproceedings{cerSemEval2017TaskSemantic2017,
	title = {{SemEval}-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation},
	url = {http://arxiv.org/abs/1708.00055},
	doi = {10/gjsnzr},
	shorttitle = {{SemEval}-2017 Task 1},
	abstract = {Semantic Textual Similarity ({STS}) measures the meaning similarity of sentences. Applications include machine translation ({MT}), summarization, generation, question answering ({QA}), short answer grading, semantic search, dialog and conversational systems. The {STS} shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring {MT} quality estimation ({MTQE}) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the {STS} Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English {STS} shared task data (2012-2017).},
	pages = {1--14},
	booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation           ({SemEval}-2017)},
	author = {Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Iñigo and Specia, Lucia},
	urldate = {2024-05-29},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1708.00055 [cs]},
}

@inproceedings{hopkinsSemEval2019Task102019,
	location = {Minneapolis, Minnesota, {USA}},
	title = {{SemEval}-2019 Task 10: Math Question Answering},
	url = {https://aclanthology.org/S19-2153},
	doi = {10/gtwwf2},
	shorttitle = {{SemEval}-2019 Task 10},
	abstract = {We report on the {SemEval} 2019 task on math question answering. We provided a question set derived from Math {SAT} practice exams, including 2778 training questions and 1082 test questions. For a significant subset of these questions, we also provided {SMT}-{LIB} logical form annotations and an interpreter that could solve these logical forms. Systems were evaluated based on the percentage of correctly answered questions. The top system correctly answered 45\% of the test questions, a considerable improvement over the 17\% random guessing baseline.},
	eventtitle = {{SemEval} 2019},
	pages = {893--899},
	booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
	publisher = {Association for Computational Linguistics},
	author = {Hopkins, Mark and Le Bras, Ronan and Petrescu-Prahova, Cristian and Stanovsky, Gabriel and Hajishirzi, Hannaneh and Koncel-Kedziorski, Rik},
	editor = {May, Jonathan and Shutova, Ekaterina and Herbelot, Aurelie and Zhu, Xiaodan and Apidianaki, Marianna and Mohammad, Saif M.},
	urldate = {2024-05-29},
	date = {2019-06},
}

@inproceedings{chatterjeeSemEval2019TaskEmoContext2019,
	location = {Minneapolis, Minnesota, {USA}},
	title = {{SemEval}-2019 Task 3: {EmoContext} Contextual Emotion Detection in Text},
	url = {https://aclanthology.org/S19-2005},
	doi = {10/gg28qz},
	shorttitle = {{SemEval}-2019 Task 3},
	abstract = {In this paper, we present the {SemEval}-2019 Task 3 - {EmoContext}: Contextual Emotion Detection in Text. Lack of facial expressions and voice modulations make detecting emotions in text a challenging problem. For instance, as humans, on reading “Why don't you ever text me!” we can either interpret it as a sad or angry emotion and the same ambiguity exists for machines. However, the context of dialogue can prove helpful in detection of the emotion. In this task, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others. To facilitate the participation in this task, textual dialogues from user interaction with a conversational agent were taken and annotated for emotion classes after several data processing steps. A training data set of 30160 dialogues, and two evaluation data sets, Test1 and Test2, containing 2755 and 5509 dialogues respectively were released to the participants. A total of 311 teams made submissions to this task. The final leader-board was evaluated on Test2 data set, and the highest ranked submission achieved 79.59 micro-averaged F1 score. Our analysis of systems submitted to the task indicate that Bi-directional {LSTM} was the most common choice of neural architecture used, and most of the systems had the best performance for the Sad emotion class, and the worst for the Happy emotion class.},
	eventtitle = {{SemEval} 2019},
	pages = {39--48},
	booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
	publisher = {Association for Computational Linguistics},
	author = {Chatterjee, Ankush and Narahari, Kedhar Nath and Joshi, Meghana and Agrawal, Puneet},
	editor = {May, Jonathan and Shutova, Ekaterina and Herbelot, Aurelie and Zhu, Xiaodan and Apidianaki, Marianna and Mohammad, Saif M.},
	urldate = {2024-05-29},
	date = {2019-06},
}

@inproceedings{mohammadSemEval2018TaskAffect2018,
	location = {New Orleans, Louisiana},
	title = {{SemEval}-2018 Task 1: Affect in Tweets},
	url = {https://aclanthology.org/S18-1001},
	doi = {10/ghm4q9},
	shorttitle = {{SemEval}-2018 Task 1},
	abstract = {We present the {SemEval}-2018 Task 1: Affect in Tweets, which includes an array of subtasks on inferring the affectual state of a person from their tweet. For each task, we created labeled data from English, Arabic, and Spanish tweets. The individual tasks are: 1. emotion intensity regression, 2. emotion intensity ordinal classification, 3. valence (sentiment) regression, 4. valence ordinal classification, and 5. emotion classification. Seventy-five teams (about 200 team members) participated in the shared task. We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful. We also analyze systems for consistent bias towards a particular race or gender. The data is made freely available to further improve our understanding of how people convey emotions through language.},
	eventtitle = {{SemEval} 2018},
	pages = {1--17},
	booktitle = {Proceedings of the 12th International Workshop on Semantic Evaluation},
	publisher = {Association for Computational Linguistics},
	author = {Mohammad, Saif and Bravo-Marquez, Felipe and Salameh, Mohammad and Kiritchenko, Svetlana},
	editor = {Apidianaki, Marianna and Mohammad, Saif M. and May, Jonathan and Shutova, Ekaterina and Bethard, Steven and Carpuat, Marine},
	urldate = {2024-05-29},
	date = {2018-06},
}

@inproceedings{vanheeSemEval2018TaskIrony2018,
	location = {New Orleans, Louisiana},
	title = {{SemEval}-2018 Task 3: Irony Detection in English Tweets},
	url = {https://aclanthology.org/S18-1005},
	doi = {10/ggwcjx},
	shorttitle = {{SemEval}-2018 Task 3},
	abstract = {This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any) is expressed (Task B). The ironic tweets were collected using irony-related hashtags (i.e. \#irony, \#sarcasm, \#not) and were subsequently manually annotated to minimise the amount of noise in the corpus. Prior to distributing the data, hashtags that were used to collect the tweets were removed from the corpus. For both tasks, a training corpus of 3,834 tweets was provided, as well as a test set containing 784 tweets. Our shared tasks received submissions from 43 teams for the binary classification Task A and from 31 teams for the multiclass Task B. The highest classification scores obtained for both subtasks are respectively F1= 0.71 and F1= 0.51 and demonstrate that fine-grained irony classification is much more challenging than binary irony detection.},
	eventtitle = {{SemEval} 2018},
	pages = {39--50},
	booktitle = {Proceedings of the 12th International Workshop on Semantic Evaluation},
	publisher = {Association for Computational Linguistics},
	author = {Van Hee, Cynthia and Lefever, Els and Hoste, Véronique},
	editor = {Apidianaki, Marianna and Mohammad, Saif M. and May, Jonathan and Shutova, Ekaterina and Bethard, Steven and Carpuat, Marine},
	urldate = {2024-05-29},
	date = {2018-06},
}

@inproceedings{wangSemEval2020TaskCommonsense2020,
	location = {Barcelona (online)},
	title = {{SemEval}-2020 Task 4: Commonsense Validation and Explanation},
	url = {https://aclanthology.org/2020.semeval-1.39},
	doi = {10/gtwwft},
	shorttitle = {{SemEval}-2020 Task 4},
	abstract = {In this paper, we present {SemEval}-2020 Task 4, Commonsense Validation and Explanation ({ComVE}), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons. Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not. The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense. In the third subtask, a participating system needs to generate the reason automatically. 39 teams submitted their valid systems to at least one subtask. For Subtask A and Subtask B, top-performing teams have achieved results closed to human performance. However, for Subtask C, there is still a considerable gap between system and human performance. The dataset used in our task can be found at https://github.com/wangcunxiang/{SemEval}2020-Task4-Commonsense-Validation-and-Explanation.},
	eventtitle = {{SemEval} 2020},
	pages = {307--321},
	booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},
	publisher = {International Committee for Computational Linguistics},
	author = {Wang, Cunxiang and Liang, Shuailong and Jin, Yili and Wang, Yilong and Zhu, Xiaodan and Zhang, Yue},
	editor = {Herbelot, Aurelie and Zhu, Xiaodan and Palmer, Alexis and Schneider, Nathan and May, Jonathan and Shutova, Ekaterina},
	urldate = {2024-05-29},
	date = {2020-12},
}

@misc{hendrickxSemEval2010TaskMultiWay2019,
	title = {{SemEval}-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals},
	url = {http://arxiv.org/abs/1911.10422},
	shorttitle = {{SemEval}-2010 Task 8},
	abstract = {In response to the continuing research interest in computational semantic analysis, we have proposed a new task for {SemEval}-2010: multi-way classification of mutually exclusive semantic relations between pairs of nominals. The task is designed to compare different approaches to the problem and to provide a standard testbed for future research. In this paper, we define the task, describe the creation of the datasets, and discuss the results of the participating 28 systems submitted by 10 teams.},
	number = {{arXiv}:1911.10422},
	publisher = {{arXiv}},
	author = {Hendrickx, Iris and Kim, Su Nam and Kozareva, Zornitsa and Nakov, Preslav and Séaghdha, Diarmuid Ó and Padó, Sebastian and Pennacchiotti, Marco and Romano, Lorenza and Szpakowicz, Stan},
	urldate = {2024-05-29},
	date = {2019-11-23},
	eprinttype = {arxiv},
	eprint = {1911.10422 [cs]},
}

@inproceedings{chenSeeingThingsDifferent2019,
	location = {Minneapolis, Minnesota},
	title = {Seeing Things from a Different Angle:Discovering Diverse Perspectives about Claims},
	url = {https://aclanthology.org/N19-1053},
	doi = {10/gkz76r},
	shorttitle = {Seeing Things from a Different Angle},
	abstract = {One key consequence of the information revolution is a significant increase and a contamination of our information supply. The practice of fact checking won't suffice to eliminate the biases in text data we observe, as the degree of factuality alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as “animals should have lawful rights”, and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a natural language understanding task, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct {PERSPECTRUM}, a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify our dataset. We use crowd-sourcing to filter out noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively. We provide a thorough analysis of the dataset to highlight key underlying language understanding challenges, and show that human baselines across multiple subtasks far outperform ma-chine baselines built upon state-of-the-art {NLP} techniques. This poses a challenge and opportunity for the {NLP} community to address.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {542--557},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Sihao and Khashabi, Daniel and Yin, Wenpeng and Callison-Burch, Chris and Roth, Dan},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	urldate = {2024-05-29},
	date = {2019-06},
}

@inproceedings{berantSemanticParsingFreebase2013,
	location = {Seattle, Washington, {USA}},
	title = {Semantic Parsing on Freebase from Question-Answer Pairs},
	url = {https://aclanthology.org/D13-1160},
	eventtitle = {{EMNLP} 2013},
	pages = {1533--1544},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	urldate = {2024-05-29},
	date = {2013-10},
}

@misc{sadatSciNLICorpusNatural2022,
	title = {{SciNLI}: A Corpus for Natural Language Inference on Scientific Text},
	url = {http://arxiv.org/abs/2203.06728},
	shorttitle = {{SciNLI}},
	abstract = {Existing Natural Language Inference ({NLI}) datasets, while being instrumental in the advancement of Natural Language Understanding ({NLU}) research, are not related to scientific text. In this paper, we introduce {SciNLI}, a large dataset for {NLI} that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on {NLP} and computational linguistics. Given that the text used in scientific literature differs vastly from the text used in everyday language both in terms of vocabulary and sentence structure, our dataset is well suited to serve as a benchmark for the evaluation of scientific {NLU} models. Our experiments show that {SciNLI} is harder to classify than the existing {NLI} datasets. Our best performing model with {XLNet} achieves a Macro F1 score of only 78.18\% and an accuracy of 78.23\% showing that there is substantial room for improvement.},
	number = {{arXiv}:2203.06728},
	publisher = {{arXiv}},
	author = {Sadat, Mobashir and Caragea, Cornelia},
	urldate = {2024-05-29},
	date = {2022-03-14},
	eprinttype = {arxiv},
	eprint = {2203.06728 [cs]},
}

@misc{lourieScruplesCorpusCommunity2021,
	title = {Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes},
	url = {http://arxiv.org/abs/2008.09094},
	shorttitle = {Scruples},
	abstract = {As {AI} systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics. We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks. A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.},
	number = {{arXiv}:2008.09094},
	publisher = {{arXiv}},
	author = {Lourie, Nicholas and Bras, Ronan Le and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2021-03-24},
	eprinttype = {arxiv},
	eprint = {2008.09094 [cs]},
}

@article{lowphansirikulScbmtenth2020LargeEnglishThai2022,
	title = {scb-mt-en-th-2020: A Large English-Thai Parallel Corpus},
	volume = {56},
	issn = {1574-020X, 1574-0218},
	url = {http://arxiv.org/abs/2007.03541},
	doi = {10/gtwwtw},
	shorttitle = {scb-mt-en-th-2020},
	abstract = {The primary objective of our work is to build a large-scale English-Thai dataset for machine translation. We construct an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources, namely news, Wikipedia articles, {SMS} messages, task-based dialogs, web-crawled data and government documents. Methodology for gathering data, building parallel texts and removing noisy sentence pairs are presented in a reproducible manner. We train machine translation models based on this dataset. Our models' performance are comparable to that of Google Translation {API} (as of May 2020) for Thai-English and outperform Google when the Open Parallel Corpus ({OPUS}) is included in the training data for both Thai-English and English-Thai translation. The dataset, pre-trained models, and source code to reproduce our work are available for public use.},
	pages = {477--499},
	number = {2},
	journaltitle = {Language Resources and Evaluation},
	shortjournal = {Lang Resources \& Evaluation},
	author = {Lowphansirikul, Lalita and Polpanumas, Charin and Rutherford, Attapol T. and Nutanong, Sarana},
	urldate = {2024-05-29},
	date = {2022-06},
	eprinttype = {arxiv},
	eprint = {2007.03541 [cs]},
}

@misc{rastogiSchemaGuidedDialogueState2020,
	title = {Schema-Guided Dialogue State Tracking Task at {DSTC}8},
	url = {http://arxiv.org/abs/2002.01359},
	abstract = {This paper gives an overview of the Schema-Guided Dialogue State Tracking task of the 8th Dialogue System Technology Challenge. The goal of this task is to develop dialogue state tracking models suitable for large-scale virtual assistants, with a focus on data-efficient joint modeling across domains and zero-shot generalization to new {APIs}. This task provided a new dataset consisting of over 16000 dialogues in the training set spanning 16 domains to highlight these challenges, and a baseline model capable of zero-shot generalization to new {APIs}. Twenty-five teams participated, developing a range of neural network models, exceeding the performance of the baseline model by a very high margin. The submissions incorporated a variety of pre-trained encoders and data augmentation techniques. This paper describes the task definition, dataset and evaluation methodology. We also summarize the approach and results of the submitted systems to highlight the overall trends in the state-of-the-art.},
	number = {{arXiv}:2002.01359},
	publisher = {{arXiv}},
	author = {Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
	urldate = {2024-05-29},
	date = {2020-02-02},
	eprinttype = {arxiv},
	eprint = {2002.01359 [cs]},
}

@misc{yangSciDTBDiscourseDependency2018,
	title = {{SciDTB}: Discourse Dependency {TreeBank} for Scientific Abstracts},
	url = {http://arxiv.org/abs/1806.03653},
	shorttitle = {{SciDTB}},
	abstract = {Annotation corpus for discourse relations benefits {NLP} tasks such as machine translation and question answering. In this paper, we present {SciDTB}, a domain-specific discourse treebank annotated on scientific articles. Different from widely-used {RST}-{DT} and {PDTB}, {SciDTB} uses dependency trees to represent discourse structure, which is flexible and simplified to some extent but do not sacrifice structural integrity. We discuss the labeling framework, annotation workflow and some statistics about {SciDTB}. Furthermore, our treebank is made as a benchmark for evaluating discourse dependency parsers, on which we provide several baselines as fundamental work.},
	number = {{arXiv}:1806.03653},
	publisher = {{arXiv}},
	author = {Yang, An and Li, Sujian},
	urldate = {2024-05-29},
	date = {2018-06-10},
	eprinttype = {arxiv},
	eprint = {1806.03653 [cs]},
}

@misc{sanyalRobustLREvaluatingRobustness2022,
	title = {{RobustLR}: Evaluating Robustness to Logical Perturbation in Deductive Reasoning},
	url = {http://arxiv.org/abs/2205.12598},
	shorttitle = {{RobustLR}},
	abstract = {Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in English natural language. While the progress is promising, it is currently unclear if these models indeed perform logical reasoning by understanding the underlying logical semantics in the language. To this end, we propose {RobustLR}, a suite of evaluation datasets that evaluate the robustness of these models to minimal logical edits in rulebases and some standard logical equivalence conditions. In our experiments with {RoBERTa} and T5, we find that the models trained in prior works do not perform consistently on the different perturbations in {RobustLR}, thus showing that the models are not robust to the proposed logical perturbations. Further, we find that the models find it especially hard to learn logical negation and disjunction operators. Overall, using our evaluation sets, we demonstrate some shortcomings of the deductive reasoning-based language models, which can eventually help towards designing better models for logical reasoning over natural language. All the datasets and code base have been made publicly available.},
	number = {{arXiv}:2205.12598},
	publisher = {{arXiv}},
	author = {Sanyal, Soumya and Liao, Zeyi and Ren, Xiang},
	urldate = {2024-05-29},
	date = {2022-11-08},
	eprinttype = {arxiv},
	eprint = {2205.12598 [cs]},
}

@article{misraSarcasmDetectionUsing2018,
	title = {Sarcasm Detection using Hybrid Neural Network},
	url = {http://arxiv.org/abs/1908.07414},
	doi = {10/gtwwtc},
	abstract = {Sarcasm Detection has enjoyed great interest from the research community, however the task of predicting sarcasm in a text remains an elusive problem for machines. Past studies mostly make use of twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. To overcome these shortcoming, we introduce a new dataset which contains news headlines from a sarcastic news website and a real news website. Next, we propose a hybrid Neural Network architecture with attention mechanism which provides insights about what actually makes sentences sarcastic. Through experiments, we show that the proposed model improves upon the baseline by {\textasciitilde} 5\% in terms of classification accuracy.},
	author = {Misra, Rishabh and Arora, Prahal},
	urldate = {2024-05-29},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1908.07414 [cs, stat]},
}

@misc{mccoyRightWrongReasons2019,
	title = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
	url = {http://arxiv.org/abs/1902.01007},
	shorttitle = {Right for the Wrong Reasons},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference ({NLI}), the task of determining whether one sentence entails another. We hypothesize that statistical {NLI} models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called {HANS} (Heuristic Analysis for {NLI} Systems), which contains many examples where the heuristics fail. We find that models trained on {MNLI}, including {BERT}, a state-of-the-art model, perform very poorly on {HANS}, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in {NLI} systems, and that the {HANS} dataset can motivate and measure progress in this area},
	number = {{arXiv}:1902.01007},
	publisher = {{arXiv}},
	author = {{McCoy}, R. Thomas and Pavlick, Ellie and Linzen, Tal},
	urldate = {2024-05-29},
	date = {2019-06-24},
	eprinttype = {arxiv},
	eprint = {1902.01007 [cs]},
}

@inproceedings{derczynskiResultsWNUT2017Shared2017,
	location = {Copenhagen, Denmark},
	title = {Results of the {WNUT}2017 Shared Task on Novel and Emerging Entity Recognition},
	url = {https://aclanthology.org/W17-4418},
	doi = {10/ggwb5p},
	abstract = {This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet “so.. kktny in 30 mins⁈” – even human experts find the entity `kktny' hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities. The task as described in this paper evaluated the ability of participating entries to detect and classify novel and emerging named entities in noisy text.},
	eventtitle = {{WNUT} 2017},
	pages = {140--147},
	booktitle = {Proceedings of the 3rd Workshop on Noisy User-generated Text},
	publisher = {Association for Computational Linguistics},
	author = {Derczynski, Leon and Nichols, Eric and van Erp, Marieke and Limsopatham, Nut},
	editor = {Derczynski, Leon and Xu, Wei and Ritter, Alan and Baldwin, Tim},
	urldate = {2024-05-29},
	date = {2017-09},
}

@inproceedings{socherRecursiveDeepModels2013,
	location = {Seattle, Washington, {USA}},
	title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
	url = {https://aclanthology.org/D13-1170},
	eventtitle = {{EMNLP} 2013},
	pages = {1631--1642},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	urldate = {2024-05-29},
	date = {2013-10},
}

@misc{kumarRealisticDataAugmentation2022,
	title = {Realistic Data Augmentation Framework for Enhancing Tabular Reasoning},
	url = {http://arxiv.org/abs/2210.12795},
	abstract = {Existing approaches to constructing training data for Natural Language Inference ({NLI}) tasks, such as for semi-structured table reasoning, are either via crowdsourcing or fully automatic methods. However, the former is expensive and time-consuming and thus limits scale, and the latter often produces naive examples that may lack complex reasoning. This paper develops a realistic semi-automated framework for data augmentation for tabular inference. Instead of manually generating a hypothesis for each table, our methodology generates hypothesis templates transferable to similar tables. In addition, our framework entails the creation of rational counterfactual tables based on human written logical constraints and premise paraphrasing. For our case study, we use the {InfoTabs}, which is an entity-centric tabular inference dataset. We observed that our framework could generate human-like tabular inference examples, which could benefit training data augmentation, especially in the scenario with limited supervision.},
	number = {{arXiv}:2210.12795},
	publisher = {{arXiv}},
	author = {Kumar, Dibyakanti and Gupta, Vivek and Sharma, Soumya and Zhang, Shuo},
	urldate = {2024-05-29},
	date = {2022-10-23},
	eprinttype = {arxiv},
	eprint = {2210.12795 [cs]},
}

@misc{zhangReCoRDBridgingGap2018,
	title = {{ReCoRD}: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},
	url = {http://arxiv.org/abs/1810.12885},
	shorttitle = {{ReCoRD}},
	abstract = {We present a large-scale dataset, {ReCoRD}, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art {MRC} systems fall far behind human performance. {ReCoRD} represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. {ReCoRD} is available at http://nlp.jhu.edu/record.},
	number = {{arXiv}:1810.12885},
	publisher = {{arXiv}},
	author = {Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
	urldate = {2024-05-29},
	date = {2018-10-30},
	eprinttype = {arxiv},
	eprint = {1810.12885 [cs]},
}

@inproceedings{bienRecipeNLGCookingRecipes2020,
	location = {Dublin, Ireland},
	title = {{RecipeNLG}: A Cooking Recipes Dataset for Semi-Structured Text Generation},
	url = {https://aclanthology.org/2020.inlg-1.4},
	doi = {10/gtwwfs},
	shorttitle = {{RecipeNLG}},
	abstract = {Semi-structured text generation is a non-trivial problem. Although last years have brought lots of improvements in natural language generation, thanks to the development of neural models trained on large scale datasets, these approaches still struggle with producing structured, context- and commonsense-aware texts. Moreover, it is not clear how to evaluate the quality of generated texts. To address these problems, we introduce {RecipeNLG} – a novel dataset of cooking recipes. We discuss the data collection process and the relation between the semi-structured texts and cooking recipes. We use the dataset to approach the problem of generating recipes. Finally, we make use of multiple metrics to evaluate the generated recipes.},
	eventtitle = {{INLG} 2020},
	pages = {22--28},
	booktitle = {Proceedings of the 13th International Conference on Natural Language Generation},
	publisher = {Association for Computational Linguistics},
	author = {Bień, Michał and Gilski, Michał and Maciejewska, Martyna and Taisner, Wojciech and Wisniewski, Dawid and Lawrynowicz, Agnieszka},
	editor = {Davis, Brian and Graham, Yvette and Kelleher, John and Sripada, Yaji},
	urldate = {2024-05-29},
	date = {2020-12},
}

@inproceedings{sapRecollectionImaginationExploring2020,
	location = {Online},
	title = {Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models},
	url = {https://aclanthology.org/2020.acl-main.178},
	doi = {10/gsgvg9},
	shorttitle = {Recollection versus Imagination},
	abstract = {We investigate the use of {NLP} as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events. We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. Finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932). Our findings highlight the potential of using {NLP} tools to study the traces of human cognition in language.},
	eventtitle = {{ACL} 2020},
	pages = {1970--1978},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Horvitz, Eric and Choi, Yejin and Smith, Noah A. and Pennebaker, James},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2024-05-29},
	date = {2020-07},
}

@misc{lammQEDFrameworkDataset2020,
	title = {{QED}: A Framework and Dataset for Explanations in Question Answering},
	url = {http://arxiv.org/abs/2009.06354},
	shorttitle = {{QED}},
	abstract = {A question answering system that in addition to providing an answer provides an explanation of the reasoning that leads to that answer has potential advantages in terms of debuggability, extensibility and trust. To this end, we propose {QED}, a linguistically informed, extensible framework for explanations in question answering. A {QED} explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. We describe and publicly release an expert-annotated dataset of {QED} explanations built upon a subset of the Google Natural Questions dataset, and report baseline models on two tasks -- post-hoc explanation generation given an answer, and joint question answering and explanation generation. In the joint setting, a promising result suggests that training on a relatively small amount of {QED} data can improve question answering. In addition to describing the formal, language-theoretic motivations for the {QED} approach, we describe a large user study showing that the presence of {QED} explanations significantly improves the ability of untrained raters to spot errors made by a strong neural {QA} baseline.},
	number = {{arXiv}:2009.06354},
	publisher = {{arXiv}},
	author = {Lamm, Matthew and Palomaki, Jennimaria and Alberti, Chris and Andor, Daniel and Choi, Eunsol and Soares, Livio Baldini and Collins, Michael},
	urldate = {2024-05-29},
	date = {2020-09-08},
	eprinttype = {arxiv},
	eprint = {2009.06354 [cs]},
}

@misc{rodriguezQuizbowlCaseIncremental2021,
	title = {Quizbowl: The Case for Incremental Question Answering},
	url = {http://arxiv.org/abs/1904.04792},
	shorttitle = {Quizbowl},
	abstract = {Scholastic trivia competitions test knowledge and intelligence through mastery of question answering. Modern question answering benchmarks are one variant of the Turing test. Specifically, answering a set of questions as well as a human is a minimum bar towards demonstrating human-like intelligence. This paper makes the case that the format of one competition -- where participants can answer in the middle of hearing a question (incremental) -- better differentiates the skill between (human or machine) players. Additionally, merging a sequential decision-making sub-task with question answering ({QA}) provides a good setting for research in model calibration and opponent modeling. Thus, embedded in this task are three machine learning challenges: (1) factoid {QA} over thousands of Wikipedia-like answers, (2) calibration of the {QA} model's confidence scores, and (3) sequential decision-making that incorporates knowledge of the {QA} model, its calibration, and what the opponent may do. We make two contributions: (1) collecting and curating a large factoid {QA} dataset and an accompanying gameplay dataset, and (2) developing a model that addresses these three machine learning challenges. In addition to offline evaluation, we pitted our model against some of the most accomplished trivia players in the world in a series of exhibition matches spanning several years. Throughout this paper, we show that collaborations with the vibrant trivia community have contributed to the quality of our dataset, spawned new research directions, and doubled as an exciting way to engage the public with research in machine learning and natural language processing.},
	number = {{arXiv}:1904.04792},
	publisher = {{arXiv}},
	author = {Rodriguez, Pedro and Feng, Shi and Iyyer, Mohit and He, He and Boyd-Graber, Jordan},
	urldate = {2024-05-29},
	date = {2021-02-11},
	eprinttype = {arxiv},
	eprint = {1904.04792 [cs]},
}

@misc{choiQuACQuestionAnswering2018,
	title = {{QuAC} : Question Answering in Context},
	url = {http://arxiv.org/abs/1808.07036},
	shorttitle = {{QuAC}},
	abstract = {We present {QuAC}, a dataset for Question Answering in Context that contains 14K information-seeking {QA} dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. {QuAC} introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
	number = {{arXiv}:1808.07036},
	publisher = {{arXiv}},
	author = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
	urldate = {2024-05-29},
	date = {2018-08-27},
	eprinttype = {arxiv},
	eprint = {1808.07036 [cs]},
}

@inproceedings{heQuestionAnswerDrivenSemantic2015,
	location = {Lisbon, Portugal},
	title = {Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language},
	url = {https://aclanthology.org/D15-1076},
	doi = {10/ggwb85},
	shorttitle = {Question-Answer Driven Semantic Role Labeling},
	eventtitle = {{EMNLP} 2015},
	pages = {643--653},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
	editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
	urldate = {2024-05-29},
	date = {2015-09},
}

@misc{thukralProbingLanguageModels2021,
	title = {Probing Language Models for Understanding of Temporal Expressions},
	url = {http://arxiv.org/abs/2110.01113},
	abstract = {We present three Natural Language Inference ({NLI}) challenge sets that can evaluate {NLI} models on their understanding of temporal expressions. More specifically, we probe these models for three temporal properties: (a) the order between points in time, (b) the duration between two points in time, (c) the relation between the magnitude of times specified in different units. We find that although large language models fine-tuned on {MNLI} have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.},
	number = {{arXiv}:2110.01113},
	publisher = {{arXiv}},
	author = {Thukral, Shivin and Kukreja, Kunal and Kavouras, Christian},
	urldate = {2024-05-29},
	date = {2021-10-03},
	eprinttype = {arxiv},
	eprint = {2110.01113 [cs]},
}

@misc{aroca-ouellettePROSTPhysicalReasoning2021,
	title = {{PROST}: Physical Reasoning of Objects through Space and Time},
	url = {http://arxiv.org/abs/2106.03634},
	shorttitle = {{PROST}},
	abstract = {We present a new probing dataset named {PROST}: Physical Reasoning about Objects Through Space and Time. This dataset contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting. We conduct an extensive analysis which demonstrates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted (e.g., most {\textless}-{\textgreater} least), and increasing the amount of pretraining data and parameters only yields minimal improvements. These results provide support for the hypothesis that current pretrained models' ability to reason about physical interactions is inherently limited by a lack of real world experience. By highlighting these limitations, we hope to motivate the development of models with a human-like understanding of the physical world.},
	number = {{arXiv}:2106.03634},
	publisher = {{arXiv}},
	author = {Aroca-Ouellette, Stéphane and Paik, Cory and Roncone, Alessandro and Kann, Katharina},
	urldate = {2024-05-29},
	date = {2021-06-07},
	eprinttype = {arxiv},
	eprint = {2106.03634 [cs]},
}

@misc{hipsonPoKiLargeDataset2020,
	title = {{PoKi}: A Large Dataset of Poems by Children},
	url = {http://arxiv.org/abs/2004.06188},
	shorttitle = {{PoKi}},
	abstract = {Child language studies are crucial in improving our understanding of child well-being; especially in determining the factors that impact happiness, the sources of anxiety, techniques of emotion regulation, and the mechanisms to cope with stress. However, much of this research is stymied by the lack of availability of large child-written texts. We present a new corpus of child-written text, {PoKi}, which includes about 62 thousand poems written by children from grades 1 to 12. {PoKi} is especially useful in studying child language because it comes with information about the age of the child authors (their grade). We analyze the words in {PoKi} along several emotion dimensions (valence, arousal, dominance) and discrete emotions (anger, fear, sadness, joy). We use non-parametric regressions to model developmental differences from early childhood to late-adolescence. Results show decreases in valence that are especially pronounced during mid-adolescence, while arousal and dominance peaked during adolescence. Gender differences in the developmental trajectory of emotions are also observed. Our results support and extend the current state of emotion development research.},
	number = {{arXiv}:2004.06188},
	publisher = {{arXiv}},
	author = {Hipson, Will E. and Mohammad, Saif M.},
	urldate = {2024-05-29},
	date = {2020-05-02},
	eprinttype = {arxiv},
	eprint = {2004.06188 [cs]},
}

@misc{hossainPresidentVowsCut2019,
	title = {"President Vows to Cut {\textless}Taxes{\textgreater} Hair": Dataset and Analysis of Creative Text Editing for Humorous Headlines},
	url = {http://arxiv.org/abs/1906.00274},
	shorttitle = {"President Vows to Cut {\textless}Taxes{\textgreater} Hair"},
	abstract = {We introduce, release, and analyze a new dataset, called Humicroedit, for research in computational humor. Our publicly available data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. We carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline. The simple edits, usually just a single word replacement, mean we can apply straightforward analysis techniques to determine what makes our edited headlines humorous. We show how the data support classic theories of humor, such as incongruity, superiority, and setup/punchline. Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor.},
	number = {{arXiv}:1906.00274},
	publisher = {{arXiv}},
	author = {Hossain, Nabil and Krumm, John and Gamon, Michael},
	urldate = {2024-05-29},
	date = {2019-06-01},
	eprinttype = {arxiv},
	eprint = {1906.00274 [cs]},
}

@misc{zhouPredictingConceptNetPath2019,
	title = {Predicting {ConceptNet} Path Quality Using Crowdsourced Assessments of Naturalness},
	url = {http://arxiv.org/abs/1902.07831},
	doi = {10.1145/3308558.3313486},
	abstract = {In many applications, it is important to characterize the way in which two concepts are semantically related. Knowledge graphs such as {ConceptNet} provide a rich source of information for such characterizations by encoding relations between concepts as edges in a graph. When two concepts are not directly connected by an edge, their relationship can still be described in terms of the paths that connect them. Unfortunately, many of these paths are uninformative and noisy, which means that the success of applications that use such path features crucially relies on their ability to select high-quality paths. In existing applications, this path selection process is based on relatively simple heuristics. In this paper we instead propose to learn to predict path quality from crowdsourced human assessments. Since we are interested in a generic task-independent notion of quality, we simply ask human participants to rank paths according to their subjective assessment of the paths' naturalness, without attempting to define naturalness or steering the participants towards particular indicators of quality. We show that a neural network model trained on these assessments is able to predict human judgments on unseen paths with near optimal performance. Most notably, we find that the resulting path selection method is substantially better than the current heuristic approaches at identifying meaningful paths.},
	author = {Zhou, Yilun and Schockaert, Steven and Shah, Julie A.},
	urldate = {2024-05-29},
	date = {2019-02-20},
	eprinttype = {arxiv},
	eprint = {1902.07831 [cs]},
}

@misc{zampieriPredictingTypeTarget2019,
	title = {Predicting the Type and Target of Offensive Posts in Social Media},
	url = {http://arxiv.org/abs/1902.09666},
	abstract = {As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset ({OLID}), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between {OLID} and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on {OLID}.},
	number = {{arXiv}:1902.09666},
	publisher = {{arXiv}},
	author = {Zampieri, Marcos and Malmasi, Shervin and Nakov, Preslav and Rosenthal, Sara and Farra, Noura and Kumar, Ritesh},
	urldate = {2024-05-29},
	date = {2019-04-16},
	eprinttype = {arxiv},
	eprint = {1902.09666 [cs]},
}

@misc{khashabiParsiNLUSuiteLanguage2021,
	title = {{ParsiNLU}: A Suite of Language Understanding Challenges for Persian},
	url = {http://arxiv.org/abs/2012.06154},
	shorttitle = {{ParsiNLU}},
	abstract = {Despite the progress made in recent years in addressing natural language understanding ({NLU}) challenges, the majority of this progress remains to be concentrated on resource-rich languages like English. This work focuses on Persian language, one of the widely spoken languages in the world, and yet there are few {NLU} datasets available for this rich language. The availability of high-quality evaluation datasets is a necessity for reliable assessment of the progress on different {NLU} tasks and domains. We introduce {ParsiNLU}, the first benchmark in Persian language that includes a range of high-level tasks -- Reading Comprehension, Textual Entailment, etc. These datasets are collected in a multitude of ways, often involving manual annotations by native speakers. This results in over 14.5\$k\$ new instances across 6 distinct {NLU} tasks. Besides, we present the first results on state-of-the-art monolingual and multi-lingual pre-trained language-models on this benchmark and compare them with human performance, which provides valuable insights into our ability to tackle natural language understanding challenges in Persian. We hope {ParsiNLU} fosters further research and advances in Persian language understanding.},
	number = {{arXiv}:2012.06154},
	publisher = {{arXiv}},
	author = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and Gheini, Mozhdeh and Kabiri, Arman and Mahabadi, Rabeeh Karimi and Memarrast, Omid and Mosallanezhad, Ahmadreza and Noury, Erfan and Raji, Shahab and Rasooli, Mohammad Sadegh and Sadeghi, Sepideh and Azer, Erfan Sadeqi and Samghabadi, Niloofar Safi and Shafaei, Mahsa and Sheybani, Saber and Tazarv, Ali and Yaghoobzadeh, Yadollah},
	urldate = {2024-05-29},
	date = {2021-07-13},
	eprinttype = {arxiv},
	eprint = {2012.06154 [cs]},
}

@misc{hePARADENewDataset2020,
	title = {{PARADE}: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge},
	url = {http://arxiv.org/abs/2010.03725},
	shorttitle = {{PARADE}},
	abstract = {We present a new benchmark dataset called {PARADE} for paraphrase identification that requires specialized domain knowledge. {PARADE} contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on {PARADE}. For example, {BERT} after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. {PARADE} can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.},
	number = {{arXiv}:2010.03725},
	publisher = {{arXiv}},
	author = {He, Yun and Wang, Zhuoer and Zhang, Yin and Huang, Ruihong and Caverlee, James},
	urldate = {2024-05-29},
	date = {2020-10-07},
	eprinttype = {arxiv},
	eprint = {2010.03725 [cs]},
}

@misc{zhangPersonalizingDialogueAgents2018,
	title = {Personalizing Dialogue Agents: I have a dog, do you have pets too?},
	url = {http://arxiv.org/abs/1801.07243},
	shorttitle = {Personalizing Dialogue Agents},
	abstract = {Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.},
	number = {{arXiv}:1801.07243},
	publisher = {{arXiv}},
	author = {Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
	urldate = {2024-05-29},
	date = {2018-09-25},
	eprinttype = {arxiv},
	eprint = {1801.07243 [cs]},
}

@article{xuOptimizingStatisticalMachine2016,
	title = {Optimizing Statistical Machine Translation for Text Simplification},
	volume = {4},
	url = {https://aclanthology.org/Q16-1029},
	doi = {10/gf24vz},
	abstract = {Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.},
	pages = {401--415},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Xu, Wei and Napoles, Courtney and Pavlick, Ellie and Chen, Quanze and Callison-Burch, Chris},
	editor = {Lee, Lillian and Johnson, Mark and Toutanova, Kristina},
	urldate = {2024-05-29},
	date = {2016},
	note = {Place: Cambridge, {MA}
Publisher: {MIT} Press},
}

@inproceedings{tiedemannParallelDataTools2012,
	location = {Istanbul, Turkey},
	title = {Parallel Data, Tools and Interfaces in {OPUS}},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf},
	abstract = {This paper presents the current status of {OPUS}, a growing language resource of parallel corpora and related tools. The focus in {OPUS} is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.},
	eventtitle = {{LREC} 2012},
	pages = {2214--2218},
	booktitle = {Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Tiedemann, Jörg},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Doğan, Mehmet Uğur and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2012-05},
}

@inproceedings{filippovaOvercomingLackParallel2013,
	location = {Seattle, Washington, {USA}},
	title = {Overcoming the Lack of Parallel Data in Sentence Compression},
	url = {https://aclanthology.org/D13-1155},
	eventtitle = {{EMNLP} 2013},
	pages = {1481--1491},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Filippova, Katja and Altun, Yasemin},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	urldate = {2024-05-29},
	date = {2013-10},
}

@inproceedings{soaresParaPatMultiMillionSentences2020,
	location = {Marseille, France},
	title = {{ParaPat}: The Multi-Million Sentences Parallel Corpus of Patents Abstracts},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.465},
	shorttitle = {{ParaPat}},
	abstract = {The Google Patents is one of the main important sources of patents information. A striking characteristic is that many of its abstracts are presented in more than one language, thus making it a potential source of parallel corpora. This article presents the development of a parallel corpus from the open access Google Patents dataset in 74 language pairs, comprising more than 68 million sentences and 800 million tokens. Sentences were automatically aligned using the Hunalign algorithm for the largest 22 language pairs, while the others were abstract (i.e. paragraph) aligned. We demonstrate the capabilities of our corpus by training Neural Machine Translation ({NMT}) models for the main 9 language pairs, with a total of 18 models. Our parallel corpus is freely available in {TSV} format and with a {SQLite} database, with complementary information regarding patent metadata.},
	eventtitle = {{LREC} 2020},
	pages = {3769--3774},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Soares, Felipe and Stevenson, Mark and Bartolome, Diego and Zaretskaya, Anna},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2020-05},
}

@inproceedings{banonParaCrawlWebScaleAcquisition2020,
	location = {Online},
	title = {{ParaCrawl}: Web-Scale Acquisition of Parallel Corpora},
	url = {https://aclanthology.org/2020.acl-main.417},
	doi = {10/gp7jf9},
	shorttitle = {{ParaCrawl}},
	abstract = {We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.},
	eventtitle = {{ACL} 2020},
	pages = {4555--4567},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Bañón, Marta and Chen, Pinzhen and Haddow, Barry and Heafield, Kenneth and Hoang, Hieu and Esplà-Gomis, Miquel and Forcada, Mikel L. and Kamran, Amir and Kirefu, Faheem and Koehn, Philipp and Ortiz Rojas, Sergio and Pla Sempere, Leopoldo and Ramírez-Sánchez, Gema and Sarrías, Elsa and Strelec, Marek and Thompson, Brian and Waites, William and Wiggins, Dion and Zaragoza, Jaume},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2024-05-29},
	date = {2020-07},
}

@misc{ananthaOpenDomainQuestionAnswering2021,
	title = {Open-Domain Question Answering Goes Conversational via Question Rewriting},
	url = {http://arxiv.org/abs/2010.04898},
	abstract = {We introduce a new dataset for Question Rewriting in Conversational Context ({QReCC}), which contains 14K conversations with 80K question-answer pairs. The task in {QReCC} is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. {QReCC} provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering ({QA}) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain {QA}. Our results set the first baseline for the {QReCC} dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.},
	number = {{arXiv}:2010.04898},
	publisher = {{arXiv}},
	author = {Anantha, Raviteja and Vakulenko, Svitlana and Tu, Zhucheng and Longpre, Shayne and Pulman, Stephen and Chappidi, Srinivas},
	urldate = {2024-05-29},
	date = {2021-04-14},
	eprinttype = {arxiv},
	eprint = {2010.04898 [cs]},
}

@inproceedings{artetxeCrosslingualTransferabilityMonolingual2020a,
	title = {On the Cross-lingual Transferability of Monolingual Representations},
	url = {http://arxiv.org/abs/1910.11856},
	doi = {10/ghgndg},
	abstract = {State-of-the-art unsupervised multilingual models (e.g., multilingual {BERT}) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual {BERT} on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset ({XQuAD}). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release {XQuAD} as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from {SQuAD} v1.1 translated into ten languages by professional translators.},
	pages = {4623--4637},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	author = {Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
	urldate = {2024-05-29},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1910.11856 [cs]},
}

@inproceedings{lisonOpenSubtitles2016ExtractingLarge2016,
	location = {Portorož, Slovenia},
	title = {{OpenSubtitles}2016: Extracting Large Parallel Corpora from Movie and {TV} Subtitles},
	url = {https://aclanthology.org/L16-1147},
	shorttitle = {{OpenSubtitles}2016},
	abstract = {We present a new major release of the {OpenSubtitles} collection of parallel corpora. The release is compiled from a large database of movie and {TV} subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of {OCR} errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.},
	eventtitle = {{LREC} 2016},
	pages = {923--929},
	booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Lison, Pierre and Tiedemann, Jörg},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2016-05},
}

@inproceedings{mausamOpenLanguageLearning2012,
	location = {Jeju Island, Korea},
	title = {Open Language Learning for Information Extraction},
	url = {https://aclanthology.org/D12-1048},
	eventtitle = {{EMNLP} 2012},
	pages = {523--534},
	booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {{Mausam} and Schmitz, Michael and Soderland, Stephen and Bart, Robert and Etzioni, Oren},
	editor = {Tsujii, Jun'ichi and Henderson, James and Paşca, Marius},
	urldate = {2024-05-29},
	date = {2012-07},
}

@misc{truongNotAnotherNegation2022,
	title = {Not another Negation Benchmark: The {NaN}-{NLI} Test Suite for Sub-clausal Negation},
	url = {http://arxiv.org/abs/2210.03256},
	shorttitle = {Not another Negation Benchmark},
	abstract = {Negation is poorly captured by current language models, although the extent of this problem is not widely understood. We introduce a natural language inference ({NLI}) test suite to enable probing the capabilities of {NLP} methods, with the aim of understanding sub-clausal negation. The test suite contains premise--hypothesis pairs where the premise contains sub-clausal negation and the hypothesis is constructed by making minimal modifications to the premise in order to reflect different possible interpretations. Aside from adopting standard {NLI} labels, our test suite is systematically constructed under a rigorous linguistic framework. It includes annotation of negation types and constructions grounded in linguistic theory, as well as the operations used to construct hypotheses. This facilitates fine-grained analysis of model performance. We conduct experiments using pre-trained language models to demonstrate that our test suite is more challenging than existing benchmarks focused on negation, and show how our annotation supports a deeper understanding of the current {NLI} capabilities in terms of negation and quantification.},
	number = {{arXiv}:2210.03256},
	publisher = {{arXiv}},
	author = {Truong, Thinh Hung and Otmakhova, Yulia and Baldwin, Timothy and Cohn, Trevor and Lau, Jey Han and Verspoor, Karin},
	urldate = {2024-05-29},
	date = {2022-10-13},
	eprinttype = {arxiv},
	eprint = {2210.03256 [cs]},
}

@misc{labanNewsHeadlineGrouping2021,
	title = {News Headline Grouping as a Challenging {NLU} Task},
	url = {http://arxiv.org/abs/2105.05391},
	abstract = {Recent progress in Natural Language Understanding ({NLU}) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of {HeadLine} Grouping ({HLG}) and a corresponding dataset ({HLGD}) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group. On {HLGD}, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of {HeadLine} Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze high-performing models with consistency tests, and find that models are not consistent in their predictions, revealing modeling limits of current architectures.},
	number = {{arXiv}:2105.05391},
	publisher = {{arXiv}},
	author = {Laban, Philippe and Bandarkar, Lucas and Hearst, Marti A.},
	urldate = {2024-05-29},
	date = {2021-05-11},
	eprinttype = {arxiv},
	eprint = {2105.05391 [cs]},
}

@inproceedings{shahNeuralMachineTranslation2019,
	title = {Neural Machine Translation System of Indic Languages -- An Attention based Approach},
	url = {http://arxiv.org/abs/2002.02758},
	doi = {10/gtwwtk},
	abstract = {Neural machine translation ({NMT}) is a recent and effective technique which led to remarkable improvements in comparison of conventional machine translation techniques. Proposed neural machine translation model developed for the Gujarati language contains encoder-decoder with attention mechanism. In India, almost all the languages are originated from their ancestral language - Sanskrit. They are having inevitable similarities including lexical and named entity similarity. Translating into Indic languages is always be a challenging task. In this paper, we have presented the neural machine translation system ({NMT}) that can efficiently translate Indic languages like Hindi and Gujarati that together covers more than 58.49 percentage of total speakers in the country. We have compared the performance of our {NMT} model with automatic evaluation matrices such as {BLEU}, perplexity and {TER} matrix. The comparison of our network with Google translate is also presented where it outperformed with a margin of 6 {BLEU} score on English-Gujarati translation.},
	pages = {1--5},
	booktitle = {2019 Second International Conference on Advanced Computational and Communication Paradigms ({ICACCP})},
	author = {Shah, Parth and Bakrola, Vishvajit},
	urldate = {2024-05-29},
	date = {2019-02},
	eprinttype = {arxiv},
	eprint = {2002.02758 [cs, stat]},
}

@misc{borkanNuancedMetricsMeasuring2019,
	title = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification},
	url = {http://arxiv.org/abs/1903.04561},
	abstract = {Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier's score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models.},
	number = {{arXiv}:1903.04561},
	publisher = {{arXiv}},
	author = {Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	urldate = {2024-05-29},
	date = {2019-05-08},
	eprinttype = {arxiv},
	eprint = {1903.04561 [cs, stat]},
}

@misc{warstadtNeuralNetworkAcceptability2019,
	title = {Neural Network Acceptability Judgments},
	url = {http://arxiv.org/abs/1805.12471},
	abstract = {This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability ({CoLA}), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al (2016) on {CoLA}. Error-analysis on specific grammatical phenomena reveals that both Lau et al.'s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.},
	number = {{arXiv}:1805.12471},
	publisher = {{arXiv}},
	author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
	urldate = {2024-05-29},
	date = {2019-10-01},
	eprinttype = {arxiv},
	eprint = {1805.12471 [cs]},
}

@misc{gruskyNewsroomDatasetMillion2020,
	title = {Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies},
	url = {http://arxiv.org/abs/1804.11283},
	shorttitle = {Newsroom},
	abstract = {We present {NEWSROOM}, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in {NEWSROOM} summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.},
	number = {{arXiv}:1804.11283},
	publisher = {{arXiv}},
	author = {Grusky, Max and Naaman, Mor and Artzi, Yoav},
	urldate = {2024-05-29},
	date = {2020-05-17},
	eprinttype = {arxiv},
	eprint = {1804.11283 [cs]},
}

@misc{santusNineFeaturesRandom2016,
	title = {Nine Features in a Random Forest to Learn Taxonomical Semantic Relations},
	url = {http://arxiv.org/abs/1603.08702},
	abstract = {{ROOT}9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced {ROOT}13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, {ROOT}9 achieves an F1 score of 90.7\%, against a baseline of 57.2\% (vector cosine). When the classification is binary, {ROOT}9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7\% vs. 69.8\%, hypernyms-random 91.8\% vs. 64.1\% and co-hyponyms-random 97.8\% vs. 79.4\%. In order to compare the performance with the state-of-the-art, we have also evaluated {ROOT}9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though {ROOT}9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.},
	number = {{arXiv}:1603.08702},
	publisher = {{arXiv}},
	author = {Santus, Enrico and Lenci, Alessandro and Chiu, Tin-Shing and Lu, Qin and Huang, Chu-Ren},
	urldate = {2024-05-29},
	date = {2016-03-29},
	eprinttype = {arxiv},
	eprint = {1603.08702 [cs]},
}

@misc{liuNaturalLanguageInference2020,
	title = {Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts},
	url = {http://arxiv.org/abs/2011.04864},
	abstract = {Natural language inference ({NLI}) is a fundamental {NLP} task, investigating the entailment relationship between two texts. Popular {NLI} datasets present the task at sentence-level. While adequate for testing semantic representations, they fall short for testing contextual reasoning over long texts, which is a natural part of the human inference process. We introduce {ConTRoL}, a new dataset for {ConTextual} Reasoning over Long texts. Consisting of 8,325 expert-designed "context-hypothesis" pairs with gold labels, {ConTRoL} is a passage-level {NLI} dataset with a focus on complex contextual reasoning types such as logical reasoning. It is derived from competitive selection and recruitment test (verbal reasoning test) for police recruitment, with expert level quality. Compared with previous {NLI} benchmarks, the materials in {ConTRoL} are much more challenging, involving a range of reasoning types. Empirical results show that state-of-the-art language models perform by far worse than educated humans. Our dataset can also serve as a testing-set for downstream tasks like Checking Factual Correctness of Summaries.},
	number = {{arXiv}:2011.04864},
	publisher = {{arXiv}},
	author = {Liu, Hanmeng and Cui, Leyang and Liu, Jian and Zhang, Yue},
	urldate = {2024-05-29},
	date = {2020-11-09},
	eprinttype = {arxiv},
	eprint = {2011.04864 [cs]},
}

@misc{cuiMuTualDatasetMultiTurn2020,
	title = {{MuTual}: A Dataset for Multi-Turn Dialogue Reasoning},
	url = {http://arxiv.org/abs/2004.04494},
	shorttitle = {{MuTual}},
	abstract = {Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce {MuTual}, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams. Compared to previous benchmarks for non-task oriented dialogue systems, {MuTual} is much more challenging since it requires a model that can handle various reasoning problems. Empirical results show that state-of-the-art methods only reach 71\%, which is far behind the human performance of 94\%, indicating that there is ample room for improving reasoning ability. {MuTual} is available at https://github.com/Nealcly/{MuTual}.},
	number = {{arXiv}:2004.04494},
	publisher = {{arXiv}},
	author = {Cui, Leyang and Wu, Yu and Liu, Shujie and Zhang, Yue and Zhou, Ming},
	urldate = {2024-05-29},
	date = {2020-04-09},
	eprinttype = {arxiv},
	eprint = {2004.04494 [cs]},
}

@misc{vermaNeuralConversationalQA2020,
	title = {Neural Conversational {QA}: Learning to Reason v.s. Exploiting Patterns},
	url = {http://arxiv.org/abs/1909.03759},
	shorttitle = {Neural Conversational {QA}},
	abstract = {Neural Conversational {QA} tasks like {ShARC} require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the {ShARCQA} task, we found indications that the models learn spurious clues/patterns in the dataset. Furthermore, we show that a heuristic-based program designed to exploit these patterns can have performance comparable to that of the neural models. In this paper we share our findings about four types of patterns found in the {ShARC} corpus and describe how neural models exploit them. Motivated by the aforementioned findings, we create and share a modified dataset that has fewer spurious patterns, consequently allowing models to learn better.},
	number = {{arXiv}:1909.03759},
	publisher = {{arXiv}},
	author = {Verma, Nikhil and Sharma, Abhishek and Madan, Dhiraj and Contractor, Danish and Kumar, Harshit and Joshi, Sachindra},
	urldate = {2024-05-29},
	date = {2020-10-09},
	eprinttype = {arxiv},
	eprint = {1909.03759 [cs]},
}

@misc{laiNaturalLanguageInference2017,
	title = {Natural Language Inference from Multiple Premises},
	url = {http://arxiv.org/abs/1710.02925},
	abstract = {We define a novel textual entailment task that requires inference over multiple premise sentences. We present a new dataset for this task that minimizes trivial lexical inferences, emphasizes knowledge of everyday events, and presents a more challenging setting for textual entailment. We evaluate several strong neural baselines and analyze how the multiple premise task differs from standard textual entailment.},
	number = {{arXiv}:1710.02925},
	publisher = {{arXiv}},
	author = {Lai, Alice and Bisk, Yonatan and Hockenmaier, Julia},
	urldate = {2024-05-29},
	date = {2017-10-08},
	eprinttype = {arxiv},
	eprint = {1710.02925 [cs]},
}

@article{kwiatkowskiNaturalQuestionsBenchmark2019,
	title = {Natural Questions: A Benchmark for Question Answering Research},
	volume = {7},
	url = {https://aclanthology.org/Q19-1026},
	doi = {10/gf6gnc},
	shorttitle = {Natural Questions},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	pages = {452--466},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
	urldate = {2024-05-29},
	date = {2019},
	note = {Place: Cambridge, {MA}
Publisher: {MIT} Press},
}

@misc{baoMultiStepDeductiveReasoning2024,
	title = {Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation},
	url = {http://arxiv.org/abs/2207.14000},
	shorttitle = {Multi-Step Deductive Reasoning Over Natural Language},
	abstract = {Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by {DeepLogic}, an end-to-end model trained to perform inference on logic programs, we introduce {IMA}-{GloVe}-{GA}, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on {RNN} with a gated attention mechanism. We evaluate {IMA}-{GloVe}-{GA} on three datasets: {PARARULES}, {CONCEPTRULES} V1 and {CONCEPTRULES} V2. Experimental results show {DeepLogic} with gated attention can achieve higher test accuracy than {DeepLogic} and other {RNN} baseline models. Our model achieves better out-of-distribution generalisation than {RoBERTa}-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datasets, we develop {PARARULE}-Plus, a large dataset with more examples that require deeper reasoning steps. Experimental results show that the addition of {PARARULE}-Plus can increase the model's performance on examples requiring deeper reasoning depths. The source code and data are available at https://github.com/Strong-{AI}-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.},
	number = {{arXiv}:2207.14000},
	publisher = {{arXiv}},
	author = {Bao, Qiming and Peng, Alex Yuxuan and Hartill, Tim and Tan, Neset and Deng, Zhenyun and Witbrock, Michael and Liu, Jiamou},
	urldate = {2024-05-29},
	date = {2024-03-30},
	eprinttype = {arxiv},
	eprint = {2207.14000 [cs]},
}

@misc{dinanMultiDimensionalGenderBias2020,
	title = {Multi-Dimensional Gender Bias Classification},
	url = {http://arxiv.org/abs/2005.00614},
	abstract = {Machine learning models are trained to find patterns in data. {NLP} models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites. Distinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers. We show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models, detecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.},
	number = {{arXiv}:2005.00614},
	publisher = {{arXiv}},
	author = {Dinan, Emily and Fan, Angela and Wu, Ledell and Weston, Jason and Kiela, Douwe and Williams, Adina},
	urldate = {2024-05-29},
	date = {2020-05-01},
	eprinttype = {arxiv},
	eprint = {2005.00614 [cs]},
}

@misc{luanMultiTaskIdentificationEntities2018,
	title = {Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction},
	url = {http://arxiv.org/abs/1808.09602},
	abstract = {We introduce a multi-task setup of identifying and classifying entities, relations, and coreference clusters in scientific articles. We create {SciERC}, a dataset that includes annotations for all three tasks and develop a unified framework called Scientific Information Extractor ({SciIE}) for with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.},
	number = {{arXiv}:1808.09602},
	publisher = {{arXiv}},
	author = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
	urldate = {2024-05-29},
	date = {2018-08-28},
	eprinttype = {arxiv},
	eprint = {1808.09602 [cs]},
}

@inproceedings{zotovaMultilingualStanceDetection2020,
	location = {Marseille, France},
	title = {Multilingual Stance Detection in Tweets: The Catalonia Independence Corpus},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.171},
	shorttitle = {Multilingual Stance Detection in Tweets},
	abstract = {Stance detection aims to determine the attitude of a given text with respect to a specific topic or claim. While stance detection has been fairly well researched in the last years, most the work has been focused on English. This is mainly due to the relative lack of annotated data in other languages. The {TW}-10 referendum Dataset released at {IberEval} 2018 is a previous effort to provide multilingual stance-annotated data in Catalan and Spanish. Unfortunately, the {TW}-10 Catalan subset is extremely imbalanced. This paper addresses these issues by presenting a new multilingual dataset for stance detection in Twitter for the Catalan and Spanish languages, with the aim of facilitating research on stance detection in multilingual and cross-lingual settings. The dataset is annotated with stance towards one topic, namely, the ndependence of Catalonia. We also provide a semi-automatic method to annotate the dataset based on a categorization of Twitter users. We experiment on the new corpus with a number of supervised approaches, including linear classifiers and deep learning methods. Comparison of our new corpus with the with the {TW}-1O dataset shows both the benefits and potential of a well balanced corpus for multilingual and cross-lingual research on stance detection. Finally, we establish new state-of-the-art results on the {TW}-10 dataset, both for Catalan and Spanish.},
	eventtitle = {{LREC} 2020},
	pages = {1368--1375},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Zotova, Elena and Agerri, Rodrigo and Nuñez, Manuel and Rigau, German},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2020-05},
}

@inproceedings{xuMSRVTTLargeVideo2016,
	title = {{MSR}-{VTT}: A Large Video Description Dataset for Bridging Video and Language},
	url = {https://ieeexplore.ieee.org/document/7780940},
	doi = {10/ggv9gj},
	shorttitle = {{MSR}-{VTT}},
	abstract = {While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content. In this paper we present {MSR}-{VTT} (standing for "{MSRVideo} to Text") which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, {MSR}-{VTT} provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 {AMT} workers. We present a detailed analysis of {MSR}-{VTT} in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches. We also provide an extensive evaluation of these approaches on this dataset, showing that the hybrid Recurrent Neural Networkbased approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on {MSR}-{VTT}.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {5288--5296},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
	urldate = {2024-05-29},
	date = {2016-06},
	note = {{ISSN}: 1063-6919},
}

@misc{fischMRQA2019Shared2019,
	title = {{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension},
	url = {http://arxiv.org/abs/1910.09753},
	shorttitle = {{MRQA} 2019 Shared Task},
	abstract = {We present the results of the Machine Reading for Question Answering ({MRQA}) 2019 shared task on evaluating the generalization capabilities of reading comprehension systems. In this task, we adapted and unified 18 distinct question answering datasets into the same format. Among them, six datasets were made available for training, six datasets were made available for development, and the final six were hidden for final evaluation. Ten teams submitted systems, which explored various ideas including data sampling, multi-task learning, adversarial training and ensembling. The best system achieved an average F1 score of 72.5 on the 12 held-out datasets, 10.7 absolute points higher than our initial baseline based on {BERT}.},
	number = {{arXiv}:1910.09753},
	publisher = {{arXiv}},
	author = {Fisch, Adam and Talmor, Alon and Jia, Robin and Seo, Minjoon and Choi, Eunsol and Chen, Danqi},
	urldate = {2024-05-29},
	date = {2019-12-20},
	eprinttype = {arxiv},
	eprint = {1910.09753 [cs]},
}

@misc{sileoMindGamesTargetingTheory2023,
	title = {{MindGames}: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},
	url = {http://arxiv.org/abs/2305.03353},
	shorttitle = {{MindGames}},
	abstract = {Theory of Mind ({ToM}) is a critical component of intelligence but its assessment remains the subject of heated debates. Prior research applied human {ToM} assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. Here, we leverage dynamic epistemic logic to isolate a particular component of {ToM} and to generate controlled problems. We also introduce new verbalization techniques to express these problems in English natural language. Our findings indicate that some language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While {GPT}-4 demonstrates superior epistemic reasoning capabilities, there is still room for improvement. Our code and datasets are publicly available (https://huggingface.co/datasets/sileod/mindgames , https://github.com/sileod/llm-theory-of-mind )},
	number = {{arXiv}:2305.03353},
	publisher = {{arXiv}},
	author = {Sileo, Damien and Lernould, Antoine},
	urldate = {2024-05-29},
	date = {2023-11-07},
	eprinttype = {arxiv},
	eprint = {2305.03353 [cs]},
}

@inproceedings{chenMOCHADatasetTraining2020,
	title = {{MOCHA}: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics},
	url = {http://arxiv.org/abs/2010.03636},
	doi = {10/gm3n4r},
	shorttitle = {{MOCHA}},
	abstract = {Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: {MOdeling} Correctness with Human Annotations. {MOCHA} contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using {MOCHA}, we train a Learned Evaluation metric for Reading Comprehension, {LERC}, to mimic human judgement scores. {LERC} outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, {LERC} achieves 80\% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. {MOCHA} presents a challenging problem for developing accurate and robust generative reading comprehension metrics.},
	pages = {6521--6532},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	author = {Chen, Anthony and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
	urldate = {2024-05-29},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2010.03636 [cs]},
}

@misc{khashabiMoreBangYour2020,
	title = {More Bang for Your Buck: Natural Perturbation for Robust Question Answering},
	url = {http://arxiv.org/abs/2004.04849},
	shorttitle = {More Bang for Your Buck},
	abstract = {While recent models have achieved human-level scores on many {NLP} datasets, we observe that they are considerably sensitive to small changes in input. As an alternative to the standard approach of addressing this issue by constructing training sets of completely new examples, we propose doing so via minimal perturbation of examples. Specifically, our approach involves first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Local perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. To evaluate the impact of this phenomenon, we consider a recent question-answering dataset ({BoolQ}) and study the benefit of our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create, it is more effective to train models using them: such models exhibit higher robustness and better generalization, while retaining performance on the original {BoolQ} dataset.},
	number = {{arXiv}:2004.04849},
	publisher = {{arXiv}},
	author = {Khashabi, Daniel and Khot, Tushar and Sabharwal, Ashish},
	urldate = {2024-05-29},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2004.04849 [cs]},
}

@misc{sileoMiningDiscourseMarkers2019,
	title = {Mining Discourse Markers for Unsupervised Sentence Representation Learning},
	url = {http://arxiv.org/abs/1903.11850},
	abstract = {Current state of the art systems in {NLP} heavily rely on manually annotated datasets, which are expensive to construct. Very little work adequately exploits unannotated data -- such as discourse markers between sentences -- mainly because of data sparseness and ineffective extraction methods. In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data. Our resulting dataset contains 174 discourse markers with at least 10k examples each, even for rare markers such as coincidentally or amazingly We use the resulting data as supervision for learning transferable sentence embeddings. In addition, we show that even though sentence representation learning through prediction of discourse markers yields state of the art results across different transfer tasks, it is not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements. Our datasets are publicly available (https://github.com/synapse-developpement/Discovery)},
	number = {{arXiv}:1903.11850},
	publisher = {{arXiv}},
	author = {Sileo, Damien and Van-De-Cruys, Tim and Pradel, Camille and Muller, Philippe},
	urldate = {2024-05-29},
	date = {2019-03-28},
	eprinttype = {arxiv},
	eprint = {1903.11850 [cs]},
}

@misc{websterMindGAPBalanced2018,
	title = {Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns},
	url = {http://arxiv.org/abs/1810.05201},
	shorttitle = {Mind the {GAP}},
	abstract = {Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release {GAP}, a gender-balanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines which demonstrate the complexity of the challenge, the best achieving just 66.9\% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.},
	number = {{arXiv}:1810.05201},
	publisher = {{arXiv}},
	author = {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},
	urldate = {2024-05-29},
	date = {2018-10-11},
	eprinttype = {arxiv},
	eprint = {1810.05201 [cs]},
}

@misc{rashkinModelingNaivePsychology2018,
	title = {Modeling Naive Psychology of Characters in Simple Commonsense Stories},
	url = {http://arxiv.org/abs/1805.06533},
	abstract = {Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people's mental states - a capability that is trivial for humans but remarkably hard for machines. To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions. Our work presents a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.},
	number = {{arXiv}:1805.06533},
	publisher = {{arXiv}},
	author = {Rashkin, Hannah and Bosselut, Antoine and Sap, Maarten and Knight, Kevin and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2018-05-16},
	eprinttype = {arxiv},
	eprint = {1805.06533 [cs]},
}

@inproceedings{kongMMActLargeScaleDataset2019,
	title = {{MMAct}: A Large-Scale Dataset for Cross Modal Human Action Understanding},
	url = {https://ieeexplore.ieee.org/document/9009579},
	doi = {10/ghfhxx},
	shorttitle = {{MMAct}},
	abstract = {Unlike vision modalities, body-worn sensors or passive sensing can avoid the failure of action understanding in vision related challenges, e.g. occlusion and appearance variation. However, a standard large-scale dataset does not exist, in which different types of modalities across vision and sensors are integrated. To address the disadvantage of vision-based modalities and push towards multi/cross modal action understanding, this paper introduces a new large-scale dataset recorded from 20 distinct subjects with seven different types of modalities: {RGB} videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios. On the basis of our dataset, we propose a novel multi modality distillation model with attention mechanism to realize an adaptive knowledge transfer from sensor-based modalities to vision-based modalities. The proposed model significantly improves performance of action recognition compared to models trained with only {RGB} information. The experimental results confirm the effectiveness of our model on cross-subject, -view, -scene and -session evaluation criteria. We believe that this new large-scale multimodal dataset will contribute the community of multimodal based action understanding.},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {8657--8666},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	author = {Kong, Quan and Wu, Ziming and Deng, Ziwei and Klinkigt, Martin and Tong, Bin and Murakami, Tomokazu},
	urldate = {2024-05-29},
	date = {2019-10},
	note = {{ISSN}: 2380-7504},
}

@misc{palMedMCQALargescaleMultiSubject2022,
	title = {{MedMCQA} : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
	url = {http://arxiv.org/abs/2203.14371},
	shorttitle = {{MedMCQA}},
	abstract = {This paper introduces {MedMCQA}, a new large-scale, Multiple-Choice Question Answering ({MCQA}) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality {AIIMS} {\textbackslash}\& {NEET} {PG} entrance exam {MCQs} covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects {\textbackslash}\& topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
	number = {{arXiv}:2203.14371},
	publisher = {{arXiv}},
	author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
	urldate = {2024-05-29},
	date = {2022-03-27},
	eprinttype = {arxiv},
	eprint = {2203.14371 [cs]},
}

@article{gutierrez-fandinoMarIASpanishLanguage2022,
	title = {{MarIA}: Spanish Language Models},
	issn = {1989-7553},
	url = {http://arxiv.org/abs/2107.07253},
	doi = {10/gtwwvp},
	shorttitle = {{MarIA}},
	abstract = {This work presents {MarIA}, a family of Spanish language models and associated resources made available to the industry and the research community. Currently, {MarIA} includes {RoBERTa}-base, {RoBERTa}-large, {GPT}2 and {GPT}2-large Spanish language models, which can arguably be presented as the largest and most proficient language models in Spanish. The models were pretrained using a massive corpus of 570GB of clean and deduplicated texts with 135 billion words extracted from the Spanish Web Archive crawled by the National Library of Spain between 2009 and 2019. We assessed the performance of the models with nine existing evaluation datasets and with a novel extractive Question Answering dataset created ex novo. Overall, {MarIA} models outperform the existing Spanish models across a variety of {NLU} tasks and training settings.},
	pages = {39--60},
	journaltitle = {Procesamiento del Lenguaje Natural},
	author = {Gutiérrez-Fandiño, Asier and Armengol-Estapé, Jordi and Pàmies, Marc and Llop-Palao, Joan and Silveira-Ocampo, Joaquín and Carrino, Casimiro Pio and Gonzalez-Agirre, Aitor and Armentano-Oller, Carme and Rodriguez-Penagos, Carlos and Villegas, Marta},
	urldate = {2024-05-29},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2107.07253 [cs]},
}

@misc{keysersMeasuringCompositionalGeneralization2020,
	title = {Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
	url = {http://arxiv.org/abs/1912.09713},
	shorttitle = {Measuring Compositional Generalization},
	abstract = {State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing {SCAN} dataset, which confirms these findings.},
	number = {{arXiv}:1912.09713},
	publisher = {{arXiv}},
	author = {Keysers, Daniel and Schärli, Nathanael and Scales, Nathan and Buisman, Hylke and Furrer, Daniel and Kashubin, Sergii and Momchev, Nikola and Sinopalnikov, Danila and Stafiniak, Lukasz and Tihon, Tibor and Tsarkov, Dmitry and Wang, Xiao and van Zee, Marc and Bousquet, Olivier},
	urldate = {2024-05-29},
	date = {2020-06-25},
	eprinttype = {arxiv},
	eprint = {1912.09713 [cs, stat]},
}

@misc{ostermannMCScriptNovelDataset2018,
	title = {{MCScript}: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge},
	url = {http://arxiv.org/abs/1803.05223},
	shorttitle = {{MCScript}},
	abstract = {We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at {SemEval} 2018 and provides challenging test cases for the broader natural language understanding community.},
	number = {{arXiv}:1803.05223},
	publisher = {{arXiv}},
	author = {Ostermann, Simon and Modi, Ashutosh and Roth, Michael and Thater, Stefan and Pinkal, Manfred},
	urldate = {2024-05-29},
	date = {2018-03-14},
	eprinttype = {arxiv},
	eprint = {1803.05223 [cs]},
}

@inproceedings{misraMeasuringSimilaritySentential2016,
	title = {Measuring the Similarity of Sentential Arguments in Dialog},
	url = {http://arxiv.org/abs/1709.01887},
	doi = {10/ggv7nq},
	abstract = {When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. Debate websites produce curated summaries of arguments on such topics; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence of one particular aspect of an argument, e.g. Morality or Second Amendment. We call these frequently paraphrased propositions {ARGUMENT} {FACETS}. Like these curated sites, our goal is to induce and identify argument facets across multiple conversations, and produce summaries. However, we aim to do this automatically. We frame the problem as consisting of two steps: we first extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity to one another. Sets of similar arguments are used to represent argument facets. We show here that we can predict {ARGUMENT} {FACET} {SIMILARITY} with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics, easily beating several reasonable baselines.},
	pages = {276--287},
	booktitle = {Proceedings of the 17th Annual Meeting of the Special Interest Group           on Discourse and Dialogue},
	author = {Misra, Amita and Ecker, Brian and Walker, Marilyn A.},
	urldate = {2024-05-29},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1709.01887 [cs]},
}

@inproceedings{mohammadMetaphorMediumEmotion2016,
	location = {Berlin, Germany},
	title = {Metaphor as a Medium for Emotion: An Empirical Study},
	url = {https://aclanthology.org/S16-2003},
	doi = {10/ggwctb},
	shorttitle = {Metaphor as a Medium for Emotion},
	eventtitle = {*{SEM} 2016},
	pages = {23--33},
	booktitle = {Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics},
	publisher = {Association for Computational Linguistics},
	author = {Mohammad, Saif and Shutova, Ekaterina and Turney, Peter},
	editor = {Gardent, Claire and Bernardi, Raffaella and Titov, Ivan},
	urldate = {2024-05-29},
	date = {2016-08},
}

@inproceedings{khashabiLookingSurfaceChallenge2018,
	location = {New Orleans, Louisiana},
	title = {Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences},
	url = {https://aclanthology.org/N18-1023},
	doi = {10/ggngmk},
	shorttitle = {Looking Beyond the Surface},
	abstract = {We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1\%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.},
	eventtitle = {{NAACL}-{HLT} 2018},
	pages = {252--262},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	urldate = {2024-05-29},
	date = {2018-06},
}

@inproceedings{zhengMARSVideoBenchmark2016,
	location = {Cham},
	title = {{MARS}: A Video Benchmark for Large-Scale Person Re-Identification},
	isbn = {978-3-319-46466-4},
	doi = {10/gtwv9w},
	shorttitle = {{MARS}},
	abstract = {This paper considers person re-identification (re-id) in videos. We introduce a new video re-id dataset, named Motion Analysis and Re-identification Set ({MARS}), a video extension of the Market-1501 dataset. To our knowledge, {MARS} is the largest video re-id dataset to date. Containing 1,261 {IDs} and around 20,000 tracklets, it provides rich visual information compared to image-based datasets. Meanwhile, {MARS} reaches a step closer to practice. The tracklets are automatically generated by the Deformable Part Model ({DPM}) as pedestrian detector and the {GMMCP} tracker. A number of false detection/tracking results are also included as distractors which would exist predominantly in practical video databases. Extensive evaluation of the state-of-the-art methods including the space-time descriptors and {CNN} is presented. We show that {CNN} in classification mode can be trained from scratch using the consecutive bounding boxes of each identity. The learned {CNN} embedding outperforms other competing methods considerably and has good generalization ability on other video re-id datasets upon fine-tuning.},
	pages = {868--884},
	booktitle = {Computer Vision – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Zheng, Liang and Bie, Zhi and Sun, Yifan and Wang, Jingdong and Su, Chi and Wang, Shengjin and Tian, Qi},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	date = {2016},
	langid = {english},
}

@misc{jinLogicalFallacyDetection2022,
	title = {Logical Fallacy Detection},
	url = {http://arxiv.org/abs/2202.13758},
	abstract = {Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims ({LogicClimate}). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46\% on Logic and 4.51\% on {LogicClimate}. We encourage future work to explore this task as (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/{causalNLP}/logical-fallacy},
	number = {{arXiv}:2202.13758},
	publisher = {{arXiv}},
	author = {Jin, Zhijing and Lalwani, Abhinav and Vaidhya, Tejas and Shen, Xiaoyu and Ding, Yiwen and Lyu, Zhiheng and Sachan, Mrinmaya and Mihalcea, Rada and Schölkopf, Bernhard},
	urldate = {2024-05-29},
	date = {2022-12-11},
	eprinttype = {arxiv},
	eprint = {2202.13758 [cs]},
}

@misc{liuLogiQAChallengeDataset2020,
	title = {{LogiQA}: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
	url = {http://arxiv.org/abs/2007.08124},
	shorttitle = {{LogiQA}},
	abstract = {Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple {QA}, and thus increasingly challenging machine reading datasets have been proposed. Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named {LogiQA}, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 {QA} instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for reinvestigating logical {AI} under the deep learning {NLP} setting. The dataset is freely available at https://github.com/lgw863/{LogiQA}-dataset},
	number = {{arXiv}:2007.08124},
	publisher = {{arXiv}},
	author = {Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
	urldate = {2024-05-29},
	date = {2020-07-16},
	eprinttype = {arxiv},
	eprint = {2007.08124 [cs]},
}

@misc{chenLogic2TextHighFidelityNatural2020,
	title = {Logic2Text: High-Fidelity Natural Language Generation from Logical Forms},
	url = {http://arxiv.org/abs/2004.14579},
	shorttitle = {Logic2Text},
	abstract = {Previous works on Natural Language Generation ({NLG}) from structured data have primarily focused on surface-level descriptions of record sequences. However, for complex structured data, e.g., multi-row tables, it is often desirable for an {NLG} system to describe interesting facts from logical inferences across records. If only provided with the table, it is hard for existing models to produce controllable and high-fidelity logical generations. In this work, we formulate logical level {NLG} as generation from logical forms in order to obtain controllable, high-fidelity, and faithful generations. We present a new large-scale dataset, {\textbackslash}textsc\{Logic2Text\}, with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which poses great challenges on the model's ability to understand the semantics. We experiment on (1) Fully-supervised training with the full datasets, and (2) Few-shot setting, provided with hundreds of paired examples; We compare several popular generation models and analyze their performances. We hope our dataset can encourage research towards building an advanced {NLG} system capable of natural, faithful, and human-like generation. The dataset and code are available at https://github.com/czyssrs/Logic2Text.},
	number = {{arXiv}:2004.14579},
	publisher = {{arXiv}},
	author = {Chen, Zhiyu and Chen, Wenhu and Zha, Hanwen and Zhou, Xiyou and Zhang, Yunkai and Sundaresan, Sairam and Wang, William Yang},
	urldate = {2024-05-29},
	date = {2020-09-23},
	eprinttype = {arxiv},
	eprint = {2004.14579 [cs]},
}

@misc{chalkidisLexGLUEBenchmarkDataset2022,
	title = {{LexGLUE}: A Benchmark Dataset for Legal Language Understanding in English},
	url = {http://arxiv.org/abs/2110.00976},
	shorttitle = {{LexGLUE}},
	abstract = {Laws and their interpretations, legal arguments and agreements{\textbackslash} are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding ({NLU}) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation ({LexGLUE}) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal {NLU} tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.},
	number = {{arXiv}:2110.00976},
	publisher = {{arXiv}},
	author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel Martin and Aletras, Nikolaos},
	urldate = {2024-05-29},
	date = {2022-11-08},
	eprinttype = {arxiv},
	eprint = {2110.00976 [cs]},
}

@misc{moradshahiLocalizingOpenOntologyQA2020,
	title = {Localizing Open-Ontology {QA} Semantic Parsers in a Day Using Machine Translation},
	url = {http://arxiv.org/abs/2010.05106},
	abstract = {We propose Semantic Parser Localizer ({SPL}), a toolkit that leverages Neural Machine Translation ({NMT}) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel {XLMR}-{LSTM} semantic parser, and (3) test the model on natural utterances curated using human translators. We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering ({QA}) on the open web, to 10 new languages for the restaurants and hotels domains. Our models achieve an overall test accuracy ranging between 61\% and 69\% for the hotels domain and between 64\% and 78\% for restaurants domain, which compares favorably to 69\% and 80\% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30\% for hotels and 40\% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a {QA} system for a new domain, leveraging machine translation, in less than 24 hours.},
	number = {{arXiv}:2010.05106},
	publisher = {{arXiv}},
	author = {Moradshahi, Mehrad and Campagna, Giovanni and Semnani, Sina J. and Xu, Silei and Lam, Monica S.},
	urldate = {2024-05-29},
	date = {2020-10-10},
	eprinttype = {arxiv},
	eprint = {2010.05106 [cs]},
}

@misc{jhamtaniLearningExplainDatasets2020,
	title = {Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering},
	url = {http://arxiv.org/abs/2010.03274},
	shorttitle = {Learning to Explain},
	abstract = {Despite the rapid progress in multihop question-answering ({QA}), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, {eQASC}, contains over 98K explanation annotations for the multihop question answering dataset {QASC}, and is the first that annotates multiple candidate explanations for each answer. The second dataset {eQASC}-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in {QASC}, to test consistency and generalization of explanation prediction models. The third dataset {eOBQA} is constructed by adding explanation annotations to the {OBQA} dataset to test generalization of models trained on {eQASC}. We show that this data can be used to significantly improve explanation quality (+14\% absolute F1 over a strong retrieval baseline) using a {BERT}-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: "X is a Y" {AND} "Y has Z" {IMPLIES} "X has Z"). We find that generalized chains maintain performance while also being more robust to certain perturbations.},
	number = {{arXiv}:2010.03274},
	publisher = {{arXiv}},
	author = {Jhamtani, Harsh and Clark, Peter},
	urldate = {2024-05-29},
	date = {2020-10-07},
	eprinttype = {arxiv},
	eprint = {2010.03274 [cs]},
}

@misc{yinLearningMineAligned2018,
	title = {Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow},
	url = {http://arxiv.org/abs/1805.08949},
	abstract = {For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language ({NL}) and code with fine-grained alignments. Stack Overflow ({SO}) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the {NL}-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from {SO} using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between {NL} and code using neural networks. These features are fed into a classifier that determines the quality of mined {NL}-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling {NL}-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.},
	number = {{arXiv}:1805.08949},
	publisher = {{arXiv}},
	author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
	urldate = {2024-05-29},
	date = {2018-05-22},
	eprinttype = {arxiv},
	eprint = {1805.08949 [cs]},
}

@misc{wangLiarLiarPants2017,
	title = {"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection},
	url = {http://arxiv.org/abs/1705.00648},
	shorttitle = {"Liar, Liar Pants on Fire"},
	abstract = {Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from {PolitiFact}.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.},
	number = {{arXiv}:1705.00648},
	publisher = {{arXiv}},
	author = {Wang, William Yang},
	urldate = {2024-05-29},
	date = {2017-05-01},
	eprinttype = {arxiv},
	eprint = {1705.00648 [cs]},
}

@inproceedings{maasLearningWordVectors2011,
	location = {Portland, Oregon, {USA}},
	title = {Learning Word Vectors for Sentiment Analysis},
	url = {https://aclanthology.org/P11-1015},
	eventtitle = {{ACL}-{HLT} 2011},
	pages = {142--150},
	booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	editor = {Lin, Dekang and Matsumoto, Yuji and Mihalcea, Rada},
	urldate = {2024-05-29},
	date = {2011-06},
}

@inproceedings{manotasLiMiTLiteralMotion2020,
	location = {Online},
	title = {{LiMiT}: The Literal Motion in Text Dataset},
	url = {https://aclanthology.org/2020.findings-emnlp.88},
	doi = {10/gtwwfr},
	shorttitle = {{LiMiT}},
	abstract = {Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text ({LiMiT}) dataset, a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion. We describe the annotation process for the dataset, analyze its scale and diversity, and report results of several baseline models. We also present future research directions and applications of the {LiMiT} dataset and share it publicly as a new resource for the research community.},
	eventtitle = {Findings 2020},
	pages = {991--1000},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Manotas, Irene and Vo, Ngoc Phuoc An and Sheinin, Vadim},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-05-29},
	date = {2020-11},
}

@misc{elsheriefLatentHatredBenchmark2021,
	title = {Latent Hatred: A Benchmark for Understanding Implicit Hate Speech},
	url = {http://arxiv.org/abs/2109.05322},
	shorttitle = {Latent Hatred},
	abstract = {Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue.},
	number = {{arXiv}:2109.05322},
	publisher = {{arXiv}},
	author = {{ElSherief}, Mai and Ziems, Caleb and Muchlinski, David and Anupindi, Vaishnavi and Seybolt, Jordyn and De Choudhury, Munmun and Yang, Diyi},
	urldate = {2024-05-29},
	date = {2021-09-11},
	eprinttype = {arxiv},
	eprint = {2109.05322 [cs]},
}

@misc{vidgenLearningWorstDynamically2021,
	title = {Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},
	url = {http://arxiv.org/abs/2012.15761},
	shorttitle = {Learning from the Worst},
	abstract = {We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of {\textasciitilde}40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes {\textasciitilde}15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54\% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also perform better on {HateCheck}, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use. Accepted at {ACL} 2021.},
	number = {{arXiv}:2012.15761},
	publisher = {{arXiv}},
	author = {Vidgen, Bertie and Thrush, Tristan and Waseem, Zeerak and Kiela, Douwe},
	urldate = {2024-05-29},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2012.15761 [cs]},
}

@misc{reedLearningMistakesCombining2020,
	title = {Learning from Mistakes: Combining Ontologies via Self-Training for Dialogue Generation},
	url = {http://arxiv.org/abs/2010.00150},
	shorttitle = {Learning from Mistakes},
	abstract = {Natural language generators ({NLGs}) for task-oriented dialogue typically take a meaning representation ({MR}) as input. They are trained end-to-end with a corpus of {MR}/utterance pairs, where the {MRs} cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor-intensive and time-consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies. Here we explore, for the first time, whether it is possible to train an {NLG} for a new larger ontology using existing training sets for the restaurant domain, where each set is based on a different ontology. We create a new, larger combined ontology, and then train an {NLG} to produce utterances covering it. For example, if one dataset has attributes for family-friendly and rating information, and the other has attributes for decor and service, our aim is an {NLG} for the combined ontology that can produce utterances that realize values for family-friendly, rating, decor and service. Initial experiments with a baseline neural sequence-to-sequence model show that this task is surprisingly challenging. We then develop a novel self-training method that identifies (errorful) model outputs, automatically constructs a corrected {MR} input to form a new ({MR}, utterance) training pair, and then repeatedly adds these new instances back into the training data. We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4\% improvement over the baseline model. We also report a human qualitative evaluation of the final model showing that it achieves high naturalness, semantic coherence and grammaticality},
	number = {{arXiv}:2010.00150},
	publisher = {{arXiv}},
	author = {Reed, Lena and Harrison, Vrindavan and Oraby, Shereen and Hakkani-Tur, Dilek and Walker, Marilyn},
	urldate = {2024-05-29},
	date = {2020-09-30},
	eprinttype = {arxiv},
	eprint = {2010.00150 [cs]},
}

@misc{ramadanLargeScaleMultiDomainBelief2018,
	title = {Large-Scale Multi-Domain Belief Tracking with Knowledge Sharing},
	url = {http://arxiv.org/abs/1807.06517},
	abstract = {Robust dialogue belief tracking is a key component in maintaining good quality dialogue systems. The tasks that dialogue systems are trying to solve are becoming increasingly complex, requiring scalability to multi domain, semantically rich dialogues. However, most current approaches have difficulty scaling up with domains because of the dependency of the model parameters on the dialogue ontology. In this paper, a novel approach is introduced that fully utilizes semantic similarity between dialogue utterances and the ontology terms, allowing the information to be shared across domains. The evaluation is performed on a recently collected multi-domain dialogues dataset, one order of magnitude larger than currently available corpora. Our model demonstrates great capability in handling multi-domain dialogues, simultaneously outperforming existing state-of-the-art models in single-domain dialogue tracking tasks.},
	number = {{arXiv}:1807.06517},
	publisher = {{arXiv}},
	author = {Ramadan, Osman and Budzianowski, Paweł and Gašić, Milica},
	urldate = {2024-05-29},
	date = {2018-07-17},
	eprinttype = {arxiv},
	eprint = {1807.06517 [cs]},
}

@misc{xieLargescaleClozeTest2018,
	title = {Large-scale Cloze Test Dataset Created by Teachers},
	url = {http://arxiv.org/abs/1711.03225},
	abstract = {Cloze tests are widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset {CLOTH}, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, {CLOTH} requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of {CLOTH}, and identify the limited ability of comprehending the long-term context to be the key bottleneck.},
	number = {{arXiv}:1711.03225},
	publisher = {{arXiv}},
	author = {Xie, Qizhe and Lai, Guokun and Dai, Zihang and Hovy, Eduard},
	urldate = {2024-05-29},
	date = {2018-08-27},
	eprinttype = {arxiv},
	eprint = {1711.03225 [cs]},
}

@misc{renKnowledgeDrivenDistractorGeneration2020,
	title = {Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions},
	url = {http://arxiv.org/abs/2004.09853},
	abstract = {In this paper, we propose a novel configurable framework to automatically generate distractive choices for open-domain cloze-style multiple-choice questions, which incorporates a general-purpose knowledge base to effectively create a small distractor candidate set, and a feature-rich learning-to-rank model to select distractors that are both plausible and reliable. Experimental results on datasets across four domains show that our framework yields distractors that are more plausible and reliable than previous methods. This dataset can also be used as a benchmark for distractor generation in the future.},
	number = {{arXiv}:2004.09853},
	publisher = {{arXiv}},
	author = {Ren, Siyu and Zhu, Kenny Q.},
	urldate = {2024-05-29},
	date = {2020-12-07},
	eprinttype = {arxiv},
	eprint = {2004.09853 [cs]},
}

@misc{rajpurkarKnowWhatYou2018,
	title = {Know What You Don't Know: Unanswerable Questions for {SQuAD}},
	url = {http://arxiv.org/abs/1806.03822},
	shorttitle = {Know What You Don't Know},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present {SQuAD} 2.0, the latest version of the Stanford Question Answering Dataset ({SQuAD}). {SQuAD} 2.0 combines existing {SQuAD} data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on {SQuAD} 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. {SQuAD} 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on {SQuAD} 1.1 achieves only 66\% F1 on {SQuAD} 2.0.},
	number = {{arXiv}:1806.03822},
	publisher = {{arXiv}},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	urldate = {2024-05-29},
	date = {2018-06-11},
	eprinttype = {arxiv},
	eprint = {1806.03822 [cs]},
}

@misc{shengInvestigatingSocietalBiases2020,
	title = {Investigating Societal Biases in a Poetry Composition System},
	url = {http://arxiv.org/abs/2011.02686},
	abstract = {There is a growing collection of work analyzing and mitigating societal biases in language understanding, generation, and retrieval tasks, though examining biases in creative tasks remains underexplored. Creative language applications are meant for direct interaction with users, so it is important to quantify and mitigate societal biases in these applications. We introduce a novel study on a pipeline to mitigate societal biases when retrieving next verse suggestions in a poetry composition system. Our results suggest that data augmentation through sentiment style transfer has potential for mitigating societal biases.},
	number = {{arXiv}:2011.02686},
	publisher = {{arXiv}},
	author = {Sheng, Emily and Uthus, David},
	urldate = {2024-05-29},
	date = {2020-11-05},
	eprinttype = {arxiv},
	eprint = {2011.02686 [cs]},
}

@misc{hawkinsInvestigatingRepresentationsVerb2020,
	title = {Investigating representations of verb bias in neural language models},
	url = {http://arxiv.org/abs/2010.02375},
	abstract = {Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as {\textbackslash}emph\{verb bias\}. Here we introduce {DAIS}, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. {GPT}-2) tend to out-perform recurrent architectures (e.g. {LSTMs}) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.},
	number = {{arXiv}:2010.02375},
	publisher = {{arXiv}},
	author = {Hawkins, Robert D. and Yamakoshi, Takateru and Griffiths, Thomas L. and Goldberg, Adele E.},
	urldate = {2024-05-29},
	date = {2020-10-15},
	eprinttype = {arxiv},
	eprint = {2010.02375 [cs]},
}

@misc{rybakKLEJComprehensiveBenchmark2020,
	title = {{KLEJ}: Comprehensive Benchmark for Polish Language Understanding},
	url = {http://arxiv.org/abs/2005.00630},
	shorttitle = {{KLEJ}},
	abstract = {In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding ({NLU}) tasks. Such a fast pace of research would not be possible without general {NLU} benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews ({AR}). To ensure a common evaluation scheme and promote models that generalize to different {NLU} tasks, the benchmark includes datasets from varying domains and applications. Additionally, we release {HerBERT}, a Transformer-based model trained specifically for the Polish language, which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based models.},
	number = {{arXiv}:2005.00630},
	publisher = {{arXiv}},
	author = {Rybak, Piotr and Mroczkowski, Robert and Tracz, Janusz and Gawlik, Ireneusz},
	urldate = {2024-05-29},
	date = {2020-05-01},
	eprinttype = {arxiv},
	eprint = {2005.00630 [cs]},
}

@misc{napolesJFLEGFluencyCorpus2017,
	title = {{JFLEG}: A Fluency Corpus and Benchmark for Grammatical Error Correction},
	url = {http://arxiv.org/abs/1702.04066},
	shorttitle = {{JFLEG}},
	abstract = {We present a new parallel corpus, {JHU} {FLuency}-Extended {GUG} corpus ({JFLEG}) for developing and evaluating grammatical error correction ({GEC}). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading {GEC} systems on this corpus, identifying specific areas in which they do well and how they can improve. {JFLEG} fulfills the need for a new gold standard to properly assess the current state of {GEC}.},
	number = {{arXiv}:1702.04066},
	publisher = {{arXiv}},
	author = {Napoles, Courtney and Sakaguchi, Keisuke and Tetreault, Joel},
	urldate = {2024-05-29},
	date = {2017-02-13},
	eprinttype = {arxiv},
	eprint = {1702.04066 [cs]},
}

@misc{millerKeyValueMemoryNetworks2016,
	title = {Key-Value Memory Networks for Directly Reading Documents},
	url = {http://arxiv.org/abs/1606.03126},
	abstract = {Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering ({QA}) has been directed towards using Knowledge Bases ({KBs}) instead, which has proven effective. Unfortunately {KBs} often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using {KBs}, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, {WikiMovies}, a {QA} dataset that contains raw text alongside a preprocessed {KB}, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing {WikiQA} benchmark.},
	number = {{arXiv}:1606.03126},
	publisher = {{arXiv}},
	author = {Miller, Alexander and Fisch, Adam and Dodge, Jesse and Karimi, Amir-Hossein and Bordes, Antoine and Weston, Jason},
	urldate = {2024-05-29},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.03126 [cs]},
}

@inproceedings{peskovItTakesTwo2020,
	location = {Online},
	title = {It Takes Two to Lie: One to Lie, and One to Listen},
	url = {https://aclanthology.org/2020.acl-main.353},
	doi = {10/gtwwfn},
	shorttitle = {It Takes Two to Lie},
	abstract = {Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.},
	eventtitle = {{ACL} 2020},
	pages = {3811--3854},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Peskov, Denis and Cheng, Benny and Elgohary, Ahmed and Barrow, Joe and Danescu-Niculescu-Mizil, Cristian and Boyd-Graber, Jordan},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2024-05-29},
	date = {2020-07},
}

@inproceedings{wesselIntroducingMBIBFirst2023,
	title = {Introducing {MBIB} -- the first Media Bias Identification Benchmark Task and Dataset Collection},
	url = {http://arxiv.org/abs/2304.13148},
	doi = {10/gtwtn4},
	abstract = {Although media bias detection is a complex multi-task problem, there is, to date, no unified benchmark grouping these evaluation tasks. We introduce the Media Bias Identification Benchmark ({MBIB}), a comprehensive benchmark that groups different types of media bias (e.g., linguistic, cognitive, political) under a common framework to test how prospective detection techniques generalize. After reviewing 115 datasets, we select nine tasks and carefully propose 22 associated datasets for evaluating media bias detection techniques. We evaluate {MBIB} using state-of-the-art Transformer techniques (e.g., T5, {BART}). Our results suggest that while hate speech, racial bias, and gender bias are easier to detect, models struggle to handle certain bias types, e.g., cognitive and political bias. However, our results show that no single technique can outperform all the others significantly. We also find an uneven distribution of research interest and resource allocation to the individual tasks in media bias. A unified benchmark encourages the development of more robust systems and shifts the current paradigm in media bias detection evaluation towards solutions that tackle not one but multiple media bias types simultaneously.},
	pages = {2765--2774},
	booktitle = {Proceedings of the 46th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	author = {Wessel, Martin and Horych, Tomáš and Ruas, Terry and Aizawa, Akiko and Gipp, Bela and Spinde, Timo},
	urldate = {2024-05-29},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2304.13148 [cs]},
}

@inproceedings{collierIntroductionBioentityRecognition2004,
	location = {Geneva, Switzerland},
	title = {Introduction to the Bio-entity Recognition Task at {JNLPBA}},
	url = {https://aclanthology.org/W04-1213},
	eventtitle = {{BioNLP} 2004},
	pages = {73--78},
	booktitle = {Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{BioNLP})},
	publisher = {{COLING}},
	author = {Collier, Nigel and Ohta, Tomoko and Tsuruoka, Yoshimasa and Tateisi, Yuka and Kim, Jin-Dong},
	editor = {Collier, Nigel and Ruch, Patrick and Nazarenko, Adeline},
	urldate = {2024-05-29},
	date = {2004-08},
}

@misc{koInquisitiveQuestionGeneration2020,
	title = {Inquisitive Question Generation for High Level Text Comprehension},
	url = {http://arxiv.org/abs/2010.01657},
	abstract = {Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets. We introduce {INQUISITIVE}, a dataset of {\textasciitilde}19K questions that are elicited while a person is reading through a document. Compared to existing datasets, {INQUISITIVE} questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on {GPT}-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate {INQUISITIVE} questions.},
	number = {{arXiv}:2010.01657},
	publisher = {{arXiv}},
	author = {Ko, Wei-Jen and Chen, Te-Yuan and Huang, Yiyan and Durrett, Greg and Li, Junyi Jessy},
	urldate = {2024-05-29},
	date = {2020-10-04},
	eprinttype = {arxiv},
	eprint = {2010.01657 [cs]},
}

@inproceedings{rodriguezInformationSeekingSpirit2020,
	title = {Information Seeking in the Spirit of Learning: a Dataset for Conversational Curiosity},
	url = {http://arxiv.org/abs/2005.00172},
	doi = {10/gm3n8c},
	shorttitle = {Information Seeking in the Spirit of Learning},
	abstract = {Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user's pre-existing knowledge. Assuming a correlation between engagement and user responses such as "liking" messages or asking followup questions, we design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know. Through crowd-sourcing of this experiment, we collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages. Responses using a user's prior knowledge increase engagement. We incorporate this knowledge into a multi-task model that reproduces human assistant policies and improves over a {BERT} content model by 13 mean reciprocal rank points.},
	pages = {8153--8172},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	author = {Rodriguez, Pedro and Crook, Paul and Moon, Seungwhan and Wang, Zhiguang},
	urldate = {2024-05-29},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2005.00172 [cs]},
}

@misc{zhangImprovingMassivelyMultilingual2020,
	title = {Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation},
	url = {http://arxiv.org/abs/2004.11867},
	abstract = {Massively multilingual models for neural machine translation ({NMT}) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual {NMT} requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening {NMT} architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on {OPUS}-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by {\textasciitilde}10 {BLEU}, approaching conventional pivot-based methods.},
	number = {{arXiv}:2004.11867},
	publisher = {{arXiv}},
	author = {Zhang, Biao and Williams, Philip and Titov, Ivan and Sennrich, Rico},
	urldate = {2024-05-29},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {2004.11867 [cs]},
}

@inproceedings{vidgenIntroducingCADContextual2021,
	location = {Online},
	title = {Introducing {CAD}: the Contextual Abuse Dataset},
	url = {https://aclanthology.org/2021.naacl-main.182},
	doi = {10/gtwwfv},
	shorttitle = {Introducing {CAD}},
	abstract = {Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.},
	eventtitle = {{NAACL}-{HLT} 2021},
	pages = {2289--2303},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Vidgen, Bertie and Nguyen, Dong and Margetts, Helen and Rossini, Patricia and Tromble, Rebekah},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	urldate = {2024-05-29},
	date = {2021-06},
}

@inproceedings{kakwaniIndicNLPSuiteMonolingualCorpora2020,
	location = {Online},
	title = {{IndicNLPSuite}: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages},
	url = {https://aclanthology.org/2020.findings-emnlp.445},
	doi = {10/gtwwfq},
	shorttitle = {{IndicNLPSuite}},
	abstract = {In this paper, we introduce {NLP} resources for 11 major Indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple {NLU} evaluation datasets ({IndicGLUE} benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and Indian English, primarily sourced from news crawls. The word embeddings are based on {FastText}, hence suitable for handling morphological complexity of Indian languages. The pre-trained language models are based on the compact {ALBERT} model. Lastly, we compile the ({IndicGLUE} benchmark for Indian language {NLU}. To this end, we create datasets for the following tasks: Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice {QA}, Winograd {NLI} and {COPA}. We also include publicly available datasets for some Indic languages for tasks like Named Entity Recognition, Cross-lingual Sentence Retrieval, Paraphrase detection, etc. Our embeddings are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the dataset will accelerate Indic {NLP} research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in {NLP} over a more diverse pool of languages. The data and models are available at https://indicnlp.ai4bharat.org.},
	eventtitle = {Findings 2020},
	pages = {4948--4961},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Kakwani, Divyanshu and Kunchukuttan, Anoop and Golla, Satish and N.C., Gokul and Bhattacharyya, Avik and Khapra, Mitesh M. and Kumar, Pratyush},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-05-29},
	date = {2020-11},
}

@misc{bhagavatulaI2D2InductiveKnowledge2023,
	title = {I2D2: Inductive Knowledge Distillation with {NeuroLogic} and Self-Imitation},
	url = {http://arxiv.org/abs/2212.09246},
	shorttitle = {I2D2},
	abstract = {Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., {GPT}-2) win over models that are orders of magnitude larger and better (e.g., {GPT}-3), if powered with novel commonsense distillation algorithms? The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of {NeuroLogic} Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model's own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date.},
	number = {{arXiv}:2212.09246},
	publisher = {{arXiv}},
	author = {Bhagavatula, Chandra and Hwang, Jena D. and Downey, Doug and Bras, Ronan Le and Lu, Ximing and Qin, Lianhui and Sakaguchi, Keisuke and Swayamdipta, Swabha and West, Peter and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2212.09246 [cs]},
}

@misc{fergusonIIRCDatasetIncomplete2020,
	title = {{IIRC}: A Dataset of Incomplete Information Reading Comprehension Questions},
	url = {http://arxiv.org/abs/2011.07127},
	shorttitle = {{IIRC}},
	abstract = {Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension ({RC}) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system's performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, {IIRC}, with more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the task. We follow recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1\% F1 on this task, while estimated human performance is 88.4\%. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.},
	number = {{arXiv}:2011.07127},
	publisher = {{arXiv}},
	author = {Ferguson, James and Gardner, Matt and Hajishirzi, Hannaneh and Khot, Tushar and Dasigi, Pradeep},
	urldate = {2024-05-29},
	date = {2020-11-13},
	eprinttype = {arxiv},
	eprint = {2011.07127 [cs]},
}

@inproceedings{baroniHowWeBLESSed2011,
	location = {Edinburgh, {UK}},
	title = {How we {BLESSed} distributional semantic evaluation},
	url = {https://aclanthology.org/W11-2501},
	eventtitle = {{GEMS} 2011},
	pages = {1--10},
	booktitle = {Proceedings of the {GEMS} 2011 Workshop on {GEometrical} Models of Natural Language Semantics},
	publisher = {Association for Computational Linguistics},
	author = {Baroni, Marco and Lenci, Alessandro},
	editor = {Pado, Sebastian and Peirsman, Yves},
	urldate = {2024-05-29},
	date = {2011-07},
}

@inproceedings{bojarHindEnCorpHindiEnglishHindionly2014,
	location = {Reykjavik, Iceland},
	title = {{HindEnCorp} - Hindi-English and Hindi-only Corpus for Machine Translation},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/835_Paper.pdf},
	abstract = {We present {HindEnCorp}, a parallel corpus of Hindi and English, and {HindMonoCorp}, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. {HindEnCorp} consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). {HindMonoCorp} amounts to 787 million tokens in 44 million sentences. Both the corpora are freely available for non-commercial research and their preliminary release has been used by numerous participants of the {WMT} 2014 shared translation task.},
	eventtitle = {{LREC} 2014},
	pages = {3550--3555},
	booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Bojar, Ondřej and Diatka, Vojtěch and Rychlý, Pavel and Straňák, Pavel and Suchomel, Vít and Tamchyna, Aleš and Zeman, Daniel},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2014-05},
}

@inproceedings{chakravarthiHopeEDIMultilingualHope2020,
	location = {Barcelona, Spain (Online)},
	title = {{HopeEDI}: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion},
	url = {https://aclanthology.org/2020.peoples-1.5},
	shorttitle = {{HopeEDI}},
	abstract = {Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion ({HopeEDI}) containing user-generated comments from the social media platform {YouTube} with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff's alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.},
	eventtitle = {{PEOPLES} 2020},
	pages = {41--53},
	booktitle = {Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media},
	publisher = {Association for Computational Linguistics},
	author = {Chakravarthi, Bharathi Raja},
	editor = {Nissim, Malvina and Patti, Viviana and Plank, Barbara and Durmus, Esin},
	urldate = {2024-05-29},
	date = {2020-12},
}

@article{russakovskyImageNetLargeScale2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10/gcgk7w},
	abstract = {The {ImageNet} Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	urldate = {2024-05-29},
	date = {2015-12-01},
	langid = {english},
}

@misc{khashabiGooAQOpenQuestion2021,
	title = {{GooAQ}: Open Question Answering with Diverse Answer Types},
	url = {http://arxiv.org/abs/2104.08727},
	shorttitle = {{GooAQ}},
	abstract = {While day-to-day questions come with a variety of answer types, the current question-answering ({QA}) literature has failed to adequately address the answer diversity of questions. To this end, we present {GooAQ}, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. {GooAQ} questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. {GooAQ} answers are mined from Google's responses to our collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections. We {benchmarkT}5 models on {GooAQ} and observe that: (a) in line with recent work, {LM}'s strong performance on {GooAQ}'s short-answer questions heavily benefit from annotated data; however, (b) their quality in generating coherent and accurate responses for questions requiring long responses (such as 'how' and 'why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. We release {GooAQ} to facilitate further research on improving {QA} with diverse response types.},
	number = {{arXiv}:2104.08727},
	publisher = {{arXiv}},
	author = {Khashabi, Daniel and Ng, Amos and Khot, Tushar and Sabharwal, Ashish and Hajishirzi, Hannaneh and Callison-Burch, Chris},
	urldate = {2024-05-29},
	date = {2021-09-10},
	eprinttype = {arxiv},
	eprint = {2104.08727 [cs]},
}

@misc{mathewHateXplainBenchmarkDataset2022,
	title = {{HateXplain}: A Benchmark Dataset for Explainable Hate Speech Detection},
	url = {http://arxiv.org/abs/2012.10289},
	shorttitle = {{HateXplain}},
	abstract = {Hate speech is a challenging issue plaguing the online social media. While better models for hate speech detection are continuously being developed, there is little research on the bias and interpretability aspects of hate speech. In this paper, we introduce {HateXplain}, the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in our dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. We utilize existing state-of-the-art models and observe that even models that perform very well in classification do not score high on explainability metrics like model plausibility and faithfulness. We also observe that models, which utilize the human rationales for training, perform better in reducing unintended bias towards target communities. We have made our code and dataset public at https://github.com/punyajoy/{HateXplain}},
	number = {{arXiv}:2012.10289},
	publisher = {{arXiv}},
	author = {Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
	urldate = {2024-05-29},
	date = {2022-04-12},
	eprinttype = {arxiv},
	eprint = {2012.10289 [cs]},
}

@misc{govindarajanHelpNeedAdvice2020,
	title = {Help! Need Advice on Identifying Advice},
	url = {http://arxiv.org/abs/2010.02494},
	abstract = {Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of advice-seeking online, as well as advice-giving in natural language generation systems. We present a dataset in English from two Reddit advice forums - r/{AskParents} and r/needadvice - annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present preliminary models showing that while pre-trained language models are able to capture advice better than rule-based systems, advice identification is challenging, and we identify directions for future research. Comments: To be presented at {EMNLP} 2020.},
	number = {{arXiv}:2010.02494},
	publisher = {{arXiv}},
	author = {Govindarajan, Venkata Subrahmanyan and Chen, Benjamin T. and Warholic, Rebecca and Erk, Katrin and Li, Junyi Jessy},
	urldate = {2024-05-29},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02494 [cs]},
}

@misc{choGroundingConversationsImprovised2020,
	title = {Grounding Conversations with Improvised Dialogues},
	url = {http://arxiv.org/abs/2004.09544},
	abstract = {Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.},
	number = {{arXiv}:2004.09544},
	publisher = {{arXiv}},
	author = {Cho, Hyundong and May, Jonathan},
	urldate = {2024-05-29},
	date = {2020-05-19},
	eprinttype = {arxiv},
	eprint = {2004.09544 [cs]},
}

@misc{zhouGoingVacationTakes2019,
	title = {"Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding},
	url = {http://arxiv.org/abs/1909.03065},
	shorttitle = {"Going on a vacation" takes longer than "Going for a walk"},
	abstract = {Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, {MCTACO}, that serves as a test set for this task. We find that the best current methods used on {MCTACO} are still far behind human performance, by about 20\%, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.},
	number = {{arXiv}:1909.03065},
	publisher = {{arXiv}},
	author = {Zhou, Ben and Khashabi, Daniel and Ning, Qiang and Roth, Dan},
	urldate = {2024-05-29},
	date = {2019-09-06},
	eprinttype = {arxiv},
	eprint = {1909.03065 [cs]},
}

@misc{vilaresHEADQAHealthcareDataset2019,
	title = {{HEAD}-{QA}: A Healthcare Dataset for Complex Reasoning},
	url = {http://arxiv.org/abs/1906.04701},
	shorttitle = {{HEAD}-{QA}},
	abstract = {We present {HEAD}-{QA}, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) {HEAD}-{QA} challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.},
	number = {{arXiv}:1906.04701},
	publisher = {{arXiv}},
	author = {Vilares, David and Gómez-Rodríguez, Carlos},
	urldate = {2024-05-29},
	date = {2019-06-11},
	eprinttype = {arxiv},
	eprint = {1906.04701 [cs]},
}

@misc{yanakaHELPDatasetIdentifying2019,
	title = {{HELP}: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning},
	url = {http://arxiv.org/abs/1904.12166},
	shorttitle = {{HELP}},
	abstract = {Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference ({NLI}). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model architectures themselves. To investigate this issue, we introduce a new dataset, called {HELP}, for handling entailments with lexical and logical phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonicity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data augmentation while others are immune to it.},
	number = {{arXiv}:1904.12166},
	publisher = {{arXiv}},
	author = {Yanaka, Hitomi and Mineshima, Koji and Bekki, Daisuke and Inui, Kentaro and Sekine, Satoshi and Abzianidze, Lasha and Bos, Johan},
	urldate = {2024-05-29},
	date = {2019-04-27},
	eprinttype = {arxiv},
	eprint = {1904.12166 [cs]},
}

@misc{degibertHateSpeechDataset2018,
	title = {Hate Speech Dataset from a White Supremacy Forum},
	url = {http://arxiv.org/abs/1809.04444},
	abstract = {Hate speech is commonly defined as any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic. Due to the massive rise of user-generated web content on social media, the amount of hate speech is also steadily increasing. Over the past years, interest in online hate speech detection and, particularly, the automation of this task has continuously grown, along with the societal impact of the phenomenon. This paper describes a hate speech dataset composed of thousands of sentences manually labelled as containing hate speech or not. The sentences have been extracted from Stormfront, a white supremacist forum. A custom annotation tool has been developed to carry out the manual labelling task which, among other things, allows the annotators to choose whether to read the context of a sentence before labelling it. The paper also provides a thoughtful qualitative and quantitative study of the resulting dataset and several baseline experiments with different classification models. The dataset is publicly available.},
	number = {{arXiv}:1809.04444},
	publisher = {{arXiv}},
	author = {de Gibert, Ona and Perez, Naiara and García-Pablos, Aitor and Cuadros, Montse},
	urldate = {2024-05-29},
	date = {2018-09-12},
	eprinttype = {arxiv},
	eprint = {1809.04444 [cs]},
}

@misc{sileoGeneratingMultiplechoiceQuestions2023,
	title = {Generating multiple-choice questions for medical question answering with distractors and cue-masking},
	url = {http://arxiv.org/abs/2303.07069},
	abstract = {Medical multiple-choice question answering ({MCQA}) is particularly difficult. Questions may describe patient symptoms and ask for the correct diagnosis, which requires domain knowledge and complex reasoning. Standard language modeling pretraining alone is not sufficient to achieve the best results. {\textbackslash}citet\{jin2020disease\} showed that focusing masked language modeling on disease name prediction when using medical encyclopedic paragraphs as input leads to considerable {MCQA} accuracy improvement. In this work, we show that (1) fine-tuning on generated {MCQA} dataset outperforms the masked language modeling based objective and (2) correctly masking the cues to the answers is critical for good performance. We release new pretraining datasets and achieve state-of-the-art results on 4 {MCQA} datasets, notably +5.7{\textbackslash}\% with base-size model on {MedQA}-{USMLE}.},
	number = {{arXiv}:2303.07069},
	publisher = {{arXiv}},
	author = {Sileo, Damien and Uma, Kanimozhi and Moens, Marie-Francine},
	urldate = {2024-05-29},
	date = {2023-03-13},
	eprinttype = {arxiv},
	eprint = {2303.07069 [cs]},
}

@misc{schusterGetYourVitamin2021,
	title = {Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence},
	url = {http://arxiv.org/abs/2103.08541},
	abstract = {Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present {VitaminC}, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in {VitaminC} are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness -- improving accuracy by 10\% on adversarial fact verification and 6\% on adversarial natural language inference ({NLI}). Moreover, the structure of {VitaminC} leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.},
	number = {{arXiv}:2103.08541},
	publisher = {{arXiv}},
	author = {Schuster, Tal and Fisch, Adam and Barzilay, Regina},
	urldate = {2024-05-29},
	date = {2021-03-15},
	eprinttype = {arxiv},
	eprint = {2103.08541 [cs]},
}

@misc{demszkyGoEmotionsDatasetFineGrained2020,
	title = {{GoEmotions}: A Dataset of Fine-Grained Emotions},
	url = {http://arxiv.org/abs/2005.00547},
	shorttitle = {{GoEmotions}},
	abstract = {Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce {GoEmotions}, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our {BERT}-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.},
	number = {{arXiv}:2005.00547},
	publisher = {{arXiv}},
	author = {Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},
	urldate = {2024-05-29},
	date = {2020-06-02},
	eprinttype = {arxiv},
	eprint = {2005.00547 [cs]},
}

@misc{wuGeneratingDataMitigate2022,
	title = {Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets},
	url = {http://arxiv.org/abs/2203.12942},
	abstract = {Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the {SNLI} and {MNLI} datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of {SNLI}-hard and {MNLI}-hard.},
	number = {{arXiv}:2203.12942},
	publisher = {{arXiv}},
	author = {Wu, Yuxiang and Gardner, Matt and Stenetorp, Pontus and Dasigi, Pradeep},
	urldate = {2024-05-29},
	date = {2022-03-24},
	eprinttype = {arxiv},
	eprint = {2203.12942 [cs]},
}

@misc{wangLSATProgressChallenges2021,
	title = {From {LSAT}: The Progress and Challenges of Complex Reasoning},
	url = {http://arxiv.org/abs/2108.00648},
	shorttitle = {From {LSAT}},
	abstract = {Complex reasoning aims to draw a correct inference based on complex rules. As a hallmark of human intelligence, it involves a degree of explicit reading comprehension, interpretation of logical knowledge and complex rule application. In this paper, we take a step forward in complex reasoning by systematically studying the three challenging and domain-general tasks of the Law School Admission Test ({LSAT}), including analytical reasoning, logical reasoning and reading comprehension. We propose a hybrid reasoning system to integrate these three tasks and achieve impressive overall performance on the {LSAT} tests. The experimental results demonstrate that our system endows itself a certain complex reasoning ability, especially the fundamental reading comprehension and challenging logical reasoning capacities. Further analysis also shows the effectiveness of combining the pre-trained models with the task-specific reasoning module, and integrating symbolic knowledge into discrete interpretable reasoning steps in complex reasoning. We further shed a light on the potential future directions, like unsupervised symbolic knowledge extraction, model interpretability, few-shot learning and comprehensive benchmark for complex reasoning.},
	number = {{arXiv}:2108.00648},
	publisher = {{arXiv}},
	author = {Wang, Siyuan and Liu, Zhongkun and Zhong, Wanjun and Zhou, Ming and Wei, Zhongyu and Chen, Zhumin and Duan, Nan},
	urldate = {2024-05-29},
	date = {2021-08-02},
	eprinttype = {arxiv},
	eprint = {2108.00648 [cs]},
}

@misc{zhaoGenderBiasCoreference2018,
	title = {Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
	url = {http://arxiv.org/abs/1804.06876},
	shorttitle = {Gender Bias in Coreference Resolution},
	abstract = {We introduce a new benchmark, {WinoBias}, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in {WinoBias} without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.},
	number = {{arXiv}:1804.06876},
	publisher = {{arXiv}},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	urldate = {2024-05-29},
	date = {2018-04-18},
	eprinttype = {arxiv},
	eprint = {1804.06876 [cs]},
}

@misc{hanFOLIONaturalLanguage2024,
	title = {{FOLIO}: Natural Language Reasoning with First-Order Logic},
	url = {http://arxiv.org/abs/2209.00840},
	shorttitle = {{FOLIO}},
	abstract = {Large language models ({LLMs}) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present {FOLIO}, a human-annotated, logically complex and diverse dataset for reasoning in natural language ({NL}), equipped with first-order logic ({FOL}) annotations. {FOLIO} consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their {FOL} annotations, which are automatically verified by an {FOL} inference engine. In addition to the main {NL} reasoning task, {NL}-{FOL} pairs in {FOLIO} constitute a new {NL}-{FOL} translation dataset. Our experiments on {FOLIO} systematically evaluate the {FOL} reasoning ability of supervised fine-tuning on medium-sized language models. For both {NL} reasoning and {NL}-{FOL} translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of {FOLIO} presents a challenge for one of the most capable \{Large Language Model ({LLM})\} publicly available, {GPT}-4.},
	number = {{arXiv}:2209.00840},
	publisher = {{arXiv}},
	author = {Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Zhou, Wenfei and Coady, James and Peng, David and Qiao, Yujie and Benson, Luke and Sun, Lucy and Wardle-Solano, Alex and Szabo, Hannah and Zubova, Ekaterina and Burtell, Matthew and Fan, Jonathan and Liu, Yixin and Wong, Brian and Sailor, Malcolm and Ni, Ansong and Nan, Linyong and Kasai, Jungo and Yu, Tao and Zhang, Rui and Fabbri, Alexander R. and Kryscinski, Wojciech and Yavuz, Semih and Liu, Ye and Lin, Xi Victoria and Joty, Shafiq and Zhou, Yingbo and Xiong, Caiming and Ying, Rex and Cohan, Arman and Radev, Dragomir},
	urldate = {2024-05-29},
	date = {2024-05-17},
	eprinttype = {arxiv},
	eprint = {2209.00840 [cs]},
}

@misc{bar-haimArgumentsKeyPoints2020,
	title = {From Arguments to Key Points: Towards Automatic Argument Summarization},
	url = {http://arxiv.org/abs/2005.01619},
	shorttitle = {From Arguments to Key Points},
	abstract = {Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed "key points", each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.},
	number = {{arXiv}:2005.01619},
	publisher = {{arXiv}},
	author = {Bar-Haim, Roy and Eden, Lilach and Friedman, Roni and Kantor, Yoav and Lahav, Dan and Slonim, Noam},
	urldate = {2024-05-29},
	date = {2020-06-09},
	eprinttype = {arxiv},
	eprint = {2005.01619 [cs]},
}

@inproceedings{jiangFreebaseQANewFactoid2019,
	location = {Minneapolis, Minnesota},
	title = {{FreebaseQA}: A New Factoid {QA} Data Set Matching Trivia-Style Question-Answer Pairs with Freebase},
	url = {https://aclanthology.org/N19-1028},
	doi = {10/ggnqnw},
	shorttitle = {{FreebaseQA}},
	abstract = {In this paper, we present a new data set, named {FreebaseQA}, for open-domain factoid question answering ({QA}) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove any false positive in these matched triples. Using this method, we are able to efficiently generate over 54K matches from about 28K unique questions with minimal cost. Our analysis shows that this data set is suitable for model training in factoid {QA} tasks beyond simpler questions since {FreebaseQA} provides more linguistically sophisticated questions than other existing data sets.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {318--323},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Kelvin and Wu, Dekun and Jiang, Hui},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	urldate = {2024-05-29},
	date = {2019-06},
}

@misc{marasovicFewShotSelfRationalizationNatural2022,
	title = {Few-Shot Self-Rationalization with Natural Language Prompts},
	url = {http://arxiv.org/abs/2111.08284},
	abstract = {Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with {NLP} systems. These models are, however, currently trained with a large amount of human-written free-text explanations for each task which hinders their broader usage. We propose to study a more realistic setting of self-rationalization using few training examples. We present {FEB} -- a standardized collection of four existing English-language datasets and associated metrics. We identify the right prompting approach by extensively exploring natural language prompts on {FEB}. Then, by using this prompt and scaling the model size, we demonstrate that making progress on few-shot self-rationalization is possible. We show there is still ample room for improvement in this task: the average plausibility of generated explanations assessed by human annotators is at most 51\% (with {GPT}-3), while plausibility of human explanations is 76\%. We hope that {FEB} and our proposed approach will spur the community to take on the few-shot self-rationalization challenge.},
	number = {{arXiv}:2111.08284},
	publisher = {{arXiv}},
	author = {Marasović, Ana and Beltagy, Iz and Downey, Doug and Peters, Matthew E.},
	urldate = {2024-05-29},
	date = {2022-04-25},
	eprinttype = {arxiv},
	eprint = {2111.08284 [cs]},
}

@misc{brazinskasFewShotLearningOpinion2020,
	title = {Few-Shot Learning for Opinion Summarization},
	url = {http://arxiv.org/abs/2004.14884},
	abstract = {Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.},
	number = {{arXiv}:2004.14884},
	publisher = {{arXiv}},
	author = {Bražinskas, Arthur and Lapata, Mirella and Titov, Ivan},
	urldate = {2024-05-29},
	date = {2020-10-10},
	eprinttype = {arxiv},
	eprint = {2004.14884 [cs, stat]},
}

@inproceedings{bojarFindings2016Conference2016,
	location = {Berlin, Germany},
	title = {Findings of the 2016 Conference on Machine Translation},
	url = {https://aclanthology.org/W16-2301},
	doi = {10/gfv3wm},
	eventtitle = {{WMT} 2016},
	pages = {131--198},
	booktitle = {Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
	publisher = {Association for Computational Linguistics},
	author = {Bojar, Ondřej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Jimeno Yepes, Antonio and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and Negri, Matteo and Névéol, Aurélie and Neves, Mariana and Popel, Martin and Post, Matt and Rubino, Raphael and Scarton, Carolina and Specia, Lucia and Turchi, Marco and Verspoor, Karin and Zampieri, Marcos},
	editor = {Bojar, Ondřej and Buck, Christian and Chatterjee, Rajen and Federmann, Christian and Guillou, Liane and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Névéol, Aurélie and Neves, Mariana and Pecina, Pavel and Popel, Martin and Koehn, Philipp and Monz, Christof and Negri, Matteo and Post, Matt and Specia, Lucia and Verspoor, Karin and Tiedemann, Jörg and Turchi, Marco},
	urldate = {2024-05-29},
	date = {2016-08},
}

@inproceedings{bojarFindings2014Workshop2014,
	location = {Baltimore, Maryland, {USA}},
	title = {Findings of the 2014 Workshop on Statistical Machine Translation},
	url = {https://aclanthology.org/W14-3302},
	doi = {10/ggwcsd},
	eventtitle = {{WMT} 2014},
	pages = {12--58},
	booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
	publisher = {Association for Computational Linguistics},
	author = {Bojar, Ondřej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and Soricut, Radu and Specia, Lucia and Tamchyna, Aleš},
	editor = {Bojar, Ondřej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Monz, Christof and Post, Matt and Specia, Lucia},
	urldate = {2024-05-29},
	date = {2014-06},
}

@misc{dingFewNERDFewShotNamed2021,
	title = {Few-{NERD}: A Few-Shot Named Entity Recognition Dataset},
	url = {http://arxiv.org/abs/2105.07464},
	shorttitle = {Few-{NERD}},
	abstract = {Recently, considerable literature has grown up around the theme of few-shot named entity recognition ({NER}), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised {NER} datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-{NERD}, a large-scale human-annotated few-shot {NER} dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-{NERD} consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type. To the best of our knowledge, this is the first few-shot {NER} dataset and the largest human-crafted {NER} dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-{NERD} is challenging and the problem requires further research. We make Few-{NERD} public at https://ningding97.github.io/fewnerd/.},
	number = {{arXiv}:2105.07464},
	publisher = {{arXiv}},
	author = {Ding, Ning and Xu, Guangwei and Chen, Yulin and Wang, Xiaobin and Han, Xu and Xie, Pengjun and Zheng, Hai-Tao and Liu, Zhiyuan},
	urldate = {2024-05-29},
	date = {2021-09-01},
	eprinttype = {arxiv},
	eprint = {2105.07464 [cs]},
}

@misc{orbachFacts2StoryControllingText2020,
	title = {Facts2Story: Controlling Text Generation by Key Facts},
	url = {http://arxiv.org/abs/2012.04332},
	shorttitle = {Facts2Story},
	abstract = {Recent advancements in self-attention neural network architectures have raised the bar for open-ended text generation. Yet, while current methods are capable of producing a coherent text which is several hundred words long, attaining control over the content that is being generated -- as well as evaluating it -- are still open questions. We propose a controlled generation task which is based on expanding a sequence of facts, expressed in natural language, into a longer narrative. We introduce human-based evaluation metrics for this task, as well as a method for deriving a large training dataset. We evaluate three methods on this task, based on fine-tuning pre-trained models. We show that while auto-regressive, unidirectional Language Models such as {GPT}2 produce better fluency, they struggle to adhere to the requested facts. We propose a plan-and-cloze model (using fine-tuned {XLNet}) which produces competitive fluency while adhering to the requested content.},
	number = {{arXiv}:2012.04332},
	publisher = {{arXiv}},
	author = {Orbach, Eyal and Goldberg, Yoav},
	urldate = {2024-05-29},
	date = {2020-12-08},
	eprinttype = {arxiv},
	eprint = {2012.04332 [cs]},
}

@article{amirkhaniFarsTailPersianNatural2023,
	title = {{FarsTail}: A Persian Natural Language Inference Dataset},
	issn = {1432-7643, 1433-7479},
	url = {http://arxiv.org/abs/2009.08820},
	doi = {10/gtwwt3},
	shorttitle = {{FarsTail}},
	abstract = {Natural language inference ({NLI}) is known as one of the central tasks in natural language processing ({NLP}) which encapsulates many fundamental aspects of language understanding. With the considerable achievements of data-hungry deep learning methods in {NLP} tasks, a great amount of effort has been devoted to develop more diverse datasets for different languages. In this paper, we present a new dataset for the {NLI} task in the Persian language, also known as Farsi, which is one of the dominant languages in the Middle East. This dataset, named {FarsTail}, includes 10,367 samples which are provided in both the Persian language as well as the indexed format to be useful for non-Persian researchers. The samples are generated from 3,539 multiple-choice questions with the least amount of annotator interventions in a way similar to the {SciTail} dataset. A carefully designed multi-step process is adopted to ensure the quality of the dataset. We also present the results of traditional and state-of-the-art methods on {FarsTail} including different embedding methods such as word2vec, {fastText}, {ELMo}, {BERT}, and {LASER}, as well as different modeling approaches such as {DecompAtt}, {ESIM}, {HBMP}, and {ULMFiT} to provide a solid baseline for the future research. The best obtained test accuracy is 83.38\% which shows that there is a big room for improving the current methods to be useful for real-world {NLP} applications in different languages. We also investigate the extent to which the models exploit superficial clues, also known as dataset biases, in {FarsTail}, and partition the test set into easy and hard subsets according to the success of biased models. The dataset is available at https://github.com/dml-qom/{FarsTail}},
	journaltitle = {Soft Computing},
	shortjournal = {Soft Comput},
	author = {Amirkhani, Hossein and {AzariJafari}, Mohammad and Pourjafari, Zohreh and Faridan-Jahromi, Soroush and Kouhkan, Zeinab and Amirak, Azadeh},
	urldate = {2024-05-29},
	date = {2023-07-07},
	eprinttype = {arxiv},
	eprint = {2009.08820 [cs]},
}

@misc{yanakaExploringTransitivityNeural2021,
	title = {Exploring Transitivity in Neural {NLI} Models through Veridicality},
	url = {http://arxiv.org/abs/2101.10713},
	abstract = {Despite the recent success of deep neural networks in natural language processing, the extent to which they can demonstrate human-like generalization capacities for natural language understanding remains unclear. We explore this issue in the domain of natural language inference ({NLI}), focusing on the transitivity of inference relations, a fundamental property for systematically drawing inferences. A model capturing transitivity can compose basic inference patterns and draw new inferences. We introduce an analysis method using synthetic and naturalistic {NLI} datasets involving clause-embedding verbs to evaluate whether models can perform transitivity inferences composed of veridical inferences and arbitrary inference types. We find that current {NLI} models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples. The data and code for our analysis are publicly available at https://github.com/verypluming/transitivity.},
	number = {{arXiv}:2101.10713},
	publisher = {{arXiv}},
	author = {Yanaka, Hitomi and Mineshima, Koji and Inui, Kentaro},
	urldate = {2024-05-29},
	date = {2021-01-26},
	eprinttype = {arxiv},
	eprint = {2101.10713 [cs]},
}

@misc{fengExploringEndtoEndDifferentiable2020,
	title = {Exploring End-to-End Differentiable Natural Logic Modeling},
	url = {http://arxiv.org/abs/2011.04044},
	abstract = {We explore end-to-end trained differentiable models that integrate natural logic with neural networks, aiming to keep the backbone of natural language reasoning based on the natural logic formalism while introducing subsymbolic vector representations and neural components. The proposed model adapts module networks to model natural logic operations, which is enhanced with a memory component to model contextual information. Experiments show that the proposed framework can effectively model monotonicity-based reasoning, compared to the baseline neural network models without built-in inductive bias for monotonicity-based reasoning. Our proposed model shows to be robust when transferred from upward to downward inference. We perform further analyses on the performance of the proposed model on aggregation, showing the effectiveness of the proposed subcomponents on helping achieve better intermediate aggregation performance.},
	number = {{arXiv}:2011.04044},
	publisher = {{arXiv}},
	author = {Feng, Yufei and Zheng, Zi'ou and Liu, Quan and Greenspan, Michael and Zhu, Xiaodan},
	urldate = {2024-05-29},
	date = {2020-11-08},
	eprinttype = {arxiv},
	eprint = {2011.04044 [cs]},
}

@misc{waddenFactFictionVerifying2020,
	title = {Fact or Fiction: Verifying Scientific Claims},
	url = {http://arxiv.org/abs/2004.14974},
	shorttitle = {Fact or Fiction},
	abstract = {We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that {SUPPORTS} or {REFUTES} a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct {SciFact}, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for {SciFact}, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to {COVID}-19 by identifying evidence from the {CORD}-19 corpus. Our experiments indicate that {SciFact} will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and {COVID}-19 fact-checking demo are available at https://scifact.apps.allenai.org.},
	number = {{arXiv}:2004.14974},
	publisher = {{arXiv}},
	author = {Wadden, David and Lin, Shanchuan and Lo, Kyle and Wang, Lucy Lu and van Zuylen, Madeleine and Cohan, Arman and Hajishirzi, Hannaneh},
	urldate = {2024-05-29},
	date = {2020-10-03},
	eprinttype = {arxiv},
	eprint = {2004.14974 [cs]},
}

@inproceedings{aggarwalExplanationsCommonsenseQANew2021,
	location = {Online},
	title = {Explanations for {CommonsenseQA}: New Dataset and Models},
	url = {https://aclanthology.org/2021.acl-long.238},
	doi = {10/gmbpvn},
	shorttitle = {Explanations for {CommonsenseQA}},
	abstract = {{CommonsenseQA} ({CQA}) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering ({QA}) task. Whereas the prior work has mostly focused on proposing {QA} models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called {ECQA}) of positive and negative properties, as well as free-flow explanations, for 11K {QA} pairs taken from the {CQA} dataset. We propose a latent representation based property retrieval model as well as a {GPT}-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats {BM}25 baseline by a relative gain of 100\% in F\_1 score, property generation model achieves a respectable F\_1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {3050--3065},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Aggarwal, Shourya and Mandowara, Divyanshu and Agrawal, Vishwajeet and Khandelwal, Dinesh and Singla, Parag and Garg, Dinesh},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	urldate = {2024-05-29},
	date = {2021-08},
}

@misc{kotonyaExplainableAutomatedFactChecking2020,
	title = {Explainable Automated Fact-Checking for Public Health Claims},
	url = {http://arxiv.org/abs/2010.09926},
	abstract = {Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset {PUBHEALTH} of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.},
	number = {{arXiv}:2010.09926},
	publisher = {{arXiv}},
	author = {Kotonya, Neema and Toni, Francesca},
	urldate = {2024-05-29},
	date = {2020-10-19},
	eprinttype = {arxiv},
	eprint = {2010.09926 [cs]},
}

@misc{nematzadehEvaluatingTheoryMind2018,
	title = {Evaluating Theory of Mind in Question Answering},
	url = {http://arxiv.org/abs/1808.09352},
	abstract = {We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models' accuracy decreases notably when random sentences are introduced to the tasks at test.},
	number = {{arXiv}:1808.09352},
	publisher = {{arXiv}},
	author = {Nematzadeh, Aida and Burns, Kaylee and Grant, Erin and Gopnik, Alison and Griffiths, Thomas L.},
	urldate = {2024-05-29},
	date = {2018-08-28},
	eprinttype = {arxiv},
	eprint = {1808.09352 [cs]},
}

@misc{rashkinEvent2MindCommonsenseInference2019,
	title = {Event2Mind: Commonsense Inference on Events, Intents, and Reactions},
	url = {http://arxiv.org/abs/1805.06939},
	shorttitle = {Event2Mind},
	abstract = {We investigate a new commonsense inference task: given an event described in a short free-form text ("X drinks coffee in the morning"), a system reasons about the likely intents ("X wants to stay awake") and reactions ("X feels alert") of the event's participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people's intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.},
	number = {{arXiv}:1805.06939},
	publisher = {{arXiv}},
	author = {Rashkin, Hannah and Sap, Maarten and Allaway, Emily and Smith, Noah A. and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2019-06-14},
	eprinttype = {arxiv},
	eprint = {1805.06939 [cs]},
}

@inproceedings{santusEVALutionEvolvingSemantic2015,
	location = {Beijing, China},
	title = {{EVALution} 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models},
	url = {https://aclanthology.org/W15-4208},
	doi = {10/gtwwf3},
	shorttitle = {{EVALution} 1.0},
	eventtitle = {{LDL} 2015},
	pages = {64--69},
	booktitle = {Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications},
	publisher = {Association for Computational Linguistics},
	author = {Santus, Enrico and Yung, Frances and Lenci, Alessandro and Huang, Chu-Ren},
	editor = {Chiarcos, Christian and {McCrae}, John Philip and Osenova, Petya and Cimiano, Philipp and Ide, Nancy},
	urldate = {2024-05-29},
	date = {2015-07},
}

@article{mollasETHOSOnlineHate2022,
	title = {{ETHOS}: an Online Hate Speech Detection Dataset},
	volume = {8},
	issn = {2199-4536, 2198-6053},
	url = {http://arxiv.org/abs/2006.08328},
	doi = {10/gtwwtt},
	shorttitle = {{ETHOS}},
	abstract = {Online hate speech is a recent problem in our society that is rising at a steady pace by leveraging the vulnerabilities of the corresponding regimes that characterise most social media platforms. This phenomenon is primarily fostered by offensive comments, either during user interaction or in the form of a posted multimedia context. Nowadays, giant corporations own platforms where millions of users log in every day, and protection from exposure to similar phenomena appears to be necessary in order to comply with the corresponding legislation and maintain a high level of service quality. A robust and reliable system for detecting and preventing the uploading of relevant content will have a significant impact on our digitally interconnected society. Several aspects of our daily lives are undeniably linked to our social profiles, making us vulnerable to abusive behaviours. As a result, the lack of accurate hate speech detection mechanisms would severely degrade the overall user experience, although its erroneous operation would pose many ethical concerns. In this paper, we present '{ETHOS}', a textual dataset with two variants: binary and multi-label, based on {YouTube} and Reddit comments validated using the Figure-Eight crowdsourcing platform. Furthermore, we present the annotation protocol used to create this dataset: an active sampling procedure for balancing our data in relation to the various aspects defined. Our key assumption is that, even gaining a small amount of labelled data from such a time-consuming process, we can guarantee hate speech occurrences in the examined material.},
	pages = {4663--4678},
	number = {6},
	journaltitle = {Complex \& Intelligent Systems},
	shortjournal = {Complex Intell. Syst.},
	author = {Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
	urldate = {2024-05-29},
	date = {2022-12},
	eprinttype = {arxiv},
	eprint = {2006.08328 [cs, stat]},
}

@misc{deyoungERASERBenchmarkEvaluate2020,
	title = {{ERASER}: A Benchmark to Evaluate Rationalized {NLP} Models},
	url = {http://arxiv.org/abs/1911.03429},
	shorttitle = {{ERASER}},
	abstract = {State-of-the-art models in {NLP} are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for {NLP} that reveal the `reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning ({ERASER}) benchmark to advance research on interpretable models in {NLP}. This benchmark comprises multiple datasets and tasks for which human annotations of "rationales" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable {NLP} systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/},
	number = {{arXiv}:1911.03429},
	publisher = {{arXiv}},
	author = {{DeYoung}, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C.},
	urldate = {2024-05-29},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {1911.03429 [cs]},
}

@misc{stanovskyEvaluatingGenderBias2019,
	title = {Evaluating Gender Bias in Machine Translation},
	url = {http://arxiv.org/abs/1906.00591},
	abstract = {We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation ({MT}). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., "The doctor asked the nurse to help her in the operation"). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word "doctor"). Our analyses show that four popular industrial {MT} systems and two recent state-of-the-art academic {MT} models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are made publicly available.},
	number = {{arXiv}:1906.00591},
	publisher = {{arXiv}},
	author = {Stanovsky, Gabriel and Smith, Noah A. and Zettlemoyer, Luke},
	urldate = {2024-05-29},
	date = {2019-06-03},
	eprinttype = {arxiv},
	eprint = {1906.00591 [cs]},
}

@misc{ravichanderEQUATEBenchmarkEvaluation2019,
	title = {{EQUATE}: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference},
	url = {http://arxiv.org/abs/1901.03735},
	shorttitle = {{EQUATE}},
	abstract = {Quantitative reasoning is a higher-order reasoning skill that any intelligent natural language understanding system can reasonably be expected to handle. We present {EQUATE} (Evaluating Quantitative Understanding Aptitude in Textual Entailment), a new framework for quantitative reasoning in textual entailment. We benchmark the performance of 9 published {NLI} models on {EQUATE}, and find that on average, state-of-the-art methods do not achieve an absolute improvement over a majority-class baseline, suggesting that they do not implicitly learn to reason with quantities. We establish a new baseline Q-{REAS} that manipulates quantities symbolically. In comparison to the best performing {NLI} model, it achieves success on numerical reasoning tests (+24.2\%), but has limited verbal reasoning capabilities (-8.1\%). We hope our evaluation framework will support the development of models of quantitative reasoning in language understanding.},
	number = {{arXiv}:1901.03735},
	publisher = {{arXiv}},
	author = {Ravichander, Abhilasha and Naik, Aakanksha and Rose, Carolyn and Hovy, Eduard},
	urldate = {2024-05-29},
	date = {2019-10-26},
	eprinttype = {arxiv},
	eprint = {1901.03735 [cs]},
}

@misc{mccreeryEffectiveTransferLearning2020,
	title = {Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to {COVID}-19 {FAQs}},
	url = {http://arxiv.org/abs/2008.13546},
	shorttitle = {Effective Transfer Learning for Identifying Similar Questions},
	abstract = {People increasingly search online for answers to their medical questions but the rate at which medical questions are asked online significantly exceeds the capacity of qualified people to answer them. This leaves many questions unanswered or inadequately answered. Many of these questions are not unique, and reliable identification of similar questions would enable more efficient and effective question answering schema. {COVID}-19 has only exacerbated this problem. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online {FAQs}, but there is no way for people to ask their question and know if it is answered on one of these pages. While many research efforts have focused on the problem of general question similarity, these approaches do not generalize well to domains that require expert knowledge to determine semantic similarity, such as the medical domain. In this paper, we show how a double fine-tuning approach of pretraining a neural network on medical question-answer pairs followed by fine-tuning on medical question-question pairs is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pretraining tasks yield an accuracy below 78.7\% on this task, our model achieves an accuracy of 82.6\% with the same number of training examples, an accuracy of 80.0\% with a much smaller training set, and an accuracy of 84.5\% when the full corpus of medical question-answer data is used. We also describe a currently live system that uses the trained model to match user questions to {COVID}-related {FAQs}.},
	number = {{arXiv}:2008.13546},
	publisher = {{arXiv}},
	author = {{McCreery}, Clara H. and Katariya, Namit and Kannan, Anitha and Chablani, Manish and Amatriain, Xavier},
	urldate = {2024-05-29},
	date = {2020-08-04},
	eprinttype = {arxiv},
	eprint = {2008.13546 [cs]},
}

@misc{mahabadiEndtoEndBiasMitigation2020,
	title = {End-to-End Bias Mitigation by Modelling Biases in Corpora},
	url = {http://arxiv.org/abs/1909.06321},
	abstract = {Several recent studies have shown that strong natural language understanding ({NLU}) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models' predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in {\textbackslash}url\{https://github.com/rabeehk/robust-nli\}.},
	number = {{arXiv}:1909.06321},
	publisher = {{arXiv}},
	author = {Mahabadi, Rabeeh Karimi and Belinkov, Yonatan and Henderson, James},
	urldate = {2024-05-29},
	date = {2020-04-23},
	eprinttype = {arxiv},
	eprint = {1909.06321 [cs]},
}

@misc{duECARENewDataset2022,
	title = {e-{CARE}: a New Dataset for Exploring Explainable Causal Reasoning},
	url = {http://arxiv.org/abs/2205.05849},
	shorttitle = {e-{CARE}},
	abstract = {Understanding causality has vital importance for various Natural Language Processing ({NLP}) applications. Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal facts to facilitate the causal reasoning process. However, such explanation information still remains absent in existing causal reasoning resources. In this paper, we fill this gap by presenting a human-annotated explainable {CAusal} {REasoning} dataset (e-{CARE}), which contains over 21K causal reasoning questions, together with natural language formed explanations of the causal questions. Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models.},
	number = {{arXiv}:2205.05849},
	publisher = {{arXiv}},
	author = {Du, Li and Ding, Xiao and Xiong, Kai and Liu, Ting and Qin, Bing},
	urldate = {2024-05-29},
	date = {2022-05-11},
	eprinttype = {arxiv},
	eprint = {2205.05849 [cs]},
}

@inproceedings{chenEKARBenchmarkRationalizing2022,
	title = {E-{KAR}: A Benchmark for Rationalizing Natural Language Analogical Reasoning},
	url = {http://arxiv.org/abs/2203.08480},
	doi = {10/grm22k},
	shorttitle = {E-{KAR}},
	abstract = {The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-{KAR}). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer. Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area.},
	pages = {3941--3955},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2022},
	author = {Chen, Jiangjie and Xu, Rui and Fu, Ziquan and Shi, Wei and Li, Zhongqiao and Zhang, Xinbo and Sun, Changzhi and Li, Lei and Xiao, Yanghua and Zhou, Hao},
	urldate = {2024-05-29},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2203.08480 [cs]},
}

@misc{camburuESNLINaturalLanguage2018,
	title = {e-{SNLI}: Natural Language Inference with Natural Language Explanations},
	url = {http://arxiv.org/abs/1812.01193},
	shorttitle = {e-{SNLI}},
	abstract = {In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-{SNLI}, can be used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain {NLI} datasets. Our dataset thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust.},
	number = {{arXiv}:1812.01193},
	publisher = {{arXiv}},
	author = {Camburu, Oana-Maria and Rocktäschel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
	urldate = {2024-05-29},
	date = {2018-12-06},
	eprinttype = {arxiv},
	eprint = {1812.01193 [cs]},
}

@inproceedings{zlabingerEffectiveCrowdAnnotationParticipants2020,
	location = {Online},
	title = {Effective Crowd-Annotation of Participants, Interventions, and Outcomes in the Text of Clinical Trial Reports},
	url = {https://aclanthology.org/2020.findings-emnlp.274},
	doi = {10/gtwwfp},
	abstract = {The search for Participants, Interventions, and Outcomes ({PIO}) in clinical trial reports is a critical task in Evidence Based Medicine. For an automatic {PIO} extraction, high-quality corpora are needed. Obtaining such a corpus from crowdworkers, however, has been shown to be ineffective since (i) workers usually lack domain-specific expertise to conduct the task with sufficient quality, and (ii) the standard approach of annotating entire abstracts of trial reports as one task-instance (i.e. {HIT}) leads to an uneven distribution in task effort. In this paper, we switch from entire abstract to sentence annotation, referred to as the {SenBase} approach. We build upon {SenBase} in {SenSupport}, where we compensate the lack of domain-specific expertise of crowdworkers by showing for each task-instance similar sentences that are already annotated by experts. Such tailored task-instance examples are retrieved via unsupervised semantic short-text similarity ({SSTS}) method – and we evaluate nine methods to find an effective solution for {SenSupport}. We compute the Cohen's Kappa agreement between crowd-annotations and gold standard annotations and show that (i) both sentence-based approaches outperform a Baseline approach where entire abstracts are annotated; (ii) supporting annotators with tailored task-instance examples is the best performing approach with Kappa agreements of 0.78/0.75/0.69 for P, I, and O respectively.},
	eventtitle = {Findings 2020},
	pages = {3064--3074},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Zlabinger, Markus and Sabou, Marta and Hofstätter, Sebastian and Hanbury, Allan},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-05-29},
	date = {2020-11},
}

@misc{pottsDynaSentDynamicBenchmark2020,
	title = {{DynaSent}: A Dynamic Benchmark for Sentiment Analysis},
	url = {http://arxiv.org/abs/2012.15349},
	shorttitle = {{DynaSent}},
	abstract = {We introduce {DynaSent} ('Dynamic Sentiment'), a new English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. {DynaSent} combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. {DynaSent} has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best models we have been able to develop; when future models solve this task, we will use them to create {DynaSent} version 2, continuing the dynamic evolution of this benchmark. Here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts. We also present evidence that {DynaSent}'s Neutral category is more coherent than the comparable category in other benchmarks, and we motivate training models from scratch for each round over successive fine-tuning.},
	number = {{arXiv}:2012.15349},
	publisher = {{arXiv}},
	author = {Potts, Christopher and Wu, Zhengxuan and Geiger, Atticus and Kiela, Douwe},
	urldate = {2024-05-29},
	date = {2020-12-30},
	eprinttype = {arxiv},
	eprint = {2012.15349 [cs]},
}

@misc{duaDROPReadingComprehension2019,
	title = {{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
	url = {http://arxiv.org/abs/1903.00161},
	shorttitle = {{DROP}},
	abstract = {Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, {DROP}, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7\% F1 on our generalized accuracy metric, while expert human performance is 96.0\%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0\% F1.},
	number = {{arXiv}:1903.00161},
	publisher = {{arXiv}},
	author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
	urldate = {2024-05-29},
	date = {2019-04-16},
	eprinttype = {arxiv},
	eprint = {1903.00161 [cs]},
}

@misc{sunDREAMChallengeDataset2019,
	title = {{DREAM}: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension},
	url = {http://arxiv.org/abs/1902.00164},
	shorttitle = {{DREAM}},
	abstract = {We present {DREAM}, the first dialogue-based multiple-choice reading comprehension dataset. Collected from English-as-a-foreign-language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our dataset contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension datasets, {DREAM} is the first to focus on in-depth multi-turn multi-party dialogue understanding. {DREAM} is likely to present significant challenges for existing reading comprehension systems: 84\% of answers are non-extractive, 85\% of questions require reasoning beyond a single sentence, and 34\% of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the {DREAM} dataset show the effectiveness of dialogue structure and general world knowledge. {DREAM} will be available at https://dataset.org/dream/.},
	number = {{arXiv}:1902.00164},
	publisher = {{arXiv}},
	author = {Sun, Kai and Yu, Dian and Chen, Jianshu and Yu, Dong and Choi, Yejin and Cardie, Claire},
	urldate = {2024-05-29},
	date = {2019-01-31},
	eprinttype = {arxiv},
	eprint = {1902.00164 [cs]},
}

@misc{parrishDoesPuttingLinguist2021,
	title = {Does Putting a Linguist in the Loop Improve {NLU} Data Collection?},
	url = {http://arxiv.org/abs/2104.07179},
	abstract = {Many crowdsourced {NLP} datasets contain systematic gaps and biases that are identified only after data collection is complete. Identifying these issues from early data samples during crowdsourcing should make mitigation more efficient, especially when done iteratively. We take natural language inference as a test case and ask whether it is beneficial to put a linguist `in the loop' during data collection to dynamically identify and address gaps in the data by introducing novel constraints on the task. We directly compare three data collection protocols: (i) a baseline protocol, (ii) a linguist-in-the-loop intervention with iteratively-updated constraints on the task, and (iii) an extension of linguist-in-the-loop that provides direct interaction between linguists and crowdworkers via a chatroom. The datasets collected with linguist involvement are more reliably challenging than baseline, without loss of quality. But we see no evidence that using this data in training leads to better out-of-domain model performance, and the addition of a chat platform has no measurable effect on the resulting dataset. We suggest integrating expert analysis {\textbackslash}textit\{during\} data collection so that the expert can dynamically address gaps and biases in the dataset.},
	number = {{arXiv}:2104.07179},
	publisher = {{arXiv}},
	author = {Parrish, Alicia and Huang, William and Agha, Omar and Lee, Soo-Hwan and Nangia, Nikita and Warstadt, Alex and Aggarwal, Karmanya and Allaway, Emily and Linzen, Tal and Bowman, Samuel R.},
	urldate = {2024-05-29},
	date = {2021-04-14},
	eprinttype = {arxiv},
	eprint = {2104.07179 [cs]},
}

@misc{camposDoQAAccessingDomainSpecific2020,
	title = {{DoQA} -- Accessing Domain-Specific {FAQs} via Conversational {QA}},
	url = {http://arxiv.org/abs/2005.01328},
	abstract = {The goal of this work is to build conversational Question Answering ({QA}) interfaces for the large body of domain-specific information available in {FAQ} sites. We present {DoQA}, a dataset with 2,437 dialogues and 10,917 {QA} pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, {DoQA} comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. In addition, we introduce a more realistic information retrieval({IR}) scenario where the system needs to find the answer in any of the {FAQ} documents. The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia {QA} dataset and fine tuning on a single {FAQ} domain, it is possible to build high quality conversational {QA} systems for {FAQs} without in-domain training data. The good results carry over into the more challenging {IR} scenario. In both cases, there is still ample room for improvement, as indicated by the higher human upperbound.},
	number = {{arXiv}:2005.01328},
	publisher = {{arXiv}},
	author = {Campos, Jon Ander and Otegi, Arantxa and Soroa, Aitor and Deriu, Jan and Cieliebak, Mark and Agirre, Eneko},
	urldate = {2024-05-29},
	date = {2020-05-18},
	eprinttype = {arxiv},
	eprint = {2005.01328 [cs]},
}

@misc{kejriwalFinetunedCommonsenseLanguage2020,
	title = {Do Fine-tuned Commonsense Language Models Really Generalize?},
	url = {http://arxiv.org/abs/2011.09159},
	abstract = {Recently, transformer-based methods such as {RoBERTa} and {GPT}-3 have led to significant experimental advances in natural language processing tasks such as question answering and commonsense reasoning. The latter is typically evaluated through multiple benchmarks framed as multiple-choice instances of the former. According to influential leaderboards hosted by the Allen Institute (evaluating state-of-the-art performance on commonsense reasoning benchmarks), models based on such transformer methods are approaching human-like performance and have average accuracy well over 80\% on many benchmarks. Since these are commonsense benchmarks, a model that generalizes on commonsense reasoning should not experience much performance loss across multiple commonsense benchmarks. In this paper, we study the generalization issue in detail by designing and conducting a rigorous scientific study. Using five common benchmarks, multiple controls and statistical analysis, we find clear evidence that fine-tuned commonsense language models still do not generalize well, even with moderate changes to the experimental setup, and may, in fact, be susceptible to dataset bias. We also perform selective studies, including qualitative and consistency analyses, to gain deeper insight into the problem.},
	number = {{arXiv}:2011.09159},
	publisher = {{arXiv}},
	author = {Kejriwal, Mayank and Shen, Ke},
	urldate = {2024-05-29},
	date = {2020-11-18},
	eprinttype = {arxiv},
	eprint = {2011.09159 [cs]},
}

@misc{bhakthavatsalamDogsHaveWhiskers2020,
	title = {Do Dogs have Whiskers? A New Knowledge Base of {hasPart} Relations},
	url = {http://arxiv.org/abs/2006.07510},
	shorttitle = {Do Dogs have Whiskers?},
	abstract = {We present a new knowledge-base of {hasPart} relationships, extracted from a large corpus of generic statements. Complementary to other resources available, it is the first which is all three of: accurate (90\% precision), salient (covers relationships a person may mention), and has high coverage of common terms (approximated as within a 10 year old's vocabulary), as well as having several times more {hasPart} entries than in the popular ontologies {ConceptNet} and {WordNet}. In addition, it contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and {WordNet}. The knowledge base is available at https://allenai.org/data/haspartkb},
	number = {{arXiv}:2006.07510},
	publisher = {{arXiv}},
	author = {Bhakthavatsalam, Sumithra and Richardson, Kyle and Tandon, Niket and Clark, Peter},
	urldate = {2024-05-29},
	date = {2020-06-12},
	eprinttype = {arxiv},
	eprint = {2006.07510 [cs]},
}

@misc{wangDoesItMake2020,
	title = {Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation},
	url = {http://arxiv.org/abs/1906.00363},
	shorttitle = {Does It Make Sense?},
	abstract = {Introducing common sense to natural language understanding systems has received increasing research attention. It remains a fundamental question on how to evaluate whether a system has a sense making capability. Existing benchmarks measures commonsense knowledge indirectly and without explanation. In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense making.},
	number = {{arXiv}:1906.00363},
	publisher = {{arXiv}},
	author = {Wang, Cunxiang and Liang, Shuailong and Zhang, Yue and Li, Xiaonan and Gao, Tian},
	urldate = {2024-05-29},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {1906.00363 [cs]},
}

@misc{perezDiscoveringLanguageModel2022,
	title = {Discovering Language Model Behaviors with Model-Written Evaluations},
	url = {http://arxiv.org/abs/2212.09251},
	abstract = {As language models ({LMs}) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with {LMs}. We explore approaches with varying amounts of human effort, from instructing {LMs} to write yes/no questions to making complex Winogender schemas with multiple stages of {LM}-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where {LMs} get worse with size. Larger {LMs} repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in {RL} from Human Feedback ({RLHF}), where more {RLHF} makes {LMs} worse. For example, {RLHF} makes {LMs} express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, {LM}-written evaluations are high-quality and let us quickly discover many novel {LM} behaviors.},
	number = {{arXiv}:2212.09251},
	publisher = {{arXiv}},
	author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and {McKinnon}, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and {DasSarma}, Nova and Rausch, Oliver and Larson, Robin and {McCandlish}, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
	urldate = {2024-05-29},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2212.09251 [cs]},
}

@misc{bhargavaDiscoSenseCommonsenseReasoning2022,
	title = {{DiscoSense}: Commonsense Reasoning with Discourse Connectives},
	url = {http://arxiv.org/abs/2210.12478},
	shorttitle = {{DiscoSense}},
	abstract = {We present {DiscoSense}, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in {DiscoSense} using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that state-of-the-art pre-trained language models struggle to perform well on {DiscoSense}, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.},
	number = {{arXiv}:2210.12478},
	publisher = {{arXiv}},
	author = {Bhargava, Prajjwal and Ng, Vincent},
	urldate = {2024-05-29},
	date = {2022-10-22},
	eprinttype = {arxiv},
	eprint = {2210.12478 [cs]},
}

@inproceedings{ushioDistillingRelationEmbeddings2021,
	title = {Distilling Relation Embeddings from Pre-trained Language Models},
	url = {http://arxiv.org/abs/2110.15705},
	doi = {10/gr67zk},
	abstract = {Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository: https://github.com/asahi417/relbert},
	pages = {9044--9062},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	author = {Ushio, Asahi and Camacho-Collados, Jose and Schockaert, Steven},
	urldate = {2024-05-29},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2110.15705 [cs]},
}

@misc{gevaDidAristotleUse2021,
	title = {Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies},
	url = {http://arxiv.org/abs/2101.02235},
	shorttitle = {Did Aristotle Use a Laptop?},
	abstract = {A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce {StrategyQA}, a question answering ({QA}) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, {StrategyQA} includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in {StrategyQA} are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87\%) on this task, while our best baseline reaches an accuracy of \${\textbackslash}sim\$66\%.},
	number = {{arXiv}:2101.02235},
	publisher = {{arXiv}},
	author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
	urldate = {2024-05-29},
	date = {2021-01-06},
	eprinttype = {arxiv},
	eprint = {2101.02235 [cs]},
}

@misc{yuDialogueBasedRelationExtraction2020,
	title = {Dialogue-Based Relation Extraction},
	url = {http://arxiv.org/abs/2004.08056},
	abstract = {We present the first human-annotated dialogue-based relation extraction ({RE}) dataset {DialogRE}, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer {DialogRE} as a platform for studying cross-sentence {RE} as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional {RE} tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of {RE} methods in a conversational setting and investigate the performance of several representative {RE} methods on {DialogRE}. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. {DialogRE} is available at https://dataset.org/dialogre/.},
	number = {{arXiv}:2004.08056},
	publisher = {{arXiv}},
	author = {Yu, Dian and Sun, Kai and Cardie, Claire and Yu, Dong},
	urldate = {2024-05-29},
	date = {2020-04-16},
	eprinttype = {arxiv},
	eprint = {2004.08056 [cs]},
}

@misc{daiDialogInpaintingTurning2022,
	title = {Dialog Inpainting: Turning Documents into Dialogs},
	url = {http://arxiv.org/abs/2205.09073},
	shorttitle = {Dialog Inpainting},
	abstract = {Many important questions (e.g. "How to eat healthier?") require conversation to establish context and explore in depth. However, conversational question answering ({ConvQA}) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce {WikiDialog} and {WebDialog}, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing {ConvQA} dataset. Furthermore, human raters judge the answer adequacy and conversationality of {WikiDialog} to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train {ConvQA} retrieval systems, we significantly advance state-of-the-art across three benchmarks ({QReCC}, {OR}-{QuAC}, {TREC} {CAsT}) yielding up to 40\% relative gains on standard evaluation metrics.},
	number = {{arXiv}:2205.09073},
	publisher = {{arXiv}},
	author = {Dai, Zhuyun and Chaganty, Arun Tejasvi and Zhao, Vincent and Amini, Aida and Rashid, Qazi Mamunur and Green, Mike and Guu, Kelvin},
	urldate = {2024-05-29},
	date = {2022-05-31},
	eprinttype = {arxiv},
	eprint = {2205.09073 [cs]},
}

@misc{xuDetoxifyingLanguageModels2021,
	title = {Detoxifying Language Models Risks Marginalizing Minority Voices},
	url = {http://arxiv.org/abs/2104.06390},
	abstract = {Language models ({LMs}) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic {LM} generations. In this work, we show that current detoxification techniques hurt equity: they decrease the utility of {LMs} on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when {LMs} are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes {LMs} more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of {LMs}.},
	number = {{arXiv}:2104.06390},
	publisher = {{arXiv}},
	author = {Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
	urldate = {2024-05-29},
	date = {2021-04-13},
	eprinttype = {arxiv},
	eprint = {2104.06390 [cs]},
}

@misc{luoDetectingStanceMedia2021,
	title = {Detecting Stance in Media on Global Warming},
	url = {http://arxiv.org/abs/2010.15149},
	abstract = {Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, "Leading scientists agree that global warming is a serious concern," framing a clause which affirms their own stance ("that global warming is serious") as an opinion endorsed ("[scientists] agree") by a reputable source ("leading"). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: "Mistaken scientists claim [...]." Our work studies opinion-framing in the global warming ({GW}) debate, an increasingly partisan issue that has received little attention in {NLP}. We introduce Global Warming Stance Dataset ({GWSD}), a dataset of stance-labeled {GW} sentences, and train a {BERT} classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other's opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across {GW}-accepting and skeptic media, though {GW}-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author's own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of {GW} stance.},
	number = {{arXiv}:2010.15149},
	publisher = {{arXiv}},
	author = {Luo, Yiwei and Card, Dallas and Jurafsky, Dan},
	urldate = {2024-05-29},
	date = {2021-01-16},
	eprinttype = {arxiv},
	eprint = {2010.15149 [cs]},
}

@misc{welleckDialogueNaturalLanguage2019,
	title = {Dialogue Natural Language Inference},
	url = {http://arxiv.org/abs/1811.00671},
	abstract = {Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference ({NLI}) and create a new natural language inference dataset called Dialogue {NLI}. We propose a method which demonstrates that a model trained on Dialogue {NLI} can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model's consistency.},
	number = {{arXiv}:1811.00671},
	publisher = {{arXiv}},
	author = {Welleck, Sean and Weston, Jason and Szlam, Arthur and Cho, Kyunghyun},
	urldate = {2024-05-29},
	date = {2019-01-17},
	eprinttype = {arxiv},
	eprint = {1811.00671 [cs]},
}

@misc{briakouDetectingFineGrainedCrossLingual2020,
	title = {Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank},
	url = {http://arxiv.org/abs/2010.03662},
	abstract = {Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual {NLP} and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual {BERT} models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.},
	number = {{arXiv}:2010.03662},
	publisher = {{arXiv}},
	author = {Briakou, Eleftheria and Carpuat, Marine},
	urldate = {2024-05-29},
	date = {2020-10-07},
	eprinttype = {arxiv},
	eprint = {2010.03662 [cs]},
}

@misc{liDailyDialogManuallyLabelled2017,
	title = {{DailyDialog}: A Manually Labelled Multi-turn Dialogue Dataset},
	url = {http://arxiv.org/abs/1710.03957},
	shorttitle = {{DailyDialog}},
	abstract = {We develop a high-quality multi-turn dialog dataset, {DailyDialog}, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on {DailyDialog} dataset and hope it benefit the research field of dialog systems.},
	number = {{arXiv}:1710.03957},
	publisher = {{arXiv}},
	author = {Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi},
	urldate = {2024-05-29},
	date = {2017-10-11},
	eprinttype = {arxiv},
	eprint = {1710.03957 [cs]},
}

@misc{nangiaCrowSPairsChallengeDataset2020,
	title = {{CrowS}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models},
	url = {http://arxiv.org/abs/2010.00133},
	shorttitle = {{CrowS}-Pairs},
	abstract = {Pretrained language models, especially masked language models ({MLMs}) have seen success across many {NLP} tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the {US}, we introduce the Crowdsourced Stereotype Pairs benchmark ({CrowS}-Pairs). {CrowS}-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In {CrowS}-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used {MLMs} we evaluate substantially favor sentences that express stereotypes in every category in {CrowS}-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.},
	number = {{arXiv}:2010.00133},
	publisher = {{arXiv}},
	author = {Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R.},
	urldate = {2024-05-29},
	date = {2020-09-30},
	eprinttype = {arxiv},
	eprint = {2010.00133 [cs]},
}

@misc{wangCrossWeighTrainingNamed2019,
	title = {{CrossWeigh}: Training Named Entity Tagger from Imperfect Annotations},
	url = {http://arxiv.org/abs/1909.01441},
	shorttitle = {{CrossWeigh}},
	abstract = {Everyone makes mistakes. So do human annotators when curating labels for named entity recognition ({NER}). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted {NER} benchmark datasets, {CoNLL}03 {NER}. We are able to identify label mistakes in about 5.38\% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93\%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, {CrossWeigh}, to handle label mistakes during {NER} model training. Specifically, it partitions the training data into several folds and train independent {NER} models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final {NER} model. Extensive experiments demonstrate significant improvements of plugging various {NER} models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: https://github.com/{ZihanWangKi}/{CrossWeigh}.},
	number = {{arXiv}:1909.01441},
	publisher = {{arXiv}},
	author = {Wang, Zihan and Shang, Jingbo and Liu, Liyuan and Lu, Lihao and Liu, Jiacheng and Han, Jiawei},
	urldate = {2024-05-29},
	date = {2019-09-03},
	eprinttype = {arxiv},
	eprint = {1909.01441 [cs]},
}

@misc{onoeCREAKDatasetCommonsense2021,
	title = {{CREAK}: A Dataset for Commonsense Reasoning over Entity Knowledge},
	url = {http://arxiv.org/abs/2109.01653},
	shorttitle = {{CREAK}},
	abstract = {Most benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall [Talmor et al., 2019], social knowledge like bumping into someone is awkward [Sap et al., 2019], and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim "Harry Potter can teach classes on how to fly on a broomstick." Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce {CREAK}, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set. Crowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that models should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles on {CREAK}. Training a model on {CREAK} improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests).},
	number = {{arXiv}:2109.01653},
	publisher = {{arXiv}},
	author = {Onoe, Yasumasa and Zhang, Michael J. Q. and Choi, Eunsol and Durrett, Greg},
	urldate = {2024-05-29},
	date = {2021-09-03},
	eprinttype = {arxiv},
	eprint = {2109.01653 [cs]},
}

@misc{huangCounterfactuallyAugmentedSNLITraining2020,
	title = {Counterfactually-Augmented {SNLI} Training Data Does Not Yield Better Generalization Than Unaugmented Data},
	url = {http://arxiv.org/abs/2010.04762},
	abstract = {A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks---datasets collected from crowdworkers to create an evaluation task---while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data---data built by minimally editing a set of seed examples to yield counterfactual labels---to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented {SNLI} dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable.},
	number = {{arXiv}:2010.04762},
	publisher = {{arXiv}},
	author = {Huang, William and Liu, Haokun and Bowman, Samuel R.},
	urldate = {2024-05-29},
	date = {2020-10-09},
	eprinttype = {arxiv},
	eprint = {2010.04762 [cs]},
}

@inproceedings{prettenhoferCrossLanguageTextClassification2010,
	location = {Uppsala, Sweden},
	title = {Cross-Language Text Classification Using Structural Correspondence Learning},
	url = {https://aclanthology.org/P10-1114},
	eventtitle = {{ACL} 2010},
	pages = {1118--1127},
	booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Prettenhofer, Peter and Stein, Benno},
	editor = {Hajič, Jan and Carberry, Sandra and Clark, Stephen and Nivre, Joakim},
	urldate = {2024-05-29},
	date = {2010-07},
}

@inproceedings{gardentCreatingTrainingCorpora2017,
	location = {Vancouver, Canada},
	title = {Creating Training Corpora for {NLG} Micro-Planners},
	url = {https://aclanthology.org/P17-1017},
	doi = {10/gf93ns},
	abstract = {In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn {KB} verbalisers. We apply our framework to {DBpedia} data and compare the resulting dataset with Wen et al. 2016's. We show that while Wen et al.'s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which {NLG} models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the {WebNLG} shared task.},
	eventtitle = {{ACL} 2017},
	pages = {179--188},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and Perez-Beltrachini, Laura},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	urldate = {2024-05-29},
	date = {2017-07},
}

@misc{qinCounterfactualStoryReasoning2019,
	title = {Counterfactual Story Reasoning and Generation},
	url = {http://arxiv.org/abs/1909.04076},
	abstract = {Counterfactual reasoning requires predicting how alternative events, contrary to what actually happened, might have resulted in different outcomes. Despite being considered a necessary component of {AI}-complete systems, few resources have been developed for evaluating counterfactual reasoning in narratives. In this paper, we propose Counterfactual Story Rewriting: given an original story and an intervening counterfactual event, the task is to minimally revise the story to make it compatible with the given counterfactual event. Solving this task will require deep understanding of causal narrative chains and counterfactual invariance, and integration of such story reasoning capabilities into conditional language generation models. We present {TimeTravel}, a new dataset of 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. Additionally, we include 80,115 counterfactual "branches" without a rewritten storyline to support future work on semi- or un-supervised approaches to counterfactual story rewriting. Finally, we evaluate the counterfactual rewriting capacities of several competitive baselines based on pretrained language models, and assess whether common overlap and model-based automatic metrics for text generation correlate well with human scores for counterfactual rewriting.},
	number = {{arXiv}:1909.04076},
	publisher = {{arXiv}},
	author = {Qin, Lianhui and Bosselut, Antoine and Holtzman, Ari and Bhagavatula, Chandra and Clark, Elizabeth and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2019-09-12},
	eprinttype = {arxiv},
	eprint = {1909.04076 [cs]},
}

@misc{chenConvFinQAExploringChain2022,
	title = {{ConvFinQA}: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering},
	url = {http://arxiv.org/abs/2210.03849},
	shorttitle = {{ConvFinQA}},
	abstract = {With the recent advance in large pre-trained language models, researchers have achieved record performances in {NLP} tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, {ConvFinQA}, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/{ConvFinQA}.},
	number = {{arXiv}:2210.03849},
	publisher = {{arXiv}},
	author = {Chen, Zhiyu and Li, Shiyang and Smiley, Charese and Ma, Zhiqiang and Shah, Sameena and Wang, William Yang},
	urldate = {2024-05-29},
	date = {2022-10-07},
	eprinttype = {arxiv},
	eprint = {2210.03849 [cs]},
}

@misc{ravichanderCONDAQAContrastiveReading2022,
	title = {{CONDAQA}: A Contrastive Reading Comprehension Dataset for Reasoning about Negation},
	url = {http://arxiv.org/abs/2211.00295},
	shorttitle = {{CONDAQA}},
	abstract = {The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that can process negation effectively, we present {CONDAQA}, the first English reading comprehension dataset which requires reasoning about the implications of negated statements in paragraphs. We collect paragraphs with diverse negation cues, then have crowdworkers ask questions about the implications of the negated statement in the passage. We also have workers make three kinds of edits to the passage -- paraphrasing the negated statement, changing the scope of the negation, and reversing the negation -- resulting in clusters of question-answer pairs that are difficult for models to answer with spurious shortcuts. {CONDAQA} features 14,182 question-answer pairs with over 200 unique negation cues and is challenging for current state-of-the-art models. The best performing model on {CONDAQA} ({UnifiedQA}-v2-3b) achieves only 42\% on our consistency metric, well below human performance which is 81\%. We release our dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to facilitate the development of future {NLP} methods that work on negated language.},
	number = {{arXiv}:2211.00295},
	publisher = {{arXiv}},
	author = {Ravichander, Abhilasha and Gardner, Matt and Marasović, Ana},
	urldate = {2024-05-29},
	date = {2022-11-01},
	eprinttype = {arxiv},
	eprint = {2211.00295 [cs]},
}

@misc{talmorCommonsenseQAExposingLimits2022,
	title = {{CommonsenseQA} 2.0: Exposing the Limits of {AI} through Gamification},
	url = {http://arxiv.org/abs/2201.05320},
	shorttitle = {{CommonsenseQA} 2.0},
	abstract = {Constructing benchmarks that test the abilities of modern natural language understanding models is difficult - pre-trained language models exploit artifacts in benchmarks to achieve human parity, but still fail on adversarial examples and make errors that demonstrate a lack of common sense. In this work, we propose gamification as a framework for data construction. The goal of players in the game is to compose questions that mislead a rival {AI} while using specific phrases for extra points. The game environment leads to enhanced user engagement and simultaneously gives the game designer control over the collected data, allowing us to collect high-quality data at scale. Using our method we create {CommonsenseQA} 2.0, which includes 14,343 yes/no questions, and demonstrate its difficulty for models that are orders-of-magnitude larger than the {AI} used in the game itself. Our best baseline, the T5-based Unicorn with 11B parameters achieves an accuracy of 70.2\%, substantially higher than {GPT}-3 (52.9\%) in a few-shot inference setup. Both score well below human performance which is at 94.1\%.},
	number = {{arXiv}:2201.05320},
	publisher = {{arXiv}},
	author = {Talmor, Alon and Yoran, Ori and Bras, Ronan Le and Bhagavatula, Chandra and Goldberg, Yoav and Choi, Yejin and Berant, Jonathan},
	urldate = {2024-05-29},
	date = {2022-01-14},
	eprinttype = {arxiv},
	eprint = {2201.05320 [cs]},
}

@misc{sahaConjNLINaturalLanguage2020,
	title = {{ConjNLI}: Natural Language Inference Over Conjunctive Sentences},
	url = {http://arxiv.org/abs/2010.10418},
	shorttitle = {{ConjNLI}},
	abstract = {Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing {NLI} stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce {ConjNLI}, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions ("and", "or", "but", "nor") with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like {RoBERTa} do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making {RoBERTa} aware of predicate semantic roles. While we observe some performance gains, {ConjNLI} is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions. Our data and code are publicly available at: https://github.com/{swarnaHub}/{ConjNLI}},
	number = {{arXiv}:2010.10418},
	publisher = {{arXiv}},
	author = {Saha, Swarnadeep and Nie, Yixin and Bansal, Mohit},
	urldate = {2024-05-29},
	date = {2020-10-21},
	eprinttype = {arxiv},
	eprint = {2010.10418 [cs]},
}

@misc{aliannejadiConvAI3GeneratingClarifying2020,
	title = {{ConvAI}3: Generating Clarifying Questions for Open-Domain Dialogue Systems ({ClariQ})},
	url = {http://arxiv.org/abs/2009.11352},
	shorttitle = {{ConvAI}3},
	abstract = {This document presents a detailed description of the challenge on clarifying questions for dialogue systems ({ClariQ}). The challenge is organized as part of the Conversational {AI} challenge series ({ConvAI}3) at Search Oriented Conversational {AI} ({SCAI}) {EMNLP} workshop in 2020. The main aim of the conversational systems is to return an appropriate answer in response to the user requests. However, some user requests might be ambiguous. In {IR} settings such a situation is handled mainly thought the diversification of the search result page. It is however much more challenging in dialogue settings with limited bandwidth. Therefore, in this challenge, we provide a common evaluation framework to evaluate mixed-initiative conversations. Participants are asked to rank clarifying questions in an information-seeking conversations. The challenge is organized in two stages where in Stage 1 we evaluate the submissions in an offline setting and single-turn conversations. Top participants of Stage 1 get the chance to have their model tested by human annotators.},
	number = {{arXiv}:2009.11352},
	publisher = {{arXiv}},
	author = {Aliannejadi, Mohammad and Kiseleva, Julia and Chuklin, Aleksandr and Dalton, Jeff and Burtsev, Mikhail},
	urldate = {2024-05-29},
	date = {2020-09-23},
	eprinttype = {arxiv},
	eprint = {2009.11352 [cs]},
}

@misc{abujabalComQACommunitysourcedDataset2019,
	title = {{ComQA}: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters},
	url = {http://arxiv.org/abs/1809.09528},
	shorttitle = {{ComQA}},
	abstract = {To bridge the gap between the capabilities of the state-of-the-art in factoid question answering ({QA}) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated. We introduce {ComQA}, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. {ComQA} questions come from the {WikiAnswers} community {QA} platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers. {ComQA} contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing {ComQA}, including the measures taken to ensure its high quality while making effective use of crowdsourcing. We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on {ComQA}, demonstrating that our dataset can be a driver of future research on {QA}.},
	number = {{arXiv}:1809.09528},
	publisher = {{arXiv}},
	author = {Abujabal, Abdalghani and Roy, Rishiraj Saha and Yahya, Mohamed and Weikum, Gerhard},
	urldate = {2024-05-29},
	date = {2019-04-10},
	eprinttype = {arxiv},
	eprint = {1809.09528 [cs]},
}

@misc{chapuisCodeswitchedInspiredLosses2021,
	title = {Code-switched inspired losses for generic spoken dialog representations},
	url = {http://arxiv.org/abs/2108.12465},
	abstract = {Spoken dialog systems need to be able to handle both multiple languages and multilinguality inside a conversation ({\textbackslash}textit\{e.g\} in case of code-switching). In this work, we introduce new pretraining losses tailored to learn multilingual spoken dialog representations. The goal of these losses is to expose the model to code-switched language. To scale up training, we automatically build a pretraining corpus composed of multilingual conversations in five different languages (French, Italian, English, German and Spanish) from {\textbackslash}texttt\{{OpenSubtitles}\}, a huge multilingual corpus composed of 24.3G tokens. We test the generic representations on {\textbackslash}texttt\{{MIAM}\}, a new benchmark composed of five dialog act corpora on the same aforementioned languages as well as on two novel multilingual downstream tasks ({\textbackslash}textit\{i.e\} multilingual mask utterance retrieval and multilingual inconsistency identification). Our experiments show that our new code switched-inspired losses achieve a better performance in both monolingual and multilingual settings.},
	number = {{arXiv}:2108.12465},
	publisher = {{arXiv}},
	author = {Chapuis, Emile and Colombo, Pierre and Labeau, Matthieu and Clavel, Chloe},
	urldate = {2024-05-29},
	date = {2021-09-09},
	eprinttype = {arxiv},
	eprint = {2108.12465 [cs]},
}

@misc{singhCOM2SENSECommonsenseReasoning2021,
	title = {{COM}2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences},
	url = {http://arxiv.org/abs/2106.00969},
	shorttitle = {{COM}2SENSE},
	abstract = {Commonsense reasoning is intuitive for humans but has been a long-term challenge for artificial intelligence ({AI}). Recent advancements in pretrained language models have shown promising results on several commonsense benchmark datasets. However, the reliability and comprehensiveness of these benchmarks towards assessing model's commonsense reasoning ability remains unclear. To this end, we introduce a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs. We propose a pairwise accuracy metric to reliably measure an agent's ability to perform commonsense reasoning over a given situation. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, we design our dataset along the dimensions of knowledge domains, reasoning scenarios and numeracy. Experimental results demonstrate that our strongest baseline ({UnifiedQA}-3B), after fine-tuning, achieves {\textasciitilde}71\% standard accuracy and {\textasciitilde}51\% pairwise accuracy, well below human performance ({\textasciitilde}95\% for both metrics). The dataset is available at https://github.com/{PlusLabNLP}/Com2Sense.},
	number = {{arXiv}:2106.00969},
	publisher = {{arXiv}},
	author = {Singh, Shikhar and Wen, Nuan and Hou, Yu and Alipoormolabashi, Pegah and Wu, Te-Lin and Ma, Xuezhe and Peng, Nanyun},
	urldate = {2024-05-29},
	date = {2021-06-02},
	eprinttype = {arxiv},
	eprint = {2106.00969 [cs]},
}

@misc{linCommonSenseEnglish2021,
	title = {Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning},
	url = {http://arxiv.org/abs/2106.06937},
	shorttitle = {Common Sense Beyond English},
	abstract = {Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models ({ML}-{LMs}) to help advance commonsense reasoning ({CSR}) beyond English. We collect the Mickey Corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving {ML}-{LMs}. We propose Mickey Probe, a language-agnostic probing task for fairly evaluating the common sense of popular {ML}-{LMs} across different languages. In addition, we also create two new datasets, X-{CSQA} and X-{CODAH}, by translating their English versions to 15 other languages, so that we can evaluate popular {ML}-{LMs} for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method -- multilingual contrastive pre-training ({MCP}). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks.},
	number = {{arXiv}:2106.06937},
	publisher = {{arXiv}},
	author = {Lin, Bill Yuchen and Lee, Seyeon and Qiao, Xiaoyang and Ren, Xiang},
	urldate = {2024-05-29},
	date = {2021-06-13},
	eprinttype = {arxiv},
	eprint = {2106.06937 [cs]},
}

@misc{luCodeXGLUEMachineLearning2021,
	title = {{CodeXGLUE}: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
	url = {http://arxiv.org/abs/2102.04664},
	shorttitle = {{CodeXGLUE}},
	abstract = {Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce {CodeXGLUE}, a benchmark dataset to foster machine learning research for program understanding and generation. {CodeXGLUE} includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. {CodeXGLUE} also features three baseline systems, including the {BERT}-style, {GPT}-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.},
	number = {{arXiv}:2102.04664},
	publisher = {{arXiv}},
	author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and Gong, Ming and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and Liu, Shujie},
	urldate = {2024-05-29},
	date = {2021-03-16},
	eprinttype = {arxiv},
	eprint = {2102.04664 [cs]},
}

@misc{hwangCOMETATOMIC2020Symbolic2021,
	title = {{COMET}-{ATOMIC} 2020: On Symbolic and Neural Commonsense Knowledge Graphs},
	url = {http://arxiv.org/abs/2010.05953},
	shorttitle = {{COMET}-{ATOMIC} 2020},
	abstract = {Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs ({CSKG}) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge. In this work, we posit that manually constructed {CSKGs} will never achieve the coverage necessary to be applicable in all situations encountered by {NLP} agents. Therefore, we propose a new evaluation framework for testing the utility of {KGs} based on how effectively implicit knowledge representations can be learned from them. With this new goal, we propose {ATOMIC} 2020, a new {CSKG} of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading {CSKGs}, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that {ATOMIC} 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of {GPT}-3 (175B parameters), while impressive, remains {\textasciitilde}12 absolute points lower than a {BART}-based knowledge model trained on {ATOMIC} 2020 despite using over 430x fewer parameters.},
	number = {{arXiv}:2010.05953},
	publisher = {{arXiv}},
	author = {Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Da, Jeff and Sakaguchi, Keisuke and Bosselut, Antoine and Choi, Yejin},
	urldate = {2024-05-29},
	date = {2021-12-16},
	eprinttype = {arxiv},
	eprint = {2010.05953 [cs]},
}

@misc{weirCOD3SDiverseGeneration2020,
	title = {{COD}3S: Diverse Generation with Discrete Semantic Signatures},
	url = {http://arxiv.org/abs/2010.02882},
	shorttitle = {{COD}3S},
	abstract = {We present {COD}3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seq models typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash ({LSH})-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply {COD}3S to causal generation, the task of predicting a proposition's plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.},
	number = {{arXiv}:2010.02882},
	publisher = {{arXiv}},
	author = {Weir, Nathaniel and Sedoc, João and Van Durme, Benjamin},
	urldate = {2024-05-29},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02882 [cs]},
}

@misc{huangCODA19UsingNonExpert2020,
	title = {{CODA}-19: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the {COVID}-19 Open Research Dataset},
	url = {http://arxiv.org/abs/2005.02367},
	shorttitle = {{CODA}-19},
	abstract = {This paper introduces {CODA}-19, a human-annotated dataset that codes the Background, Purpose, Method, Finding/Contribution, and Other sections of 10,966 English abstracts in the {COVID}-19 Open Research Dataset. {CODA}-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were acquired by majority vote. The inter-annotator agreement (Cohen's kappa) between the crowd and the biomedical expert (0.741) is comparable to inter-expert agreement (0.788). {CODA}-19's labels have an accuracy of 82.2\% when compared to the biomedical expert's labels, while the accuracy between experts was 85.0\%. Reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature, and also serve as the battery of {AI}/{NLP} research, but obtaining expert annotations can be slow. We demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against {COVID}-19.},
	number = {{arXiv}:2005.02367},
	publisher = {{arXiv}},
	author = {Huang, Ting-Hao 'Kenneth' and Huang, Chieh-Yang and Ding, Chien-Kuang Cornelia and Hsu, Yen-Chia and Giles, C. Lee},
	urldate = {2024-05-29},
	date = {2020-09-17},
	eprinttype = {arxiv},
	eprint = {2005.02367 [cs]},
}

@misc{chenCODAHAdversariallyAuthored2019,
	title = {{CODAH}: An Adversarially Authored Question-Answer Dataset for Common Sense},
	url = {http://arxiv.org/abs/1904.04365},
	shorttitle = {{CODAH}},
	abstract = {Commonsense reasoning is a critical {AI} capability, but it is difficult to construct challenging datasets that test common sense. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the datasets to achieve human-level scores. We introduce the {CODAH} dataset, an adversarially-constructed evaluation dataset for testing common sense. {CODAH} forms a challenging extension to the recently-proposed {SWAG} dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult dataset, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this procedure and evaluate the performance of multiple state-of-the-art question answering systems on our dataset. We observe a significant gap between human performance, which is 95.3\%, and the performance of the best baseline accuracy of 67.5\% by the {BERT}-Large model.},
	number = {{arXiv}:1904.04365},
	publisher = {{arXiv}},
	author = {Chen, Michael and D'Arcy, Mike and Liu, Alisa and Fernandez, Jared and Downey, Doug},
	urldate = {2024-05-29},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1904.04365 [cs]},
}

@misc{poliakCollectingDiverseNatural2018,
	title = {Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation},
	url = {http://arxiv.org/abs/1804.08207},
	abstract = {We present a large-scale collection of diverse natural language inference ({NLI}) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common {NLI} structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the {DNC}: Diverse Natural Language Inference Collection. The {DNC} is available online at https://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources.},
	number = {{arXiv}:1804.08207},
	publisher = {{arXiv}},
	author = {Poliak, Adam and Haldar, Aparajita and Rudinger, Rachel and Hu, J. Edward and Pavlick, Ellie and White, Aaron Steven and Van Durme, Benjamin},
	urldate = {2024-05-29},
	date = {2018-08-29},
	eprinttype = {arxiv},
	eprint = {1804.08207 [cs]},
}

@inproceedings{steinCombiningEmbeddedAccelerometers2013,
	location = {New York, {NY}, {USA}},
	title = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},
	isbn = {978-1-4503-1770-2},
	url = {https://doi.org/10.1145/2493432.2493482},
	doi = {10/gtwv9t},
	series = {{UbiComp} '13},
	abstract = {This paper introduces a publicly available dataset of complex activities that involve manipulative gestures. The dataset captures people preparing mixed salads and contains more than 4.5 hours of accelerometer and {RGB}-D video data, detailed annotations, and an evaluation protocol for comparison of activity recognition algorithms. Providing baseline results for one possible activity recognition task, this paper further investigates modality fusion methods at different stages of the recognition pipeline: (i) prior to feature extraction through accelerometer localization, (ii) at feature level via feature concatenation, and (iii) at classification level by combining classifier outputs. Empirical evaluation shows that fusing information captured by these sensor types can considerably improve recognition performance.},
	pages = {729--738},
	booktitle = {Proceedings of the 2013 {ACM} international joint conference on Pervasive and ubiquitous computing},
	publisher = {Association for Computing Machinery},
	author = {Stein, Sebastian and {McKenna}, Stephen J.},
	urldate = {2024-05-29},
	date = {2013-09-08},
}

@misc{schlegelCanTransformersReason2022,
	title = {Can Transformers Reason in Fragments of Natural Language?},
	url = {http://arxiv.org/abs/2211.05417},
	abstract = {State-of-the-art deep-learning-based approaches to Natural Language Processing ({NLP}) are credited with various capabilities that involve reasoning with natural language texts. In this paper we carry out a large-scale empirical study investigating the detection of formally valid inferences in controlled fragments of natural language for which the satisfiability problem becomes increasingly complex. We find that, while transformer-based language models perform surprisingly well in these scenarios, a deeper analysis re-veals that they appear to overfit to superficial patterns in the data rather than acquiring the logical principles governing the reasoning in these fragments.},
	number = {{arXiv}:2211.05417},
	publisher = {{arXiv}},
	author = {Schlegel, Viktor and Pavlov, Kamen V. and Pratt-Hartmann, Ian},
	urldate = {2024-05-29},
	date = {2022-11-10},
	eprinttype = {arxiv},
	eprint = {2211.05417 [cs]},
}

@misc{ghosalCICERODatasetContextualized2022,
	title = {{CICERO}: A Dataset for Contextualized Commonsense Inference in Dialogues},
	url = {http://arxiv.org/abs/2203.13926},
	shorttitle = {{CICERO}},
	abstract = {This paper addresses the problem of dialogue reasoning with contextualized commonsense inference. We curate {CICERO}, a dataset of dyadic conversations with five types of utterance-level reasoning-based inferences: cause, subsequent event, prerequisite, motivation, and emotional reaction. The dataset contains 53,105 of such inferences from 5,672 dialogues. We use this dataset to solve relevant generative and discriminative tasks: generation of cause and subsequent event; generation of prerequisite, motivation, and listener's emotional reaction; and selection of plausible alternatives. Our results ascertain the value of such dialogue-centric commonsense knowledge datasets. It is our hope that {CICERO} will open new research avenues into commonsense-based dialogue reasoning.},
	number = {{arXiv}:2203.13926},
	publisher = {{arXiv}},
	author = {Ghosal, Deepanway and Shen, Siqi and Majumder, Navonil and Mihalcea, Rada and Poria, Soujanya},
	urldate = {2024-05-29},
	date = {2022-04-06},
	eprinttype = {arxiv},
	eprint = {2203.13926 [cs]},
}

@misc{zhangChrEnCherokeeEnglishMachine2020,
	title = {{ChrEn}: Cherokee-English Machine Translation for Endangered Language Revitalization},
	url = {http://arxiv.org/abs/2010.04791},
	shorttitle = {{ChrEn}},
	abstract = {Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world, and the number is declining every year. To help save this endangered language, we introduce {ChrEn}, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, {ChrEn} is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare {SMT} (phrase-based) versus {NMT} ({RNN}-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and {BERT}/Multilingual-{BERT}) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 {BLEU} for in-domain and 6.5/5.0 {BLEU} for out-of-domain Chr-En/{EnChr} translations, respectively, and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization. Our data, code, and demo will be publicly available at https://github.com/{ZhangShiyue}/{ChrEn}},
	number = {{arXiv}:2010.04791},
	publisher = {{arXiv}},
	author = {Zhang, Shiyue and Frey, Benjamin and Bansal, Mohit},
	urldate = {2024-05-29},
	date = {2020-10-09},
	eprinttype = {arxiv},
	eprint = {2010.04791 [cs]},
}

@misc{karimClassificationBenchmarksUnderresourced2020,
	title = {Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-{LSTM} Network},
	url = {http://arxiv.org/abs/2004.07807},
	abstract = {Exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices but also enables people to express anti-social behaviour like online harassment, cyberbullying, and hate speech. Numerous works have been proposed to utilize these data for social and anti-social behaviours analysis, document characterization, and sentiment analysis by predicting the contexts mostly for highly resourced languages such as English. However, there are languages that are under-resources, e.g., South Asian languages like Bengali, Tamil, Assamese, Telugu that lack of computational resources for the {NLP} tasks. In this paper, we provide several classification benchmarks for Bengali, an under-resourced language. We prepared three datasets of expressing hate, commonly used topics, and opinions for hate speech detection, document classification, and sentiment analysis, respectively. We built the largest Bengali word embedding models to date based on 250 million articles, which we call {BengFastText}. We perform three different experiments, covering document classification, sentiment analysis, and hate speech detection. We incorporate word embeddings into a Multichannel Convolutional-{LSTM} ({MConv}-{LSTM}) network for predicting different types of hate speech, document classification, and sentiment analysis. Experiments demonstrate that {BengFastText} can capture the semantics of words from respective contexts correctly. Evaluations against several baseline embedding models, e.g., Word2Vec and {GloVe} yield up to 92.30\%, 82.25\%, and 90.45\% F1-scores in case of document classification, sentiment analysis, and hate speech detection, respectively during 5-fold cross-validation tests.},
	number = {{arXiv}:2004.07807},
	publisher = {{arXiv}},
	author = {Karim, Md Rezaul and Chakravarthi, Bharathi Raja and {McCrae}, John P. and Cochez, Michael},
	urldate = {2024-05-29},
	date = {2020-04-19},
	eprinttype = {arxiv},
	eprint = {2004.07807 [cs, stat]},
}

@misc{sinhaCLUTRRDiagnosticBenchmark2019,
	title = {{CLUTRR}: A Diagnostic Benchmark for Inductive Reasoning from Text},
	url = {http://arxiv.org/abs/1908.06177},
	shorttitle = {{CLUTRR}},
	abstract = {The recent success of natural language understanding ({NLU}) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named {CLUTRR}, to clarify some key issues related to the robustness and systematicity of {NLU} systems. Motivated by classic work on inductive logic programming, {CLUTRR} requires that an {NLU} system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. {CLUTRR} allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art {NLU} models (e.g., {BERT} and {MAC}) and a graph neural network model that works directly with symbolic inputs---with the graph-based model exhibiting both stronger generalization and greater robustness.},
	number = {{arXiv}:1908.06177},
	publisher = {{arXiv}},
	author = {Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L.},
	urldate = {2024-05-29},
	date = {2019-09-03},
	eprinttype = {arxiv},
	eprint = {1908.06177 [cs, stat]},
}

@misc{yanakaCanNeuralNetworks2019,
	title = {Can neural networks understand monotonicity reasoning?},
	url = {http://arxiv.org/abs/1906.06448},
	abstract = {Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference ({NLI}) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset ({MED}). Performance by state-of-the-art {NLI} models on the new test set is substantially worse, under 55\%, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning.},
	number = {{arXiv}:1906.06448},
	publisher = {{arXiv}},
	author = {Yanaka, Hitomi and Mineshima, Koji and Bekki, Daisuke and Inui, Kentaro and Sekine, Satoshi and Abzianidze, Lasha and Bos, Johan},
	urldate = {2024-05-29},
	date = {2019-06-27},
	eprinttype = {arxiv},
	eprint = {1906.06448 [cs]},
}

@inproceedings{radlinskiCoachedConversationalPreference2019,
	location = {Stockholm, Sweden},
	title = {Coached Conversational Preference Elicitation: A Case Study in Understanding Movie Preferences},
	url = {https://aclanthology.org/W19-5941},
	doi = {10/ggvx59},
	shorttitle = {Coached Conversational Preference Elicitation},
	abstract = {Conversational recommendation has recently attracted significant attention. As systems must understand users' preferences, training them has called for conversational corpora, typically derived from task-oriented conversations. We observe that such corpora often do not reflect how people naturally describe preferences. We present a new approach to obtaining user preferences in dialogue: Coached Conversational Preference Elicitation. It allows collection of natural yet structured conversational preferences. Studying the dialogues in one domain, we present a brief quantitative analysis of how people describe movie preferences at scale. Demonstrating the methodology, we release the {CCPE}-M dataset to the community with over 500 movie preference dialogues expressing over 10,000 preferences.},
	eventtitle = {{SIGDIAL} 2019},
	pages = {353--360},
	booktitle = {Proceedings of the 20th Annual {SIGdial} Meeting on Discourse and Dialogue},
	publisher = {Association for Computational Linguistics},
	author = {Radlinski, Filip and Balog, Krisztian and Byrne, Bill and Krishnamoorthi, Karthik},
	editor = {Nakamura, Satoshi and Gasic, Milica and Zukerman, Ingrid and Skantze, Gabriel and Nakano, Mikio and Papangelis, Alexandros and Ultes, Stefan and Yoshino, Koichiro},
	urldate = {2024-05-29},
	date = {2019-09},
}

@inproceedings{saraviaCARERContextualizedAffect2018,
	location = {Brussels, Belgium},
	title = {{CARER}: Contextualized Affect Representations for Emotion Recognition},
	url = {https://aclanthology.org/D18-1404},
	doi = {10/gk496t},
	shorttitle = {{CARER}},
	abstract = {Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.},
	eventtitle = {{EMNLP} 2018},
	pages = {3687--3697},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Saravia, Elvis and Liu, Hsien-Chi Toby and Huang, Yen-Hao and Wu, Junlin and Chen, Yi-Shin},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	urldate = {2024-05-29},
	date = {2018-10},
}

@misc{wolfsonBreakItQuestion2020,
	title = {Break It Down: A Question Understanding Benchmark},
	url = {http://arxiv.org/abs/2001.11770},
	shorttitle = {Break It Down},
	abstract = {Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation ({QDMR}) for questions. {QDMR} constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality {QDMRs} can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their {QDMRs}. We demonstrate the utility of {QDMR} by showing that (a) it can be used to improve open-domain question answering on the {HotpotQA} dataset, (b) it can be deterministically converted to a pseudo-{SQL} formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into {QDMR} structures, and show that it substantially outperforms several natural baselines.},
	number = {{arXiv}:2001.11770},
	publisher = {{arXiv}},
	author = {Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},
	urldate = {2024-05-29},
	date = {2020-01-31},
	eprinttype = {arxiv},
	eprint = {2001.11770 [cs]},
}

@misc{xiaoCAIL2018LargeScaleLegal2018,
	title = {{CAIL}2018: A Large-Scale Legal Dataset for Judgment Prediction},
	url = {http://arxiv.org/abs/1807.02478},
	shorttitle = {{CAIL}2018},
	abstract = {In this paper, we introduce the {\textbackslash}textbf\{C\}hinese {\textbackslash}textbf\{{AI}\} and {\textbackslash}textbf\{L\}aw challenge dataset ({CAIL}2018), the first large-scale Chinese legal dataset for judgment prediction. {\textbackslash}dataset contains more than \$2.6\$ million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more detailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected to be inferred according to the fact descriptions of cases. For comparison, we implement several conventional text classification baselines for judgment prediction and experimental results show that it is still a challenge for current models to predict the judgment results of legal cases, especially on prison terms. To help the researchers make improvements on legal judgment prediction, both {\textbackslash}dataset and baselines will be released after the {CAIL} competition{\textbackslash}footnote\{http://cail.cipsc.org.cn/\}.},
	number = {{arXiv}:1807.02478},
	publisher = {{arXiv}},
	author = {Xiao, Chaojun and Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and Xu, Jianfeng},
	urldate = {2024-05-29},
	date = {2018-07-03},
	eprinttype = {arxiv},
	eprint = {1807.02478 [cs]},
}

@misc{glocknerBreakingNLISystems2018,
	title = {Breaking {NLI} Systems with Sentences that Require Simple Lexical Inferences},
	url = {http://arxiv.org/abs/1805.02266},
	abstract = {We create a new {NLI} test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the {SNLI} test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on {SNLI}, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.},
	number = {{arXiv}:1805.02266},
	publisher = {{arXiv}},
	author = {Glockner, Max and Shwartz, Vered and Goldberg, Yoav},
	urldate = {2024-05-29},
	date = {2018-05-06},
	eprinttype = {arxiv},
	eprint = {1805.02266 [cs]},
}

@inproceedings{derczynskiBroadTwitterCorpus2016,
	location = {Osaka, Japan},
	title = {Broad Twitter Corpus: A Diverse Named Entity Recognition Resource},
	url = {https://aclanthology.org/C16-1111},
	shorttitle = {Broad Twitter Corpus},
	abstract = {One of the main obstacles, hampering method development and comparative evaluation of named entity recognition in social media, is the lack of a sizeable, diverse, high quality annotated corpus, analogous to the {CoNLL}'2003 news dataset. For instance, the biggest Ritter tweet corpus is only 45,000 tokens – a mere 15\% the size of {CoNLL}'2003. Another major shortcoming is the lack of temporal, geographic, and author diversity. This paper introduces the Broad Twitter Corpus ({BTC}), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of {NLP} experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire. The corpus is released openly, including source text and intermediate annotations.},
	eventtitle = {{COLING} 2016},
	pages = {1169--1179},
	booktitle = {Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
	publisher = {The {COLING} 2016 Organizing Committee},
	author = {Derczynski, Leon and Bontcheva, Kalina and Roberts, Ian},
	editor = {Matsumoto, Yuji and Prasad, Rashmi},
	urldate = {2024-05-29},
	date = {2016-12},
}

@misc{atamanBianetParallelNews2018,
	title = {Bianet: A Parallel News Corpus in Turkish, Kurdish and English},
	url = {http://arxiv.org/abs/1805.05095},
	shorttitle = {Bianet},
	abstract = {We present a new open-source parallel corpus consisting of news articles collected from the Bianet magazine, an online newspaper that publishes Turkish news, often along with their translations in English and Kurdish. In this paper, we describe the collection process of the corpus and its statistical properties. We validate the benefit of using the Bianet corpus by evaluating bilingual and multilingual neural machine translation models in English-Turkish and English-Kurdish directions.},
	number = {{arXiv}:1805.05095},
	publisher = {{arXiv}},
	author = {Ataman, Duygu},
	urldate = {2024-05-29},
	date = {2018-05-14},
	eprinttype = {arxiv},
	eprint = {1805.05095 [cs]},
}

@inproceedings{guThreeLevelsGeneralization2021,
	title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},
	url = {http://arxiv.org/abs/2011.07743},
	doi = {10/gnnfbt},
	shorttitle = {Beyond I.I.D.},
	abstract = {Existing studies on question answering on knowledge bases ({KBQA}) mainly operate with the standard i.i.d assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d may be neither reasonably achievable nor desirable on large-scale {KBs} because 1) true user distribution is hard to capture and 2) randomly sample training examples from the enormous space would be highly data-inefficient. Instead, we suggest that {KBQA} models should have three levels of built-in generalization: i.i.d, compositional, and zero-shot. To facilitate the development of {KBQA} models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, {GrailQA}, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel {BERT}-based {KBQA} model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like {BERT} in the generalization of {KBQA}.},
	pages = {3477--3488},
	booktitle = {Proceedings of the Web Conference 2021},
	author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
	urldate = {2024-05-01},
	date = {2021-04-19},
	eprinttype = {arxiv},
	eprint = {2011.07743 [cs]},
}

@misc{patelAreNLPModels2021,
	title = {Are {NLP} Models really able to Solve Simple Math Word Problems?},
	url = {http://arxiv.org/abs/2103.07191},
	abstract = {The problem of designing {NLP} solvers for math word problems ({MWP}) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level {MWPs} containing one-unknown arithmetic word problems, such problems are often considered "solved" with the bulk of research attention moving to more complex {MWPs}. In this paper, we restrict our attention to English {MWPs} taught in grades four and lower. We provide strong evidence that the existing {MWP} solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that {MWP} solvers that do not have access to the question asked in the {MWP} can still solve a large fraction of {MWPs}. Similarly, models that treat {MWPs} as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, {SVAMP}, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on {SVAMP}, thus showing that much remains to be done even for the simplest of the {MWPs}.},
	number = {{arXiv}:2103.07191},
	publisher = {{arXiv}},
	author = {Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
	urldate = {2024-05-29},
	date = {2021-04-15},
	eprinttype = {arxiv},
	eprint = {2103.07191 [cs]},
}

@misc{bastanAuthorSentimentPrediction2020,
	title = {Author's Sentiment Prediction},
	url = {http://arxiv.org/abs/2011.06128},
	abstract = {We introduce {PerSenT}, a dataset of crowd-sourced annotations of the sentiment expressed by the authors towards the main entities in news articles. The dataset also includes paragraph-level sentiment annotations to provide more fine-grained supervision for the task. Our benchmarks of multiple strong baselines show that this is a difficult classification task. The results also suggest that simply fine-tuning document-level representations from {BERT} isn't adequate for this task. Making paragraph-level decisions and aggregating them over the entire document is also ineffective. We present empirical and qualitative analyses that illustrate the specific challenges posed by this dataset. We release this dataset with 5.3k documents and 38k paragraphs covering 3.2k unique entities as a challenge in entity sentiment analysis.},
	number = {{arXiv}:2011.06128},
	publisher = {{arXiv}},
	author = {Bastan, Mohaddeseh and Koupaee, Mahnaz and Son, Youngseo and Sicoli, Richard and Balasubramanian, Niranjan},
	urldate = {2024-05-29},
	date = {2020-11-11},
	eprinttype = {arxiv},
	eprint = {2011.06128 [cs]},
}

@misc{alva-manchegoASSETDatasetTuning2020,
	title = {{ASSET}: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations},
	url = {http://arxiv.org/abs/2005.00481},
	shorttitle = {{ASSET}},
	abstract = {In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces {ASSET}, a new dataset for assessing sentence simplification in English. {ASSET} is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in {ASSET} are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using {ASSET}, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.},
	number = {{arXiv}:2005.00481},
	publisher = {{arXiv}},
	author = {Alva-Manchego, Fernando and Martin, Louis and Bordes, Antoine and Scarton, Carolina and Sagot, Benoît and Specia, Lucia},
	urldate = {2024-05-29},
	date = {2020-05-01},
	eprinttype = {arxiv},
	eprint = {2005.00481 [cs]},
}

@misc{jereticAreNaturalLanguage2020,
	title = {Are Natural Language Inference Models {IMPPRESsive}? Learning {IMPlicature} and {PRESupposition}},
	url = {http://arxiv.org/abs/2004.03066},
	shorttitle = {Are Natural Language Inference Models {IMPPRESsive}?},
	abstract = {Natural language inference ({NLI}) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of {NLI} models to make pragmatic inferences remains understudied. We create an {IMPlicature} and {PRESupposition} diagnostic dataset ({IMPPRES}), consisting of {\textgreater}25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types. We use {IMPPRES} to evaluate whether {BERT}, {InferSent}, and {BOW} {NLI} models trained on {MultiNLI} (Williams et al., 2018) learn to make pragmatic inferences. Although {MultiNLI} appears to contain very few pairs illustrating these inference types, we find that {BERT} learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by "some" as entailments. For some presupposition triggers like "only", {BERT} reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. {BOW} and {InferSent} show weaker evidence of pragmatic reasoning. We conclude that {NLI} training encourages models to learn some, but not all, pragmatic inferences.},
	number = {{arXiv}:2004.03066},
	publisher = {{arXiv}},
	author = {Jeretic, Paloma and Warstadt, Alex and Bhooshan, Suvrat and Williams, Adina},
	urldate = {2024-05-29},
	date = {2020-07-13},
	eprinttype = {arxiv},
	eprint = {2004.03066 [cs]},
}

@misc{davidsonAutomatedHateSpeech2017,
	title = {Automated Hate Speech Detection and the Problem of Offensive Language},
	url = {http://arxiv.org/abs/1703.04009},
	abstract = {A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.},
	number = {{arXiv}:1703.04009},
	publisher = {{arXiv}},
	author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
	urldate = {2024-05-29},
	date = {2017-03-11},
	eprinttype = {arxiv},
	eprint = {1703.04009 [cs]},
}

@misc{armengol-estapeAreMultilingualModels2021,
	title = {Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan},
	url = {http://arxiv.org/abs/2107.07903},
	shorttitle = {Are Multilingual Models the Best Choice for Moderately Under-resourced Languages?},
	abstract = {Multilingual language models have been a crucial breakthrough as they considerably reduce the need of data for under-resourced languages. Nevertheless, the superiority of language-specific models has already been proven for languages having access to large amounts of data. In this work, we focus on Catalan with the aim to explore to what extent a medium-sized monolingual language model is competitive with state-of-the-art large multilingual models. For this, we: (1) build a clean, high-quality textual Catalan corpus ({CaText}), the largest to date (but only a fraction of the usual size of the previous work in monolingual language models), (2) train a Transformer-based language model for Catalan ({BERTa}), and (3) devise a thorough evaluation in a diversity of settings, comprising a complete array of downstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and Classification, Text Classification, Question Answering, and Semantic Textual Similarity, with most of the corresponding datasets being created ex novo. The result is a new benchmark, the Catalan Language Understanding Benchmark ({CLUB}), which we publish as an open resource, together with the clean textual corpus, the language model, and the cleaning pipeline. Using state-of-the-art multilingual models and a monolingual model trained only on Wikipedia as baselines, we consistently observe the superiority of our model across tasks and settings.},
	number = {{arXiv}:2107.07903},
	publisher = {{arXiv}},
	author = {Armengol-Estapé, Jordi and Carrino, Casimiro Pio and Rodriguez-Penagos, Carlos and Bonet, Ona de Gibert and Armentano-Oller, Carme and Gonzalez-Agirre, Aitor and Melero, Maite and Villegas, Marta},
	urldate = {2024-05-29},
	date = {2021-07-16},
	eprinttype = {arxiv},
	eprint = {2107.07903 [cs]},
}

@misc{kulkarniAQuaMuSeAutomaticallyGenerating2020,
	title = {{AQuaMuSe}: Automatically Generating Datasets for Query-Based Multi-Document Summarization},
	url = {http://arxiv.org/abs/2010.12694},
	shorttitle = {{AQuaMuSe}},
	abstract = {Summarization is the task of compressing source document(s) into coherent and succinct passages. This is a valuable tool to present users with concise and accurate sketch of the top ranked documents related to their queries. Query-based multi-document summarization ({qMDS}) addresses this pervasive need, but the research is severely limited due to lack of training and evaluation datasets as existing single-document and multi-document summarization datasets are inadequate in form and scale. We propose a scalable approach called {AQuaMuSe} to automatically mine {qMDS} examples from question answering datasets and large document corpora. Our approach is unique in the sense that it can general a dual dataset -- for extractive and abstractive summaries both. We publicly release a specific instance of an {AQuaMuSe} dataset with 5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl. Extensive evaluation of the dataset along with baseline summarization model experiments are provided.},
	number = {{arXiv}:2010.12694},
	publisher = {{arXiv}},
	author = {Kulkarni, Sayali and Chammas, Sheide and Zhu, Wan and Sha, Fei and Ie, Eugene},
	urldate = {2024-05-29},
	date = {2020-10-23},
	eprinttype = {arxiv},
	eprint = {2010.12694 [cs]},
}

@misc{musaAnsweringScienceExam2019,
	title = {Answering Science Exam Questions Using Query Rewriting with Background Knowledge},
	url = {http://arxiv.org/abs/1809.05726},
	abstract = {Open-domain question answering ({QA}) is an important problem in {AI} and {NLP} that is emerging as a bellwether for progress on the generalizability of {AI} methods and techniques. Much of the progress in open-domain {QA} systems has been realized through advances in information retrieval methods and corpus construction. In this paper, we focus on the recently introduced {ARC} Challenge dataset, which contains 2,590 multiple choice questions authored for grade-school science exams. These questions are selected to be the most challenging for current {QA} systems, and current state of the art performance is only slightly better than random chance. We present a system that rewrites a given question into queries that are used to retrieve supporting text from a large corpus of science-related text. Our rewriter is able to incorporate background knowledge from {ConceptNet} and -- in tandem with a generic textual entailment system trained on {SciTail} that identifies support in the retrieved results -- outperforms several strong baselines on the end-to-end {QA} task despite only being trained to identify essential terms in the original source question. We use a generalizable decision methodology over the retrieved evidence and answer candidates to select the best answer. By combining query rewriting, background knowledge, and textual entailment our system is able to outperform several strong baselines on the {ARC} dataset.},
	number = {{arXiv}:1809.05726},
	publisher = {{arXiv}},
	author = {Musa, Ryan and Wang, Xiaoyan and Fokoue, Achille and Mattei, Nicholas and Chang, Maria and Kapanipathi, Pavan and Makni, Bassem and Talamadupula, Kartik and Witbrock, Michael},
	urldate = {2024-05-29},
	date = {2019-04-05},
	eprinttype = {arxiv},
	eprint = {1809.05726 [cs]},
}

@article{liAdversarySocialGood2024,
	title = {Adversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy},
	volume = {18},
	issn = {1556-4681, 1556-472X},
	url = {http://arxiv.org/abs/2306.02488},
	doi = {10/gtwwxw},
	shorttitle = {Adversary for Social Good},
	abstract = {Social media has drastically reshaped the world that allows billions of people to engage in such interactive environments to conveniently create and share content with the public. Among them, text data (e.g., tweets, blogs) maintains the basic yet important social activities and generates a rich source of user-oriented information. While those explicit sensitive user data like credentials has been significantly protected by all means, personal private attribute (e.g., age, gender, location) disclosure due to inference attacks is somehow challenging to avoid, especially when powerful natural language processing ({NLP}) techniques have been effectively deployed to automate attribute inferences from implicit text data. This puts users' attribute privacy at risk. To address this challenge, in this paper, we leverage the inherent vulnerability of machine learning to adversarial attacks, and design a novel text-space Adversarial attack for Social Good, called Adv4SG. In other words, we cast the problem of protecting personal attribute privacy as an adversarial attack formulation problem over the social media text data to defend against {NLP}-based attribute inference attacks. More specifically, Adv4SG proceeds with a sequence of word perturbations under given constraints such that the probed attribute cannot be identified correctly. Different from the prior works, we advance Adv4SG by considering social media property, and introducing cost-effective mechanisms to expedite attribute obfuscation over text data under the black-box setting. Extensive experiments on real-world social media datasets have demonstrated that our method can effectively degrade the inference accuracy with less computational cost over different attribute settings, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection.},
	pages = {1--24},
	number = {2},
	journaltitle = {{ACM} Transactions on Knowledge Discovery from Data},
	shortjournal = {{ACM} Trans. Knowl. Discov. Data},
	author = {Li, Xiaoting and Chen, Lingwei and Wu, Dinghao},
	urldate = {2024-05-29},
	date = {2024-02-29},
	eprinttype = {arxiv},
	eprint = {2306.02488 [cs]},
}

@misc{wallaceAnalyzingDynamicAdversarial2022,
	title = {Analyzing Dynamic Adversarial Training Data in the Limit},
	url = {http://arxiv.org/abs/2110.08514},
	abstract = {To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection ({DADC}), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running {DADC} over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running {DADC} over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term {DADC}, where we collect 20 rounds of {NLI} examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on {DADC} examples make 26\% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that {DADC} yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.},
	number = {{arXiv}:2110.08514},
	publisher = {{arXiv}},
	author = {Wallace, Eric and Williams, Adina and Jia, Robin and Kiela, Douwe},
	urldate = {2024-05-29},
	date = {2022-09-26},
	eprinttype = {arxiv},
	eprint = {2110.08514 [cs]},
}

@misc{liuEmpiricalStudyModelagnostic2020,
	title = {An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference},
	url = {http://arxiv.org/abs/2010.03777},
	abstract = {The prior work on natural language inference ({NLI}) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the {NLI} models robust to multiple distinct adversarial attacks while keeping or even strengthening the models' generalization power. We firstly benchmark prevailing neural {NLI} models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts ({MoE}) ensemble method and show that it's nontrivial to mitigate multiple {NLI} biases at the same time, and that model-level ensemble method outperforms {MoE} ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen {NLI} models.},
	number = {{arXiv}:2010.03777},
	publisher = {{arXiv}},
	author = {Liu, Tianyu and Zheng, Xin and Ding, Xiaoan and Chang, Baobao and Sui, Zhifang},
	urldate = {2024-05-29},
	date = {2020-10-17},
	eprinttype = {arxiv},
	eprint = {2010.03777 [cs]},
}

@misc{hendrycksAligningAIShared2023,
	title = {Aligning {AI} With Shared Human Values},
	url = {http://arxiv.org/abs/2008.02275},
	abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the {ETHICS} dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the {ETHICS} dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward {AI} that is aligned with human values.},
	number = {{arXiv}:2008.02275},
	publisher = {{arXiv}},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
	urldate = {2024-05-29},
	date = {2023-02-17},
	eprinttype = {arxiv},
	eprint = {2008.02275 [cs]},
}

@misc{minAmbigQAAnsweringAmbiguous2020,
	title = {{AmbigQA}: Answering Ambiguous Open-domain Questions},
	url = {http://arxiv.org/abs/2004.10645},
	shorttitle = {{AmbigQA}},
	abstract = {Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce {AmbigQA}, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct {AmbigNQ}, a dataset covering 14,042 questions from {NQ}-open, an existing open-domain {QA} benchmark. We find that over half of the questions in {NQ}-open are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for {AmbigQA} which we show benefit from weakly supervised learning that incorporates {NQ}-open, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.},
	number = {{arXiv}:2004.10645},
	publisher = {{arXiv}},
	author = {Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	urldate = {2024-05-29},
	date = {2020-10-04},
	eprinttype = {arxiv},
	eprint = {2004.10645 [cs]},
}

@misc{saxtonAnalysingMathematicalReasoning2019,
	title = {Analysing Mathematical Reasoning Abilities of Neural Models},
	url = {http://arxiv.org/abs/1904.01557},
	abstract = {Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.},
	number = {{arXiv}:1904.01557},
	publisher = {{arXiv}},
	author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
	urldate = {2024-05-29},
	date = {2019-04-02},
	eprinttype = {arxiv},
	eprint = {1904.01557 [cs, stat]},
}

@misc{ebertVisuospatialDatasetNaturalistic2020,
	title = {A Visuospatial Dataset for Naturalistic Verb Learning},
	url = {http://arxiv.org/abs/2010.15225},
	abstract = {We introduce a new dataset for training and evaluating grounded language models. Our data is collected within a virtual reality environment and is designed to emulate the quality of language data to which a pre-verbal child is likely to have access: That is, naturalistic, spontaneous speech paired with richly grounded visuospatial context. We use the collected data to compare several distributional semantics models for verb learning. We evaluate neural models based on 2D (pixel) features as well as feature-engineered models based on 3D (symbolic, spatial) features, and show that neither modeling approach achieves satisfactory performance. Our results are consistent with evidence from child language acquisition that emphasizes the difficulty of learning verbs from naive distributional data. We discuss avenues for future work on cognitively-inspired grounded language learning, and release our corpus with the intent of facilitating research on the topic.},
	number = {{arXiv}:2010.15225},
	publisher = {{arXiv}},
	author = {Ebert, Dylan and Pavlick, Ellie},
	urldate = {2024-05-29},
	date = {2020-10-28},
	eprinttype = {arxiv},
	eprint = {2010.15225 [cs]},
}

@misc{kimAbstractiveSummarizationReddit2019,
	title = {Abstractive Summarization of Reddit Posts with Multi-level Memory Networks},
	url = {http://arxiv.org/abs/1811.00783},
	abstract = {We address the problem of abstractive summarization in two directions: proposing a novel dataset and a new model. First, we collect Reddit {TIFU} dataset, consisting of 120K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing datasets that mostly use formal documents as source such as news articles. Thus, our dataset could less suffer from some biases that key sentences usually locate at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks ({MMN}), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the Reddit {TIFU} dataset is highly abstractive and the {MMN} outperforms the state-of-the-art summarization models.},
	number = {{arXiv}:1811.00783},
	publisher = {{arXiv}},
	author = {Kim, Byeongchang and Kim, Hyunwoo and Kim, Gunhee},
	urldate = {2024-05-29},
	date = {2019-04-08},
	eprinttype = {arxiv},
	eprint = {1811.00783 [cs]},
}

@misc{kuhnAceWikiCollaborativeOntology2008,
	title = {{AceWiki}: Collaborative Ontology Management in Controlled Natural Language},
	url = {http://arxiv.org/abs/0807.4623},
	shorttitle = {{AceWiki}},
	abstract = {{AceWiki} is a prototype that shows how a semantic wiki using controlled natural language - Attempto Controlled English ({ACE}) in our case - can make ontology management easy for everybody. Sentences in {ACE} can automatically be translated into first-order logic, {OWL}, or {SWRL}. {AceWiki} integrates the {OWL} reasoner Pellet and ensures that the ontology is always consistent. Previous results have shown that people with no background in logic are able to add formal knowledge to {AceWiki} without being instructed or trained in advance.},
	number = {{arXiv}:0807.4623},
	publisher = {{arXiv}},
	author = {Kuhn, Tobias},
	urldate = {2024-05-29},
	date = {2008-07-29},
	eprinttype = {arxiv},
	eprint = {0807.4623 [cs]},
}

@inproceedings{birkeActiveLearningIdentification2007,
	location = {Rochester, New York},
	title = {Active Learning for the Identification of Nonliteral Language},
	url = {https://aclanthology.org/W07-0104},
	pages = {21--28},
	booktitle = {Proceedings of the Workshop on Computational Approaches to Figurative Language},
	publisher = {Association for Computational Linguistics},
	author = {Birke, Julia and Sarkar, Anoop},
	editor = {Feldman, Anna and Lu, Xiaofei},
	urldate = {2024-05-29},
	date = {2007-04},
}

@misc{cuiSpanExtractionDatasetChinese2019,
	title = {A Span-Extraction Dataset for Chinese Machine Reading Comprehension},
	url = {http://arxiv.org/abs/1810.07366},
	doi = {10.18653/v1/D19-1600},
	abstract = {Machine Reading Comprehension ({MRC}) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in English. In this paper, we introduce a Span-Extraction dataset for Chinese machine reading comprehension to add language diversities in this area. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several baseline systems as well as anonymous submissions for demonstrating the difficulties in this dataset. With the release of the dataset, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension ({CMRC} 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research. Resources are available: https://github.com/ymcui/cmrc2018},
	author = {Cui, Yiming and Liu, Ting and Che, Wanxiang and Xiao, Li and Chen, Zhipeng and Ma, Wentao and Wang, Shijin and Hu, Guoping},
	urldate = {2024-05-29},
	date = {2019-08-29},
	eprinttype = {arxiv},
	eprint = {1810.07366 [cs]},
}

@misc{szomiuPuzzleBasedDatasetNatural2021,
	title = {A Puzzle-Based Dataset for Natural Language Inference},
	url = {http://arxiv.org/abs/2112.05742},
	doi = {10.13140/RG.2.2.19206.09289},
	abstract = {We provide here a dataset for tasks related to natural language understanding and natural language inference. The dataset contains logical puzzles in natural language from three domains: comparing puzzles, knighs and knaves, and zebra puzzles. Each puzzle is associated with the entire set of atomic questions that can be generated based on the relations and individuals occurring in the text. For each question we provide the correct answer: entailment, contradiction or ambiguity. The answer's correctness is verified against theorem provers. Good puzzles have two properties: (i) each piece of information is necessary and (ii) no unnecessary information is provided. These properties make puzzles interesting candidates for machine comprehension tasks.},
	author = {Szomiu, Roxana and Groza, Adrian},
	urldate = {2024-05-29},
	date = {2021-12-10},
	eprinttype = {arxiv},
	eprint = {2112.05742 [cs]},
}

@misc{sileoPragmaticsCenteredEvaluationFramework2022,
	title = {A Pragmatics-Centered Evaluation Framework for Natural Language Understanding},
	url = {http://arxiv.org/abs/1907.08672},
	abstract = {New models for natural language understanding have recently made an unparalleled amount of progress, which has led some researchers to suggest that the models induce universal text representations. However, current benchmarks are predominantly targeting semantic phenomena; we make the case that pragmatics needs to take center stage in the evaluation of natural language understanding. We introduce {PragmEval}, a new benchmark for the evaluation of natural language understanding, that unites 11 pragmatics-focused evaluation datasets for English. {PragmEval} can be used as supplementary training data in a multi-task learning setup, and is publicly available, alongside the code for gathering and preprocessing the datasets. Using our evaluation suite, we show that natural language inference, a widely used pretraining task, does not result in genuinely universal representations, which presents a new challenge for multi-task learning.},
	number = {{arXiv}:1907.08672},
	publisher = {{arXiv}},
	author = {Sileo, Damien and Van-de-Cruys, Tim and Pradel, Camille and Muller, Philippe},
	urldate = {2024-05-29},
	date = {2022-04-04},
	eprinttype = {arxiv},
	eprint = {1907.08672 [cs]},
}

@inproceedings{marelliSICKCureEvaluation2014,
	location = {Reykjavik, Iceland},
	title = {A {SICK} cure for the evaluation of compositional distributional semantic models},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf},
	abstract = {Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models ({CDSMs}) by providing {SICK} (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. {SICK} consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that {CDSMs} are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of {CDSMs}. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The {SICK} data set was used in {SemEval}-2014 Task 1, and it freely available for research purposes.},
	eventtitle = {{LREC} 2014},
	pages = {216--223},
	booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Marelli, Marco and Menini, Stefano and Baroni, Marco and Bentivogli, Luisa and Bernardi, Raffaella and Zamparelli, Roberto},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-29},
	date = {2014-05},
}

@misc{siripragadaMultilingualParallelCorpora2020,
	title = {A Multilingual Parallel Corpora Collection Effort for Indian Languages},
	url = {http://arxiv.org/abs/2007.07691},
	abstract = {We present sentence aligned parallel corpora across 10 Indian Languages - Hindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi, Punjabi, and English - many of which are categorized as low resource. The corpora are compiled from online sources which have content shared across languages. The corpora presented significantly extends present resources that are either not large enough or are restricted to a specific domain (such as health). We also provide a separate test corpus compiled from an independent online source that can be independently used for validating the performance in 10 Indian languages. Alongside, we report on the methods of constructing such corpora using tools enabled by recent advances in machine translation and cross-lingual retrieval using deep neural network based methods.},
	number = {{arXiv}:2007.07691},
	publisher = {{arXiv}},
	author = {Siripragada, Shashank and Philip, Jerin and Namboodiri, Vinay P. and Jawahar, C. V.},
	urldate = {2024-05-29},
	date = {2020-07-15},
	eprinttype = {arxiv},
	eprint = {2007.07691 [cs]},
}

@article{margolisMultivehicleCoveringTour2022,
	title = {A multi-vehicle covering tour problem with speed optimization},
	volume = {79},
	issn = {0028-3045, 1097-0037},
	url = {http://arxiv.org/abs/1909.12435},
	doi = {10/gjx6s4},
	abstract = {The multi-vehicle covering tour problem with time windows ({MCTPTW}) aims to construct a set of maximal coverage routes for a fleet of vehicles that serve (observe) a secondary set of sites given a fixed time schedule, coverage requirements, and energy restrictions. The problem is formulated as a mixed-integer second-order cone programming ({MISOCP}) model and is an extension of both the multi-covering tour problem and the vehicle routing problem with time windows under energy constraints. Further, we study a special case of the proposed model and develop a labeling algorithm to solve its Lagrangian relaxation problem, which exploits the combinatorial structure exhibited by an optimal solution to the Lagrangian relaxation.},
	pages = {119--142},
	number = {2},
	journaltitle = {Networks},
	shortjournal = {Networks},
	author = {Margolis, Joshua T. and Song, Yongjia and Mason, Scott J.},
	urldate = {2024-05-29},
	date = {2022-03},
	eprinttype = {arxiv},
	eprint = {1909.12435 [math]},
}

@misc{salvatoreLogicalbasedCorpusCrosslingual2019,
	title = {A logical-based corpus for cross-lingual evaluation},
	url = {http://arxiv.org/abs/1905.05704},
	abstract = {At present, different deep learning models are presenting high accuracy on popular inference datasets such as {SNLI}, {MNLI}, and {SciTail}. However, there are different indicators that those datasets can be exploited by using some simple linguistic patterns. This fact poses difficulties to our understanding of the actual capacity of machine learning models to solve the complex task of textual inference. We propose a new set of syntactic tasks focused on contradiction detection that require specific capacities over linguistic logical forms such as: Boolean coordination, quantifiers, definite description, and counting operators. We evaluate two kinds of deep learning models that implicitly exploit language structure: recurrent models and the Transformer network {BERT}. We show that although {BERT} is clearly more efficient to generalize over most logical forms, there is space for improvement when dealing with counting operators. Since the syntactic tasks can be implemented in different languages, we show a successful case of cross-lingual transfer learning between English and Portuguese.},
	number = {{arXiv}:1905.05704},
	publisher = {{arXiv}},
	author = {Salvatore, Felipe and Finger, Marcelo and Hirata Jr, Roberto},
	urldate = {2024-05-29},
	date = {2019-10-23},
	eprinttype = {arxiv},
	eprint = {1905.05704 [cs]},
}

@misc{ningMultiAxisAnnotationScheme2018,
	title = {A Multi-Axis Annotation Scheme for Event Temporal Relations},
	url = {http://arxiv.org/abs/1804.07828},
	abstract = {Existing temporal relation ({TempRel}) annotation schemes often have low inter-annotator agreements ({IAA}) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate {TempRels} based on start-points only. A pilot expert annotation using the proposed scheme shows significant improvement in {IAA} from the conventional 60's to 80's (Cohen's Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.},
	number = {{arXiv}:1804.07828},
	publisher = {{arXiv}},
	author = {Ning, Qiang and Wu, Hao and Roth, Dan},
	urldate = {2024-05-29},
	date = {2018-05-13},
	eprinttype = {arxiv},
	eprint = {1804.07828 [cs]},
}

@misc{miaoDiverseCorpusEvaluating2021,
	title = {A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers},
	url = {http://arxiv.org/abs/2106.15772},
	abstract = {We present {ASDiv} (Academia Sinica Diverse {MWP} Dataset), a diverse (in terms of both language patterns and problem types) English math word problem ({MWP}) corpus for evaluating the capability of various {MWP} solvers. Existing {MWP} corpora for studying {AI} progress remain limited either in language usage patterns or in problem types. We thus present a new English {MWP} corpus with 2,305 {MWPs} that cover more text patterns and most problem types taught in elementary school. Each {MWP} is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given {MWP} corpus, and demonstrate that {ASDiv} is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of {MWP} solvers more faithfully.},
	number = {{arXiv}:2106.15772},
	publisher = {{arXiv}},
	author = {Miao, Shen-Yun and Liang, Chao-Chun and Su, Keh-Yih},
	urldate = {2024-05-29},
	date = {2021-06-29},
	eprinttype = {arxiv},
	eprint = {2106.15772 [cs]},
}

@misc{dasigiDatasetInformationSeekingQuestions2021,
	title = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
	url = {http://arxiv.org/abs/2105.03011},
	abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present {QASPER}, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an {NLP} practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of {NLP} practitioners who also provide supporting evidence to answers. We find that existing models that do well on other {QA} tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking {QA}, which our dataset is designed to facilitate.},
	number = {{arXiv}:2105.03011},
	publisher = {{arXiv}},
	author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
	urldate = {2024-05-29},
	date = {2021-05-06},
	eprinttype = {arxiv},
	eprint = {2105.03011 [cs]},
}

@misc{tandonDatasetTrackingEntities2020,
	title = {A Dataset for Tracking Entities in Open Domain Procedural Text},
	url = {http://arxiv.org/abs/2011.08092},
	abstract = {We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky,opaque, and clear. Previous formulations of this task provide the text and entities involved,and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples(entity, at-tribute, before-state, after-state)for each step,where the entity, attribute, and state values must be predicted from an open vocabulary. Using crowdsourcing, we create {OPENPI}1, a high-quality (91.5\% coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from {WikiHow}.com. A current state-of-the-art generation model on this task achieves 16.1\% F1 based on {BLEU} metric, leaving enough room for novel model architectures.},
	number = {{arXiv}:2011.08092},
	publisher = {{arXiv}},
	author = {Tandon, Niket and Sakaguchi, Keisuke and Mishra, Bhavana Dalvi and Rajagopal, Dheeraj and Clark, Peter and Guerquin, Michal and Richardson, Kyle and Hovy, Eduard},
	urldate = {2024-05-29},
	date = {2020-10-30},
	eprinttype = {arxiv},
	eprint = {2011.08092 [cs]},
}

@misc{luoDualReinforcementLearning2019,
	title = {A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer},
	url = {http://arxiv.org/abs/1905.10060},
	abstract = {Unsupervised text style transfer aims to transfer the underlying style of text but keep its main content unchanged without parallel data. Most existing methods typically follow two steps: first separating the content from the original style, and then fusing the content with the desired style. However, the separation in the first step is challenging because the content and style interact in subtle ways in natural language. Therefore, in this paper, we propose a dual reinforcement learning framework to directly transfer the style of the text via a one-step mapping model, without any separation of content and style. Specifically, we consider the learning of the source-to-target and target-to-source mappings as a dual task, and two rewards are designed based on such a dual structure to reflect the style accuracy and content preservation, respectively. In this way, the two one-step mapping models can be trained via reinforcement learning, without any use of parallel data. Automatic evaluations show that our model outperforms the state-of-the-art systems by a large margin, especially with more than 8 {BLEU} points improvement averaged on two benchmark datasets. Human evaluations also validate the effectiveness of our model in terms of style accuracy, content preservation and fluency. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/{DualLanST}.},
	number = {{arXiv}:1905.10060},
	publisher = {{arXiv}},
	author = {Luo, Fuli and Li, Peng and Zhou, Jie and Yang, Pengcheng and Chang, Baobao and Sui, Zhifang and Sun, Xu},
	urldate = {2024-05-29},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1905.10060 [cs]},
}

@misc{kangDatasetPeerReviews2018,
	title = {A Dataset of Peer Reviews ({PeerRead}): Collection, Insights and {NLP} Applications},
	url = {http://arxiv.org/abs/1804.09635},
	shorttitle = {A Dataset of Peer Reviews ({PeerRead})},
	abstract = {Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes ({PeerRead} v1) providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including {ACL}, {NIPS} and {ICLR}. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel {NLP} tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21\% error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as 'originality' and 'impact'.},
	number = {{arXiv}:1804.09635},
	publisher = {{arXiv}},
	author = {Kang, Dongyeop and Ammar, Waleed and Dalvi, Bhavana and van Zuylen, Madeleine and Kohlmeier, Sebastian and Hovy, Eduard and Schwartz, Roy},
	urldate = {2024-05-29},
	date = {2018-04-25},
	eprinttype = {arxiv},
	eprint = {1804.09635 [cs]},
}

@misc{bowmanLargeAnnotatedCorpus2015,
	title = {A large annotated corpus for learning natural language inference},
	url = {http://arxiv.org/abs/1508.05326},
	abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
	number = {{arXiv}:1508.05326},
	publisher = {{arXiv}},
	author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	urldate = {2024-05-29},
	date = {2015-08-21},
	eprinttype = {arxiv},
	eprint = {1508.05326 [cs]},
}

@misc{anticiCorpusSentencelevelSubjectivity2024,
	title = {A Corpus for Sentence-level Subjectivity Detection on English News Articles},
	url = {http://arxiv.org/abs/2305.18034},
	abstract = {We develop novel annotation guidelines for sentence-level subjectivity detection, which are not limited to language-specific cues. We use our guidelines to collect {NewsSD}-{ENG}, a corpus of 638 objective and 411 subjective sentences extracted from English news articles on controversial topics. Our corpus paves the way for subjectivity detection in English and across other languages without relying on language-specific tools, such as lexicons or machine translation. We evaluate state-of-the-art multilingual transformer-based models on the task in mono-, multi-, and cross-language settings. For this purpose, we re-annotate an existing Italian corpus. We observe that models trained in the multilingual setting achieve the best performance on the task.},
	number = {{arXiv}:2305.18034},
	publisher = {{arXiv}},
	author = {Antici, Francesco and Galassi, Andrea and Ruggeri, Federico and Korre, Katerina and Muti, Arianna and Bardi, Alessandra and Fedotova, Alice and Barrón-Cedeño, Alberto},
	urldate = {2024-05-29},
	date = {2024-05-24},
	eprinttype = {arxiv},
	eprint = {2305.18034 [cs]},
}

@inproceedings{durmusCorpusModelingUser2019,
	title = {A Corpus for Modeling User and Language Effects in Argumentation on Online Debating},
	url = {http://arxiv.org/abs/1906.11310},
	doi = {10/gtwwtb},
	abstract = {Existing argumentation datasets have succeeded in allowing researchers to develop computational methods for analyzing the content, structure and linguistic features of argumentative text. They have been much less successful in fostering studies of the effect of "user" traits -- characteristics and beliefs of the participants -- on the debate/argument outcome as this type of user information is generally not available. This paper presents a dataset of 78, 376 debates generated over a 10-year period along with surprisingly comprehensive participant profiles. We also complete an example study using the dataset to analyze the effect of selected user traits on the debate outcome in comparison to the linguistic features typically employed in studies of this kind.},
	pages = {602--607},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	author = {Durmus, Esin and Cardie, Claire},
	urldate = {2024-05-29},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1906.11310 [cs]},
}

@misc{williamsBroadCoverageChallengeCorpus2018,
	title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
	url = {http://arxiv.org/abs/1704.05426},
	abstract = {This paper introduces the Multi-Genre Natural Language Inference ({MultiNLI}) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of {NLI}, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.},
	number = {{arXiv}:1704.05426},
	publisher = {{arXiv}},
	author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R.},
	urldate = {2024-05-29},
	date = {2018-02-19},
	eprinttype = {arxiv},
	eprint = {1704.05426 [cs]},
}

@misc{mostafazadehCorpusEvaluationFramework2016,
	title = {A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories},
	url = {http://arxiv.org/abs/1604.01696},
	abstract = {Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the {NLP} community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of {\textasciitilde}50k five-sentence commonsense stories, {ROCStories}, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.},
	number = {{arXiv}:1604.01696},
	publisher = {{arXiv}},
	author = {Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
	urldate = {2024-05-29},
	date = {2016-04-06},
	eprinttype = {arxiv},
	eprint = {1604.01696 [cs]},
}

@inproceedings{perazziBenchmarkDatasetEvaluation2016,
	title = {A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation},
	url = {https://ieeexplore.ieee.org/document/7780454},
	doi = {10/ggdmmw},
	abstract = {Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named {DAVIS} (Densely Annotated {VIdeo} Segmentation), consists of fifty high quality, Full {HD} video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motionblur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {724--732},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Perazzi, F. and Pont-Tuset, J. and {McWilliams}, B. and Van Gool, L. and Gross, M. and Sorkine-Hornung, A.},
	urldate = {2024-05-29},
	date = {2016-06},
	note = {{ISSN}: 1063-6919},
}

@inproceedings{weiAirDialogueEnvironmentGoalOriented2018,
	location = {Brussels, Belgium},
	title = {{AirDialogue}: An Environment for Goal-Oriented Dialogue Research},
	url = {https://aclanthology.org/D18-1419},
	doi = {10/gf6gq2},
	shorttitle = {{AirDialogue}},
	abstract = {Recent progress in dialogue generation has inspired a number of studies on dialogue systems that are capable of accomplishing tasks through natural language interactions. A promising direction among these studies is the use of reinforcement learning techniques, such as self-play, for training dialogue agents. However, current datasets are limited in size, and the environment for training agents and evaluating progress is relatively unsophisticated. We present {AirDialogue}, a large dataset that contains 301,427 goal-oriented conversations. To collect this dataset, we create a context-generator which provides travel and flight restrictions. We then ask human annotators to play the role of a customer or an agent and interact with the goal of successfully booking a trip given the restrictions. Key to our environment is the ease of evaluating the success of the dialogue, which is achieved by using ground-truth states (e.g., the flight being booked) generated by the restrictions. Any dialogue agent that does not generate the correct states is considered to fail. Our experimental results indicate that state-of-the-art dialogue models can only achieve a score of 0.17 while humans can reach a score of 0.91, which suggests significant opportunities for future improvement.},
	eventtitle = {{EMNLP} 2018},
	pages = {3844--3854},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Wei, Wei and Le, Quoc and Dai, Andrew and Li, Jia},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	urldate = {2024-05-01},
	date = {2018-10},
}

@inproceedings{iyyerSearchbasedNeuralStructured2017,
	location = {Vancouver, Canada},
	title = {Search-based Neural Structured Learning for Sequential Question Answering},
	url = {https://aclanthology.org/P17-1167},
	doi = {10/gf6nx8},
	abstract = {Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational {QA} setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our model effectively leverages the sequential context to outperform state-of-the-art {QA} systems that are designed to answer highly complex questions.},
	eventtitle = {{ACL} 2017},
	pages = {1821--1831},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Iyyer, Mohit and Yih, Wen-tau and Chang, Ming-Wei},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	urldate = {2024-05-01},
	date = {2017-07},
}

@inproceedings{zhaoWildChat1MChatGPT2023,
	title = {{WildChat}: 1M {ChatGPT} Interaction Logs in the Wild},
	url = {https://openreview.net/forum?id=Bl8u7ZRlbM},
	shorttitle = {{WildChat}},
	abstract = {Chatbots such as {GPT}-4 and {ChatGPT} are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to {ChatGPT} for online users in exchange for their affirmative, consensual opt-in to anonymously collect their chat transcripts and request headers. From this, we compiled {WildChat}, a corpus of 1 million user-{ChatGPT} conversations, which consists of over 2.5 million interaction turns. We compare {WildChat} with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In addition to timestamped chat transcripts, we enrich the dataset with demographic data, including state, country, and hashed {IP} addresses, alongside request headers. This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions. Finally, because it captures a broad range of use cases, we demonstrate the dataset’s potential utility in fine-tuning instruction-following models. {WildChat} is released at https://wildchat.allen.ai under {AI}2 {ImpACT} Licenses.},
	booktitle = {Proceedings of the Twelfth International Conference on Learning Representations},
	author = {Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian},
	urldate = {2024-05-01},
	date = {2023-10-13},
	langid = {english},
}

@inproceedings{myersConversationalScaffoldingAnalogybased2020,
	location = {Valletta, Malta},
	title = {Conversational Scaffolding: An Analogy-based Approach to Response Prioritization in Open-domain Dialogs:},
	isbn = {978-989-758-395-7},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0008939900690078},
	doi = {10/gtsq86},
	shorttitle = {Conversational Scaffolding},
	eventtitle = {12th International Conference on Agents and Artificial Intelligence},
	pages = {69--78},
	booktitle = {Proceedings of the 12th International Conference on Agents and Artificial Intelligence},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Myers, Will and Etchart, Tyler and Fulda, Nancy},
	urldate = {2024-05-02},
	date = {2020},
}

@inproceedings{dasThousandFramesJust2013,
	location = {Portland, {OR}, {USA}},
	title = {A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6619184/},
	doi = {10/gtsqzr},
	shorttitle = {A Thousand Frames in Just a Few Words},
	eventtitle = {2013 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2634--2641},
	booktitle = {2013 {IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Das, Pradipto and Xu, Chenliang and Doell, Richard F. and Corso, Jason J.},
	urldate = {2024-05-02},
	date = {2013-06},
}

@inproceedings{epsteinOopsPredictingUnintentional2020,
	location = {Seattle, {WA}, {USA}},
	title = {Oops! Predicting Unintentional Action in Video},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156404/},
	doi = {10/ghbckh},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {916--926},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Epstein, Dave and Chen, Boyuan and Vondrick, Carl},
	urldate = {2024-05-02},
	date = {2020-06},
}

@inproceedings{dasToyotaSmarthomeRealWorld2019,
	location = {Seoul, Korea (South)},
	title = {Toyota Smarthome: Real-World Activities of Daily Living},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008135/},
	doi = {10/ghfjc7},
	shorttitle = {Toyota Smarthome},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {833--842},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Das, Srijan and Dai, Rui and Koperski, Michal and Minciullo, Luca and Garattoni, Lorenzo and Bremond, Francois and Francesca, Gianpiero},
	urldate = {2024-05-02},
	date = {2019-10},
}

@inproceedings{heilbronActivityNetLargescaleVideo2015,
	location = {Boston, {MA}, {USA}},
	title = {{ActivityNet}: A large-scale video benchmark for human activity understanding},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298698/},
	doi = {10/gfsvdw},
	shorttitle = {{ActivityNet}},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {961--970},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
	urldate = {2024-05-02},
	date = {2015-06},
}

@inproceedings{hendricksLocalizingMomentsVideo2018,
	location = {Brussels, Belgium},
	title = {Localizing Moments in Video with Temporal Language},
	url = {https://aclanthology.org/D18-1168},
	doi = {10/gtsqzq},
	abstract = {Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel {TEMPOral} reasoning in video and language ({TEMPO}) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences ({TEMPO} - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans ({TEMPO} - Human Language).},
	eventtitle = {{EMNLP} 2018},
	pages = {1380--1390},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Hendricks, Lisa Anne and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	urldate = {2024-05-02},
	date = {2018-10},
}

@inproceedings{habibianVideoStoryNewMultimedia2014,
	location = {Orlando Florida {USA}},
	title = {{VideoStory}: A New Multimedia Embedding for Few-Example Recognition and Translation of Events},
	isbn = {978-1-4503-3063-3},
	url = {https://dl.acm.org/doi/10.1145/2647868.2654913},
	doi = {10/ggs25n},
	shorttitle = {{VideoStory}},
	eventtitle = {{MM} '14: 2014 {ACM} Multimedia Conference},
	pages = {17--26},
	booktitle = {Proceedings of the 22nd {ACM} international conference on Multimedia},
	publisher = {{ACM}},
	author = {Habibian, Amirhossein and Mensink, Thomas and Snoek, Cees G.M.},
	urldate = {2024-05-02},
	date = {2014-11-03},
	langid = {english},
}

@inproceedings{kuehneLanguageActionsRecovering2014,
	location = {Columbus, {OH}, {USA}},
	title = {The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},
	isbn = {978-1-4799-5118-5},
	url = {https://ieeexplore.ieee.org/document/6909500/},
	doi = {10/gqdc3v},
	shorttitle = {The Language of Actions},
	eventtitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {780--787},
	booktitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
	urldate = {2024-05-02},
	date = {2014-06},
}

@article{idreesTHUMOSChallengeAction2017,
	title = {The {THUMOS} Challenge on Action Recognition for Videos "in the Wild"},
	volume = {155},
	issn = {10773142},
	url = {http://arxiv.org/abs/1604.06182},
	doi = {10/f9rwnr},
	abstract = {Automatically recognizing and localizing wide ranges of human actions has crucial importance for video understanding. Towards this goal, the {THUMOS} challenge was introduced in 2013 to serve as a benchmark for action recognition. Until then, video action recognition, including {THUMOS} challenge, had focused primarily on the classification of pre-segmented (i.e., trimmed) videos, which is an artificial task. In {THUMOS} 2014, we elevated action recognition to a more practical level by introducing temporally untrimmed videos. These also include `background videos' which share similar scenes and backgrounds as action videos, but are devoid of the specific actions. The three editions of the challenge organized in 2013--2015 have made {THUMOS} a common benchmark for action classification and detection and the annual challenge is widely attended by teams from around the world. In this paper we describe the {THUMOS} benchmark in detail and give an overview of data collection and annotation procedures. We present the evaluation protocols used to quantify results in the two {THUMOS} tasks of action classification and temporal detection. We also present results of submissions to the {THUMOS} 2015 challenge and review the participating approaches. Additionally, we include a comprehensive empirical study evaluating the differences in action recognition between trimmed and untrimmed videos, and how well methods trained on trimmed videos generalize to untrimmed videos. We conclude by proposing several directions and improvements for future {THUMOS} challenges.},
	pages = {1--23},
	journaltitle = {Computer Vision and Image Understanding},
	shortjournal = {Computer Vision and Image Understanding},
	author = {Idrees, Haroon and Zamir, Amir R. and Jiang, Yu-Gang and Gorban, Alex and Laptev, Ivan and Sukthankar, Rahul and Shah, Mubarak},
	urldate = {2024-05-02},
	date = {2017-02},
	eprinttype = {arxiv},
	eprint = {1604.06182 [cs]},
}

@inproceedings{karpathyLargeScaleVideoClassification2014,
	location = {Columbus, {OH}, {USA}},
	title = {Large-Scale Video Classification with Convolutional Neural Networks},
	isbn = {978-1-4799-5118-5},
	url = {https://ieeexplore.ieee.org/document/6909619},
	doi = {10/gf4hdn},
	eventtitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1725--1732},
	booktitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	urldate = {2024-05-02},
	date = {2014-06},
}

@inproceedings{longSearchingActionsHyperbole2020,
	location = {Seattle, {WA}, {USA}},
	title = {Searching for Actions on the Hyperbole},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157196/},
	doi = {10/gg99tb},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1138--1147},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Long, Teng and Mettes, Pascal and Shen, Heng Tao and Snoek, Cees G. M.},
	urldate = {2024-05-02},
	date = {2020-06},
}

@inproceedings{liTrackingNaturalLanguage2017,
	location = {Honolulu, {HI}},
	title = {Tracking by Natural Language Specification},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100260/},
	doi = {10/ggfgvw},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {7350--7358},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Li, Zhenyang and Tao, Ran and Gavves, Efstratios and Snoek, Cees G. M. and Smeulders, Arnold W. M.},
	urldate = {2024-05-02},
	date = {2017-07},
}

@inproceedings{materzynskaJesterDatasetLargeScale2019,
	location = {Seoul, Korea (South)},
	title = {The Jester Dataset: A Large-Scale Video Dataset of Human Gestures},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022297/},
	doi = {10/gh5k47},
	shorttitle = {The Jester Dataset},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision Workshop ({ICCVW})},
	pages = {2874--2882},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision Workshop ({ICCVW})},
	publisher = {{IEEE}},
	author = {Materzynska, Joanna and Berger, Guillaume and Bax, Ingo and Memisevic, Roland},
	urldate = {2024-05-02},
	date = {2019-10},
}

@inproceedings{marszalekActionsContext2009,
	location = {Miami, {FL}},
	title = {Actions in context},
	isbn = {978-1-4244-3992-8},
	url = {https://ieeexplore.ieee.org/document/5206557/},
	doi = {10/d5bs7p},
	eventtitle = {2009 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPR} Workshops)},
	pages = {2929--2936},
	booktitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Marszalek, Marcin and Laptev, Ivan and Schmid, Cordelia},
	urldate = {2024-05-02},
	date = {2009-06},
}

@inproceedings{mallaTITANFutureForecast2020,
	location = {Seattle, {WA}, {USA}},
	title = {{TITAN}: Future Forecast Using Action Priors},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156550/},
	doi = {10/gg99rg},
	shorttitle = {{TITAN}},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {11183--11193},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Malla, Srikanth and Dariush, Behzad and Choi, Chiho},
	urldate = {2024-05-02},
	date = {2020-06},
}

@article{rohrbachRecognizingFineGrainedComposite2016,
	title = {Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data},
	volume = {119},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1502.06648},
	doi = {10/f8w6kp},
	abstract = {Activity recognition has shown impressive progress in recent years. However, the challenges of detecting fine-grained activities and understanding how they are combined into composite activities have been largely overlooked. In this work we approach both tasks and present a dataset which provides detailed annotations to address them. The first challenge is to detect fine-grained activities, which are defined by low inter-class variability and are typically characterized by fine-grained body motions. We explore how human pose and hands can help to approach this challenge by comparing two pose-based and two hand-centric features with state-of-the-art holistic features. To attack the second challenge, recognizing composite activities, we leverage the fact that these activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. We show the benefits of our hand-centric approach for fine-grained activity classification and detection. For composite activity recognition we find that decomposition into attributes allows sharing information across composites and is essential to attack this hard task. Using script data we can recognize novel composites without having training data for them.},
	pages = {346--373},
	number = {3},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Rohrbach, Marcus and Rohrbach, Anna and Regneri, Michaela and Amin, Sikandar and Andriluka, Mykhaylo and Pinkal, Manfred and Schiele, Bernt},
	urldate = {2024-05-01},
	date = {2016-09},
	eprinttype = {arxiv},
	eprint = {1502.06648 [cs]},
}

@inproceedings{xiangCDADCommonDaily2022,
	location = {New Orleans, {LA}, {USA}},
	title = {{CDAD}: A Common Daily Action Dataset with Collected Hard Negative Samples},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66548-739-9},
	url = {https://ieeexplore.ieee.org/document/9857185/},
	doi = {10/gtsqzp},
	shorttitle = {{CDAD}},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	pages = {3920--3929},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	publisher = {{IEEE}},
	author = {Xiang, Wangmeng and Li, Chao and Li, Ke and Wang, Biao and Hua, Xian-Sheng and Zhang, Lei},
	urldate = {2024-05-02},
	date = {2022-06},
}

@inproceedings{wongunchoiWhatAreThey2009,
	location = {Kyoto, Japan},
	title = {What are they doing? : Collective activity classification using spatio-temporal relationship among people},
	isbn = {978-1-4244-4442-7},
	url = {http://ieeexplore.ieee.org/document/5457461/},
	doi = {10/fv4tmg},
	shorttitle = {What are they doing?},
	eventtitle = {2009 {IEEE} 12th International Conference on Computer Vision Workshops, {ICCV} Workshops},
	pages = {1282--1289},
	booktitle = {2009 {IEEE} 12th International Conference on Computer Vision Workshops, {ICCV} Workshops},
	publisher = {{IEEE}},
	author = {{Wongun Choi} and Shahid, Khuram and Savarese, Silvio},
	urldate = {2024-05-02},
	date = {2009-09},
}

@inproceedings{yalesongTVSumSummarizingWeb2015,
	location = {Boston, {MA}, {USA}},
	title = {{TVSum}: Summarizing web videos using titles},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7299154/},
	doi = {10/gfsj74},
	shorttitle = {{TVSum}},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {5179--5187},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {{Yale Song} and Vallmitjana, Jordi and Stent, Amanda and Jaimes, Alejandro},
	urldate = {2024-05-02},
	date = {2015-06},
}

@article{nanFeTaQAFreeformTable2022,
	title = {{FeTaQA}: Free-form Table Question Answering},
	volume = {10},
	url = {https://aclanthology.org/2022.tacl-1.3},
	doi = {10/gtsqx3},
	shorttitle = {{FeTaQA}},
	abstract = {Existing table question answering datasets contain abundant factual questions that primarily evaluate a {QA} system's comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question–answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce {FeTaQA}, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs. {FeTaQA} is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based {QA} systems and an end-to-end method based on large pretrained text generation models, and show that {FeTaQA} poses a challenge for both methods.},
	pages = {35--49},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Nan, Linyong and Hsieh, Chiachun and Mao, Ziming and Lin, Xi Victoria and Verma, Neha and Zhang, Rui and Kryściński, Wojciech and Schoelkopf, Hailey and Kong, Riley and Tang, Xiangru and Mutuma, Mutethia and Rosand, Ben and Trindade, Isabel and Bandaru, Renusree and Cunningham, Jacob and Xiong, Caiming and Radev, Dragomir and Radev, Dragomir},
	editor = {Roark, Brian and Nenkova, Ani},
	urldate = {2024-05-01},
	date = {2022},
	note = {Place: Cambridge, {MA}
Publisher: {MIT} Press},
}

@online{yeSelFeeIterativeSelfRevising2023,
	title = {{SelFee}: Iterative Self-Revising {LLM} Empowered by Self-Feedback Generation},
	url = {https://kaistai.github.io/SelFee/},
	titleaddon = {{LK} Lab},
	author = {Ye, Seonghyeon and Jo, Yongrae and Kim, Doyoung and Kim, Sungdong and Hwang, Hyeonbin and Seo, Minjoon},
	urldate = {2024-05-01},
	date = {2023-05-31},
}

@inproceedings{liuAsgardPortableArchitecture2013,
	location = {Vancouver, {BC}, Canada},
	title = {Asgard: A portable architecture for multilingual dialogue systems},
	isbn = {978-1-4799-0356-6},
	url = {http://ieeexplore.ieee.org/document/6639301/},
	doi = {10/gtsqxn},
	shorttitle = {Asgard},
	eventtitle = {{ICASSP} 2013 - 2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {8386--8390},
	booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	publisher = {{IEEE}},
	author = {Liu, Jingjing and Pasupat, Panupong and Cyphers, Scott and Glass, Jim},
	urldate = {2024-05-01},
	date = {2013-05},
}

@inproceedings{janinICSIMeetingCorpus2003,
	location = {Hong Kong, China},
	title = {The {ICSI} Meeting Corpus},
	volume = {1},
	isbn = {978-0-7803-7663-2},
	url = {http://ieeexplore.ieee.org/document/1198793/},
	doi = {10/d7sbd8},
	eventtitle = {International Conference on Acoustics, Speech and Signal Processing ({ICASSP}'03)},
	pages = {I--364--I--367},
	booktitle = {2003 {IEEE} International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. ({ICASSP} '03).},
	publisher = {{IEEE}},
	author = {Janin, A. and Baron, D. and Edwards, J. and Ellis, D. and Gelbart, D. and Morgan, N. and Peskin, B. and Pfau, T. and Shriberg, E. and Stolcke, A. and Wooters, C.},
	urldate = {2024-05-01},
	date = {2003},
}

@inproceedings{parikhToTToControlledTableToText2020,
	location = {Online},
	title = {{ToTTo}: A Controlled Table-To-Text Generation Dataset},
	url = {https://aclanthology.org/2020.emnlp-main.89},
	doi = {10/gm3nmg},
	shorttitle = {{ToTTo}},
	abstract = {We present {ToTTo}, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.},
	eventtitle = {{EMNLP} 2020},
	pages = {1173--1186},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Parikh, Ankur and Wang, Xuezhi and Gehrmann, Sebastian and Faruqui, Manaal and Dhingra, Bhuwan and Yang, Diyi and Das, Dipanjan},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-05-01},
	date = {2020-11},
}

@inproceedings{chenHybridQADatasetMultiHop2020,
	location = {Online},
	title = {{HybridQA}: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data},
	url = {https://aclanthology.org/2020.findings-emnlp.91},
	doi = {10/gpmd4x},
	shorttitle = {{HybridQA}},
	abstract = {Existing question answering datasets focus on dealing with homogeneous information, based either only on text or {KB}/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present {HybridQA}, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the {EM} scores obtained by two baselines are below 20\%, while the hybrid model can achieve an {EM} over 40\%. This gap suggests the necessity to aggregate heterogeneous information in {HybridQA}. However, the hybrid model's score is still far behind human performance. Hence, {HybridQA} can serve as a challenging benchmark to study question answering with heterogeneous information.},
	eventtitle = {Findings 2020},
	pages = {1026--1036},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Wenhu and Zha, Hanwen and Chen, Zhiyu and Xiong, Wenhan and Wang, Hong and Wang, William Yang},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-05-01},
	date = {2020-11},
}

@inproceedings{martinMuDoCoCorpusMultidomain2020,
	location = {Marseille, France},
	title = {{MuDoCo}: Corpus for Multidomain Coreference Resolution and Referring Expression Generation},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.13},
	shorttitle = {{MuDoCo}},
	abstract = {This paper proposes a new dataset, {MuDoCo}, composed of authored dialogs between a fictional user and a system who are given tasks to perform within six task domains. These dialogs are given rich linguistic annotations by expert linguists for several types of reference mentions and named entity mentions, either of which can span multiple words, as well as for coreference links between mentions. The dialogs sometimes cross and blend domains, and the users exhibit complex task switching behavior such as re-initiating a previous task in the dialog by referencing the entities within it. The dataset contains a total of 8,429 dialogs with an average of 5.36 turns per dialog. We are releasing this dataset to encourage research in the field of coreference resolution, referring expression generation and identification within realistic, deep dialogs involving multiple domains. To demonstrate its utility, we also propose two baseline models for the downstream tasks: coreference resolution and referring expression generation.},
	eventtitle = {{LREC} 2020},
	pages = {104--111},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Martin, Scott and Poddar, Shivani and Upasani, Kartikeya},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-05-01},
	date = {2020-05},
}

@inproceedings{fabbriConvoSummConversationSummarization2021,
	location = {Online},
	title = {{ConvoSumm}: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining},
	url = {https://aclanthology.org/2021.acl-long.535},
	doi = {10/gmf9qs},
	shorttitle = {{ConvoSumm}},
	abstract = {While online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues–viewpoints–assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. To create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. Furthermore, we incorporate argument mining through graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {6866--6880},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Fabbri, Alexander and Rahman, Faiaz and Rizvi, Imad and Wang, Borui and Li, Haoran and Mehdad, Yashar and Radev, Dragomir},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	urldate = {2024-05-01},
	date = {2021-08},
}

@inproceedings{liuDuRecDialBilingualParallel2021,
	location = {Online and Punta Cana, Dominican Republic},
	title = {{DuRecDial} 2.0: A Bilingual Parallel Corpus for Conversational Recommendation},
	url = {https://aclanthology.org/2021.emnlp-main.356},
	doi = {10/gtsqxr},
	shorttitle = {{DuRecDial} 2.0},
	abstract = {In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset ({DuRecDial} 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between {DuRecDial} 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in {DuRecDial} 2.0 is annotated in two languages, both English and Chinese, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on {DuRecDial} 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of {DuRecDial} 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.},
	eventtitle = {{EMNLP} 2021},
	pages = {4335--4347},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Zeming and Wang, Haifeng and Niu, Zheng-Yu and Wu, Hua and Che, Wanxiang},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	urldate = {2024-05-01},
	date = {2021-11},
}

@inproceedings{chenActionBasedConversationsDataset2021,
	location = {Online},
	title = {Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems},
	url = {https://aclanthology.org/2021.naacl-main.239},
	doi = {10/gtsqxt},
	shorttitle = {Action-Based Conversations Dataset},
	abstract = {Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset ({ABCD}), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8\% absolute accuracy) still exists to reach human-level performance on {ABCD}.},
	eventtitle = {{NAACL}-{HLT} 2021},
	pages = {3002--3017},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Derek and Chen, Howard and Yang, Yi and Lin, Alexander and Yu, Zhou},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	urldate = {2024-05-01},
	date = {2021-06},
}

@inproceedings{liMTOPComprehensiveMultilingual2021,
	location = {Online},
	title = {{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark},
	url = {https://aclanthology.org/2021.eacl-main.257},
	doi = {10/gtsqxq},
	shorttitle = {{MTOP}},
	abstract = {Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called {MTOP}, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.},
	eventtitle = {{EACL} 2021},
	pages = {2950--2962},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher = {Association for Computational Linguistics},
	author = {Li, Haoran and Arora, Abhinav and Chen, Shuohui and Gupta, Anchit and Gupta, Sonal and Mehdad, Yashar},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	urldate = {2024-05-01},
	date = {2021-04},
}

@inproceedings{chenDialogSumRealLifeScenario2021,
	location = {Online},
	title = {{DialogSum}: A Real-Life Scenario Dialogue Summarization Dataset},
	url = {https://aclanthology.org/2021.findings-acl.449},
	doi = {10/gtsqxs},
	shorttitle = {{DialogSum}},
	eventtitle = {Findings 2021},
	pages = {5062--5074},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Yulong and Liu, Yang and Chen, Liang and Zhang, Yue},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	urldate = {2024-05-01},
	date = {2021-08},
}

@inproceedings{chiuSalesBotTransitioningChitChat2022,
	location = {Dublin, Ireland},
	title = {{SalesBot}: Transitioning from Chit-Chat to Task-Oriented Dialogues},
	url = {https://aclanthology.org/2022.acl-long.425},
	doi = {10/gtsqxw},
	shorttitle = {{SalesBot}},
	abstract = {Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a movie on Friday night, playing a song. These two directions have been studied separately due to their different purposes. However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios. Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction. To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful open-domain dialogue generation model can be easily leveraged. The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality, showing that our released data has a great potential of guiding future research directions and commercial activities. Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches.},
	eventtitle = {{ACL} 2022},
	pages = {6143--6158},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Chiu, Ssu and Li, Maolin and Lin, Yen-Ting and Chen, Yun-Nung},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	urldate = {2024-05-01},
	date = {2022-05},
}

@inproceedings{komeiliInternetAugmentedDialogueGeneration2022,
	location = {Dublin, Ireland},
	title = {Internet-Augmented Dialogue Generation},
	url = {https://aclanthology.org/2022.acl-long.579},
	doi = {10/gr75db},
	abstract = {The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or {FAISS}-based retrieval (Lewis et al., 2020b).},
	eventtitle = {{ACL} 2022},
	pages = {8460--8478},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	urldate = {2024-05-01},
	date = {2022-05},
}

@inproceedings{chawlaCaSiNoCorpusCampsite2021,
	location = {Online},
	title = {{CaSiNo}: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems},
	url = {https://aclanthology.org/2021.naacl-main.254},
	doi = {10/gtsqxv},
	shorttitle = {{CaSiNo}},
	abstract = {Automated systems that negotiate with humans have broad applications in pedagogy and conversational {AI}. To advance the development of practical negotiation systems, we present {CaSiNo}: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/{CaSiNo}},
	eventtitle = {{NAACL}-{HLT} 2021},
	pages = {3167--3185},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Chawla, Kushal and Ramirez, Jaysa and Clever, Rene and Lucas, Gale and May, Jonathan and Gratch, Jonathan},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	urldate = {2024-05-01},
	date = {2021-06},
}

@inproceedings{chenSummScreenDatasetAbstractive2022,
	location = {Dublin, Ireland},
	title = {{SummScreen}: A Dataset for Abstractive Screenplay Summarization},
	url = {https://aclanthology.org/2022.acl-long.589},
	doi = {10/gtsqxx},
	shorttitle = {{SummScreen}},
	abstract = {We introduce {SummScreen}, a summarization dataset comprised of pairs of {TV} series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, {TV} scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to {TV} series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions.},
	eventtitle = {{ACL} 2022},
	pages = {8602--8615},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Mingda and Chu, Zewei and Wiseman, Sam and Gimpel, Kevin},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	urldate = {2024-05-01},
	date = {2022-05},
}

@inproceedings{larsonEvaluationDatasetIntent2019,
	location = {Hong Kong, China},
	title = {An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction},
	url = {https://aclanthology.org/D19-1131},
	doi = {10/gqrfvv},
	abstract = {Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope—i.e., queries that do not fall into any of the system's supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.},
	eventtitle = {{EMNLP}-{IJCNLP} 2019},
	pages = {1311--1316},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Larson, Stefan and Mahendran, Anish and Peper, Joseph J. and Clarke, Christopher and Lee, Andrew and Hill, Parker and Kummerfeld, Jonathan K. and Leach, Kevin and Laurenzano, Michael A. and Tang, Lingjia and Mars, Jason},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	urldate = {2024-05-01},
	date = {2019-11},
}

@inproceedings{mukherjeeECTSumNewBenchmark2022,
	location = {Abu Dhabi, United Arab Emirates},
	title = {{ECTSum}: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts},
	url = {https://aclanthology.org/2022.emnlp-main.748},
	doi = {10/gtsqxz},
	shorttitle = {{ECTSum}},
	abstract = {Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial documents, discussing facts and figures, have largely been unexplored, majorly due to the unavailability of suitable datasets. In this work, we present {ECTSum}, a new dataset with transcripts of earnings calls ({ECTs}), hosted by publicly traded companies, as documents, and experts-written short telegram-style bullet point summaries derived from corresponding Reuters articles. {ECTs} are long unstructured documents without any prescribed length limit or format. We benchmark our dataset with state-of-the-art summarization methods across various metrics evaluating the content quality and factual consistency of the generated summaries. Finally, we present a simple yet effective approach, {ECT}-{BPS}, to generate a set of bullet points that precisely capture the important facts discussed in the calls.},
	eventtitle = {{EMNLP} 2022},
	pages = {10893--10906},
	booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Mukherjee, Rajdeep and Bohra, Abhinav and Banerjee, Akash and Sharma, Soumya and Hegde, Manjunath and Shaikh, Afreen and Shrivastava, Shivani and Dasgupta, Koustuv and Ganguly, Niloy and Ghosh, Saptarshi and Goyal, Pawan},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	urldate = {2024-05-01},
	date = {2022-12},
}

@inproceedings{qianDatabaseSearchResults2022,
	location = {Seattle, United States},
	title = {Database Search Results Disambiguation for Task-Oriented Dialog Systems},
	url = {https://aclanthology.org/2022.naacl-main.85},
	doi = {10/gtsqx2},
	abstract = {As task-oriented dialog systems are becoming increasingly popular in our lives, more realistic tasks have been proposed and explored. However, new practical challenges arise. For instance, current dialog systems cannot effectively handle multiplesearch results when querying a database, due to the lack of such scenarios in existing public datasets. In this paper, we propose Database Search Result ({DSR}) Disambiguation, a novel task that focuses on disambiguating database search results, which enhances user experience by allowing them to choose from multiple options instead of just one. To study this task, we augment the popular task-oriented dialog datasets ({MultiWOZ} and {SGD}) with turns that resolve ambiguities by (a) synthetically generating turns through a pre-defined grammar, and (b) collecting human paraphrases for a subset. We find that training on our augmented dialog data improves the model's ability to deal with ambiguous scenarios, without sacrificing performance on unmodified turns. Furthermore, pre-fine tuning and multi-task learning help our model to improve performance on {DSR}-disambiguation even in the absence of in-domain data, suggesting that it can be learned as a universal dialog skill. Our data and code will be made publicly available.},
	eventtitle = {{NAACL}-{HLT} 2022},
	pages = {1158--1173},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Qian, Kun and Kottur, Satwik and Beirami, Ahmad and Shayandeh, Shahin and Crook, Paul and Geramifard, Alborz and Yu, Zhou and Sankar, Chinnadhurai},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	urldate = {2024-05-01},
	date = {2022-07},
}

@inproceedings{quanGECOREndtoEndGenerative2019,
	location = {Hong Kong, China},
	title = {{GECOR}: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue},
	url = {https://aclanthology.org/D19-1462},
	doi = {10/gk3btq},
	shorttitle = {{GECOR}},
	abstract = {Ellipsis and co-reference are common and ubiquitous especially in multi-turn dialogues. In this paper, we treat the resolution of ellipsis and co-reference in dialogue as a problem of generating omitted or referred expressions from the dialogue context. We therefore propose a unified end-to-end Generative Ellipsis and {CO}-reference Resolution model ({GECOR}) in the context of dialogue. The model can generate a new pragmatically complete user utterance by alternating the generation and copy mode for each user utterance. A multi-task learning framework is further proposed to integrate the {GECOR} into an end-to-end task-oriented dialogue. In order to train both the {GECOR} and the multi-task learning framework, we manually construct a new dataset on the basis of the public dataset {CamRest}676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the {GECOR} model significantly outperforms the sequence-to-sequence (seq2seq) baseline model in terms of {EM}, {BLEU} and F1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with {GECOR} achieves a higher success rate of task completion than {TSCP}, a state-of-the-art end-to-end task-oriented dialogue model.},
	eventtitle = {{EMNLP}-{IJCNLP} 2019},
	pages = {4547--4557},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Quan, Jun and Xiong, Deyi and Webber, Bonnie and Hu, Changjian},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	urldate = {2024-05-01},
	date = {2019-11},
}

@inproceedings{guptaMMQAMultidomainMultilingual2018,
	location = {Miyazaki, Japan},
	title = {{MMQA}: A Multi-domain Multi-lingual Question-Answering Framework for English and Hindi},
	url = {https://aclanthology.org/L18-1440},
	shorttitle = {{MMQA}},
	eventtitle = {{LREC} 2018},
	booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Gupta, Deepak and Kumari, Surabhi and Ekbal, Asif and Bhattacharyya, Pushpak},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
	urldate = {2024-05-01},
	date = {2018-05},
}

@inproceedings{peskovMultiDomainGoalOrientedDialogues2019,
	location = {Hong Kong, China},
	title = {Multi-Domain Goal-Oriented Dialogues ({MultiDoGO}): Strategies toward Curating and Annotating Large Scale Dialogue Data},
	url = {https://aclanthology.org/D19-1460},
	doi = {10/gkr9hj},
	shorttitle = {Multi-Domain Goal-Oriented Dialogues ({MultiDoGO})},
	abstract = {The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as virtual assistants become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, linguistic diversity, domain coverage, or annotation granularity. In this paper, we present strategies toward curating and annotating large scale goal oriented dialogue data. We introduce the {MultiDoGO} dataset to overcome these limitations. With a total of over 81K dialogues harvested across six domains, {MultiDoGO} is over 8 times the size of {MultiWOZ}, the other largest comparable dialogue dataset currently available to the public. Over 54K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the “customer”) is paired with a trained annotator (the “agent”). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our strategies on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for classification on the agent and customer utterances as well as slot labeling for each domain.},
	eventtitle = {{EMNLP}-{IJCNLP} 2019},
	pages = {4526--4536},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Peskov, Denis and Clarke, Nancy and Krone, Jason and Fodor, Brigi and Zhang, Yi and Youssef, Adel and Diab, Mona},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	urldate = {2024-05-01},
	date = {2019-11},
}

@inproceedings{hemphillATISSpokenLanguage1990,
	title = {The {ATIS} Spoken Language Systems Pilot Corpus},
	url = {https://aclanthology.org/H90-1021},
	doi = {10/cz3442},
	eventtitle = {{HLT} 1990},
	booktitle = {Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990},
	author = {Hemphill, Charles T. and Godfrey, John J. and Doddington, George R.},
	urldate = {2024-05-01},
	date = {1990},
}

@inproceedings{talmorWebKnowledgeBaseAnswering2018,
	location = {New Orleans, Louisiana},
	title = {The Web as a Knowledge-Base for Answering Complex Questions},
	url = {https://aclanthology.org/N18-1059},
	doi = {10/gkz2k6},
	abstract = {Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, {ComplexWebQuestions}, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.},
	eventtitle = {{NAACL}-{HLT} 2018},
	pages = {641--651},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Talmor, Alon and Berant, Jonathan},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	urldate = {2024-05-01},
	date = {2018-06},
}

@inproceedings{pasupatCompositionalSemanticParsing2015,
	location = {Beijing, China},
	title = {Compositional Semantic Parsing on Semi-Structured Tables},
	url = {https://aclanthology.org/P15-1142},
	doi = {10/gfz98s},
	eventtitle = {{ACL}-{IJCNLP} 2015},
	pages = {1470--1480},
	booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Pasupat, Panupong and Liang, Percy},
	editor = {Zong, Chengqing and Strube, Michael},
	urldate = {2024-05-01},
	date = {2015-07},
}

@inproceedings{shangUnsupervisedAbstractiveMeeting2018,
	location = {Melbourne, Australia},
	title = {Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization},
	url = {https://aclanthology.org/P18-1062},
	doi = {10/gm8jvb},
	abstract = {We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to {NLP} to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the {AMI} and {ICSI} corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.},
	eventtitle = {{ACL} 2018},
	pages = {664--674},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Shang, Guokan and Ding, Wensi and Zhang, Zekun and Tixier, Antoine and Meladianos, Polykarpos and Vazirgiannis, Michalis and Lorré, Jean-Pierre},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	urldate = {2024-05-01},
	date = {2018-07},
}

@inproceedings{yihValueSemanticParse2016,
	location = {Berlin, Germany},
	title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
	url = {https://aclanthology.org/P16-2033},
	doi = {10/gkz3hq},
	eventtitle = {{ACL} 2016},
	pages = {201--206},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
	editor = {Erk, Katrin and Smith, Noah A.},
	urldate = {2024-05-01},
	date = {2016-08},
}

@inproceedings{elasriFramesCorpusAdding2017,
	location = {Saarbrücken, Germany},
	title = {Frames: a corpus for adding memory to goal-oriented dialogue systems},
	url = {https://aclanthology.org/W17-5526},
	doi = {10/gtsqx4},
	shorttitle = {Frames},
	abstract = {This paper proposes a new dataset, Frames, composed of 1369 human-human dialogues with an average of 15 turns per dialogue. This corpus contains goal-oriented dialogues between users who are given some constraints to book a trip and assistants who search a database to find appropriate trips. The users exhibit complex decision-making behaviour which involve comparing trips, exploring different options, and selecting among the trips that were discussed during the dialogue. To drive research on dialogue systems towards handling such behaviour, we have annotated and released the dataset and we propose in this paper a task called frame tracking. This task consists of keeping track of different semantic frames throughout each dialogue. We propose a rule-based baseline and analyse the frame tracking task through this baseline.},
	eventtitle = {{SIGDIAL} 2017},
	pages = {207--219},
	booktitle = {Proceedings of the 18th Annual {SIGdial} Meeting on Discourse and Dialogue},
	publisher = {Association for Computational Linguistics},
	author = {El Asri, Layla and Schulz, Hannes and Sharma, Shikhar and Zumer, Jeremie and Harris, Justin and Fine, Emery and Mehrotra, Rahul and Suleman, Kaheer},
	editor = {Jokinen, Kristiina and Stede, Manfred and {DeVault}, David and Louis, Annie},
	urldate = {2024-05-01},
	date = {2017-08},
}

@inproceedings{moonOpenDialKGExplainableConversational2019,
	location = {Florence, Italy},
	title = {{OpenDialKG}: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs},
	url = {https://aclanthology.org/P19-1081},
	doi = {10/ggt4wm},
	shorttitle = {{OpenDialKG}},
	abstract = {We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph ({KG}) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog {\textbackslash}textless-{\textbackslash}textgreater {KG} parallel corpus called {OpenDialKG}, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale {KG} with 1M+ facts. We then propose the {DialKG} Walker model that learns the symbolic transitions of dialog contexts as structured traversals over {KG}, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a {KG} walk path for each entity retrieved, providing a natural way to explain conversational reasoning.},
	eventtitle = {{ACL} 2019},
	pages = {845--854},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Moon, Seungwhan and Shah, Pararth and Kumar, Anuj and Subba, Rajen},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	urldate = {2024-05-01},
	date = {2019-07},
}

@inproceedings{yuSParCCrossDomainSemantic2019,
	location = {Florence, Italy},
	title = {{SParC}: Cross-Domain Semantic Parsing in Context},
	url = {https://aclanthology.org/P19-1443},
	doi = {10/gj4fwd},
	shorttitle = {{SParC}},
	abstract = {We present {SParC}, a dataset for cross-{domainSemanticParsing} {inContext} that consists of 4,298 coherent question sequences (12k+ individual questions annotated with {SQL} queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of {SParC} and show that it introduces new challenges compared to existing datasets. {SParC} demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-{SQL} models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2\% over all questions and less than10\% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc.},
	eventtitle = {{ACL} 2019},
	pages = {4511--4523},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Tao and Zhang, Rui and Yasunaga, Michihiro and Tan, Yi Chern and Lin, Xi Victoria and Li, Suyi and Er, Heyang and Li, Irene and Pang, Bo and Chen, Tao and Ji, Emily and Dixit, Shreya and Proctor, David and Shim, Sungrok and Kraft, Jonathan and Zhang, Vincent and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	urldate = {2024-05-01},
	date = {2019-07},
}
