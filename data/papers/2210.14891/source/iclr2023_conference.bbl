\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[dam(2017)]{dampe2017direct}
Direct detection of a break in the teraelectronvolt cosmic-ray spectrum of
  electrons and positrons.
\newblock \emph{Nature}, 552\penalty0 (7683):\penalty0 63--66, nov 2017.
\newblock \doi{10.1038/nature24475}.
\newblock URL \url{https://doi.org/10.1038%2Fnature24475}.

\bibitem[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and
  Sedghi]{abnar2021exploring}
Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi.
\newblock Exploring the limits of large scale pre-training, 2021.

\bibitem[Ainslie et~al.(2023)Ainslie, Lei, de~Jong, Onta침칩n, Brahma,
  Zemlyanskiy, Uthus, Guo, Lee-Thorp, Tay, Sung, and Sanghai]{ainslie2023colt5}
Joshua Ainslie, Tao Lei, Michiel de~Jong, Santiago Onta침칩n, Siddhartha
  Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi~Tay,
  Yun-Hsuan Sung, and Sumit Sanghai.
\newblock Colt5: Faster long-range transformers with conditional computation,
  2023.

\bibitem[Alabdulmohsin et~al.(2022)Alabdulmohsin, Neyshabur, and
  Zhai]{Alabdulmohsi2022revisiting}
Ibrahim Mansour~I Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock In \emph{NeurIPS 2022}, 2022.
\newblock URL \url{https://arxiv.org/abs/2209.06640}.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones,
  Joseph, Mann, DasSarma, et~al.]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and
  Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain,
  Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Bansal et~al.(2022)Bansal, Ghorbani, Garg, Zhang, Cherry, Neyshabur,
  and Firat]{bansal2022data}
Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Colin Cherry, Behnam
  Neyshabur, and Orhan Firat.
\newblock Data scaling laws in nmt: The effect of noise and architecture.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1466--1482. PMLR, 2022.

\bibitem[Batzner et~al.(2022)Batzner, Musaelian, Sun, Geiger, Mailoa,
  Kornbluth, Molinari, Smidt, and Kozinsky]{Batzner_2022}
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan~P. Mailoa,
  Mordechai Kornbluth, Nicola Molinari, Tess~E. Smidt, and Boris Kozinsky.
\newblock E(3)-equivariant graph neural networks for data-efficient and
  accurate interatomic potentials.
\newblock \emph{Nature Communications}, 13\penalty0 (1), may 2022.
\newblock \doi{10.1038/s41467-022-29939-5}.
\newblock URL \url{https://doi.org/10.1038%2Fs41467-022-29939-5}.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, Van Den~Driessche, Lespiau, Damoc, Clark,
  et~al.]{borgeaud2022improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
  Rutherford, Katie Millican, George~Bm Van Den~Driessche, Jean-Baptiste
  Lespiau, Bogdan Damoc, Aidan Clark, et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International conference on machine learning}, pp.\
  2206--2240. PMLR, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Clark et~al.(2022)Clark, De~Las~Casas, Guy, Mensch, Paganini,
  Hoffmann, Damoc, Hechtman, Cai, Borgeaud, et~al.]{clark2022unified}
Aidan Clark, Diego De~Las~Casas, Aurelia Guy, Arthur Mensch, Michela Paganini,
  Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian
  Borgeaud, et~al.
\newblock Unified scaling laws for routed language models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4057--4086. PMLR, 2022.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2056. PMLR, 2020.

\bibitem[Cobbe et~al.(2021{\natexlab{a}})Cobbe, Kosaraju, Bavarian, Chen, Jun,
  Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021{\natexlab{a}}.

\bibitem[Cobbe et~al.(2021{\natexlab{b}})Cobbe, Hilton, Klimov, and
  Schulman]{cobbe2021phasic}
Karl~W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman.
\newblock Phasic policy gradient.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2020--2027. PMLR, 2021{\natexlab{b}}.

\bibitem[Cortes et~al.(1994)Cortes, Jackel, Solla, Vapnik, and
  Denker]{cortes1994learning}
Corinna Cortes, Lawrence~D Jackel, Sara~A Solla, Vladimir Vapnik, and John~S
  Denker.
\newblock Learning curves: Asymptotic values and rate of convergence.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  327--334, 1994.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dettmers \& Zettlemoyer(2022)Dettmers and
  Zettlemoyer]{dettmers2022case}
Tim Dettmers and Luke Zettlemoyer.
\newblock The case for 4-bit precision: k-bit inference scaling laws.
\newblock \emph{arXiv preprint arXiv:2212.09720}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Evans et~al.(2021)Evans, Cotton-Barratt, Finnveden, Bales, Balwit,
  Wills, Righetti, and Saunders]{evans2021truthful}
Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit,
  Peter Wills, Luca Righetti, and William Saunders.
\newblock Truthful ai: Developing and governing ai that does not lie.
\newblock \emph{arXiv preprint arXiv:2110.06674}, 2021.

\bibitem[Fei-Fei et~al.(2004)Fei-Fei, Fergus, and Perona]{fei2004learning}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In \emph{2004 conference on computer vision and pattern recognition
  workshop}, pp.\  178--178. IEEE, 2004.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{frantar2023massive}
Elias Frantar and Dan Alistarh.
\newblock Massive language models can be accurately pruned in one-shot.
\newblock \emph{arXiv preprint arXiv:2301.00774}, 2023.

\bibitem[Ganguli et~al.(2022)Ganguli, Hernandez, Lovitt, Askell, Bai, Chen,
  Conerly, Dassarma, Drain, Elhage, et~al.]{ganguli2022predictability}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna
  Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et~al.
\newblock Predictability and surprise in large generative models.
\newblock In \emph{2022 ACM Conference on Fairness, Accountability, and
  Transparency}, pp.\  1747--1764, 2022.

\bibitem[Hendrycks \& Mazeika(2022)Hendrycks and Mazeika]{hendrycks2022x}
Dan Hendrycks and Mantas Mazeika.
\newblock X-risk analysis for ai research.
\newblock \emph{arXiv preprint arXiv:2206.05862}, 2022.

\bibitem[{Henighan} et~al.(2020){Henighan}, {Kaplan}, {Katz}, {Chen}, {Hesse},
  {Jackson}, {Jun}, {Brown}, {Dhariwal}, {Gray}, {Hallacy}, {Mann}, {Radford},
  {Ramesh}, {Ryder}, {Ziegler}, {Schulman}, {Amodei}, and
  {McCandlish}]{2020arXiv201014701H}
Tom {Henighan}, Jared {Kaplan}, Mor {Katz}, Mark {Chen}, Christopher {Hesse},
  Jacob {Jackson}, Heewoo {Jun}, Tom~B. {Brown}, Prafulla {Dhariwal}, Scott
  {Gray}, Chris {Hallacy}, Benjamin {Mann}, Alec {Radford}, Aditya {Ramesh},
  Nick {Ryder}, Daniel~M. {Ziegler}, John {Schulman}, Dario {Amodei}, and Sam
  {McCandlish}.
\newblock {Scaling Laws for Autoregressive Generative Modeling}.
\newblock \emph{arXiv e-prints}, art. arXiv:2010.14701, October 2020.

\bibitem[{Hernandez} et~al.(2021){Hernandez}, {Kaplan}, {Henighan}, and
  {McCandlish}]{2021arXiv210201293H}
Danny {Hernandez}, Jared {Kaplan}, Tom {Henighan}, and Sam {McCandlish}.
\newblock {Scaling Laws for Transfer}.
\newblock \emph{arXiv e-prints}, art. arXiv:2102.01293, February 2021.

\bibitem[{Hestness} et~al.(2017){Hestness}, {Narang}, {Ardalani}, {Diamos},
  {Jun}, {Kianinejad}, {Patwary}, {Yang}, and {Zhou}]{2017arXiv171200409H}
Joel {Hestness}, Sharan {Narang}, Newsha {Ardalani}, Gregory {Diamos}, Heewoo
  {Jun}, Hassan {Kianinejad}, Md. Mostofa~Ali {Patwary}, Yang {Yang}, and Yanqi
  {Zhou}.
\newblock {Deep Learning Scaling is Predictable, Empirically}.
\newblock \emph{arXiv e-prints}, art. arXiv:1712.00409, December 2017.

\bibitem[Hilton et~al.(2023)Hilton, Tang, and Schulman]{hilton2023scaling}
Jacob Hilton, Jie Tang, and John Schulman.
\newblock Scaling laws for single-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2301.13442}, 2023.

\bibitem[Hunter(2007)]{Hunter:2007}
J.~D. Hunter.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0
  90--95, 2007.
\newblock \doi{10.1109/MCSE.2007.55}.

\bibitem[Jones(2021)]{jones2021scaling}
Andy~L Jones.
\newblock Scaling scaling laws with board games.
\newblock \emph{arXiv preprint arXiv:2104.03113}, 2021.

\bibitem[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain,
  Perez, Schiefer, Dodds, DasSarma, Tran-Johnson, et~al.]{kadavath2022language}
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan
  Perez, Nicholas Schiefer, Zac~Hatfield Dodds, Nova DasSarma, Eli
  Tran-Johnson, et~al.
\newblock Language models (mostly) know what they know.
\newblock \emph{arXiv preprint arXiv:2207.05221}, 2022.

\bibitem[{Kaplan} et~al.(2020){Kaplan}, {McCandlish}, {Henighan}, {Brown},
  {Chess}, {Child}, {Gray}, {Radford}, {Wu}, and
  {Amodei}]{icm2020arXiv200108361K}
Jared {Kaplan}, Sam {McCandlish}, Tom {Henighan}, Tom~B. {Brown}, Benjamin
  {Chess}, Rewon {Child}, Scott {Gray}, Alec {Radford}, Jeffrey {Wu}, and Dario
  {Amodei}.
\newblock {Scaling Laws for Neural Language Models}.
\newblock \emph{arXiv e-prints}, art. arXiv:2001.08361, January 2020.

\bibitem[Karpathy(2020)]{Karpathy2020}
Andrej Karpathy.
\newblock mingpt.
\newblock \url{https://github.com/karpathy/minGPT}, 2020.

\bibitem[Ko et~al.(2023)Ko, D'souza, Nguyen, Balestriero, and
  Hooker]{ko2023fair}
Wei-Yin Ko, Daniel D'souza, Karina Nguyen, Randall Balestriero, and Sara
  Hooker.
\newblock Fair-ensemble: When fairness naturally emerges from deep ensembling.
\newblock \emph{arXiv preprint arXiv:2303.00586}, 2023.

\bibitem[Ko{\v{c}}isk{\`y} et~al.(2018)Ko{\v{c}}isk{\`y}, Schwarz, Blunsom,
  Dyer, Hermann, Melis, and Grefenstette]{kovcisky2018narrativeqa}
Tom{\'a}{\v{s}} Ko{\v{c}}isk{\`y}, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
  Karl~Moritz Hermann, G{\'a}bor Melis, and Edward Grefenstette.
\newblock The narrativeqa reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  6:\penalty0 317--328, 2018.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{European conference on computer vision}, pp.\  491--507.
  Springer, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJzIBfZAb}.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2021deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (12):\penalty0 124003, 2021.

\bibitem[Neumann \& Gros(2022)Neumann and Gros]{neumann2022scaling}
Oren Neumann and Claudius Gros.
\newblock Scaling laws for a multi-agent reinforcement learning model.
\newblock \emph{arXiv preprint arXiv:2210.00849}, 2022.

\bibitem[Nichol \& Dhariwal(2021)Nichol and Dhariwal]{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8162--8171. PMLR, 2021.

\bibitem[OpenAI(2023)]{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Cao,
  Cheng, Chung, Grella, GV, et~al.]{peng2023rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi
  Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}, 2023.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Radford et~al.(2022)Radford, Kim, Xu, Brockman, McLeavey, and
  Sutskever]{radford2022robust}
Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
  Ilya Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock \emph{arXiv preprint arXiv:2212.04356}, 2022.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv e-prints}, 2019.

\bibitem[Ramasesh et~al.(2022)Ramasesh, Lewkowycz, and
  Dyer]{ramasesh2022effect}
Vinay~Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer.
\newblock Effect of scale on catastrophic forgetting in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Rosenfeld et~al.(2019)Rosenfeld, Rosenfeld, Belinkov, and
  Shavit]{DBLP:journals/corr/abs-1909-12673}
Jonathan~S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock \emph{CoRR}, abs/1909.12673, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.12673}.

\bibitem[Rosenfeld et~al.(2021)Rosenfeld, Frankle, Carbin, and
  Shavit]{rosenfeld2021predictability}
Jonathan~S Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit.
\newblock On the predictability of pruning across scales.
\newblock \emph{arXiv preprint arXiv:2006.10621}, 2021.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shridhar et~al.(2021)Shridhar, Manuelli, and Fox]{shridhar2021cliport}
Mohit Shridhar, Lucas Manuelli, and Dieter Fox.
\newblock Cliport: What and where pathways for robotic manipulation.
\newblock In \emph{Proceedings of the 5th Conference on Robot Learning (CoRL)},
  2021.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and
  Morcos]{https://doi.org/10.48550/arxiv.2206.14486}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari~S.
  Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning, 2022.
\newblock URL \url{https://arxiv.org/abs/2206.14486}.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun2017revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  843--852, 2017.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 24261--24272, 2021.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, et~al.]{virtanen2020scipy}
Pauli Virtanen, Ralf Gommers, Travis~E Oliphant, Matt Haberland, Tyler Reddy,
  David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
  Bright, et~al.
\newblock Scipy 1.0: fundamental algorithms for scientific computing in python.
\newblock \emph{Nature methods}, 17\penalty0 (3):\penalty0 261--272, 2020.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Chi, Le,
  and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022{\natexlab{b}}.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Mita, Wah, Schroff, Belongie,
  and Perona]{welinder2010caltech}
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff,
  Serge Belongie, and Pietro Perona.
\newblock Caltech-ucsd birds 200.
\newblock 2010.

\bibitem[Xie \& Yuille(2020)Xie and Yuille]{Xie2020Intriguing}
Cihang Xie and Alan Yuille.
\newblock Intriguing properties of adversarial training at scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HyxJhCEFDS}.

\bibitem[Zeng et~al.(2021)Zeng, Florence, Tompson, Welker, Chien, Attarian,
  Armstrong, Krasin, Duong, Sindhwani, et~al.]{zeng2021transporter}
Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien,
  Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani,
  et~al.
\newblock Transporter networks: Rearranging the visual world for robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, pp.\  726--747. PMLR, 2021.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and
  Beyer]{DBLP:journals/corr/abs-2106-04560}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock \emph{CoRR}, abs/2106.04560, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.04560}.

\end{thebibliography}
