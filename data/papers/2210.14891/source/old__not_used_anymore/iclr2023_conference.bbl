\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[dam(2017)]{dampe2017direct}
Direct detection of a break in the teraelectronvolt cosmic-ray spectrum of
  electrons and positrons.
\newblock \emph{Nature}, 552\penalty0 (7683):\penalty0 63--66, nov 2017.
\newblock \doi{10.1038/nature24475}.
\newblock URL \url{https://doi.org/10.1038%2Fnature24475}.

\bibitem[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and
  Sedghi]{abnar2021exploring}
Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi.
\newblock Exploring the limits of large scale pre-training, 2021.

\bibitem[Alabdulmohsin et~al.(2022)Alabdulmohsin, Neyshabur, and
  Zhai]{Alabdulmohsi2022revisiting}
Ibrahim Mansour~I Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock In \emph{NeurIPS 2022}, 2022.
\newblock URL \url{https://arxiv.org/abs/2209.06640}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2056. PMLR, 2020.

\bibitem[Cortes et~al.(1994)Cortes, Jackel, Solla, Vapnik, and
  Denker]{cortes1994learning}
Corinna Cortes, Lawrence~D Jackel, Sara~A Solla, Vladimir Vapnik, and John~S
  Denker.
\newblock Learning curves: Asymptotic values and rate of convergence.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  327--334, 1994.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Evans et~al.(2021)Evans, Cotton-Barratt, Finnveden, Bales, Balwit,
  Wills, Righetti, and Saunders]{evans2021truthful}
Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit,
  Peter Wills, Luca Righetti, and William Saunders.
\newblock Truthful ai: Developing and governing ai that does not lie.
\newblock \emph{arXiv preprint arXiv:2110.06674}, 2021.

\bibitem[Fei-Fei et~al.(2004)Fei-Fei, Fergus, and Perona]{fei2004learning}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In \emph{2004 conference on computer vision and pattern recognition
  workshop}, pp.\  178--178. IEEE, 2004.

\bibitem[Ganguli et~al.(2022)Ganguli, Hernandez, Lovitt, Askell, Bai, Chen,
  Conerly, Dassarma, Drain, Elhage, et~al.]{ganguli2022predictability}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna
  Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et~al.
\newblock Predictability and surprise in large generative models.
\newblock In \emph{2022 ACM Conference on Fairness, Accountability, and
  Transparency}, pp.\  1747--1764, 2022.

\bibitem[Hendrycks \& Mazeika(2022)Hendrycks and Mazeika]{hendrycks2022x}
Dan Hendrycks and Mantas Mazeika.
\newblock X-risk analysis for ai research.
\newblock \emph{arXiv preprint arXiv:2206.05862}, 2022.

\bibitem[{Hernandez} et~al.(2021){Hernandez}, {Kaplan}, {Henighan}, and
  {McCandlish}]{2021arXiv210201293H}
Danny {Hernandez}, Jared {Kaplan}, Tom {Henighan}, and Sam {McCandlish}.
\newblock {Scaling Laws for Transfer}.
\newblock \emph{arXiv e-prints}, art. arXiv:2102.01293, February 2021.

\bibitem[{Hestness} et~al.(2017){Hestness}, {Narang}, {Ardalani}, {Diamos},
  {Jun}, {Kianinejad}, {Patwary}, {Yang}, and {Zhou}]{2017arXiv171200409H}
Joel {Hestness}, Sharan {Narang}, Newsha {Ardalani}, Gregory {Diamos}, Heewoo
  {Jun}, Hassan {Kianinejad}, Md. Mostofa~Ali {Patwary}, Yang {Yang}, and Yanqi
  {Zhou}.
\newblock {Deep Learning Scaling is Predictable, Empirically}.
\newblock \emph{arXiv e-prints}, art. arXiv:1712.00409, December 2017.

\bibitem[Hunter(2007)]{Hunter:2007}
J.~D. Hunter.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0
  90--95, 2007.
\newblock \doi{10.1109/MCSE.2007.55}.

\bibitem[{Kaplan} et~al.(2020){Kaplan}, {McCandlish}, {Henighan}, {Brown},
  {Chess}, {Child}, {Gray}, {Radford}, {Wu}, and
  {Amodei}]{icm2020arXiv200108361K}
Jared {Kaplan}, Sam {McCandlish}, Tom {Henighan}, Tom~B. {Brown}, Benjamin
  {Chess}, Rewon {Child}, Scott {Gray}, Alec {Radford}, Jeffrey {Wu}, and Dario
  {Amodei}.
\newblock {Scaling Laws for Neural Language Models}.
\newblock \emph{arXiv e-prints}, art. arXiv:2001.08361, January 2020.

\bibitem[Karpathy(2020)]{Karpathy2020}
Andrej Karpathy.
\newblock mingpt.
\newblock \url{https://github.com/karpathy/minGPT}, 2020.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{European conference on computer vision}, pp.\  491--507.
  Springer, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2021deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (12):\penalty0 124003, 2021.

\bibitem[Neumann \& Gros(2022)Neumann and Gros]{neumann2022scaling}
Oren Neumann and Claudius Gros.
\newblock Scaling laws for a multi-agent reinforcement learning model.
\newblock \emph{arXiv preprint arXiv:2210.00849}, 2022.

\bibitem[Nichol \& Dhariwal(2021)Nichol and Dhariwal]{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8162--8171. PMLR, 2021.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Rosenfeld et~al.(2019)Rosenfeld, Rosenfeld, Belinkov, and
  Shavit]{DBLP:journals/corr/abs-1909-12673}
Jonathan~S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock \emph{CoRR}, abs/1909.12673, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.12673}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 24261--24272, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, et~al.]{virtanen2020scipy}
Pauli Virtanen, Ralf Gommers, Travis~E Oliphant, Matt Haberland, Tyler Reddy,
  David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
  Bright, et~al.
\newblock Scipy 1.0: fundamental algorithms for scientific computing in python.
\newblock \emph{Nature methods}, 17\penalty0 (3):\penalty0 261--272, 2020.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Chi, Le,
  and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022{\natexlab{b}}.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Mita, Wah, Schroff, Belongie,
  and Perona]{welinder2010caltech}
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff,
  Serge Belongie, and Pietro Perona.
\newblock Caltech-ucsd birds 200.
\newblock 2010.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and
  Beyer]{DBLP:journals/corr/abs-2106-04560}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock \emph{CoRR}, abs/2106.04560, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.04560}.

\end{thebibliography}
