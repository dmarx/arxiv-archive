\section{Conclusions}
\noindent{\bf Summary.}

(TODO: This is just the abstract copied; and Surya's

We have presented a smoothly broken power law functional form that accurately models the scaling behaviors (of artificial neural networks) (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, or training dataset size varies) for each task from a very large and diverse set of upstream and downstream (i.e. zero-shot, prompted, and fine-tuned) tasks. These tasks include large-scale vision tasks, large-scale unsupervised language tasks, arithmetic, and reinforcement learning. This functional form yields extrapolations of scaling behavior that often are an order of magnitude more accurate than other functional forms for modeling the scaling behavior of artificial neural networks. Furthermore, this functional form accurately models many scaling behaviors that other functional are mathematically incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic.

\noindent{\bf Limitations.} The most notable limitation   

\noindent{\bf Ethical considerations.} A potential negative societal impact could be that  

\noindent{\bf Future work.}
 