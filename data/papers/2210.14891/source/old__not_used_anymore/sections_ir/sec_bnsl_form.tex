\section{Proposed Functional Form: Broken Neural Scaling Laws }
\label{sec:bnsl}
\vspace{-0.08in}
The general functional form of a broken neural scaling law (BNSL) is given as follows:
%\begin{equation}
%y = \left(\frac{a}{x}\right)^b \left(1 + %\left(\frac{x}{c}\right)^d\right)^{-f} + g
%\label{eq:UniversalScalingLaw}
%\end{equation}
% \begin{equation}
% y = \left(\frac{a}{x}\right)^b \left(1 + \left(\frac{x}{c}\right)^d\right)^{-f} \left(1 + \left(\frac{x}{c}\right)^d\right)^{-f} + g
% \label{eq:UniversalScalingLaw}
% \end{equation}
%\todo[inline]{ How to best communicate that elements of equation 1 are added and removed based on number of breaks (see below)}
\begin{equation}
\vspace{-0.07in}
y =  a + \left(\frac{b}{x}\right)^c \prod_{i=1}^n \left(1 + \left(\frac{x}{d_i}\right)^{f_i}\right)^{-g_i},
\label{eq:UniversalScalingLaw}
\vspace{-0.07in}
\end{equation}
\begin{comment}
\iffalse
When $a = 0$, applying logarithms to both sides yields:
\begin{equation}
\log (y) =  c \log (b) - c \log(x) -  \sum_{i=1}^n g_i \log\left(1 + \left(\frac{x}{d_i}\right)^{f_i}\right).
%\label{eq:UniversalScalingLaw}
\end{equation}
TODO: I (David) made the above to try and get more intuition on this.  I guess it's not actually piece-wise linear (hence the SMOOTHLY broken)?
\fi
\end{comment}
%\begin{equation}
%y =  a + b^{-c} \prod_{i=1}^n \left(1 + {d_i}{x}^{-f_i}\right)^{-g_i}
%\label{eq:UniversalScalingLaw}
%\end{equation}
%In equation \ref{eq:UniversalScalingLaw}, 
where $y$ represents the performance evaluation metric (e.g. prediction error, cross entropy, BLEU score percentage, F1 score percentage, reward, or Elo rating) and $x$ represent whatever quantity is being scaled (e.g. number of model parameters, amount of compute used for training, or training dataset size). The variables $a, b, c, d_1 ...  d_n, f_1 ... f_n, g_1 ... g_n$ are all constants that are fit via curve fitting libraries such as SciPy \cite{virtanen2020scipy}.

The variable $n$ represents the number of breaks in the functional form, for a total of $n+1$ (approximately) linear regions on a log-log plot. A ``break" is defined as a transition between one interval that forms a straight line on a log-log plot and another interval that forms a straight line on a log-log plot.
The graph of this function (in a log-log plot) is not strictly speaking piece-wise linear; instead it smoothly interpolates between the different (approximately) linear regions. TODO: say something about why that is good.
%Most of our experiments use $n=1$, but we also show that 


%TODO: explain that there are varying number of variables $d_i, f_i, g_i$ depending on the number of breaks in the functional form.

Variable $a$ represents the limit as to how far the value of $y$ (performance evaluation metric) can be reduced (or maximized) as x (whatever quantity is being scaled) goes to infinity. Variable $b$ represents the offset of functional form on a log-log plot (analogous to $b$ in $y=mx+b$ on a linear-linear plot). Variable $c$ represents the slope of the first linear region on a log-log plot. Variable $d_i$ represents where on the x-axis the break between the $(i)$th and the $(i+1)$th linear region (on a log-log plot) occurs. Variable $f_i$ represents the slope of the $(i+1)$th linear region on a log-log plot. Variable $g_i$ represents the sharpness of break between the $(i)$th and the $(i+1)$th linear region on a log-log plot.



%TODO: note that equation \ref{eq:UniversalScalingLaw} works fine for upstream scaling too; we just name paper downstream scaling to emphasize the scenarios (i.e. downstream scaling) in which all its terms become necessary.

%\be
%\label{eq:PowerLawPlusConstant}
%L(x) = L_\infty + \left( \frac{x_0}{x}  \right)^{\alpha_x} 
%\ee

% \todo[inline]{this paragraph goes elsewhere} 
% We show mathematically and empirically that previous functional forms are unable to express inflection points, and we show mathematically and empirically that our functional form is able to express inflection points.



%We show that the functional form $y=a+(b/(x+c))^d$ (equation 1) very accurately fits 75\% of downstream tasks from the GPT-3 arXiv paper.


%When the best fit of equation 1 converges to values of d much larger than 1, the functional form equation 1 seems to not be sufficient and will probably need to be replaced with a new functional form.

%We find that the functional form $y=a+(b/((1+h)^x+c))^d$ (equation 2) is capable of fitting a subset of the downstream tasks (the downstream tasks for which equation 1 converges to values of d much larger than 1). However, equation 2 is much harder to fit due to small changes in h drastically changing y.


\iffalse
\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\columnwidth]{figures/inflection_points/Two_Digit_Addition_Zero-Shot_Zoom.png}
\includegraphics[width=1.0\columnwidth]{figures/inflection_points/Two_Digit_Addition_Zero-Shot.png}
\caption{
Black points are test errors at various model sizes (numbers of parameters). Red line is functional form from equation \ref{eq:UniversalScalingLaw} fit to the black points. Left figure is zoomed in version of right figure. Task is Downstream Two Digit Addition (Zero-Shot) from \citet{brown2020language}. %\citet{2020arXiv200514165B}. 
All plots axes are \textbf{linear-linear} (not log-log).
}
\label{fig:Inflection_Point_example}
\end{figure*}
\fi


\iffalse
Our downstream forecasts show that an infinitely large (simultaneously infinitely deep and infinitely wide) GPT trained using the same 300 billion tokens as GPT-3 does not surpass human performance on any downstream task when using the few-shot in-context learning methodology from GPT-3 paper.
The one exception is fake news generation, for which our downstream forecast shows that humans would rate news articles generated by the model to be better than human written news articles 54.37\% of the time.

We perform ablations on the number of points and supremum of the points used for the fitting the scaling law functional form. These ablations provide some intuitions for how much to trust our forecasts.
\fi

