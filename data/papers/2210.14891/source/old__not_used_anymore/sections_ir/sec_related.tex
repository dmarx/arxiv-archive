\section{Related Work}
To the best of our knowledge,\citet{cortes1994learning} was the first paper to model the scaling of multilayer neural network's performance as a power law 
%(also known as a scaling law) (plus a constant) \ir{[I would delete both those phrases in ()] } 
of the form $y=ax^b + c$ in which $x$ refers to training dataset size and $y$ refers to test error.
Later, \citet{2017arXiv171200409H} showed that this functional form holds over many orders of magnitude, while \citet{DBLP:journals/corr/abs-1909-12673} demonstrated  that the same functional form applies when $x$ refers to model size (number of parameters). \citet{icm2020arXiv200108361K} brought ``neural" scaling laws to the mainstream and showed that there is an additional scaling law with respect to compute. \citet{DBLP:journals/corr/abs-2106-04560} introduced the functional form $y=a(x+d)^b + c$, where  d represents the scale at which the performance starts to improve beyond the  random guess loss (a constant) and transitions to a  power law scaling regime. \citet{abnar2021exploring} proposed a functional form that relates downstream performance to upstream performance. \citet{Alabdulmohsi2022revisiting} proposed functional form $(y - \epsilon_{\infty}) / ((\epsilon_{0} - y)^a) = bx$ for relating scale to performance and released a scaling laws benchmark dataset that we use in section 

%\todo[inline]{NEED TO TALK ABOUT HOW EQUATION 6.1 OF SCALING LAWS FOR TRANSFER (plus a trivial constant for irreducible entropy) IS MATHEMATICALLY EQUIVALENT TO BNSL}

Smoothly broken power law functional forms equivalent to equation \ref{eq:UniversalScalingLaw} are common in the astrophysics literature (e.g. \cite{dampe2017direct}), but have not been used to fit scaling data in machine learning.
%\citet{2021arXiv210201293H}  - this has full list of authors/too long/not the format used in biblio for a conference
\citet{brown2020language}
described a smoothly broken power law functional form (consisting of 5 constants after reducing redundant variables) in equation 6.1 of their paper to relate scale and downstream performance. 
This is mathematically equivalent to our BNSL with a single break; however: (i) they proposed it only in the specific context of how fine-tuning plus transfer learning scales as a function of the model size. Their functional form contains a break only with respect to the number of model parameters but not with the dataset size; (ii) they only mentioned this equation in passing and do not attempt to fit any data using this functional form; (iii) they arrived at it simply via combining the scaling law for transfer that is the focus of their work with a scaling law for pretraining data, (iv) they did not identify it as a smoothly broken power law, or note any qualitative advantages of this functional form. (v) they do not talk about the family of functional forms with multiple breaks. 

%but to the best of our knowledge have never been mentioned or utilized in any literature that involves artificial neural networks.


TODO: Maybe mention "Limits of Large-Scale Training" Function
%TODO: mention that \citet{DBLP:journals/corr/abs-1909-12673} attempts to model transition from random guessing to power law
%TODO: explain why a,b,c,d are all non-negative.
