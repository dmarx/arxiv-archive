
\section{Introduction}
The number of model parameters, amount of compute used for training, and training dataset size of the most capable artificial neural networks keeps increasing and will probably keep rapidly increasing for the foreseeable future. However, no organization currently has direct access to these larger resources of the future; and it has been empirically verified many, many times (e.g., see Figure 2 (right) of \cite{tolstikhin2021mlp}) that methods which perform best at smaller scales often are no longer the best performing methods at larger scales. To work on, identify, and steer the methods that are most probable to stand the test-of-time as these larger resources come online, one needs a way to predict how all relevant performance evaluation metrics of artificial neural networks vary in all relevant settings as scale increases. 


\todo[inline]{don't copy the abstract (exactly), paraphrase}
To address this need, we present a smoothly broken power law functional form, referred to by us as a \textbf{broken neural scaling law (BNSL)}, that accurately models the scaling behaviors of artificial neural networks for each task from a very large and diverse set of upstream and downstream (i.e. zero-shot, prompted, and fine-tuned) tasks. These tasks include large-scale vision tasks, large-scale unsupervised language tasks, arithmetic, and reinforcement learning. This functional form yields extrapolations of scaling behavior that often are an order of magnitude more accurate than previous functional forms for modeling the scaling behavior of artificial neural networks. Moreover, this functional form accurately models the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp transitions present in the scaling behavior of tasks such as arithmetic.


%One collects some experimental data in the smaller (compute, data, or number of parameters) regime, and then fits a neural scaling law functional form (e.g. via a curve fitting library such as SciPy \cite{virtanen2020scipy}.

%Currently variants of power laws branded as neural scaling laws are main means one uses to make such predictions. One collects some experimental data in the smaller (compute, data, or number of parameters) regime, and then fits a neural scaling law functional form (e.g. via a curve fitting library such as SciPy \cite{virtanen2020scipy}.

%Neural Scaling Laws as they currently are used have some issues. (TODO: this is tricky to phrase because scaling laws for transfer exists and is equivalent to ours)

\iffalse

\section{DK Introduction}
\label{sec:intro}

\todo[inline]{TODO: this needs references all over the place}


%Conventional wisdom and machine learning has long held that more data is always better, but there is a sweet spot for the number of parameters or amount of training: too much of either risks overfitting.
%For modern deep learning, however it seems that more is (almost) always better. % TODO: reference other results like double descent?  Or probably that should come later...
In the largest scale regimes of modern deep learning, it seems that increasing the number of model parameters, amount of compute used for training, or training dataset size (almost) always yields better results. % TODO: reference other results like double descent?  Or probably that should come later...
In fact, scaling up deep learning seems to yield predictable performance improvements, which can be modeled using neural scaling laws.
Remarkably, the same simple functional form, a modified power law, applies equally well to data, parameters, and compute, the three axes along which scaling is measured.
%, provided that none of the three is a bottleneck (TODO: more details on that?). Ethan: It works fine even if bottlenecked.

Currently, the main practical value of neural scaling laws is in forecasting the performance of larger models and datasets without needing to train or collect them.  
Indeed, several works have shown that models that underperform at smaller scales can be predicted to reach state of the art performance at larger scales \citep{}, although such predictions have not always held up \citep{}.
However, it is difficult to translate predictions about performance metrics such as negative log likelihood into predictions about qualitative behaviors and capabilities.
For instance, \citet{GPT} famously document the emergence of in-context meta-learning in large-scale language models.
``Downstream'' performance, i.e.\ that which is measured on metrics other than the training loss, reveals striking discontinuities as a function of scale. % TODO: note that it's not all downstream, and discuss fine-tuning vs. 0-shot
Models suddenly go from near-chance level to state-of-the-art performance on tasks such as TODO once a critical scale is reached.

%TODO: say something more here about how we crush it 

Predicting the emergence of novel capabilities at scale could prove crucial to responsibly developing and deploying increasingly advanced AI systems. % TODO: say more, e.g. mention deception or something?
Doing so requires methods of identifying and measuring relevant capabilities, and scaling laws that can accurately forecast how these capabilities develop at scale. % of modelling their learning dynamics.
However, the functional forms of scaling laws applied in prior work are not up to this challenge.

One salient defect is that they can only represent monotonic functions.
They thus fail to model the striking phenomena of double-descent \citep{}, where increased scale temporarily decreases test performance before ultimately leading to further improvements. %, and grokking \citep{}, where test performance remains near chance levels 
They also lack the expressive power to model inflection points, which can be observed empirically for many downstream tasks, and even some upstream tasks, such as our $N$-digit arithmetic task, or the modular arithmetic task introduced by \citet{grokking} in their work on ``grokking''. 
%For this task, test performance remains near chance levels long after training accuracy has reached 100\% before eventually climbing towards 100\% as well on synthetic arithmetic tasks.
%, and grokking \citep{}, where test performance remains near chance levels long after training accuracy has reached 100\% before eventually climbing towards 100\% as well on synthetic arithmetic tasks.
%Such discontinuities can sometimes even be observed on the ``upstream'' test loss; for instance, we observe them in a simple $N$-digit arithmetic task.
%Another striking phenomena of modern deep learning that 

%also fail to fit the striking phenomena of double-descent \citep{}, where increased scale temporarily decreases test performance before ultimately leading to further improvements.%, and grokking \citep{}, where test performance remains near chance levels long after training accuracy has reached 100\% before eventually climbing towards 100\% as well on synthetic arithmetic tasks.
%Modelling, predicting, and ultimately understanding these phenomena will 
Our work presents a functional form capable of modeling all of these diverse behaviors, and empirically validates it's effectiveness in extrapolating scaling behavior.
%Besides the qualitative di
%TODO: last 2 sentences above are now a bit out of place...
TODO: say something more here about how we crush it (i.e. enumerate all the scaling data we fit).

\fi

%TODO:  Say something like: "Sometimes the non-broken Scaling laws don't even work upstream when you have weird stuff like Grokking or double descent or unusual data sets like our N-digit addition".




\iffalse
\subsection{old Introduction}


%\textbf{\texttt{http://icml.cc/}}
\todo[inline]{motivation: why scaling laws?}
%Machine learning involves optimizing an upstream training objective and evaluating downstream metric(s).

%The endgame of machine learning is going to involve training models with more parameters than the human brain on the entire internet. To make claims today about what machine learning methods will be most effective during the endgame, we must rely on forecasts because no organization currently has enough compute to run these future workloads empirically. The measurement that makes the most sense to forecast is downstream task performance scaling because TODO

One of the principal goals of deep learning research is to find a functional form that characterizes the relationship between performance and scale in all settings. Such a functional form would allow one to better predict that which scales best on all the evaluations that one cares about. TODO: Revisiting NSL had pretty good list of why to care about scaling laws. In pursuit of this goal, we present some key results.

%In this paper, we focus on the 190 downstream task scaling evaluations reported in the GPT-3 arXiv paper because it reports the downstream scaling information for the largest number of tasks of any paper we are aware of.

%main result:
\ir{We demonstrate that the following functional form (also known as smoothly broken power law) is accurately capturing (and extrapolating) multiple performance metrics on a wide range of downstream tasks, in several settings
$
y =  a + \left(\frac{b}{x}\right)^c \prod_{i=1}^n \left(1 + \left(\frac{x}{d_i}\right)^{f_i}\right)^{-g_i}.
$
}

%----IR
%The endgame of machine learning is going to involve training models with more parameters than the human brain on the entire internet. To make claims today about what machine learning methods will be most effective during the endgame, we must rely on forecasts because no organization currently has enough compute to run these future workloads empirically. The measurement that makes the most sense to forecast is downstream task performance scaling because TODO

%One of the principal goals of deep learning research is to find a single functional form that characterizes the relationship between performance and scale in all settings. In pursuit of this goal, we present some key results.  %% IR: a bit controversial statement about finding a single functional form as one of the principal goals of DL.

%In this paper, we focus on the 190 downstream task scaling evaluations reported in the GPT-3 arXiv paper because it reports the downstream scaling information for the largest number of tasks of any paper we are aware of.
%----
\fi