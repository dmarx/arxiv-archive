# Introduction {#sec:intro}

As the trend towards larger machine-learning models continue, from BERT [@bert] with 340 million parameters, GPT-2 [@gpt-2] with 1.5 billion parameters, to GPT-3 [@gpt3] with 175 billion parameters, model training and inferencing have to be distributed. Moreover, as the computations become resource hungry, optimizing for even the last percentage can have huge benefits in terms of time, energy, and money savings [@gpt3cost; @strubell2019energy].

In machine learning systems today, computation and communication are treated as independent abstractions implemented in different libraries. For instance, computation libraries, such as cuBLAS [@cublas] and cuDNN [@cudnn], provide optimized tensor algebra operations, while communication libraries, like NVIDIA Collective Communications Library [@nccl], provide high-performance implementations of collective communication, such as AllReduce. Machine learning frameworks, such as PyTorch [@pytorch], call computation and communication kernels from these libraries. Thus, in machine learning applications built atop of such frameworks, the computation and communication operations are invoked separately.

While this separation allows independent optimization of computation and communication kernels, breaking this abstraction boundary can unlock new optimizations that are otherwise not feasible. These optimizations include the following. *Interface* optimization eliminates a mismatch between the caller and the callee of an abstraction. For example, a machine learning model's parameters are stored in non-contiguous buffers, one buffer per layer and hence, need to copy all buffers into a single buffer before calling a collective communication like AllReduce. This copy can be avoided if the communication operation takes a list of arrays as input instead of requiring a single buffer. *Fusion* optimization decreases memory bandwidth usage by generating a single kernel to perform multiple communication and computation operations. *Reorder* optimization moves the computation before or after the communication, thereby either distributing the computation or enabling new fusion possibilities. Finally, *overlapping* optimization orchestrates multiple computation and communication operations in a fine-grained manner to fully utilize both network and compute resources. We elaborate on this possibility below.

![Speedup of co-optimized overlapping over sequential MatMul and AllReduce(for model parallel GPT-2 Model input matrix of \[B$\times$1024, 768\] and weights of \[768, 3072\]) on 16 Tesla V100 GPUs. [\[fig:matmul-overlap-intro\]]{#fig:matmul-overlap-intro label="fig:matmul-overlap-intro"}](figures/matmul-overlap-with-streams-intro_small.pdf){#fig:matmul-overlap-intro width="\\linewidth"}

In model parallelism, which is one of the distributed machine learning approaches, each layer is distributed across multiple GPUs [@megatronlm] and the computation for each layer consists of a matrix multiplication (MatMul) on each node followed by an AllReduce. The existing implementation of model parallelism calls individually optimized library functions for MatMul and AllReduce. However, the implementation cannot utilize both network and computation resources simultaneously because the network is idle during MatMul. We can completely utilize both network and computation resources simultaneously by overlapping the computation of MatMul with the communication of AllReduce in a fine-grained manner. The idea is to slice the output into smaller chunks and start the AllReduce communication on a chunk as soon as the MatMul kernel has computed it. To ensure minimum wait time for the AllReduce kernel, we need to schedule the MatMul kernel to compute chunks in the order the AllReduce kernel communicates them. For instance, in the ring algorithm for AllReduce, the $n^{\text{th}}$ node sends the chunks to the next node in the order starting from the $n^{\text{th}}$ chunk. As such, the MatMul kernel on the $n^{\text{th}}$ node needs to generate the chunks in this order. Furthermore, we need to invoke only one MatMul kernel and AllReduce kernel to avoid the overhead of launching multiple kernels. Figure [1](#fig:matmul-overlap-intro){reference-type="ref" reference="fig:matmul-overlap-intro"} shows that this fine-grained overlapping of MatMul with AllReduce can hide 80% of the execution time of MatMul and provides 1.36$\times$ speedup.

However, manually writing these optimizations for each scenario is unproductive, for example, the implementation of above overlapping optimization contains $\approx$2k lines of CUDA code. Thus, in this paper, we show that by carefully designing a *language* for expressing combinations of computation and communication the benefits of existing machine learning framework's abstraction can be maintained while simultaneously allowing a *compiler* to apply powerful optimizations.

![image](figures/overview-2.pdf){width="\\linewidth"}

To this effect, we propose [CoCoNet]{.smallcaps}[^1] for generating co-optimized custom computation and communication kernels. Figure [\[fig:overview\]](#fig:overview){reference-type="ref" reference="fig:overview"} presents the overview of [CoCoNet]{.smallcaps}. [CoCoNet]{.smallcaps} includes a domain specific language (DSL) to express programs containing both computation and communication operations. Inspired by Halide [@halide], [CoCoNet]{.smallcaps} includes a *scheduling* language to specify an execution schedule of the program using a set of transformations. [CoCoNet]{.smallcaps}'s *autotuner* automatically applies these transformations to optimize a program by breaking the communication and computation boundary. Hence, [CoCoNet]{.smallcaps} enables users to quickly generate optimized implementations for specific hardware, topology, and data sizes. [CoCoNet]{.smallcaps}'s *code generator* automatically generates high-performance computation and communication kernels from a program and its schedule. We used [CoCoNet]{.smallcaps} to optimize data-parallel training, model-parallel inference, and pipeline-parallel inference. [CoCoNet]{.smallcaps} generated kernels for the Adam [@adam] and LAMB [@lamb] optimizers speeds up the training time of BERT models by upto 1.68$\times$ and can train BERT 3.9 Billion parameter models using only data parallelism, which is not possible with state of the arts. [CoCoNet]{.smallcaps}'s kernels for model parallelism speeds up the inference in BERT 3.9 Billion and GPT-2 8.2 Billion parameter models by upto 1.51$\times$. [CoCoNet]{.smallcaps}'s optimized pipeline parallelism kernels speeds up inference times in GPT-2 8.2 Billion and GPT-3 175 Billion parameter models by upto 1.77$\times$. Our implementation of [CoCoNet]{.smallcaps} is available at <https://github.com/parasailteam/coconet>.

# The [CoCoNet]{.smallcaps} DSL {#sec:dsl}

The [CoCoNet]{.smallcaps} DSL extends the data representation in existing machine learning frameworks and provides constructs to express both computation and communication. The [CoCoNet]{.smallcaps} DSL is embedded in C++. Unifying the expression of computation and communication for distributed machine learning in the same DSL is the foundation to enable optimizations across computation and communication.

In this paper, we follow the MPI [@mpi] terminology: `RANK` is the process ID of a distributed process, `GROUP` is a set of concurrent distributed processes, and `WORLD` is the `GROUP` that includes all processes. [CoCoNet]{.smallcaps} supports dividing consecutive ranks into one or more process groups.

``` {.DSL language="DSL"}
Tensor w(FP16, [H,H], Sliced(0), WORLD, RANK); |\label{line:mp:inputensor-begin}||\label{line:mp:continuous-tensor}|
Tensor b(FP16, [H], Replicated, WORLD); |\label{line:mp:inputensor-end}| 
Tensor in(FP16, [B,S,H], Sliced(2), WORLD, RANK);
Tensor r(FP16, [B,S,H], Replicated, WORLD);

// layer(FP16, [B,S,H], Local, WORLD, RANK)
Var layer = MatMul(in, w); |\label{line:mp:matmul}|
// sum(FP16, [B,S,H], Replicated, WORLD)
Var sum = AllReduce("+", layer); |\label{line:mp:allreduce}|
// dropout(FP16, [B,S,H], Replicated, WORLD)
Var dropout = Dropout(sum + b, 0.1); |\label{line:mp:pointwise-start}|
// out(FP16, [B,S,H], Replicated, WORLD)
Var out = dropout + r;|\label{line:mp:pointwise-end}|

Execute self_attention({w,in,b,r}, {out});
```

[\[fig:traditional-mp\]]{#fig:traditional-mp label="fig:traditional-mp"}

## Tensor Layout

[CoCoNet]{.smallcaps} extends the concept of a tensor in machine learning frameworks from a single device data into distributed forms. Besides item datatype, like `FP32` and `FP16`, and shape, a [CoCoNet]{.smallcaps} tensor also includes a *layout* that describes the distributed allocation of tensor's data across a set of ranks. There are three layouts for a tensor: *sliced*, *replicated*, and *local*. A *sliced* tensor is equally distributed among all nodes in a group along a specified dimension with `RANK` identifying the slice for that process. For example, in Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"}, which describes the Megatron-LM [@megatronlm] model parallel logic of Self-Attention layer in [CoCoNet]{.smallcaps}, `w` is sliced among all ranks in `WORLD` in the first dimension and `in` is sliced in the third dimension. A tensor can also be *replicated* across all ranks in a group where it has the same value on each rank and it does not have a rank identifier. For example, the bias `b` and the residual connection `r` are replicated as shown in Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"}. A *local* tensor has same shape on all ranks but different values on all ranks. A local tensor requires `RANK` to identify the values. For example, in Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"}, `layer` is a local tensor that represents the result of MatMul operation. A `Scalar` is a zero-dimensional tensor that represents a variable available on all ranks. We discuss the layout of intermediate tensors in the next section.

## [CoCoNet]{.smallcaps}'s Operations

A [CoCoNet]{.smallcaps} program inherits the concept of data-flow graph (DFG) from existing machine learning frameworks with operations as vertices and data dependencies as edges. Operations in [CoCoNet]{.smallcaps} can be classified as (i) local computations, such as pointwise computations, matrix multiplication, and convolution, and (ii) cross rank communication operations, such as AllReduce, AllGather, and P2P Send-Recv. Table [1](#tab:operations){reference-type="ref" reference="tab:operations"} shows all operations supported by [CoCoNet]{.smallcaps}.

A `Var` represents the intermediate tensor obtained after performing an operation. In the example of Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"}, the linear layer's weight (`w`) and the input (`in`) are sliced across all ranks while the bias (`b`) and residual (`r`) are replicated on all ranks. A `Var`'s shape and distribution layout are inferred based on the operation and inputs to the operation. For example, line [\[line:mp:matmul\]](#line:mp:matmul){reference-type="ref" reference="line:mp:matmul"} performs a MatMul operation on the input (`in`) and weights (`w`). Since MatMul between two sliced tensors produces a local tensor, `layer` represents the partial result with *local* layout. At line [\[line:mp:allreduce\]](#line:mp:allreduce){reference-type="ref" reference="line:mp:allreduce"}, AllReduce computes the sum of `layer` of all ranks and returns a *replicated* tensor with the same values on each rank. The computations at lines [\[line:mp:pointwise-start\]](#line:mp:pointwise-start){reference-type="ref" reference="line:mp:pointwise-start"}--[\[line:mp:pointwise-end\]](#line:mp:pointwise-end){reference-type="ref" reference="line:mp:pointwise-end"} add the bias, use dropout as an activation, and add the residual. At line [\[line:mp:pointwise-start\]](#line:mp:pointwise-start){reference-type="ref" reference="line:mp:pointwise-start"}, the addition of `sum` and `b` follows PyTorch's broadcast semantics[^2] by replicating `b` in all dimensions of `sum`. Thus, the shape and layout of output of these operations are same as `sum`. Finally, `Execute` defines the name, inputs, and outputs of the program.

::: {#tab:operations}
  ------------------- --------------------------------------------
   **Communication**  AllReduce, AllGather, ReduceScatter,
    **Operations**    Reduce, Broadcast, P2P Send-Recv
      **Layers**      Matrix Multiplication, Convolution
    **Activations**   Dropout, tanh, ReLU
      **Tensor**      $+$, $-$, $*$, $\div$, Norm, ReduceTensor,
    **Operations**    Sqrt, Pow, Update
  ------------------- --------------------------------------------

  : Operations supported by [CoCoNet]{.smallcaps} includes all common communication and computation operations. [\[tab:operations\]]{#tab:operations label="tab:operations"}
:::

## Fused Collective Communication Operations {#sec:fuse-comm-coll}

[CoCoNet]{.smallcaps} enables efficient computations on the output of communication by providing fused collective communication operations, such as FusedAllReduce. Consider the AllReduce in Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"} followed by a Dropout (lines  [\[line:mp:allreduce\]](#line:mp:allreduce){reference-type="ref" reference="line:mp:allreduce"}--[\[line:mp:pointwise-start\]](#line:mp:pointwise-start){reference-type="ref" reference="line:mp:pointwise-start"}). The abstraction in existing machine learning frameworks requires the output of AllReduce to be stored in memory and then re-loaded by Dropout. FusedAllReduce avoids such stores and loads by directly passing the output of communication to following computations through registers. In addition to the argument of AllReduce, a FusedAllReduce takes computations as extra arguments. Section [6.2](#sec:code-gen-fused){reference-type="ref" reference="sec:code-gen-fused"} discusses the implementation of Fused Collective Communication Operations.

## Overlapping Operations {#sec:overlap-comm-coll}

[CoCoNet]{.smallcaps} supports overlapping multiple dependent computation and communication operations using the `Overlap` construct. For example, consecutive MatMul and AllReduce in Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"} (lines [\[line:mp:matmul\]](#line:mp:matmul){reference-type="ref" reference="line:mp:matmul"}--[\[line:mp:allreduce\]](#line:mp:allreduce){reference-type="ref" reference="line:mp:allreduce"}) can be overlapped to fully utilize both network and computation resources. Section [6.3](#sec:overlap-impl){reference-type="ref" reference="sec:overlap-impl"} discusses the implementation of this construct.

## Custom Operations

In [CoCoNet]{.smallcaps}, the implementation of an operator needs to define three key properties of the operator: (i) syntax, (ii) semantics, and (iii) code generation. The syntax of an operator is defined using C++ constructors and the semantics are defined by implementing rules to describe the layout and size of the output tensor based on the input tensors. Finally, the code generation requires implementing a function to generate a call to existing libraries or generate fused GPU kernels. The implementation of syntax and semantics can be achieved in a few lines of code, however, implementing the code generation for complex operations like Matrix Multiplication and Convolution can potentially take hundreds of lines of code. Fortunately, in practice the code generation for complex operations can call an optimized implementation of existing libraries.

## Communication Collectives

may move to section implementation In addition to the AllReduce primitive mentioned above, communication libraries like NCCL [@nccl], support several collective communications based on the MPI standard [@mpi]. The communication collectives in NCCL take an input buffer $b_i$ of size $N_i$ and writes to an output buffer $b_o$ of size $N_o$. *AllReduce* performs a reduction operation on $b_i$ and leaves identical copies of $b_o$ on all ranks. *AllGather* gathers all $N_i$ values of $b_i$ from all ranks to $b_o$, such that, $N_o = N_i \times |\texttt{WORLD}\xspace|$. *ReduceScatter* performs a reduction operation on $b_i$ and scatter the result among all ranks in $b_o$, such that, $N_o = N_i \div |\texttt{WORLD}\xspace|$. *Reduce* takes a root rank $r$ and performs reduction on $b_i$ and only writes the result to $b_o$ of $r$. *Broadcast* takes a root rank $r$. It copies $N_i$ values of $b_i$ of rank $r$ and leaves identical copies in $b_o$ of all ranks.

## Equivalent Programs using Transformations

The way to express the same algorithm in [CoCoNet]{.smallcaps} DSL is not unique. For example, as shown in Figure [\[fig:reducescatter\]](#fig:reducescatter){reference-type="ref" reference="fig:reducescatter"}, existing works [@zero; @gshard] distributes the computation by first doing ReduceScatter to divide the summed values among all rank, then perform computation on the divided tensor, and finally performs AllGather to share the output of computation. Next section describes several output-invariant transformation between different concrete programs.

[CoCoNet]{.smallcaps} performs operations on 1-dimensional arrays, we refer to as Tensors. [CoCoNet]{.smallcaps} supports two types of operations that takes one or more tensors as input and returns one tensor. First type is computations. [CoCoNet]{.smallcaps} supports pointwise computations, where $i^{th}$ element of the output depends on the $i^{th}$ element of inputs, and reductions over a tensor using `ReduceTensor` construct. Moreover, [CoCoNet]{.smallcaps} supports casting of one element type to another. This casting is useful for several scenarios like mixed precision training. Second type are communication collectives supported by NCCL [@nccl]. A program written in [CoCoNet]{.smallcaps} forms a Directed Acyclic Graph (DAG) of stages, where each *stage* performs one or more computations and stage represent the output of the computation. The properties of a stage, such as the size in each dimension, element type, are determined using the semantics of the computation.

Figure [\[fig:traditional-sgd\]](#fig:traditional-sgd){reference-type="ref" reference="fig:traditional-sgd"} implements Adam in [CoCoNet]{.smallcaps}. Tensors input to the program, such as gradient and weight, are defined using the `Tensor` (lines [\[line:adam:inputensor-begin\]](#line:adam:inputensor-begin){reference-type="ref" reference="line:adam:inputensor-begin"}--[\[line:adam:inputensor-end\]](#line:adam:inputensor-end){reference-type="ref" reference="line:adam:inputensor-end"}).

Each tensor has four properties associated with it:

1.  **Element Type** of the tensor, such as, integers and floats.

2.  **Size**, i.e., the number of elements in the tensor.

3.  **Set of Ranks** that stores this tensor in their memory. A special set `WORLD` contains all ranks.

4.  **Layout** [CoCoNet]{.smallcaps} supports three layouts of a tensor. A tensor of size $N$ is stored on one or more ranks in a memory location of $N$ elements. A tensor is a complete tensor that is stored on all ranks in `WORLD` with each rank containing same contents. Since a tensor is a special kind of tensor, [CoCoNet]{.smallcaps} allows conversion of tensor to a tensor. A tensor of size $N$ stored on all ranks in `WORLD`, is equally distributed among all ranks, such that, $i^{th}$ rank stores the $i^{th}$ part of tensor.

Single dimension tensors that are input to the program and are available on all ranks are represented using `Scalar`. Line [\[line:adam:scalars\]](#line:adam:scalars){reference-type="ref" reference="line:adam:scalars"} defines `lr`, `beta1`, `beta2` as variables. Each communication collective returns a `Stage` object. Lines [\[line:adam:avg\]](#line:adam:avg){reference-type="ref" reference="line:adam:avg"} uses AllReduce to do elementwise reduction by summing all $i^{th}$ elements of `g` stored on all ranks and uses `avg` to represent the mean of these values. Pointwise computations involving arithmetic, comparison, and bitwise operators can be expressed in a natural manner. Pointwise computations are valid only when either both tensors are of same size and have same layout because operation is done on corresponding elements of both tensors, or when one or more tensor is a variable, hence, operation is done on the variable and each element of the tensor. Lines [\[line:adam:pointwise-begin\]](#line:adam:pointwise-begin){reference-type="ref" reference="line:adam:pointwise-begin"}--[\[line:adam:pointwise-end\]](#line:adam:pointwise-end){reference-type="ref" reference="line:adam:pointwise-end"} expresses pointwise computations to do the parameter update. Finally, a `Pipeline` construct is used to define all inputs to the program and outputs of the program, which [CoCoNet]{.smallcaps} uses to create the DAG stages. `Pipeline::genCode` method generates CUDA code for all stages and links to NCCL API. With default schedule, [CoCoNet]{.smallcaps} generated code for Adam is shown in Figure [\[fig:FusedSGD\]](#fig:FusedSGD){reference-type="ref" reference="fig:FusedSGD"}.

#### Sliced Computations

Many applications includes operation where $i^{th}$ rank performs computation on the $i^{th}$ part of the input to produce $i^{th}$ part of the output and then all parts of output can be combined using AllGather. Recent techniques, such as Zero [@zero], utilizes this approach to do parameter update. In [CoCoNet]{.smallcaps}, we can express this algorithm as shown in Figure [\[fig:reduce-scatter-sgd\]](#fig:reduce-scatter-sgd){reference-type="ref" reference="fig:reduce-scatter-sgd"}. Lines [\[line:sliced-adam:inputensor-sliced-begin\]](#line:sliced-adam:inputensor-sliced-begin){reference-type="ref" reference="line:sliced-adam:inputensor-sliced-begin"}--[\[line:sliced-adam:inputensor-sliced-end\]](#line:sliced-adam:inputensor-sliced-end){reference-type="ref" reference="line:sliced-adam:inputensor-sliced-end"} declares momentum and velocity tensors with the sliced layout. At line [\[line:sliced-adam:reducescatter\]](#line:sliced-adam:reducescatter){reference-type="ref" reference="line:sliced-adam:reducescatter"}, ReduceScatter performs element-wise summation of gradient across the ranks to return a sliced stage. Since momentum, velocity, and gradients are of sliced layout, we can perform all the pointwise computations. However, before doing parameter update, we need to slice the parameters and then perform the computations(line [\[line:sliced-adam:sliced-parameter\]](#line:sliced-adam:sliced-parameter){reference-type="ref" reference="line:sliced-adam:sliced-parameter"}). Finally, an AllGather is introduced to gather all slices of updated parameters and store them in a layout (line [\[line:sliced-adam:allgather\]](#line:sliced-adam:allgather){reference-type="ref" reference="line:sliced-adam:allgather"}).

### Mixed Precisions

[CoCoNet]{.smallcaps} supports mixed precision applications where tensors can be of different type using its `Cast` construct, which cast all elements of the tensor to other type. For example, in a parameter update that takes parameters in 32-bit Floats but gradients in 16-bit Floats, a `Cast` computation on gradients will cast all gradients from 16-bit Floats to 32-bit Floats.

### Tensor Reduction

Many applications requires a reduction over tensors, such as, calculating the norm. For example, LAMB Optimizer `\cite{}`{=latex} calculates the norm of parameters and uses this norm in the parameter update. [CoCoNet]{.smallcaps} provides `ReduceTensor` construct to perform a reduction computation on a tensor. `ReduceTensor` supports several reduction operators, such as, sum, max, min. `ReduceTensor` supports performing reduction on both and sliced tensors and outputs a `Stage` containing a single element.

## [CoCoNet]{.smallcaps}'s Functional Primitives

probably needs to be removed - Abhinav [\[sec:functional-prmitives\]]{#sec:functional-prmitives label="sec:functional-prmitives"}

Given a program in [CoCoNet]{.smallcaps}'s DSL described above, [CoCoNet]{.smallcaps} translates to a set of functional primitives. This provides a unified abstraction for computation and communication allowing us to define rewrite rules for transformations, validating the program using type checking rules, and generating code.

We start with standard functional primitives. Starting with the base definition of $\textit{List}[T]$ as a vector of some type $T$, we define a tensor of $n$ dimensions recursively as a list of tensors of $n-1$ dimensions. Thus, a matrix is of type $\textit{List}[\textit{List}[T]]$ and so on. The function $map(f:T \rightarrow S, l:\textit{List}[T])) \rightarrow \textit{List}[S]$ applies the function $f$ elementwise to the input list $l$. The function $fold(l:\textit{List}[T], f:S\times T \rightarrow S, a:S) \rightarrow S$ starts with the initial value $a$ and iteratively updates $a$ with $f(a,i)$ for each element $i$ in $l$. When $f$ is associative, we use $reduce$ instead of $fold$ to explicitly capture its parallel semantics. Another useful function is $zip$ that converts a pair of lists into a list of pairs. Also, we assume a $transpose$ function that permutes the dimension of a given tensor. We represent $map(f:T \rightarrow S)$ as the 'curried' version of $map$ that applies $f$ to an input $\textit{List}[T]$ and returns a $\textit{List}[S]$. Similarly, we use $\textit{lift}(f:T\times S \rightarrow U)$ as the 'zipped' version of the function that takes in a $\textit{List}[T]$ and $\textit{List}[S]$ and applies $f$ elementwise to return a $\textit{List}[U]$. For example, $\textit{lift}(+)$ represents vector addition, which allows us to define vector reduction as $reduce(lv: \textit{List}[\textit{List}[T]], \textit{lift}(+), 0)$.

A key contribution of this work is to extend these functional primitives to work seamlessly with distributed tensors. A $\textit{RepList}[T]$ represents a replicated tensor and a $\textit{SlicedList}[T]$ represents a sliced tensor. We also allow a tensor to be located at a particular rank $r$ with $\textit{LocList}_{r}[T]$, with the function $rank$ returning the rank of such a list. Now we define primitives on these distributed tensors.

The function $slice$ distributes a tensor along its first dimension to all the ranks in the `WORLD` resulting in a sliced tensor. The function $join$ is its inverse. Similarly, the function $repl$ replicates a tensor to all the ranks in the `WORLD` resulting in a replicated tensor, while $drop$ is its inverse. Finally, the function $store$ places a tensor at a particular rank, while $load$ removes this association.

$\begin{array}{l}
slice(l: \textit{List}[T]) \rightarrow \textit{SlicedList}[T]\\
join(sl: \textit{SlicedList}[T]) \rightarrow \textit{List}[T]\\
repl(l: \textit{List}[T]) \rightarrow \textit{RepList}[T]\\  
drop(sl: \textit{RepList}[T]) \rightarrow \textit{List}[T]\\
store(l: \textit{List}[T], r: Int) \rightarrow \textit{LocList}_{r}[T] \\
load(ll: \textit{LocList}_{r}[T]) \rightarrow \textit{List}[T]
\end{array}$

These primitives allow us to capture the tensor computations and communication that occur in a distributed ML program. The base functional primitives allow us to capture tensor computations. For example, pointwise computations can be captured with $map$, dot product of two vectors with pointwise multiplication followed by a $reduce$, matrix-vector multiplication as a sequence of dot products, and so on.

Lastly, our extensions for distributed tensors capture common communication collectives as shown below. These use the $reduce$ function on at least two dimensional sliced tensor. $$reduce(sl: \textit{SlicedList}[T], f : (T \times T \rightarrow T)) \rightarrow \textit{List}[T]$$

$\begin{array}{l@{}l}
\text{Reduce}(sl: \textit{SlicedList}[T], f, r) &: store(reduce(sl, f), r)\\
\text{Broadcast}(ll: \textit{LocList}_{r}[T]) &: repl(load(ll))\\
\text{AllReduce}(sl: \textit{SlicedList}[T], f) &: repl(reduce(sl, f))\\
\text{ReduceScatter}(sl: \textit{SlicedList}[T], f) &: slice(reduce(sl, f))\\
\text{AllGather}(sl: \textit{SlicedList}[T]) &: repl(join(sl))\\
\end{array}$

![image](figures/coconet-example.pdf){width="\\linewidth"}

# [CoCoNet]{.smallcaps} Transformations {#sec:schedule}

[CoCoNet]{.smallcaps} provides four semantics preserving *transformations* to optimize a program written in the DSL. All transformations are valid based on rules described in the sections below. [CoCoNet]{.smallcaps} automatically checks the validity of each transformation based on these rules and throws an error for an invalid transformation.

We call an order of transformations a *schedule*. A user can manually specify the schedule to optimize the program. Additionally, a user can invoke the autotuner to automatically find the best performing schedule for the given problem sizes and the underlying architecture. Below we present each transformation by applying them on the program from Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"} and show equivalent [CoCoNet]{.smallcaps} programs generated after applying each transformation in Figure [\[fig:mp-schedules\]](#fig:mp-schedules){reference-type="ref" reference="fig:mp-schedules"}.

## Splitting Communication

The `split` transformation breaks a collective communication operation into two communication operations. One of the two split policies supported by [CoCoNet]{.smallcaps} is

**AllReduce Split RS-AG** splits an AllReduce into a ReduceScatter to produce a tensor and an AllGather on the tensor to return a tensor.

**Running Example** The AllReduce in Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"} is split into `rsSum` that does a ReduceScatter on `layer` and `agSum` that does an AllGather on `rsSum`.

``` {.DSL language="DSL" numbers="none"}
(rsSum, agSum) = split(layer, ARSplitRSAG);
```

The program of Figure [\[fig:mp-schedules\]](#fig:mp-schedules){reference-type="ref" reference="fig:mp-schedules"} is the implementation of this schedule where the input to Dropout is replaced by `agSum`.

***Validity*** Since an AllReduce can always be split to a ReduceScatter and an AllGather, this transformation is always valid.

![Equivalent programs (from Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"}) using AllReduce (on left) or using ReduceScatter + AllGather (on right).](figures/reduceScatter.pdf){#fig:model-parallel-using-reducescatter width=".9\\linewidth"}

## Reordering Operations

The `reorder` transformation swaps operations with an AllGather or a Broadcast in the DFG of a program. We explain this transformation for AllGather below:

**AllGather Reorder** reorders an AllGather with communication and computation operations. This transformation changes the layout of the operations, the input and output of operations, and the input and output of the AllGather. We explain this transformation below using the running example.

**Running Example** In Figure [\[fig:mp-schedules\]](#fig:mp-schedules){reference-type="ref" reference="fig:mp-schedules"}, applying the `reorder` transformation changes the program to by reordering AllGather(`agSum`) with computations `d` and `out`. The reorder transformation replaces these operations in the DFG with three new operations: `scD` and `scOut`, both of which performs sliced computations, and `agOut`, which gathers the final result of computations.

``` {.DSL language="DSL" numbers="none"}
(scD, scOut, agOut) = reorder(d, out, agSum);
```

The new sliced computations perform the same operations as original computations with two differences: (i) the output of AllGather used in the computation is replaced by the input of AllGather, and (ii) since the input of AllGather is sliced, all tensors input to the computations are also sliced along the same dimension as the input of AllGather. After reorder, `scD` performs the same computation as `d` but `scD` takes `rsSum` and `Slice(r)` as input. Therefore, the layout of `scOut` is also sliced while the computation is same as `out`. Furthermore, the new AllGather is performed on the outputs of the computations, for example, after reorder, the AllGather(`agOut`) is performed on `scOut`. Figure [2](#fig:model-parallel-using-reducescatter){reference-type="ref" reference="fig:model-parallel-using-reducescatter"} shows the workflow of this schedule.

***Validity*** The `reorder` transformation is valid only if operations being reordered with an AllGather can be sliced along the dimension the AllGather is performed. The rules of slicing an operation depend on the type of operation and the dimensions of inputs to the operations. For example, `d` and `out` can be sliced because the computations have the same dimensions as `agOut`. Section [4](#sec:opt-workloads){reference-type="ref" reference="sec:opt-workloads"} shows how P2P Send can be reordered with an AllGather.

## Fusing Operations {#sec:sched:fusion}

Fusing multiple computations is a common technique used by existing compilers [@tvm18; @distributed-halide; @fireiron; @polymage-gpu; @halide]. [CoCoNet]{.smallcaps} extends this concept to fuse multiple computations and communications in a single operation and provides this capability using the `fuse` transformation. Below we explain two fuse policies supported by [CoCoNet]{.smallcaps}:

**Computation Fuse** fuses a series of computations in a single operation that performs all these operations.

**AllReduce Fuse** fuses a series of ReduceScatter, sliced computations, and AllGather operations in a single FusedAllReduce that performs all these operations.

**Running Example** We can fuse ReduceScatter(`rsSum`), computations (`scD` and `scOut`), and AllGather(`agOut`) in program of Figure [\[fig:mp-schedules\]](#fig:mp-schedules){reference-type="ref" reference="fig:mp-schedules"} into a FusedAllReduce to obtain program .

``` {.DSL language="DSL" numbers="none"}
fuseAR = fuse(rsSum, scOut, agOut, ARFuse);
```

The `comp` method of `fusedAR` specifies the computation to be fused with FusedAllReduce and returned `out` is the output.

***Validity*** Fusing multiple operations into one operation is valid only if the dependencies in the DFG after fusion are preserved.

## Overlapping Operations {#overlapping-operations}

[CoCoNet]{.smallcaps} provides the `overlap` transformation to overlap a series of producer-consumer operations to utilize multiple resources of hardware simultaneously.

**Running Example** In the program of Figure [\[fig:mp-schedules\]](#fig:mp-schedules){reference-type="ref" reference="fig:mp-schedules"} we overlap the matrix multiplication (`layer`) with FusedAllReduce(`fuseAR`) to obtain program in .

``` {.DSL language="DSL" numbers="none"}
layerWithAR = overlap(layer, fusedAR);
```

***Validity*** Overlapping multiple operations is valid only when all operations have a producer-consumer relationship between them.

## Automatic Exploration of Schedules

[CoCoNet]{.smallcaps} provides an *autotuner* to automatically explore the space of all schedules of a program and return the schedule that provides the best performance for the underlying architecture and input sizes. First, the autotuner fuses all pointwise computations up to a pre-defined threshold to decrease the search space and then exhaustively explores the schedule space in a breadth first search manner. Finally, the autotuner generates code for all schedules in its search space, executes all programs, and returns the schedule with minimum execution time. Table [\[tab:loc-autotuner-time\]](#tab:loc-autotuner-time){reference-type="ref" reference="tab:loc-autotuner-time"} shows that the autotuner takes only a few seconds to explore the schedule space for all workloads.

``` {.numberLines .DSL language="DSL" numbers="left"}
Var avg = AllReduce("+", g); |\label{line:adam:avg}|
Var m_ = Update(m, (m*beta1+(1-beta1)*avg));|\label{line:adam:pointwise-begin}||\label{line:update:m}|
Var v_ = Update(v, (v*beta2+(1-beta1)*avg*avg));|\label{line:update:v}|
Var m1 = m_/(1-Pow(beta1, t));
Var v1 = v_/(1-Pow(beta2, t));
Var p_ = Update(p, (p - lr * m1/(Sqrt(v1))));|\label{line:adam:pointwise-end}|

Execute adam({g,p,v,m,lr}, {p_});
```

``` {.numberLines .DSL language="DSL" numbers="left"}
comps = fuse(m_, v_, m1, v1, p_, 
             ComputationFuse);|\label{line:adam-schedule:fuse-comp}|
(rsG, agG) = split(avg, ARSplitRSAG); |\label{line:adam-schedule:split}|
(scComp, agP, agM, agV) = reorder(agG, comps, 
                                  AGReorder);|\label{line:adam-schedule:reorder}|  
asSlice(m); asSlice(v); dead(agM); dead(agV); |\label{line:adam-schedule:slice-m-v}| |\label{line:adam-schedule:remove-m-v-allgather}|
fuseAR = fuse(rsG, scComp, agP, AllReduceFuse);|\label{line:adam-schedule:fuse-allreduce}|
```

# Distributed Workloads in [CoCoNet]{.smallcaps} {#sec:opt-workloads}

We additionally optimized two distributed machine learning workloads using [CoCoNet]{.smallcaps}: (i) parameter update using Adam [@adam], and (ii) point-to-point communication in pipeline parallelism.

**Adam in Data Parallel Training**: Figure [\[fig:traditional-adam\]](#fig:traditional-adam){reference-type="ref" reference="fig:traditional-adam"} shows the traditional implementation of parameter update using Adam. First, all ranks average the gradients using AllReduce and then perform computations to update the optimizer state and model parameters. `Update` updates the values of a tensor and reflects the new values in that position in the DFG (lines [\[line:update:m\]](#line:update:m){reference-type="ref" reference="line:update:m"}--[\[line:update:v\]](#line:update:v){reference-type="ref" reference="line:update:v"}). Figure [\[fig:adam-schedule\]](#fig:adam-schedule){reference-type="ref" reference="fig:adam-schedule"} presents a schedule that optimizes this by distributing the computation on all ranks in a single kernel. Line [\[line:adam-schedule:fuse-comp\]](#line:adam-schedule:fuse-comp){reference-type="ref" reference="line:adam-schedule:fuse-comp"} fuses all computations in `comps`. Line [\[line:adam-schedule:split\]](#line:adam-schedule:split){reference-type="ref" reference="line:adam-schedule:split"} splits the AllReduce into a ReduceScatter and an AllGather, such that computations take output of AllGather(`agG`) as input. Line [\[line:adam-schedule:reorder\]](#line:adam-schedule:reorder){reference-type="ref" reference="line:adam-schedule:reorder"} reorders AllGather with computations, such that, each rank performs computations on a slice of tensors. Line [\[line:adam-schedule:slice-m-v\]](#line:adam-schedule:slice-m-v){reference-type="ref" reference="line:adam-schedule:slice-m-v"} slices optimizer states on all ranks to decrease memory usage and removes corresponding AllGather. Finally, line [\[line:adam-schedule:fuse-allreduce\]](#line:adam-schedule:fuse-allreduce){reference-type="ref" reference="line:adam-schedule:fuse-allreduce"} fuses all operations in a single kernel.

![In Megatron-LM each GPU sends redundant data. [\[fig:p2p-fusion-1\]]{#fig:p2p-fusion-1 label="fig:p2p-fusion-1"}](figures/pipeline-p2p-fusion-1.pdf){#fig:p2p-fusion-1 width=".85\\linewidth"}

![Communication operations can be overlapped at the granularity of each *communication buffer tile* of data in single kernel call.[\[fig:p2p-fusion-3\]]{#fig:p2p-fusion-3 label="fig:p2p-fusion-3"}](figures/pipeline-p2p-fusion-3.pdf){#fig:p2p-fusion-3 width=".85\\linewidth"}

**Point-to-Point Communication in Pipeline Parallelism**: Figure [3](#fig:p2p-fusion-1){reference-type="ref" reference="fig:p2p-fusion-1"} shows a scenario of pipeline parallelism in Megatron-LM with two transformer layers assigned to two groups each with two ranks. Rank $i$ in group $j$ is shown by $(j,i)$. Each group uses model parallelism within its transformer layer. Pipeline parallelism in Megatron-LM works as follows. First, all ranks in the first group reduce their input using AllReduce to get replicated output. Then each rank performs pointwise computations over the replicated output. Finally, the first group sends the result of computations to the corresponding rank in the second group using point-to-point (P2P) sends. (Line [\[line:p2p:comp2\]](#line:p2p:comp2){reference-type="ref" reference="line:p2p:comp2"} in Figure [\[fig:traditional-p2p\]](#fig:traditional-p2p){reference-type="ref" reference="fig:traditional-p2p"} shows these computations but are omitted in Figure [\[fig:P2Ptimeline\]](#fig:P2Ptimeline){reference-type="ref" reference="fig:P2Ptimeline"} for simplicity). Since the output of AllReduce in Figure [3](#fig:p2p-fusion-1){reference-type="ref" reference="fig:p2p-fusion-1"} is replicated, redundant data is sent using P2P. We can avoid this redundant communication by splitting the AllReduce to ReduceScatter and AllGather and reordering the P2Ps with the AllGather. Hence, the inter-group communication is reduced by the group size. We can further optimize by overlapping all communication operations. Figure [4](#fig:p2p-fusion-3){reference-type="ref" reference="fig:p2p-fusion-3"} shows that if the buffers are split into multiple tiles (`T0`--`T2` in the figure), intra-group and inter-group communications can be overlapped.

Figure [\[fig:traditional-p2p\]](#fig:traditional-p2p){reference-type="ref" reference="fig:traditional-p2p"} is the original program, while Figure [\[fig:p2p-schedule\]](#fig:p2p-schedule){reference-type="ref" reference="fig:p2p-schedule"} optimizes it by applying transformations. Line [\[line:p2p:fuse-send\]](#line:p2p:fuse-send){reference-type="ref" reference="line:p2p:fuse-send"} fuses the P2P send with computations. Line [\[line:p2p:split\]](#line:p2p:split){reference-type="ref" reference="line:p2p:split"} splits the AllReduce and reorders the returned AllGather with the fused P2P send at Line [\[line:p2p:reorder\]](#line:p2p:reorder){reference-type="ref" reference="line:p2p:reorder"}. Hence, P2P send and computations are performed on only a slice of data on the next group where the AllGather is also performed. Finally, all three new operations get overlapped in Line [\[line:p2p:fuseAR\]](#line:p2p:fuseAR){reference-type="ref" reference="line:p2p:fuseAR"}.

``` {.numberLines .DSL language="DSL" numbers="left"}
Var sum = AllReduce("+", in);
Var send = Dropout(recv+b,0.1) + r;|\label{line:p2p:comp1}||\label{line:p2p:comp2}|
Var output = Send(send, 
                  GroupRank(GROUP+1, RANK));

Execute transformer({in}, {output});
		
```

``` {.numberLines .DSL language="DSL" numbers="left"}
fuseSend = fuse(send, output, SendFuse);|\label{line:p2p:fuse-send}|
(rsSum, agSum) = split(sum, ARSplitRSAG); |\label{line:p2p:split}|
(scSend, agOut) = reorder(fuseSend, agSum, 
                          AGReorder); |\label{line:p2p:reorder}|
overlapOut = overlap(rsSum, scSend, agOut); |\label{line:p2p:fuseAR}|
		
```

``` {.DSL language="DSL"}
Variable lr(Float);
    Variable m(Float);
    Tensor g(Float, SIZE, GPUs);
    Tensor p(Float, SIZE, GPUs);
    Tensor v(Float, SIZE, GPUs);
    
    Stage sum = AllReduce("+", g);
    Stage v_ = m * v + sum/GPUs;
    Stage p_ = p - lr * v_;
    
    Pipeline pipeline({g, p, m, lr}, {v}, {p_});
```

``` {.DSL language="DSL"}
Stage sumRS;
        Stage sumAG;
        pipeline.split(sum, &sumRS, &sumAG, 
                       ReduceScatterAllGather); 
```

``` {.DSL language="DSL"}
sumRS = ReduceScatter(g); 
                sumAG = AllGather(sumRS)
                //Replace all references of 
                //sum with sumAG
```

``` {.DSL language="DSL"}
Stage v1_, vAG;
        pipeline.reorder(v_, sumAG, v1_, vAG); 
```

``` {.DSL language="DSL"}
v1_ = m * slice(v) + sumRS/GPUs; 
                vAG = AllGather(v1_);
                p_ = p - lr * vAG;
```

``` {.DSL language="DSL"}
Stage p1_, pAG;
        pipeline.reorder(vAG, p_, p1_, pAG); 
        pipeline.asSlice(v); 
        pipeline.fuse({sumRS, v1_, p1_, pAG}, 
                      AllReduce); 
        pipeline.storeAt({p_, p},{v_, v});
```

``` {.DSL language="DSL"}
v1_ = m * slice(v) + sumRS/GPUs; 
        pAG_ = slice(p) - lr * v1_; 
        p1_ = AllGather(pAG_);
```

``` {.DSL language="DSL"}
pipeline.asSlice(v); 
        pipeline.fuse({sumRS, v1_, p1_, pAG}, 
                        AllReduce); 
        pipeline.storeAt({p1_, p},{v1_, v});
```

``` {.DSL language="DSL"}
Variable lr(Float);
    Variable m(Float);
    Tensor g(Float, SIZE, GPUs);
    Tensor p(Float, SIZE, GPUs);
    Tensor v(Float, SIZE, GPUs);
    Tensor m(Float, SIZE, GPUs);
    
    Stage sum = AllReduce("+", g);
    Stage m_ = beta1 * m + (1-beta1)*sum;
    Stage v_ = beta2 * v + (1-beta2)*(sum*sum);
    Stage p_ = p - lr * m_/(1-beta1)/(sqrt(v_/(1-beta2)));
    
    Pipeline pipeline({g, p, m, lr}, {v}, {p_});
```

``` {.DSL language="DSL"}
Stage sumRS;
        Stage sumAG;
        pipeline.split(sum, &sumRS, &sumAG, 
                       ReduceScatterAllGather); 
```

``` {.DSL language="DSL"}
sumRS = ReduceScatter(g); 
                sumAG = AllGather(sumRS)
                //Replace all references of 
                //sum with sumAG
```

``` {.DSL language="DSL"}
Stage mv;
        pipeline.fuse({m_,v_}, mv) ;
        Stage mv_, mvAG;
        pipeline.reorder(mv_, sumAG, mv1_, mvAG); 
```

``` {.DSL language="DSL"}
Stage m_ = beta1 * slice(m) + (1-beta1)*sumRS;
                Stage v_ = beta2 * slice(v) + (1-beta2)*(sumRS*sumRS);
                vAG = AllGather(v1_);
                mAG = AllGather(m1_);
                p_ = p - lr * f(mAG, vAG);
```

``` {.DSL language="DSL"}
Stage p1_, pAG;
        pipeline.reorder(mvAG, p_, p1_, pAG); 
        pipeline.asSlice(v); 
        pipeline.asSlice(m); 
        
```

``` {.DSL language="DSL"}
v1_ = m * slice(v) + sumRS/GPUs; 
        pAG_ = slice(p) - lr * v1_; 
        p1_ = AllGather(pAG_);
```

``` {.DSL language="DSL"}
pipeline.asSlice(v); 
        pipeline.fuse({sumRS, v1_, p1_, pAG}, 
                        AllReduce); 
        pipeline.storeAt({p1_, p},{v1_, v});
```

``` {.DSL language="DSL"}
Variable lr(Float);
        Variable m(Float);
        Tensor g(Float, SIZE, GPUs);
        Tensor p(Float, SIZE, GPUs);
        Tensor v(Float, SIZE, GPUs);
        Tensor m(Float, SIZE, GPUs);
        
        Stage sum = AllReduce("+", g);
        Stage m_ = beta1 * m + (1-beta1)*sum;
        Stage v_ = beta2 * v + (1-beta2)*(sum*sum);

        Stage r1 = Reduce("+", p * p);
        Stage p_upd = m_ / sqrt(v_ + eps) + lambda * p
        Stage r2 = Reduce("+", p_upd * p_upd)

        Stage p_ = p - r1/r2*lr* p_upd
        
        Pipeline pipeline({g, p, m, lr}, {v}, {p_});
```

``` {.DSL language="DSL"}
Variable lr(Float);
        Variable m(Float);
        Tensor g(Float, SIZE, GPUs);
        Tensor p(Float, SIZE, GPUs);
        Tensor v(Float, SIZE, GPUs);
        Tensor m(Float, SIZE, GPUs);
        Stage p_slc = slice(p)


        Stage sumRS = ReduceScatter("+", g);
        Stage m_slc = beta1 * slice(m) + (1-beta1)*sumRS;
        Stage v_slc = beta2 * slice(v) + (1-beta2)*(sumRS*sumRS);
        Stage p_upd_slc = m_slc / sqrt(v_slc + eps) + lambda * p_slc

        Stage r1 = AllReduce("+", p_slc * p_slc);
        Stage r2 = AllReduce("+", p_upd_slc * p_upd_slc)

        Stage p_slc_ = p_slc - r1/r2*lr* p_upd_slc

        Stage p_ = AllGather(p_slc_)

        Pipeline pipeline({g, p, m, lr}, {v}, {p_});
```

[CoCoNet]{.smallcaps} automatically checks validity of each computation based on a set of *validation rules*. Each validation rule checks the properties of all tensors involved in the computation and assign values to all properties of the output tensor. Figure [\[fig:rules\]](#fig:rules){reference-type="ref" reference="fig:rules"} presents the validation rules for all computations supported by [CoCoNet]{.smallcaps}. In the validation rules the four properties of a tensor (T) are represented as follows: (i) $t{}$ is the type of elements, (ii) $n$ is the number of elements, (iii) $R{}$ is the set of ranks, and (iv) $a{}$ is the allocation type. Below we explain the validation rules of all computations.

#### Pointwise Computations

Pointwise computations are valid only if the input tensors have same value of properties and the output tensor will have properties with these values (rules [E-BinaryOp]{.smallcaps} and [E-UnaryOp]{.smallcaps}).

#### Communication Collectives

Each computation involving a NCCL Communication Collective computations has a validation rule associated, which follows the semantics of the computation defined in the NCCL documentation `\cite{}`{=latex}. AllReduce on the input tensor is valid only if the input is a tensor and present on all ranks in the `WORLD`([E-AllReduce]{.smallcaps}). After AllReduce, the output tensor has same properties as the input tensor. Similar to AllReduce, ReduceScatter requires same conditions on the input tensor but the output tensor will be equally among all ranks in `WORLD`(rule [E-ReduceScatter]{.smallcaps}). Since AllGather gathers all slices from all ranks and stores these slices in tensors on all ranks, AllGather requires input tensor to be a tensor stored on all ranks in `WORLD` and the output tensor is a (rule [E-AllGather]{.smallcaps}). Slicing a tensor is possible only if it is continuous and the output tensor is stored on all ranks where the input tensor was stored (rule [E-Scatter]{.smallcaps}). So far we have presented the validation rules that can be checked during the compile time. However, the rules for allocation type for Reduce and Broadcast must be dynamically checked because both requires a particular rank as input, which must be checked at the runtime if it belongs to `WORLD`. These dynamic checks are automatically generated by [CoCoNet]{.smallcaps} code generator (discussed in Section [\[sec:code-gen\]](#sec:code-gen){reference-type="ref" reference="sec:code-gen"}). Reduce can be called on tensors that are present on all ranks in `WORLD` with the target rank in the `WORLD` and the output tensor is available on the target rank only (rule [E-Reduce]{.smallcaps}). On the other hand, Broadcast is valid only for tensors and when all nodes in `WORLD` will receive the output tensor(rule [E-Broadcast]{.smallcaps}).

#### LoadTensorAtRank

#### ReduceTensor

Reducing a single tensor is possible on both or tensors. However, the tensor must be stored on all ranks in `WORLD`(rule [E-ReduceTensor]{.smallcaps}). The output tensor will contain only one element, which will have different value for different ranks.

#### Cast

According to the validation rule [E-Cast]{.smallcaps} the output tensor has same properties as the input tensor, except the output's element type is the argument of Cast.

#### Example

We illustrate type checking on [CoCoNet]{.smallcaps} code in Figure [\[fig:FusedSGD\]](#fig:FusedSGD){reference-type="ref" reference="fig:FusedSGD"} and Figure [\[fig:reduce-scatter-sgd\]](#fig:reduce-scatter-sgd){reference-type="ref" reference="fig:reduce-scatter-sgd"}. In Figure [\[fig:FusedSGD\]](#fig:FusedSGD){reference-type="ref" reference="fig:FusedSGD"}, AllReduce produces a stage of same size, element type, and on nodes. Then the computations on lines [11.6.0.2](#){reference-type="ref" reference=""}--[11.6.0.2](#){reference-type="ref" reference=""} again produces tensors with same properties. In contrast, Figure [\[fig:reduce-scatter-sgd\]](#fig:reduce-scatter-sgd){reference-type="ref" reference="fig:reduce-scatter-sgd"} uses a ReduceScatter to generate a tensor of size To be continued - Abhinav

$\begin{array}{@{}l@{}r@{\,}c@{\,}l@{}}
        \textbf{Tensors} & T{}^{[t{},n,R{}, a{}]}\\
        \text{Element Type} & t{} & \in & \ldots\\ %\{u8, i8, f16, \cdots,  u64, i64, f64\} \\
        \text{Tensor Size} & n & \in & Z^{+} \\
        \text{Set of all ranks} & WORLD & \subseteq & Z^{+} \\
        \text{Set of Storage Locations} & R{} & \subseteq & WORLD\\
        \text{Allocation Type} & a{} & \in & \{\text{\text{complete}\xspace, Sliced}\}\\
        \text{Pointwise Computations} & \texttt{op} & \in &{+, -, *, /, pow}\\
\end{array}$

# Schedules in [CoCoNet]{.smallcaps} {#sec:schedule}

In [CoCoNet]{.smallcaps} a user can create a schedule of a program by applying several transformations to the algorithm. Like in Halide [@halide], each transformation changes the code generated for the program but does not require any change to the original algorithm. These transformations are based on rewrite rules for communication collectives and computations.

\|l\|l\|l\|l\| Name & Input Stages & Output Stages\
[AllReduce]{.smallcaps} [Split 1]{.smallcaps} &$i_1 = AllReduce(t)$ &$\begin{array}{l}o_1 = ReduceScatter\xspace(t)\\ 
          o_2 = AllGather\xspace(o_1)\end{array}$\
[AllReduce]{.smallcaps} [Fuse 1]{.smallcaps} & $\begin{array}{l}i_1 = ReduceScatter\xspace(t)\\
            i_2 = AllGather\xspace(i_1)\end{array}$ & $o_1 = AllReduce(t)$\
[AllReduce]{.smallcaps} [Split 2]{.smallcaps} &$i_1 = AllReduce(t)$ & $\begin{array}{l}o_1 = Reduce(t, r)\\ 
        o_2 = Broadcast(o_1, r)\end{array}$\
[AllReduce]{.smallcaps} [Fuse 2]{.smallcaps} & $\begin{array}{l}i_1 = Reduce(t, r)\\ 
          i_2 = Broadcast(o_1, r)\end{array}$ & $o_1 = AllReduce(t)$\
&$\begin{array} {lcl}i_1 = AllGather\xspace(t)  
    \\i_2 = f(i_1, s) \end{array}$ & $\begin{array} {lcl} o_1 = f(t, Slice(s)) 
    \\o_2 = AllGather\xspace(o_1)\end{array}$\
[Broadcast]{.smallcaps} [Reorder]{.smallcaps} & $\begin{array} {lcl} i_1 = Broadcast(t, r)  
      \\ i_2 = f(i_1, s)\end{array}$ & $\begin{array} {lcl} o_1 = f(i_1, Load(s, r))  
      \\ o_2 = Broadcast(o_1)\end{array}$\
[Fuse]{.smallcaps} [Computations]{.smallcaps} & $\begin{array}{lcl} i_1 = f_1(t_1)  
    \\ i_2 = f_2(t_2) \\
    \ldots\\
    i_N = f_N(t_N)\end{array}$ & $\begin{array}{lcl} o_1 = FusedCompute(f_1, f_2, \ldots, f_N, \\
    ~~~~~~~~~~~~~~~~~~t_1, t_2,\ldots,t_N)\end{array}$\
[FusedAllReduce]{.smallcaps} & $\begin{array}{l}i_1 = ReduceScatter\xspace(t)\\ 
    i_2 = f_1(i_1, s_1, s_2, \ldots)\\
    i_3 = AllGather\xspace(i_1)\end{array}$ & $\begin{array}{l} o_1 = FusedAllReduce(t, f_1, s_1, s_2, \ldots)\end{array}$\

\|l\|l\|l\|l\|l\| &\
& & &\
&[AllReduce Split 1]{.smallcaps} &$i_1 = AllReduce(t)$ &$\begin{array}{l}o_1 = ReduceScatter\xspace(t)\\ 
          o_2 = AllGather\xspace(o_1)\end{array}$\
& [AllReduce Fuse 1]{.smallcaps} & $\begin{array}{l}i_1 = ReduceScatter\xspace(t)\\
            i_2 = AllGather\xspace(i_1)\end{array}$ & $o_1 = AllReduce(t)$\
&[AllReduce Split 2]{.smallcaps} &$i_1 = AllReduce(t)$ & $\begin{array}{l}o_1 = Reduce(t, r)\\ 
        o_2 = Broadcast(o_1, r)\end{array}$\
& [AllReduce Fuse 2]{.smallcaps} & $\begin{array}{l}i_1 = Reduce(t, r)\\ 
          i_2 = Broadcast(o_1, r)\end{array}$ & $o_1 = AllReduce(t)$\
$map(f, AllGather(t)) \equiv AllGather(map(f, t))$ & [AllGather Reorder]{.smallcaps} &$\begin{array} {lcl}i_1 = AllGather\xspace(t)  
    \\i_2 = f(i_1, s) \end{array}$ & $\begin{array} {lcl} o_1 = f(t, Slice(s)) 
    \\o_2 = AllGather\xspace(o_1) \\ 
    o_3 = AllGather\xspace(t) \end{array}$\
$map(f, Broadcast(t))\equiv Broadcast(map(f, t))$ & [Broadcast Reorder]{.smallcaps} & $\begin{array} {lcl} i_1 = Broadcast(t, r)  
      \\ i_2 = f(i_1, s)\end{array}$ & $\begin{array} {lcl} o_1 = f(i_1, Load(s, r))  
      \\ o_2 = Broadcast(o_1) \\ 
       o_3 = Broadcast(t, r)\end{array}$\

## Identities

The rewrite rules in [CoCoNet]{.smallcaps} follow from a core set of identities on the primitives. First, these primitives are inverse of each other:

$\begin{array}{ll}
  slice(join(sl)) &\iff sl\\
  join(slice(l)) &\iff l\\
  repl(drop(sl)) &\iff sl\\
  drop(repl(l))  &\iff l\\
  load(store(l,r)) & \iff l\\
  store(load(ll), rank(ll)) & \iff ll
\end{array}$

Another important set of primitives rely on the commutativity of computation with tensor layout changes. Specifically:

$\begin{array}{ll}
  map(f, repl(l)) &\iff repl(map(f, l))\\
  map(f, drop(rl)) &\iff drop(map(f, rl))\\
  map(f, join(sl)) &\iff join(map(f, sl))\\
  map(f, slice(l)) &\iff slice(map(f, l))\\
  map(f, load(ll)) &\iff load(map(f, ll))\\
  map(f, store(l, r)) &\iff store(map(f, l), r)
\end{array}$

Finally, we use function composition as a way to fuse functions. $$map(f, map(g, l)) \iff map(f \circ g, l)$$

## Rewrite Rules

We use the identities to explain four useful rewrite rules.

### Split/Fuse Communication Rewrite rules

A source sequence of one communication collective can be *split* into a target sequence containing two or more communication collectives. Conversely, a source sequence with more than one communication collectives can be *fused* into a target sequence containing a single communication collectives. We focus on split/fuse rules for AllReduce, which can be split and fused in two different ways.

First, AllReduce on a tensor can be rewritten into a target sequence that first performs a ReduceScatter on the tensor to produce tensors and then performs an AllGather computation on these tensors to obtain a tensor. Conversely, a source sequence of ReduceScatter and AllGather can be rewritten into a single AllReduce. Our functional primitives expresses these rewrite rules. Similarly, by applying $slice$ on the output of $reduce$ and then replication using $repl$ and $join$, we get ReduceScatter and AllGather. Hence, in terms of functional primitives this split/fuse rewrite rule is represented as: $$repl(join(slice(reduce(l, f))) \iff repl(reduce(l, f))$$

Second, we can split AllReduce on a tensor into a sequence of Reduce and Broadcast, which first reduces the tensor on one of the ranks in `WORLD` and then broadcasts this tensor to all ranks in `WORLD`. Conversely, a source sequence of Reduce and Broadcast can be rewritten to into a single AllReduce. Using functional primitives this rewrite rule is represented as: $$repl(load(store(reduce(l, f), r))) \iff repl(reduce(l, f))$$

### Commutativity Rewrite rules

A source sequence can be *reordered* to create a target sequence, which specify new execution order of computations of the source. We define two commutativity rewrite rules. The first rule takes a source sequence where a computation is performed on the output of AllGather. This source sequence can be converted to a target sequence, where the computation is performed on the input of AllGather and the AllGather is performed on the output of the computation. Since input of AllGather is a tensor, the computation is also performed on the tensor. Furthermore, the converse of the above reorder is also possible. In the form of our functional primitives, this rule can be written as: $$map(f, repl(join(l))) \iff repl(join(map(f, l)))$$ The second rule reorders a source sequence where a computation is performed on the output of Broadcast into a target sequence where computation is performed on the input of Broadcast and Broadcast is performed on the output of the computation. In the form of functional primitives, this rule can be written as: $$map(f, repl(load(l)) \iff repl(load(map(f, l)))$$ Commutativity rules are defined only for AllGather and Broadcast because these two collectives changes the layout of a tensor but other collectives also perform computations.

### Fuse Computations Rewrite Rules

In general, many computations can be fused into a single one. Existing DSLs [@halide; @lift-cgo17; @lift-cgo18; @distributed-halide], fuse two computations into a single one to exploit memory locality. We can represent such fusion using function composition ($\circ$) $$map(f, map(g, l)) \iff map(f \circ g, l)$$

### Fuse Computation-Communication Rewrite Rule

[CoCoNet]{.smallcaps} contains fused communication collectives, that fuses the computation acting on the output with the communication that produces it. These collectives decrease the redundant reads and writes that come when a communication collective writes its output to global memory buffer and then the computation reads from that buffer. Instead, in fused collectives the output element are passed through registers, removing expensive memory traffic. Table [2](#tab:fused-comm-collectives){reference-type="ref" reference="tab:fused-comm-collectives"} presents the fused communication collectives supported in [CoCoNet]{.smallcaps} and their representation as functional primitives. For each collective, the computation $g$ is fused at a place, which is the first time in the collective when the output is produced. For example, in FusedReduce$g$ is performed over the output of $reduce$ by one rank because the output is produced for first time after $reduce$. Similarly, in FusedAllReduce$g$ is performed on the output of $reduce$ and not on the output of $repl$.

We can rewrite sequences of computation and communication collectives into a single fused communication collective. One such rewrite rule fuses the computation on ReduceScatter and the AllGather on the output of computation in FusedAllReduce. We can represent this rewrite rule in the functional primitives as: $$\begin{aligned}
  repl(join(slice(map(g, reduce(l, f))))) \iff \\ repl(map(g, reduce(l, f)))\end{aligned}$$ [CoCoNet]{.smallcaps} contains all such transformations based on these rewrite rules.

::: {#tab:fused-comm-collectives}
  -------------------- ---------------------------------- -- -- --
                                                                
  Collective           Functional Primitives                    
  FusedReduce          $store(map(g, reduce(l, f), r))$         
  FusedBroadcast       $repl(map(g, l))$                        
  FusedAllReduce       $repl(map(g, reduce(l, f)))$             
  FusedReduceScatter   $slice(map(g, reduce(l, f)))$            
  FusedAllGather       $repl(join(map(g, l)))$                  
  -------------------- ---------------------------------- -- -- --

  : [CoCoNet]{.smallcaps}'s Fused Communication Collectives expressed using functional primitives. Each fused collective can have one or more computations fused in it. [\[tab:fused-comm-collectives\]]{#tab:fused-comm-collectives label="tab:fused-comm-collectives"}
:::

#### Semantics Non-Preserving

Like transferring delta in 16-bits.

Based on these rewrite rules, [CoCoNet]{.smallcaps} provides two transformations that can be applied to one or more stages.

# The [CoCoNet]{.smallcaps} Code Generator {#sec:runtime}

[CoCoNet]{.smallcaps} generates CUDA kernels for computation and communication operations for running on a distributed system with NVIDIA GPUs. For each operation, [CoCoNet]{.smallcaps} either generates (i) a call to a collective communication operation, (ii) a CUDA kernel for fused computations, (iii) a CUDA kernel for fused-collective communications (Section [6.2](#sec:code-gen-fused){reference-type="ref" reference="sec:code-gen-fused"}), or (iv) CUDA kernels for overlapping of communication and computation operations (Section [6.3](#sec:overlap-impl){reference-type="ref" reference="sec:overlap-impl"}). Moreover, [CoCoNet]{.smallcaps} generates code for performing operations on multiple non-contiguous tensors (Section [6.4](#sec:scattered-tensors){reference-type="ref" reference="sec:scattered-tensors"}). After generating CUDA kernels, [CoCoNet]{.smallcaps} traverses the program's DFG to generate kernel calls. [CoCoNet]{.smallcaps} wraps generated programs as custom operators and integrates them into PyTorch, so that, applications like Megatron-LM can invoke them directly (Section [6.5](#sec:pytorch-integration){reference-type="ref" reference="sec:pytorch-integration"}). We now discuss how [CoCoNet]{.smallcaps} adapts NVIDIA Collective Communication Library (NCCL), a widely-used hand-optimized high performance communication library, into a runtime to execute above CUDA kernels.

## NCCL Architecture {#sec:nccl-arch}

NCCL communicates data stored in the global memory of one GPU to a memory location on another GPU using CUDA kernels. NCCL's CUDA kernels perform communication by directly copying data from memory of one GPU to another GPU using GPUDirect Remote Data Memory Access [@gpudirect]. NCCL's architecture defines four key properties: (i) topology, (ii) protocols, (iii) channels, and (iv) threads in a thread block of the CUDA kernel. NCCL automatically sets key configuration values for these properties based on the size of the input buffer, network architecture, and the size of `WORLD`. To ensure good performance, [CoCoNet]{.smallcaps}'s code generation must carefully reconfigure these properties when extending NCCL to custom communication and computation. We now provide a high level overview of these properties.

**Topology** NCCL creates logical topologies, such as ring and tree, over the underlying interconnect network.

**Channels** NCCL maps copies of a logical topology on the underlying interconnect network. Each copy is called a channel and is assigned to one CUDA thread block.

**Protocols** NCCL sends data using one of the three protocols: `LL`, `LL128`, and `Simple`. These protocols make different tradeoffs between latency and bandwidth based on the type of inter-node synchronization used: `LL` has the lowest latency and `Simple` provides the highest bandwidth.

**Number of Threads** NCCL sets a fixed number of threads for each channel (and thread block). NCCL's kernels have high register usage, which limits the number of thread blocks per SM to one.

**NCCL Workflow** After determining the topology, protocol, number of channels, and number of threads, NCCL calls its CUDA kernel for communication. Each collective communication has three levels of tiling to fully utilize the massive parallelism of GPUs. Data is first divided into *buffer tiles* equal to the size of the communication buffer. Each buffer tile is further divided among all ranks and channels to obtain *chunks*. Each channel communicates a chunk of data at a time. The *threads* in channels copy elements in and out of the buffers and apply reduction operations (`sum`, `min`, `max`) if needed. We now present details about [CoCoNet]{.smallcaps}'s code generation.

## Fused Collective Communications {#sec:code-gen-fused}

Fused Collective Communication extends NCCL's existing kernels to enable arbitrary pointwise computations and reductions (i.e., beyond `min`, `max`, and `sum`). We inspected more than 10K lines of code in NCCL to identify where computations can be added to pass intermediate values from communication to fused computations directly through registers. [CoCoNet]{.smallcaps} supports fusion of both pointwise operations and reductions into NCCL collectives.

Each NCCL protocol utilizes a different mechanism for communication and [CoCoNet]{.smallcaps} generates code for all of them. The important features of a protocol are the pack type (64-bit for `LL`, 128-bit for `LL128` and `Simple`) and the load/store access pattern (shared memory for LL128, global memory for `LL` and `Simple`). [CoCoNet]{.smallcaps} generates template code for all element types in NCCL, and dispatches accordingly at runtime. There are some subtleties in the code generation worth discussing:

**Mixed Precision** When the element types of computations and the input tensors are different, [CoCoNet]{.smallcaps} finds the largest element type and based on the pack type of the protocol calculates how many elements can be loaded at once. All code will then be generated to operate on these many elements.

**Sliced Tensor** When a sliced tensor is used by a fused collective communication, all memory accesses performed need to be mapped to elements of the sliced tensor. [CoCoNet]{.smallcaps} generates code that produces this mapping. To perform an AllGather on sliced tensors, the inverse of this mapping is produced.

**Tensor Reduction** To reduce a tensor, each rank reduces locally and do an AllReduce. This AllReduce reuses already established connections among ranks in the surrounding communication kernel to avoid extra startup latency.

![Workflow of overlap on **rank 0**. Rank 0 starts with chunk 0. ](figures/overlap-example-rank-0.pdf){width="\\linewidth"}

![Workflow of overlap on **rank 1**. Rank 1 starts with chunk 1. ](figures/overlap-example-rank-1.pdf){width="\\linewidth"}

## Overlapping of Communication and Computation {#sec:overlap-impl}

Overlapping of computation and communication has been studied in the context of executing stencil computations in a distributed system [@Barigou2017; @6799131; @10.1145/2503210.2503289; @7573826; @distributed-halide; @KOZIRIS20031138; @7336201; @10.1145/1810085.1810091; @8121995; @10.1007/978-3-319-58667-0_18; @sc20:pencil]. These works use non-blocking MPI operations to communicate data and simultaneously perform computations on CPUs. A similar approach for overlapping of computation and communication operations for a GPU workload would involve dividing all operations into sub-operations and ensuring dependency between sub-operations using CUDA streams. However, this approach would provide sub-optimal performance because each sub-operation is performed on only a part of data, which leads to in-efficient computation and under-utilization of communication bandwidth.

Figure [\[fig:workflow-overlap\]](#fig:workflow-overlap){reference-type="ref" reference="fig:workflow-overlap"} shows how the fine-grained overlapping of [CoCoNet]{.smallcaps} addresses this issue using the example of a MatMul followed by a ring AllReduce. First, it schedules the MatMul kernel (based on CUTLASS [@cutlass]) to produce chunks in the same order as the AllReduce consumes them. Here, the $n^{\text{th}}$ rank sends chunks in the order starting from the $n^{\text{th}}$ chunk. Hence, the MatMul kernel on $n^{\text{th}}$ rank produces chunks in the same order. Second, [CoCoNet]{.smallcaps} invokes both kernels only once on different streams and synchronizes the AllReduce with the MatMul using an efficient fine-grained spin-lock on a memory buffer to ensure that the AllReduce wakes up as soon as the MatMul produces a chunk. Third, to provide opportunities to tune the 2-D tile sizes of the MatMul kernel, [CoCoNet]{.smallcaps} generates a 2-D AllReduce kernel that communicates 2-D chunks, while NCCL AllReduce only supports 1-D continuous chunk.

The example in Figure [\[fig:workflow-overlap\]](#fig:workflow-overlap){reference-type="ref" reference="fig:workflow-overlap"} works as follows. At T = , all ranks invoke MatMul and AllReduce kernels. On rank 0, after computing chunk 0, the MatMul kernel wakes the AllReduce kernel at T = , which starts communicating chunk 0. While on rank 1, at T = the MatMul kernel wakes the AllReduce kernel to communicate chunk 1. Concurrently, both MatMul kernels compute their corresponding next chunk. At T = , MatMul kernels finished computing chunk 1 on rank 0 and chunk 2 on rank 1 and wakes up corresponding AllReduce kernels to communicate these chunks. This process continues until all chunks are processed.

This process allows the MatMul kernel and AllReduce to be overlapped in a fine-grained manner, which reduces the startup latency of AllReduce. Since AllReduce communicates on the same chunk sizes, it achieves maximum communication bandwidth. Furthermore, the MatMul kernel achieves maximum efficiency because the kernel is invoked on the full matrix size. Figure [1](#fig:matmul-overlap-intro){reference-type="ref" reference="fig:matmul-overlap-intro"} shows that this overlapping provides up to 1.36$\times$ better performance and hides more than 80% of the MatMul time.

## Operations on Scattered Tensors {#sec:scattered-tensors}

In data parallelism, communication and computation occur on different layers of widely different sizes. Since machine learning frameworks allocate parameters and gradients of layers in non-contiguous buffers, gradients are copied to a large buffer to avoid launching multiple AllReduce operations.

[CoCoNet]{.smallcaps} supports generating a single kernel for both computation and communication operations acting on non-contiguous tensors. In this section, we show how [CoCoNet]{.smallcaps} modifies NCCL to generate a single communication kernel for scattered tensors. This code generation is non-trivial because NCCL has several design decisions based on the assumption that it is communicating a single contiguous buffer. For example, each thread of a NCCL channel copies only a few elements in each iteration, and hence indexing the correct tensor at a particular offset requires a linear search through all non-contiguous tensors, which can lead to significant overhead. [CoCoNet]{.smallcaps} solves this problem by first dividing each tensor into buckets of size at most 2$^{10}$ elements and then assigning buckets to warps in a round-robin manner. This mechanism allows each thread to quickly find the offset in a tensor, since a warp can directly index in its assigned bucket. [CoCoNet]{.smallcaps} pre-calculates the number of buckets that belong to the same contiguous buffer and calculates the offset for all of them once.

The process of breaking each tensor to buckets has computation overhead and extra memory requirements. Since this bucketing is done only once on the CPU and training tasks run for thousands of iterations on the same tensors, the computation overhead is negligible. Each bucket is represented by a pair of 64-bit tensor address and a 32-bit offset into the associated tensor, leading to $12 \times \left \lceil \frac{N}{2^{10}}\right \rceil$ bytes of extra memory for a tensor with $N$ elements. However, this memory overhead is negligible for large models. For example, for BERT model with 334M elements, the memory requirement is 0.6%. Table [3](#tab:scattered-tensors){reference-type="ref" reference="tab:scattered-tensors"} shows that the overhead of scattered tensors is insignificant over contiguous tensors.

::: {#tab:scattered-tensors}
   **Optimizer**   **Scattered Tensor**  **Single Tensor**
  --------------- ---------------------- -------------------
       Adam              33.89 ms        33.21 ms
       LAMB              37.04 ms        36.71 ms

  : Time to perform parameter update of all 360 tensors of BERT using Adam/LAMB on 256 Tesla V100 GPUs with scattered tensors implementation and a single contiguous tensor of size equal to the sum of size of all tensors.[\[tab:scattered-tensors\]]{#tab:scattered-tensors label="tab:scattered-tensors"}
:::

## PyTorch Integration {#sec:pytorch-integration}

We integrated [CoCoNet]{.smallcaps} generated code as a function to PyTorch's `torch.distributed` module. This design allows us to re-use the logic for initializing NCCL and provide compatibility with models already using `torch.distributed`. We added wrapper functions for calling [CoCoNet]{.smallcaps} generated operations. These wrapper functions prepare the arguments for calling [CoCoNet]{.smallcaps}'s operations, which includes pre-calculating pointers to the buckets for scattered tensors and clearing the spin-lock buffers for overlapping. Machine learning models can invoke [CoCoNet]{.smallcaps} functions using PyTorch.

# Evaluation {#sec:experiments}

This section evaluates the effectiveness of [CoCoNet]{.smallcaps} through standalone experiments and end-to-end distributed machine learning scenarios of data, model, and pipeline parallelism.

Our experiments are performed on a cluster of 16 NVIDIA DGX-2 nodes where each node contains dual 24-core Intel Xeon CPUs and 16 NVIDIA Tesla V100 (32GB) GPUs. Each GPU within a node is connected to six NVSwitches with six NVLinks (25 GBps per NVLink). Nodes are connected with 8 non-blocking EDR InfiniBand (100 Gbps) network. All nodes run Ubuntu 20.04, CUDA 11.3, cuDNN 8.2 and PyTorch 1.10.

## Data Parallel Training

In data parallelism, communication involves an AllReduce of gradients among all ranks. The output is used by the optimizer to update the model parameters. We evaluate [CoCoNet]{.smallcaps} generated code for two widely-used optimizers, Adam and LAMB. All our experiments in this section were performed on all 16 DGX-2 nodes in our cluster.

### Standalone Experiments

We first perform standalone experiments to explore different [CoCoNet]{.smallcaps} schedules over a range of input tensors from $2^{10}$ to $2^{30}$ elements. The autotuner generates and executes implementations with different configurations, including all NCCL protocols and all channels from 2 to 64. For each tensor, the autotuner reports the best average result of 1000 iterations.

**Baselines** The baselines perform parameter update by first doing AllReduce over gradients and then call FusedAdam or FusedLAMB from NVIDIA Apex [@apex]. Both FusedAdam and FusedLAMB fuses all the parameter update computations.

**[CoCoNet]{.smallcaps} Schedules** The autotuner generates following three schedules of Adam and LAMB by applying different [CoCoNet]{.smallcaps} transformations for each input size and reports the best schedule to the user for each input size:

1.  **AR-Opt** (Opt = Adam/LAMB) refer to the traditional parameter update technique, i.e., an AllReduce over gradients and then each GPU individually performs the optimizer computation. These schedules fuse all computations into a single kernel, thereby simulating the baseline implementations of FusedAdam and FusedLAMB.

2.  **GShard-Eq** or **RS-Opt-AG** (Opt = Adam/LAMB) are generated from *AR-Opt* by first splitting the AllReduce into ReduceScatter and AllGather, and then reordering AllGather with the fused optimizer computations. Hence, these schedules distribute parameter update across all ranks, similar to GShard [@gshard] and ZeRO [@zero]. Since GShard does not support execution on GPUs, we refer to this schedule as GShard-Eq in our results.

3.  **fuse(RS-Opt-AG)** (Opt = Adam/LAMB) are generated by fusing all operations of *RS-Opt-AG* into FusedAllReduce.

#### Results

![Mixed-precision Adam. AR-Adam(AR-A) runs best till 2$^{16}$. fuse(RS-A-AG) represents fuse(RS-Adam-AG) and runs best after 2$^{17}$.](figures/results-adamfp16-256-gpus.pdf){#fig:bandwidth64GPUs:adam width="\\linewidth"}

![Mixed-precision LAMB. AR-LAMB(AR-L) runs best till 2$^{16}$. fuse(RS-L-AG) represents fuse(RS-LAMB-AG) and runs best after 2$^{17}$.](figures/results-lambfp16-256-gpus.pdf){#fig:bandwidth64GPUs:lamb width="\\linewidth"}

Figure [\[fig:bandwidth64GPUs\]](#fig:bandwidth64GPUs){reference-type="ref" reference="fig:bandwidth64GPUs"} shows the speedup of [CoCoNet]{.smallcaps} schedules over the baseline for several tensor sizes. The results are shown for mixed-precision [@mixed-precision-training] using Float 16, and the results for Float 32 are qualitatively similar. In these figures, UB represents the cost of AllReduce alone without doing any computation, and thus is the upper bound of possible speedups.

Even though the *AR-Opt* schedules emulate the baseline implementations, they are faster on smaller tensors. This is because the baseline implementations perform additional preprocessing to optimize the amount of thread-parallelism and instruction-level parallelism per invocation. While this preprocessing cost hurts smaller tensors, its benefit shows up for larger tensors where *AR-Opt* performs worse.

Since *GShard-Eq* and *fuse(RS-Opt-AG)* schedules distribute the optimizer computation, they perform better than the baseline for large tensors. The performance of *fuse(RS-Opt-AG)* shows the advantage of fusing computation and communication kernels as these schedules achieve near optimal speedups for large tensors. These schedules are respectively 13% and 14% faster than GShard-Eq for Adam and LAMB.

For smaller tensor sizes, multiple kernel calls are required for GShard-Eq schedules significantly hurt performance. Interestingly, *fuse(RS-Opt-AG)* schedules are slower than *AR-Opt* schedules for smaller tensor sizes though they require one less kernel call because the fused kernels have a higher register usage, thereby restricting the thread-level parallelism. This demonstrates that the fusion of communication and computation is not always a good idea.

Table [\[tab:loc:data-parallel\]](#tab:loc:data-parallel){reference-type="ref" reference="tab:loc:data-parallel"} shows that the lines of generated code for each schedule are significantly more than the implementation in [CoCoNet]{.smallcaps} and the autotuner explored all schedules in 10 seconds. In summary, [CoCoNet]{.smallcaps} provides performance improvements over baselines with fewer lines of code. The *AR-Opt* and the *fuse(RS-Opt-AG)* reach close to optimal performance for smaller and larger tensors respectively. This amounts to a speedup of 1.2$\times$ to 1.7$\times$ for Adam and 1.35$\times$ to 2.0$\times$ for LAMB. There is no schedule that performs best for all sizes, which demonstrates the need for the autotuner.

![FP32 Adam on 2 GPUs](figures/results-adam-2-gpus.pdf){width="\\columnwidth"}

![FP32 Adam on 4 GPUs](figures/results-adam-4-gpus.pdf){width="\\columnwidth"}

![FP32 Adam on 8 GPUs](figures/results-adam-8-gpus.pdf){width="\\columnwidth"}

![FP32 Adam on 16 GPUs](figures/results-adam-16-gpus.pdf){width="\\columnwidth"}

![FP32 Adam on 32 GPUs](figures/results-adam-32-gpus.pdf){width="\\columnwidth"}

![FP32 Adam on 64 GPUs](figures/results-adam-64-gpus.pdf){width="\\columnwidth"}

![MixedPrecision Adam on 2 GPUs](figures/results-adamfp16-2-gpus.pdf){width="\\columnwidth"}

![MixedPrecision Adam on 4 GPUs](figures/results-adamfp16-4-gpus.pdf){width="\\columnwidth"}

![MixedPrecision Adam on 8 GPUs](figures/results-adamfp16-8-gpus.pdf){width="\\columnwidth"}

![MixedPrecision Adam on 16 GPUs](figures/results-adamfp16-16-gpus.pdf){width="\\columnwidth"}

![MixedPrecision Adam on 32 GPUs](figures/results-adamfp16-32-gpus.pdf){width="\\columnwidth"}

![MixedPrecision Adam on 64 GPUs](figures/results-adamfp16-64-gpus.pdf){width="\\columnwidth"}

![FP32 LAMB on 2 GPUs](figures/results-lamb-2-gpus.pdf){width="\\columnwidth"}

![FP32 LAMB on 4 GPUs](figures/results-lamb-4-gpus.pdf){width="\\columnwidth"}

![FP32 LAMB on 8 GPUs](figures/results-lamb-8-gpus.pdf){width="\\columnwidth"}

![FP32 LAMB on 16 GPUs](figures/results-lamb-16-gpus.pdf){width="\\columnwidth"}

![FP32 LAMB on 32 GPUs](figures/results-lamb-32-gpus.pdf){width="\\columnwidth"}

![FP32 LAMB on 64 GPUs](figures/results-lamb-64-gpus.pdf){width="\\columnwidth"}

![MixedPrecision LAMB on 2 GPUs](figures/results-lambfp16-2-gpus.pdf){width="\\columnwidth"}

![MixedPrecision LAMB on 4 GPUs](figures/results-lambfp16-4-gpus.pdf){width="\\columnwidth"}

![MixedPrecision LAMB on 8 GPUs](figures/results-lambfp16-8-gpus.pdf){width="\\columnwidth"}

![MixedPrecision LAMB on 16 GPUs](figures/results-lambfp16-16-gpus.pdf){width="\\columnwidth"}

![MixedPrecision LAMB on 32 GPUs](figures/results-lambfp16-32-gpus.pdf){width="\\columnwidth"}

![MixedPrecision LAMB on 64 GPUs](figures/results-lambfp16-64-gpus.pdf){width="\\columnwidth"}

::: {#tab:loc:pipeline-parallel}
  ----------------------- ----- ---- --
  **Schedule**                       
  CUDA                               
  [CoCoNet]{.smallcaps}              
  Time                               
  *AR-Adam*                  16   12 
  *RS-Adam-AG*               24   16 
  *fuse(RS-Adam-AG)*        150   17 
  *AR-LAMB*                  80   15 
  *RS-LAMB-AG*              140   17 
  *fuse(RS-LAMB-AG)*        220   18 
  ----------------------- ----- ---- --

  : Pipeline Parallel Transformer Layer[\[tab:loc:pipeline-parallel\]]{#tab:loc:pipeline-parallel label="tab:loc:pipeline-parallel"}
:::

::: {#tab:loc:pipeline-parallel}
  ------------------------ -------------- ---- --
  **Schedule**                                 
  CUDA                                         
  [CoCoNet]{.smallcaps}                        
  Time                                         
  *MM-AR-C*                            20   10 
  *MM-RS-C-AG*                        140   13 
  *ol(MM,fuse(RS-C-AG))*     $\approx$ 2k   14 
  ------------------------ -------------- ---- --

  : Pipeline Parallel Transformer Layer[\[tab:loc:pipeline-parallel\]]{#tab:loc:pipeline-parallel label="tab:loc:pipeline-parallel"}
:::

::: {#tab:loc:pipeline-parallel}
  -------------------------- -------------- ---- --
  **Schedule**                                   
  CUDA                                           
  [CoCoNet]{.smallcaps}                          
  Time                                           
  *AR-P2P-C-AG*                          20   10 
  *RS-P2P-C-AG*                         140   13 
  *ol(RS,fuse(P2P-C),AG))*     $\approx$ 2k   14 
  -------------------------- -------------- ---- --

  : Pipeline Parallel Transformer Layer[\[tab:loc:pipeline-parallel\]]{#tab:loc:pipeline-parallel label="tab:loc:pipeline-parallel"}
:::

### Integeration with BERT {#sec:experiments:bert}

We use [CoCoNet]{.smallcaps} generated optimizers to train three large BERT models from NVIDIA [@nvbert]. We use mixed precision training with both Adam with 8192 global batch size and LAMB with 65536 global batch size.

**Baselines** We consider three baselines for this experiment:

-   **NV BERT** [@nvbert] is the NVIDIA BERT Script. It copies gradients of each layer into a single buffer, calls AllReduce on the buffer, and copy back the results into original gradients. Finally, it calls either FusedAdam or FusedLAMB.

-   **PyTorch DDP** [@pytorch-ddp] stores all gradients in buckets of 25MB and overlaps the AllReduce on each gradient bucket with computations during training. After reducing all gradients it calls FusedAdam or FusedLAMB.

-   **ZeRO** [@zero] copies gradients into a contiguous buffer and then distributes Adam's computation similar to *RS-Opt-AG* schedules above. The ZeRO implementation of LAMB does not support distributing optimizer state among GPUs because significant engineering efforts are required to implement reduction over distributed gradients and weights in a distributed LAMB implementation [@deepspeed490].

**[CoCoNet]{.smallcaps} Integeration** We integrated the scattered tensors implementation of *fuse(RS-Opt-AG)* schedule for both Adam and LAMB in PyTorch. These implementations provide three benefits over the baselines: (i) the scattered tensor implementation avoids copying all gradients to a single buffer and allocating this buffer, (ii) the fused schedule performs best for the tensor sizes used in BERT, and (iii) the fused schedule distributes memory of optimizer state among all GPUs.

  ----------------- ---------------------- --------- ------------- ------ ----------------------- -------------- -------------- -------------- -- -- --
    **Optimizer**    **\# of Parameters**                                                                                                            
   (lr)3-6 (lr)7-9                          NV BERT   PyTorch DDP   ZeRO   [CoCoNet]{.smallcaps}     NV BERT      PyTorch DDP        ZeRO            
                            336 M             32          32         32             32             1.18$\times$   1.22$\times$   1.10$\times$        
                            1.2 B              8           8         32             32             1.53$\times$   1.52$\times$   1.10$\times$        
                            3.9 B             OOM         OOM        8               8                  --             --        1.22$\times$        
                             336M             64          64         64             128            1.20$\times$   1.20$\times$   1.15$\times$        
                             1.2B              8           8         8              64             1.67$\times$   1.68$\times$   1.64$\times$        
                             3.9B             OOM         OOM       OOM              8                  --             --             --             
  ----------------- ---------------------- --------- ------------- ------ ----------------------- -------------- -------------- -------------- -- -- --

**Results** Table [\[tab:bert-results\]](#tab:bert-results){reference-type="ref" reference="tab:bert-results"} shows the speedup provided by [CoCoNet]{.smallcaps} in training three BERT models over baselines. For Adam optimizer, [CoCoNet]{.smallcaps} provides speedup over all baselines in training BERT 336M because [CoCoNet]{.smallcaps}'s fused schedules perform better than other implementations. [CoCoNet]{.smallcaps} provides even higher speedup on larger BERT models because the fused schedules decrease memory usage by distributing Adam's state over all GPUs, which improves the efficiency of matrix multiplication GPU kernels by enabling higher batch size per iteration. For example, for BERT 1.2B [CoCoNet]{.smallcaps} provides 1.53$\times$ speedup over NV BERT and PyTorchDDP because of the optimized fused schedule and higher batch size enabled by [CoCoNet]{.smallcaps}. On 3.9B parameter model, NV BERT and PyTorch go Out of Memory. ZeRO also supports higher batch size for BERT 1.2B and 3.9B but [CoCoNet]{.smallcaps} still gives speedup because of the advantages of scattered tensor implementation of fused schedules.

Results for LAMB are similar. [CoCoNet]{.smallcaps} provides up to 1.64$\times$ speedup over all baselines. For LAMB, the speedup over ZeRO is higher than Adam because ZeRO does not support distributing LAMB optimizer state, and hence, supports smaller batch sizes as compared to [CoCoNet]{.smallcaps}.

In summary, [CoCoNet]{.smallcaps} significantly improves data-parallel training time of BERT models. [CoCoNet]{.smallcaps}'s schedules can be automatically generated and [CoCoNet]{.smallcaps}'s scattered tensors implementation can support a wide range of optimizers. Not only does the fusion of computation and communication lead to performance improvement over the baselines of PyTorch DDP and ZeRO, it also decreases the memory usage, which helps in increasing the batch size to train models faster.

## Model Parallelism

Megatron-LM [@megatronlm] uses a model parallel approach for inference and training of transformer models, such as BERT [@bert] and GPT-2 [@gpt-2]. A transformer layer contains a self-attention block and a multi-layer perceptron (MLP) block. Last few operations of a self-attention block are the same computations as shown in Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"}. An MLP block's last operations are similar to Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"} with the input tensor and weight sizes as $[B, S, 4 \times H]$ and $[4 \times H, H]$ ($B$, $S$, and $H$ are batch size, sequence length, and hidden size, respectively). Since model parallelism is applied within one node, all experiments in this section are performed on a single NVIDIA DGX-2 node.

### Standalone Experiments

We first perform standalone experiments to evaluate different schedules generated by the autotuner. We compare following schedules for model parallel self-attention code of Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"} and similar operations of multi-layer perceptron:

1.  **Megatron-LM** is the baseline implementation of Figure [\[fig:traditional-mp\]](#fig:traditional-mp){reference-type="ref" reference="fig:traditional-mp"} in Megatron-LM.

2.  **MM-AR-C** improves the Megatron-LM implementation by fusing all pointwise computations into one kernel.

3.  **GShard-Eq** or **MM-RS-C-AG** uses the same techniques as GShard. It is generated from *MM-AR-C* by splitting the AllReduce into a ReduceScatter and an AllGather, and reorders AllGather with computations. This schedule represents GShard because GShard is not available for GPUs.

4.  **ol(MM,fuse(RS-C-AG)** is generated from the previous schedule by fusing the ReduceScatter, computation, and AllGather into a FusedAllReduce and then overlapping it with the MatMul. The autotuner returned this as the best schedule and hence represents [CoCoNet]{.smallcaps} in our results.

![Times of [CoCoNet]{.smallcaps}'s schedules of model parallel self-attention and multi-layer perceptron of GPT-2 normalized to corresponding Megatron-LM's implementation. [\[fig:matmul-overlap\]]{#fig:matmul-overlap label="fig:matmul-overlap"}](figures/matmul-overlap-16-gpus_combined.pdf){#fig:matmul-overlap width="\\linewidth"}

**Results** We evaluate these schedules with sizes of GPT-2 8.3 Billion parameter model (i.e., $S=1024$, $H=3072$) for 8 and 16 batch sizes. Figure [7](#fig:matmul-overlap){reference-type="ref" reference="fig:matmul-overlap"} shows the times of all schedules normalized to the time of implementation in Megatron-LM. *MM-AR-C* schedule provides speedup over Megatron-LM's implementation because this schedule fuses all pointwise computations in a single GPU kernel. GShard-Eq (*MM-RS-C-AG*) provides 1.15$\times$ to 1.29$\times$ speedup over Megatron-LM by distributing computations on all ranks. [CoCoNet]{.smallcaps}'s best schedule (*ol(MM,fuse(RS-C-AG))*) provides 1.42$\times$ to 1.70$\times$ speedup over Megatron-LM and 1.21$\times$ to 1.34$\times$ over GShard-Eq because it overlaps FusedAllReduce with the matrix multiplication. Table [\[tab:loc:model-parallel\]](#tab:loc:model-parallel){reference-type="ref" reference="tab:loc:model-parallel"} shows that the lines of generated CUDA code for each schedule are significantly more than the implementation in [CoCoNet]{.smallcaps} and the autotuner explored all schedules in 12 seconds.

### Integration with Megatron-LM {#sec:experiments:model-parallel:integeration}

After integrating [CoCoNet]{.smallcaps}'s overlap schedule in Megatron-LM, we found that [CoCoNet]{.smallcaps} improved inference times of BERT 3.9B parameter model by 1.51$\times$ and GPT-2 8.3B parameter model by 1.48$\times$. Hence, overlapping matrix multiplication with fused collective communication significantly improves inference times.

## Pipeline Parallelism

[CoCoNet]{.smallcaps} can decrease inference times in pipeline parallelism by fusing computation and communication and overlapping multiple communication operations. We evaluate [CoCoNet]{.smallcaps} on computations of model and pipeline parallelism in Megatron-LM for GPT-2 8.3B and GPT-3 175B parameter models. A transformer layer contains several operations but the operations of interest for this experiment are presented in Figure [\[fig:traditional-p2p\]](#fig:traditional-p2p){reference-type="ref" reference="fig:traditional-p2p"}. All experiments in this section are performed on all 16 NVIDIA DGX-2 nodes.

### Standalone Experiments

We first perform standalone experiments to evaluate different schedules generated by the autotuner. We compare the following schedules for pipeline parallelism code of Figure [\[fig:traditional-p2p\]](#fig:traditional-p2p){reference-type="ref" reference="fig:traditional-p2p"}:

1.  **Megatron-LM** is the implementation of Figure [\[fig:traditional-p2p\]](#fig:traditional-p2p){reference-type="ref" reference="fig:traditional-p2p"} in Megatron-LM and serves as a baseline for this experiment.

2.  **AR-C-P2P-AG** is generated by slicing the output of AllReduce to perform sliced P2P sends and computations, and finally an AllGather to collect the output of computations. This schedule improves over Megatron-LM by slicing the P2P sends and fusing all the computations.

3.  **GShard-Eq** or **RS-C-P2P-AG** is generated from the previous schedule by splitting the AllReduce into a ReduceScatter and an AllGather, then reordering the AllGather with P2P send and computations. Since this schedule is similar to GShard, it represents GShard-Eq in our results.

4.  **ol(RS,fuse(C-P2P),AG)** is generated from previous schedule by fusing computations with P2P sends, and overlapping all three communication operations (Figure [4](#fig:p2p-fusion-3){reference-type="ref" reference="fig:p2p-fusion-3"}). This schedule is returned by the autotuner as the best schedule and hence, represents [CoCoNet]{.smallcaps} in our results.

**Results** Figure [8](#fig:pipeline-overlap){reference-type="ref" reference="fig:pipeline-overlap"} shows the breakdown of each operation with one transformer layer assigned to each node. The sequence length ($S=2048$) and the hidden size ($H=12288$) are of GPT-3 175B model. [CoCoNet]{.smallcaps}'s best schedule *ol(RS,fuse(C-P2P),AG)* is 11.75$\times$--12.21$\times$ faster than Megatron-LM's implementation, 2.84$\times$ faster than *AR-C-P2P-AG*, and 1.66$\times$--1.72$\times$ faster than GShard (*RS-C-P2P-AG*). The speedups are because: (i) sliced P2P reduces cross node communication volume, (ii) fusing communication and computation operations improves memory bandwidth utilization, and (iii) overlapping communication using different connections (NVLink within node and InfiniBand across nodes) improves network bandwidth utilization, while other schedules utilize only one stack at a time. Table [6](#tab:loc:pipeline-parallel){reference-type="ref" reference="tab:loc:pipeline-parallel"} shows that the lines of generated CUDA code for each schedule are significantly more than the implementation in [CoCoNet]{.smallcaps} and the autotuner explored all schedules in 11 seconds.

### Integration with Megatron-LM {#sec:experiments:pipeline-parallel:integeration}

We evaluated the inference throughput of GPT-2 8.3B and GPT-3 175B parameter models by integrating [CoCoNet]{.smallcaps}'s *ol(RS,fuse(C-P2P),AG)* schedule in Megatron-LM. Table [7](#tab:pipeline-results){reference-type="ref" reference="tab:pipeline-results"} shows the speedups achieved by [CoCoNet]{.smallcaps}.

![Times of three schedules for GPT-3 175B in [CoCoNet]{.smallcaps} for pipeline and model parallelism normalized to Megatron-LM's corresponding implementation. [\[fig:pipeline-overlap\]]{#fig:pipeline-overlap label="fig:pipeline-overlap"}](figures/p2p-fusion-break-down-256-cluster_V100.pdf){#fig:pipeline-overlap width="1\\linewidth"}

[CoCoNet]{.smallcaps} significantly improves inference throughput of GPT-3 and GPT-2 due to its fusion and fine-grained overlapping of multiple communication operations.

::: {#tab:pipeline-results}
  ------------------ --------- ---- -------------- --
  Model                                            
  per node                                         
  Micro Batch Size    Speedup                      
  GPT-2 8.3B             5      16   1.77$\times$  
  GPT-3 175B             6      2    1.33$\times$  
  ------------------ --------- ---- -------------- --

  : Speedup in inference by [CoCoNet]{.smallcaps}'s implementation of pipeline parallelism for GPT-2 and GPT-3. Layers per node were obtained by equally distributing layers on all nodes. To evenly distribute layers of GPT-2, number of layers were increased to the nearest multiple of 16, i.e., 80. [\[tab:pipeline-results\]]{#tab:pipeline-results label="tab:pipeline-results"}
:::

# Related Work {#sec:related}

**Distributed Machine Learning Abstractions** Existing machine learning frameworks [@mxnet; @tensorflow; @jia2014caffe; @pytorch; @sergeev2018horovod] and DSLs [@tvm18; @distributed-halide] provide abstractions for writing distributed machine learning workloads. Similar to [CoCoNet]{.smallcaps}, in these abstractions, a distributed machine learning program takes input tensors, performs operations on tensors, and returns tensors as the output. However, unlike these abstractions, [CoCoNet]{.smallcaps} preserves the layout information for each tensor. The layout information enables [CoCoNet]{.smallcaps} to perform static type checking of each operation, and automatically perform transformations on the program, which is not possible with existing abstractions.

**Distributed Neural Network Training** Several works have improved data-, model-, and pipeline-parallel techniques for both training and inference. Mesh-Tensorflow [@meshtensorflow] and GShard [@gshard] create *shards* of weights and model state that can be split among ranks. Horovod [@sergeev2018horovod] introduced the *Tensor Fusion* optimization that copies all gradients to a single buffer of 64MB, calls AllReduce on the buffer, and then copies the updated value to original gradients. ZeRO [@zero] splits weights and model state among ranks and uses ReduceScatter and AllGather to distribute computation. FlexFlow [@flexflow] performs operator splitting as a way to represent both data-parallelism and model-parallelism, but does not optimize computation with communication. [CoCoNet]{.smallcaps} provides several optimizations over these works that are possible only by breaking the abstraction: (i) scattered tensors that remove extra storage and memory copy operations, (ii) fusion communication collectives, and (iii) novel communication and computation overlapping techniques. PyTorch's DDP [@pytorch-ddp] overlaps AllReduce of gradients with the forward and backward pass. However, unlike [CoCoNet]{.smallcaps}, PyTorch's DDP requires extra memory for overlapping, which can increase training time for very large models [@megatronlm-github] and do not support slicing of optimizer parameter update that significantly decrease memory usage. GPipe [@gpipe], Pipedream [@pipedream], and Narayanan et al. [@narayanan2021efficient] proposed pipeline training to improve model parallelism, by dividing the forward and backward pass into several mini-batches, which are then pipelined across devices. vPipe [@vpipe] improves these works by providing higher GPU utilization. [CoCoNet]{.smallcaps} improves on these works by overlapping inter and intra-node communication operations. BytePS [@osdi20:byteps] utilizes CPU in heterogenous clusters to improve training, which is complementary to [CoCoNet]{.smallcaps}.

**Optimizing Stencil Computations** Prior works have proposed several DSLs and optimizations for data-parallel stencil computations on CPUs, GPUs, and other accelerators. Halide [@halide] and Fireiron [@fireiron] separate the algorithm and schedule, which describes the optimizations like fusion, and loop tiling. TVM [@tvm18] extends Halide for generating optimized compute kernels. [Lift]{.smallcaps} [@lift-cgo18; @lift-cgo17] and PolyMage [@polymage-gpu] automatically optimize stencil computations for a single GPU. Distributed-Halide [@distributed-halide] extends Halide with scheduling primitives that allow distributing parallelizable dimensions of loops. [CoCoNet]{.smallcaps} extends these works to reason about and compose collective communication with computation, which is crucial for distributed machine learning scenarios.

**Overlapping Computation and Communication** State-of-the-art works on overlapping [@Barigou2017; @KOZIRIS20031138; @7336201; @10.1145/1810085.1810091; @10.1007/978-3-319-58667-0_18] use either pipelined execution to overlap communication and computation or non-blocking MPI operations. Pencil [@sc20:pencil] improves upon these works by performing pipelining within a process and supports computations in multiple connected iteration spaces. Several techniques distribute tiles and automatically generate communication [@10.1145/2503210.2503289; @distributed-halide; @8121995]. Basu et. al. [@6799131] uses overlapped tiling in each process to remove communication between processes. Denis and Trahay [@7573826] studied the efficiency of overlap. dCUDA [@dcuda] provides hardware supported overlap. These works for MPI+OpenMP are valid for CPU based stencil computations that require sends and receives to share the halo regions. However, unlike [CoCoNet]{.smallcaps}, these works do not support overlapping between collectives communication and complex computations like convolutions and matrix multiplications. [CoCoNet]{.smallcaps} supports overlapping multiple computation and communication operations on GPUs without an accelerator.

# Conclusion {#sec:conclusion}

This paper introduced [CoCoNet]{.smallcaps}, a language to describe distributed machine learning workloads and optimize them across computation and communication boundary. We show that [CoCoNet]{.smallcaps} generated code significantly improves several training and inference times of large language models. In the future we plan to automate the optimizations through smart search.

# Data Availability Statement

The artifact for this paper [@coconet-artifact] contains the source code of our implementation of [CoCoNet]{.smallcaps} and the benchmarking infrastructure to reproduce all the results in Section [7](#sec:experiments){reference-type="ref" reference="sec:experiments"}.

We thank the reviewers and our shepherd, Tyler Sorensen, for their constructive feedback. This work was partially supported by the National Science Foundation grant CCF-2052696.

# Artifact Appendix

## Abstract

This artifact appendix describes how to reproduce results for standalone experiments in Figure [\[fig:bandwidth64GPUs\]](#fig:bandwidth64GPUs){reference-type="ref" reference="fig:bandwidth64GPUs"},  [7](#fig:matmul-overlap){reference-type="ref" reference="fig:matmul-overlap"}, and  [8](#fig:pipeline-overlap){reference-type="ref" reference="fig:pipeline-overlap"} and integration results in Section [7.1.2](#sec:experiments:bert){reference-type="ref" reference="sec:experiments:bert"}, [7.2.2](#sec:experiments:model-parallel:integeration){reference-type="ref" reference="sec:experiments:model-parallel:integeration"}, and  [7.3.2](#sec:experiments:pipeline-parallel:integeration){reference-type="ref" reference="sec:experiments:pipeline-parallel:integeration"}. This artifact includes the [CoCoNet]{.smallcaps} DSL and compiler, and [CoCoNet]{.smallcaps}'s generated code integrated with PyTorch, Megatron-LM, and NVIDIA Bert. To reproduce the results, the experiments should be executed on a system similar to our experimental system. However, all experiments can be executed on a system with more than one NVIDIA GPUs.

## Artifact Check-list (Meta-information)

-   **Program:** [CoCoNet]{.smallcaps} DSL and compiler written in C++.

-   **Compilation:** A C++ compiler (`g++` or `clang`) to compile [CoCoNet]{.smallcaps}. A C++ compiler with MPI support (`mpicxx`) and CUDA compiler (`nvcc`) to compile generated programs.

-   **Binary:** Each [CoCoNet]{.smallcaps} program compiles to a binary that generates an MPI program containing CUDA kernels.

-   **Data set:** BERT, GPT-2, and GPT-3 training datasets for integration experiments.

-   **Run-time environment:** Ubuntu 20.04 with Python 3.7+, CUDA 11.0+, and OpenMPI 4.0+.

-   **Hardware:** We performed experiments on 16 NVIDIA DGX-2 nodes, i.e., a total of 256 NVIDIA Tesla V100 GPUs. However, the experiments can be executed on any system with two or more GPUs.

-   **Run-time state:** Python, MPI, and CUDA.

-   **Execution:** Use `mpirun` to run the experiments.

-   **Metrics:** Decrease in execution time of benchmarks.

-   **Output:** Execution time of each experiment and [CoCoNet]{.smallcaps} speedup over baselines.

-   **Experiments:** Execution of standalone experiments and training and inference tasks of BERT, GPT-2, and GPT-3 models.

-   **How much disk space required (approximately)?:** 100 GB in total. 90% of the space usage is required for storing dataset.

-   **How much time will be spent in preparing the workflow (approximately)?:** 1 hour.

-   **How much time is needed to complete experiments (approximately)?:** 5 hours.

-   **Publicly available?:** Yes.

## Description

### How to Access

The [CoCoNet]{.smallcaps} implementation and the benchmarking infrastructure used in our evaluation are publicly available as the artifact [@coconet-artifact]. This artifact contains a zip file with two directories: (i) `coconet`, which is the implementation of [CoCoNet]{.smallcaps}, and (ii) `coconet-experiments`, which is the benchmarking infrastructure. Latest versions of these directories are available at <https://github.com/parasailteam/coconet> and <https://github.com/parasailteam/coconet-experiments>.

### Hardware Dependencies

All benchmarks can be executed on a distributed system with two or more NVIDIA GPUs. However, our results will be reproducible on the evaluation system described in Section [7](#sec:experiments){reference-type="ref" reference="sec:experiments"}.

### Software Dependencies

Our experiments require a system running Ubuntu 20.04 with Python 3.8+ and CUDA 11.0+. Prerequisites and their installation procedure is described in `README.md` files of `coconet` and `coconet-experiments` directories.

### Data Sets

The standalone benchmarks (Figure [\[fig:bandwidth64GPUs\]](#fig:bandwidth64GPUs){reference-type="ref" reference="fig:bandwidth64GPUs"}, [7](#fig:matmul-overlap){reference-type="ref" reference="fig:matmul-overlap"}, and  [8](#fig:pipeline-overlap){reference-type="ref" reference="fig:pipeline-overlap"}) do not require any dataset. Datasets required for executing experiments in Section [7.1.2](#sec:experiments:bert){reference-type="ref" reference="sec:experiments:bert"}, [7.2.2](#sec:experiments:model-parallel:integeration){reference-type="ref" reference="sec:experiments:model-parallel:integeration"}, and  [7.3.2](#sec:experiments:pipeline-parallel:integeration){reference-type="ref" reference="sec:experiments:pipeline-parallel:integeration"} can be obtained by following *Dataset* section of `README.md` in `coconet-experiments`.

## Installation

Following instructions have been tested with Ubuntu 20.04.

#### Standalone Experiments Dependencies

Install dependencies by following the *Prerequisites* section in `README.md` file of `coconet` directory.

#### Integration Experiments Dependencies

Follow the *Prerequisites* section in `README.md` file of `coconet-experiments` directory to build PyTorch and install all dependencies for Megatron-LM and NVIDIA Bert.

## Experiment Workflow

### Standalone Experiments {#appendix:sec-standalone}

This section describe how to execute standalone experiments of Section [7](#sec:experiments){reference-type="ref" reference="sec:experiments"} and produce results for Figure [\[fig:bandwidth64GPUs\]](#fig:bandwidth64GPUs){reference-type="ref" reference="fig:bandwidth64GPUs"}, Figure [7](#fig:matmul-overlap){reference-type="ref" reference="fig:matmul-overlap"}, and Figure [8](#fig:pipeline-overlap){reference-type="ref" reference="fig:pipeline-overlap"}. All of these experiments will take 1 hour combined.

1.  Install all [CoCoNet]{.smallcaps} prerequisites in `coconet/README.md`.

2.  The `experiments/` directory contains all scripts for standalone experiments.

    ``` {.bash language="bash"}
    $ cd coconet/experiments/
    ```

3.  Since all our experiments uses MPI to run the executable on all GPUs, set the environment variable `NPROC` to the number of GPUs in the system. In our experiments, we set `NPROC` to 256 as follows:

    ``` {.bash language="bash"}
    $ export NPROC=256
    ```

    **Note**: Setting `NPROC` to a value more than the number of GPUs in a system can lead to failed experiments.

4.  If the experiments are performed on a system with multiple nodes then additional arguments to `mpirun` can be passed by setting the `MPI_ARGS` environment variable.

#### Data-Parallel Experiments

1.  To execute standalone data parallel experiments execute `data-parallel-exp.py`. This script takes a directory to store the results as an argument. Additionally, the script requires `MASTER_ADDR` and `MASTER_PORT` to be passed as `MPI_RUN_ARGS`. If the experiments are done on a single system, then it is common to set `MASTER_ADDR=127.0.0.1` and `MASTER_PORT=10000`.

    ``` {.bash language="bash"}
    $ export MPI_ARGS="-x MASTER_ADDR=127.0.0.1"
    $ export MPI_ARGS="$MPI_ARGS -x MASTER_PORT=10000"
    $ python data-parallel-exp.py results/
    ```

    The above execution of script will execute all data parallel executables and store the results in the `results` directory.

2.  Generate both graphs of Figure [\[fig:bandwidth64GPUs\]](#fig:bandwidth64GPUs){reference-type="ref" reference="fig:bandwidth64GPUs"} by executing the script `gen-data-parallel-graphs.py`. This script takes the directory with results generated in the previous step as an argument.

    ``` {.bash language="bash"}
    $ python gen-data-parallel-graphs.py results/
    ```

    Graphs are stored in two files of `experiments` directory: `results-adam-fp16.pdf` and `results-lamb-fp16.pdf`.

#### Model-Parallel Experiments

1.  To execute standalone model-parallel experiments execute `model-parallel-exp.py`. Similar to the previous script, this script also takes a directory to store results as its argument.

    ``` {.bash language="bash"}
    $ python model-parallel-exp.py results/
    ```

    The script will execute all model parallel executables and stores the results in the `results` directory.

2.  Generate Figure [7](#fig:matmul-overlap){reference-type="ref" reference="fig:matmul-overlap"} by executing following script. This script will take above results directory as its argument.

    ``` {.bash language="bash"}
    $ python gen-model-parallel-graphs.py results/
    ```

    Graph is stored as `results-model-parallel.pdf`.

#### Pipeline-Parallel Experiments

1.  To execute standalone pipeline-parallel experiments execute `pipeline-parallel-exp.py`. This script also requires a directory to store results as its command line argument.

    ``` {.bash language="bash"}
    $ python pipeline-parallel-exp.py results/
    ```

    Above execution of the script will execute all pipeline parallel executables and store the results in `results` directory.

2.  To generate Figure [8](#fig:pipeline-overlap){reference-type="ref" reference="fig:pipeline-overlap"} execute the script\
    `gen-pipeline-parallel-graphs.py`. This script takes the directory containing above results as its argument.

    ``` {.bash language="bash"}
    $ python gen-pipeline-parallel-graphs.py results/
    ```

    The graph is stored in `results-model-parallel.pdf`.

### Integration Experiments {#appendix:sec-integration}

In this section, we will execute the integration experiments of Section [7.1.2](#sec:experiments:bert){reference-type="ref" reference="sec:experiments:bert"}, [7.2.2](#sec:experiments:model-parallel:integeration){reference-type="ref" reference="sec:experiments:model-parallel:integeration"}, and [7.3.2](#sec:experiments:pipeline-parallel:integeration){reference-type="ref" reference="sec:experiments:pipeline-parallel:integeration"}.

#### Prerequisites

Install prerequisites and obtain dataset by following the steps in `coconet-experiments/README.md`.

#### Data-Parallel Training

Go to `Nvidia-Bert` directory and execute `coconet-experiments.py`.

``` {.bash language="bash"}
$ cd NV-BERT 
$ python coconet-experiments.py
```

This script will execute data parallel training experiments and then print Table [\[tab:bert-results\]](#tab:bert-results){reference-type="ref" reference="tab:bert-results"}. This experiment will take 1 hour to complete. This script contains maximum batch sizes supported by each implementation for our evaluation system of 256 Tesla V100 GPUs. It is possible that for a different system the maximum batch size will be different. The batch size dictionary in `coconet-experiments.py` can be modified to find maximum batch size for underlying system.

#### Model-Parallel Inference

Go to `MegatronLM-Model-Parallel` directory and execute `coconet-experiments.py`.

``` {.bash language="bash"}
$ cd MegatronLM-Model-Parallel
$ python coconet-experiments.py
```

This script will execute model parallel inference experiments and then print the values in Section [7.2.2](#sec:experiments:model-parallel:integeration){reference-type="ref" reference="sec:experiments:model-parallel:integeration"}. This experiment will take less than 30 minutes to complete.

#### Pipeline-Parallel Inference

Execute `coconet-experiments.py` in the directory `MegatronLM-Pipeline-Parallel`.

``` {.bash language="bash"}
$ cd MegatronLM-Pipeline-Parallel
$ python coconet-experiments.py
```

This script will execute pipeline parallel inference experiments and then print the table in Section [7.3.2](#sec:experiments:pipeline-parallel:integeration){reference-type="ref" reference="sec:experiments:pipeline-parallel:integeration"}. This experiment will take 3 hour to complete.

## Evaluation and Expected Results

#### Standalone Experiments {#standalone-experiments-3}

The figures generated by the experiments of Section [11.5.1](#appendix:sec-standalone){reference-type="ref" reference="appendix:sec-standalone"} can be matched with the figures: [\[fig:bandwidth64GPUs\]](#fig:bandwidth64GPUs){reference-type="ref" reference="fig:bandwidth64GPUs"}, [7](#fig:matmul-overlap){reference-type="ref" reference="fig:matmul-overlap"}, and [8](#fig:pipeline-overlap){reference-type="ref" reference="fig:pipeline-overlap"}.

#### Integration Experiments {#integration-experiments}

The results generated in experiments of Section [11.5.2](#appendix:sec-integration){reference-type="ref" reference="appendix:sec-integration"} can be matched with the results in Section [7.1.2](#sec:experiments:bert){reference-type="ref" reference="sec:experiments:bert"}, [7.2.2](#sec:experiments:model-parallel:integeration){reference-type="ref" reference="sec:experiments:model-parallel:integeration"}, and [7.3.2](#sec:experiments:pipeline-parallel:integeration){reference-type="ref" reference="sec:experiments:pipeline-parallel:integeration"}.

[^1]: [CoCoNet]{.smallcaps} stands for \"**[Co]{.underline}**mmunication and **[Co]{.underline}**mputation optimization for neural **[Net]{.underline}**works.

[^2]: https://pytorch.org/docs/stable/notes/broadcasting.html
