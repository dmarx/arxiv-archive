
\documentclass{article} % For LaTeX2e
\usepackage[table,xcdraw]{xcolor}

\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{dsfont}

\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[capitalise,noabbrev]{cleveref}

% Define a custom dark blue color
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.5}

% hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=darkblue,      % Color for internal links (sections, pages)
    citecolor=darkblue,      % Color for citation links (bibliography)
    urlcolor=darkblue        % Color for external links (URLs)
}

\title{FLARE: Faithful Logic-Aided Reasoning and Exploration
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

 \iclrfinalcopy

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\author{%
Erik Arakelyan$^{\dagger1}$ \quad Pasquale Minervini$^{2}$$^{3}$ \quad Pat Verga$^4$ \quad Patrick Lewis$^4$ \quad Isabelle Augenstein$^1$ \\
$^1$University of Copenhagen \qquad $^2$University of Edinburgh \qquad $^3$Miniml.AI \qquad $^4$Cohere \\
\texttt{\{erik.a, augenstein\}@di.ku.dk} \qquad  \texttt{p.minervini@ed.ac.uk} \\ \texttt{\{pat, patrick\}@cohere.com}
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ours}{\texorpdfstring{FLARE}\xspace}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\blfootnote{$^{\dagger}$Corresponding author.}

\begin{abstract}
%
%Modern Question Answering (QA) and Reasoning benchmarks have favoured Large Language Models (LLMs) for their strong performance. Such models commonly utilise prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope.
%
Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope.
%
However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model.
%
%On the other end of the spectrum, methods such as Faithful CoT (F-CoT) propose to use LLMs with symbolic methods to formalise the natural language text and produce a solution using a solver.
%
On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers.
%
While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce \textbf{F}aithful \textbf{L}ogic-\textbf{A}ided \textbf{R}easoning and \textbf{E}xploration (\textbf{\ours}), a novel interpretable approach for traversing the problem space using %exact 
task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space.
%
Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers.  Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks.
%
We also show that model faithfulness positively correlates with overall performance and further demonstrate that {\textbf{\ours}} allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.  
%
\end{abstract}

\section{Introduction} \label{sec:intro}
%
Complex Reasoning in natural Question Answering (QA) tasks assumes the capability to explore the problem space of the designated query with a formalised set of facts, relations, commonsense knowledge and logical implications.
%
In line with this, LLMs have been enhanced with CoT \citep{wei2022chain} prompting, which supplements the QA process by generating intermediate reasoning chains given a set of in-context examples \citep{NEURIPS2020_1457c0d6}, as shown in \cref{fig:flare}.
%
This allowed for advancement in commonsense \citep{madaan2022language}, symbolic \citep{wang2022self,sprague2024cot} and mathematical \citep{jie2023design} reasoning.
%
Although CoT allows for a problem exploration in natural language steps, such an approach has been shown to cause performance degradation for nuanced reasoning tasks involving multi-hop planning \citep{valmeekam2022large,suzgun-etal-2023-challenging}, problem exploration \citep{yao2022react} and arithmetic \citep{hendrycks2021measuring, madaan2022text}.
%
These discrepancies arise as CoT suffers from a limited ability to decompose, search, verify and backtrack using intermediate rationale chains~\citep{yao2022react}, cascading hallucinations and errors \citep{DBLP:conf/nips/LingFLHLMS23} and that natural language might not be an optimal representation for describing the reasoning process \citep{DBLP:conf/icml/0002LZCHSL0XI24}.
%
Simultaneously, LLM output has been shown to be unfaithful and inconsistent w.r.t. the intermediate CoT rationale \citep{DBLP:conf/acl/JacoviBBHHT0AG24,lanham2023measuring,DBLP:conf/nips/TurpinMPB23}.
%
% These problems are depicted in \cref{fig:flare}, where although the intermediate answers have suggested the answer \emph{no}, the final model output preferred \emph{yes}.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth, clip]{figures/FLARE_splash_bare.pdf}
    \caption{A depiction of the \emph{plan}, \emph{code} and simulated \emph{search} in {\ours}. Each module has a breakdown of the relevant components composed by the LLM explained in \cref{sec:method}.}
    \label{fig:flare}
\end{figure*}
%
%
% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\textwidth, clip]{figures/splash_alt-cropped.pdf}
%     % \caption{A direct comparison of CoT \cite{wei2022chain}, F-CoT \cite{DBLP:conf/acl/Xu0P0LH24} and \ours.}
%     \caption{An overview of the \emph{plan}, \emph{code} and \emph{search} modules used in \ours.}
%     \label{fig:flare}
% \end{figure}
%
To circumvent the problem of CoT faithfulness and allow for more robust reasoning during QA, \citet[Faithful CoT]{DBLP:conf/acl/Xu0P0LH24} suggested generating code which is further executed using an external symbolic solver.
%
Producing and executing code allows one to both create an output that can be conditioned on heuristic search in the problem space and perform backtracking.
%
However, strict translations of natural language queries into code, such as autoformalisation \citep{DBLP:conf/mkm/Szegedy20,DBLP:conf/mkm/WangKU18}, is a non-trivial task involving direct inference of implicit commonsense and domain-specific knowledge and the ability to align abstract and informal concepts directly to constrained formal definitions for further execution~\citep{DBLP:conf/nips/WuJLRSJS22}.
%
An example query \emph{``Do all parts of the aloe vera plant taste good?''} seen in \cref{fig:flare} is not trivial to formalise, nor to provide a strict algorithmic solution for it, making it much more suitable for soft reasoning.
%
Using external solvers makes such fuzzy reasoning impossible and requires consistently generating syntactically correct executable code.
%
While some LLMs have coding capabilities stemming from their pretraining \citep{DBLP:journals/corr/abs-2406-00515,DBLP:journals/corr/abs-2408-10914}, relative code consistency is more probable with models explicitly trained for coding \citep{DBLP:journals/corr/abs-2107-03374}.
%

%
To overcome these problems with CoT and F-CoT, we propose Faithful Logic-Aided Reasoning and Exploration (\ours), an interpretable method that allows for planning, fuzzy reasoning, and traversing the problem space with backtracking, exact task decomposition, and measuring faithfulness.
%
In \ours, given a natural language query, we prompt an LLM to sequentially generate a \emph{plan} that includes an analysis and the logical steps necessary for formalising and answering the question, a logical programming Prolog \citep{wielemaker2012swi} \emph{code} that allows formalising the query into a set of facts, relations and their composition forming the problem space and the \emph{search} which is an LLM generated simulation of exhaustive multi-hop traversal of that space from the code.
%
An illustration of {\ours} can be seen in \cref{fig:flare}.
%
In our framework, the generated code must not be consistently executable by an external solver, allowing for the soft-formalisation of natural language. Although we see that even generalist LLMs are able to produce executable code in $\geq 50\%$ of cases.
%
{\ours} allows us to measure the faithfulness of the outcome w.r.t. the simulated multi-hop logical traversal by directly comparing the search paths generated from executable Prolog code to that LLM generation. This comparison also allows for pinpointing model hallucinations and inconsistencies.
%
We systematically study the effectiveness of our method using $4$ general-purpose LLMs of varying scales across $9$ diverse benchmarks, covering Math World Problems, Multi-hop QA and Relation inference and show that our method achieves state-of-the-art results in $7$ out of $9$ in comparison to CoT and F-CoT. We also show that the method is rather competitive for models tuned for coding, with an average overall increase of $16\%$ over F-Cot and $9\%$ over CoT. Our findings show that model accuracy strongly correlates with the faithfulness of the reasoning process towards the problem space. We also provide ablations showing that the model can interpretably pinpoint hallucinations, search degeneracies, and the limitations of the search over the problem space. 
%
Our key contributions are the following: 
\begin{itemize}[leftmargin=*]
    \item We introduce {\ours} a novel paradigm for logic-aided and interpretable formalisation and search over the problem space in QA and reasoning tasks.
    \item We perform a systematic evaluation across $9$ benchmarks and $4$ models of varying scales, showing the advantages of using {\ours} for QA in a few-shot setup over prior approaches.
    \item The modularity of {\ours} allows defining a simple ingrained method for measuring model faithfulness and shows that it is strongly correlated with performance.
    \item We further show that using {\ours} allows us to interpretably and rigorously detect hallucinations along with sub-optimal and inconsistent reasoning patterns.
\end{itemize}

\section{Faithful Logic-Aided Reasoning and Exploration}
\label{sec:method}
\subsection{LLM Simulated Search}
\label{subsec:generate}

{\ours} comprises three modules for generating a \emph{plan}, \emph{code} and simulated \emph{search} for answering a natural language query $\mathcal{Q} = \{T_1 \dots T_{|\mathcal{Q}|}\}$, where each $T_i$ is a token.

\paragraph{Generating A Plan}
% 
For each query $\mathcal{Q}$, given an LLM $\mathcal{M}$, we initially prompt it to generate a \emph{plan}, $\mathcal{P}$, which should be comprised of task explanation, analysis and a plan for further formalising the query. An example of this can be seen in the \emph{plan} section in \cref{fig:flare}. We use in-context few shot examples $\mathcal{E}_{\mathcal{P}}$ of such \emph{plan} generations along with \textbf{greedy} decoding for obtaining the final plan.
%
\begin{align}
    \mathcal{P}_i \sim \argmax p_\mathcal{M}(T_i \mid \mathcal{E}_{\mathcal{P}}, \mathcal{Q})
\end{align}

\paragraph{Generating Code}

After generating the \emph{plan}, we prompt the LLM $\mathcal{M}$ to generate a Prolog code $\mathcal{C}$, an example of which can be seen in \cref{fig:flare}. We append executable code generation samples $\mathcal{C}_{\text{sample}}$ to the previous in-context examples $\mathcal{E}_{\mathcal{P}}$ and obtain few-shot code generation demonstrations $\mathcal{E}_{\mathcal{C}} =[\mathcal{E}_{\mathcal{P}};\mathcal{C}_{\text{sample}}]$
%
\begin{gather}
\label{eq:code}
     \mathcal{C}_i \sim \argmax p_\mathcal{M}(T_i \mid \mathcal{E}_{\mathcal{C}}, \mathcal{Q}, \mathcal{P})
     \\
     \mathcal{F}_{\text{\emph{code}}}, \mathcal{R}_{\text{\emph{code}}} , \mathcal{G}_{\text{\emph{code}}} = \text{EXTRACT}(\mathcal{C}_i) \nonumber
 \end{gather}

\paragraph{Benefits of Prolog}

Prolog is a symbolic logic-programming engine \citep{DBLP:conf/acm/Bowen79} used for heuristic search over Horn Clauses \citep{DBLP:journals/jlp/ChandraH85}. It is a declarative programming paradigm \citep{DBLP:conf/agp/Lloyd94}, meaning that the code is expressed as the logic of computation. In particular, this logic is formalised as a set of facts $\mathcal{F}$ and relations $\mathcal{R}$ forming our problem space, while the final goal $\mathcal{G}$ is a first-order logic combination of them. As a default, Prolog uses a depth-first search (DFS) strategy \citep{DBLP:conf/acm/Bowen79} for sub-goal decomposition and feasible traversal of the problem space that satisfies the goal $\mathcal{G}$. Such a traversal is referred to as the \emph{trace}. At each trace step, the program can either confirm or invalidate the sub-goal using the feasibility of fact and relation combinations, expand the search tree or retry satisfying a failed sub-goal with new combinations. An example of such a search can be observed in \cref{fig:flare}.
%
It is possible to complete an exhaustive search, exploring all possible paths that do or do not satisfy the goal. 
%
These characteristics are beneficial as we can explicitly access and segment the facts and relations that form the problem space and the search strategy used for query formalisation. As Prolog is declarative, it is sufficient to use a regexp heuristic for the segmentation, which is referred to as EXTRACT in \cref{eq:code} and \cref{eq:search}.
%
Furthermore, including exhaustive traversal traces in-context allows the LLM to simulate sub-goal decomposition, backtracking, intermediate goal invalidation, etc. We discuss this in more depth in the next paragraph.

\paragraph{Simulating Search}

After generating the logic-programming \emph{code}, we want to simulate program execution by generating a problem space traversal trace with our LLM $\mathcal{M}$. We update our in-context samples by appending search traces $\mathcal{S}_{\text{sample}}$ constructed from Prolog execution of sample codes $\mathcal{C}_{\text{sample}}$, i.e. $\mathcal{E}_{\mathcal{S}} =[\mathcal{E}_{\mathcal{C}};\mathcal{S}_{\text{sample}}]$. 
%
\begin{gather}
\label{eq:search}
     \mathcal{S}_i \sim \argmax p_\mathcal{M}(T_i \mid \mathcal{E}_{\mathcal{S}}, \mathcal{Q}, \mathcal{P}, \mathcal{C}) \\
     \mathcal{A}_{\text{\emph{search}}},
     \mathcal{F}_{\text{\emph{search}}}, \mathcal{R}_{\text{\emph{search}}} = \text{EXTRACT}(\mathcal{S}_i) \nonumber
 \end{gather}
%
 During iterative problem space traversal, we can segment the facts $\mathcal{F}_{\text{\emph{search}}}$, relations $\mathcal{R}_{\text{\emph{search}}}$, completed and backtracked paths with their answers $\mathcal{A}_{\text{\emph{search}}}$ used during the search simulation. To get the final answer we update in-context samples with the correct final answers $\mathcal{A}_{\text{sample}}$ from the executed search $\mathcal{S}_{\text{sample}}$, $\mathcal{E}_{\mathcal{A}} =[\mathcal{E}_{\mathcal{S}};\mathcal{A}_{\text{sample}}]$. 
%
 \begin{align}
      \mathcal{A}_{\text{\emph{Final}}} \sim \argmax p_\mathcal{M}(T_i \mid \mathcal{E}_{\mathcal{A}}, \mathcal{Q}, \mathcal{P}, \mathcal{C}, \mathcal{})
 \end{align}
%
The prompts used for generating each part in {\ours} can be seen in \cref{append:prompts}. 

\subsection{Detecting Reasoning Inconsistencies}
\label{subsec:reasoning_inc}

For each query $\mathcal{Q}$ given the \emph{code} $\mathcal{C}$ and the simulated \emph{search} $\mathcal{S}$ along with the extracted facts $\mathcal{F}_{\text{\emph{code}}}, \mathcal{F}_{\text{\emph{search}}}$ and relations $\mathcal{R}_{\text{\emph{code}}}, \mathcal{R}_{\text{\emph{search}}}$ from each designated module, we aim to detect the inconsistencies during the reasoning process of the LLM. We use exact string matching between all these facts and relations in \emph{code} and simulated \emph{search}.
%
\begin{gather}
% \mathds{1}: \mathcal{F}_{\text{\emph{code}}} \times \mathcal{F}_{\text{\emph{search}}} \times \mathcal{R}_{\text{\emph{code}}} \times \mathcal{R}_{\text{\emph{search}}} \rightarrow \{0, 1\} = \begin{cases}
%       1 \quad \text{if} \quad \mathcal{F}^{i_{\text{match}}}_{\text{\emph{code}}} = \mathcal{F}^{j_{\text{match}}}_{\text{\emph{search}}} \{\forall i, j: \exists i_{\text{match}}, j_{\text{match}} \} \\
%        \mathcal{R}^i_{\text{\emph{code}}} = \mathcal{R}^j_{\text{\emph{search}}} \exists i, j
% \end{cases}
\forall i, \exists j \quad \text{such that} \quad \mathcal{F}_{\text{code}}^i = \mathcal{F}_{\text{search}}^j \quad \text{and} \quad \forall v, \exists q \quad \mathcal{R}_{\text{code}}^v = \mathcal{R}_{\text{search}}^q \\
\forall j, \exists i \quad \text{such that} \quad \mathcal{F}_{\text{code}}^i = \mathcal{F}_{\text{search}}^j \quad \text{and} \quad \forall q, \exists v \quad \mathcal{R}_{\text{code}}^v = \mathcal{R}_{\text{search}}^q
\end{gather}
%
With this framework in mind, we define two reasoning failure modes. 
%
In the \emph{first} failure mode, given that some fact or relation was used in the simulated \emph{search} but did not exist in the generated \emph{code}, i.e. $\exists j \text{ such that } \mathcal{F}_{\text{search}}^j \notin \mathcal{F}_{\text{code}}$, we claim that the LLM has \emph{hallucinated}. We postulate that the model either produced incomplete knowledge during formalisation to \emph{code} or created a piece of non-existing information during the \emph{search}. We do not consider facts that emerged during a direct inference step within the simulated search during our calculation. For example, if we are dealing with a mathematical query $4\cdot (5+6) = ?$, the search would involve separately evaluating the expression $5+6=11$. In this case, $11$ will not be treated as a hallucinated fact within the search but rather as an emergent fact obtained from direct inference.   
%
The \emph{second} failure mode is the reciprocal case, where a fact or relation present in the \emph{code} is not used during the \emph{search}. We refer to this phenomenon as \emph{sub-optimal reasoning} as it shows that the LLM could not explore the problem space completely or injected unsuitable knowledge during formalisation into \emph{code}. 

\input{results}

\subsection{Measuring Faithfulness}

We propose a method to measure the faithfulness of the LLM reasoning process when using {\ours}. As mentioned in \cref{subsec:generate}, for each query in a dataset $\mathcal{D} = [\mathcal{Q}_1, \dots ,\mathcal{Q}_{|\mathcal{D}|}]$, we generate a set of codes $\Phi = [\mathcal{C}_1, \dots ,\mathcal{C}_{|\Phi|}]$ and simulated problem space searches $\Psi = [\mathcal{S}_1, \dots, \mathcal{S}_{|\Psi|}]$. We use the Prolog engine to execute all of the codes $\Phi$ and obtain a set of correctly written programs $\Phi^\prime$ and exact search paths $\Psi^\prime$. As we do not require explicit programmatic correctness during inference in {\ours} for any code $\mathcal{C}_i$, some Prolog executions resulting in an error are filtered out in $\Psi^\prime$. To assess model reasoning faithfulness towards code formalisations, we compare the search paths $\Phi^\prime$ obtained from Prolog execution with their designated counterparts $\Phi_{\text{\emph{gen}}}^\prime$ generated by the LLM from the same code. We use ROUGE \citep{lin-2004-rouge} to compute the matching score for each executed and simulated search path. In particular, we use ROUGE-Lsum, which uses the longest common subsequence (LCS) over each line to obtain the final score. This method fits our cause as a line in a Prolog search execution represents a single logic step within the traversal. This allows us to measure the similarity of the reasoning contents and structure in exact and simulated searches.  

\section{Experimental Setup}
\label{sec:ex_setup}
\subsection{Datasets}
To evaluate {\ours}, we use a benchmark of $9$ tasks that cover Math Word Problems (MWP), multi-hop QA and relation inference. For testing numeric and mathematical reasoning, we follow CoT \citep{wei2022chain} by including GSM8K \citep{DBLP:journals/corr/abs-2110-14168}, SVAMP \citep{DBLP:conf/naacl/PatelBG21}, MultiArith \citep{DBLP:conf/emnlp/RoyR15}, ASDiv \citep{DBLP:conf/acl/MiaoLS20} and AQuA \citep{DBLP:conf/acl/LingYDB17}. Among these, GSM8K, SVAMP, MultiArith and ASDiv cover elementary and middle school arithmetic word problems with a set of integers or decimals as the answer. AQuA is a multiple-choice numerical, symbolic reasoning dataset where each answer is a mathematical expression containing notations, values and expressions not defined in the query.
%
We also test {\ours} using three multi-hop QA datasets. We use StrategyQA \citep{DBLP:journals/tacl/GevaKSKRB21}, which is a boolean QA task that requires sub-goal decomposition and a multi-hop reasoning strategy to answer. The example \emph{``Do all parts of the aloe vera plant taste good?''} used in \cref{fig:flare}, is taken from StratedyQA. The multi-hop QA testing also includes Date and Sports Understanding, subsets of BIG-Bench \citep{srivastava2023beyond}. The tasks involve inferring an exact date given some calculations in the relative time period and understanding if an artificially created sports statement is feasible.
%
Finally, we assess {\ours} on Relational Inference using CLUTRR \citep{DBLP:conf/emnlp/SinhaSDPH19}, which involves inferring the familial relation between two entities mentioned in a natural language description of the partial family graph. 
%
We adopt the same in-context samples used in \citep{DBLP:conf/acl/Xu0P0LH24, wei2022chain}. The complete descriptions, statistics and examples for each dataset can be found in \cref{tab:data_stat} in \cref{append:prompts}. 

\begin{figure}[t!]
    \centering
    \includegraphics[clip=true,width=\textwidth]{figures/model_accuracy_vs_rouge_All_Datasets.pdf}
    \caption{The trend of mean model accuracy w.r.t mean faithfulness (ROUGE-Lsum) for all the models. Faithfulness is positively correlated with model performance.}
    \label{fig:faith_vs_acc}
\end{figure}

\subsection{Benchmarks}

We compare {\ours} with CoT \citep{wei2022chain} as a prompting method that reasons using natural language chains and with F-CoT \citep{DBLP:conf/acl/Xu0P0LH24} that formalises the query into a code and offload the reasoning to an external symbolic solver. We use Llama3.1 (8B) \citep{DBLP:journals/corr/abs-2407-21783}, CmDR (30B) \citep{cohere2024commandr}, CmDR+ (100B) \citep{cohere2024commandr}  and GPT3.5 \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} ($\geq 100B$ \citep{DBLP:journals/corr/abs-2303-10420}). As the coding model OpenAI Codex (code-DaVinci-002) \citep{DBLP:journals/corr/abs-2107-03374} used in F-CoT has been deprecated, we replace it with the new GPT3.5 as suggested by OpenAI and recalculate the results accordingly.


\section{Results}
\label{sec:results}

\subsection{Main Results}

\input{prompt_only}
\begin{figure}[t!]
    \centering
    \includegraphics[clip=true,width=\textwidth]{figures/code_exec_by_dataset.pdf}
    \caption{The figure shows the percentage of executable code per model (right) and the accuracy of the executable code when answering the queries (left).}
    \label{fig:code_perc}
\end{figure}

To evaluate {\ours}, we use a set of models of varying sizes on diverse benchmarks, as defined in \cref{sec:ex_setup}. We compare the performance of each model while using {\ours}, CoT and F-CoT prompting. The results for F-CoT and CoT on all the models are computed using the codebase of the original study \citep{DBLP:conf/acl/Xu0P0LH24}.

\paragraph{LLMs for general reasoning} 

Our results, presented in \cref{tab:results_main}, show that using {\ours} allows the LLMs to achieve state-of-the-art results on $7$ out of $9$ datasets, with an average $28\%$ increase over CoT. We can see a clear trend that {\ours} increases the performance compared to CoT and F-CoT for all the models of varying scales. We also see that LLMs that are not explicitly tuned for coding suffer massive degeneracies when using F-CoT. We postulate that they are unable to consistently produce executable programs that satisfy a predefined scheme in F-CoT, thus resulting in an error during execution. This further highlights the value of simulating program execution using an LLM instead of using external solvers.
%
The results show that using {\ours} yields more benefit on datasets that require longer chains of multi-hop and symbolic reasoning, like AQuA and StrategyQA.


\paragraph{LLMs for code generation}

To understand the effect of {\ours} on models tuned for coding, we use GPT3.5 \citep{NEURIPS2020_1457c0d6} as it was the OpenAI suggested succession model for Codex \citep{DBLP:journals/corr/abs-2107-03374} which is used in F-CoT and possesses strong coding capabilities \citep{DBLP:journals/corr/abs-2303-10420}. The results in \cref{tab:results_main} show that using {\ours} is beneficial for models that are tuned for coding and boost the accuracy with a $16\%$ increase over F-CoT and $9\%$ over CoT. The reason is that many natural language queries with non-trivial formalisations are more suited to be tackled with more commonsense soft reasoning than direct code execution. This is evident in \cref{tab:results_main} where {\ours} and CoT are consistently better than F-CoT in StrategyQA, Sports and CLUTRR. 
%
The opposite case of numeric and algorithmic heavy reasoning tasks is also covered by {\ours} as it maintains strong performance similar to F-CoT on MWP problems \cref{tab:results_main}.
%
Consequently, {\ours} allows combining algorithmic formalisation with simulated soft-reasoning, circumventing the pitfalls of using a deterministic external solver while still producing a query formalisation and problem space traversal. 

\subsection{Is simulating search useful?}

To understand if simulating a search over the problem space is useful, we compare the performance of {\ours} where we only generate the \emph{plan} without the subsequent \emph{code} or \emph{search} components. We refer to this framework setup as \emph{plan-only}, which can be seen in \cref{fig:flare} if we were to use only the \emph{plan} for answer generation. We completed this ablation using CmDR, CmDR+, and GPT-3.5, and we used GSM8K, AQuA, and StrategyQA as our baselines. The results in \cref{tab:plan_only} confirm that all of the models suffer massive performance degradation from $61.1 \rightarrow 49.9$ when omitting the \emph{code} and the \emph{search} components of {\ours}. We hypothesise that this is caused by insufficient problem space exploration when using the \emph{plan-only} setting.   
%
Furthermore, we have already seen in \cref{tab:results_main} that in methods, like F-CoT, that do not use simulated problem space exploration for soft-reasoning and only rely on \emph{plan} and \emph{code}, the performance also deteriorates even resulting in a complete breakdown of reasoning over the designated datasets. This can be viewed as a constrained version of {\ours} with \emph{code-only} execution.
%
Consequently, our results show that simulating problem space traversal is highly beneficial as it avoids the pitfalls posed by \emph{plan-only} and \emph{code-only} modes by exploring the problem space more rigorously and soft-reasoning during that traversal instead of using external solvers.

\subsection{Faithful Reasoning Improves Performance}
\input{reasoning_stats_basic}

As described in \cref{sec:method}, using {\ours} allows us to measure the faithfulness of the LLM reasoning process by comparing the simulated problem space traversals ${\Phi}^\prime_\text{\emph{gen}}$ with actual traces ${\Phi}^\prime$ produced from a symbolic Prolog solver. To do this, we initially compute the percentage of syntactically correct executable code each LLM produces. We can see from the right part of \cref{fig:code_perc} that all of the models are capable of producing correct executable Prolog code in $67\%$ of cases on average and $\geq 50\%$ of cases at the very least. This shows that the simulated searches ${\Phi}^\prime_\text{\emph{gen}}$ can be considered a representative sample that will be further used to accurately measure the faithfulness of the simulated search w.r.t. the generated code. After measuring the reasoning faithfulness for each model, we want to understand what impact it has on the performance of the LLM. In \cref{fig:faith_vs_acc}, we segment the models w.r.t. their ROUGE-Lsum scores. The results show that model performance is strongly positively correlated with reasoning faithfulness. 
%
However, we also observe in the left part of \cref{fig:code_perc} that executing semantically precise code results in an accurate answer only in $47\%$ of cases on average. Indeed, having a simulated search trace with a ROUGE-Lsum faithfulness score of $1$, would be equivalent to simply executing the program as proposed in F-CoT. Yet we have priorly shown that F-CoT struggles with reasoning tasks that are hard to formalise and require multi-hop commonsense and soft reasoning.
%
These two discoveries show that optimal LLM reasoning, conditioned on a search in the problem space, should be increasingly faithful toward the facts, relations and the search strategy defined within the code while simultaneously maintaining the capability for soft-reasoning along more abstractly defined concepts. Our results show that {\ours} allows LLMs to maintain a similar reasoning capacity.

\subsection{What is important during the search?}
\input{reasoning_stats_deep}

We expand the analysis of the simulated search traces to detect the reasons which can lead to optimal reasoning within an LLM. For this purpose, we calculate several statistics, like the average number of explored paths, average and total hops and failures per path, for each model during the simulated traversal. The failure in a path is an invalidation of a solution for a sub-goal explored during the search, which is used for backtracking, as explained in \cref{sec:method}. Calculating these statistics is simple as the \emph{search} component of {\ours}, seen in \cref{fig:flare}, is a structured simulation of a Prolog trace, where each line contains a hop of reasoning inference.
%
We split these statistics for the reasoning paths that lead to correct or incorrect outcomes. Our results in \cref{tab:reasoning_stats} show that LLM performance and reasoning optimally are not directly connected to the amount of explored paths or multi-hop inferences per path. We also see that traces that lead to incorrect answers have a higher number of failures per path and in total. We explain this phenomenon with the hypothesis that LLMs with traces that were optimal for reasoning and led to correct answers could skip exploring degenerate solutions due to strong commonsense reasoning capabilities.
%
Further analyses focus on identifying inconsistencies and failure modes (\cref{subsec:reasoning_inc}). By comparing relations in code with those in search traces, we measure emergent hallucinations and unused relations, highlighting areas of sub-optimal reasoning. Additionally, we assess the uniqueness of emergent facts per inference hop, which indicates the extent of problem-space exploration (\cref{tab:reasoning_stats_deep}).
%
The results in \cref{tab:reasoning_stats_deep} show consistently over each model that, on average, traces that lead to correct answers had a higher percentage of unique emergent facts and overlap in the relations used between the code and search, while the portion of underutilized relations was lower. This means that optimal reasoning with an LLM requires a great degree of problem-space exploration with fewer relation hallucinations during the search and more relation utilization from the defined code. This aligns with our prior discoveries, which show a strong correlation between simulated search faithfulness towards the formalised code and model performance. Our framework {\ours} has these reasoning patterns ingrained within its inference pipeline. 

\begin{figure}[t!]
    \centering
    \includegraphics[clip=true,width=0.935\textwidth]{figures/effect_of_scale.pdf}
    \caption{The effect of the model parameter scale from 8B to 100B+ on model accuracy (left) and faithfulness (right).}
    \label{fig:effect_of_scale}
\end{figure}

\subsection{The effect of scale}
\input{scale_effect}

We want to assess the impact of the number of parameters in the model on the overall performance and faithfulness. The results in \cref{fig:effect_of_scale} show no precise relation between model scale, performance and faithfulness. However, scaled models from the same family, i.e. CmDR (30B) and CmDR+ (100B), show improvements in reasoning faithfulness and model performance. We can also see in  \cref{tab:scale_effect} that as the model size increases, the average number of hops and the portion of hallucinations and unutilised knowledge decreases. This further confirms our prior assumptions that models with strong commonsense soft-reasoning capabilities can skip steps during the search while maintaining the knowledge and structure of the traversal strategy outlined in the code. 

\section{Related Work}
\label{sec:related}

\paragraph{Reasoning in Natural Language}
Few-shot prompting \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} has been shown to be an effective approach for increasing the reasoning capabilities of LLMs in natural language generation \citep{DBLP:journals/corr/abs-2102-01672, DBLP:conf/acl/ReifIYCCW22,DBLP:conf/iclr/SanhWRBSACSRDBX22}. LLM reasoning can be further enhanced with prompting techniques such as CoT \citep{wei2022chain}, which attempts to segment reasoning into explicitly written intermediate steps. Concurrent work has also proposed that models \emph{``think step by step''} \citep{DBLP:conf/nips/KojimaGRMI22}, or divide the problem into subtasks before the solution \citep[Least-to-Most]{DBLP:conf/iclr/ZhouSHWS0SCBLC23}. These approaches have been shown to suffer from arithmetic inaccuracies \citep{DBLP:conf/nips/LewkowyczADDMRS22, DBLP:conf/nips/HendrycksBKABTS21} and reasoning inconsistencies \citep{DBLP:journals/corr/abs-2209-07686}. Further attempts have been made to add a planning stage before reasoning by dividing the process into recursive plan formulation and execution steps \citep{DBLP:conf/iclr/YaoZYDSN023,DBLP:conf/acl/WangXLHLLL23}. The \emph{plan} generation step in {\ours} is a hybrid technique inspired by these methods but focused on generating a natural language strategy for formalising the query into code. 
%
\paragraph{Reasoning with Search}

Several lines of work propose using techniques to expand the reasoning paths over the problem space. Self-consistency decoding \citep{DBLP:conf/iclr/0002WSLCNCZ23} is an approach used to sample many natural language reasoning paths and take a majority vote for an answer. Another popular approach is Tree-of-Thoughts (ToT, \citet{DBLP:conf/nips/YaoYZS00N23}), which proposes to explore the problem space with reasoning similar to a tree traversal, where each state is created and evaluated using an LLM. Similar techniques try to adapt symbolic search approaches akin to DFS, BFS \citep{DBLP:conf/aaai/BestaBKGPGGLNNH24}, A$^*$ \citep{DBLP:journals/corr/abs-2402-14083} or other combinations \citep{DBLP:journals/corr/abs-2404-03683} with direct tuning \citep{DBLP:journals/corr/abs-2402-14083}, imitation training \citep{DBLP:conf/nips/YangSAN22} or few-shot prompting \citep{DBLP:journals/corr/abs-2404-01230}. It must be noted that all of these techniques have only been tested in constrained mathematical puzzle-solving and algorithmic domains like the 24 Game \citep{DBLP:conf/nips/YangSAN22}, Countdown \citep{wiki:Countdown}, Sorting \citep{DBLP:conf/aaai/BestaBKGPGGLNNH24}, maze solving \citep{DBLP:conf/nips/YangSAN22}, Sokoban \citep{DBLP:journals/corr/abs-2402-14083} and others.
%
Although the \emph{search} component of {\ours} has some similarities to these techniques, we argue that our method allows for generalistic reasoning with interpretable multi-hop search through iterative logic-based problem space exploration.

\paragraph{Reasoning with Formalisation}

Another line of research has tried formalising natural language queries into code \citep{DBLP:conf/icml/GaoMZ00YCN23,DBLP:conf/icml/0002LZCHSL0XI24} or pseudo-code \citep{DBLP:journals/corr/abs-2404-02575,DBLP:journals/corr/abs-2404-03683}. This allows the translation of the query into a strict structure and offloads the reasoning and search components to deterministic solvers like Python \cite{DBLP:journals/tmlr/ChenM0C23}, PDDL \cite{DBLP:conf/acl/Xu0P0LH24, DBLP:journals/corr/abs-2304-11477}, DataLog \cite{DBLP:conf/acl/Xu0P0LH24} and others. While models are reasonably capable of synthesising programs \citep{DBLP:journals/corr/abs-2108-07732, DBLP:conf/iclr/NijkampPHTWZSX23} and benefit from the use of code in numerical and algorithmic reasoning settings \citep{DBLP:journals/tmlr/ChenM0C23,DBLP:conf/icml/GaoMZ00YCN23}, the usage of code for general QA has not been rigorously explored. The reasons are that formalisation from natural language into a strict and executable code is challenging \citep{DBLP:conf/nips/WuJLRSJS22}, following the exact syntactic constraints of the programming language not abundantly used during pre-training is onerous \citep{DBLP:journals/corr/abs-2404-00971} and can require models explicitly tuned for coding \citep{DBLP:journals/corr/abs-2107-03374}. Using an external solver for reasoning also limits the capability for soft reasoning in commonsense knowledge and implications. Although we formalise the natural language query into a logic programming Prolog program during the \emph{code} generation part of {\ours}, we do not explicitly require the code to be executable and do not use external solvers during inference. This allows for the further use of the LLM for soft-reasoning to simulate code execution in a logic-based problem space traversal similar to Prolog while circumventing the need for code tuning a generalist model.
%

\paragraph{Reasoning Faithfulness}

An explanation is considered \emph{faithful} if it explicitly and accurately describes the reasoning process of the model during inference \citep{DBLP:conf/dsaa/GilpinBYBSK18, DBLP:conf/acl/JacoviG20}. In the context of prompting techniques such as CoT, we are interested in the faithfulness of the intermediate reasoning chains towards the final output. Faithful intermediate reasoning chains should not just look \emph{plausible} \citep{herman2017promise} but have exact reflections of the problem exploration and reasoning used to arrive at the final answer. Natural language reasoning chains prevalent in CoT and similar methods are shown to be unfaithful, either masking the reasoning biases \citep{DBLP:conf/nips/TurpinMPB23} of the model or outright ignoring the intermediate reasoning \citep{DBLP:journals/corr/abs-2307-13702}. In {\ours}, we introduce a method to seamlessly measure the faithfulness of the final outcome w.r.t. completed search.

\section{Conclusion}

This work introduces {\ours}, a novel approach for logic-aided interpretable formalisation and reasoning with simulated search over the problem space. We show that models of varying scales obtain state-of-the-art results compared to prompting paradigms like CoT and F-CoT. We further pinpoint that using {\ours} allows us to perform soft-reasoning with simulated search, making it flexible for diverse reasoning benchmarks. We introduce a method to measure model reasoning faithfulness w.r.t. the problem formalization ingrained within {\ours}. Our results show that model performance is positively correlated with the faithfulness of the reasoning process. The systematic studies of the method show the benefits of using simulated search compared to natural language reasoning and external symbolic solvers. We further show that using {\ours} allows us to interpretably and rigorously detect hallucinations and sub-optimal and inconsistent reasoning patterns.

\section*{Reproducibility Report}

To reproduce the results of our study, we provide the complete codebase, processing pipelines and prompts for each dataset. The only model hyper-parameter we explicitly fix is the temperature for greedy decoding. We also make the inference of all of the models using {\ours}, F-CoT and CoT across all of the datasets publicly available for further experimentation and exploration.

\section*{Acknowledgments}
$\begin{array}{l}\includegraphics[width=1cm]{figures/LOGO_ERC-FLAG_EU.jpg} \end{array}$ 
%
Erik is partially funded by a DFF Sapere Aude research leader grant under grant agreement No 0171-00034B, as well as by a NEC PhD fellowship.
%
Pasquale was partially funded by ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence), EPSRC (grant no.\ EP/W002876/1), an industry grant from Cisco, and a donation from Accenture LLP.
%
Isabelle's research is partially funded by the European Union (ERC, ExplainYourself, 101077481).
%
This work was further supported by by the Pioneer Centre for AI, DNRF grant number P1, the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh.
%

\bibliography{references}
\bibliographystyle{iclr2025_conference}
\newpage{}
\appendix
\section{Appendix}
\label{append:prompts}

\subsection{LLM Prompts}
We define straight-forward prompts for generating \emph{plan}, \emph{code} and \emph{search} simulation in {\ours}, which can be observed in \cref{tab:prompts}. 

\subsection{Dataset Statistics}

The datasets used in this study encompass a variety of domains, specifically targeting the performance of the models in interpreting Math Word Problems, multi-hop question answering, and relational inference. Table \ref{tab:data_stat} provides a detailed breakdown of each dataset, including the number of few-shot in-context samples (shots), the number of test samples, and representative examples from each dataset. The datasets provide a comprehensive basis for evaluating the models' abilities to handle complex tasks across different domains, facilitating an in-depth analysis of model performance under few-shot conditions.


\input{prompts}
\input{data_stat}





\end{document}
