\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openai_gym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem[Camastra \& Vinciarelli(2002)Camastra and
  Vinciarelli]{camastra-2002-TPAMI-estimating-the-intrinsic-dimension}
Francesco Camastra and Alessandro Vinciarelli.
\newblock Estimating the intrinsic dimension of data with a fractal-based
  method.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 24\penalty0 (10):\penalty0 1404--1407, 2002.

\bibitem[Chen et~al.(2015)Chen, Wilson, Tyree, Weinberger, and
  Chen]{chen-2015-arXiv-compressing-neural-networks}
Wenlin Chen, James~T. Wilson, Stephen Tyree, Kilian~Q. Weinberger, and Yixin
  Chen.
\newblock Compressing neural networks with the hashing trick.
\newblock \emph{CoRR}, abs/1504.04788, 2015.
\newblock URL \url{http://arxiv.org/abs/1504.04788}.

\bibitem[Dauphin \& Bengio(2013)Dauphin and
  Bengio]{dauphin-2013-arXiv-big-neural-networks-waste}
Yann Dauphin and Yoshua Bengio.
\newblock Big neural networks waste capacity.
\newblock \emph{CoRR}, abs/1301.3583, 2013.
\newblock URL \url{http://arxiv.org/abs/1301.3583}.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, G{\"{u}}l{\c{c}}ehre, Cho,
  Ganguli, and Bengio]{dauphin2014identifying-and-attacking-the-saddle}
Yann Dauphin, Razvan Pascanu, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho,
  Surya Ganguli, and Yoshua Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock \emph{CoRR}, abs/1406.2572, 2014.
\newblock URL \url{http://arxiv.org/abs/1406.2572}.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, de~Freitas,
  et~al.]{denil-2013-NIPS-predicting-parameters-in-deep}
Misha Denil, Babak Shakibi, Laurent Dinh, Nando de~Freitas, et~al.
\newblock Predicting parameters in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2148--2156, 2013.

\bibitem[Fukunaga \& Olsen(1971)Fukunaga and
  Olsen]{fukunaga-1971-ITC-an-algorithm-for-finding-intrinsic}
Keinosuke Fukunaga and David~R Olsen.
\newblock An algorithm for finding intrinsic dimensionality of data.
\newblock \emph{IEEE Transactions on Computers}, 100\penalty0 (2):\penalty0
  176--183, 1971.

\bibitem[Giryes et~al.(2016)Giryes, Sapiro, and Bronstein]{giryes2016deep}
Raja Giryes, Guillermo Sapiro, and Alexander~M Bronstein.
\newblock Deep neural networks with random gaussian weights: a universal
  classification strategy?
\newblock \emph{IEEE Trans. Signal Processing}, 2016.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Artificial Intelligence and Statistics}, 2010.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and
  Saxe]{goodfellow-2015-ICLR-qualitatively-characterizing-neural}
Ian~J. Goodfellow, Oriol Vinyals, and Andrew~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2015)}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6544}.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{ICLR}, 2016.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{CVPR}, 2015.

\bibitem[Hinton \& Van~Camp(1993)Hinton and
  Van~Camp]{hinton-1993-COLT-keeping-the-neural-networks}
Geoffrey~E Hinton and Drew Van~Camp.
\newblock Keeping neural networks simple by minimizing the description length
  of the weights.
\newblock In \emph{Proceedings of the sixth annual conference on Computational
  learning theory}, pp.\  5--13. ACM, 1993.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{iandola2016squeezenet}
Forrest~N Iandola, Song Han, Matthew~W Moskewicz, Khalid Ashraf, William~J
  Dally, and Kurt Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Kaiser et~al.(2017)Kaiser, Gomez, Shazeer, Vaswani, Parmar, Jones, and
  Uszkoreit]{kaiser-2017-arXiv-one-model-to-learn-them}
Lukasz Kaiser, Aidan~N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion
  Jones, and Jakob Uszkoreit.
\newblock One model to learn them all.
\newblock \emph{CoRR}, abs/1706.05137, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.05137}.

\bibitem[K{\'e}gl(2003)]{kegl-2003-NIPS-intrinsic-dimension-estimation}
Bal{\'a}zs K{\'e}gl.
\newblock Intrinsic dimension estimation using packing numbers.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  697--704, 2003.

\bibitem[Kingma \& Ba(2014)Kingma and
  Ba]{kingma-2014-arXiv-adam:-a-method-for-stochastic}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath,
  Kumaran, and
  Hadsell]{kirkpatrick-2017-PNAS-overcoming-catastrophic-forgetting}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.
\newblock \doi{10.1073/pnas.1611835114}.
\newblock URL \url{http://www.pnas.org/content/114/13/3521.abstract}.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical report}, 2009.

\bibitem[Le et~al.(2013)Le, Sarl{\'o}s, and Smola]{le2013fastfood}
Quoc Le, Tam{\'a}s Sarl{\'o}s, and Alex Smola.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In \emph{ICML}, 2013.

\bibitem[Levina \& Bickel(2005)Levina and
  Bickel]{levina-2005-NIPS-maximum-likelihood-estimation}
Elizaveta Levina and Peter~J Bickel.
\newblock Maximum likelihood estimation of intrinsic dimension.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  777--784, 2005.

\bibitem[Li et~al.(2006)Li, Hastie, and Church]{li2006verysparse}
Ping Li, T.~Hastie, and K.~W. Church.
\newblock Very sparse random projections.
\newblock In \emph{KDD}, 2006.

\bibitem[{Louizos} et~al.(2017){Louizos}, {Ullrich}, and
  {Welling}]{louizos-2017-arXiv-bayesian-compression-for-deep}
C.~{Louizos}, K.~{Ullrich}, and M.~{Welling}.
\newblock {Bayesian Compression for Deep Learning}.
\newblock \emph{ArXiv e-prints}, May 2017.

\bibitem[{Mnih} et~al.(2013){Mnih}, {Kavukcuoglu}, {Silver}, {Graves},
  {Antonoglou}, {Wierstra}, and {Riedmiller}]{mnih2013playing-atari-with}
V.~{Mnih}, K.~{Kavukcuoglu}, D.~{Silver}, A.~{Graves}, I.~{Antonoglou},
  D.~{Wierstra}, and M.~{Riedmiller}.
\newblock {Playing Atari with Deep Reinforcement Learning}.
\newblock \emph{ArXiv e-prints}, December 2013.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and
  Sun]{ren-2015-faster-r-cnn:-towards}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  91--99, 2015.

\bibitem[Rissanen(1978)]{rissanen-1978-automatica-modeling-by-shortest-data}
Jorma Rissanen.
\newblock Modeling by shortest data description.
\newblock \emph{Automatica}, 14\penalty0 (5):\penalty0 465--471, 1978.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 2015.

\bibitem[{Salimans} et~al.(2017){Salimans}, {Ho}, {Chen}, and
  {Sutskever}]{salimans-2017-arXiv-evolution-strategies-as-a-scalable}
T.~{Salimans}, J.~{Ho}, X.~{Chen}, and I.~{Sutskever}.
\newblock {Evolution Strategies as a Scalable Alternative to Reinforcement
  Learning}.
\newblock \emph{ArXiv e-prints}, March 2017.

\bibitem[Stanley \& Miikkulainen(2002)Stanley and
  Miikkulainen]{stanley-2002-EvoComp-evolving-neural-networks}
Kenneth Stanley and Risto Miikkulainen.
\newblock Evolving neural networks through augmenting topologies.
\newblock \emph{Evolutionary computation}, 10\penalty0 (2):\penalty0 99--127,
  2002.

\bibitem[Tenenbaum et~al.(2000)Tenenbaum, De~Silva, and
  Langford]{tenenbaum-2000-Science-a-global-geometric-framework}
Joshua~B Tenenbaum, Vin De~Silva, and John~C Langford.
\newblock A global geometric framework for nonlinear dimensionality reduction.
\newblock \emph{science}, 290\penalty0 (5500):\penalty0 2319--2323, 2000.

\bibitem[Tieleman \& Hinton(2012)Tieleman and
  Hinton]{tieleman-2012-lecture-6.5-rmsprop:-divide}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{NIPS}, 2016.

\bibitem[Yang et~al.(2015)Yang, Moczulski, Denil, de~Freitas, Smola, Song, and
  Wang]{yang-2015-CVPR-deep-fried-convnets}
Zichao Yang, Marcin Moczulski, Misha Denil, Nando de~Freitas, Alex Smola,
  Le~Song, and Ziyu Wang.
\newblock Deep fried convnets.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1476--1483, 2015.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}
