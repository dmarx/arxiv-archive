{
  "arxivId": "1804.08838",
  "title": "Measuring the Intrinsic Dimension of Objective Landscapes",
  "authors": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski",
  "abstract": "Many recently trained neural networks employ large numbers of parameters to\nachieve good performance. One may intuitively use the number of parameters\nrequired as a rough gauge of the difficulty of a problem. But how accurate are\nsuch notions? How many parameters are really needed? In this paper we attempt\nto answer this question by training networks not in their native parameter\nspace, but instead in a smaller, randomly oriented subspace. We slowly increase\nthe dimension of this subspace, note at which dimension solutions first appear,\nand define this to be the intrinsic dimension of the objective landscape. The\napproach is simple to implement, computationally tractable, and produces\nseveral suggestive conclusions. Many problems have smaller intrinsic dimensions\nthan one might suspect, and the intrinsic dimension for a given dataset varies\nlittle across a family of models with vastly different sizes. This latter\nresult has the profound implication that once a parameter space is large enough\nto solve a problem, extra parameters serve directly to increase the\ndimensionality of the solution manifold. Intrinsic dimension allows some\nquantitative comparison of problem difficulty across supervised, reinforcement,\nand other types of learning where we conclude, for example, that solving the\ninverted pendulum problem is 100 times easier than classifying digits from\nMNIST, and playing Atari Pong from pixels is about as hard as classifying\nCIFAR-10. In addition to providing new cartography of the objective landscapes\nwandered by parameterized models, the method is a simple technique for\nconstructively obtaining an upper bound on the minimum description length of a\nsolution. A byproduct of this construction is a simple approach for compressing\nnetworks, in some cases by more than 100 times.",
  "url": "http://arxiv.org/abs/1804.08838v1",
  "issue_number": 198,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/198",
  "created_at": "2024-12-24T02:44:47.418353",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 26,
  "last_read": "2024-12-24T02:44:47.419675"
}