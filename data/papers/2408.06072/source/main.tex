\documentclass{article}


% ready for submission
% \usepackage{neurips_2024}
% \usepackage[preprint]{neurips_2024}
\usepackage{iclr2025_conference}

\usepackage{graphicx} 
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}
\usepackage[usestackEOL]{stackengine}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage[most]{tcolorbox}
\usepackage{minitoc}
\usepackage{appendix}
\usepackage{titletoc}
\usepackage{float}
 
\newcommand{\hide}[1]{}

\newcommand{\model}{CogVideoX\xspace} 
\newcommand{\dong}[1]{\textbf{\color{red}[(Dong: #1 )]}}
\newcommand{\gxt}[1]{\textbf{\color{cyan}[Gu: #1 ]}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\aspace}{\hspace{1em}}

\newtcolorbox{promptbox}[1][]{
  breakable,
  title=#1,
  colback=gray!5,
  colframe=black,
  colbacktitle=gray!15,
  coltitle=black,
  fonttitle=\bfseries,
  bottomrule=1.5pt,
  toprule=1.5pt,
  leftrule=1pt,
  rightrule=1pt,
  arc=0pt,
  outer arc=0pt,
  enhanced,
  before upper={\parindent=1.5em}
}


% author formatting
\usepackage{authblk}
\renewcommand\Authands{, } %
\renewcommand{\Authfont}{\bfseries}
\makeatletter
\renewcommand\AB@affilsepx{, \protect\Affilfont}
\makeatother

\title{
\includegraphics[width=0.07\textwidth]{images/logo.png}
CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer}

\author{Zhuoyi Yang$^{\star}$ \aspace  Jiayan Teng$^{\star}$ \aspace  Wendi Zheng  \aspace Ming Ding \aspace  Shiyu Huang \\ 
Jiazheng Xu \aspace
Yuanming Yang\aspace  Wenyi Hong\aspace  Xiaohan Zhang \aspace Guanyu Feng  \\ 
Da Yin  
\aspace Xiaotao Gu  \aspace  Yuxuan Zhang \aspace Weihan Wang  \aspace Yean Cheng \\ Ting Liu \aspace   Bin Xu \aspace  
  Yuxiao Dong \aspace  Jie Tang \\
~\\ 
\textnormal{Zhipu AI \aspace Tsinghua University}
}
% \affil[]{Zhipu AI \aspace Tsinghua University}
\affil[]{}


%  \hide{
% \author{Zhuoyi Yang$^{\star}$ \aspace {\bf Jiayan Teng}$^{\star}$ \aspace {\bf Wendi Zheng} \aspace {\bf Ming Ding} \aspace {\bf Shiyu Huang} \\
% \aspace {\bf Jiazheng Xu} \aspace
% {\bf Yuanming Yang} \aspace {\bf Xiaohan Zhang} \aspace {\bf Xiaotao Gu} \aspace  {\bf Guanyu Feng}\aspace \\
%  {\bf Da Yin} 
% \aspace {\bf Wenyi Hong} \aspace  {\bf Weihan Wang} \aspace
%  {\bf Yean Cheng} \aspace {\bf Yuxuan Zhang} \aspace  \\
%  {\bf Ting Liu} \aspace {\bf Bin Xu}  \aspace {\bf Yuxiao Dong} \aspace {\bf Jie Tang}~\\
% $^{1}$Zhipu AI \aspace $^{2}$Tsinghua University\\
% \textmd{\href{https://github.com/THUDM/CogVideo}{https://github.com/THUDM/CogVideo}} 
% }
% }

% $^1$Zhipu AI\ \ \ \ \ \ $^2$Tsinghua University 
% \href{https://github.com/THUDM/CogVideo}{https://github.com/THUDM/CogVideo}
 
% \newcommand{\yzy}{\textcolor{red}{\textbf{ZHUOYI}}}
% \newcommand{\tjy}{\textcolor{red}{\textbf{JIAYAN}}}
\newcommand{\anonymous}{{\textit{Anonymous}}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.\begin{document}

\begin{document}


\maketitle

\renewcommand{\thefootnote}{}
\footnotetext{*Equal contributions. Core contributors: Zhuoyi, Jiayan, Wendi, Ming, and Shiyu.}
\footnotetext{{\{yangzy22,tengjy24\}@mails.tsinghua.edu.cn, \{yuxiaod,jietang\}@tsinghua.edu.cn}}
\footnotetext[1]{Visiting our demo website \href{https://yzy-thu.github.io/CogVideoX-demo/}{demo} to watch more generated videos!}
\renewcommand{\thefootnote}{\arabic{footnote}}

% \footnotemark[1]

\begin{figure}[ht]
\vspace{-10mm}
\centering
\includegraphics[width=\textwidth]{images/front.jpg}
\caption{
CogVideoX can generate long-duration, high-resolution videos with coherent actions and rich semantics.}
\label{fig:exampleImage}
\end{figure}

\begin{abstract}
%  Previous video generation models often had limited movement and short durations, making it difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues and, train the CogVideoX., 
% To efficently model video data, we propose to levearge a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions. 
% To improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. 
% By employing a progressive training technique, \model is adept at producing coherent, long-duration videos characterized by significant motions. 
% In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method. 
% It significantly helps enhance the performance of \model, improving both generation quality and semantic alignment. 
% We introduce \model, large-scale diffusion transformer models designed for generating videos based on text prompts.

% Results show that \model demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. 
% The model weights of both the 3D Causal VAE and \model are publicly available at \anonymous.
%\url{https://github.com/THUDM/CogVideo}. 
% and \url{https://huggingfaces.co/THUDM/CogVideoX}. 

% Previous video generation models often had limited movement and short durations, making it difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues and, introduce \model, large-scale diffusion transformer models which can generate 768$\times$ 1360 at 16 fps, 10 seconds videos based on text prompts. 
% First, we propose to levearge a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, \model is adept at producing coherent, long-duration, different shape videos characterized by significant motions. 
% In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method. 
% It significantly helps enhance the performance of \model, improving both generation quality and semantic alignment. 
% Results show that \model demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. 
% The model weights of both the 3D Causal VAE and \model are publicly available at \anonymous.

We present \model, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768$\times$ 1360 pixels. 
Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. 
First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, \model is adept at producing coherent, long-duration, different shape videos characterized by significant motions. 
In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method,
% . It significantly helps enhance the performance of \model, improving both generation quality and semantic alignment. 
greatly contributing to the generation quality and semantic alignment. 
Results show that \model demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. 
% The model weights of the 3D Causal VAE, the video caption model, and CogVideoX are open-source.
The model weight of both 3D Causal VAE, Video caption model and \model are publicly available at \href{https://github.com/THUDM/CogVideo}{https://github.com/THUDM/CogVideo}.


\end{abstract}


\input{sections/introduction}
\input{sections/vae}
\input{sections/model}
\input{sections/pretraining}
\input{sections/data}

%%%%%\input{sections/abaltion}

\input{sections/results}



\input{sections/conclusion}


% \begin{comment}
\subsubsection*{Acknowledgments}
%This research was supported by Zhipu AI. Thanks to BiliBili for data support. Thanks to all our collaborators and partners from Knowledge Engineering Group (KEG) and Zhipu AI.

% We would like to thank Xiaohan Zhang, Da Yin, Guanyu Feng, Ting Liu, Wei Jia, Jiajun Xu and all the data annotators, infra-operating staff, collaborators, and partners as well as everyone at Zhipu AI and Tsinghua University not explicitly mentioned in the report who have provided support, feedback, and contributed to the \model.
We would like to thank all the data annotators, infrastructure operators, collaborators, and partners. We also extend our gratitude to everyone at Zhipu AI and Tsinghua University who have provided support, feedback, or contributed to the \model, even if not explicitly mentioned in this report.
We would also like to greatly thank BiliBili for technical discussions. 
% We would also like to greatly thank BiliBili for data support. 
% We would also like to thank Yuxuan Zhang and Wei Jia from Zhipu AI as well as the teams at Hugging Face, ModelScope, WiseModel, and others for their help on the open-sourcing efforts of the GLM family of models.

% \end{comment}

% \section*{References}
\bibliography{reference}
\bibliographystyle{iclr2025_conference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
% \mtcaddchapter  % 告诉 minitoc 开始新的章节目录



\appendix

\section*{Appendix Contents}  % 手写附录的目录
\begin{itemize}
    \item \textbf{Appendix A:} Training Details
    \item \textbf{Appendix B:} Loss Curve
    \item \textbf{Appendix C:} More Examples
    \item \textbf{Appendix D:} Image To Video Model
    \item \textbf{Appendix E:} Caption Upsampler
    \item \textbf{Appendix F:} Dense Video Caption Data Generation
    \item \textbf{Appendix G:} Video Caption Example
    \item \textbf{Appendix H:} Video to Video via CogVideoX and CogVLM2-Caption
    \item \textbf{Appendix I:} Human Evaluation Details
    \item \textbf{Appendix J:} Data Filtering Details
    
\end{itemize}

\input{appendix/training}
\input{appendix/loss_curve}
\input{appendix/more_examles}
\input{appendix/image2video}
\input{appendix/caption_upsampler}
\input{appendix/video_caption_gen}
\input{appendix/video_caption_example}
\input{appendix/video2video}
\input{appendix/human_evaluation}
\input{appendix/data_filter}

\end{document}