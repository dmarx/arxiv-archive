---
abstract: |
  We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768$\times$ 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at <https://github.com/THUDM/CogVideo>.
author:
- |
  Zhuoyi Yang$^{\star}$ Jiayan Teng$^{\star}$ Wendi Zheng Ming Ding Shiyu Huang  
  Jiazheng Xu Yuanming YangWenyi HongXiaohan Zhang Guanyu Feng  
  Da Yin Xiaotao Gu Yuxuan Zhang Weihan Wang Yean Cheng  
  Ting Liu Bin Xu Yuxiao Dong Jie Tang  
  Â   
  <span class="nodecor">Zhipu AI Tsinghua University</span>
bibliography:
- reference.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: " <span class=\"image placeholder\" original-image-src=\"images/logo.png\" original-image-title=\"\" width=\"7%\">image</span> CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer"
---





<figure id="fig:exampleImage">
<span class="image placeholder" data-original-image-src="images/front.jpg" data-original-image-title="" width="\textwidth"></span>
<figcaption> CogVideoX can generate long-duration, high-resolution videos with coherent actions and rich semantics.</figcaption>
</figure>

### Acknowledgments

We would like to thank all the data annotators, infrastructure operators, collaborators, and partners. We also extend our gratitude to everyone at Zhipu AI and Tsinghua University who have provided support, feedback, or contributed to the CogVideoX, even if not explicitly mentioned in this report. We would also like to greatly thank BiliBili for technical discussions.

# Appendix Contents

- **Appendix A:** Training Details

- **Appendix B:** Loss Curve

- **Appendix C:** More Examples

- **Appendix D:** Image To Video Model

- **Appendix E:** Caption Upsampler

- **Appendix F:** Dense Video Caption Data Generation

- **Appendix G:** Video Caption Example

- **Appendix H:** Video to Video via CogVideoX and CogVLM2-Caption

- **Appendix I:** Human Evaluation Details

- **Appendix J:** Data Filtering Details
