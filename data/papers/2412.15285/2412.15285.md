---
abstract: |
  Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, we formalize the concept of *two-phase pretraining* and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by $3.4$% and $17$% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends.
author:
- |
  Steven Y. Feng[^1]$^{2}$[^2], Shrimai Prabhumoye$^{*}$$^{1,3}$, Kezhi Kong$^{1}$, Dan Su$^{1}$,  
  **Mostofa Patwary$^{1}$, Mohammad Shoeybi$^{1}$, Bryan Catanzaro$^{1}$**  
  NVIDIA$^{1}$, Stanford University$^{2}$, Boston University$^{3}$  
  `sprabhumoye@nvidia.com`  
bibliography:
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  Maximize Your Data’s Potential: Enhancing LLM  
  Accuracy with Two-Phase Pretraining
---





# Introduction

Large language models (LLM) are typically pretrained on large amounts of data in the order of billions (B) or trillions (T) of tokens derived from multiple data sources such as web crawl, books, papers, patents, mathematical and legal documents, and so forth . To develop a state-of-the-art model, it is critical to understand the nature of these data sources and to make informed decisions about optimal data blending (how different data sources are weighed during pretraining) and training strategies. These decisions typically involve running multiple large-scale experiments to empirically investigate the optimal training data blend(s) and ordering of data.

<figure id="fig:train_pipeline">
<span class="image placeholder" data-original-image-src="figures/curriculum-fig.png" data-original-image-title="" width="100%"></span>
<figcaption>Diagram of our two phase training pipeline. Phase-1 blend encourages data diversity and phase-2 blend is focused on high quality datasets. </figcaption>
</figure>

Most advanced models  do not divulge information on the data blends that are used, nor the ablation studies informing the data mixing and ordering decisions. Recent works provide high-level data blend information about a small portion of pretraining by encouraging the upsampling of certain domains towards the end. In general, there exists a knowledge gap regarding how to craft and choose an optimal data blend(s) for the entire training process, and the generalizability of data blends and ordering strategies to larger token horizons and model sizes.

In this work, we address the above knowledge gap by understanding optimal data blends and ordering strategies for training LLM. We formalize and extensively explore a two-phase training approach (Figure ) that balances diversity and quality: phase-1 emphasizes diverse, high-quality web crawl data, while phase-2 focuses on high-quality data sources such as math, code, and wiki data. Specifically, in this work we propose to use downsampled data to prototype and explore multiple blends at a smaller scale of 1T tokens. We craft our blends based on quality of the data source and the number of epochs to be seen during pretraining. We then demonstrate the effectiveness of our approach at a 15T token scale using the full data.

We evaluated on a comprehensive set of downstream tasking covering knowledge, reasoning, coding and math benchmarks. Our experiments illustrate that a quality and epoch based blend is better than a blend based on natural distribution by $13.2$% and the two-phase approach is better than random ordering of data (blend is based on quality and epochs) by an average of $3.4$% across downstream tasks. Furthermore, our results on downsampled data generalize across longer 15T token horizons on full data and larger model sizes, demonstrating the scalability and robustness of the two-phase approach. We also provide a fine-grained quality analysis of web crawl data, revealing optimal blending strategies to balance diversity and quality.

We share and highlight a series of findings made to create blends and order in our two-phase approach. Our main contributions are:

# A Two-Phase Approach to Pretraining

In this work, we explore a two-phased approach to pretraining: phase-1 ($\mathcal{P}_1$) then phase-2 ($\mathcal{P}_2$). Figure  demonstrates our two-phased approach. In each phase, we explore different data blends based on the quality and number of epochs to be seen of a data source. In phase-1 ($\mathcal{P}_1$), we explore a general data distribution which consists of a mix of web crawl data, medium-quality data, and low amounts of high-quality data. In phase-2 ($\mathcal{P}_2$), we explore a blend which includes task data and emphasizes high-quality datasets such as math, code, and high-quality web crawl (§). As seen in Figure , our model sees the first general data blend during $\mathcal{P}_1$ for the majority of training, then a different data blend focused on high quality data during the shorter $\mathcal{P}_2$ of training.

The steps to create blends for $\mathcal{P}_1$ and $\mathcal{P}_2$ are: 1) Downsample a data source by a factor of $f$, 2) Estimate the quality of a data source (§), 3) Estimate the epochs to be seen in the whole pretraining (§) and finally 4) distribute the epochs appropriately in $\mathcal{P}_1$ and $\mathcal{P}_2$(§). The downsampling factor $f$ is based on the final total token budget which we assume to be $15$T similar to . Hence, for us $f=1/15$ i.e for each data source, the number of tokens available for pretraining is $1/15^{th}$ of the total token in that dataset. Downsampling helps to observe the impact of epochs of datasets at a smaller scale of 1T tokens and then can be used to scale the blend to a longer token horizon of 15T tokens using the full data.

#### Baselines:

Since our blends are based on quality and epoch based analyses of the data as well as the ordering of the data in the two phases, we consider the following two baselines: 1) Natural Distribution Blend (BASE-ND): This blend is based on ratio of the number of tokens available in each data source. The weight for each dataset is equal to the total number of tokens in that dataset divided by the sum of tokens available in all the datasets. This weighting is neither based on quality nor the epochs to be seen for the dataset. 2) Random Order Pretraining (BASE-RO): This blend is based on quality and epochs of each dataset but does not use two phases to train the model. The weight for each dataset here is the same as our two-phase approach but the order in which the the dataset is seen during pretraining is random.

# Experimental Setup

<div class="center">

</div>

## Data Sources

Our pretraining corpus spans a vast range of text data sources that cover several domains, types of data, and languages. We broadly divide our datasets into the following categories and their token counts in billions is shown in Table .

<div class="center">

</div>

## Data Blends for Each Phase

The final blends in $\mathcal{P}_1$ and $\mathcal{P}_2$ are based on quality and epoch based ablations shown in § and §. The insights from these studies are incorporated in Table  and .

In $\mathcal{P}_1$, we encourage diversity in data by including a high percentage of web crawl data which consists of high, medium, and low-quality crawl. We want to introduce a limited amount of high-quality data such as math, code, and wiki in $\mathcal{P}_1$. In $\mathcal{P}_2$, the emphasis is primarily on high-quality datasets and only includes a limited amount of medium-quality data. For example, in $\mathcal{P}_2$, we only use high-quality crawl instead of medium or low-quality (see §).

Table  details the five blends explored in $\mathcal{P}_1$. These blends are designed to compare the proportion of high-level categories with each other. The difference between $\mathtt{Blend1}$ and $\mathtt{Blend2}$ is that $\mathtt{Blend2}$ has less code and more medium-quality datasets compared to $\mathtt{Blend1}$. $\mathtt{Blend3}$ has less web crawl and more medium-quality datasets compared to $\mathtt{Blend1}$. $\mathtt{Blend4}$ has less web crawl and more high-quality datasets compared to $\mathtt{Blend1}$. $\mathtt{Blend5}$ is designed to have majority web crawl at the cost of code and medium-quality data.

Table  outlines the five blends explored in $\mathcal{P}_2$. In $\mathcal{P}_2$, we use more epochs and higher proportions of high-quality data such as high-quality web crawl, math, wiki, and code data. $\mathtt{Blend3}$ has more code and less medium-quality datasets compared to $\mathtt{Blend1}$, and $\mathtt{Blend4}$ has more high-quality web crawl and less medium-quality datasets compared to $\mathtt{Blend1}$. $\mathtt{Blend2}$ has a more balanced distribution among the data categories, while $\mathtt{Blend5}$ upsamples math data more heavily.

<div class="center">

</div>

## Model Specifications

We experiment using the Megatron model, an autoregressive causal left-to-right LLM, with the Tiktokenizer . We downsample all our data by factor $f=1/15$. Hence, only $1/15$ of the tokens shown in Table  will be available for pretraining. We perform all our investigations using an 8 billion parameter model trained on 1 trillion total tokens. Furthermore, we test our two-phase approach by scaling along two dimensions: (1) we scale the token horizon to 1.7T tokens on a 8B model, and (2) we scale the parameters of the model to 25B and train on 1T tokens. Additionally, we train a 8B model on 15T tokens on full data (not downsampled) to observe if decisions made with downsampled data scales. Specifics on model architecture and hyperparameters are shared in Appendix .

## Evaluation Suite

To comprehensively assess our models, we use various benchmarks that evaluate different capabilities. These can be broadly divided into the following 4 categories, of which we report the final averages. We assess 5-shot accuracy for MMLU , 0-shot accuracy[^3] for reasoning tasks: CommonsenseQA , ARC-Easy & Challenge , PIQA , WinoGrande , HellaSwag , OpenBookQA , RACE , 0-shot accuracy for code benchmarks: HumanEval (+) and MBPP (+) , and 8-shot chain-of-thought (CoT) accuracy for GSM8K . We also report a final overall *Avg.* for most results, which is an average over all individual evaluation tasks.

<div class="center">

</div>

# Results for Two-Phase Pretraining

We compare our best blends $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$[^4] using two-phase training with two baselines: 1) BASE-ND: the weights are determined by the tokens available in each dataset and are not based on quality, and 2) BASE-RO: the weights for all the datasets are the same in this and $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$. The only difference is the order in which the data is presented during training (random or two-phased). Table illustrates that using a quality and epoch based blend is on average $13.2$% better than natural distribution blend (compare BASE-RO vs BASE-ND) across downstream tasks. It also presents that using our two-phase training approach noticeably improves average accuracy by $3.4$% compared to BASE-RO and $17$% compared to BASE-ND. This empirically demonstrates that the strategy of two-phase training is useful and tasks such as code and math are sensitive to the ordering of high-quality data in the second phase.

We scale our best blend $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$ to 15T tokens and use the full dataset to train a 8B model. All the previous experiments are performed on downsampled data and 1T scale. This means that the number of epochs is constant in both the runs. Table  shows that blends crafted at smaller scale can generalize to longer token budgets if the quality and epochs of the datasets are maintained at scale. This shows the generalizability of our two-phase approach to pretraining as well as quality- and epoch-based approach to designing blends.

<div class="center">

</div>

## Determining Blends

<figure id="fig:phase1-val-loss">
<span class="image placeholder" data-original-image-src="figures/phase1_validation_loss.pdf" data-original-image-title="" width="90%"></span>
<figcaption>Phase-1 validation loss for different <span class="math inline">\(\mathcal{P}_1\)</span> blends.</figcaption>
</figure>

<div class="center">

</div>

As discussed in §, we explore five different blends for phase-1.[^5] We train an 8B model on downsampled data for 1T tokens for all five blends and eliminate blends based on a separately held-out validation split. Fig.  illustrates the validation loss for all five blends. As we can see, $\mathtt{Blend5}$ and $\mathtt{Blend2}$ had 2.8% and 2.1% higher validation loss, respectively, relative to $\mathtt{Blend4}$ at approx. 250B tokens. Hence, we discontinue these two blends at that point. Since, the validation loss of the remaining three blends was within a margin of 1%, we periodically evaluate their accuracy on downstream tasks. Table  shows the results of the remaining three phase-1 blends at various token counts. At each token evaluation point – 200B, 250B and 629B, we see that $\mathtt{Blend3}$ is consistently worse than the other two blends. Hence, we eliminate this blend after 629B tokens of training. For this experiment, we switch from $\mathcal{P}_1$ to $\mathcal{P}_2$ after $\approx$<!-- -->70% of training, i.e the last 30% of training is $\mathcal{P}_2$. In §, we explore varying the percentage of $\mathcal{P}_2$.

Results in Table  follow intuition since $\mathtt{Blend4}$ has the highest amount of high-quality data and is hence better than $\mathtt{Blend1}$ and $\mathtt{Blend3}$. $\mathtt{Blend3}$ has more medium-quality data at the cost of web crawl compared to $\mathtt{Blend1}$. This result confirms that books, papers, and $\mathtt{CC_{dv}}$ are of medium-quality compared to our high-quality datasets and our web crawl blend.

<div class="center">

</div>

Finally, we explore five different blends of $\mathcal{P}_2$ described in Table  in combination with $\mathcal{P}_1$-$\mathtt{Blend1}$ and $\mathcal{P}_1$-$\mathtt{Blend4}$. Hence, we have ten different combinations of $\mathcal{P}_1$ and $\mathcal{P}_2$ blends. Table  shows the results on all ten combinations of blends. We find that $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$ performs the best on average. Table  also presents the final results of $\mathcal{P}_1$-$\mathtt{Blend1}$ and $\mathcal{P}_1$-$\mathtt{Blend4}$ if only the $\mathcal{P}_1$ blend was continued for 1T tokens without ever switching to $\mathcal{P}_2$ blends. It shows that switching to any of the $\mathcal{P}_2$ blends for training is better than continuing the $\mathcal{P}_1$ blends for all metrics. We observe the largest absolute gains in GSM8K and code of 14.6% and 9.4%, respectively, for $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$.

<div class="center">

</div>

## Scaling

We further explore scaling our best blend along two dimensions: (1) a longer token horizon of 1.7 trillion tokens and (2) larger model size of 25B parameters. For a longer token horizon, we aim to assess whether the blend can be used as is or if adjustments are necessary to prevent overfitting (observed in §). Note that this is different from scaling to 15T token where we use the full data. Here we still use the downsampled data and scale to 1.7T tokens and hence the epochs seen of each dataset would be higher. Since high number of epochs of high-quality datasets are primarily seen in $\mathcal{P}_2$ of pretraining, we create a new blend, $\mathcal{P}_2$-$\mathtt{Blend6}$[^6], which is an epoch-adjusted version of $\mathcal{P}_2$-$\mathtt{Blend1}$ to ensure that we do not see more than 8 epochs of certain high-quality data sources like math and task data. Table  shows the comparison of scaling from 1T to 1.7T total tokens. We see that $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend6}$ is on average $2.2$% better than $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$, illustrating that we need to adjust our blends according to the epoch counts of high-quality data for optimal results. Both the 1.7T models are better than 1T, demonstrating that we can still obtain higher downstream accuracies by training on more tokens, even if it means training on more than 8 epochs of high-quality data.

We also investigate if our best blend can scale to a larger model size. Given the high number of epochs of high-quality data in $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$, we also want to determine if a model with a larger capacity might memorize the data and overfit on it. Figure  shows that the validation loss is always decreasing for the 25B model, indicating that there is no overfitting with $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$. Table  shows results for the 25B model compared to the 8B model on this data blend combination. Understandably, the 25B model is substantially better across the board, demonstrating that the two-phase training approach and data blend combination can also scale to larger model sizes.

<div class="center">

</div>

<figure>
<span class="image placeholder" data-original-image-src="figures/25b_validation_loss.pdf" data-original-image-title="" width="90%"></span>
<figcaption>Validation loss for the 25B model using two-phase training with <span class="math inline">\(\mathcal{P}_1\)</span>-<span class="math inline">\(\mathtt{Blend4}\)</span>-<span class="math inline">\(\mathcal{P}_2\)</span>-<span class="math inline">\(\mathtt{Blend1}\)</span>. </figcaption>
</figure>

# Ablations

This section details quality-based data blending, epoch study and percentage of phase-2 to be conducted in the pretraining. Additional fine-grained analyses of data blends and study on learning rate schedule to be used in phase-2 is shown in Appendix § and §.

## Quality-Based Data Blending

The data blends of our two-phase approach are mainly based on the assessment of each data source’s quality. Hence, we carry out extensive experiments to find an optimal data blend for web crawl documents. While previous work  mentions that web crawl documents like Common Crawl (CC) form a large majority of their pretraining data, none of them share a recipe on how to mix different slices of CC. Some recent work on constructing crawl-based pretraining datasets   directly use the high quality crawl documents in pretraining but provide no specific data mixing strategy. In this section, we provide comprehensive details on how to create a data blend for CC documents and use it effectively in our phase-1 and phase-2 of pretraining. Additionally, we provide a quality assessment of other datasets like $\mathtt{CC_{dv}}$, papers, books and our high quality datasets. We compare them with medium, and high quality web crawl to position them optimally in our $\mathcal{P}_1$ and $\mathcal{P}_2$ blends.

#### Quality-Based Blending for Web Crawl:

Each document in our web crawl data is classified into one of five quality categories: High, Medium-High, Medium, Medium-Low, and Low using the classifier from .

We investigate various blends using quality-based weighted sampling approach[^7] for all of web crawl data from 99 CC snapshots for our phase-1 of pretraining. The idea is to upsample high and medium-quality crawl documents while avoiding a high quantity of low-quality data. The overall idea for the four web crawl blends in Table  is to iteratively decrease the percentage of tokens from High and Medium-High and increase the tokens in the lower categories. The results in Table [^8] demonstrate that eliminating the tail-end of the web crawl data belonging to Medium-Low and Low quality categories is beneficial as opposed to to keeping them for diversity. Based on these results, we choose $\mathtt{CC}$-$\mathtt{Blend1}$ as the final data blend for web crawl documents to be used in all our final $\mathcal{P}_1$ blends (§ and Table ). For $\mathcal{P}_2$, we only use web crawl data that belongs to *High*-quality category.

<div class="center">

</div>

<figure id="fig:crawl_epoch">
<p><span class="image placeholder" data-original-image-src="figures/crawl_epoch.pdf" data-original-image-title="" width="95%">image</span><br />
</p>
<figcaption>MMLU accuracy (%) vs. number of epochs of high-quality crawl in the data mix.</figcaption>
</figure>

#### Quality Estimation of Other Datasets:

We assess how our $\mathtt{CC_{dv}}$, papers, books and high quality datasets such as math, code and Wiki compare to $\mathtt{CC}$-$\mathtt{Medium}$,$\mathtt{CC}$-$\mathtt{Medium}$-$\mathtt{High}$ and $\mathtt{CC}$-$\mathtt{High}$ quality crawl data. We continue training the last checkpoint of $\mathcal{P}_1$-$\mathtt{Blend4}$, for an additional 50B tokens using a data mix that consisted $66$% of the data being tested, mixed with $34$% of $\mathtt{CC}$-$\mathtt{High}$.

The results in Table show that $\mathtt{CC_{dv}}$, papers and books datasets have similar accuracies to $\mathtt{CC}$-$\mathtt{Medium}$-$\mathtt{High}$ on the majority of benchmarks, and lag behind $\mathtt{CC}$-$\mathtt{High}$. As such, we group them under the "medium-quality" data category for our experiments (see §). The high quality datasets have an average accuracy better than $\mathtt{CC}$-$\mathtt{High}$.

## Epoch-Based Analysis

We take the number of epochs of high quality datasets into account while creating our $\mathcal{P}_1$ and $\mathcal{P}_2$ blends. We experiment with different numbers of epochs for high-quality crawl, math, and task data.

Since, majority of web crawl is used in $\mathcal{P}_1$, we pretrained an 8B model with 1T tokens, using different epochs of high-quality crawl tokens in the data mix, and evaluate each model’s MMLU score. Note that we keep the overall percentage of web crawl the same in all the experiments. As we can see in Figure , increasing the number of high-quality tokens increases the MMLU score until 6 epochs. We primarily present MMLU score because these experiments do not include high amount of math or code data.

<div class="center">

</div>

Since, majority of math and task-data is seen in $\mathcal{P}_2$, Table  presents results for different numbers of epochs for them in $\mathcal{P}_2$. It shows that $\approx$ 8 epochs of math is a good balance while not sacrificing accuracy on MMLU and reasoning. For task data, all metrics generally improve with more epochs, although there appears to be diminishing returns on several past epoch 8. Note that 8 epochs of both math and task data corresponds to our best $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$ blend combo from §.

## Optimal Duration of Phase-2

We investigate the percentage of phase-2 to use in the whole pretraining regime. We experiment with 0 to 50% of $\mathcal{P}_2$ in the whole of pretraining. The longer the duration of $\mathcal{P}_2$, the shorter the duration of $\mathcal{P}_1$. We use the $\mathcal{P}_1$-$\mathtt{Blend4}$-$\mathcal{P}_2$-$\mathtt{Blend1}$ blend combination that we found best in §.

Table illustrates that a higher percentage of $\mathcal{P}_2$ until 40% is better overall, especially in math and code. Going above this, e.g. to $\mathcal{P}_2$ as 50% of training, downstream accuracies start to degrade across the board, potentially due to overfitting.

<div class="center">

</div>

# Related Work

Selecting and structuring pretraining datasets is important to improve model generalization and efficiency. emphasize openness and accessibility of models, assemble an open corpus of trillions of tokens for large-scale training, and release a truly Open Language Model, including its framework, training data, and code. Studies such as demonstrating that refined data selection impacts model accuracies more significantly than simply the quantity of data. But these studies are primarily aimed at CC data and they do not suggest any data mixing strategies for pretraining. provide a systematic approach to building effective LLM pretraining datasets with ablations on data attributes, and existing curation, selection, and sampling methods. In our work, we provide a systematic approach to craft data blends and to order the data in pretraining.

Strategic weighting and timing of data usage can also noticeably impact model accuracies. Techniques like domain upsampling towards the end of training have been shown to be effective. provide details about high level blends for their pretraining process. In contrast, our work provides fine grained details about the data blend creation process along with actionable steps that model developers can use to develop data blends and order. Prior work investigates optimizing data mixtures based on clustering methods, manually designed domain composition weights, proxy models or reference models to determine data composition weights and sample-level data selection. Our work primarily focuses on data ordering and scaling of data blends in pretraining and can be used in conjunction with other data sampling techniques.

Curriculum learning approaches inspired by human learning offer an ordered way to introduce data gradually to enhance model learning. investigate cognitively-motivated curriculum-based training including vocabulary, and objective curricula, and outline and the challenges and potential solutions for designing effective curricula. Our work shows that ordering of data based on quality in pretraining LLMs has a significant impact of downstream accuracies.

# Conclusion

In conclusion, through extensive experiments, we demonstrate the effectiveness of a two-phase pretraining approach for LLM. For the initial training phase, a more general data distribution consisting of mainly of web crawl proves most effective, while phase two benefits from a comprehensive data blend, with additional focus on math, code, and task data. Phase-two for the last $\approx$<!-- -->40% of training yields the best results, and over-extending it leads to diminishing returns. Increasing model size and token horizon further enhances accuracy, demonstrating the scalability of our approach. Importantly, we also show that considering both the quality of the data (including web crawl) and the number of epochs of each data source is crucial to attain optimal results and prevent overfitting.

# Limitations

Some limitations of our work include our present suite of models and evaluation benchmarks. We can extend our work and show the effectiveness of two-phase pretraining approach on more LLM architectures such as Mamba , other hybrid SSM based architectures  and mixture of experts . While our evaluation benchmarks are quite comprehensive, we could potentially expand to an even broader range of evaluations, including nuanced domain-specific or interactive tasks, or more theory of mind and developmental psychology-inspired benchmarks. This includes assessing capabilities such as analogical reasoning . Further, our scaling experiments could be expanded. Scaling up to hundreds of billions of parameters or significantly longer training may yield additional insights. Lastly, while our work focuses on two-phase training and shows its efficacy, we can potentially investigate multi-phase training, and the impact of the order of the phases. However, we believe this is more suited for future work. Overall, these are directions to potentially improve and expand upon our work. Despite these potential limitations, we feel that our current work is an insightful and useful contribution to the research community.

# Ethical Considerations

Our research uses publicly available and commonly used datasets in LLM development. These sources, including Common Crawl, Wikipedia, and code repositories, are widely adopted in the research community. We examined the quality and origins of our data, prioritizing high-quality, domain-relevant data sources to improve LLM capabilities in a responsible manner. However, web crawl data may inherently contain biases or inappropriate content despite filtering efforts. We used established data cleaning and quality assurance procedures but acknowledge that potential biases may persist and impact model behavior in certain circumstances.

We recognize that scaling models and exploring data blending strategies require significant computational resources, which may raise environmental concerns. To mitigate this, we focused on efficient training strategies, such as two-phase training, to improve accuracy without excessively increasing resource usage. Future studies could benefit from exploring energy-efficient training methods to further minimize the environmental impact.

Our models, data blends, and accompanying publication are intended solely for research purposes, with no intended real-world application without additional safety evaluations. We caution against deploying models based on our methods without thorough testing, as they may carry unknown risks, particularly when applied to tasks involving sensitive or personal information. Our work aims to advance the understanding of LLM training strategies, and we feel that it is an important contribution to the research community. We encourage researchers to expand upon our work while further investigating the ethical and societal implications of LLM.

# Model Specifications

We use RoPE position embeddings , RMSNorm layer normalization , with Grouped Query Attention . The maximum sequence length is 4096. We use a global batch size of 1536, and the Adam optimizer with $\beta=(0.9,0.95)$ and $\epsilon=1e$-$08$.

$\mathcal{P}_1$ training uses cosine $LR$ decay with an initial $LR$ of $3e$-$4$ and targeted to reach a min-$LR$ of $3e$-$6$ at the end of the full training run (both phases). We start $\mathcal{P}_2$ with the intermediate $LR$ reached at the end of $\mathcal{P}_1$, and anneal using cosine $LR$ decay to $3e$-$6$ (§). Our experiments are run using up to 1024 NVIDIA H100 GPUs.

# Detailed Two-Phase Pretraining Results (Reasoning, MMLU, Code)

Tables to contain detailed evaluation results of the major experiments reported in § for reasoning, MMLU, and code, broken down by individual categories and benchmarks. They correspond to the results found in Tables to in §.

Table  shows the comparison of $\mathtt{Blend1}$ and $\mathtt{Blend6}$ used in scaling experiments in Table . If we use the same $\mathtt{Blend1}$ as is and train for more number of tokens (1.7T) then the number of epochs seen of each dataset would be higher compared to 1T training.

<div class="center">

</div>

# Details of Quality-Based Data Blend

We first compare a baseline blend (ND) which uses the natural distribution of tokens with a smartly constructed weighted sampling blend (WS). ND is based on the number of tokens that belong in each category as opposed to utilizing the quality label i.e. if 59% of the tokens belong to *Low* then 59% of tokens seen during pretraining would belong to *Low*. We then create a data blend (WS) based on weighted sampling of high and medium-quality tokens. The idea is to upsample high and medium-quality crawl documents and not use the low-quality data at all. Table  shows the token percentages that belong to each of the five quality labels for both ND and WS blends. Table  illustrates the results of the two models trained on the ND and WS data blends of web crawl, respectively. We see that our data blend (WS) outperforms on most of the evaluation tasks by a large margin, and the improvement on MMLU is substantial.

# Finegrained $\mathcal{P}_2$ Blend Experiments

We investigate fine-grained $\mathcal{P}_2$ blends to determine the optimal blend. For these experiments, we use a model trained on a $\mathcal{P}_1$ blend for 900B tokens (10% $\mathcal{P}_2$ duration), with a linear $LR$ decay to $0$.

#### Crawl, Math, & Code:

We investigate different percentages of high-quality crawl, math, and code data as shown in Table . Table demonstrates that a higher amount of math data (i.e. 30%) helps across the board. However, code data results are mixed, as too much code without enough math ($\mathtt{CMC}$-$\mathtt{B1}$) seems to hurt all non-code metrics. Comparing ($\mathtt{CMC}$-$\mathtt{B2}$) vs. ($\mathtt{CMC}$-$\mathtt{B3}$), more than 15% code does not add as much value, as gains saturate. Trading off crawl data for more code data also slightly hurts MMLU. As such, we decide that a final blend consisting of a higher amount of crawl and math with a moderate amount of code seems best overall. This corresponds to $\mathtt{CMC}$-$\mathtt{B3}$ in Table , which consists of 30% crawl, 33% math, and 15% code.

#### Task Data:

Second, we investigate the inclusion of task data. Specifically, adding FLAN and synthetically-generated GSM8K-train data (similar to data augmentation approaches ) to the $\mathtt{CMC}$-$\mathtt{B3}$ blend. Our FLAN data consists of a mixture of normal FLAN and FLAN-CoT (chain-of-thought) data. We compare 10 and 20 epochs of FLAN. These blends can be found in Table , with the results in Table . We can see that including synthetic GSM8K-train and FLAN data noticeably improves GSM8K scores while not detrimenting the other benchmarks. In fact, FLAN data also helps further improve MMLU and reasoning. 20 epochs of FLAN seems better than 10 epochs overall. Hence, including task data for $\mathcal{P}_2$ of training seems to be a good idea.

#### All Data Mixture:

Lastly, we investigate a final $\mathcal{P}_2$ data mixture which is a combination of all the data sources we tried, including FLAN, GSM8K, and relatively higher amounts of math and code data. For this experiment, we use 30% upsampling with $LR$ cosine decay to $3e-6$. This blend can be found in Table , with the results at the bottom of Table .[^9] We find that mixing all data sources helps greatly with GSM8K, noticeably with coding and reasoning, while retaining accuracy on MMLU. Hence, the final $\mathcal{P}_2$ blends we investigate in § (Table ) are motivated by these ablations – they are blends of all data sources, including task data, with higher proportions of math and code.

# Annealing Learning Rate Schedule

We investigate different learning rate ($LR$) schedules for phase $\mathcal{P}_2$. Specifically, using the same $\mathcal{P}_2$ blend for a 10% duration, we try different $LR$ strategies. We compare cosine vs. linear $LR$ decay functions, and also compare decaying to a final $LR$ of $0$ vs. $3e$-$6$ (1% of the original $\mathcal{P}_1$ starting $LR$ of $3e$-$4$). Not decaying $LR$ entirely to 0 leaves room for post-training, which is likely preferable.

As seen in Table , there is a negligible difference between linear and cosine $LR$ decay, so we choose cosine decay for consistency with $\mathcal{P}_1$. We also see that $LR$ decay to $3e$-$6$ is comparable to decaying all the way to $0$, while leaving room for post-training. Hence, our final chosen annealing strategy is cosine $LR$ decay to $3e$-$6$, which we use for our final two-phase experiments in §.

<div class="center">

</div>

<div class="center">

</div>

<div class="center">

</div>

<div class="center">

</div>

[^1]: equal contribution

[^2]: Work done during internship at NVIDIA

[^3]: We use normalized accuracy for ARC-Easy, ARC-Challenge, PIQA, HellaSwag, and OpenBookQA.

[^4]: see § Tab.  on how we select this blend and § Tab.  for the duration of $\mathcal{P}_2$ for best results.

[^5]: More detailed evaluation results of the major experiments in this section broken down by individual reasoning, MMLU, and code benchmarks and categories can be found in §.

[^6]: We show comparison of $\mathtt{Blend1}$ and $\mathtt{Blend6}$ in Table .

[^7]: We show comparison of quality-based blending with natural token distribution-based blend in §.

[^8]: The average in this table is primarily based on reasoning tasks and MMLU because these blends do not have math or code data.

[^9]: The $\mathtt{CMC}$-$\mathtt{Blend3}$-$30\%$ result at the bottom of Table is also using 30% upsampling with $LR$ cosine decay to $3e-6$.
