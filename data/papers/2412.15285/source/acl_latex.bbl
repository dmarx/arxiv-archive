\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebron, and Sanghai}]{ainslie-etal-2023-gqa}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-main.298} {{GQA}: Training generalized multi-query transformer models from multi-head checkpoints}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4895--4901, Singapore. Association for Computational Linguistics.

\bibitem[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton}]{MBPP-austin2021programsynthesislargelanguage}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021.
\newblock \href {https://arxiv.org/abs/2108.07732} {Program synthesis with large language models}.
\newblock \emph{Preprint}, arXiv:2108.07732.

\bibitem[{Baumgartner et~al.(2020)Baumgartner, Zannettou, Keegan, Squire, and Blackburn}]{baumgartner2020pushshiftredditdataset}
Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020.
\newblock \href {https://arxiv.org/abs/2001.08435} {The pushshift reddit dataset}.
\newblock \emph{Preprint}, arXiv:2001.08435.

\bibitem[{Bisk et~al.(2019)Bisk, Zellers, Bras, Gao, and Choi}]{PIQA-bisk2019piqareasoningphysicalcommonsense}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi. 2019.
\newblock \href {https://arxiv.org/abs/1911.11641} {Piqa: Reasoning about physical commonsense in natural language}.
\newblock \emph{Preprint}, arXiv:1911.11641.

\bibitem[{Blakeney et~al.(2024)Blakeney, Paul, Larsen, Owen, and Frankle}]{domain-upsampling-blakeney2024doesdatasparkjoy}
Cody Blakeney, Mansheej Paul, Brett~W. Larsen, Sean Owen, and Jonathan Frankle. 2024.
\newblock \href {https://arxiv.org/abs/2406.03476} {Does your data spark joy? performance gains from domain upsampling at the end of training}.
\newblock \emph{Preprint}, arXiv:2406.03476.

\bibitem[{Brown et~al.(2020)}]{brown2020languagemodelsfewshotlearners}
Tom~B. Brown et~al. 2020.
\newblock \href {https://arxiv.org/abs/2005.14165} {Language models are few-shot learners}.
\newblock \emph{Preprint}, arXiv:2005.14165.

\bibitem[{Chen et~al.(2021)}]{HumanEval-chen2021evaluatinglargelanguagemodels}
Mark Chen et~al. 2021.
\newblock \href {https://arxiv.org/abs/2107.03374} {Evaluating large language models trained on code}.
\newblock \emph{Preprint}, arXiv:2107.03374.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{ARC-clark2018thinksolvedquestionanswering}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.
\newblock \href {https://arxiv.org/abs/1803.05457} {Think you have solved question answering? try arc, the ai2 reasoning challenge}.
\newblock \emph{Preprint}, arXiv:1803.05457.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{GSM8K-cobbe2021trainingverifierssolvemath}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.
\newblock \href {https://arxiv.org/abs/2110.14168} {Training verifiers to solve math word problems}.
\newblock \emph{Preprint}, arXiv:2110.14168.

\bibitem[{Computer(2023)}]{together2023redpajama}
Together Computer. 2023.
\newblock \href {https://github.com/togethercomputer/RedPajama-Data} {Redpajama: an open dataset for training large language models}.

\bibitem[{Dubey et~al.(2024{\natexlab{a}})Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024{\natexlab{a}}.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}.

\bibitem[{Dubey et~al.(2024{\natexlab{b}})}]{llama3-dubey2024llama3herdmodels}
Abhimanyu Dubey et~al. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2407.21783} {The llama 3 herd of models}.
\newblock \emph{Preprint}, arXiv:2407.21783.

\bibitem[{Feng et~al.(2021)Feng, Gangal, Wei, Chandar, Vosoughi, Mitamura, and Hovy}]{feng-etal-2021-survey}
Steven~Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.findings-acl.84} {A survey of data augmentation approaches for {NLP}}.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pages 968--988, Online. Association for Computational Linguistics.

\bibitem[{Feng et~al.(2024)Feng, Goodman, and Frank}]{feng-etal-2024-child}
Steven~Y. Feng, Noah Goodman, and Michael Frank. 2024.
\newblock \href {https://aclanthology.org/2024.emnlp-main.1231} {Is child-directed speech effective training data for language models?}
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 22055--22071, Miami, Florida, USA. Association for Computational Linguistics.

\bibitem[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy}]{gao2020pile800gbdatasetdiverse}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020.
\newblock \href {https://arxiv.org/abs/2101.00027} {The pile: An 800gb dataset of diverse text for language modeling}.
\newblock \emph{Preprint}, arXiv:2101.00027.

\bibitem[{Glorioso et~al.(2024)Glorioso, Anthony, Tokpanov, Whittington, Pilault, Ibrahim, and Millidge}]{glorioso2024zamba}
Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. 2024.
\newblock Zamba: A compact 7b ssm hybrid model.
\newblock \emph{arXiv preprint arXiv:2405.16712}.

\bibitem[{Gokaslan and Cohen(2019)}]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen. 2019.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}.

\bibitem[{Groeneveld et~al.(2024)}]{olmo-groeneveld-etal-2024-olmo}
Dirk Groeneveld et~al. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.841} {{OLM}o: Accelerating the science of language models}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15789--15809, Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{Gu and Dao(2023)}]{gu2023mamba}
Albert Gu and Tri Dao. 2023.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2021MMLU}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
\newblock \href {https://openreview.net/forum?id=d7KBjmI3GmQ} {Measuring massive multitask language understanding}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Kingma and Ba(2017)}]{kingma2017adammethodstochasticoptimization}
Diederik~P. Kingma and Jimmy Ba. 2017.
\newblock \href {https://arxiv.org/abs/1412.6980} {Adam: A method for stochastic optimization}.
\newblock \emph{Preprint}, arXiv:1412.6980.

\bibitem[{Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy}]{RACE-lai-etal-2017-race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.
\newblock \href {https://doi.org/10.18653/v1/D17-1082} {{RACE}: Large-scale {R}e{A}ding comprehension dataset from examinations}.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pages 785--794, Copenhagen, Denmark. Association for Computational Linguistics.

\bibitem[{Lauren\c{c}on et~al.(2022)}]{bigscience}
Hugo Lauren\c{c}on et~al. 2022.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2022/file/ce9e92e3de2372a4b93353eb7f3dc0bd-Paper-Datasets_and_Benchmarks.pdf} {The bigscience roots corpus: A 1.6tb composite multilingual dataset}.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 31809--31826. Curran Associates, Inc.

\bibitem[{Li et~al.(2024{\natexlab{a}})Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora et~al.}]{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al. 2024{\natexlab{a}}.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock \emph{arXiv preprint arXiv:2406.11794}.

\bibitem[{Li et~al.(2024{\natexlab{b}})}]{li2024datacomplm}
Jeffrey Li et~al. 2024{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=CNWdWn47IE} {Datacomp-{LM}: In search of the next generation of training sets for language models}.
\newblock In \emph{The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Li et~al.(2023)}]{li2023starcoder}
Raymond Li et~al. 2023.
\newblock \href {https://openreview.net/forum?id=KoFOg41haE} {Starcoder: may the source be with you!}
\newblock \emph{Transactions on Machine Learning Research}.
\newblock Reproducibility Certification.

\bibitem[{Lieber et~al.(2024)Lieber, Lenz, Bata, Cohen, Osin, Dalmedigos, Safahi, Meirom, Belinkov, Shalev-Shwartz et~al.}]{lieber2024jamba}
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et~al. 2024.
\newblock Jamba: A hybrid transformer-mamba language model.
\newblock \emph{arXiv preprint arXiv:2403.19887}.

\bibitem[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei et~al.}]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al. 2023.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock In \emph{International Conference on Machine Learning}, pages 22631--22648. PMLR.

\bibitem[{Longpre et~al.(2024)Longpre, Yauney, Reif, Lee, Roberts, Zoph, Zhou, Wei, Robinson, Mimno et~al.}]{longpre2024pretrainer}
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et~al. 2024.
\newblock A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 3245--3276.

\bibitem[{Martinez et~al.(2023)Martinez, McGovern, Goriely, Davis, Caines, Buttery, and Beinborn}]{martinez-etal-2023-climb}
Richard~Diehl Martinez, Hope McGovern, Zebulon Goriely, Christopher Davis, Andrew Caines, Paula Buttery, and Lisa Beinborn. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.conll-babylm.10} {{CLIMB} {--} curriculum learning for infant-inspired model building}.
\newblock In \emph{Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning}, pages 112--127, Singapore. Association for Computational Linguistics.

\bibitem[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{openbookQA-mihaylov-etal-2018-suit}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1260} {Can a suit of armor conduct electricity? a new dataset for open book question answering}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 2381--2391, Brussels, Belgium. Association for Computational Linguistics.

\bibitem[{Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch, Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, and Gal}]{pmlr-v162-mindermann22a}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal. 2022.
\newblock \href {https://proceedings.mlr.press/v162/mindermann22a.html} {Prioritized training on points that are learnable, worth learning, and not yet learnt}.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 15630--15649. PMLR.

\bibitem[{Nvidia et~al.(2024)}]{nvidia2024nemotron4340btechnicalreport}
Nvidia et~al. 2024.
\newblock \href {https://arxiv.org/abs/2406.11704} {Nemotron-4 340b technical report}.
\newblock \emph{Preprint}, arXiv:2406.11704.

\bibitem[{OpenAI(2023)}]{tiktoken}
OpenAI. 2023.
\newblock tiktoken.
\newblock \url{https://github.com/openai/tiktoken}.
\newblock Accessed: 2024-12-14.

\bibitem[{OpenAI et~al.(2024)}]{gpt4-openai2024gpt4technicalreport}
OpenAI et~al. 2024.
\newblock \href {https://arxiv.org/abs/2303.08774} {Gpt-4 technical report}.
\newblock \emph{Preprint}, arXiv:2303.08774.

\bibitem[{Parmar et~al.(2024{\natexlab{a}})Parmar, Prabhumoye, Jennings, Liu, Jhunjhunwala, Wang, Patwary, Shoeybi, and Catanzaro}]{parmar-etal-2024-data}
Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo~Liu, Aastha Jhunjhunwala, Zhilin Wang, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2024{\natexlab{a}}.
\newblock \href {https://aclanthology.org/2024.emnlp-main.596} {Data, data everywhere: A guide for pretraining dataset construction}.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 10671--10695, Miami, Florida, USA. Association for Computational Linguistics.

\bibitem[{Parmar et~al.(2024{\natexlab{b}})}]{parmar2024nemotron415btechnicalreport}
Jupinder Parmar et~al. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2402.16819} {Nemotron-4 15b technical report}.
\newblock \emph{Preprint}, arXiv:2402.16819.

\bibitem[{Paster et~al.(2024)Paster, Santos, Azerbayev, and Ba}]{paster2024openwebmath}
Keiran Paster, Marco~Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2024.
\newblock \href {https://openreview.net/forum?id=jKHmjlpViu} {Openwebmath: An open dataset of high-quality mathematical web text}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Penedo et~al.(2024{\natexlab{a}})Penedo, Kydl{\'\i}{\v{c}}ek, allal, Lozhkov, Mitchell, Raffel, Werra, and Wolf}]{fineweb-penedo2024the}
Guilherme Penedo, Hynek Kydl{\'\i}{\v{c}}ek, Loubna~Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro~Von Werra, and Thomas Wolf. 2024{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=n6SCkn2QaG} {The fineweb datasets: Decanting the web for the finest text data at scale}.
\newblock In \emph{The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Penedo et~al.(2024{\natexlab{b}})Penedo, Kydl{\'\i}{\v{c}}ek, Lozhkov, Mitchell, Raffel, Von~Werra, Wolf et~al.}]{penedo2024fineweb}
Guilherme Penedo, Hynek Kydl{\'\i}{\v{c}}ek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von~Werra, Thomas Wolf, et~al. 2024{\natexlab{b}}.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.
\newblock \emph{arXiv preprint arXiv:2406.17557}.

\bibitem[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi}]{WinoGrande-sakaguchi2019winograndeadversarialwinogradschema}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.
\newblock \href {https://arxiv.org/abs/1907.10641} {Winogrande: An adversarial winograd schema challenge at scale}.
\newblock \emph{Preprint}, arXiv:1907.10641.

\bibitem[{Shao et~al.(2024)Shao, Li, Fei, Yan, Lin, and Qiu}]{shao2024balanced}
Yunfan Shao, Linyang Li, Zhaoye Fei, Hang Yan, Dahua Lin, and Xipeng Qiu. 2024.
\newblock Balanced data sampling for language model training with clustering.
\newblock \emph{arXiv preprint arXiv:2402.14526}.

\bibitem[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean}]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}.

\bibitem[{Shen et~al.(2023)Shen, Tao, Ma, Neiswanger, Liu, Wang, Tan, Hestness, Vassilieva, Soboleva et~al.}]{shen2023slimpajama}
Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, et~al. 2023.
\newblock Slimpajama-dc: Understanding data combinations for llm training.
\newblock \emph{arXiv preprint arXiv:2309.10818}.

\bibitem[{Shoeybi et~al.(2020)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}]{shoeybi2020megatronlmtrainingmultibillionparameter}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020.
\newblock \href {https://arxiv.org/abs/1909.08053} {Megatron-lm: Training multi-billion parameter language models using model parallelism}.
\newblock \emph{Preprint}, arXiv:1909.08053.

\bibitem[{Snowflake(2024)}]{snowflake-arctic}
Snowflake. 2024.
\newblock Snowflake arctic: The best llm for enterprise ai — efficiently intelligent, truly open.
\newblock \url{https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-\\models-snowflake/}.
\newblock Accessed: 2024-12-14.

\bibitem[{Soldaini et~al.(2024)}]{soldaini-etal-2024-dolma}
Luca Soldaini et~al. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.840} {Dolma: an open corpus of three trillion tokens for language model pretraining research}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15725--15788, Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{{Stack Exchange}(Accessed 2024)}]{stackexchange}
{Stack Exchange}. Accessed 2024.
\newblock \href {https://archive.org/details/stackexchange} {Stack exchange data dump}.

\bibitem[{Su et~al.(2024)Su, Kong, Lin, Jennings, Norick, Kliegl, Patwary, Shoeybi, and Catanzaro}]{su2024nemotroncctransformingcommoncrawl}
Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2024.
\newblock \href {https://arxiv.org/abs/2412.02595} {Nemotron-cc: Transforming common crawl into a refined long-horizon pretraining dataset}.
\newblock \emph{Preprint}, arXiv:2412.02595.

\bibitem[{Su et~al.(2021)Su, Lu, Pan, Wen, and Liu}]{rope-paper}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu. 2021.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv preprint arXiv:2104.09864}.

\bibitem[{Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant}]{talmor-etal-2019-commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1421} {{C}ommonsense{QA}: A question answering challenge targeting commonsense knowledge}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4149--4158, Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Team et~al.(2024{\natexlab{a}})Team, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e} et~al.}]{team2024gemma}
Gemma Team, Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al. 2024{\natexlab{a}}.
\newblock Gemma 2: Improving open language models at a practical size.
\newblock \emph{arXiv preprint arXiv:2408.00118}.

\bibitem[{Team et~al.(2024{\natexlab{b}})}]{gemmateam2024gemmaopenmodelsbased}
Gemma Team et~al. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2403.08295} {Gemma: Open models based on gemini research and technology}.
\newblock \emph{Preprint}, arXiv:2403.08295.

\bibitem[{Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia, and Gitman}]{toshniwal2024openmathinstruct}
Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. 2024.
\newblock Openmathinstruct-1: A 1.8 million math instruction tuning dataset.
\newblock \emph{arXiv preprint arXiv:2402.10176}.

\bibitem[{Wang et~al.(2022)Wang, Chen, and Zhu}]{curriculum-survey}
Xin Wang, Yudong Chen, and Wenwu Zhu. 2022.
\newblock \href {https://doi.org/10.1109/TPAMI.2021.3069908} {A survey on curriculum learning}.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44(9):4555--4576.

\bibitem[{Webb et~al.(2023)Webb, Holyoak, and Lu}]{webb2023emergentanalogicalreasoninglarge}
Taylor Webb, Keith~J. Holyoak, and Hongjing Lu. 2023.
\newblock \href {https://arxiv.org/abs/2212.09196} {Emergent analogical reasoning in large language models}.
\newblock \emph{Preprint}, arXiv:2212.09196.

\bibitem[{Xie et~al.(2023{\natexlab{a}})Xie, Pham, Dong, Du, Liu, Lu, Liang, Le, Ma, and Yu}]{doremi-NEURIPS2023_dcba6be9}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy~S Liang, Quoc~V Le, Tengyu Ma, and Adams~Wei Yu. 2023{\natexlab{a}}.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2023/file/dcba6be91359358c2355cd920da3fcbd-Paper-Conference.pdf} {Doremi: Optimizing data mixtures speeds up language model pretraining}.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~36, pages 69798--69818. Curran Associates, Inc.

\bibitem[{Xie et~al.(2023{\natexlab{b}})Xie, Santurkar, Ma, and Liang}]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang. 2023{\natexlab{b}}.
\newblock Data selection for language models via importance resampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:34201--34227.

\bibitem[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang et~al.}]{yang2024qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et~al. 2024.
\newblock Qwen2 technical report.
\newblock \emph{arXiv preprint arXiv:2407.10671}.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers-etal-2019-hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1472} {{H}ella{S}wag: Can a machine really finish your sentence?}
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4791--4800, Florence, Italy. Association for Computational Linguistics.

\bibitem[{Zhang and Sennrich(2019)}]{RMSNorm}
Biao Zhang and Rico Sennrich. 2019.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf} {Root mean square layer normalization}.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc.

\end{thebibliography}
