@article{blakeney2024does,
  title={Does your data spark joy? Performance gains from domain upsampling at the end of training},
  author={Blakeney, Cody and Paul, Mansheej and Larsen, Brett W and Owen, Sean and Frankle, Jonathan},
  journal={arXiv preprint arXiv:2406.03476},
  year={2024}
}

@article{chen2024towards,
  title={Towards Effective and Efficient Continual Pre-training of Large Language Models},
  author={Chen, Jie and Chen, Zhipeng and Wang, Jiapeng and Zhou, Kun and Zhu, Yutao and Jiang, Jinhao and Min, Yingqian and Zhao, Wayne Xin and Dou, Zhicheng and Mao, Jiaxin and others},
  journal={arXiv preprint arXiv:2407.18743},
  year={2024}
}

@article{winata2023overcoming,
  title={Overcoming catastrophic forgetting in massively multilingual continual learning},
  author={Winata, Genta Indra and Xie, Lingjue and Radhakrishnan, Karthik and Wu, Shijie and Jin, Xisen and Cheng, Pengxiang and Kulkarni, Mayank and Preotiuc-Pietro, Daniel},
  journal={arXiv preprint arXiv:2305.16252},
  year={2023}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Yu and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{yu2024mates,
  title={MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models},
  author={Yu, Zichun and Das, Spandan and Xiong, Chenyan},
  journal={arXiv preprint arXiv:2406.06046},
  year={2024}
}

@inproceedings{
wettig2024qurating,
title={QuRating: Selecting High-Quality Data for Training Language Models},
author={Alexander Wettig and Aatmik Gupta and Saumya Malik and Danqi Chen},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=GLGYYqPwjy}
}

@article{grosse2023studying,
  title={Studying large language model generalization with influence functions},
  author={Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage, Nelson and Tamkin, Alex and Tajdini, Amirhossein and Steiner, Benoit and Li, Dustin and Durmus, Esin and Perez, Ethan and others},
  journal={arXiv preprint arXiv:2308.03296},
  year={2023}
}

@inproceedings{liu2021influence,
  title={Influence selection for active learning},
  author={Liu, Zhuoming and Ding, Hao and Zhong, Huaping and Li, Weijia and Dai, Jifeng and He, Conghui},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9274--9283},
  year={2021}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

@inproceedings{vermorel2005multi,
  title={Multi-armed bandit algorithms and empirical evaluation},
  author={Vermorel, Joannes and Mohri, Mehryar},
  booktitle={European conference on machine learning},
  pages={437--448},
  year={2005},
  organization={Springer}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%         10         %%%%%%%%%%%%%%%%%%%%%%%%
@book{pfeffermann2009sample,
  title={Sample surveys: design, methods and applications},
  author={Pfeffermann, Danny and Rao, Calyampudi Radhakrishna},
  year={2009},
  publisher={Elsevier}
}

@article{bae2022if,
  title={If influence functions are the answer, then what is the question?},
  author={Bae, Juhan and Ng, Nathan and Lo, Alston and Ghassemi, Marzyeh and Grosse, Roger B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17953--17967},
  year={2022}
}

@inproceedings{xialess,
  title={LESS: Selecting Influential Data for Targeted Instruction Tuning},
  author={Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@article{xie2023data,
  title={Data selection for language models via importance resampling},
  author={Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34201--34227},
  year={2023}
}

@article{abbas2023semdedup,
  title={Semdedup: Data-efficient learning at web-scale through semantic deduplication},
  author={Abbas, Amro and Tirumala, Kushal and Simig, D{\'a}niel and Ganguli, Surya and Morcos, Ari S},
  journal={arXiv preprint arXiv:2303.09540},
  year={2023}
}

@article{everaert2023gio,
  title={Gio: Gradient information optimization for training dataset selection},
  author={Everaert, Dante and Potts, Christopher},
  journal={arXiv preprint arXiv:2306.11670},
  year={2023}
}

@article{choe2024your,
  title={What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions},
  author={Choe, Sang Keun and Ahn, Hwijeen and Bae, Juhan and Zhao, Kewen and Kang, Minsoo and Chung, Youngseog and Pratapa, Adithya and Neiswanger, Willie and Strubell, Emma and Mitamura, Teruko and others},
  journal={arXiv preprint arXiv:2405.13954},
  year={2024}
}

@article{penedo2024fineweb,
  title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
  author={Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2406.17557},
  year={2024}
}

@inproceedings{zhang2024autonomous,
  title={Autonomous data selection with language models for mathematical texts},
  author={Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C},
  booktitle={ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models},
  year={2024}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%         20           %%%%%%%%%%%%%%%%


@article{marion2023less,
  title={When less is more: Investigating data pruning for pretraining llms at scale},
  author={Marion, Max and {\"U}st{\"u}n, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara},
  journal={arXiv preprint arXiv:2309.04564},
  year={2023}
}



@inproceedings{albalak2023efficient,
  title={Efficient online data mixing for language model pre-training},
  author={Albalak, Alon and Pan, Liangming and Raffel, Colin and Wang, William Yang},
  booktitle={R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models},
  year=2023
}

@article{xie2024doremi,
  title={Doremi: Optimizing data mixtures speeds up language model pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy S and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lin2024rho,
  title={Rho-1: Not all tokens are what you need},
  author={Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and Shen, Yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and others},
  journal={arXiv preprint arXiv:2404.07965},
  year={2024}
}

@article{slimpajama,
    title={https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama url: https://huggingface.co/datasets/cerebras/SlimPajama-627B},
    author={None},
    journal={None},
    year=2023
}

@article{soldaini2024dolma,
  title={Dolma: An open corpus of three trillion tokens for language model pretraining research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  journal={arXiv preprint arXiv:2402.00159},
  year={2024}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@article{lee2021deduplicating,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2107.06499},
  year={2021}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{tang2024rethinking,
  title={Rethinking optimization and architecture for tiny language models},
  author={Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe},
  journal={arXiv preprint arXiv:2402.02791},
  year={2024}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%          30         %%%%%%%%%%%%%%%%%%%%%%%




@article{muennighoff2024scaling,
  title={Scaling data-constrained language models},
  author={Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{pruthi2020estimating,
  title={Estimating training data influence by tracing gradient descent},
  author={Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19920--19930},
  year={2020}
}

@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}




@inproceedings{wang2020optimizing,
  title={Optimizing data usage via differentiable rewards},
  author={Wang, Xinyi and Pham, Hieu and Michel, Paul and Anastasopoulos, Antonios and Carbonell, Jaime and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={9983--9995},
  year={2020},
  organization={PMLR}
}



@article{su2023enhanced,
  title={Enhanced transformer with rotary position embedding., 2021},
  author={Su, J and Lu, Y and Pan, S and Murtadha, A and Wen, B and Roformer, Y Liu},
  journal={DOI: https://doi. org/10.1016/j. neucom},
  year={2023}
}

@inproceedings{ankner2024perplexed,
  title={Perplexed by Perplexity: Perplexity-Based Pruning with Small Reference Models},
  author={Ankner, Zachary and Blakeney, Cody and Sreenivasan, Kartik and Marion, Max and Leavitt, Matthew L and Paul, Mansheej},
  booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year=2024
}


@article{albalak2024improving,
  title={Improving few-shot generalization by exploring and exploiting auxiliary data},
  author={Albalak, Alon and Raffel, Colin A and Wang, William Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%          40          %%%%%%%%%%%%%%%%%%%%%%%%%

@article{gao10256836framework,
  title={A framework for few-shot language model evaluation, 12 2023},
  author={Gao, L and Tow, J and Abbasi, B and Biderman, S and Black, S and DiPofi, A and Foster, C and Golding, L and Hsu, J and Le Noac’h, A and others},
  journal={URL https://zenodo. org/records/10256836},
  volume={7},
  year=2023
}

@inproceedings{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@inproceedings{lai2017race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={785--794},
  year={2017}
}

@inproceedings{talmor2019commonsenseqa,
  title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4149--4158},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{sap2019social,
  title={Social IQa: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4463--4473},
  year={2019}
}

@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{welbl2017crowdsourcing,
  title={Crowdsourcing multiple choice science questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  journal={arXiv preprint arXiv:1707.06209},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}


@article{engstrom2024dsdm,
  title={Dsdm: Model-aware dataset selection with datamodels},
  author={Engstrom, Logan and Feldmann, Axel and Madry, Aleksander},
  journal={arXiv preprint arXiv:2401.12926},
  year={2024}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}


@article{tirumala2023d4,
  title={D4: Improving llm pretraining via document de-duplication and diversification},
  author={Tirumala, Kushal and Simig, Daniel and Aghajanyan, Armen and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53983--53995},
  year={2023}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{sachdeva2024train,
  title={How to Train Data-Efficient LLMs},
  author={Sachdeva, Noveen and Coleman, Benjamin and Kang, Wang-Cheng and Ni, Jianmo and Hong, Lichan and Chi, Ed H and Caverlee, James and McAuley, Julian and Cheng, Derek Zhiyuan},
  journal={arXiv preprint arXiv:2402.09668},
  year={2024}
}

@article{meng2021coco,
  title={Coco-lm: Correcting and contrasting text sequences for language model pretraining},
  author={Meng, Yu and Xiong, Chenyan and Bajaj, Payal and Bennett, Paul and Han, Jiawei and Song, Xia and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23102--23114},
  year={2021}
}

@article{bajaj2022metro,
  title={Metro: Efficient denoising pretraining of large scale autoencoding language models with model generated signals},
  author={Bajaj, Payal and Xiong, Chenyan and Ke, Guolin and Liu, Xiaodong and He, Di and Tiwary, Saurabh and Liu, Tie-Yan and Bennett, Paul and Song, Xia and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2204.06644},
  year={2022}
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, K},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@article{meng2022pretraining,
  title={Pretraining text encoders with adversarial mixture of training signal generators},
  author={Meng, Yu and Xiong, Chenyan and Bajaj, Payal and Tiwary, Saurabh and Bennett, Paul and Han, Jiawei and Song, Xia},
  journal={arXiv preprint arXiv:2204.03243},
  year={2022}
}

@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{hoffmann2022empirical,
  title={An empirical analysis of compute-optimal large language model training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30016--30030},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{castro2023data,
  title={Data-sharing markets: model, protocol, and algorithms to incentivize the formation of data-sharing consortia},
  author={Castro Fernandez, Raul},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={2},
  pages={1--25},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{ghorbani2019data,
  title={Data shapley: Equitable valuation of data for machine learning},
  author={Ghorbani, Amirata and Zou, James},
  booktitle={International conference on machine learning},
  pages={2242--2251},
  year={2019},
  organization={PMLR}
}

@article{huang2023citation,
  title={Citation: A key to building responsible and accountable large language models},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2307.02185},
  year={2023}
}

@inproceedings{jia2019towards,
  title={Towards efficient data valuation based on the shapley value},
  author={Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and G{\"u}rel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1167--1176},
  year={2019},
  organization={PMLR}
}

@inproceedings{worledge2024unifying,
  title={Unifying corroborative and contributive attributions in large language models},
  author={Worledge, Theodora and Shen, Judy Hanwen and Meister, Nicole and Winston, Caleb and Guestrin, Carlos},
  booktitle={2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  pages={665--683},
  year={2024},
  organization={IEEE}
}

@inproceedings{zhao2023addressing,
  title={Addressing budget allocation and revenue allocation in data market environments using an adaptive sampling algorithm},
  author={Zhao, Boxin and Lyu, Boxiang and Fernandez, Raul Castro and Kolar, Mladen},
  booktitle={International Conference on Machine Learning},
  pages={42081--42097},
  year={2023},
  organization={PMLR}
}

@article{longpre2023pretrainer,
  title={A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity},
  author={Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and others},
  journal={arXiv preprint arXiv:2305.13169},
  year={2023}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{raventos2024pretraining,
  title={Pretraining task diversity and the emergence of non-bayesian in-context learning for regression},
  author={Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dong2023abilities,
  title={How abilities in large language models are affected by supervised fine-tuning data composition},
  author={Dong, Guanting and Yuan, Hongyi and Lu, Keming and Li, Chengpeng and Xue, Mingfeng and Liu, Dayiheng and Wang, Wei and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2310.05492},
  year={2023}
}

@inproceedings{zhao2023blessing,
  title={Blessing of Class Diversity in Pre-training},
  author={Zhao, Yulai and Chen, Jianshu and Du, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={283--305},
  year={2023},
  organization={PMLR}
}

@inproceedings{yu2022can,
  title={Can data diversity enhance learning generalization?},
  author={Yu, Yu and Khadivi, Shahram and Xu, Jia},
  booktitle={Proceedings of the 29th international conference on computational linguistics},
  pages={4933--4945},
  year={2022}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{wenzek2019ccnet,
  title={CCNet: Extracting high quality monolingual datasets from web crawl data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:1911.00359},
  year={2019}
}

@article{hadi2023survey,
  title={A survey on large language models: Applications, challenges, limitations, and practical usage},
  author={Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others},
  journal={Authorea Preprints},
  year={2023},
  publisher={Authorea}
}

@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}

@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}

@article{liu2020logiqa,
  title={Logiqa: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.08124},
  year={2020}
}

@inproceedings{ueno2020rich,
  title={Rich information is affordable: A systematic performance analysis of second-order optimization using K-FAC},
  author={Ueno, Yuichiro and Osawa, Kazuki and Tsuji, Yohei and Naruse, Akira and Yokota, Rio},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2145--2153},
  year={2020}
}