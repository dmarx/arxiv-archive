\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, and de~Freitas]{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, and Nando de~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Bartlett and Mendelson(2003)]{bartlett2003rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and {Gauss}ian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 2003.

\bibitem[Burer and Monteiro(2003)]{burer2003nonlinear}
Samuel Burer and Renato~DC Monteiro.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock \emph{Mathematical Programming}, 95\penalty0 (2):\penalty0 329--357,
  2003.

\bibitem[Chaudhari et~al.(2016)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2016entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{arXiv preprint arXiv:1611.01838}, 2016.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Ge et~al.(2011)Ge, Jiang, and Ye]{ge2011note}
Dongdong Ge, Xiaoye Jiang, and Yinyu Ye.
\newblock A note on the complexity of lp minimization.
\newblock \emph{Mathematical programming}, 2011.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{NIPS}, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason~D. Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock \emph{arXiv preprint}, 2018.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 1997.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Ji and Telgarsky(2018)]{ji2018risk}
Ziwei Ji and Matus Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300}, 2018.

\bibitem[Journ{\'e}e et~al.(2010)Journ{\'e}e, Bach, Absil, and
  Sepulchre]{journee2010low}
Michel Journ{\'e}e, Francis Bach, P-A Absil, and Rodolphe Sepulchre.
\newblock Low-rank optimization on the cone of positive semidefinite matrices.
\newblock \emph{SIAM Journal on Optimization}, 20\penalty0 (5):\penalty0
  2327--2351, 2010.

\bibitem[Kakade et~al.(2009)Kakade, Sridharan, and
  Tewari]{kakade2009complexity}
Sham~M Kakade, Karthik Sridharan, and Ambuj Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In \emph{Advances in neural information processing systems}, 2009.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
Jason~D. Lee, Max Simchowitz, Michael~I. Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{29th Annual Conference on Learning Theory}, 2016.

\bibitem[Li et~al.(2017)Li, Ma, and Zhang]{li2017algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix recovery.
\newblock \emph{arXiv preprint arXiv:1712.09203}, 2017.

\bibitem[Muresan(2009)]{muresan2009concrete}
Marian Muresan.
\newblock \emph{A concrete approach to classical analysis}, volume~14.
\newblock Springer, 2009.

\bibitem[Nacson et~al.(2018)Nacson, Lee, Gunasekar, Srebro, and
  Soudry]{nacson2018convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel
  Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock \emph{arXiv preprint arXiv:1803.01905}, 2018.

\bibitem[Neyshabur et~al.(2015{\natexlab{a}})Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
Behnam Neyshabur, Ruslan~R Salakhutdinov, and Nati Srebro.
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2015{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{b}})Neyshabur, Tomioka, and
  Srebro]{neyshabur2015search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In \emph{International Conference on Learning Representations},
  2015{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Tomioka, Salakhutdinov, and
  Srebro]{neyshabur2017geometry}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock \emph{arXiv preprint}, 2017.

\bibitem[Nguyen and Hein(2017)]{nguyen2017loss}
Quynh Nguyen and Matthias Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock \emph{arXiv preprint arXiv:1704.08045}, 2017.

\bibitem[Rockafellar(1979)]{rockafellar1979directionally}
R~Tyrrell Rockafellar.
\newblock Directionally lipschitzian functions and subdifferential calculus.
\newblock \emph{Proceedings of the London Mathematical Society}, 1979.

\bibitem[Smith(2018)]{Smith2018}
Le~Smith, Kindermans.
\newblock {Don't Decay the Learning Rate, Increase the Batch Size}.
\newblock In \emph{ICLR}, 2018.

\bibitem[Soudry et~al.(2017)Soudry, Hoffer, and Srebro]{soudry2017implicit}
Daniel Soudry, Elad Hoffer, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{arXiv preprint arXiv:1710.10345}, 2017.

\bibitem[Telgarsky(2013)]{telgarsky2013margins}
Matus Telgarsky.
\newblock Margins, shrinkage and boosting.
\newblock In \emph{Proceedings of the 30th International Conference on
  International Conference on Machine Learning-Volume 28}, pages II--307. JMLR.
  org, 2013.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\end{thebibliography}
