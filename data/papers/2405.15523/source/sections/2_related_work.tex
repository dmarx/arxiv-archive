
\subsection{Memorization in LLMs}

It has been previously established that LLMs are capable of memorizing training examples~\cite{song2017machine, yeom2018privacy}. Further works provided qualitative evidence by injecting artificial sequences into the training set~\cite{carlini2019secret, henderson2018ethical, thakkar2020understanding}, or matching the model's output with the known training dataset~\cite{carlini2021extracting}.

Membership inference attacks (MIAs) have been used to measure memorization. First proposed for data aggregates~\cite{homer2008resolving}, MIAs became a widely used tool to infer whether a given data point was used to train a machine learning (ML) model~\cite{shokri2017membership, sablayrolles2019white, nasr2018comprehensive}. In the context of LLMs, MIAs has been utulized to study the memorization of naturally occurring duplicates~\cite{kandpal2022deduplicating}, injected synthetic sequences~\cite{meeus2024copyright}, or with a neighbourhood sampling method~\cite{mattern2023membership}. Reference model loss has also been used to control for "hardness" of the example~\cite{kandpal2022deduplicating, duan2024membership}.

Separately, data extraction has been studied both as a privacy risk, and as a metric for memorization. Prior work provided scalable techniques~\cite{nasr2023scalable} and quantitative analysis~\cite{carlini2022quantifying} of training data extraction, demonstrating the risk to be highly prominent~\cite{lukas2023analyzing, mireshghallah2023can, zhang2023make, bordt2024elephants}. Model scale, level of duplication, and context size have been shown~\cite{kandpal2022deduplicating, carlini2022quantifying} to be the major factors affecting memorization. Finally, Biderman et al.~\cite{biderman2024emergent} has shown that memorization of large models can be extrapolated from lower-compute runs.

In this work, we study LLM training in the presence of fuzzy duplicates, and their impact on memorization. Importantly for our work, previous research~\cite{ippolito2022preventing, peng2023near} has considered a related question from the model output perspective, i.e. how often do models generate near-duplicates of its training data. Authors argue that studying verbatim regurgitation is too restrictive and does not address the risks associated with the generation of near-exact training samples. Ippolito et al.~\cite{ippolito2022preventing} adopts a permissive definition of a "duplicate" (based on BLEU score and Levenshtein distance), demonstrating that LLMs often produce near-duplicates of sequences from the training set. Peng at al.~\cite{peng2023near} proposes an efficient algorithm for finding near-duplicates in large datasets, and demonstrated that a notable proportion of sequences \emph{generated} by GPT-2 and GPT-Neo models have near-duplicates in their respective training datasets.

\subsection{Copyright and data provenance}

With the ongoing copyright lawsuits against model developers~\cite{nytimes,silvermanmeta,anthropic} and high-quality training data increasingly considered as a competitive advantage~\cite{penedo2023refinedweb}, it becomes less common for companies to disclose the full information on the training dataset~\cite{bommasani2023foundation}. As such, research has emerged to verify whether a certain data point has been used for training.

Relevant for this work, copyright traps~\cite{meeus2024copyright} and data watermarks~\cite{wei2024proving, wang2023wasa} have been proposed to improve membership inference. These methods propose injecting purposely designed synthetic sequences into the original text. Randomness in the injected sequences creates a basis for a hypothesis test, while controlled number of repetitions allows for statistically significant results even in models that do not exhibit extractable memorization. Both methods, however, are vulnerable to sequence-level deduplication of a training dataset.

Further methods for training data detection in language models often rely on either data extraction or MIAs. First, prompt crafting techniques have been suggested to elicit verbatim training data regurgitation from the model~\cite{duarte2024cop, karamolegkou2023copyright}. Second, document- and sequence-level MIAs~\cite{meeus2023did, shi2023detecting,mattern2023membership} have been proposed to infer the presence of a document in the training set. Later work~\cite{duan2024membership}, however, argues that much of a success of prior research on MIAs against LLMs could be attributed to a data distribution shift, and not model memorization, raising questions of their utility in practice.

In the image domain, methods have been proposed to add imperceptible perturbations to make the content traceable throughout the model training process, often referred to as \textit{radioactive data}~\cite{atli2022effectiveness, sablayrolles2020radioactive}. Further work introduced backdoor watermarking~\cite{li2020open}, clean-label poisoning~\cite{shafahi2018poison}, and data isotopes~\cite{wenger2024data}, relaxing assumptions on access to the model, full dataset, and labelling process.

\subsection{Training data deduplication} 

LLMs are typically trained on large text datasets scraped from the internet, and often contain a significant amount of duplicated pieces of text~\cite{broder1997syntactic, shivakumar1998finding}. Prior work has found that training data deduplication benefits LLM training by reducing computational cost to reach similar or even better performance~\cite{lee2022deduplicating,hernandez2022scaling,allamanis2019adverse,tirumala2024d4,hoffmann2022training,xue2024repeat}. Multiple strategies for large-scale deduplication of text have been proposed and put into practice for the creation of high-quality LLM training datasets. 

First, \emph{exact} deduplication has been performed at the document-, and then the substring-level. Lee et al.~\cite{lee2022deduplicating} use a suffix array~\cite{manber1993suffix} constructed on the full training dataset to remove exact duplicated substrings of at least $50$ tokens. This strategy has, for instance, been used to create the RefinedWeb Dataset~\cite{penedo2023refinedweb} and MADLAD-400~\cite{kudugunta2024madlad}. In the creation of Dolma~\cite{soldaini2024dolma}, exact deduplication is performed on the document- and paragraph-level using Bloom filters~\cite{bloom1970space}. 

Second, \emph{approximate} deduplication has been performed at the document level. For this, locality sensitive hashing schemes such as MinHash~\cite{broder1997resemblance} have been used to efficiently identify near-duplicate documents in large datasets. Lee et al.~\cite{lee2022deduplicating} uses MinHash to approximate the Jaccard distance~\cite{jaccard1912distribution} between all documents. Similar approaches to remove near-duplicates have been employed to create RefinedWeb Dataset~\cite{penedo2023refinedweb}, MADLAD-400~\cite{kudugunta2024madlad} and SlimPajama~\cite{cerebras2023slimpajama}, but also for the creation of the training dataset for a Chinese LLM~\cite{zeng2021pangu}, for Gopher~\cite{rae2021scaling}, Pythia~\cite{biderman2023pythia}, and for GPT-3~\cite{brown2020language}. Alternatively, semantic deduplication has also been proposed, removing documents whose embeddings computed using pre-trained LLMs are closer than a certain threshold~\cite{abbas2023semdedup,kaddour2023minipile}. 
