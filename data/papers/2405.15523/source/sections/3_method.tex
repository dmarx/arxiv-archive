
\subsection{Threat model}
\label{sec:threat_model}

We consider an attacker who is in possession of a collection of texts $\mathcal{T}$, which they can modify before publishing. In the context of copyright, this represents a content owner, who does not wish their content to be used for training without their explicit consent. Following Meeus et al.~\cite{meeus2024copyright}, we assume the attacker to inject synthetic text sequences, and refer to such sequences as \emph{trap sequences}.

We assume the attacker to synthetically generate a \emph{reference} trap sequence $X_{\text{ref}}$ using a reference language model $\textit{LM}_{\text{ref}}$ with tokenizer $T_{\text{ref}}$. The length of the $X_{\text{ref}}$ is defined by the number of tokens using reference tokenizer $T_{\text{ref}}$, or $L_{\text{ref}}(X_{\text{ref}}) = |T_{\text{ref}}(X_{\text{ref}})|$. We then consider the attacker to inject $n_{\text{dup}}$ sequences of text into $\mathcal{T}$ to create trap-injected texts $\mathcal{T}'$. Instead of injecting $n_{\text{dup}}$ exact duplications of $X_{\text{ref}}$ as originally proposed~\cite{meeus2024copyright}, we now consider the attacker to generate \emph{fuzzy} trap sequences $\{X_i \mid i=1 \ldots n_{\text{dup}}\}$ to be injected. Note that we always inject the reference trap sequence, or $X_1 = X_{\text{ref}}$, and that all fuzzy trap sequences $X_i$ are generated from $X_{\text{ref}}$ (See Sec.~\ref{sec:method_gen_fuzzy}).

We then assume the modified collection of texts $\mathcal{T}'$ to be made publicly available, and potentially picked up by LLM developers. The target model for the attacker is the language model \textit{LM} that has been pretrained on dataset $\mathcal{D}$. We also assume the attacker to have query access to \textit{LM}, which outputs predicted probabilities. The attacker's goal is now to infer whether $X_{\text{ref}}$ has been seen by \textit{LM}, or if $X_{\text{ref}} \in \mathcal{D}$ or not. In the context of copyright, the attacker can use a sequence-level MIA on the trap sequence $X_{\text{ref}}$ to infer whether their collection $\mathcal{T}'$ has been used for training, as $X_{\text{ref}}$ is synthetically generated to be unique to $\mathcal{T}$.

\subsection{Generating fuzzy trap sequences}
\label{sec:method_gen_fuzzy}

Given the reference trap sequence $X_{\text{ref}}$, we here propose the generation of fuzzy trap sequences $\{X_i \mid i=2 \ldots n_{\text{dup}}\}$ (we always include the reference sequence, so $X_1 = X_\text{ref}$). For this, we first use the tokenizer $T_{\text{MLM}}$ of a masked language model $\textit{MLM}$ to encode the reference trap sequence (a sequence of textual characters) to  $T_{\text{MLM}}(X_{\text{ref}}) = \{t_1,\ldots,t_N\}$. Note that here, due to the different tokenization of $T_{\text{ref}}$ and $T_{\text{MLM}}$, $N$ slightly differs from $L_{\text{ref}}$. 

We then generate $n_{\text{dup}}$ fuzzy duplicates for $X_{\text{ref}}$ by replacing $R$ tokens in  $T_{\text{MLM}}(X_{\text{ref}})$. Unless stated otherwise, we uniformly sample without replacement $R$ indices of tokens to be replaced. To replace a token, we randomly sample another, yet distinct token from the top $k$ tokens following the probability distribution predicted by the $\textit{MLM}$. Note that, when $k$ equals the size of the entire vocabulary $\mathcal{V}_{\text{MLM}}$ of tokenizer $T_{\text{MLM}}$, or $k=|\mathcal{V}_{\text{MLM}}|$, we replace the token with a randomly sampled token from $\mathcal{V}_{\text{MLM}}$. We provide further analysis of the impact of token replacements in Appendix~\ref{app:fuzzy_trap_sequences}.