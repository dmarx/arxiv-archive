
\textbf{Feasibility of copyright traps.} We here focus on addressing one core limitation of implementing copyright traps in practice: the potential accidental removal during training data deduplication~\cite{lee2022deduplicating,kudugunta2024madlad,penedo2023refinedweb}. We now consider other potential limitations. First, traps might be affected by quality filters. For instance, prior work has implemented filtering based on language ~\cite{penedo2023refinedweb,soldaini2024dolma}, certain heuristics (e.g. removing sequences with a high ratio of special characters)~\cite{kudugunta2024madlad,rae2021scaling,laurenccon2022bigscience} and perplexity~\cite{wenzek2019ccnet}. We here argue that trap sequences are designed to resemble human-generated text, and preprocessing techniques sufficiently aggressive to remove traps are also likely to hurt model utility. While we do not believe any of the current filtering techniques to affect fuzzy copyright traps, we leave for future work to explore if the wide variety of filtering mechanisms~\cite{albalak2024survey} could remove trap sequences. Second, (fuzzy) trap sequences likely impact the readability of the original content. However, as also argued when introduced~\cite{meeus2024copyright,wei2024proving}, traps injected in web-based publications can be made invisible to a human reader yet picked up by a scraper. Lastly, we acknowledge that traps can potentially be removed by an informed and motivated adversary with techniques targeted specifically at trap sequences.

\textbf{Privacy and confidentiality.} Our findings highlight new challenges in addressing privacy issues associated with LLMs. Sec.~\ref{sec:naturally_occuring} elaborates on challenges for post-hoc studies on LLM memorization. Further, we argue that deduplication does not necessarily eliminate privacy risks in LLMs~\cite{kandpal2022deduplicating}, as some duplicate sequences also tend to have many fuzzy duplicates in the same dataset. Thus, even the more aggressive sequence-level deduplication techniques would not be able to remove fuzzy duplicates and leave the data prone to memorization. 

\textbf{Potential for misuse.} With the injection of carefully designed fuzzy sequences and their memorization in a target LLM becoming feasible, we also acknowledge the potential misuse of this technology. For instance, malicious actors could leverage similar techniques to inject targeted misinformation into public content, which could potentially be propagated through LLMs deployed in practice. Future work could study whether our findings on memorization apply to the spread of misinformation and investigate potential mitigation strategies.  

