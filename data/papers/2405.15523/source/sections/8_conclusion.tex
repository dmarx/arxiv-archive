
Copyright traps are unique synthetic sequences injected into original content, enabling its detectability in LLM training. However, traps rely on exact duplication across the content -up to 1,000 times-, making them vulnerable to accidental removal as part of commonly deployed deduplication strategies. We here propose \emph{fuzzy} copyright traps. By replacing a small amount of tokens across duplication, traps become increasingly unlikely to be removed - while we find their detectability to only decrease slightly. In our setup, fuzzy trap sequences with $4$ token replacements in a sequence of $100$ tokens, the mean MIA AUC only drops from $0.90$ to $0.87$ - while now being highly unlikely to be removed by deduplication techniques. 

The fact that fuzzy duplicates are memorized to a similar extent as exact duplicates has implications for LLM memorization and confidentiality. First, we find that in a dataset widely used to train LLMs and study post-hoc memorization, The Pile, many duplicated sequences also have a significant amount of fuzzy duplicates - almost 30\% have at least one, but potentially many more, fuzzy duplicates with as little as 4 mismatched tokens.
We expect this to be a common feature of large text datasets, and to present a major confounding factor in studying memorization using duplicate sequences from such datasets.
Further, our findings highlight additional challenges in training LLMs on private or confidential data. We show that techniques like data deduplication might not be sufficient to eliminate risks of information leakage.