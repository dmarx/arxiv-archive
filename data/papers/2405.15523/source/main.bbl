\begin{thebibliography}{10}

\bibitem{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication.
\newblock {\em arXiv preprint arXiv:2303.09540}, 2023.

\bibitem{pile_uncopyrighted}
Devin Gulliver~(alt account).
\newblock Monology/pile-uncopyrighted Â· datasets at hugging face.

\bibitem{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock {\em arXiv preprint arXiv:2402.16827}, 2024.

\bibitem{allamanis2019adverse}
Miltiadis Allamanis.
\newblock The adverse effects of code duplication in machine learning models of code.
\newblock In {\em Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software}, pages 143--153, 2019.

\bibitem{atli2022effectiveness}
Buse~Gul Atli~Tekgul and N~Asokan.
\newblock On the effectiveness of dataset watermarking.
\newblock In {\em Proceedings of the 2022 ACM on International Workshop on Security and Privacy Analytics}, pages 93--99, 2022.

\bibitem{barnard2023self}
Francois Barnard, Marlize Van~Sittert, and Sirisha Rambhatla.
\newblock Self-diagnosis and large language models: A new front for medical misinformation.
\newblock {\em arXiv preprint arXiv:2307.04910}, 2023.

\bibitem{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In {\em Proceedings of the 2021 ACM conference on fairness, accountability, and transparency}, pages 610--623, 2021.

\bibitem{bertail2008bootstrapping}
Patrice Bertail, St{\'e}phan Cl{\'e}men{\c{c}}con, and Nicolas Vayatis.
\newblock On bootstrapping the roc curve.
\newblock {\em Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem{biderman2024emergent}
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff.
\newblock Emergent and predictable memorization in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der Wal.
\newblock Pythia: A suite for analyzing large language models across training and scaling, 2023.

\bibitem{bloom1970space}
Burton~H Bloom.
\newblock Space/time trade-offs in hash coding with allowable errors.
\newblock {\em Communications of the ACM}, 13(7):422--426, 1970.

\bibitem{bommasani2023foundation}
Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang.
\newblock The foundation model transparency index.
\newblock {\em arXiv preprint arXiv:2310.12941}, 2023.

\bibitem{bordt2024elephants}
Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, and Rich Caruana.
\newblock Elephants never forget: Memorization and learning of tabular data in large language models.
\newblock {\em arXiv preprint arXiv:2404.06209}, 2024.

\bibitem{broder1997resemblance}
Andrei~Z Broder.
\newblock On the resemblance and containment of documents.
\newblock In {\em Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)}, pages 21--29. IEEE, 1997.

\bibitem{broder1997syntactic}
Andrei~Z Broder, Steven~C Glassman, Mark~S Manasse, and Geoffrey Zweig.
\newblock Syntactic clustering of the web.
\newblock {\em Computer networks and ISDN systems}, 29(8-13):1157--1166, 1997.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{carlini2022quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{carlini2019secret}
Nicholas Carlini, Chang Liu, {\'U}lfar Erlingsson, Jernej Kos, and Dawn Song.
\newblock The secret sharer: Evaluating and testing unintended memorization in neural networks.
\newblock In {\em 28th USENIX Security Symposium (USENIX Security 19)}, pages 267--284, 2019.

\bibitem{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In {\em 30th USENIX Security Symposium (USENIX Security 21)}, pages 2633--2650, 2021.

\bibitem{duan2024membership}
Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi.
\newblock Do membership inference attacks work on large language models?
\newblock {\em arXiv preprint arXiv:2402.07841}, 2024.

\bibitem{duarte2024cop}
Andr{\'e}~V Duarte, Xuandong Zhao, Arlindo~L Oliveira, and Lei Li.
\newblock De-cop: Detecting copyrighted content in language models training data.
\newblock {\em arXiv preprint arXiv:2402.09910}, 2024.

\bibitem{faysse2024croissantllm}
Manuel Faysse, Patrick Fernandes, Nuno Guerreiro, Ant{\'o}nio Loison, Duarte Alves, Caio Corro, Nicolas Boizard, Jo{\~a}o Alves, Ricardo Rei, Pedro Martins, et~al.
\newblock Croissantllm: A truly bilingual french-english language model.
\newblock {\em arXiv preprint arXiv:2402.00786}, 2024.

\bibitem{anthropic}
FinancialTimes.
\newblock \url{https://www.ft.com/content/0965d962-5c54-4fdc-aef8-18e4ef3b9df5}, Oct 2023.

\bibitem{pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The {P}ile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem{projectgutenberg}
Michael Hart.
\newblock Project gutenberg.

\bibitem{henderson2018ethical}
Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan~Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau.
\newblock Ethical challenges in data-driven dialogue systems.
\newblock In {\em Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society}, pages 123--129, 2018.

\bibitem{hernandez2022scaling}
Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et~al.
\newblock Scaling laws and interpretability of learning from repeated data.
\newblock {\em arXiv preprint arXiv:2205.10487}, 2022.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models, 2022.

\bibitem{homer2008resolving}
Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John~V Pearson, Dietrich~A Stephan, Stanley~F Nelson, and David~W Craig.
\newblock Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays.
\newblock {\em PLoS genetics}, 4(8):e1000167, 2008.

\bibitem{ippolito2022preventing}
Daphne Ippolito, Florian Tram{\`e}r, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher~A Choquette-Choo, and Nicholas Carlini.
\newblock Preventing verbatim memorization in language models gives a false sense of privacy.
\newblock {\em arXiv preprint arXiv:2210.17546}, 2022.

\bibitem{jaccard1912distribution}
Paul Jaccard.
\newblock The distribution of the flora in the alpine zone. 1.
\newblock {\em New phytologist}, 11(2):37--50, 1912.

\bibitem{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock {\em arXiv preprint arXiv:2401.04088}, 2024.

\bibitem{kaddour2023minipile}
Jean Kaddour.
\newblock The minipile challenge for data-efficient language models.
\newblock {\em arXiv preprint arXiv:2304.08442}, 2023.

\bibitem{kandpal2022deduplicating}
Nikhil Kandpal, Eric Wallace, and Colin Raffel.
\newblock Deduplicating training data mitigates privacy risks in language models.
\newblock In {\em International Conference on Machine Learning}, pages 10697--10707. PMLR, 2022.

\bibitem{karamolegkou2023copyright}
Antonia Karamolegkou, Jiaang Li, Li~Zhou, and Anders S{\o}gaard.
\newblock Copyright violations and large language models.
\newblock In {\em The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem{wired}
Kate Knibbs.
\newblock The battle over books3 could change ai forever.
\newblock \href{https://www.wired.com/story/battle-over-books3/}{wired-battle-over-books3}, 2023.

\bibitem{kudugunta2024madlad}
Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat.
\newblock Madlad-400: A multilingual and document-level large audited dataset.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{laurenccon2022bigscience}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova~del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou, Eduardo Gonz{\'a}lez~Ponferrada, Huu Nguyen, et~al.
\newblock The bigscience roots corpus: A 1.6 tb composite multilingual dataset.
\newblock {\em Advances in Neural Information Processing Systems}, 35:31809--31826, 2022.

\bibitem{lee2022deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8424--8445, 2022.

\bibitem{li2020open}
Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, and Shu-Tao Xia.
\newblock Open-sourced dataset protection via backdoor watermarking.
\newblock {\em arXiv preprint arXiv:2010.05821}, 2020.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{silvermanmeta}
LLMLitigation.
\newblock Kadrey, silverman, golden v meta platforms, inc.
\newblock \url{https://llmlitigation.com/pdf/03417/kadrey-meta-complaint.pdf}, 2023.

\bibitem{lukas2023analyzing}
Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-B{\'e}guelin.
\newblock Analyzing leakage of personally identifiable information in language models.
\newblock In {\em 2023 IEEE Symposium on Security and Privacy (SP)}, pages 346--363. IEEE, 2023.

\bibitem{magar2022data}
Inbal Magar and Roy Schwartz.
\newblock Data contamination: From memorization to exploitation.
\newblock {\em arXiv preprint arXiv:2203.08242}, 2022.

\bibitem{manber1993suffix}
Udi Manber and Gene Myers.
\newblock Suffix arrays: a new method for on-line string searches.
\newblock {\em siam Journal on Computing}, 22(5):935--948, 1993.

\bibitem{mattern2023membership}
Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Sch{\"o}lkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick.
\newblock Membership inference attacks against language models via neighbourhood comparison.
\newblock {\em arXiv preprint arXiv:2305.18462}, 2023.

\bibitem{meeus2023did}
Matthieu Meeus, Shubham Jain, Marek Rei, and Yves-Alexandre de~Montjoye.
\newblock Did the neurons read your book? document-level membership inference for large language models.
\newblock {\em arXiv preprint arXiv:2310.15007}, 2023.

\bibitem{meeus2024copyright}
Matthieu Meeus, Igor Shilov, Manuel Faysse, and Yves-Alexandre de~Montjoye.
\newblock Copyright traps for large language models.
\newblock {\em arXiv preprint arXiv:2402.09363}, 2024.

\bibitem{mireshghallah2023can}
Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin Choi.
\newblock Can llms keep a secret? testing privacy implications of language models via contextual integrity theory.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{nasr2023scalable}
Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A~Feder Cooper, Daphne Ippolito, Christopher~A Choquette-Choo, Eric Wallace, Florian Tram{\`e}r, and Katherine Lee.
\newblock Scalable extraction of training data from (production) language models.
\newblock {\em arXiv preprint arXiv:2311.17035}, 2023.

\bibitem{nasr2018comprehensive}
Milad Nasr, Reza Shokri, and Amir Houmansadr.
\newblock Comprehensive privacy analysis of deep learning.
\newblock In {\em Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP)}, pages 1--15, 2018.

\bibitem{nytimes}
NewYorkTimes.
\newblock The times sues openai and microsoft over a.i. use of copyrighted work.
\newblock \url{https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html}, Dec 2023.

\bibitem{gpt4techreport}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \href{https://cdn.openai.com/papers/gpt-4.pdf}{https://cdn.openai.com/papers/gpt-4.pdf}, 2023.

\bibitem{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock {\em arXiv preprint arXiv:2306.01116}, 2023.

\bibitem{peng2023near}
Zhencan Peng, Zhizhi Wang, and Dong Deng.
\newblock Near-duplicate sequence search at scale for large language model memorization evaluation.
\newblock {\em Proceedings of the ACM on Management of Data}, 1(2):1--18, 2023.

\bibitem{kpullygutenberg}
Katherine Pully.
\newblock Gutenberg scraper.
\newblock \url{https://github.com/kpully/gutenberg_scraper}, 2020.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em arXiv e-prints}, 2019.

\bibitem{atlantic}
Alex Reisner.
\newblock These 183,000 books are fueling the biggest fight in publishing and tech.
\newblock \href{https://www.theatlantic.com/technology/archive/2023/09/books3-database-generative-ai-training-copyright-infringement/675363/}{the-atlantic-books3-copyright}, 2023.

\bibitem{sablayrolles2020radioactive}
Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv{\'e} J{\'e}gou.
\newblock Radioactive data: tracing through training.
\newblock In {\em International Conference on Machine Learning}, pages 8326--8335. PMLR, 2020.

\bibitem{sablayrolles2019white}
Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Herv{\'e} J{\'e}gou.
\newblock White-box vs black-box: Bayes optimal strategies for membership inference.
\newblock In {\em International Conference on Machine Learning}, pages 5558--5567. PMLR, 2019.

\bibitem{sainz2023did}
Oscar Sainz, Jon~Ander Campos, Iker Garc{\'\i}a-Ferrero, Julen Etxaniz, and Eneko Agirre.
\newblock Did chatgpt cheat on your test, 2023.

\bibitem{samuelson2023generative}
Pamela Samuelson.
\newblock Generative ai meets copyright.
\newblock {\em Science}, 381(6654):158--161, 2023.

\bibitem{schaeffer2023pretraining}
Rylan Schaeffer.
\newblock Pretraining on the test set is all you need.
\newblock {\em arXiv preprint arXiv:2309.08632}, 2023.

\bibitem{shafahi2018poison}
Ali Shafahi, W~Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{shi2023detecting}
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer.
\newblock Detecting pretraining data from large language models.
\newblock {\em arXiv preprint arXiv:2310.16789}, 2023.

\bibitem{shivakumar1998finding}
Narayanan Shivakumar and Hector Garcia-Molina.
\newblock Finding near-replicas of documents on the web.
\newblock In {\em International Workshop on the World Wide Web and Databases}, pages 204--212. Springer, 1998.

\bibitem{shokri2017membership}
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
\newblock Membership inference attacks against machine learning models.
\newblock In {\em 2017 IEEE symposium on security and privacy (SP)}, pages 3--18. IEEE, 2017.

\bibitem{cerebras2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}, June 2023.

\bibitem{solaiman2021process}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society (palms) with values-targeted datasets.
\newblock {\em Advances in Neural Information Processing Systems}, 34:5861--5873, 2021.

\bibitem{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock {\em arXiv preprint arXiv:2402.00159}, 2024.

\bibitem{song2017machine}
Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov.
\newblock Machine learning models that remember too much.
\newblock In {\em Proceedings of the 2017 ACM SIGSAC Conference on computer and communications security}, pages 587--601, 2017.

\bibitem{thakkar2020understanding}
Om~Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Fran{\c{c}}oise Beaufays.
\newblock Understanding unintended memorization in federated learning.
\newblock {\em arXiv preprint arXiv:2006.07490}, 2020.

\bibitem{tirumala2024d4}
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
\newblock D4: Improving llm pretraining via document de-duplication and diversification.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.
\newblock {\em URL https://arxiv. org/abs/2307.09288}, 2023.

\bibitem{authorsguild}
USAuthorsGuild.
\newblock More than 15,000 authors sign authors guild letter calling on ai industry leaders to protect writers.
\newblock \href{https://authorsguild.org/news/thousands-sign-authors-guild-letter-calling-on-ai-industry-leaders-to-protect-writers/}{authors-guild-open-letter}, 2023.

\bibitem{wang2023wasa}
Jingtan Wang, Xinyang Lu, Zitong Zhao, Zhongxiang Dai, Chuan-Sheng Foo, See-Kiong Ng, and Bryan Kian~Hsiang Low.
\newblock Wasa: Watermark-based source attribution for large language model-generated data.
\newblock {\em arXiv preprint arXiv:2310.00646}, 2023.

\bibitem{wei2024proving}
Johnny Tian-Zheng Wei, Ryan~Yixiang Wang, and Robin Jia.
\newblock Proving membership in llm pretraining data via data watermarks.
\newblock {\em arXiv preprint arXiv:2402.10892}, 2024.

\bibitem{wenger2024data}
Emily Wenger, Xiuyu Li, Ben~Y Zhao, and Vitaly Shmatikov.
\newblock Data isotopes for data provenance in dnns.
\newblock {\em Proceedings on Privacy Enhancing Technologies}, 2024.

\bibitem{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock {\em arXiv preprint arXiv:1911.00359}, 2019.

\bibitem{xue2024repeat}
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
\newblock To repeat or not to repeat: Insights from scaling llm under token-crisis.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{yeom2018privacy}
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.
\newblock Privacy risk in machine learning: Analyzing the connection to overfitting.
\newblock In {\em 2018 IEEE 31st computer security foundations symposium (CSF)}, pages 268--282. IEEE, 2018.

\bibitem{zeng2021pangu}
Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi~Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et~al.
\newblock Pangu-a: Large-scale autoregressive pretrained chinese language models with auto-parallel computation.
\newblock {\em arXiv preprint arXiv:2104.12369}, 2021.

\bibitem{zhang2023siren}
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, et~al.
\newblock Siren's song in the ai ocean: A survey on hallucination in large language models.
\newblock {\em arXiv preprint arXiv:2309.01219}, 2023.

\bibitem{zhang2023make}
Zhuo Zhang, Guangyu Shen, Guanhong Tao, Siyuan Cheng, and Xiangyu Zhang.
\newblock Make them spill the beans! coercive knowledge extraction from (production) llms.
\newblock {\em arXiv preprint arXiv:2312.04782}, 2023.

\end{thebibliography}
