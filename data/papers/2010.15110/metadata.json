{
  "arxivId": "2010.15110",
  "title": "Deep learning versus kernel learning: an empirical study of loss\n  landscape geometry and the time evolution of the Neural Tangent Kernel",
  "authors": "Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy, Surya Ganguli",
  "abstract": "In suitably initialized wide networks, small learning rates transform deep\nneural networks (DNNs) into neural tangent kernel (NTK) machines, whose\ntraining dynamics is well-approximated by a linear weight expansion of the\nnetwork at initialization. Standard training, however, diverges from its\nlinearization in ways that are poorly understood. We study the relationship\nbetween the training dynamics of nonlinear deep networks, the geometry of the\nloss landscape, and the time evolution of a data-dependent NTK. We do so\nthrough a large-scale phenomenological analysis of training, synthesizing\ndiverse measures characterizing loss landscape geometry and NTK dynamics. In\nmultiple neural architectures and datasets, we find these diverse measures\nevolve in a highly correlated manner, revealing a universal picture of the deep\nlearning process. In this picture, deep network training exhibits a highly\nchaotic rapid initial transient that within 2 to 3 epochs determines the final\nlinearly connected basin of low loss containing the end point of training.\nDuring this chaotic transient, the NTK changes rapidly, learning useful\nfeatures from the training data that enables it to outperform the standard\ninitial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid\nchaotic transient, the NTK changes at constant velocity, and its performance\nmatches that of full network training in 15% to 45% of training time. Overall,\nour analysis reveals a striking correlation between a diverse set of metrics\nover training time, governed by a rapid chaotic to stable transition in the\nfirst few epochs, that together poses challenges and opportunities for the\ndevelopment of more accurate theories of deep learning.",
  "url": "https://arxiv.org/abs/2010.15110",
  "issue_number": 245,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/245",
  "created_at": "2024-12-24T19:26:00.807486",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null
}