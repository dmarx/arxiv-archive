\section{Theoretical Preliminaries}

In this section, we introduce duality maps, a means of constructing duality maps based on norms, and finally a norm called the \textit{modular norm} that is well-suited to describe the geometry of general neural architectures.

\subsection{Duality Maps}\label{sec:steepest-descent}

Given a vector space $\mathcal{V}$, we say that a function $f:\mathcal{V} \to \R$ is a \textit{linear functional} on $\mathcal{V}$ if $f$ is linear. We define the \textit{dual space} $\mathcal{V}^*$ to be the set of linear functionals on the vector space $\mathcal{V}$. The dual space is itself a vector space provided that addition is defined pointwise $(f+g)(x) \defeq f(x) + g(x)$ and scalar multiplication is defined pointwise $(\alpha f)(x) \defeq \alpha f(x)$ for any scalar $\alpha$. By \textit{duality map} we simply mean any function that sends members of the dual vector space $\mathcal{V}^*$ to the primal vector space $\mathcal{V}$. The function need not be an involution.

Let $\el:\weights \to \R$ denote the loss of a differentiable machine learning model with weight space $\weights=\R^n$. The Taylor expansion of the loss at weight setting $\vw \in \weights$ is given by:
\begin{equation}\label{eq:taylor}
    \el(\vw+\Delta\vw) = \el(\vw) + \nabla_\vw\el(\vw)^\top \Delta \vw + \text{higher-order terms}.
\end{equation}
Observe that, in the first-order term, the gradient $\nabla_\vw\el(\vw)$ is acting as a linear functional: it is pairing with the weight vector $\Delta \vw \in \weights$ in a linear way to produce a real number. As such, we shall say that the gradient belongs to the dual weight space: $\nabla_\vw\el(\vw)\in\weights^*$. We shall forbid ourselves from directly subtracting a member of the dual weight space $\weights^*$ from the weight space $\weights$. If we would like to conduct a gradient descent update, then we had better find a duality map to send the gradient back to the primal space $\weights$.

This restriction may seem absurd! After all, here the weight space $\weights$ and its dual $\weights^*$ are both just $\R^n$. However, insisting upon this type check serves to remind us that the curvature of the loss function may be highly heterogeneous. The next section will show one way to construct duality maps to account for this.

\subsection{Steepest Descent on a Normed Space}\label{sec:steepest-descent-prelim}

Suppose that we have found a \textit{norm} $\norm{\cdot}:\weights\to\R$ and a \textit{sharpness parameter} $\lambda >0$ that serve as a good model of the higher-order terms in the Taylor expansion of the loss function given in \cref{eq:taylor}:
\begin{equation}\label{eq:taylor-approx}
    \el(\vw+\Delta\vw) \approx \el(\vw) + \nabla_\vw \el(\vw)^\top\Delta\vw + \frac{\lambda}{2}\cdot \norm{\Delta \vw}^2.
\end{equation}In other words, the norm provides a good characterization of the heterogeneity in curvature of the loss function. Then it makes sense to solve for a weight update $\Delta \vw$ by minimizing the right-hand side of \cref{eq:taylor-approx}. We will show that the minimizer can be expressed in terms of a \textit{dual norm} and a \textit{duality map}:

\begin{mydefinition}[Dual norm] Given a norm $\norm{\cdot}:\R^n\to\R$, the dual norm $\norm{\cdot}^\dagger$ of a vector $\vg \in \R^n$ is given by:
\begin{equation}
\norm{\vg}^\dagger\defeq\max_{\vt\in\R^n:\norm{\vt}=1} \vg^\top \vt.
\end{equation}
\end{mydefinition}

\begin{mydefinition}[Duality map based on a norm] Given a norm $\norm{\cdot}:\R^n\to\R$, we consider the duality map:
\begin{equation}
\dualize_{\norm{\cdot}} \vg \defeq \argmax_{\vt\in\R^n:\norm{\vt}=1} \vg^\top \vt,
\end{equation}
where, if the $\argmax$ is not unique, $\dualize_{\norm{\cdot}}$ returns any maximizer.    
\end{mydefinition}

Given these definitions, minimizing the expression in the right-hand side of \cref{eq:taylor-approx} can be done using the following standard proposition, for which \citet{bernstein2024old} provide a proof:
\begin{myproposition}[Steepest descent under a norm]\label{prop:steepest} For any $\vg \in \R^n$ thought of as ``the gradient'', any $\lambda \geq 0$ thought of as ``the sharpness'', and any norm $\norm{\cdot}:\R^n\to\R$ with dual norm $\norm{\cdot}^\dagger$ and duality map $\dualize_{\norm{\cdot}}$:
\begin{align}\label{eq:dual-steepest}
    \argmin_{\Delta \vw \in \R^n} \left[\vg^\top \Delta \vw + \frac{\lambda}{2} \, \norm{\Delta \vw}^2 \right] = - \frac{\norm{\vg}^\dagger}{\lambda} \times \dualize_{\norm{\cdot}} \vg.
\end{align}
\end{myproposition}
In words: to find the minimizer of a linear term penalized by a squared norm, we need only evaluate the dual norm and a duality map. In this paper, we focus on constructing a duality map for the \textit{modular norm}, which is defined on general neural architectures. The next section reviews duality maps for more standard norms.

\subsection{Basic Norms and Duality Maps}\label{sec:basic-norms}
Many basic norms and duality maps are already covered in prior work \citep{spectral-descent-1,spectral-descent-2,spectral-descent-4,flynn2017duality}. For some warmup examples, the following duality maps for vector norms are standard:
\begin{myexample}[Duality map for the Euclidean norm] For a vector $\vg\in\R^d$, we have $\dualize_{\norm{\cdot}_2} \vg = \vg / \norm{\vg}_2$.
\end{myexample}

\begin{myexample}[Duality map for the infinity norm] For a vector $\vg \in \R^d$, we have $\smash{\dualize_{\norm{\cdot}_\infty} \vg = \sign(\vg)}$, where the sign function is applied entrywise and we are free to take $\sign(0)=0$.
\end{myexample}
In neural networks, the weight spaces of individual layers tend to have matrix structure. And layers with the same shape weight matrix may have semantically different input and output spaces---think \textit{embedding} versus \textit{linear} layers in a transformer. As such, we will need duality maps for different \textit{induced operator norms}:
\begin{mydefinition}[Induced 
operator norm]\label{def:induced} Given a matrix $\mM\in\R^{d_\out \times d_\inn}$ and two normed vector spaces $(\R^{d_\inn},\norm{\cdot}_\alpha)$ and $(\R^{d_\out},\norm{\cdot}_\beta)$, the ``$\alpha$ to $\beta$'' induced operator norm is given by:
\begin{equation}
    \norm{\mM}_{\alpha\to\beta} = \max_{\substack{\vx \in \R^{d_\inn}}} 
    \frac{\norm{\mM\vx}_\beta}{\norm{\vx}_\alpha}.
\end{equation}
\end{mydefinition}
For tensors, we define the duality map via $\dualize_{\norm{\cdot}} \mG \defeq \argmax_{\norm{\mT}=1} \flatten(\mG)^\top \flatten(\mT)$. For linear layers, we will need the duality map for the $\rms\to\rms$ induced operator norm. This ends up as a rescaled version of the spectral norm duality map from prior work \citep{spectral-descent-4,flynn2017duality}.
\begin{myexample}[Duality map for the $\rms \to \rms$ operator norm] For a vector $\vv\in\R^d$, we define the RMS norm to be the normalized Euclidean norm: $\norm{\vv}_\rms = \norm{\vv}_2 / \sqrt{d}$. Given a matrix $\mW \in \R^{d_\out \times d_\inn}$, the $\rms \to \rms$ induced operator norm resolves to a rescaled spectral norm: $\norm{\mW}_{\rms \to \rms} = \sqrt{d_\inn / d_\out} \times \norm{\mW}_* $, where $\norm{\cdot}_*$ denotes the standard spectral norm. For a matrix $\mG \in \R^{d_\out \times d_\inn}$ with reduced singular value decomposition $\mG = \mU\mSigma\mV^\top$, the corresponding duality map is given by $\dualize_{\norm{\cdot}_{\rms \to \rms}} \mG = \sqrt{d_\out/d_\inn} \times \mU \mV^\top.$
\end{myexample}
And for embedding layers, we will need the duality map for the $\ell_1 \to \rms$ operator norm:
\begin{myexample}[Duality map for the $\ell_1 \to \rms$ operator norm] Given a matrix $\mW \in \R^{d_\out \times d_\inn}$, the $\ell_1 \to \rms$ induced operator norm resolves to the max $\rms$ norm of the columns: $\norm{\mW}_{\ell_1\to\rms} = \max_i \norm{\mathrm{col}_i(\mW)}_\rms$. For a matrix $\mG \in \R^{d_\out \times d_\inn}$, the corresponding duality map $\dualize_{\norm{\cdot}_{\ell_1 \to \rms}} \mG$ simply normalizes each column of $\mG$ to have unit RMS norm: $\mathrm{col}_i(\mG) \mapsto \mathrm{col}_i(\mG) / \norm{\mathrm{col}_i(\mG)}_\rms$ for each $i=1,...,d_\inn$.
\end{myexample}

\subsection{The Modular Norm}
\label{sec:modular-norm}

The \textit{modular norm} \citep{modula} is intended to help characterize the heterogeneous curvature of general neural architectures. The construction first defines an abstract \textit{module} type along with a notion of what is a good, or \textit{well-normed}, module. Then \textit{combination rules} are given for constructing new well-normed modules from a library of existing well-normed modules. So modules are a special case of \textit{combinator pattern} from functional programming \citep{haskellwiki_combinator}. Modules are also related to a \textit{monoidal category} from category theory \citep{fong2019invitation}. We begin by defining the abstract notion of a \textit{module}:

\begin{mydefinition}[Module]\label{def:module} Given input vector space $\inputs$, output vector space $\outputs$ and weight vector space $\weights$, a module $\module$ is an object with the following four attributes:
\begin{enumerate}[label=\normalfont(\alph*)]
\setlength\itemsep{0em}
    \item a function, $\module\for: \weights\times\inputs\to\outputs$, which maps an input and a weight vector to an output;
    \item a number, $\module\mass\geq0$, which is used to set the proportion of feature learning that this module contributes to any supermodule;
    \item a number, $\module\lip \geq0$, which estimates the module's sensitivity to input perturbations;
    \item a norm over the weight space, $\module\nor : \weights \to \R_{\geq0}$, sometimes abbreviated to just $\norm{\cdot}_\module$.
\end{enumerate}
\end{mydefinition}
We shall care most about modules that are \textit{well-normed}, which amounts to requiring that the forward function is Lipschitz-continuous in the weights with constant 1 and in the inputs with constant $\module\lip$:

\begin{mydefinition}[Well-normed module]\label{def:well-normed}
Let $\module$ be a module on $(\inputs, \outputs, \weights)$, where the input and output spaces have respective norms $\norm{\cdot}_{\inputs}$ and $\norm{\cdot}_{\outputs}$. $\module$ is well-normed if for all inputs $\vx\in\inputs$ and weights $\vw \in\weights$:
\begin{align}
\norm{\nabla_\vw\module\for(\vw,\vx)\diamond\Delta \vw}_\outputs &\leq \module\nor(\Delta \vw) &&\text{for all }\Delta\vw\in\weights;\\
\norm{\nabla_\vx\module\for(\vw,\vx)\diamond\Delta \vx}_\outputs & \leq \module\lip * \norm{\Delta \vx}_\inputs &&\text{for all }\Delta\vx\in\inputs.
\end{align}
\end{mydefinition}
The $\diamond$ operator denotes summation over any shared tensor indices. This definition of well-normed-ness can be used as a guiding principle in the design of a library of atomic (i.e. handwritten) modules. First, norms should be assigned to the input and output space of each module based on the semantics of $\module\for$. Then a norm $\module\nor$ should be assigned to the module's weight space and a number $\module\lip$ should be chosen to make the module well-normed. Examples are given in \cref{sec:atomic-duality}.

Given such a library of well-normed atomic modules, a compound module built through any arbitrary sequence of \textit{module compositions} and \textit{module concatenations} is automatically well-normed \citep{modula}. And if the atomic modules in the library are not only well-normed but are also \textit{smooth} in an appropriate sense, then \citet{modula} give an automatic procedure for computing \textit{sharpness coefficients} for any compound module built from the library. The relevant definition of module composition is as follows:

\begin{mydefinition}[Module composition]\label{def:composition} Consider module $\module_1$ with input, output and weight space $(\inputs_1,\outputs_1,\weights_1)$ and module $\module_2$ with input, output and weight space $(\inputs_2,\outputs_2,\weights_2)$. $\module_1$ and $\module_2$ are \textit{composable} if $\inputs_2 = \outputs_1$. Their composite module $\module=\module_2\circ\module_1$ has input, output and weight space $(\inputs_1,\outputs_2,\weights_1 \times \weights_2)$ and attributes:
\begin{enumerate}[label=\normalfont(\alph*)]
\setlength\itemsep{0em}
\item $\module\for((\vw_1,\vw_2),\vx)) = \module_2\for(\vw_2,\module_1\for(\vw_1,\vx))$;%

\item $\module\mass = \module_1\mass + \module_2\mass$;%

\item $\module\lip = \module_1\lip * \module_2\lip$;%

\item $\module\nor((\vw_1, \vw_2))$ given by:
\begin{equation*}\label{eq:norm_composition}
     \max\left(
    \module_2\lip * \frac{\module\mass}{\module_1\mass} * \module_1\nor(\vw_1),
    \frac{\module\mass}{\module_2\mass} * \module_2\nor(\vw_2)\right),
\end{equation*}
where if $\module_1\mass$ or $\module_2\mass$ is zero, the corresponding term in the $\max$ is set to zero.
\end{enumerate}
\end{mydefinition}
So the composite norm is taken to be a weighted max over the norms of the two sub-modules, where the weight space of the first module is coupled to the input sensitivity of the second module. The module masses provide freedom to tune the importance of each sub-module in the norm, and \citet{modula} prove that module mass provides precise control over the amount of feature learning that can happen in each sub-module.

Module concatenation is defined in a similar way to module composition:
\begin{mydefinition}[Module concatenation]\label{def:concatenation} Consider module $\module_1$ with input, output and weight space $(\inputs_1,\outputs_1,\weights_1)$ and module $\module_2$ with input, output and weight space $(\inputs_2,\outputs_2,\weights_2)$. We say that $\module_1$ and $\module_2$ are concatenatable if their input spaces match: $\inputs_1 = \inputs_2$. The tuple module $\module=(\module_1,\module_2)$ has input, output and weight space $(\inputs_1, \outputs_1\times\outputs_2, \weights_1\times\weights_2)$ and the following list of attributes:
\begin{enumerate}[label=\normalfont(\alph*)]
\setlength\itemsep{0em}
\item $\module\for((\vw_1,\vw_2),\vx)) = (\module_1\for(\vw_1,\vx), \module_2\for(\vw_2,\vx))$;%
\item $\module\mass = \module_1\mass + \module_2\mass$;%
\item $\module\lip = \module_1\lip + \module_2\lip$;%
\item $\module\nor(\vw_1, \vw_2)$ given by:
\begin{equation*}\label{eq:norm_addition}
    \max\left(
    \frac{\module\mass}{\module_1\mass} * \module_1\nor(\vw_1),
    \frac{\module\mass}{\module_2\mass} * \module_2\nor(\vw_2)\right),
\end{equation*}
where if $\module_1\mass$ or $\module_2\mass$ is zero, the corresponding term in the $\max$ is set to zero.
\end{enumerate}
\end{mydefinition}

A shortcoming of the paper by \citet{modula} is that the power of the modular norm is not fully leveraged. In particular, the authors do \textit{modular normalization} of training, where weight updates to modules are sometimes just na\"ively divided by their norm. In this paper we make fuller use of the geometry implied by the modular norm by constructing the corresponding duality map, which we call \textit{modular dualization}.
