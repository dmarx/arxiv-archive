\clearpage
\section{Discussion}
\label{sec:epilogue}

This paper develops the theory of \textit{modular duality} and the procedure of \textit{modular dualization} as means to construct duality maps for general neural architectures. Here, we comment on implications and connections.

\subsection{A Type System for Deep Learning}

Part of the inspiration for this work is the idea of building a fully-fledged \textit{type system} for deep learning. We think that activation spaces should be typed by their intended norm and the intended size of activations in that norm. This information would help in the construction of well-normed modules (see $\cref{sec:atomic-duality}$). Modules should be typed according to \cref{def:module}. And, as suggested in the introduction, gradients should be explicitly typed as dual vectors. A duality map should flip the type of a dual vector to a primal vector. We plan to use the Modula deep learning package \citep{modula} as a testbed for these ideas.

\subsection{Neural Network Speedrunning}

We believe that the ideas in this paper can help in the design of faster training methods. In fact, a new NanoGPT training speed record was recently set \citep{jordan2024cifar10} using a Newton-Schulz-based duality map. We communicated the method to Keller Jordan through our workshop paper \citep{bernstein2024old}.

\subsection{Modular Duality: A Unifying Theoretical Framework for Fast and Scalable Training}

An important topic in contemporary optimization research is the design of fast and scalable training methods for neural networks. In fact, the theme of the Optimization for Machine Learning workshop at this year's NeurIPS conference is ``scaling up optimization'' \citep{opt2024}. Two popular methods in this research space are \textit{maximal update parameterization} \citep[$\mu$P]{Yang2021TensorPI}, which allows for increasing network width without changing the optimal learning rate, and \textit{Shampoo} \citep{Gupta2018ShampooPS}, a variant of which \citep{Shi2023DistributedShampoo} won a speed challenge at the inaugural AlgoPerf optimization competition \citep{Dahl2023AlgoPerf}.

We showed in \cref{sec:atomic-duality} that essential features of both $\mu$P and Shampoo are recovered from the single duality map $\linear\dua$. We think that, on a basic theoretical level, $\mu$P and Shampoo should be viewed as partial approximations to this duality map. This observation helps put $\mu$P and Shampoo on a consistent theoretical footing, orients the methods with respect to overlooked prior work on spectral descent \citep{spectral-descent-4} and duality structure gradient descent \citep{flynn2017duality}, and suggests new ways to generalize these methods to arbitrary layer types and network architectures via the modular norm and modular dualization.

\subsection{On the Alignment of Activations and Updates}

Recent work \citep{my-spectral,everett2024scaling,modula} has singled out the following question as important to the design of scalable deep learning systems: \textit{to what extent do gradient updates to neural network layers align with incoming activation vectors?} This question is important since it helps inform how large weight updates need to be to induce a certain amount of change in layer outputs. Duality maps such as $\linear\dua$ and $\conv\dua$ may help simplify the answer to this question, since they project gradients to scaled semi-orthogonal matrices for which all singular values have the same magnitude.


\subsection{A Numerical Paradox: \textit{The Weights Don't Change!}}

Past work \citep{NEURIPS2019_0d1a9651,math9182246} has pointed out an apparent paradox in deep learning: the weights seem to move a vanishing amount from initialization in the limit of large network width. This finding has motivated a substantial amount of work on linearized training dynamics \citep{NTKjacot}. We attempted to resolve this paradox in prior work by showing that the weights move a roughly constant amount at any width when the change is measured in spectral norm \citep{my-spectral}. But duality maps change the story again: $\linear\dua$ ramps up the stable rank of updates, so the weights should move a non-trivial relative amount at large width \textit{even in the Frobenius norm}---provided the batch size is not too small.

\section{Conclusion}

This paper has proposed a recursive procedure called \textit{modular dualization} for building duality maps for general neural architectures. The procedure unifies past strands of optimization research on Shampoo \citep{Gupta2018ShampooPS} and $\mu$P \citep{Yang2021TensorPI}. Partial implementations have already led to significant wall-clock speedups in transformer training \citep{jordan2024cifar10}. The rectangular Newton-Schulz iteration provides a GPU-friendly and numerically stable means of dualizing under the $\rms\to\rms$ operator norm, while avoiding some of the downsides of sketching-based approaches \citep{spectral-descent-4}. Overall, we hope that our theory of \textit{modular duality} provides a clarifying toolkit for the design and analysis of deep learning systems.
