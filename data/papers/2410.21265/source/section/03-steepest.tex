\setlist{topsep=0pt,itemsep=0pt,parsep=5pt,partopsep=0pt}

\section{Modular Dualization}
\label{sec:steepest-descent-in-the-modular-norm}

In this section, we construct a duality map for general neural architectures. Our strategy is to first write down duality maps for atomic modules, i.e.\ individual layers. We then extend to arbitrary compound modules, i.e.\ full neural networks, by showing how duality maps should pass through composition and concatenation.

\subsection{Duality Maps for Atomic Modules}
\label{sec:atomic-duality}

To construct a duality map for an atomic module $\atom$, the idea is to first fix norms on the input and output spaces that respect the semantics of $\atom\for$. We should select norms that describe both how large we would like the inputs and outputs to be, and in what geometry we would like the outputs to evolve. Then we place a norm on the weight space such that $\atom$ is well-normed: this is typically the operator norm (\cref{def:induced}) induced by the input and output norms. Finally we are in position to solve for the duality map, which we shall call $\atom\dua$. We now give some examples of this procedure for the basic layer types of $\linear$, $\embed$ and $\conv$. The results are summarized in \cref{tab:dualize}.

We start with the canonical example of an atomic module:
\begin{myexample}[The $\linear$ module] The $\linear$ module sends inputs from $\inputs = \R^{d_\inn}$ to outputs in $\outputs = \R^{d_\out}$. The weight space is given by the matrix space $\weights = \R^{d_\out \times d_\inn}$. We endow the $\linear$ module with attributes:

\begin{enumerate}
    \setlength\itemsep{0em}
    \item $\linear\for(\mW,\vx) = \mW\vx$, the matrix-vector product;
    \item $\linear\lip = 1$;
    \item $\linear\mass = \mu$, where $\mu\geq 0$ is a hyperparameter;
    \item $\linear\nor(\mW) = \norm{\mW}_{\rms\to\rms}$, the $\rms\to\rms$ induced operator norm.
\end{enumerate}
Since the $\linear$ module is intended to map to and from vectors of roughly unit $\rms$ norm, we place the $\rms$ norm on both the input and output space: $\norm{\cdot}_\inputs = \norm{\cdot}_\rms$ and $\norm{\cdot}_\outputs = \norm{\cdot}_\rms$. Then $\linear$ is well-normed if the inputs and weights belong to the unit balls $\left\{\vx\in\R^{d_\inn}:\norm{\vx}_\inputs \leq 1\right\}$ and $\left\{\mW \in \R^{d_\out\times d_\inn}:\linear\nor(\mW)\leq1\right\}$. Referring back to \cref{sec:basic-norms}, the duality map corresponding to $\linear\nor$ is then given by:

\begin{enumerate}
\setcounter{enumi}{4}
\setlength\itemsep{0em}
    \item $\linear\dua(\mG) = \sqrt{\frac{d_\out}{d_\inn}} \times \mU\mV^\top$, where the gradient $\mG \in \R^{d_\out \times d_\inn}$ has reduced SVD $\mG = \mU \mSigma \mV^\top$.
\end{enumerate}
\end{myexample}

This single duality map recovers essential features of both \textit{maximal update parameterization} \citep[$\mu$P]{Yang2021TensorPI} and \textit{Shampoo} \citep{Gupta2018ShampooPS}. In particular, the factor of $\sqrt{d_\out / d_\inn}$ in $\linear\dua$ recovers spectral update scaling \citep{my-spectral} that leads to $\mu$P. (Initializing such that $\linear\nor(\mW)=1$ also recovers $\mu$P initialization scaling.) And the mapping $\mG \mapsto \mU\mV^\top$ is equivalent to Shampoo without accumulation \citep{bernstein2024old}. As such, we believe that duality maps may help reconcile different strands of deep learning research and provide a unifying basis for fast and scalable training algorithms.

The $\embed$ module provides a useful counterpoint to the $\linear$ module. The difference between the two modules stems from the fact that the input spaces of $\embed$ and $\linear$ have different semantics.
\begin{myexample}[The $\embed$ module] The $\embed$ module sends inputs from $\inputs = \R^{d_\inn}$ to outputs in $\outputs = \R^{d_\out}$. The weight space is given by the matrix space $\weights = \R^{d_\out \times d_\inn}$. We endow the $\embed$ module with attributes:

\begin{enumerate}
\setlength\itemsep{0em}
    \item $\embed\for(\mW,\vx) = \mW\vx$, the matrix-vector product;
    \item $\embed\lip = 1$;
    \item $\embed\mass = \mu$, where $\mu\geq 0$ is a hyperparameter;
    \item $\embed\nor(\mW) = \norm{\mW}_{\ell_1\to\rms}$, the $\ell_1\to\rms$ induced operator norm.
\end{enumerate}
$\embed$ is intended to map from one-hot vectors to vectors of roughly unit $\rms$ norm, so we place the $\ell_1$ norm on the input space and the $\rms$ norm on the output space: $\norm{\cdot}_\inputs = \norm{\cdot}_{\ell_1}$ and $\norm{\cdot}_\outputs = \norm{\cdot}_\rms$. Then $\embed$ is well-normed if the inputs and weights belong to the unit balls $\left\{\vx\in\R^{d_\inn}:\norm{\vx}_\inputs \leq 1\right\}$ and $\left\{\mW \in \R^{d_\out\times d_\inn}:\embed\nor(\mW)\leq1\right\}$. Referring back to \cref{sec:basic-norms}, the duality map for $\embed\nor$ is:

\begin{enumerate}
\setcounter{enumi}{4}
\setlength\itemsep{0em}
    \item $\embed\dua(\mG)$ performs the mapping $\mathrm{col}_j(\mG) \mapsto \frac{\mathrm{col}_j(\mG)}{ \norm{\mathrm{col}_j(\mG)}_\rms}$ for each column index $j=1,...,d_\inn$.
\end{enumerate}
\end{myexample}

Finally, we consider a $\conv$ module with a $k\times k$ kernel. $\conv$ has a more involved tensor structure than $\linear$ and $\embed$. The calculations work by slicing up the weight tensor into a collection of $k^2$ matrices.

\begin{myexample}[The $\conv$ module] The $\conv$ module sends inputs from $\inputs = \R^{W_\inn\times H_\inn\times d_\inn}$ to outputs in $\outputs = \R^{W_\out \times H_\out \times d_\out}$. We think of this as mapping an input image of width $W_\inn$, height $H_\inn$ and with $d_\inn$ color channels to an output image of width $W_\out$, height $H_\out$ and with $d_\out$ color channels. The weight space is given by the tensor space $\weights = \R^{d_\out \times d_\inn\times k \times k}$, where $k$ is the kernel size. We endow $\conv$ with attributes:

\begin{enumerate}
\setlength\itemsep{0em}
    \item $\conv\for(\mW,\vx) = \mW\circledast\vx$, where $\circledast$ denotes 2D convolution;
    \item $\conv\lip = 1$;
    \item $\conv\mass = \mu$, where $\mu\geq 0$ is a hyperparameter;
    \item $\conv\nor(\mW) = k^2 \max_{i,j=1}^k \norm{\mW_{\cdot \cdot ij}}_{\rms \to \rms}$, the max $\rms\to\rms$ norm over kernel indices.
\end{enumerate}
We would like pixel intensities in the inputs and outputs to be order one and undergo order one change. We formalize this by taking the input and output norms to be the spatial maximum of the RMS norms of all the color channel vectors: $\norm{\vx}_\inputs = \max_{w=1}^{W_\inn}\max_{h=1}^{H_\inn} \norm{\vx_{wh\cdot}}_\rms$ and $\norm{\vy}_\outputs = \max_{w=1}^{W_\out}\max_{h=1}^{H_\out} \norm{\vy_{wh\cdot}}_\rms$. Then $\conv$ is well-normed if the inputs and weights belong to the unit balls $\left\{\vx\in\R^{W_\inn\times H_\inn\times d_\inn}:\norm{\vx}_\inputs \leq 1\right\}$ and $\left\{\mW \in \R^{d_\out\times d_\inn\times k \times k}:\conv\nor(\mW)\leq1\right\}$. Since the duality map for a max of norms decouples into one duality map per sub-norm, the duality map corresponding to $\conv\nor$ is given by:

\begin{enumerate}
\setcounter{enumi}{4}
\setlength\itemsep{0em}
    \item $\conv\dua(\mG)$ does $\mG_{\cdot\cdot ij} \mapsto \frac{1}{k^2} \sqrt{\frac{d_\out}{d_\inn}} \times \mU_{ij} \mV_{ij}^\top$, where $\mG_{\cdot\cdot ij}$ has reduced SVD $\mU_{ij}\mSigma_{ij}\mV_{ij}^\top$.
\end{enumerate}
\end{myexample}

\input{figure/dualize}

\subsection{Duality Maps for Bond Modules}

\citet{modula} define another class of basic modules: \textit{bond modules}. Bonds are handwritten modules without weights. An example of a bond is the $\relu$ nonlinearity. For a bond $\bond$, the weight space is the zero vector space $\weights =\{0\}$ and the modular norm $\bond\nor = 0\mapsto 0$. As such, the corresponding duality map is also $\bond\dua = 0\mapsto 0$. In a software package, one need not write norms or duality maps for bond modules.

\subsection{Duality Maps for Compound Modules}
\label{sec:compound-duality}

First, given two composable modules $\module_1$ and $\module_2$, the duality map for the composite $\module= \module_2\circ\module_1$ is given by:
\begin{equation}\label{eq:composite-duality}
    \module\dua(\vg_1, \vg_2) = \left(\frac{1}{\module_2\lip} * \frac{\module_1\mass}{\module\mass} * \module_1\dua(\vg_1), \frac{\module_2\mass}{\module\mass} * \module_2\dua(\vg_2)\right).
\end{equation}
And second, given two concatenatable modules $\module_1$ and $\module_2$, the duality map for the tuple $\module = (\module_1, \module_2)$ is:
\begin{equation}\label{eq:tuple-duality}
    \module\dua(\vg_1, \vg_2) = \left(\frac{\module_1\mass}{\module\mass} * \module_1\dua(\vg_1), \frac{\module_2\mass}{\module\mass} * \module_2\dua(\vg_2)\right).
\end{equation}
The proofs of \cref{eq:composite-duality,eq:tuple-duality} follow in a straightforward manner from
\cref{def:composition,def:concatenation}.


