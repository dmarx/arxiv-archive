\section{Introduction}

In this paper, we pursue a rigorous and first-principles theoretical framework for designing neural network training algorithms. We hope that building such a framework will facilitate the design of a next generation of fast and scalable optimizers that are automatically tailored to different neural architectures.

While gradient descent is the workhorse of modern machine learning, the most vanilla form of the algorithm does not, in our view, pass a basic \textit{type check}. For a gradient update to type check, we insist that the gradient must be passed through a duality map before being multiplied by a learning rate and applied to the weights:
\begin{align}
    &\mathtt{weight} - \mathtt{LR}\,\mathtt{*}\,\mathtt{weight.grad} &&\texttt{type check: \textcolor{BrickRed}{failed!}}\\
    &\mathtt{weight} - \mathtt{LR}\,\mathtt{*}\,\mathtt{dualize}(\mathtt{weight.grad}) &&\texttt{type check: \textcolor{ForestGreen}{passed!}}
\end{align}
Why? The reason is that the loss function may not be equally smooth in all directions in weight space, and there is no reason for the sizes of different components of the raw gradient vector to respect this heterogeneity. In other words, \textit{the geometry of the loss function may be non-isotropic}. Insisting on a type check should force the user to become cognizant of this issue and to find a suitable duality map. A good duality map should adjust the size and direction of the gradient to respect the smoothness structure of the loss function.

Duality maps on vector spaces are commonplace in physics and applied math. Examples include the \textit{musical isomorphism} in differential geometry \citep{grosse2022metrics}, \textit{raising and lowering indices} in general relativity \citep{carroll2019spacetime} and the \textit{bra-ket notation} in quantum mechanics \citep{sakurai2020modern}. Duality maps are also central to several optimization theories including \textit{mirror descent} \citep{nemirovsky_yudin_1983}, \textit{natural gradient descent} \citep{amari2016information} and \textit{steepest descent on a normed space} \citep{Boyd_Vandenberghe_2004}. Despite the efforts of some prescient papers \citep{spectral-descent-4,flynn2017duality}, the latter kind of duality map involving normed vector spaces is yet to puncture the deep learning mainstream.

We believe that duality is a key theoretical concept that will help in building performant large-scale machine learning systems. To support this belief, we show in this paper that two important and seemingly disparate methods in contemporary optimization research may be seen as approximations to a single duality map. These methods are \textit{maximal update parameterization} \citep{Yang2021TensorPI}, which is aimed at scalable training, and \textit{Shampoo} \citep{Shi2023DistributedShampoo}, which is targeted at fast training. We show in \cref{sec:atomic-duality} that both methods emerge as partial approximations to a single duality map induced by the RMS--RMS operator norm.

The main contribution of this paper is to describe a procedure for constructing duality maps for general neural architectures. The procedure, which we call \textit{modular dualization}, works in three steps:
\begin{itemize}[leftmargin=1.45cm, itemindent=0cm]
    \item[\textbf{Step 1:}] Operator norms are assigned to individual layers based on the input-output semantics of each layer;
    \item[\textbf{Step 2:}] Based on these operator norms, duality maps are constructed for individual layers;
    \item[\textbf{Step 3:}] Given the layerwise duality maps and the structure of the neural architecture, a single duality map is recursively induced on the full weight space of the architecture. 
\end{itemize}

To instantiate this procedure for a rich family of neural architectures---including convolutional networks and transformers---we write down duality maps for $\linear$, $\embed$ and $\conv$ layers. We also provide GPU-friendly algorithms for computing these duality maps. Overall, we hope that modular dualization will help in the principled design of the machine learning systems of the future.
