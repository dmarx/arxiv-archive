\section{Related Work}

This paper constructs a duality map for general neural architectures. Our approach is based on assigning operator norms to individual network layers and using these layerwise norms to recursively induce a duality map on the full neural architecture. The most closely related prior work is a series of papers on \textit{spectral descent} \citep{spectral-descent-2,spectral-descent-4,spectral-descent-1} and a paper on \textit{duality structure gradient descent}  \citep{flynn2017duality}.

Spectral descent has been applied to restricted Boltzmann machines \citep{spectral-descent-2} and discrete graphical models \citep{spectral-descent-1}, but let us focus on the more closely related paper on spectral descent for deep learning \citep{spectral-descent-4}. In that paper, the authors propose assigning the Schatten-$\infty$ norm (a.k.a.\ spectral norm) to individual linear layers. This assignment is based on the observation that neural networks admit natural majorization bounds in the Schatten-$\infty$ norm. The authors call the corresponding duality map for linear layers the ``\#-operator''---a name presumably inspired by the musical isomorphism \citep{grosse2022metrics}. The authors propose a cheap approximation to the \#-operator based on sketching \citep{Martinsson_Tropp_2020}, and they also propose a way to mix RMSprop-style pre-conditioning information \citep{tieleman_rmsprop_2012} into the weight updates. In contrast to our work, the authors only derive duality maps for single linear layers, and these maps are then heuristically extended to all-layer updates. Nonetheless, the authors achieve substantial wall clock speedups using variants of spectral descent to train small networks.

Now, let us turn our attention to duality structure gradient descent \citep{flynn2017duality}, which constructs a duality map on the full weight space of the neural architecture based on identifying a \textit{Finsler structure} \citep{deimling1985nonlinear} inherent to neural networks. Similar to modular dualization, \citet{flynn2017duality}'s duality map works by assigning duality maps to each layer and then inducing a duality map on the full weight space. The substantial difference to our approach is that \citet{flynn2017duality} leverages a weighted sum ($L_1$ combination) of layerwise norms to construct his full duality map. This leads to optimization methods that only update a single layer at each iteration, and the methods need to be heuristically extended to achieve all-layer updates. In contrast, we leverage the modular norm \citep{modula}, which takes a weighted max ($L_\infty$ combination) of layerwise norms. In turn, our duality map leads directly to more conventional 
all-layer optimizers.

Another important difference between our work on modular duality and prior work on duality structure gradient descent is that we fully ``modularize'' our theory---meaning that our construction is explicitly recursive---and as such it is easy to code up into a software package. In this regard, we are inspired by a line of work that attempts to build optimization algorithms that automatically adapt to the structure of general computation graphs. The earliest work we know of in this category is the PhD thesis of \citet{Grant2004} on disciplined convex programming, which aims to infer the convexity properties of general functions by breaking them up into subexpressions and applying composition theorems from convex analysis. More recent progress in this vein includes work on universal majorization-minimization algorithms \citep{Streeter2022AutomaticallyBT,streeter2023universal} and related papers on automatic majorization \citep{Tran2015FastDT,agd-2023}.
