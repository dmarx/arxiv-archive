\begin{abstract}
An old idea in optimization theory says that since the gradient is a \textit{dual vector} it may not be subtracted from the weights without first being mapped to the \textit{primal space} where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call \textit{modular dualization}, forms a unifying theoretical basis for training algorithms that are a) \textit{fast} and b) \textit{scalable}. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing $\embed$, $\linear$ and $\conv$ layers---the latter two methods are based on a rectangular Newton-Schulz iteration \citep{kovarik1970iterative,bjoerck1971}. A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.
\end{abstract}
