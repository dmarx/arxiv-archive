
\section{Fast Duality Maps}
\label{sec:fast}

For modular dualization to be practically feasible, we need ways of computing duality maps quickly. Inspecting the duality maps listed in \cref{tab:dualize}, we see that $\embed\dua$ is easy to implement since it just involves computing vector norms of matrix columns. But $\linear\dua$ and $\conv\dua$ involve the projection:
\begin{equation}\label{eq:polarize}
    \mG = \mU\mSigma \mV^\top \mapsto \mU\mV^\top,
\end{equation}
where $\mU\mSigma \mV^\top$ is the reduced SVD of the matrix $\mG$. Since computing SVDs can be slow \citep{spectral-descent-4,flynn2017duality}, here we discuss three fast approximations to \cref{eq:polarize} via sketching, iterations for inverse matrix roots, and a family of \textit{rectangular Newton-Schulz} iterations. Which method works best in practice may depend on the condition number of the matrix $\mG$ or the available computational resources.

\subsection{Sketching}

Sketching is a randomized method \citep{Martinsson_Tropp_2020} that can be used to build low-rank approximations to the SVD. \citet{spectral-descent-4} already used sketching to provide a fast approximation to their $\#$-operator. More recent papers have experimented with sketching in the context of Shampoo-type algorithms \citep{sketchy}. A potential downside of approximating \cref{eq:polarize} via sketching is that randomized SVD methods usually try to accurately approximate the largest singular values of a matrix \citep[Section 11.2]{Martinsson_Tropp_2020} while the value of \cref{eq:polarize} may lie in its action on the small singular values.

\subsection{Iterations for Inverse Matrix Roots}

Given a full rank matrix $\mG$ with reduced SVD $\mU\mSigma\mV^\top$, we have that:
\begin{equation}
    \mU\mV^\top = (\mG\mG^\top)^{-\sfrac{1}{4}} \,\mG\, (\mG^\top\mG)^{-\sfrac{1}{4}} = (\mG\mG^\top)^{-\sfrac{1}{2}} \,\mG = \mG\, (\mG^\top\mG)^{-\sfrac{1}{2}}.
\end{equation}
This provides a route to approximating \cref{eq:polarize} since one can compute inverse matrix roots such as $(\mG\mG^\top)^{-\sfrac{1}{2}}$ via Newton iteration \citep{lakic}. This is discussed in Chapter 7 of \citet{higham}'s book and also see \citet{Anil2020ScalableSecondOrder}'s paper. Care must be taken with inverses whenever the matrix $\mG$ is ill-conditioned.

\subsection{Rectangular Newton-Schulz Iteration}

We developed a ``rectangular Newton-Schulz iteration'' for computing $\mU\mV^\top$ by adapting Equation 5.22 in \citet{higham}'s book for computing the ``matrix sign function''. We later discovered that this iteration has a long history \citep{kovarik1970iterative, bjoerck1971}. In short, the method works by first normalizing the matrix $\mG$ according to $\mX_0 = \mG / \norm{\mG}_{\ell_2 \to \ell_2}$ (or alternatively $\mX_0 = \mG / \norm{\mG}_F$) and then iterating:
    \begin{equation}\label{eq:newton-schulz}
        \mX_{t+1} = \frac{3}{2} \cdot \mX_t - \frac{1}{2} \cdot \mX_t \mX_t^\top \mX_t,
    \end{equation}
    then as $t\to\infty$, the sequence $\mX_t \to \mU \mV^\top$. To see this, one can plot the univariate cubic function $f(x) \defeq \tfrac{3}{2} \cdot x - \tfrac{1}{2}\cdot x^3$ and see that, for $0 < x < \sqrt{3}$, iterating this cubic will push $x$ closer and closer to $+1$. The final step is to realize that the effect of the iteration in \cref{eq:newton-schulz} is to apply this cubic $f(x)$ to each singular value of $\mX_t$. This shows that the spectral normalization $\mX_0 = \mG / \norm{\mG}_{\ell_2 \to \ell_2}$ is stronger than what is required: we need only ensure that $\mX_0$ has singular values no greater than $\sqrt{3}$ for the iteration to converge.

    The iteration in \cref{eq:newton-schulz} has the advantage over sketching that it always works on all singular values, and since the iteration does not compute inverse matrix roots it is well-behaved even on low-rank matrices.

    Finally, there are in fact a family of degree $2n+1$ polynomial iterations of the form
    \begin{equation}
    \mX_{t+1} = a \cdot \mX_t + b  \cdot \mX_t \mX_t^\top \mX_t + c \cdot (\mX_t \mX_t^\top)^2 \mX_t + ... + z \cdot (\mX_t \mX_t^\top)^n \mX_t
    \end{equation}
    for suitable $a,b,c,...,z$  that could be used instead of \cref{eq:newton-schulz}. One should choose coefficients $a, b,c,...,z$ so that the univariate polynomial $g(x) = a \cdot x + b \cdot x^3 + c \cdot x^5 + ... + z\cdot x^{2n+1}$ is a suitable approximation to $\sign(x)$. One may try to further accelerate the iteration by ``tuning'' the coefficients $a,b,c,...,z$ empirically.

