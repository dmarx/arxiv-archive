\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(2016)]{amari2016information}
Shun-ichi Amari.
\newblock \emph{Information Geometry and Its Applications}.
\newblock Springer, 2016.

\bibitem[Anil et~al.(2020)Anil, Gupta, Koren, Regan, and Singer]{Anil2020ScalableSecondOrder}
Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer.
\newblock Scalable second order optimization for deep learning.
\newblock \emph{arXiv:2002.09018}, 2020.

\bibitem[Bernstein \& Newhouse(2024)Bernstein and Newhouse]{bernstein2024old}
Jeremy Bernstein and Laker Newhouse.
\newblock Old optimizer, new norm: An anthology.
\newblock In \emph{Workshop on Optimization for Machine Learning}, 2024.

\bibitem[Bernstein et~al.(2023)Bernstein, Mingard, Huang, Azizan, and Yue]{agd-2023}
Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue.
\newblock {A}utomatic {G}radient {D}escent: {D}eep {L}earning without {H}yperparameters.
\newblock \emph{arXiv:2304.05187}, 2023.

\bibitem[Bj\"{o}rck \& Bowie(1971)Bj\"{o}rck and Bowie]{bjoerck1971}
\r{A}ke Bj\"{o}rck and C.~Bowie.
\newblock An iterative algorithm for computing the best estimate of an orthogonal matrix.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1971.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{Boyd_Vandenberghe_2004}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Carlson et~al.(2015{\natexlab{a}})Carlson, Cevher, and Carin]{spectral-descent-2}
David Carlson, Volkan Cevher, and Lawrence Carin.
\newblock Stochastic spectral descent for restricted {B}oltzmann machines.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2015{\natexlab{a}}.

\bibitem[Carlson et~al.(2016)Carlson, Hsieh, Collins, Carin, and Cevher]{spectral-descent-1}
David Carlson, Ya-Ping Hsieh, Edo Collins, Lawrence Carin, and Volkan Cevher.
\newblock Stochastic spectral descent for discrete graphical models.
\newblock \emph{Selected Topics in Signal Processing}, 2016.

\bibitem[Carlson et~al.(2015{\natexlab{b}})Carlson, Collins, Hsieh, Carin, and Cevher]{spectral-descent-4}
David~E. Carlson, Edo Collins, Ya-Ping Hsieh, Lawrence Carin, and Volkan Cevher.
\newblock Preconditioned spectral descent for deep learning.
\newblock In \emph{Neural Information Processing Systems}, 2015{\natexlab{b}}.

\bibitem[Carroll(2019)]{carroll2019spacetime}
Sean~M. Carroll.
\newblock \emph{Spacetime and Geometry: An Introduction to General Relativity}.
\newblock Cambridge University Press, 2019.

\bibitem[Dahl et~al.(2023)Dahl, Schneider, Nado, Agarwal, Sastry, Hennig, Medapati, Eschenhagen, Kasimbeg, Suo, Bae, Gilmer, Peirson, Khan, Anil, Rabbat, Krishnan, Snider, Amid, Chen, Maddison, Vasudev, Badura, Garg, and Mattson]{Dahl2023AlgoPerf}
George~E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli~Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel~L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris~J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, and Peter Mattson.
\newblock Benchmarking neural network training algorithms.
\newblock \emph{arXiv:2306.07179}, 2023.

\bibitem[Deimling(1985)]{deimling1985nonlinear}
Klaus Deimling.
\newblock \emph{Nonlinear Functional Analysis}.
\newblock Springer Berlin, Heidelberg, 1985.

\bibitem[Everett et~al.(2024)Everett, Xiao, Wortsman, Alemi, Novak, Liu, Gur, Sohl-Dickstein, Kaelbling, Lee, and Pennington]{everett2024scaling}
Katie~E. Everett, Lechao Xiao, Mitchell Wortsman, Alexander~A. Alemi, Roman Novak, Peter~J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie~Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington.
\newblock Scaling exponents across parameterizations and optimizers.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Feinberg et~al.(2023)Feinberg, Chen, Sun, Anil, and Hazan]{sketchy}
Vladimir Feinberg, Xinyi Chen, Y.~Jennifer Sun, Rohan Anil, and Elad Hazan.
\newblock Sketchy: Memory-efficient adaptive regularization with frequent directions.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Flynn(2017)]{flynn2017duality}
Thomas Flynn.
\newblock The duality structure gradient descent algorithm: Analysis and applications to neural networks.
\newblock \emph{arXiv:1708.00523}, 2017.

\bibitem[Fong \& Spivak(2019)Fong and Spivak]{fong2019invitation}
Brendan Fong and David~I. Spivak.
\newblock \emph{An Invitation to Applied Category Theory: Seven Sketches in Compositionality}.
\newblock Cambridge University Press, 2019.

\bibitem[Grant(2004)]{Grant2004}
Michael~Charles Grant.
\newblock \emph{Disciplined Convex Programming}.
\newblock {PhD} dissertation, Stanford University, 2004.

\bibitem[Grosse(2022)]{grosse2022metrics}
Roger Grosse.
\newblock Metrics.
\newblock Lecture 3 of CSC2541: Neural Net Training Dynamics, 2022.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{Gupta2018ShampooPS}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[{Haskell Wiki Contributors}(2007)]{haskellwiki_combinator}
{Haskell Wiki Contributors}.
\newblock Combinator pattern.
\newblock Haskell Wiki, 2007.
\newblock URL \url{https://wiki.haskell.org/Combinator_pattern}.

\bibitem[Higham(2008)]{higham}
Nicholas~J. Higham.
\newblock \emph{Functions of Matrices}.
\newblock Society for Industrial and Applied Mathematics, 2008.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{NTKjacot}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: {C}onvergence and generalization in neural networks.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Jesus et~al.(2021)Jesus, Antunes, da~Costa, Dorogovtsev, Mendes, and Aguiar]{math9182246}
Ricardo~J. Jesus, Mário~L. Antunes, Rui~A. da~Costa, Sergey~N. Dorogovtsev, José F.~F. Mendes, and Rui~L. Aguiar.
\newblock Effect of initial configuration of weights on training and function of artificial neural networks.
\newblock \emph{Mathematics}, 2021.

\bibitem[Jordan(2024)]{jordan2024cifar10}
Keller Jordan.
\newblock New training speed record for @karpathy’s 124{M}-parameter {NanoGPT} setup: 3.28 {F}ineweb validation loss in 3.7{B} training tokens.
\newblock \url{https://x.com/kellerjordan0/status/1842300916864844014}, 2024.

\bibitem[Kovarik(1970)]{kovarik1970iterative}
Zdislav Kovarik.
\newblock Some iterative methods for improving orthonormality.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1970.

\bibitem[Lakić(1998)]{lakic}
Slobodan Lakić.
\newblock On the computation of the matrix k-th root.
\newblock \emph{Journal of Applied Mathematics and Mechanics}, 1998.

\bibitem[Large et~al.(2024)Large, Liu, Huh, Bahng, Isola, and Bernstein]{modula}
Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, and Jeremy Bernstein.
\newblock Scalable optimization in the modular norm.
\newblock In \emph{Neural Information Processing Systems}, 2024.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein, and Pennington]{NEURIPS2019_0d1a9651}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Martinsson \& Tropp(2020)Martinsson and Tropp]{Martinsson_Tropp_2020}
Per-Gunnar Martinsson and Joel~A. Tropp.
\newblock Randomized numerical linear algebra: Foundations and algorithms.
\newblock \emph{Acta Numerica}, 2020.

\bibitem[Nemirovsky \& Yudin(1983)Nemirovsky and Yudin]{nemirovsky_yudin_1983}
Arkady~S. Nemirovsky and David~B. Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley, 1983.

\bibitem[OPT(2024)]{opt2024}
OPT.
\newblock {Optimization for Machine Learning}, 2024.
\newblock URL \url{https://opt-ml.org/}.

\bibitem[Sakurai \& Napolitano(2020)Sakurai and Napolitano]{sakurai2020modern}
J.~J. Sakurai and Jim Napolitano.
\newblock \emph{Modern Quantum Mechanics}.
\newblock Cambridge University Press, 2020.

\bibitem[Shi et~al.(2023)Shi, Lee, Iwasaki, Gallego-Posada, Li, Rangadurai, Mudigere, and Rabbat]{Shi2023DistributedShampoo}
Hao-Jun~Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li, Kaushik Rangadurai, Dheevatsa Mudigere, and Michael Rabbat.
\newblock A distributed data-parallel {PyTorch} implementation of the distributed {S}hampoo optimizer for training neural networks at-scale.
\newblock \emph{arXiv:2309.06497}, 2023.

\bibitem[Streeter(2023)]{streeter2023universal}
Matthew Streeter.
\newblock Universal majorization-minimization algorithms.
\newblock \emph{arXiv:2308.00190}, 2023.

\bibitem[Streeter \& Dillon(2022)Streeter and Dillon]{Streeter2022AutomaticallyBT}
Matthew~J. Streeter and Joshua~V. Dillon.
\newblock Automatically bounding the {T}aylor remainder series: Tighter bounds and new applications.
\newblock \emph{arXiv:2212.11429}, 2022.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman_rmsprop_2012}
Tijmen Tieleman and Geoffrey Hinton.
\newblock {RMSprop}.
\newblock \emph{Coursera: Neural Networks for Machine Learning}, Lecture 6.5, 2012.

\bibitem[Tran et~al.(2015)Tran, Ono, and Vincent]{Tran2015FastDT}
Dung~T. Tran, Nobutaka Ono, and Emmanuel Vincent.
\newblock Fast {DNN} training based on auxiliary function technique.
\newblock \emph{International Conference on Acoustics, Speech and Signal Processing}, 2015.

\bibitem[Yang \& Hu(2021)Yang and Hu]{Yang2021TensorPI}
Greg Yang and Edward~J. Hu.
\newblock Tensor programs {IV}: Feature learning in infinite-width neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Yang et~al.(2023)Yang, Simon, and Bernstein]{my-spectral}
Greg Yang, James~B. Simon, and Jeremy Bernstein.
\newblock A spectral condition for feature learning.
\newblock \emph{arXiv:2310.17813}, 2023.

\end{thebibliography}
