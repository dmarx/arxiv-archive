\newcommand{\stackargs}[2]{\renewcommand{\arraystretch}{1.1}\begin{tabular}{@{}c@{}}#1 \\ #2\end{tabular}\renewcommand{\arraystretch}{1.4}}

\renewcommand{\stackargs}[2]{#1 #2}

\renewcommand{\arraystretch}{1.4}
\begin{table}
    \centering
    \begin{tabular}{@{}cccc@{}}
        \toprule
         \textsf{\textbf{Module}} & \textsf{\textbf{Weight Space}} $\bm\weights$ & $\bm{\mathsf{Module}\nor}$ & $\bm{\mathsf{Module}\dua}$ \\
        \midrule
        $\linear$ & $\R^{d_\out \times d_\inn}$ & $\mW \mapsto\norm{\mW}_{\rms \to \rms}$ & $\mG \mapsto \sqrt{\frac{d_\out}{d_\inn}} \times \mU \mV^\top$ \\
        
        $\embed$ & $\R^{d_\out \times d_\inn}$ & $\mW \mapsto\norm{\mW}_{\ell_1 \to \rms}$ & $\mathrm{col}_j(\mG) \mapsto \frac{\mathrm{col}_j(\mG)}{ \norm{\mathrm{col}_j(\mG)}_\rms}$ \\
        
        $\conv$ & $\R^{d_\out \times d_\inn \times k \times k}$ & $\mW \mapsto k^2 \max_{i,j=1}^k \norm{\mW_{\cdot \cdot ij}}_{\rms \to \rms}$ & $\mG_{\cdot\cdot ij} \mapsto \frac{1}{k^2} \sqrt{\frac{d_\out}{d_\inn}} \times \mU_{ij} \mV_{ij}^\top$ \\
        \bottomrule
    \end{tabular}
    \caption{\captiontitle{Duality maps for three atomic modules: $\bm\linear$, $\bm\embed$, and $\bm\conv$.} These atomic modules are sufficient to build convolutional neural networks and transformers. In $\linear\dua$, we let $\mU \mSigma \mV^\top$ denote the reduced SVD of the gradient matrix $\mG$. In $\conv\dua$, we let $\smash{\mU_{ij} \mSigma_{ij} \mV_{ij}^\top}$ denote the reduced SVD of the slice of the gradient tensor $\mG_{\cdot\cdot ij}$ at kernel indices $i$ and $j$. \cref{sec:fast} provides GPU-friendly algorithms for computing these duality maps based on a family of Newton-Schulz iterations.}
    \label{tab:dualize}
\end{table}
\renewcommand{\arraystretch}{1}
