---
abstract: |
  Domain reweighting is an emerging research area aimed at adjusting the relative weights of different data sources to improve the effectiveness and efficiency of language model pre-training. This paper demonstrates that the optimal composition of training data from different domains is scale-dependent, challenging the existing practice of determining optimal mixtures through small-scale experiments and directly applying them at larger scales. We derive an analytical model for the dependence of optimal weights on data scale and introduce <span class="smallcaps">AutoScale</span>, a novel, practical approach for optimizing data compositions at potentially large training data scales. <span class="smallcaps">AutoScale</span> first uses a principled optimization framework to find optimal compositions at smaller, feasible scales, then predicts optimal compositions at larger scales using our derived model. Our evaluation on GPT-2 Large and BERT pre-training demonstrates <span class="smallcaps">AutoScale</span>’s effectiveness in improving training convergence and downstream performance. Particularly, for GPT-2 Large on RedPajama, <span class="smallcaps">AutoScale</span> decreases validation perplexity 28% faster than baselines, with up to 38% speed-up over unweighted training, achieving the best performance across downstream tasks. This work provides insights into the varying benefits of data sources across training scales for language models, contributing to the burgeoning research on scale-dependent data curation. Code is open-sourced[^1].
author:
- |
  Feiyang Kang[^2] $\,^{\dagger}$  
  Virginia Tech  
  `fyk@vt.edu`  
  Yifan Sun$^{*}$  
  UIUC  
  `yifan50@illinois.edu`  
  Bingbing Wen  
  University of Washington  
  `bingbw@uw.edu`  
  Si Chen  
  Virginia Tech  
  `chensi@vt.edu`  
  Dawn Song  
  UC Berkeley  
  `dawnsong@gmail.com`  
  Rafid Mahmood  
  University of Ottawa & NVIDIA  
  `mahmood@telfer.uottawa.ca` Ruoxi Jia$^{\dagger}$  
  Virginia Tech  
  `ruoxijia@vt.edu`  
bibliography:
- iclr2025_conference.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "<span class=\"smallcaps\">AutoScale</span>: Automatic Prediction of Compute-optimal Data Composition for Training LLMs"
---





# Introduction

Large language models (LLMs) are pre-trained on vast datasets sourced from diverse domains. However, the immense computational demands of this process, coupled with limited resources, create a pressing need to enhance the effectiveness and efficiency of pre-training. A promising approach to address this challenge is through *domain reweighting*—adjusting the relative proportions of data from different sources .

Showing encouraging potentials, though, current implementation techniques face significant limitations. A prevailing technique is to first optimize data composition for a smaller proxy model and at a smaller data scale . Yet, this optimization often employs alternative objectives that may not align with the primary goal of minimizing evaluation loss. Moreover, the optimized weights are directly applied to training the target model on much larger data scales, implicitly assuming that the "optimal data composition" remains constant across data scales. This assumption of scale-invariance, however, may not hold in practice, potentially leading to suboptimal performance when scaling up. While research has scale-dependent data selection at the individual data point level for vision models , it remains unclear whether this scale dependence applies to domain-level optimization, or how such scaling behavior might manifest in language models.

In parallel, an increasingly popular practice is to directly adopt domain weights that were designed for training previous models , such as those used for LLaMA . However, these weights are optimized for specific applications that may differ from the desired use case of the target model. Given the limitations of these approaches, many in the industry still rely on heuristics for mixing domain data . These limitations highlight the ongoing need for more adaptive and scale-aware methods to determine effective domain weights across various model sizes and target scenarios.

This work explicitly investigates and confirms the scale-dependence of optimal domain mixing, characterizing its scaling behavior. Based on these findings, we develop a practical methodology: optimizing domain weights at smaller, affordable scales and leveraging derived scaling laws to predict optimal mixing at much larger target scales. We lay out an overview of this work and main results in Fig. . Our contributions are summarized as follows.

**Principled algorithmic framework for optimal domain mixing.** Investigating the scaling law of domain mixing extends beyond mere empirical study. It requires a mathematical definition of optimal domain mixing and a tractable algorithm to solve for optimal weights. Our first contribution is formulating the optimal mixing problem as a bi-level optimization. However, existing general bi-level optimization techniques  are intractable in this context due to their reliance on second-order information. We propose a novel approach tailored to our problem context that leverages scaling laws to estimate the dependence of the learned model’s loss on the weights, effectively reducing the bi-level problem to a single level. Our algorithm requires retraining models only linearly in the number of data domains, making it feasible for exploratory studies.

**Uncovering and quantifying the scale-dependence of optimal domain composition.**  
Leveraging the algorithm developed in , we conduct empirical studies to optimize domain weights at different training data scales. Our results demonstrate that the optimal data composition varies with the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using small-scale experiments will not yield optimal data mixtures for larger scales. We further derive an analytical framework for modeling the functional relationship between optimal data composition and training data scales.

**Practical algorithm for optimal domain mixing.** While the algorithm in  has made optimal domain mixing feasible for exploratory studies, its retraining requirements limit its practicality to smaller scales. To enable data composition optimization at large scales, we propose <span class="smallcaps">AutoScale</span>. This method works by finding optimal data compositions at smaller, computationally feasible scales, fitting a predictor using our analytical model for the scale-dependency of optimal composition mentioned in , and finally using this predictor to determine optimal data composition at larger scales. Since one only needs to train models on small data scales where re-training is affordable, $\textsc{AutoScale}$ does not require using proxy models with a smaller parameter size, avoiding transferability issues between domain weights optimized with different model sizes.

**Robust performance gains across models and datasets.** Our evaluation of <span class="smallcaps">AutoScale</span> on both decoder-only and encoder-only models demonstrates its consistent ability to achieve significant computational savings. For instance, in pre-training `GPT-2 Large` on the `RedPajama` dataset, <span class="smallcaps">AutoScale</span> decreases validation perplexity 28% faster than any baseline, with up to 38% speed-up compared to training without reweighting. It also achieves the best overall performance across downstream tasks. Additionally, we present intriguing findings regarding the varying benefits of traditionally perceived high-quality and low-quality data sources across different training scales. Specifically, we observe that data sources with standardized formats, such as `Wikipedia` and scientific papers—often regarded as high-quality—are most beneficial at smaller scales but exhibit sharp diminishing returns as the training data scales up. Conversely, with increased compute, data sources containing diverse examples, such as `CommonCrawl`, demonstrate continued reductions in training loss even at considerably large training data scales.

<figure id="fig:example1">
<div class="center">
<p><span class="image placeholder" data-original-image-src="figs/agg4.png" data-original-image-title="" width=".85\textwidth">image</span> </p>
</div>
<figcaption><span>Overview and main results. I. Optimizing domain weights with the proposed <u>D</u>irect <u>D</u>ata <u>O</u>ptimization (<span class="smallcaps">DDO</span>) algorithm for pre-training 774M Decoder-only LMs (<code>GPT-2 Large</code>). Optimal domain weights depend on the scale of training data. A consistent shift can be observed (<em>data sources with standardized formats, such as <code>Wikipedia</code> and scientific papers—often regarded as high-quality—are most beneficial at smaller scales but exhibit sharp diminishing returns as the training data scales up. Conversely, with increased compute, data sources containing diverse examples, such as <code>CommonCrawl</code>, demonstrate continued reductions in training loss even at considerably large training data scales.</em>). Using domain weights optimized for a different scale yields sub-optimal results, failing to fully realize the benefits of domain reweighting. II. Optimal domain data quantity (y-axis) for different training data scales (x-axis) shows high linearity (<span class="math inline">\(R^2=0.998\)</span>) on log-log plot, suggesting the shifting pattern can be well predicted by exponential-style functions. We fit <span class="smallcaps">AutoScale</span> to predict optimal domain weights for larger training scales. As we scale up, data sources with diverse samples (e.g., <code>C4</code>) are upweighted relative to domains with standard format (e.g., <code>Wikipedia</code>). III. Training 774M Decoder-only LMs for 10B tokens (96k steps). <span class="smallcaps">AutoScale</span>-predicted domain weights decrease validation PPL at least <span class="math inline">\(28\%\)</span> faster than any baseline with up to <span class="math inline">\(38\%\)</span> speed up, achieving best overall task performance. </span> </figcaption>
</figure>

# Related Work

**Domain Reweighting.** An emerging line of research strives to optimize the composition of training data for LLMs pre-training with *domain reweighting* , i.e., adjusting the relative proportion of data from different data sources to "best" (in terms of training efficiency, final model performance, etc.) train the model. <span class="smallcaps">DoReMi</span> first trains a small reference model, and then trains a second proxy model with GroupDRO to minimize the excessive domain loss relative to the reference model, where the domain weights of the proxy model will be the output. <span class="smallcaps">DOGE</span> trains a proxy model while tracking the first-order gradient of the model on evaluation domains (i.e., data influence) and optimizes domain weights based on the gradients, relying on infinitesimal approximations which may or may not be accurate for models trained with a practical learning rate. <span class="smallcaps">Data Mixing Laws</span> trains a number of proxy models to run a coarse grid search on the space of data mixtures and interpolate their performance with exponential functions to find the minimum. Similarly, `RegMix` trains a regression model to represent the relationship between training data mixtures and resulting model performance and optimize data composition based on it.

These methods often rely on ad-hoc hyperparameter tuning via trial and error, achieving varying results. *Further, the optimized weights are directly applied to training the target model on magnitudes of larger data scales.* This implicitly poses a strong assumption that the "optimal data composition" is invariant of model sizes or data scales. Yet, optimal data composition is likely to shift with data size. *Optimal curation at a smaller scale may not remain optimal at the target scale* . provides a recent survey for this fast-evolving field. We refer to App.  for broader discussions.

**Scaling Laws.** Extensive research shows that *Neural Scaling Laws*, predicting how the model performance changes with the scale of training data, model parameters, and computation budget , to be accurate in various tasks from vision and text processing to LLM pre-training and evaluations . proposes compute-optimal scaling for LLM pretraining data scales together with the model’s parameter sizes. Yet, recent progress shows no sign of saturation in pre-training even for models pre-trained on a considerably larger data scale than recommended by . shows that data from different sources generally scale at different rates. Seminal work sheds light on the possibility of attaining beyond-neural scaling law performance if one could find the best training dataset for each training data scale. This work is connected to the research on scaling laws in two ways. First, we leverage scaling laws to model the functional relationship between the quantity of data from each domain and trained model performance, allowing optimizing the training data composition in a reasonable time with high precision; further, *this work contribute to a novel dimension of scaling laws–scaling optimal data compositions with the training data scale, providing original insights, clear empirical evidence, and theoretical frameworks which enable further analysis.*

# Optimal Data Composition is Scale-Dependent and Predictable

For "compute-optimal" domain weights, the goal is to find an optimal training data composition such that, for a given compute budget (i.e., training data size), the empirical validation loss, measured in perplexity (PPL), is minimized . Formulating this as a bi-level optimization problem, in this section, we first introduce an original solution approach via scaling law approximations, which allows solving it efficiently and effectively. Then, with this solution approach, we solve for the optimal domain weights under different training data scales. Our results demonstrate that the optimal data composition for a fixed compute budget depends on the scale of the training data. Via the lens of scaling laws, this work pioneers in deriving an analytical framework for modeling the functional relationship between optimal data composition and training data scales.

## Compute-Optimal Training Data Compositions

Consider training an LLM on a data composition $S$ from $m$ domains, $D_1, D_2, \cdots, D_m$. Let $S=\{S_1, S_2,\cdots, S_m\}$ denote the training dataset where $S_i$ is the subset of training data from each domain. The domain weights $\mathbf{w}=[w_1,w_2, \cdots, w_m]^T$ are defined as the proportions of data for each domain. Namely, letting $N=|S|$ denote the amount of total tokens of training data, domain weights are given as $w_i = N_i/N$, where $N_i$ denotes the amount of tokens for training subset $S_i$.

Let $\boldsymbol{\theta}^*(S)$ denote the parameters of a learning algorithm (i.e., the model) trained on data $S$ with empirical risk minimization (ERM), given as $\boldsymbol{\theta}^*(S):= {\arg\min}_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta},S)$ where $\mathcal{L}(\boldsymbol{\theta},S)$ denotes the loss of model parameterized by $\boldsymbol{\theta}$ evaluated on data $S$, which is the training loss. Since training data $S$ can be equivalently defined by its data quantity and domain weights $(N,\mathbf{w})$, we define a slight change of notation $\boldsymbol{\theta}^*(N,\mathbf{w}) := \boldsymbol{\theta}^*(S)$ and will use $S$ and $(N,\mathbf{w})$ interchangeably. We would like to maximize the amount of information gain and achieve maximum loss reduction during training, given as $\min_{\mathbf{w}\in \mathbb{W}^m}\mathcal{L}(\boldsymbol{\theta}^*(N,\mathbf{w}), D^v) = \min_{\mathbf{w}\in \mathbb{W}^m}\sum_{i=1}^m \mathcal{L} (\boldsymbol{\theta}^*(N,\mathbf{w}), D_i^v)$, where $D^v$ and $D^v_i$ denote total validation data and validation data of individual domain $i$, respectively; the space of weights $\mathbb{W}^m$ is the hyperplane of the probability simplex $\mathbb{W}^m=\{\mathbf{w}|w_1+w_2+\cdots+w_m=1\}\cap\{\mathbf{w}|0\leq w_i \leq 1, \forall i \in \{1,2,\cdots,m\}\}$. Define minor simplifications of notations for the validation losses $\mathcal{L}^v(\theta, D^v) := \mathcal{L}(\theta, D^v)$ and $\mathcal{L}^v_i(\theta, D^v) := \mathcal{L}(\theta, D^v_i)$. Then, the optimal domain weights, $\mathbf{w^*}$, are given as the minimizer of the objective, $$\begin{aligned}
\label{eqn:upper}
   \mathbf{w^*}=\arg\min_{\mathbf{w}\in \mathbb{W}^m}\sum_{i=1}^m \mathcal{L}_i^v (\boldsymbol{\theta}^*(N,\mathbf{w})) \quad  \text{s.t.}\,\,\,\,\boldsymbol{\theta}^*(N,\mathbf{w}) = \arg\min_{\boldsymbol{\theta}}\mathcal{L}({\boldsymbol{\theta}},(N, \mathbf{w}))\vspace{-1.3em}
\end{aligned}$$ where perplexity is adopted as the loss metric. This formulation is a bi-level optimization problem, where the outer problem seeks the optimal domain weights, while the inner problem is training the model with ERM on the data defined by certain weights. A general approach is to solve it with gradient descent, $\mathbf{w}^{t+1} = \mathbf{w}^{t} - \eta\cdot \frac{\partial \mathcal{L}_{v}(\boldsymbol{\theta}^*(N,\mathbf{w}^t))}{\partial \mathbf{w}}$. Since there is no tractable form of analytical expression for $\boldsymbol{\theta}^*$, this gradient needs to be estimated with empirical methods (e.g., approximated by finite difference), requiring repetitive re-training of the model at each update .

### Solution via Scaling-law-inspired Approximations

Directly optimizing training data composition by solving bi-level optimization problems involves repetitive model retraining, which can be prohibitively expensive even at small scales. Current work mostly employs heuristic methods to conduct this optimization on smaller models trained with fewer data, achieving varying results in different cases. To crystalize the relationship between optimal data compositions and training data scales and obtain a clear image of the complete landscape, we propose an *original* approach to this problem. We propose to first fit a scaling function to the outer loss (validation loss) $\mathcal{L}^v$ as a function of domain weights $\mathbf{w}$, effectively reducing the bi-level problem to a single level, which can be solved efficiently via regular gradient descent, allowing finding the global optimum efficiently and accurately.

To begin with, neural scaling laws suggest the relationship between a model’s evaluation loss and the size of its training data can be well-represented by power law functions $\mathcal{L}_{v}(\boldsymbol{\theta}^*(N,\mathbf{w})) =  N^{-\gamma}+\ell_0$ where constants $\ell_0$ denotes some irreducible loss and $\gamma\geq 0$ is some scaling coefficient. Drawing inspirations from , which formulates the scaling laws for transfer learning, we propose the following approximation to model the scaling relationship between model loss and training data quantity from different sources/domains.

Consider a model trained on data with size $N$ and domain weights $\mathbf{w}$. Define constant $N_0^i$ which estimates the evaluation loss when the amount of training data from domain $i$ is zero (i.e., $N_i'=0$), which effectively measures the effect of data from all other domains. From this regard, $N_0^i$ can be interpreted as the *equivalent data size* for training data from domains other than $i$. Notably, this formulation aligns with empirical findings in the prior literature . Then, for training data defined by $(N,\mathbf{w})$ where the amount of training data from *domain* $D_i$ is $N_i=N\cdot w_i$, evaluation loss can be expressed as a function of $N_i$: $\mathcal{L}_{v}(\boldsymbol{\theta}^*(N,\mathbf{w})) = (N_0^i+N_i)^{-\gamma_i}+\ell_i$ where $\gamma_i,\ell_i$ are constants associated with domain $i$. If the amount of training data from *one domain* $D_i$ is changed from $N_i$ to $N_i'$ with the amount of training from other domains unchanged, we approximate the new model’s evaluation loss, $\mathcal{L}_i'$, after re-training with a power law function of $N_i'$: $$\begin{aligned}
\vspace{-1.5em}
\label{eqn:scaling_law}
    \mathcal{L}_{v}(\boldsymbol{\theta}^*(N',\mathbf{w'})) = (N_0^i+N_i')^{-\gamma_i}+\ell_i :=\mathcal{L}_i'\vspace{-1em}
\end{aligned}$$ where $N'=N+(N_i'-N_i)$ denotes the updated amount of training data, and $w_i'=N_i'/N'$ denotes the updated domain weights.

We propose the following procedure to fit the parameters in Eq. (). We re-train two models with different data quantities for domain $i$, $N_i^{+}$ and $N_i^{-}$ where $N_i^{-} < N_i<N_i^{+}$, and compute their evaluation loss, $\mathcal{L}_i^+$ and $\mathcal{L}_i^-$, respectively[^3]. Then, together with evaluation loss $\mathcal{L}_v^0=\mathcal{L}_v(\boldsymbol{\theta}^*(N,\mathbf{w}))$ for the original model trained with $N_i$, the parameters $\gamma_i$, $\ell_i$ and $N_0^i$ can be estimated via ordinary least square (OLS) fitting, $$N_0^i,\gamma_i,\ell_i = \arg\min_{N_0^i,\gamma_i,\ell_i} [\mathcal{L}_v^0-(N_0^i + N_i)^{-\gamma_i}-\ell_i]^2+[\mathcal{L}_i^+-(N_0^i + N_i^+)^{-\gamma_i}-\ell_i]^2+[\mathcal{L}_i^--(N_0^i + N_i^-)^{-\gamma_i}-\ell_i]^2$$ Compared to the original model, the difference in evaluation loss *due to the change of data for domain $D_i$* is given as $\mathcal{L}_{v}(\boldsymbol{\theta}^*(N',\mathbf{w'}))-\mathcal{L}_{v}(\boldsymbol{\theta}^*(N,\mathbf{w})) = (N_0^i+N_i')^{-\gamma_i} - (N_0^i+N_i)^{-\gamma_i}.$ Repeating this process and fitting the scaling functions *for each domain*, finally, we express the evaluation loss as a function of the amount of data from each domain as their summation: $\mathcal{L}_{v}(\boldsymbol{\theta}^*(N',\mathbf{w'}))-\mathcal{L}_{v}(\boldsymbol{\theta}^*(N,\mathbf{w})) = \sum_{i=1}^m \left[ (N_0^i+N_i')^{-\gamma_i}- (N_0^i+N_i)^{-\gamma_i} \right]$ where $N'=N+\sum_i(N_i'-N_i)$ and $w_i'=N_i'/N'$. *Empirically, evaluation loss is shown to be well represented by such function form as depicted in Fig. ()*, which shows fitting validation loss with the proposed power-law functions for training 774M Decoder-only LMs (`GPT-2 Large`), directly approximating how loss changes with each domain’s data quantity. Results with Encoder-only LMs (`BERT`) demonstrates the same trend (Fig. (b)). This representation lends us an analytical form for the desired objective.

To derive the final objective, we add the constraint for the total amount of training data to be the same as before, i.e., $\sum_i N_i' = N' = N$, which explicates our interest in reweighting data from each domain without changing the training data size. Then, by definition, domain data quantity $N_i'=w_i'\cdot N'=w_i'\cdot N$. Note that $(N_0^i+N_i)^{-\gamma_i}$ is independent of $\mathbf{w}'$, making it orthogonal to the optimization problem. Finally, our problem becomes $$\mathbf{w^*} =\arg\min_{\mathbf{w'}\in \mathbb{W}^m}\sum_{i=1}^m \left[ (N_0^i+N_i')^{-\gamma_i}- (N_0^i+N_i)^{-\gamma_i} \right] =\arg\min_{\mathbf{w'}\in \mathbb{W}^m}\sum_{i=1}^m (N_0^i+w_i'\cdot N)^{-\gamma_i}. \vspace{-0.5em}$$ Since the objective is defined as the summation of convex functions, we end up with a convex optimization problem. With the constraint on the probability simplex and the objective being easily differentiable, the problem can be solved extremely efficiently using *projected gradient descent* . We term this solution approach as <span class="smallcaps">DDO</span> Algorithm (<u>D</u>irect <u>D</u>ata <u>O</u>ptimization). We provide its pseudocode below and an operational pipeline in App. .

<div class="center">

</div>

## Optimal Data Compositions are Scale-Dependent

With the <span class="smallcaps">DDO</span> algorithm, for a fixed model training pipeline and data sources, we conducted empirical studies to optimize domain weights at different training data scales. Our results demonstrate that the optimal data composition for a fixed compute budget depends on the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using small-scale experiments will not yield optimal data mixtures for larger models.

Fig. (a)(b) shows the results on optimizing domain weights with <span class="smallcaps">DDO</span> algorithm for pre-training 774M Decoder-only LMs (`GPT-2 Large`). Optimal domain weights depend on the scale of training data. A consistent shift can be observed. Using domain weights optimized for a different scale yields sub-optimal results, failing to realize the benefits of domain reweighting fully. *These results clearly show that the hypothesis, "optimal data composition is invariant of data scales", implicitly assumed by many existing works, is largely untrue.* On the contrary, a consistent pattern can be observed in how optimal data compositions shift with the scale of training data. For example, data sources with standard format such as `Wikipedia` and scientific papers, regarded as high quality, are most beneficial at smaller scales but observe sharp diminishing returns as training data scales up. With more compute, data sources with diverse examples, such as `CommonCrawl`, demonstrate continued reductions in training loss even for larger training data scales.

Foreshadowed by , beyond-neural scaling law performance might be attained if one could find the best training dataset for each training data scale. This treasure chest remains unexplored in the context of training LLMs. *This consistent pattern of shifts suggests predictability in the relationship between optimal composition and training data scales*, holding the promise to unlock substantial improvements in training efficiency and model performance.

## Deriving Scaling Laws for Predicting Optimal Data Composition

Following the above findings, this work pioneers in deriving an analytical framework for modeling the functional relationship between optimal data composition and training data scales. Via the lens of scaling laws, the analysis lays out theoretical foundations that could be of independent interest.

Recall that neural scaling laws give the relationship between evaluation loss and training data quantity as $\mathcal{L} =  N^{-\gamma} + \ell_0$ where $\mathcal{L}$ is the evaluation loss (e.g., perplexity), $\ell_0$ denotes some irreducible loss, and $\gamma\geq 0$ are some constant. $(\ell_0, \gamma)$ can be fitted empirically. Without loss of generality, consider a standard case where the evaluation metric is aggregated loss over multiple independent tasks where each training sample will only contribute to a single task and the loss of each task only scales with the amount of training data contributing to this task as a power law function. Then, for a total of $m$ tasks, the aggregated evaluation loss scales as the following $\mathcal{L} =\ell_0+\sum_{i=1}^m \beta_i\cdot N_i^{-\gamma_i},$ where $\ell_0$ denotes some irreducible loss, $N_i$ denotes the quantity of data contributing to task $i$ and constants $\beta_i\geq 0$ and $\gamma_i\geq 0$ are coefficients associated with task $i$. Define diagonal matrix $\mathbf{N}=diag\{N_1,N_2,\cdots N_m\}$. For a training data scale $N=\sum_i N_i$, define compute-optimal data composition $\mathbf{N}=diag\{N_1^*, N_2^*,\cdots N_m^*\}$ as the minimizer of $\mathcal{L}$, given as $\mathbf{N^*} = {\arg\min}_{\sum_i N_i=N} \ell_0+\sum_{i=1}^m \beta_i\cdot N_i^{-\gamma_i}$. We propose the following theorem, which states the optimal data composition scales in exponential-style functions with the amount of training data and can be directly predictable from that of smaller scales.

<div class="theorem">

**Theorem 1**.  * Consider the following optimization problem $\min_{\mathbf{N}} \left\{ \sum_{i=1}^m \beta_i N_i^{-\gamma_i} \Bigg| \sum_{i=1}^m N_i = N \right\}.$ For any two compute budgets $N^{(1)} \neq N^{(2)}$, let $\mathbf{N}^{(1)*}$ and $\mathbf{N}^{(2)*}$ be their respective minimizers. Then, there always exists some third data composition $\mathbf{N}^{(3)*} = \mathbf{N}^{(2)*}(\mathbf{N}^{(1)*}) ^{-1} \mathbf{N}^{(2)*}$ such that $\mathbf{N}^{(3)*}$ is the minimizer for data budget $N^{(3)}=\sum_{i=1}^m N^{(3)}_i$, given as $\mathbf{N}^{(3)*} = \arg\min_{\mathbf{N}} \left\{ \sum_{i=1}^m \beta_i N_i^{-\gamma_i} \Bigg| \sum_{i=1}^m N_i = N^{(3)} \right\}.$*

</div>

See App.  for the formal theorem statement and a complete proof. Examples for illustration are also provided in . We built our theory from a standardized example which assumes the evaluation metric is composed of independent tasks with separate scaling laws. In App. , we further extend this theory to a general case where the same conclusion can be reached without the independence assumption, where we consider the evaluation to be composed of a number of *independent* sub-tasks (e.g., "latent skills" ) which are hidden variables. Finally, we note that empirical results are shown to be highly consistent with the derivations above: In Fig. (c), optimal domain data quantity (y-axis) for different training data scales (x-axis) shows high linearity ($R^2$ = 0.998) on log-log plot, *suggesting the shifting pattern can be well-described by the exponential-style function forms described above.*

# \[1.0\]

In this section, we introduce a practical paradigm for finding optimal data compositions developed based on the theoretical analyses and empirical insights introduced above. Having shown the consistent pattern of shifts in optimal data composition with the scale of training data and unveiled its predictability from scaling law analysis, moving forward, this paper presents a novel tool–$\textsc{AutoScale}$, which automatically predicts optimal training data compositions at larger scales based on compositions optimized at smaller scales.

Theoretical analysis above shows that the optimal quantity for each domain scales in *exponential-style functions* with training data size. We leverage this result to enable the automatic prediction of optimal training data compositions at larger scales from optimal compositions at small scales. First, for two smaller training data scales $N^{(1)}$ and $N^{(2)}$ where $N^{(1)} \neq N^{(2)}$, find their optimal training data compositions $\mathbf{N^{(1)*}}$ and $\mathbf{N^{(2)}*}$ where $\sum_i N_i^{(1)*}=N^{(1)}$ and $\sum_i N_i^{(2)*}=N^{(2)}$ using <span class="smallcaps">DDO</span> algorithm provided in Sec. .

Since $N^{(1)}$ and $N^{(2)}$ are small data scales where re-training these models is affordable, $\textsc{AutoScale}$ *does not require using proxy models with a smaller parameter size, avoiding the transferability issue between domain weights optimized on different models*. Then, $\mathbf{N^{(1)*}}$ and $\mathbf{N^{(2)*}}$ yield the optimal training data composition at the next scale as $\mathbf{N^{(3)*}} = \mathbf{N^{(2)*}}(\mathbf{N^{(1)*}})^{-1}\mathbf{N^{(2)*}}$, where $N_i^{(3)*}=(N_i^{(2)*})^2/N_i^{(1)*}$ is the optimal amount of training data for each domain. This gives that for data scale $N^{(3)}=\sum_i N_i^{(3)*}$, optimal domain weights are given as $w_i^{(3)*}=N_i^{(3)*}/N^{(3)}$. Then, $\mathbf{N^{(3)*}}$ can be combined with either $\mathbf{N^{(1)*}}$ or $\mathbf{N^{(2)*}}$ to make the next prediction. Repeat this process until the target training data scale is reached. The procedure is described in the pseudocode above with operational pipelines provided in App. .

# Empirical Results

Two sets of empirical studies are conducted: Causal Language Modeling (CLM) in Sec. , and Masked Language Modeling (MLM) in Sec. . We train models with up to 10B tokens and report the number of steps saved to reach the same evaluation loss (perplexity). We also report downstream task performance to benchmark performance improvements after training the same number of steps.

## Experimental setup

In Sec. , we pretrain 774M Decoder-only LMs (`GPT-2 Large` architecture ) **from scratch** on the `RedPajama` dataset . `RedPajama` dataset is an open-source reproduction of the training data used for LLaMA-1/2 models , totaling 1.2T tokens from 7 data domains with proportions: `Common Crawl` (67%), `C4` (15%), `GitHub` (4.5%), `Wikipedia` (4.5%), `ArXiv` (2.5%), and `StackExchange` (2.0%). In Sec. , we pretrain 110M Encoder-only LMs (`BERT-base` architecture ) **from scratch** on data from 5 typical sources—`Amazon Reviews`, `Arxiv`, `Books`, `Wikipedia`, and `Open WebText Corpus` . Further details are in App.  and . Runtime and GPU hours are documented in App. .

## Causal Language Modeling with Decoder-only LMs (GPT)

**Evaluation** We test the perplexity on the held-out dataset, comprising 10K samples each from the 7 domains. For downstream tasks, we include: `BoolQ` (zero-shot), `HellaSwag` (zero-shot, 10-shot), `PIQA` (zero-shot), `TruthfulQA` (zero-shot), `PubMedQA` (10-shot), `CrowsPairs` (25-shot), and `ARC-Easy` (zero-shot). Additionally, `BBH Novel Concepts` task is added to the aggregated results for models trained beyond 10B tokens, making a total of 9 tasks. We select tasks that ensure the model’s performance surpasses random guessing, spanning from question answering and commonsense inference to bias identification and scientific problem solving. These tasks provide a comprehensive assessment of model performance . We adopt the evaluation framework from . More details are available in App. .

**Baselines** We report results for our methods (<span class="smallcaps">DDO</span> and <span class="smallcaps">AutoScale</span>) and 5 baselines–<span class="smallcaps">Uniform</span>, <span class="smallcaps">LLaMA weights</span> (curated), <span class="smallcaps">DoReMi</span> (LLaMA weights initialization), <span class="smallcaps">Data Mixing Laws from </span> and <span class="smallcaps">DoReMi</span> from (uniform initialization). Uniform weights uniformly sample data from all domains, resulting in the same number of training tokens from each domain. LLaMA weights are a set of curated domain weights heuristically tuned for training LLaMA-1/2 models. We implemented <span class="smallcaps">DoReMi</span> proposed in . <span class="smallcaps">DoReMi</span> trains two smaller-scale auxiliary models (proxy models). First, a reference model is trained with the dataset’s original domain weights, which are the LLaMA weights for `RedPajama` dataset. Then, optimized domain weights are obtained by using a proxy model to minimize the worst-case excess loss across different domains. We train both auxiliary models for 50K steps. Implementation details are available in App. . Besides, we compare with 2 domain weights from existing literature, which are optimized on the same dataset, `RedPajama`, with similar Decoder-only LMs. <span class="smallcaps">Data Mixing Laws</span> first performs a grid search on the space of possible data mixtures and records evaluation loss for proxy models trained on these mixtures. Then, the loss is interpolated with exponential functions to find the optimal domain weights for the proxy model. <span class="smallcaps">DOGE</span> also implements <span class="smallcaps">DoReMi</span> with auxiliary models trained for 50K steps but with the reference model trained with uniform weights. We evaluate the model trained on these domain weights to present a complete landscape.

**Direct Data Optimization (<span class="smallcaps">DDO</span>):** We conduct <span class="smallcaps">DDO</span> Algorithm to optimize domain weights for models (774M Decoder-only LMs) trained from scratch with 30M to 1.2B tokens. *As depicted in Fig. (a), optimal domain weights for each training data scale are visibly different and demonstrate a clear shifting pattern*. We found data sources with standard format such as `Wikipedia` and scientific papers, regarded as high quality, are most beneficial at smaller scales and observe sharp diminishing returns as the training data scales up. With more compute, data sources with diverse examples, such as `CommonCrawl`, continue to reduce training loss for even larger training data scales. In Fig. (b), we validated this observation in Fig. (b), where we trained two models with 0.3B tokens with domain weights optimized at 0.3B tokens and 1.2B tokens, and two models with 1.2B tokens with these weights, respectively. *<span style="color: teal">Takeaway 1: the results show that, the optimal weights are only optimal at the scale it is optimized and become suboptimal when applied on other scales.</span>*

**Predicting Optimal Weights at Larger Scales with <span class="smallcaps">AutoScale</span>:** With <span class="smallcaps">DDO</span>-optimized weights from models trained up to 0.6B tokens, we fit <span class="smallcaps">AutoScale</span> predictor and use it to visualize how the optimal domain weights will shift as we continue scaling up training data. Depicted in Fig. (d) and Fig. , as the training data scale grows, data sources with diverse examples, such as `C4` and `CommonCrawl`, will take up a considerable proportion of training data. Therefore, we expect LLaMA weights will perform better when the training data scale is sufficiently large. The results also suggest training on data from `Books` domain will continue to provide benefits. <span style="color: teal">*Takeaway 2: thus,* <span class="smallcaps">AutoScale</span>*-predicted domain weights give a larger weight to Books domain compared to baselines which counterintuitively downweight high-quality book contents*.</span>

Subsequently, to examine <span class="smallcaps">AutoScale</span>-predicted weights, we train models on larger scales with 3B, 5B, and 10B tokens. On 3B training data, we compare <span class="smallcaps">AutoScale</span>-predicted weights with Uniform weights, LLaMA weights, <span class="smallcaps">DoReMi</span> weights from (uniform initialization), and <span class="smallcaps">Data Mixing Laws</span> weights from . In both 3B and 5B results (Fig. ), <span class="smallcaps">AutoScale</span> *achieves the lowest validation perplexity after the same steps, at least $25\%$ faster than any baseline with up to $37\%$ speed up*. Provided in Table , <span class="smallcaps">AutoScale</span>-predicted weights significantly reduced the loss on `Books` domain and also achieved much lowered worst-domain perplexity. When testing the few-shot performance on 8 downstream tasks, the model trained with <span class="smallcaps">AutoScale</span>-predicted weights achieves the best overall performance (Table ). Results for models trained with 10B tokens are depicted in Fig. (e)(f), where we added the comparison with <span class="smallcaps">DoReMi</span> initialized with LLaMA weights. <span style="color: teal">*Takeaway 3:* <span class="smallcaps">AutoScale</span>*-predicted weights consistently outperform any baseline with a $28\%$ to $38\%$ margin and demonstrate advantageous performance on downstream tasks.* </span> Echoing our predictions, as training data scales up, LLaMA weights visibly outperform uniform domain weights. See App.  for additional results .

## Masked Language Modeling with Encoder-only LMs (BERT)

We evaluate the model’s MLM loss on held-out validation datasets, comprising 10K samples each from the 5 training domains. Additionally, as an auxiliary evaluation, we test the MLM loss on 3 non-training held-out domains. To be consistent with the perplexity loss used in CLM, we report the exponential cross-entropy loss for MLM. We evaluate the model’s task performance on `GLUE` benchmark (with 8 diverse tasks for natural language understanding (NLU)) and `SQuAD` (a large-scale QA dataset). See App.  for more details. Uniform weights are used as the baseline.

**Direct Data Optimization (<span class="smallcaps">DDO</span>):** We conduct <span class="smallcaps">DDO</span> Algorithm to optimize domain weights for proxy models (110M Encoder-only LMs) trained from scratch with MLM on 1GB data. Results for <span class="smallcaps">DDO</span>-optimized weights are shown in Fig. . *Takeaway 3a:* <span class="smallcaps">DDO</span> *visibly decreased the model’s validation loss on all training domains as well as held-out non-training domains, demonstrating its effectiveness in improving training efficiency and model utility*. When testing on `GLUE` *benchmark and* `SQuAD` *dataset, consistent with the reduced evaluation loss,* <span class="smallcaps">DDO</span>-optimized weights are shown to improve the model’s performance on downstream tasks by a notable margin.

<figure id="fig:figure2">
<figure id="fig:figure2a">
<span class="image placeholder" data-original-image-src="bertfigs/bert180pr.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>Validation Loss (<span class="math inline">\(\downarrow\)</span> lower is better)</figcaption>
</figure>
<figure id="fig:figure2b">
<span class="image placeholder" data-original-image-src="bertfigs/glue180pr.png" data-original-image-title="" width="\textwidth"></span>
<figcaption>Task Performance (<span class="math inline">\(\uparrow\)</span> higher is better)</figcaption>
</figure>
<figcaption><span>Optimizing domain weights with <span class="smallcaps">DDO</span> algorithm for pre-training Encoder-only LMs (<code>BERT</code>). <span class="smallcaps">DDO</span> substantially reduces validation loss. After reweighting, all training domains’ loss has either decreased or remained unchanged. Out-of-domain loss on non-training domains has also decreased considerably. Enhanced performance ishas been observed on all <code>GLUE</code> tasks (eval metric: <code>cola</code>: Matt. corr., <code>stsb</code>: Pearson corr., rest: acc.) and <code>SQuAD</code> (acc.). </span></figcaption>
</figure>

**Predicting Optimal Weights at Larger Scales with <span class="smallcaps">AutoScale</span>:** With <span class="smallcaps">DDO</span>-optimized weights from models trained up to 0.5B tokens, we fit <span class="smallcaps">AutoScale</span> predictor and use it to predict how the optimal domain weights will shift as we continue scaling up training data. Depicted in Fig. , similar to the pattern described above, as the training data scale grows, data sources with diverse examples, such as WebText and Amazon Reviews, become increasingly important over standard domains, such as Wikipedia and Arxiv. One hypothesis is such data sources contain samples on diverse topics and language styles, providing rich information compared to domains with clean, standard text. We train models with MLM for up to 288k steps ($\sim120\%$ of the pertaining data size for original `BERT-base` ). Table shows that, compared to without reweighting (uniform weights), <span class="smallcaps">AutoScale</span>*-predicted weights speed up training by 16.7% on most data scales with a 10% speedup on the largest scale, validating its consistent effectiveness*. *<span style="color: teal">Takeaway 4: nonetheless, the speedup is less impressive than in the results for Decoder-only LMs, demonstrating the different response to domain reweighting for models with different architecture or language modeling objectives</span>.* This is also hinted in Fig. (b), where the evaluation loss has a similar response to data from different domains, suggesting limited potential for performance improvements from domain reweighting.

# Conclusions

In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, showcasing that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. Addressing this challenge, we propose <span class="smallcaps">AutoScale</span>, an automated tool that finds a compute-optimal data composition for training at any desired target scale. In empirical studies with pre-training 774M Decoder-only LMs and Encoder-only LMs, <span class="smallcaps">AutoScale</span> decreases validation perplexity at least 28% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks.

#### Limitations $\&$ Future Work

The promising results achieved by <span class="smallcaps">AutoScale</span> in optimizing data composition for large-scale language model pretraining open up some intriguing avenues for future exploration. (1) *Generalizability*: It will be interesting to extend this research to larger-scale settings, other data modalities, and more comprehensive evaluation benchmarks, and re-examine the validity of insights provided by experiments at the scale that we work on. (2) *Direct optimization of downstream performance*: In practice, the capabilities of LLMs are characterized by their performance on various downstream tasks, and the perplexity loss that we focused on in this study is only a rough, inaccurate proxy for downstream performance. It will be interesting to extend <span class="smallcaps">AutoScale</span> to directly optimize downstream performance. (3) *More fine-grained data curation*: <span class="smallcaps">AutoScale</span> works with fixed data domains and only optimizes how the domains are mixed together, confining the optimization space. Intuitively, if one can strategically select the corpus within each domain and even adapt the data selection strategy to different stages of training, further improvements could be achieved.

#### Broader Impacts

Reducing the complexity and resource requirements associated with pretraining LLMs, <span class="smallcaps">AutoScale</span> contributes to the democratization of AI. Smaller organizations, academic institutions, and individual researchers can more easily participate in cutting-edge AI research and development, fostering innovation and collaboration across the AI community. Moreover, learning from massive amounts of data requires large and costly computational resources, which not only consume substantial energy but also generate a significant carbon footprint, contributing to environmental issues. Furthermore, these resources quickly become obsolete due to the rapid pace of technological advancements, leading to e-waste. This research makes contributions to mitigating these issues by improving the efficiency of resource utilization in AI training.

# Acknowledgement

This work is supported in part by the National Science Foundation under grants IIS-2312794, IIS2313130, OAC-2239622, Amazon-Virginia Tech Initiative in Efficient and Robust Machine Learning, AWS computational credits, and the Commonwealth Cyber Initiative. The authors are grateful for Ankit Battawar and Alix Delgado from AWS, whose dedicated help and support were crucial for securing computing resources and implementing empirical studies.

[^1]: <https://github.com/feiyang-k/AutoScale>

[^2]: Equal contribution. $^\dagger$Correspondence to: Feiyang Kang and Ruoxi Jia $<$`fyk, ruoxijia@vt.edu`$>$.

[^3]: Empirically, we found setting the perturbation ratio, $r=N_i/N_i^-=N_i^+/N_i=3$, produces reliable results.
