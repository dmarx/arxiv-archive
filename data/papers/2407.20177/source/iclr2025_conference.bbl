\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aharoni and Goldberg(2020)]{aharoni2020unsupervised}
Roee Aharoni and Yoav Goldberg.
\newblock Unsupervised domain clusters in pretrained language models.
\newblock \emph{arXiv preprint arXiv:2004.02105}, 2020.

\bibitem[AI@Meta(2024)]{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Alabdulmohsin et~al.(2022)Alabdulmohsin, Neyshabur, and Zhai]{alabdulmohsin2022revisiting}
Ibrahim~M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 22300--22312, 2022.

\bibitem[Albalak et~al.(2023)Albalak, Pan, Raffel, and Wang]{albalak2023efficient}
Alon Albalak, Liangming Pan, Colin Raffel, and William~Yang Wang.
\newblock Efficient online data mixing for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2312.02406}, 2023.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang, Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock \emph{arXiv preprint arXiv:2402.16827}, 2024.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Bertsekas(1997)]{bertsekas1997nonlinear}
Dimitri~P Bertsekas.
\newblock Nonlinear programming.
\newblock \emph{Journal of the Operational Research Society}, 48\penalty0 (3):\penalty0 334--334, 1997.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem[Borsos et~al.(2020)Borsos, Mutny, and Krause]{borsos2020coresets}
Zal{\'a}n Borsos, Mojmir Mutny, and Andreas Krause.
\newblock Coresets via bilevel optimization for continual learning and streaming.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14879--14890, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2024)Chen, Roberts, Bhatia, Wang, Zhang, Sala, and R{\'e}]{chen2024skill}
Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce~Zhang, Frederic Sala, and Christopher R{\'e}.
\newblock Skill-it! a data-driven skills framework for understanding and training language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Coleman et~al.(2019)Coleman, Yeh, Mussmann, Mirzasoleiman, Bailis, Liang, Leskovec, and Zaharia]{coleman2019selection}
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock \emph{arXiv preprint arXiv:1906.11829}, 2019.

\bibitem[Colson et~al.(2007)Colson, Marcotte, and Savard]{colson2007overview}
Beno{\^\i}t Colson, Patrice Marcotte, and Gilles Savard.
\newblock An overview of bilevel optimization.
\newblock \emph{Annals of operations research}, 153:\penalty0 235--256, 2007.

\bibitem[Computer(2023)]{together2023redpajama}
Together Computer.
\newblock Redpajama: An open source recipe to reproduce llama training dataset, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Devlin et~al.(2018{\natexlab{a}})Devlin, Chang, Lee, and Toutanova]{DBLP:journals/corr/abs-1810-04805}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{CoRR}, abs/1810.04805, 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1810.04805}.

\bibitem[Devlin et~al.(2018{\natexlab{b}})Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018{\natexlab{b}}.

\bibitem[Fan et~al.(2023)Fan, Pagliardini, and Jaggi]{fan2023doge}
Simin Fan, Matteo Pagliardini, and Martin Jaggi.
\newblock Doge: Domain reweighting with generalization estimation.
\newblock \emph{arXiv preprint arXiv:2310.15393}, 2023.

\bibitem[Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao, Mercat, Fang, Li, Keh, et~al.]{gadre2024language}
Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et~al.
\newblock Language models scale reliably with over-training and on downstream tasks.
\newblock \emph{arXiv preprint arXiv:2403.08540}, 2024.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding, Hsu, McDonell, Muennighoff, et~al.]{gao2021framework}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et~al.
\newblock A framework for few-shot language model evaluation.
\newblock \emph{Version v0. 0.1. Sept}, 2021.

\bibitem[Ghorbani and Zou(2019)]{ghorbani2019data}
Amirata Ghorbani and James Zou.
\newblock Data shapley: Equitable valuation of data for machine learning.
\newblock In \emph{International Conference on Machine Learning}, pages 2242--2251. PMLR, 2019.

\bibitem[Gokaslan and Cohen(2019)]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem[Goyal et~al.()Goyal, Maini, Lipton, Raghunathan, and Kolter]{goyal2024science}
Sachin Goyal, Pratyush Maini, Zachary~Chase Lipton, Aditi Raghunathan, and J~Zico Kolter.
\newblock The science of data filtering: Data curation cannot be compute agnostic.
\newblock In \emph{ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models}.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo, Beltagy, Downey, and Smith]{gururangan2020don}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug Downey, and Noah~A Smith.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock \emph{arXiv preprint arXiv:2004.10964}, 2020.

\bibitem[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish]{hernandez2021scaling}
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
\newblock Scaling laws for transfer.
\newblock \emph{arXiv preprint arXiv:2102.01293}, 2021.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Jia et~al.(2019)Jia, Dao, Wang, Hubis, Gurel, Li, Zhang, Spanos, and Song]{jia2019efficient}
Ruoxi Jia, David Dao, Boxin Wang, Frances~Ann Hubis, Nezihe~Merve Gurel, Bo~Li, Ce~Zhang, Costas~J Spanos, and Dawn Song.
\newblock Efficient task-specific data valuation for nearest neighbor algorithms.
\newblock \emph{arXiv preprint arXiv:1908.08619}, 2019.

\bibitem[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{jin2019pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W Cohen, and Xinghua Lu.
\newblock Pubmedqa: A dataset for biomedical research question answering.
\newblock \emph{arXiv preprint arXiv:1909.06146}, 2019.

\bibitem[Just et~al.(2023{\natexlab{a}})Just, Chen, Kang, Zhang, Sahu, and Jia]{just2023asr}
Hoang~Anh Just, I-Fan Chen, Feiyang Kang, Yuanzhi Zhang, Anit~Kumar Sahu, and Ruoxi Jia.
\newblock Asr data selection from multiple sources: A practical approach on performance scaling.
\newblock \emph{NeurIPS 2023 Workshop on Efficient Natural Language and Speech Processing (ENLSP)}, 2023{\natexlab{a}}.

\bibitem[Just et~al.(2023{\natexlab{b}})Just, Kang, Wang, Zeng, Ko, Jin, and Jia]{just2023lava}
Hoang~Anh Just, Feiyang Kang, Tianhao Wang, Yi~Zeng, Myeongseob Ko, Ming Jin, and Ruoxi Jia.
\newblock Lava: Data valuation without pre-specified learning algorithms.
\newblock In \emph{11th International Conference on Learning Representations, ICLR}, page to appear, 2023{\natexlab{b}}.

\bibitem[Kang et~al.(2023)Kang, Just, Sahu, and Jia]{kang2023performance}
Feiyang Kang, Hoang~Anh Just, Anit~Kumar Sahu, and Ruoxi Jia.
\newblock Performance scaling via optimal transport: Enabling data selection from partially revealed sources.
\newblock \emph{arXiv preprint arXiv:2307.02460}, 2023.

\bibitem[Kang et~al.(2024)Kang, Just, Sun, Jahagirdar, Zhang, Du, Sahu, and Jia]{kang2024get}
Feiyang Kang, Hoang~Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit~Kumar Sahu, and Ruoxi Jia.
\newblock Get more for less: Principled data selection for warming up fine-tuning in llms.
\newblock \emph{12th International Conference on Learning Representations, ICLR}, 2024.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kaushal et~al.(2019)Kaushal, Iyer, Kothawade, Mahadev, Doctor, and Ramakrishnan]{kaushal2019learning}
Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav Doctor, and Ganesh Ramakrishnan.
\newblock Learning from less data: A unified data subset selection and active learning framework for computer vision.
\newblock In \emph{2019 IEEE Winter Conference on Applications of Computer Vision (WACV)}, pages 1289--1299. IEEE, 2019.

\bibitem[Killamsetty et~al.(2021)Killamsetty, Durga, Ramakrishnan, De, and Iyer]{killamsetty2021grad}
Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer.
\newblock Grad-match: Gradient matching based data subset selection for efficient deep model training.
\newblock In \emph{International Conference on Machine Learning}, pages 5464--5474. PMLR, 2021.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pages 1885--1894. PMLR, 2017.

\bibitem[Kwon and Zou(2023)]{kwon2023data}
Yongchan Kwon and James Zou.
\newblock Data-oob: Out-of-bag estimate as a simple and efficient data value.
\newblock \emph{arXiv preprint arXiv:2304.07718}, 2023.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Liu et~al.(2024)Liu, Zheng, Muennighoff, Zeng, Dou, Pang, Jiang, and Lin]{liu2024regmix}
Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin.
\newblock Regmix: Data mixture as regression for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2407.01492}, 2024.

\bibitem[Liu et~al.(2021)Liu, Gao, Zhang, Meng, and Lin]{liu2021investigating}
Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin.
\newblock Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44\penalty0 (12):\penalty0 10045--10067, 2021.

\bibitem[Mahmood et~al.(2022{\natexlab{a}})Mahmood, Lucas, Acuna, Li, Philion, Alvarez, Yu, Fidler, and Law]{mahmood2022much}
Rafid Mahmood, James Lucas, David Acuna, Daiqing Li, Jonah Philion, Jose~M Alvarez, Zhiding Yu, Sanja Fidler, and Marc~T Law.
\newblock How much more data do i need? estimating requirements for downstream tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 275--284, 2022{\natexlab{a}}.

\bibitem[Mahmood et~al.(2022{\natexlab{b}})Mahmood, Lucas, Alvarez, Fidler, and Law]{mahmood2022optimizing}
Rafid Mahmood, James Lucas, Jose~M Alvarez, Sanja Fidler, and Marc Law.
\newblock Optimizing data collection for machine learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 29915--29928, 2022{\natexlab{b}}.

\bibitem[Maini et~al.(2024)Maini, Seto, Bai, Grangier, Zhang, and Jaitly]{maini2024rephrasing}
Pratyush Maini, Skyler Seto, He~Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock \emph{arXiv preprint arXiv:2401.16380}, 2024.

\bibitem[McKinzie et~al.(2024)McKinzie, Gan, Fauconnier, Dodge, Zhang, Dufter, Shah, Du, Peng, Weers, et~al.]{mckinzie2024mm1}
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et~al.
\newblock Mm1: Methods, analysis \& insights from multimodal llm pre-training.
\newblock \emph{arXiv preprint arXiv:2403.09611}, 2024.

\bibitem[Mehta et~al.(2024)Mehta, Sekhavat, Cao, Horton, Jin, Sun, Mirzadeh, Najibi, Belenko, Zatloukal, et~al.]{mehta2024openelm}
Sachin Mehta, Mohammad~Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et~al.
\newblock Openelm: An efficient language model family with open-source training and inference framework.
\newblock \emph{arXiv preprint arXiv:2404.14619}, 2024.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch, Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, et~al.]{mindermann2022prioritized}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot, Sebastian Farquhar, et~al.
\newblock Prioritized training on points that are learnable, worth learning, and not yet learnt.
\newblock In \emph{International Conference on Machine Learning}, pages 15630--15649. PMLR, 2022.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Bilmes, and Leskovec]{mirzasoleiman2020coresets}
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{International Conference on Machine Learning}, pages 6950--6960. PMLR, 2020.

\bibitem[Nangia et~al.(2020)Nangia, Vania, Bhalerao, and Bowman]{nangia2020crows}
Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel~R Bowman.
\newblock Crows-pairs: A challenge dataset for measuring social biases in masked language models.
\newblock \emph{arXiv preprint arXiv:2010.00133}, 2020.

\bibitem[Park et~al.(2022)Park, Ahmad, and Hain]{park2022unsupervised}
Chanho Park, Rehan Ahmad, and Thomas Hain.
\newblock Unsupervised data selection for speech recognition with contrastive loss ratios.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 8587--8591. IEEE, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[Rosenberg et~al.(2023)Rosenberg, Ramabhadran, Zhang, and Baskar]{rosenberg2023guided}
Andrew Rosenberg, Bhuvana Ramabhadran, Yu~Zhang, and Murali~Karthick Baskar.
\newblock Guided data selection for masked speech modeling, April~6 2023.
\newblock US Patent App. 17/820,871.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Schoch et~al.(2023)Schoch, Mishra, and Ji]{schoch2023data}
Stephanie Schoch, Ritwick Mishra, and Yangfeng Ji.
\newblock Data selection for fine-tuning large language models using transferred shapley values.
\newblock \emph{arXiv preprint arXiv:2306.10165}, 2023.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis, Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
\newblock \emph{arXiv preprint arXiv:2111.02114}, 2021.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and Morcos]{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data pruning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 19523--19536, 2022.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Tiong et~al.()Tiong, Zhao, Li, Hoi, Xiong, and Li]{tiong2024toward}
Anthony Tiong, Junqi Zhao, Junnan Li, Steven Hoi, Caiming Xiong, and Boyang Li.
\newblock Toward data-driven skill identification for general-purpose vision-language models.
\newblock In \emph{ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}, pages 353--355, 2018.

\bibitem[Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen]{wettig2024qurating}
Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.
\newblock Qurating: Selecting high-quality data for training language models.
\newblock \emph{arXiv preprint arXiv:2402.09739}, 2024.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Xie et~al.(2023)Xie, Santurkar, Ma, and Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{arXiv preprint arXiv:2302.03169}, 2023.

\bibitem[Xie et~al.(2024)Xie, Pham, Dong, Du, Liu, Lu, Liang, Le, Ma, and Yu]{xie2024doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy~S Liang, Quoc~V Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model pretraining.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Ye et~al.(2024)Ye, Liu, Sun, Zhou, Zhan, and Qiu]{ye2024data}
Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu.
\newblock Data mixing laws: Optimizing data mixtures by predicting language modeling performance.
\newblock \emph{arXiv preprint arXiv:2403.16952}, 2024.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang et~al.(2024)Zhang, Khanduri, Tsaknakis, Yao, Hong, and Liu]{zhang2024introduction}
Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, and Sijia Liu.
\newblock An introduction to bilevel optimization: Foundations and applications in signal processing and machine learning.
\newblock \emph{IEEE Signal Processing Magazine}, 41\penalty0 (1):\penalty0 38--59, 2024.

\end{thebibliography}
