{
  "arxivId": "2407.20177",
  "title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for\n  Training LLMs",
  "authors": "Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, Ruoxi Jia",
  "abstract": "Domain reweighting is an emerging research area aimed at adjusting the\nrelative weights of different data sources to improve the effectiveness and\nefficiency of language model pre-training. This paper demonstrates that the\noptimal composition of training data from different domains is scale-dependent,\nchallenging the existing practice of determining optimal mixtures through\nsmall-scale experiments and directly applying them at larger scales. We derive\nan analytical model for the dependence of optimal weights on data scale and\nintroduce *AutoScale*, a novel, practical approach for optimizing data\ncompositions at potentially large training data scales. *AutoScale* first uses\na principled optimization framework to find optimal compositions at smaller,\nfeasible scales, then predicts optimal compositions at larger scales using our\nderived model. Our evaluation on GPT-2 Large and BERT pre-training demonstrates\n*AutoScale*'s effectiveness in improving training convergence and downstream\nperformance. Particularly, for GPT-2 Large on RedPajama, *AutoScale* decreases\nvalidation perplexity 28% faster than baselines, with up to 38% speed-up over\nunweighted training, achieving the best performance across downstream tasks.\nThis work provides insights into the varying benefits of data sources across\ntraining scales for language models, contributing to the burgeoning research on\nscale-dependent data curation. Code is open-sourced.",
  "url": "https://arxiv.org/abs/2407.20177",
  "issue_number": 677,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/677",
  "created_at": "2024-12-30T20:00:56.236499",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 41,
  "last_read": "2024-12-30T21:14:30.006913",
  "last_visited": "2024-12-30T21:12:13.256000+00:00",
  "main_tex_file": null,
  "published_date": "2024-07-29T17:06:30Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "stat.ML"
  ]
}