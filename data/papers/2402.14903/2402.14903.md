---
abstract: |
  Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.
bibliography:
- references.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
---





<figure id="fig:main_result">
<p><span class="image placeholder" data-original-image-src="figures/Figure1.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.</figcaption>
</figure>

# Introduction

Large language models (LLMs) are often lauded as demonstrating the benefits of end-to-end learning over inductive biases. However, an often overlooked part of the pipeline, preventing it from being end-to-end, is tokenization: the segmenting of an input sequence of bytes into discrete tokens. Tokenization consists of two halves: training, in which a vocabulary of tokens and statistics are learned over a given corpus, and segmenting, where a function uses the trained vocabulary and statistics to map sequences of bytes to tokens. Each tokenization scheme may impart different inductive biases on the model due to the way in which bytes of input sequences are grouped – in this work, we study these tokenization-dependent effects on numerical reasoning in state-of-the art models (GPT-3.5, GPT-4) by considering the tokenization of numbers in arithmetic problems.

<figure id="fig:4digit_tokenization_split">
<p><span class="image placeholder" data-original-image-src="figures/p50k_base_4digit_nowhitespace_partitions.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>Comparison of how <code>p50k_base</code>, the tokenizer for GPT-3, and <code>cl100k_base</code>, the tokenizer for GPT-3.5 and GPT-4, segments 4 digit strings into tokens. <code>cl100k_base</code> standardized number tokenization to chunks of 3 digits, left-to-right, resulting in all N-digit numbers being segmented the same way.</figcaption>
</figure>

Though many techniques have been proposed for tokenization, the prevailing methods in today’s frontier models are variants of Byte Pair Encoding (BPE) . BPE is a statistical approach to tokenization that is learnt from a dataset of, in the case of LLMs, text. Intuitively, BPE compresses the dataset by iteratively creating tokens for the most commonly occurring subsequences. Specifically, BPE begins with a token vocabulary consisting of each character in the text (e.g. letters, numbers, punctuation).[^1] Using this vocabulary, bigram statistics (i.e. frequencies of pairs) are calculated, and the most common bigram is added to the vocabulary and merged in the dataset. This process is repeated up until a prespecified, maximum vocabulary size is reached. After the tokenizer is learned, tokenization of new text proceeds by iteratively merging characters/tokens in the same order as learned on the training dataset.

Naively applying BPE on internet-scale corpora leads to very idiosyncratic number tokenization . In the training phase, which numbers receive dedicated tokens is very adhoc – for example, $710$ may get a dedicated token, while $711$ will not (Figure ). In the segmenting phase, these adhoc tokens will lead to different partitionings of numbers of the same length. In Figure , we illustrate the different partitionings of all 4 digit numbers when using the `p50k_base` tokenizer that was used to train GPT-3 . To control for effects on downstream performance from such idiosyncratic tokenization, prior work has used formatting, such as spaces or commas, to separate individual digits, ensuring each digit maps to a single token.

Newer models, and their corresponding tokenizers, indicate that LLM practitioners across different labs have also tried to control for idiosyncratic tokenization (Table ).[^2] The PaLM , LLaMa and Mistral[^3] models switch to single-digit tokenization, similar to that enforced by . Interestingly, GPT-3.5 and GPT-4’s tokenizer, `cl100k_base`, introduces tokens for all 1-, 2-, and 3-digit strings.[^4] Tokenization of numbers by these GPT models defaults to breaking a long number into 3-digit chunks, left-to-right, which we hypothesize (and later show) may create issues for numerical reasoning.

These varied approaches to tokenization by today’s frontier LLMs indicate a lack of convergence in the field on best practices and call for a deeper analysis of (positive or negative) inductive biases imparted by various tokenization schemes. In this work, we provide the first systematic comparison of model performance on the same numerical reasoning tasks with varied tokenization. Specifically, we consider the latest GPT models on few-shot arithmetic tasks. We vary the tokenization direction to be the default left-to-right (L2R) or right-to-left (R2L). We find that model accuracy is up to 20% higher when using R2L tokenization (Figure , Section ). We then provide a thorough analysis of error patterns across these two tokenizations (Section ). We find that the difference in performance between R2L and L2R tokenization in GPT-3.5 can largely be explained by an extremely stereotyped and surprising error pattern (Section ), perhaps indicating the presence of some systematic, but flawed, reasoning. Next, we show that chain-of-thought-inspired approaches, where a model is asked to repeat an input in R2L tokenization, recover the accuracy otherwise lost due to L2R tokenization (Section ). Finally, we conclude by studying how these effects may change with model version, finding that larger models are better able to override the tokenization-induced effects but, as of yet, unable to eliminate them (Section ). Overall, we view these results as compelling evidence towards significant tokenization-dependent inductive biases in large language models, and hope they lead model practitioners to conduct careful pre-training ablations with varying tokenization schemes, especially for numerical reasoning.

# Methods

## Experiment setup

We evaluate GPT models through the Chat Completions endpoint on the OpenAI API[^5] on few-shot addition problems. We control for addend digit length, ranging from 7 to 9 digits (chosen since this way each addend is 3 tokens long). For most experiments, we use 90 random problems, with 10 problems for each addend digit length pair (e.g., 10 problems where the addends are both 7 digits long, 10 problems where the first addend is 7 digits and the second is 8, etc.). For shots, we consider 1-, 2-, 4-, and 8-shots. Shots are sampled randomly for each “query” problem, and are provided to the model as a multi-turn dialogue. We control shots to have the same form (digit lengths, tokenization direction, etc.) as the query problem. We use the default system prompt “You are a helpful assistant.” for maximum reproducibility.[^6] Python code for some example 2-shot queries to the model are presented in Appendix  for maximum clarity. We use greedy decoding (temperature=0) in all experiments. Accuracy was computed by extracting numbers from model responses.

Most experiments in the paper are run using the `gpt-3.5-turbo-0301` model checkpoint, though in Section  we look into how results extend to newer versions of the same model (`gpt-3.5-turbo-0613`) and to the, presumably larger, GPT-4 models (`gpt-4-0314, gpt-4-0613`). All code and full results tables can be found at <https://github.com/aadityasingh/TokenizationCounts>.

## Varying L2R vs. R2L tokenization

The ChatCompletion API only allows for input text, not input tokens, so it’s tricky to conduct tokenization-varying experiments. To force the model to use R2L tokenization for numbers, we add commas every 3-digits from the right (see Figure ). Since the tokenizer doesn’t contain any tokens with numbers and commas, the commas get tokenized separately, effectively enforcing a different segmentation of digits. We use this setting to illustrate our main results, and conduct various controls to ensure that our observed effect is due to tokenization as opposed to other confounds.

# Right-to-left tokenization improves model performance

## Main results

When using commas to separate digits and enforce R2L tokenization, we observed greatly improved average performance (8-shot result in Figure ). We found that increasing the number of shots (Figure ) led to a larger increase for the L2R tokenization (from 68.5% 1-shot to 75.6% 8-shot) than for the R2L tokenization (from 95.6% 1-shot to 97.8% 8-shot) indicating that in-context learning may slightly mitigate the (harmful) bias of L2R tokenization. Given this finding and the plateau-ing in performance with increasing shots, we report only 8-shot results for the remainder of the work as this makes L2R tokenization the most competitive.

<figure id="fig:token_dir_shots">
<p><span class="image placeholder" data-original-image-src="figures/r2l_v_l2r_w_shots.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>Effect of R2L vs L2R tokenization with increasing shots.</figcaption>
</figure>

## Controlling for comma-based semantic priors

Though this result is already compelling, we realize that commas are often used to separate digits in the manner depicted in Figure , so the observed effect may be confounded by prevalence in training data . One might argue that comma separation is actually bringing the input closer to the training distribution of the model, so it’s not a surprise that models perform better. To control for this and focus in on tokenization, we consider alternate, single-token separators: `’ ’, ’.’, ’$’, ’#’` (note we’ll refer to `’ ’` as `<space>` for clarity). For example, the number `8302080` would be written as `8#302#080` when input to the model.

Results are shown in Figure . We find that the model is largely agnostic to the separator used, indicating that tokenization is likely the dominant effect, rather than the specific choice of using commas.

<figure id="fig:control_delim">
<p><span class="image placeholder" data-original-image-src="figures/control_delimiter.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>8-shot accuracy when using different delimiters for R2L tokenization. Dotted lines show results from Figure <span class="math inline">\(\ref{fig:main_result}\)</span> for comparison. Overall, we see choice of delimiter matters less than direction of tokenization.</figcaption>
</figure>

## Controlling for “thinking tokens”

Another confound with the above experiment may be that adding commas both increases the number of tokens input to as well as generated by the model. Thus, to generate the same answer, the model has access to more computation steps (i.e., FLOPs). There is a worry that models may use these repetitive *thinking tokens* to perform additional useful computations . In practice, this seems not to happen without further training , but we conducted experiments to verify this in our setting.

To control for thinking tokens, we consider two types of controls. In the first, we use separators to enforce L2R tokenization – this enforces an exact match in prompt token counts. Second, we consider adding 1 or 2 spaces before and after the `+` and `=` sign to increase the number of tokens[^7] in the L2R case (where no separator is used). Both of these have the benefit of adding extremely “predictable” tokens (when using 8-shots), allowing the model to possibly use the extra computation steps for “thinking”.

In Figure , we find that neither of these controls, when applied to L2R tokenized sequences, recovers the performance of R2L tokenization. In fact, we found that using separators with the L2R tokenization often hurt performance, likely because this is an uncommon representation—upon qualitative inspection of a few examples, we found the model sometimes “auto-corrects” the inputs by hallucinating trailing zeros. We believe these experiments effectively rule out the “thinking token” confound.

<figure id="fig:control_thinking_token">
<p><span class="image placeholder" data-original-image-src="figures/control_thinking_token.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>8-shot accuracy for various “thinking token” controls. Dotted lines show results from Figure <span class="math inline">\(\ref{fig:main_result}\)</span> for comparison. We also experimented with other delimiters for L2R tokenization (all those from Figure <span class="math inline">\(\ref{fig:control_delim}\)</span>), but found similarly poor results. Overall, “thinking tokens” do not recover the performance boost from using comma-enforced R2L tokenization.</figcaption>
</figure>

# Error analysis reveals stereotyped patterns

Given the robust effect observed in Section , we were curious to see if there were any patterns in the errors. Below, we summarize our key findings.

## L2R tokenization is significantly worse when answer is longer than addends

As noted in Section , we balanced our dataset of problems based on input digit length. Upon inspection of problems the model got incorrect when using L2R tokenization, we noted that errors seemed more likely when the answer was longer than the addends (e.g., a problem where 7 digit number + 7 digit number = 8 digit number, of the form depicted in Figure ). To test this hypothesis, we conducted a new experiment where we controlled for addend lengths *and* answer lengths. Specifically, we generated 100 random problems for each possible triplet of digit lengths where addends and answer have a length of 7 to 9 digits (full list in Appendix ). The remainder of our experiments in this section will use this expanded set of problems to show the robustness of the found error patterns.

<figure id="fig:answer_length">
<p><span class="image placeholder" data-original-image-src="figures/answer_length_error.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>When the answer is the same length in digits as an addend (length match), both tokenization schemes perform similarly (left). When the answer is a different length in digits than either addend (length mismatch), L2R tokenization destroys model performance, dropping to 8.25% (right).</figcaption>
</figure>

<figure id="fig:carries">
<p><span class="image placeholder" data-original-image-src="figures/num_carries.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>Accuracy as a function of number of carries. For L2R tokenization, we exclude problems where the answer length does not match at least one addend, as the model misses most of those (92%) as shown in Section <span class="math inline">\(\ref{sec:error_length}\)</span>. If number of carries (a human notion of difficulty) was correlated to model performance, we would expect a negative slope. The lack of any trend suggests model performance is largely independent of number of carries.</figcaption>
</figure>

We reproduced our main phenomenon (Section ), and further affirmed our intuitions about error patterns. As shown in Figure , we find that L2R tokenization has similar performance to R2L tokenization when the answer’s length in digits is the same as one of the inputs (which we refer to as the “length match” condition). When the answer is longer than the inputs (due to a final carry), L2R tokenization is significantly worse, with accuracy dropping down to 8.25% – we refer to this as the “length mismatch” condition. We suspect that this strong effect may be due to the misalignment between input and output tokenizations (as illustrated in Figure ) rather than some carry-related notion of problem difficulty, which we explore in the next few subsections.

## Errors do not seem correlated to number of carries

A natural hypothesis given the above result may be that errors might just be correlated to some notion of difficulty, such as carries. In Figure , we find that this is generally not the case. Specifically, we consider the accuracy on subsets of problems based on how many carries are needed to solve them.[^8] The lack of a clear positive or negative trend indicates that model performance is not strongly affected by the number of carries.

## Length mismatch problems yield stereotyped “digit 4” error pattern

If not carries, what could be causing the surprising error pattern in Figure ? In Figure a, we find that the errors when using L2R tokenization are extremely stereotyped and not at all intuitive. Specifically, in the length mismatch condition, the model *always* gets the fourth digit wrong. Furthermore, the model *always* gets the first 3 digits correct (corresponding to the first output token). In terms of how far off the model is on digit 4, Figure b shows that there’s a slight preference to off-by-one errors, but overall the specific substitution appears quite haphazard.

We found this result extremely surprising. In cognitive science, such stereotyped error patterns are often used as evidence of underlying systematic processing. While the mechanism for addition in LLMs remains unclear, we find this striking, tokenization-dependent error pattern[^9] as highly suggestive of some underlying algorithm (in contrast to suggestions that LLMs may be performing arithmetic using some “fuzzy” matching to similar problems in training). We provide further evidence of stereotyped error patterns by analyzing model log probabilities in Appendix .

<figure id="fig:error_digit4">
<figure>
<span class="image placeholder" data-original-image-src="figures/len_mismatch_nosep_digit4.pdf" data-original-image-title="" width="0.95\columnwidth"></span>
<figcaption>Location of errors</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/len_mismatch_nosep_digit4_offby.pdf" data-original-image-title="" width="0.95\columnwidth"></span>
<figcaption>Magnitude of errors</figcaption>
</figure>
<figcaption><strong>a)</strong> Error patterns for L2R tokenization over problems where the answer digit length is different than the addend lengths. “None” indicates the problems in this case that the model got correct (8.25%). “Other” indicates problems where the model doesn’t provide a valid answer or provides an answer of the wrong length (0.5%). Of the remaining 91.25% which the model gets incorrect, it shockingly <em>always</em> gets digit 4 wrong. In addition, it sometimes gets other digits (5, 6 or 7) wrong. <strong>b)</strong> For the errors in digit 4, we show the magnitude of the mistake. For example, if the correct value of digit 4 is <code>2</code> and the model response has digit 4 equal to <code>5</code>, it would be off by <code>3</code>. We see a slight preference to off-by-1 errors, but error magnitudes are fairly evenly distributed.</figcaption>
</figure>

## Off-by-one errors at token boundaries account for nearly all remaining errors

After accounting for the main source of error, we analyzed the remaining errors across both tokenization methods: 25 out of the 1300 problems for R2L tokenization, and 56 out of the 900 problems in the length match condition for L2R tokenization.

For R2L tokenization, of the 25 problems missed, 24 are due to off-by-one (either above or below) errors. For L2R tokenization, of the 56 problems missed, 53 are due to off-by-one (either above or below) errors. For nearly all these off-by-one errors,[^10] regardless of tokenization direction, we find that the error itself occurs in *the last digit of an output token*. This result suggests that off-by-one errors are more likely across token boundaries as opposed to in the middle of a 3-digit token. This hypothesis, with preliminary evidence, connects to works on length generalization – using 3-digit tokens may make length generalization easier as models only need to cross token boundaries every third digit (as opposed to every digit).

# Models are able to convert from L2R to R2L tokenization, improving performance

## Main results

With the above results showing that number tokenization can strongly affect numerical reasoning, we ask if models can be prompted to take problems in a less preferred tokenization (L2R) and convert them to a more preferred tokenization (R2L) to improve performance. Inspired by chain-of-thought approaches , we few-shot prompt models to take problems with one tokenization direction, and then repeat the problem and answer it using a different tokenization direction. In Figure , we find that models indeed perform nearly as well at addition when converting L2R tokenization to R2L themselves as to when they receive the problem in R2L tokenization in the first place. Performance increases with the number of shots when converting L2R to R2L since the model adheres more to the (helpful) suggested repetition style. These results indicate that models can convert between tokenizations to solve problems correctly, but do not do so implicitly in the forward pass.

<figure id="fig:repeat_style">
<p><span class="image placeholder" data-original-image-src="figures/repeat_style.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>Few-shot accuracy when models receive a problem with one tokenization direction, then repeat and answer it in another.</figcaption>
</figure>

## Controlling for output tokenization

One confound with the above experiment may just be that the model improves when it’s asked to generate *answers* with R2L tokenization. To control for this, we conduct a similar experiment, but without few-shot prompting the model to repeat the problem: the few-shot prompt provides answers with a different tokenization direction than the input, incentivizing the model to answer with this tokenization direction (see Appendix  for an example prompt). In Figure , we see that just answering with R2L tokenization does not improve performance (purple curve) to the degree that repeating in R2L tokenization does (purple curve, Figure ), when starting from L2R tokenization. This effect indicates that it is important for the model to also *see* the problem in the preferred tokenization (by repeating it), rather than just answering in the preferred tokenization.

<figure id="fig:answer_style">
<p><span class="image placeholder" data-original-image-src="figures/answer_style.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>Few-shot accuracy when models receive a problem in one tokenization format, and answer it in another. The distinction between this and Figure <span class="math inline">\(\ref{fig:repeat_style}\)</span> is that models <em>do not</em> repeat the problem in this case. We note that when giving a model a problem in R2L tokenization and prompting it to answer in L2R tokenization, the model actually gets <em>worse</em> with more shots, since for fewer shots, the model ends up ignoring the few-shot prompt and answers in its preferred R2L tokenization. Specifically, adherence to the prompted formatting for R2L<span class="math inline">\(\rightarrow\)</span>L2R increases from just 13.3% with 1 shot to 98.9% with 8 shots.</figcaption>
</figure>

<figure id="fig:main_effect_new_models">
<p><span class="image placeholder" data-original-image-src="figures/accuracy_diff_tokenization_new_models_v2.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>8-shot performance of various OpenAI models on the same addition problems as Figure <span class="math inline">\(\ref{fig:token_dir_shots}\)</span>. The newer version sof GPT-3.5 appears to perform equally poorly. For GPT-4, we see a large tokenization dependent effect in the March model, which becomes weaker (but still present) in the June model. The GPT-4 turbo model shows a slight regression in overall performance with the tokenization-dependent effect becoming stronger again.</figcaption>
</figure>

# Tokenization-dependent effects mostly extend to future models

Through the previous sections, we’ve demonstrated a strong tokenization-dependent effect. In this section, we address the question: does this effect extend to newer models?

As shown in Figure , we find that generally, yes: tokenization-dependent effects persist. We consider five “held-out” OpenAI models, which allow us to consider how tokenization-dependent effects shift when models are updated (`gpt-3.5-turbo-0613`, `gpt-3.5-turbo-1106`) or scaled up (`gpt-4-0314`, `gpt-4-0613`) and then scaled back down (`gpt-4-1106-preview`, which is a “turbo” model[^11]). Later versions of GPT-3.5 exhibit as strong an effect due to tokenization direction. The effect is mitigated slightly in GPT-4’s March version, and mitigated strongly in GPT-4’s most recent version.[^12] Specifically, GPT-4 models appear to be better at performing arithmetic across the board (for both tokenization directions). Interestingly, in the most recent GPT-4 Turbo model, the effect of tokenization becomes stronger again. Furthermore, Figure  shows that the digit length mismatch between answer and addends is again the main reason for the performance drop when using L2R tokenization, in both GPT-3.5 and GPT-4 models. We believe that the increased scale of training GPT-4 (likely in both parameter count and data seen) allows it to better override the tokenization-induced inductive bias that leads GPT-3.5 models to perform worse (analogous to scale helping mitigate tokenization-induced spelling difficulties ). The resurgence of tokenization-dependent effects in the newest GPT-4 Turbo model (which is presumably smaller than GPT-4) supports this hypothesis.

<figure id="fig:error_new_models">
<p><span class="image placeholder" data-original-image-src="figures/accuracy_diff_len_match_fourcol_v2.pdf" data-original-image-title="" width="\columnwidth">image</span> </p>
<figcaption>8-shot performance of various OpenAI models on answer length controlled problems (see Section <span class="math inline">\(\ref{sec:error_length}\)</span>), separated by whether the answer length is the same as one of the addends. We see the effect from Section <span class="math inline">\(\ref{sec:error_length}\)</span> reproduces strongly in the newer version of GPT-3.5. The effect is still present in GPT-4,<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> but not as strongly. Interestingly, the effect is stronger in the latest GPT-4 Turbo model as compared to GPT-4.</figcaption>
</figure>
<aside id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>We were unable to run experiments on <code>gpt-4-0314</code> for this analysis as the model has been removed from the API, despite a stated deprecation date of June 13, 2024<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

# Related work

**Tokenization methods** The two leading tokenization methods are Unigram and BPE . While older work in NLP show the benefits of Unigram over BPE , BPE remains the most commonly used tokenization method by modern LLM practitioners. Within BPE, different models often make different hard-coded choices, such as removing long tokens of consecutive whitespace or enforcing single-digit tokenization of numbers . Our work demonstrates tokenization-dependent effects from one such choice, the use of 1-, 2-, and 3- digit tokens by OpenAI models . One way around such issues could be tokenizer-free methods (e.g., MEGABYTE , which uses patch-based schemes and doesn’t assume fixed tokens), but we suspect these schemes will also carry their own inductive biases. also introduce a continuous number encoding scheme meant to circumvent tokenization artifacts, but their approach is limited to cases where model outputs are purely numerical, and not interleaved with text.

**Tokenization artifacts in LLMs** A growing set of results has emerged around various tokenization-related artifacts in LLMs. Similar to scratchpad prompting , found that separating letters into individual tokens can help in sorting words by the second letter. Other work has focused on specific tokens that can negatively affect model performance. found many tokens which were artifacts of the data used to pre-train the tokenizer, but presumably weren’t present in the model’s training data, leading to highly unpredictable (and often comical) completions. find artifacts due to mismatches in tokenization in extractive Q&A tasks, which may have connection to some of our experiments in Section . propose *token healing* to avoid many tokenization-related issues by removing the last few tokens from a prompt and allowing the model to complete them; this approach has connections to work on asking models to *rephrase-and-respond* and our experiments on prompting the model to repeat with its preferred tokenization direction (Section ). Our work builds on these past scattered artifacts and provides a systematic analysis of tokenization-direction-dependent effects on numerical reasoning in frontier LLMs.

**Arithmetic tasks as a testbed for numerical reasoning in LLMs** With the increased interest in measuring frontier models on math reasoning , an accompanying body of work studies language models in more controlled settings, such as arithmetic. use arithmetic tasks to show that pre-training term frequencies[^13] can affect numerical reasoning in GPT-J models trained on the Pile dataset . Similarly, showed that GPT-3.5 and GPT-4 are better at computing linear functions that are more common in training data (such as the Fahrenheit to Celsius conversion) than close alternatives. Other work instead focuses on various modifications that can help models generalize to longer arithmetic tasks. and both point out that having autoregressive models perform addition in reversed order yields a simpler algorithm to learn and results in better performance, which is complementary to our emphasis on the importance of “reversed” (i.e. right-to-left) tokenization alignment. also conduct preliminary error analyses of model mistakes, though their algorithmic prompts force models to split tokens into digits (similar to ). Our work broadly lies in this category of work using arithmetic tasks to study numerical reasoning; we chose to focus on tokenization-dependent effects, and found surprisingly consistent, stereotyped error patterns (Section ), adding to this rich body of literature.

# Discussion

In this work, we analyze tokenization-dependent effects on numerical reasoning in GPT-3.5 and GPT-4. We found that the hard-coded choice of 1-, 2- and 3- digit number tokens, tokenized left-to-right, gives rise to stereotyped error patterns when compared to right-to-left tokenization. We proposed a mitigation, where the model is asked to repeat the answer in its preferred tokenization format. Finally, we showed that the effect is stronger in smaller models (such as GPT-3.5), emphasizing the significance of tokenization-dependent inductive biases in an era where many practitioners are focusing on packing capabilities into smaller models through overtraining and distillation . Overall, we believe this evidence strongly suggests inductive biases from tokenization can significantly influence model performance on numerical reasoning tasks.

Modern frontier LLMs mostly use single-digit tokens (Table ), with GPT-3.5 and GPT-4 being a key exception in their use of up-to-3-digit tokens. We hypothesize that the latter choice may have been made to achieve a better *compression rate*: models “see” more numerical data for the same number of training tokens.[^14] Furthermore, this choice could have benefits for *length generalization* , as we allude to in Section . However, we’ve also demonstrated how the misalignment between inputs and outputs when using L2R tokenization (Section ) can lead to large drops in accuracy, especially on smaller models (GPT-3.5, GPT-4 Turbo). Such misalignment would not be an issue when using single-digit tokens.

To make progress on which number tokenization choices are best to use (e.g., the single-digit tokens of LLaMa and PaLM, or the up-to-3 digit tokens of GPT-3.5 and GPT-4), the “gold experiment” would be to train the same model architecture on the same dataset, but with varying number tokenization strategies. Beyond the expense of this experiment (making it intractable in academic settings), a key question also becomes how to “compute”-control. The better compression ratio of up-to-3 digit tokens means a token-controlled experiment would result in some models “seeing” more data. We hope our work leads model practitioners to consider such ablations, with proper controls.

Beyond applicability to model practitioners, our work also provides an interesting set of tokenizaiton-dependent phenomenon for interpretability researchers to explore. Prior work has used techniques such as *path patching* to identify sub-circuits in LLMs that perform arithmetic tasks, but restricted to single token operands. Building off our results, it would be interesting to elucidate the mechanisms behind systematic error patterns, especially in the case of multi-token operands. The robustness of the “digit 4” error on GPT-3.5 points to some systematic mechanism, which could shed light on underlying algorithms that emerge to perform arithmetic tasks.

# Acknowledgements

The authors would like to acknowledge Andrew Saxe, Ted Moskovitz, Kira Düsterwald, Felix Hill, Xavier Garcia, Dan Roberts, and William Held for insightful discussions and feedback on the draft. A.K.S. is funded by the Gatsby Charitable Foundation.

# Tokenization differences between frontier LLMs

<figure id="fig:claude_token_coverage">
<span class="image placeholder" data-original-image-src="figures/anthropic_3digit_tokens_no_ws_20rows.pdf" data-original-image-title="" width="90%"></span>
<figcaption>The equivalent of Figure <span class="math inline">\(\ref{fig:p50k_tokens}\)</span> but for the Claude tokenizer. All 3-digit number strings, colored red when the string does not have a corresponding single token dedicated to it. The lack of systematicity suggests that Claude tokenizes numbers using pure BPE. Note also, however, that token coverage is generally higher than in Figure <span class="math inline">\(\ref{fig:p50k_tokens}\)</span>, likely in part because the Claude tokenizer has a larger vocabulary size (65k tokens) than OpenAI’s <code>p50k_base</code> (50k tokens).</figcaption>
</figure>

<figure id="fig:claude_4digit_partitions">
<span class="image placeholder" data-original-image-src="figures/claude_4digit_no_ws_partitions.pdf" data-original-image-title="" width="40%"></span>
<figcaption>The equivalent of Figure <span class="math inline">\(\ref{fig:4digit_tokenization_split}\)</span> but for the Claude tokenizer. Note that this distribution looks more like <code>p50k_base</code> than <code>cl100k_base</code> in Figure <span class="math inline">\(\ref{fig:4digit_tokenization_split}\)</span>. This, along with Figure <span class="math inline">\(\ref{fig:claude_token_coverage}\)</span> above shows that Claude’s tokenizer exhibits a lack of systemacity when tokenizing numbers, suggesting the use of pure BPE number tokens, rather than something bespoke (as other current models use; see Table <span class="math inline">\(\ref{tab:tokenization_strategies}\)</span>).</figcaption>
</figure>

# Experiments with other system prompts

We also conducted our main experiment with an alternate, custom system prompt (as opposed to the default `’You are a helpful assistant.’`). The prompt we used was:

    You are MathGPT, an expert at solving math problems. When given a math problem, 
    you respond only by repeating the problem statement and appending the answer. You 
    do not say any other words. 

Results using this prompt are presented in Figure . We found it lead to small improvements in performance at low shot numbers (e.g., 1-shot) but these diminished at 8-shots. To maximize the reproducibility and applicability of our results, we decided to just use the default prompt. As we report 8-shot results throughout most of the paper, we doubt the system prompt would have a large effect on our results, given Figure .

<figure id="fig:mathgpt">
<span class="image placeholder" data-original-image-src="figures/r2l_v_l2r_w_shots_v_mathgpt.pdf" data-original-image-title="" width="50%"></span>
<figcaption>Comparison of R2L and L2R tokenization strategies for different numbers of shots and using two different system prompts.</figcaption>
</figure>

# Frequency effects

Given the findings of prior work on numerical reasoning demonstrating frequency effects , we also investigated whether or not our observed error patterns could be explained by term frequency. While we do not have access to the pre-training data of GPT-3.5 and GPT-4 models, we use the tokenizer merge ranks[^15] as a signal of term frequency. We analyze the expanded set of problems used for the error analysis in Section . Our results are summarized below:

**When making an error, GPT-3.5 is slightly more likely to output a more frequent token.** For each token in the model response on problems where it makes a mistake, we consider if the outputted incorrect token is more or less frequent (has lower or higher merge rank) than the correct one. Of the 25 errors made by the model when using R2L tokenization, 15 involve substituting in a more frequent token (60%, $p=0.115$ using a binomial null distribution assuming chance is 50%). Of the 425 errors made when using L2R tokenization, 238 involve substituting in a more frequent token (56%, $p=0.005$ using a binomial null distribution assuming chance is 50%). While we do see a significant effect in the L2R tokenization case, the margin is relatively small, which suggests that token frequency is not the dominant reason behind the error patterns.

**When using L2R tokenization in the length mismatch case, GPT-3.5 errors do not show strong correlation to token frequency.** In Section , we found that GPT-3.5 always gets the fourth digit wrong (Figure a). We then found correlation in the specific error in digit 4 to the magnitude difference between correct digit and digit in the model response (Figure b). Here, we ask if the substituted token 2 (whose first digit would be digit 4 of the response) is correlated to frequency in training data. Specifically, for each problem, we rank the tokens corresponding to the 10 possible “digit 4 mistakes” by merge rank. In Figure , we show the distribution of ranks across all 365 problems where the model makes “digit 4” errors. If models are preferentially substituting in more frequent tokens, we would expect to see a negative trend from the top left to the bottom right (as we did in Figure b). In Figure , we see a slight preference for outputting the most likely token (roughly 16% of the time, where chance would be 10%), but overall we see no clear trend.

<figure id="fig:digit4_tok_freq">
<span class="image placeholder" data-original-image-src="figures/len_mismatch_token2_substitution_rank.pdf" data-original-image-title="" width="60%"></span>
<figcaption>Distribution of relative rank of substituted incorrect token 2 in model response when using L2R tokenization in the length mismatch condition.</figcaption>
</figure>

**Off-by-one errors do not seem to be correlated to answer token frequency.** In Section , we found that the vast majority of remaining errors (for R2L tokenization, and for L2R tokenization in the length match condition) are off-by-one errors in the units digit of a token. Here, we ask if the specific substitution by the model is correlated to token frequency, measured by merge rank. Specifically, we condition on the model possibly making an off-by-one error, which means there are 3 possible output tokens (the correct token, the correct token minus one, the correct token plus one). We then rank these tokens based on merge rank, and see if the model preferentially picks the token with lowest merge rank. Of the 24 off-by-one errors when using R2L tokenization, we find the model only picks the “most frequent” token 7 times. Of the 53 off-by-one errors when using L2R tokenization, we find the model only picks the “most frequent” token 17 times. Both of these are essentially what we would expect by chance (one out of three), which suggests that output token frequency effects are not a dominant factor in why the model makes off-by-one errors.

Overall, we find mild to no evidence of token frequency effects in our experiments. This could be due to the presumably larger scale of GPT-3.5 (as compared to GPT-J, used by ). However, we note that our method of measuring token frequency is imperfect—relying on BPE merge ranks to signal frequency as we do not have access to pre-training data. Future work could study such associations further in newer, larger models with open pretraining data .

# Stereotyped patterns in model log probabilities

Mirroring the results of Section , we found stereotyped patterns in model log probabilities (“logprob”). Specifically, the OpenAI API returns the top 5 tokens at each position with their corresponding logprobs. We analyzed these log probabilities in three cases: L2R tokenization on length mismatch problems, L2R tokenization on length match problems, and R2L tokenization on all problems. These conditions mirror the most salient error effects we found in Section , with the former leading to “digit 4” errors, and the latter two leading to mostly off-by-one errors.

In addition to the raw logprobs, we computed an additional entropy metric (per output token) to measure model uncertainty in its output. Since access is restricted to the top 5 logprobs, we use the following lower bound, $H_{\text{lower}}$, to the true entropy: $$\begin{aligned}
H_{\text{true}} &\equiv - \sum_{i=1}^V p_i \log\left(p_i\right) \\
&= - \left(\sum_{i=1}^5 p_i \log\left(p_i\right) + \sum_{i=6}^V p_i \log\left(p_i\right) \right) \\
&\geq - \left(\sum_{i=1}^5 p_i \log\left(p_i\right) + \sum_{i=6}^V p_i \log\left(p_5\right) \right) \\
&= - \left(\sum_{i=1}^5 p_i \log\left(p_i\right) + \left(1-\sum_{i=1}^5 p_i\right) \cdot \log\left(p_5\right)\right) \\
&\equiv H_{\text{lower}},
\end{aligned}$$ where $p_i$ denotes the probability of the $i$-th most likely token. We use the natural logarithm for entropy, so all entropies are in nats (not bits).

For the “digit 4” error pattern (Section ), we find an interesting trend in model entropy. The entropy both on problems it gets incorrect (91.25%) and correct (8.25%) is roughly the same (2.066 and 2.061 respectively). Even when the model gets the question right, it’s unsure of its answer, suggesting that it might just be guessing a second output token with the right tens and ones digit and random hundreds digit. Providing further evidence for this mechanism, we observe that, of the problems where the model makes an error, about half (49.6%) of the time the correct answer appears in the top 5 output tokens. This is in line with what we would see for random guessing from the 10 tokens. That said, the model may exhibit some degree of bias towards the correct output, as evidenced by the downward trend in Figure b.

For the off-by-one error patterns (Section ), we observe a qualitatively different trend. Specifically, of the 53 off-by-one errors when using L2R tokenization on length match problems, in all cases we find that the second most likely token is the correct answer. We observe the same effect on the 25 off-by-one errors when using R2L tokenization. Furthermore, the entropy in both cases is around $0.45 \pm 0.05$, indicating that the model puts most of its weight on these top 2 most likely tokens. Unlike in the “digit 4” case, model entropy on correct problems is significantly lower (approximately 0.03, averaged across dataset and tokens) indicating that the model is “confidently correct” when using L2R tokenization on length match problems or R2L tokenization on all problems. Interestingly, on the subset of length match problems that the model answers correctly in both L2R and R2L tokenization, we found the model is slightly more confident when using R2L tokenization (which aligns with our intuition, as the model is also more often correct when using R2L tokenization)—see Figure .

<figure id="fig:correct_logprob_hist">
<p><span class="image placeholder" data-original-image-src="figures/correct_logprob_diff_across_tokenization.pdf" data-original-image-title="" width="60%">image</span> </p>
<figcaption>Histogram of difference in answer log probabilities (summed over tokens) on length match problems that the model answers correctly using both R2L and L2R tokenization. Black dotted line signifies 0. Red dotted line shows the average difference—on average, the model is more “confident” when using R2L tokenization.</figcaption>
</figure>

These results demonstrate that, depending on tokenization direction and alignment between input and output tokenization, we observe stereotyped patterns in model log probabilities. When using L2R tokenization on length mismatch problems, the model appears to make a magnitude-biased guess between all possible fourth digits (corresponding to 10 possible tokens[^16]). In the other cases, the model is mostly confidently correct. When it does make an error, it’s almost always an off-by-one error (Section ) where it’s uncertain between its chosen off-by-one incorrect answer and the true answer, but does not really consider other outputs beyond these two.[^17]

# Additional experimental details

All code and raw results can be found at <https://github.com/aadityasingh/TokenizationCounts>.

## Length control for error analysis

As described in Section , after noticing errors mostly come from the length mismatch condition in our original experiments (which used 90 problems, balanced by input digit length), we conducted a larger experiment where we controlled for input and output digit lengths. Specifically, we considered the following (addend1_length, addend2_length, answer_length) triplets: (7,7,7), (7,7,8), (8,7,8), (7,8,8), (8,7,9), (7,8,9), (8,8,8), (8,8,9), (9,7,9), (7,9,9), (9,8,9), (8,9,9), (9,9,9). Problems in each condition were sampled randomly so as to satisfy the digit length constraints for each triplet. We sampled 100 problems for each triplet, for a total of 1300 problems.

## Access dates

Given the changing nature of the OpenAI API, we report access dates for all experiments below. We tried to use the supposed “fixed” models for all experiments, but did notice some non-determinism, even at temperature 0—an issue that may be due to non-determinism in floating point arithmetic. We also note that the `gpt-4-0314` appears to have been early-deprecated, as we can no longer access it despite the supposed June 13, 2024 deprecation date on <https://platform.openai.com/docs/deprecations>.

Access dates by figure in main text:

- `gpt-3.5-turbo-0301`, Figure : April 7, 2023

- `gpt-3.5-turbo-0301`, Figure : May 18, 2023

- `gpt-3.5-turbo-0301`, Figure  left two columns: May 18, 2023

- `gpt-3.5-turbo-0301`, Figure  right two columns: April 7, 2023

- `gpt-3.5-turbo-0301`, Figure -: January 25, 2024

- `gpt-3.5-turbo-0301`, Figure - May 24, 2024

- `gpt-4-0314`, Figure : May 2, 2023

- `gpt-3.5-turbo-0613`, Figure -: January 25, 2024

- `gpt-3.5-turbo-1106`, Figure -: January 29, 2024

- `gpt-4-0613`, Figure -: January 25, 2024

- `gpt-4-1106-preview`, Figure -: January 29, 2024

## Example prompts

In this section, we provide example prompts we used for various experiments. For simplicity, we use the same query for each prompt shown below, and we only use 2 shots (most experiments in the main text are done with 8 shots). In practice, we sampled shots randomly (controlling for digit length to match the query length) for each query, as explained in Section . For the experiments described in Section  and Appendix , the shots were also controlled to have the same answer length as the query. The examples we present below, though, are for the runs in the rest of the paper (where only input digit lengths are controlled). For maximum clarity, we display prompts as the list of dictionaries that gets sent to OpenAI’s API and roughly in the order used for figures in the paper. Following the advice at `https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples`, we make use of the multi-turn chat dialog to prompt the model, as opposed to one big user message with all the examples.

<figure>
<p>L2R tokenization, input-digit-controlled for two 7-digit numbers:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]</code></pre>
</figure>

<figure>
<p>R2L tokenization, input-digit-controlled for two 7-digit numbers:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3,790,206+6,739,555=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10,529,761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6,777,159+7,096,168=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13,873,327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8,302,080+3,529,456=&#39;}]</code></pre>
</figure>

<figure>
<p>R2L tokenization, delimiter-control condition using <code>’#’</code>:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3#790#206+6#739#555=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10#529#761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6#777#159+7#096#168=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13#873#327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8#302#080+3#529#456=&#39;}]</code></pre>
</figure>

<figure>
<p>L2R tokenization, thinking token control by using separators in L2R tokenization:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;379,020,6+673,955,5=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;105,297,61&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;677,715,9+709,616,8=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;138,733,27&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;830,208,0+352,945,6=&#39;}]</code></pre>
</figure>

<figure>
<p>L2R tokenization, thinking token control by using 2 extra spaces:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206  +  6739555  =  &#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159  +  7096168  =  &#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080  +  3529456  =  &#39;}]</code></pre>
</figure>

<figure>
<p>L2R tokenization, thinking token control by using 2 extra spaces:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206  +  6739555  =  &#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159  +  7096168  =  &#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080  +  3529456  =  &#39;}]</code></pre>
</figure>

<figure>
<p>Repeat L2R <span class="math inline">\(\rightarrow\)</span> R2L:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;3,790,206+6,739,555=10,529,761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;6,777,159+7,096,168=13,873,327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]</code></pre>
</figure>

<figure>
<p>Repeat control L2R <span class="math inline">\(\rightarrow\)</span> L2R:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;3790206+6739555=10529761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;6777159+7096168=13873327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]</code></pre>
</figure>

<figure>
<p>Output control L2R <span class="math inline">\(\rightarrow\)</span> R2L:</p>
<pre><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10,529,761&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;}, 
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13,873,327&#39;}, 
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]</code></pre>
</figure>

[^1]: More precisely, single bytes are used to handle multilinguality, but this level of description suffices for our needs.

[^2]: A noteable exception are Anthropic’s Claude models, which still use pure BPE number tokens (see Appendix ).

[^3]: Verified by inspecting the tokens from huggingface <https://huggingface.co/docs/transformers/main/en/model_doc/mistral>.

[^4]: This tokenization is enforced by the cryptic `pat_str` parameter in their tokenization library, <https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py> Line 76.

[^5]: <https://platform.openai.com/>

[^6]: We did experiment with other system prompts and found minimal differences (Appendix ).

[^7]: Specifically, the token count in the 8-shot prompt increases from 195 to 213 to 240 when going from 0 to 1 to 2 spaces. For reference, the R2L token count is 247.

[^8]: Since we didn’t explicitly control for carries when generating problems, the number of problems with a given number of carries varies. We only considered cases where we had at least 50 problems.

[^9]: This error pattern is not present when using R2L tokenization.

[^10]: Specifically, all 24 off-by-one errors in the R2L case, and 51 of the 53 off-by-one errors in the L2R case.

[^11]: We assume this is a smaller, maybe distilled, version of GPT-4.

[^12]: We find it interesting that the March to June update to GPT-4 improved performance, but the corresponding update to GPT-3.5 did not – without knowing what these updates entail, however, it’s hard to draw conclusions as to why this may be the case.

[^13]: We also considered frequency effects by utilizing BPE merge ranks (given that we do not have access to the pre-training data of GPT-3.5 and GPT-4 models) as an approximate for frequency. We didn’t find a strong effect, expanded results are provided in Appendix .

[^14]: 3-digit number tokens also reduce inference-time compute when models use numbers in their output, which could be an important consideration when serving models at scale.

[^15]: Recall that tokens are created roughly in order of decreasing frequency in the corpus used to train the tokenizer.

[^16]: A completely random guess over 10 tokens would correspond to an entropy of about 2.3, which is in line with the lower bound observed (of about 2.06) and the finding of a slight mangitude bias (which would decrease the entropy from 2.3).

[^17]: A completely random guess over 2 tokens would correspond to an entropy of about 0.69, which is in line with the lower bound observed (of about 0.45).
