{
  "arxivId": "2402.14903",
  "title": "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier LLMs",
  "authors": "Aaditya K. Singh, DJ Strouse",
  "abstract": "Tokenization, the division of input text into input tokens, is an often\noverlooked aspect of the large language model (LLM) pipeline and could be the\nsource of useful or harmful inductive biases. Historically, LLMs have relied on\nbyte pair encoding, without care to specific input domains. With the increased\nuse of LLMs for reasoning, various number-specific tokenization schemes have\nbeen adopted, with popular models like LLaMa and PaLM opting for single-digit\ntokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and\n3-digit numbers. In this work, we study the effect this choice has on numerical\nreasoning through the use of arithmetic tasks. We consider left-to-right and\nright-to-left tokenization for GPT-3.5 and -4, finding that right-to-left\ntokenization (enforced by comma separating numbers at inference time) leads to\nlargely improved performance. Furthermore, we find that model errors when using\nstandard left-to-right tokenization follow stereotyped error patterns,\nsuggesting that model computations are systematic rather than approximate. We\nshow that the model is able to convert between tokenizations easily, thus\nallowing chain-of-thought-inspired approaches to recover performance on\nleft-to-right tokenized inputs. We also find the gap between tokenization\ndirections decreases when models are scaled, possibly indicating that larger\nmodels are better able to override this tokenization-dependent inductive bias.\nIn summary, our work performs the first study of how number tokenization\nchoices lead to differences in model performance on arithmetic tasks,\naccompanied by a thorough analysis of error patterns. We hope this work\ninspires practitioners to more carefully ablate number tokenization-related\nchoices when working towards general models of numerical reasoning.",
  "url": "https://arxiv.org/abs/2402.14903",
  "issue_number": 78,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/78",
  "created_at": "2024-12-25T08:53:17.551409",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-19T22:43:04.367Z"
}