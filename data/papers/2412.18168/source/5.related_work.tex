\section{Related Work}
Pairwise loss functions lie in the core of existing implicit CF. Since they aim to maximize the posterior probability $\prod_ {(u, i_p, i_n) \in D}p(\Theta \mid i_p>_{u}i_n)$, there have been two major research directions. The first line focuses on how to effectively form pairwise relationships (i.e., $>_{u}$). BPR~\cite{RFG12, SGZ20,YYG21} assumes that an interacted item is more similar to the user than a randomly sampled uninteracted item. CML~\cite{HYC17} encodes not only users' preferences but also the user-user and item-item similarity. SimpleX~\cite{MZW21} introduces massive negative samples and removes uninformative items. UIB~\cite{ZZY22} introduces a user interest boundary to penalize samples crossing a threshold. The second line of research aims to find better negative items to contrast positive items. PNN~\cite{ZCH24} introduces a neutral class and a novel positive-neutral-negative learning paradigm. The state-of-the-art focuses on identifying hard negative samples that help tighten a model's decision boundary. DNS~\cite{ZCW13} selects negative items that are more similar to a user. IRGAN~\cite{WYZ17} utilizes a generative adversarial network to compute the probabilities of negative samples by a min-max game. ReinforcedNS~\cite{DQH19} uses reinforcement learning to guide the sampling process. MixGCF~\cite{HDD21} integrates information from a graph structure and positive samples to enhance the hardness of negative items. GDNS~\cite{ZZH22} develops a gain-aware function to calculate the probability of an item being a real negative sample. SRNS~\cite{DQY20} favors low-variance samples to avoid false negative samples. DENS~\cite{LCZ23} disentangles relevant and irrelevant factors of samples to select appropriate negative samples. ANS~\cite{ZCL23} proposes to generate synthetic negative samples to improve performance. 
