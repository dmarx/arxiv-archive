\section{Methodology}
In this section, we introduce a concrete implementation of the PRP. Note that these steps can be implemented by different methods and thus the overall paradigm is generic. The implementation hinges on two fundamental components: a ranker for generating pseudo-rankings and a ranking loss function tailored to effectively leverage the ranking information.

\subsection{Ranker}
The absence of ranking information in the dataset presents a significant challenge. To address this, we introduce a ranker module capable of generating accurate pseudo-rankings to facilitate training. The items to be ranked are represented as $M = \{i_1, i_2, i_3, \dots, i_k\}$, which are randomly sampled from the dataset. The ranker aims to learn a user-specific mapping $\gamma_{u}: M \rightarrow \pi_{u}$, where $\pi_{u}$ denotes the ranking result. Specifically, $\pi_{u}(v)$ represents the item at position $v$ in the ranking, and $\gamma_{u}(i)$ denotes the position of item $i$ in $\pi_{u}$. This mapping is required to satisfy the condition: $\forall i_v, i_w \in M, \text{ if } \gamma_{u}(i_v) < \gamma_{u}(i_w), \text{ then } i_v >_u i_w \text{ and } s_u(i_v) > s_u(i_w)$. We also impose an additional constraint aligned with a common assumption in CF: interacted items should receive higher scores than non-interacted ones~\cite{PC13,RFG12, MZW21}.

To realize this objective, an intuitive approach is to design a neural network that takes user $u$ and set $M$ as input, outputting scores for all items to enable ranking:
\begin{equation}
\begin{aligned}
s_u(i_1),s_u(i_2),\ldots, s_u(i_k) &= Rank_{u}(\mathbf{e}_{u},\mathbf{e}_{1},\mathbf{e}_{2},...,\mathbf{e}_{k}), \\
\pi_{u} = argsort(s_u(i_1),&s_u(i_2),\ldots, s_u(i_k)).
\label{eq:Prank}
\end{aligned}
\end{equation}
The $Rank_{u}$ module can employ any network architecture. However, for efficiency, we use a simple multi-layer perceptron (MLP) in this work. Despite its simplicity, this approach lacks a specific training objective, making it insufficient to guarantee the acquisition of reliable ranking knowledge. Consequently, there is no direct assurance that the network's output will correspond to the desired ranking outcome. Therefore, an additional training objective is required to guide the network's learning process. However, the lack of ranking information in the dataset creates a classic chicken-and-egg dilemma.

To circumvent this issue, we propose a noise injection mechanism to construct ranking signals based on interacted items, denoted as $\mathbf{e}_{p}^1$. The core intuition is that injecting a small amount of noise into an item $\mathbf{e}_{p}$ produces a new item $\mathbf{e}_{p}^2$ with slightly altered semantics, reflecting a slight decrease in user preference. Given the substantial noise magnitude, resulting in $\mathbf{e}_{p}^3$, the item's semantics degrade significantly, leading to a further reduction in user preference~\cite{HHD18,ZPL23}. We formalize this by applying two different magnitudes of noise to the original item $\mathbf{e}_{p}$, generating a set $\mathcal{P}_{n}$:

\begin{equation}
\begin{split}
\begin{aligned}
&\mathcal{P}_{n} = \{ i_{p}^{m} | \mathbf{e}_{p}^{m}=\mathcal{T}_{\theta_m}(\mathbf{e}_{p}),m=1,2,3\},\\
&\mathcal{T}_{\theta_m}(\mathbf{e}_{p}) = \mathbf{e}_{p} + \theta_{m} \epsilon_{u}, \\
% &\forall \theta_{v} < \theta_{w}, \; \gamma_u(i_p^{v})<\gamma_{u}(i_p^{w}).
\end{aligned}
\end{split}
\end{equation}
$\mathcal{T}_{\theta_m}$ represents the noise injection function, $\epsilon_{u}$ denotes the noise, and $\theta_m$ denotes the noise magnitude. We set $\theta_1 = 0, \theta_3 \gg \theta_2$. While random noise injection is a straightforward idea, it is unreliable. If a user’s preferences are robust, significant noise is necessary to alter their ranking preferences. Consequently, the noise distribution should be user-dependent. Assuming no additional prior knowledge, we model the noise as following a Gaussian distribution, with the parameters generated as follows:
\begin{align}
\mu_u &= MLP_1(\mathbf{e}_{u}),\\
\log \sigma_u^2 &= MLP_2(\mu_i).
\end{align}
We opt to fit $\log \sigma_u^2$ instead of $\sigma_u^2$ directly due to the non-negativity constraint on $\sigma_u^2$, which would otherwise require an activation function~\cite{YZL23}. Once we obtain the mean $\mu_u$ and variance $\sigma_u^2$, the latent noise $\epsilon_{u}$ is generated by sampling from $\mathcal{N}(\mu_u, \sigma_u^2)$. Direct optimization of this process is infeasible due to its non-differentiable nature. To address this, we employ the reparameterization trick~\cite{CLJH22}:
\begin{align}
\epsilon_{u} &= \mu_u + \sigma_u \cdot \eta,\\
\eta &\sim \mathcal{N}(0, \mathbf{I}),
\end{align}
where $\eta \sim \mathcal{N}(0, \mathbf{I})$, with $\mathbf{I}$ as the identity matrix, is Gaussian noise. The resulting ranking is then processed by our ranking loss function, denoted as $\mathcal{L}_{p}$, to supervise the ranker’s training:
\begin{equation}
\begin{split}
\begin{aligned}
\mathcal{L}_{p} = \mathcal{L}_{rank}^{\alpha}(u,\gamma_{u}(\mathcal{P}_{n})).
\end{aligned}
\end{split}
\end{equation}

\subsection{Ranking Loss Function}
With the ranker generating pseudo-ranking data, the challenge shifts to designing an appropriate loss function. Existing loss functions, typically following a pairwise paradigm, are inadequate for capturing complex ranking information. Our ideal CF theory suggests that recommendation tasks can be viewed as multiple ordinal classification problems. While a classification loss might seem applicable, the absence of explicit labels such as $(top1, top2, top3, \ldots, topN)$ renders this approach unfeasible. Nevertheless, the classification perspective inspires the design of our loss function. In classification tasks, the classical objective is to output scores such that the score for the target class exceeds that for non-target classes:
\begin{equation}
\max \sum_{v=1}^{k} \sum_{w=1, w\neq v}^{k} \max(z(v) - z(w) , 0).
\end{equation}
This formulation, however, only addresses the classification task and fails to account for the ordinal nature of ranking. Moreover, without label information, computing $z(\cdot)$ is impractical. To incorporate ordinal properties, we reformulate the problem to ensure that $\forall v < w, s_u(\pi_{u}(v)) > s_u(\pi_{u}(w))$:
\begin{align}
\mathcal{L}_{rank} &= \sum_{v=1}^{k} \sum_{w=v+1}^{k} \max(s_u(\pi_{u}(w)) - s_u(\pi_{u}(v)), 0)
\label{eq:rank_all}
\\
&= \sum_{v=1}^{k} \max(\max_{w \neq v}\{s_u(\pi_{u}(w))\} - s_u(\pi_{u}(v)), 0).
\label{eq:rank_max}
\end{align}
Equation~\ref{eq:rank_all} effectively decomposes the ranking problem into multiple sub-ranking problems. For example, if the ranking is $(i_3, i_1, i_2)$, the scores should satisfy the sub-rankings $s_u(i_3) > s_u(i_1) > s_u(i_2)$ and $s_u(i_1) > s_u(i_2)$. By exploiting the principle of transitivity, we establish that if the conditions $s_u(i_3) > s_u(i_1)$ and $s_u(i_1) > s_u(i_2)$ hold, we can directly infer the ordering $s_u(i_3) > s_u(i_1) > s_u(i_2)$, which ultimately leads to Equation~\ref{eq:rank_max}. However, the non-differentiability of the max function renders it unsuitable as a loss function, necessitating further modification:
\begin{align}
\mathcal{L}_{rank} &\approx \sum_{v=1}^{k} \log(1+\exp(\max_{w \neq v}\{s_u(\pi_{u}(w))\} - s_u(\pi_{u}(v))) \\ 
&\approx \sum_{v=1}^{k-1} \log(1+\exp(s_u(\pi_{u}(v+1)) - s_u(\pi_{u}(v))).
\end{align}
In the derivation above, we first replace the non-differentiable $max(\cdot,0)$ with $\operatorname{softplus}$ function $log(1+\exp(\cdot))$. Thanks to the ranker's result, we can then simply use $v+1$ to represent $\max_{w \neq v}\{s_u(\pi_{u}(w))\}$. This substitution eliminates the need for significant additional computational effort to find $\max_{w \neq v}\{s_u(\pi_{u}(w))\}$, thereby significantly enhancing the algorithm's efficiency. For instance, in the ranking $(i_3, i_1, i_2)$ ensuring $s_u(i_3) > s_u(i_1)$ and $s_u(i_1) > s_u(i_2)$ naturally constructs $s_u(i_3) > s_u(i_1) > s_u(i_2)$.

Given that the pseudo-ranking from the ranker may not always be accurate, we introduce a confidence mechanism. Specifically, we introduce a confidence coefficient $\alpha_v$. If a ranking is deemed inaccurate, we reduce its corresponding coefficient to lessen its impact on the overall loss:
\begin{align}
\mathcal{L}_{rank}^{\alpha} = \sum_{v=1}^{k-1} \alpha_{v} \log(1+\exp(s_u(\pi_{u}(v+1)) - s_u(\pi_{u}(v))).
\end{align}
An intuitive approach to designing $\alpha_{v}$ would be to set it as a hyperparameter, but this would require extensive expert knowledge and dataset-specific tuning. Inspired by previous works~\cite{GDH22,LLW19,WWS22}, we propose learning $\alpha_v$ based on gradient. If a sub-ranking produces a gradient that deviates from typical training values, it is likely incorrect, and we accordingly reduce its weight. First, we calculate the absolute gradient value $g_{v} = |\nabla_{\pi_{u}(v)} \mathcal{L}_{rank}|$ for all sub-rankings and identify the largest gradient value $max(g_{v})$. We then divide the gradients into ten groups ${G_1,G_2, \ldots, G_{10}}$ according to the interval  $[0, max(g_{v})]$. For a particular gradient $g_v$, if it belongs to group $G_v$, we calculate the number $N_{v}$ of gradients in that group and the ratio of the sum of gradients across all groups:
\begin{align}
\alpha_{v} = \frac{N_v}{\sum_{y=0}^{10}N_y}.
\end{align}
This approach ensures that if the number of gradients in a group is small, indicating that they are outliers, $\alpha_{v}$ is reduced, aligning with our objective. This concept can be interpreted as a form of statistical gradient density. Given that the gradient is inherently a continuous quantity, direct calculation of the density might yield $1/N$ for each gradient density. To address this, we approximate the density by grouping the gradients. The final loss function is thus formulated as follows:
\begin{align}
\mathcal{L} = \mathcal{L}_{rank}^{\alpha}(u,\pi_u) + \beta \mathcal{L}_p,
\end{align}
where $\mathcal{L}_{rank}(u,\pi_u)$ processes the ranking information from the ranker. $\mathcal{L}_p$ aids the ranker in learning ranking knowledge. $\beta$ is a hyperparameter to control the relative weights.

\subsection{Discussion}
Our newly proposed loss function offers a more effective approximation of the ideal CF. This prompts further reflection: \textit{is there an intuitive connection between our loss function and the classic loss?}
\begin{theorem}
\label{th:3}
BPR is a special case of ranking loss where $k=2$. BPR with hard negative sampling is a special case of longest sub-ranking loss where $k>2$. 
% Softmax cross-entropy loss is a special case of longest sub-ranking loss where $k>2$ and use $\operatorname{LogSumExp}$ to approximate the $\max$ function.
\end{theorem}
\begin{proof}
\begin{align}
\mathcal{L}_{rank} &= -\sum_{v=1}^{k-1} \log\left(\frac{1}{\exp(s_u(\pi_{u}(v)) - s_u(\pi_{u}(v+1))) + 1}\right)\\
&= -\sum_{v=1}^{k-1} \log(\sigma(s_u(\pi_{u}(v)) - s_u(\pi_{u}(v+1)))) \\
&= -\sum_{v=1}^{k-1} \log(\sigma(s_u(\pi_{u}(v)) - \max_{w \neq v}\{s_u(\pi_{u}(w))\})).
\end{align}
It shows that if we set $k=2$, our method is equivalent to BPR, where $\pi_{u}(v)$ is a positive sample, and $\max_{w \neq v}\{s_u(\pi_{u}(w))\}$ is equal to a negative sample. When $k>2$, obviously, BPR only considers a certain longest sub-ranking, where $\pi_{u}(v)$ is still a positive sample, $\max_{w \neq v}\{s_u(\pi_{u}(w))\}$ is equivalent to performing \textbf{hard negative sampling} from $M - \{\pi_{u}(v)\}$ and selecting the hardest sample. 
% Due to space constraints, the derivation of the softmax cross-entropy loss can be found in Appendix.

\end{proof}
