\section{Introduction}
Collaborative filtering (CF) has been one of the most fundamental techniques in recommender systems due to its simplicity and effectiveness~\cite{MZW21}. It leverages user-item interactions to learn user preferences and make top-K recommendations~\cite{HLZ17,WHW19,HDW20}. To find the top-K items, a CF model normally needs to generate a full ranking over the entire item universe. To fulfill this task, ideally, given a set of users $U$, one should derive a CF model that maximizes the posterior probability $\prod_ {u \in U}p(\Theta \mid \Psi_u)$, where $\Theta$ denotes the set of the model's parameters that need to learn, and $\Psi_u$ is user $u \in U$'s full ranking of all items. However, this ideal optimization goal is unattainable in real-world scenarios, as it is impractical for users to fully rank a vast number of items.

In practice, most CF methods~\cite{HDW20,WWF21,YYX22} rely on pairwise loss functions to approximate full rankings. We refer to these as pairwise CF. Its core idea is to make positive item $i_p$ (i.e., an item interacted by a user) more similar to user $u$ than negative item $i_n$. For example, Bayesian personalized ranking (BPR)~\cite{RFG12} pairs each positive item with a negative item to form a pairwise relationship for training. Formally, a pairwise loss function forms a pairwise relationship $i_p >_{u} i_n$ (i.e., $u$ prefers item $i_p$ to $i_n$) as the ground truth. In this context, the optimization objective is to maximize the posterior probability $\prod_ {(u, i_p, i_n) \in D}p(\Theta \mid i_p>_{u} i_n)$ based on a set of pairwise relationships, where $D$ is the training dataset. Despite the strong performance of pairwise CF models, approximating full rankings with pairwise relationships introduces a significant gap compared to the ideal CF model. In this paper, we provide a novel analysis using the concept of multiple ordinal classification to \textit{reveal the inherent gap between the pairwise approximation and the ideal case}. However, bridging the gap in practice encounters two formidable challenges:
\begin{itemize}
    \item \textbf{None of the real-world datasets contains full ranking information}: Obtaining full rankings requires extensive user participation, a feat that is unachievable in most practical scenarios.
    \item \textbf{Lack of a loss function that is capable of handling ranking information}: Current CF loss functions focus on non-ranking relationships, such as pairwise or listwise~\cite{XLW08}, and are not designed to effectively and efficiently process ranking data.
\end{itemize}
To overcome the aforementioned challenges, we introduce a novel pseudo-ranking paradigm (PRP). For the first challenge, we propose a ranker module to generate a pseudo-ranking of given items. However, training the ranker demands supervised signal (i.e. ranking information), presenting a classic chicken-and-egg dilemma. We innovatively propose a noise injection mechanism to add different levels of noise to positive samples to construct a ranking in line with user preferences. The intuition is that when the injection noise is very small, the semantics of the constructed $i_p^2$ will not change significantly. When the injection noise is large, the semantics of the positive item $i_p^3$ will be seriously broken~\cite{HHD18,ZPL23}, and users are difficult to love the item. Accordingly, $i_p >_u i_p^2 >_u i_p^3$ is constructed as a supervision. For the second challenge, inspired by the multiple ordinal classification concept, we then put forward a ranking loss function grounded in classification principles. This function ensures that an item's score is proportional to its ranking order: the higher the rank, the higher the score. To further ensure our method's robustness against potential inaccuracies in pseudo-rankings provided by the ranker, we equip the ranking loss function with an original gradient-based confidence mechanism, which detects outliers by assessing gradient density. By reducing the weights of outliers, the impact of inaccurate ranking information on training can be alleviated. We summarize our main contributions as follows:
\begin{itemize}
    \item To the best of our knowledge, we are the first to use the multiple ordinal classification concept to approximate ideal CF rather than traditional pairwise loss. We provide a new direction for future development of loss functions.
    \item We propose a novel ranker, which can generate pseudo-rankers according to user preferences through a noise injection mechanism, effectively solving the problem that the existing dataset does not contain ranking information.
    \item We propose a novel loss fucnion, which can directly deal with ranking data that traditional loss fucnions cannot directly deal with, and a gradient-based confidence mechanism, which can judge outliers by gradients and alleviate inaccurate ranking information.
    \item We conduct extensive experiments on four real-world datasets that confirm that PRP can achieve significant improvements over representative state-of-the-art methods. Moreover, when combined with PRP, a wide variety of mainstream CF models can consistently and substantially boost their performance. 
\end{itemize}
