\section{What are models converging to?}
%In this section, we first present our main argument that representations converge to a statistical model of events that generate the data we observe. Then, we highlight three different mechanisms in modern machine learning that play a role in this convergence.

By now we hope to have convinced the reader that representations are converging, and that task and data pressures, combined with increasing model capacity, can help explain why. We next turn our attention to \textit{what} exactly is the endpoint of all this convergence. We only argue that representations are converging up to their kernels. Our hypothesis in this section is that \textit{the kernel we are converging toward is one defined by coccurrence probability}. For example, the following statements are consistent with this general hypothesis: the distance, in representation space, between two images is proportional to the rate at which those iamges cooccurrence in videos; the distance between two word embeddings is proportional to the rate at which \jh{unfinished setence.}

This part of our hypothesis is the most speculative. Like any good hypothesis, we do not aim for the precise truth, in all its gory detail, but rather an approximation to the truth that is directionally correct yet simple enough to be useful. Future work can certainly refine the arguments we give below.

%\paragraph{Preliminaries}

% We use terms as follows:
% \begin{itemize}
%     \item A \textbf{statistical model} is a probability distribution $P: \mathcal{X} \rightarrow \mathbb{R}_+$, that maps observations in some arbitrary domain $\mathcal{X}$ to nonnegative real numbers, and is normalized.
%     \item A \textbf{projection} is any mapping $f: \mathcal{X} \rightarrow \mathcal{Y}$.
%     \item A \textbf{platonic event} $Z$ is an indicator variable that can take on any value in a potentially infinite set, and that indexes values in each observable domain $X$, $Y$, etc. That is, there is a mapping $f_{X}: \mathcal{Z} \rightarrow \mathcal{X}$ for each domain where measurements can be made. In the mathematical analysis below, we will restrict our attention to the special case where $\mathcal{Z}$ is finite and all the $f$'s are bijective.
% \end{itemize}

% It is easiest to think about the scenario where there are physical events $Z$ and measurements of these events $X$ and $Y$. But our argument doesn't rely on $Z$ having any priority over $X$ and $Y$. $Z$ need not be any more real or fundamental. Indeed, Plato rejected the idea that there is a physics that is more real than our measurements. Instead, we will simply argue that all models converge to the same distribution $P(Z)$, where $Z$ indexes events.

% An outline of our argument is as follows:
% \begin{enumerate}
%     \item We claim first that common representation learning objectives are benefitted by a \textit{statistical model} over their domain of observations (i.e. $P(\cdot)$).
%     \item We conjecture that due to this fact, these algorithms learn internal representations of $P(\cdot)$
%     \item Certain models factor $P(Z) = P(Z_2 | Z_1) P(Z_1)$. These models involve bivariate functions $K: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}_+$.
%     \item This bivariate function has the same form as a kernel. It is the kernel for some representation $f$. 
%     \item Under ideal conditions, the kernel is the same between all views.
% \end{enumerate}

% Assuming discrete events and bijective projections, $P_X$ is a finite length vector $\mathbf{p}_x$ and $P_Y$ is a finite length vector $\mathbf{p}_y$ and $p_x = Ap_y$, where $A$ is some permutation matrix.

% \subsection{Convergence Toward Platonic Reality}

% We present now a more specific form of the general argument. This specific form focuses on the case where we are modeling the cooccurrence of two events, $a$ and $b$. We do not claim that this specific case characterizes all aspects of representation convergence. We suspect there is a broader framework in which representations are converging to a common statistical model of more general form. However, we focus here on the cooccurrence case as something more concrete we can presently sink our teeth into.

% First, we will construct an idealized scenario where our argument holds. Then, we will argue that real algorithms are not so far off from this scenario.

The outline of our argument is as follows:
\begin{enumerate}
    \item First we define a family of representation learners, in an idealized world, that result in representational convergence.
    \item Then we argue that this world and this family of learners are reflective of real representation learning problems.
    \item We demonstrate that these results hold in practice on a simple color modeling problem, \textcolor{red}{and also on language modeling}.
    \item Finally we point out limitations of this model.
\end{enumerate}

\subsection{An idealized model of the world}
We consider a world that works as follows. The world consists of a sequence of events, $z_1, \ldots, z_T$. Each event can be observed in various ways. An observation is some function $\texttt{obs}: \mathcal{Z} \rightarrow \cdot$ that maps from event to an arbitrary measurement space, such as pixels, sounds, mass, force, torque, words, paragraphs, etc.

You can think of an event as corresponding to the state of the world at some point in time, but it is also fine to just consider an event as any variable that indexes observations, with no further physical meaning. Specifically, the role of events is just to define a precise notion of \textit{cooccurrence}: two observations $x$ and $y$ cooccur if and only if $x = \texttt{obs}_X(z)$ and $y = \texttt{obs}_Y(z)$ for some event $z$. Then we can define the joint probability over these observations as:
\begin{align}
    P(x,y) %&\triangleq \frac{1}{T} \sum_{z \in \mathbf{Z}} [\texttt{cooccur}(x,y,z)]\\
           &\triangleq \frac{1}{T} \sum_{z \in \mathbf{Z}} [\mathds{1}(x = \texttt{obs}_X(z)) \\ & \quad\quad\quad\quad\quad\text{ and } \mathds{1}(y = \texttt{obs}_Y(z))]
\end{align}
We consider here a world of countably many discrete events; extensions to continuous and unbounded worlds are left to future work. The set of all events in our world is labeled as $\mathbf{Z} \triangleq \{z_1, \ldots, z_T\}$.

Observations are considered to be stochastic, where $P_\texttt{obs}(X | Z = z)$ is the distribution over $x$-value you will get when you observe the event $z$. The observation function $\texttt{obs}_X(z)$ simply draws a sample from $P_\texttt{obs}(X | Z = z)$.

We can also take multiple measurements of the same event using the same observation function. In this case, we will index the distinct observations with a superscript, like so:
\begin{align}
    x^1, \ldots, x^n \stackrel{\text{iid}}{\sim} P_\texttt{obs}(X | Z = z)
\end{align}
This allows us to define a joint distribution $P(x^1,x^2)$ over two iid observations of the same event.% (note that this is just a special case of $P(x,y)$, where $x$ and $y$ have matching marginals).

Figure \ref{XX} depicts the graphical model for our idealized world.

\subsection{A family of learners that exhibit convergence}
Consider a learner whose objective is to model cooccurrences, in the idealized world we have defined. The learner fits a model $p_{\theta}$ to data sampled from $P_{\texttt{obs}}(X | Z=z)$. 
%Without loss of generality, we represent $p_{\theta}$ as follows:
%\begin{align}
%    p_{\theta}(x^1,x^2) = g(f(x^1), f(x^2))
%\end{align}

We choose the following form for our model:
\begin{align}
    p_{\theta}(x^1,x^2) = e^{-\langle f_X(x^1), f_X(x^2) \rangle} \label{eqn:coccurrence_model}
\end{align}
%We call this a \textit{platonic learner} because, as we will next show, it converges to a platonic representation in our idealized world, in particular, a representation whose kernel is invariant to observation modality.

We will show that this model will converge to a particular kernel, in the limit of infinite model parameters and training data points, over the large family of model architectures and observation modalities.

\paragraph{Invariance to model architecture} MLPs, transformers, CNNs, etc are all universal approximators. Therefore they will converge to the same function in the limit of infinite width/depth and infinite data, if they share the same objective. Therefore, if trained well enough and sufficiently high capacity, will obtain $p_{\theta}(x^1,x^2) = P(x^1, x^2)$ and that if we train the same on $\texttt{obs}_Y$, we have $p_{\theta}(y^1,y^2) = P(y^1, y^2)$
\jh{is this paragraph needed?}

\paragraph{Invariance to observation modality}
Let us assume we have achieved $p_{\theta}(x^1,x^2) = P(x^1, x^2)$ and $p_{\theta}(y^1,y^2) = P(y^1, y^2)$. 

Define:
\begin{align}
    x(\epsilon) = \texttt{obs}_X(z, \epsilon)
\end{align}


\begin{align}
    P(x^1,x^2) &= \frac{1}{T} \sum_{z \in \mathbf{Z}} \mathbb{E}_{\epsilon^1, \epsilon^2}[\mathds{1}(x^1 = \texttt{obs}_X(z, \epsilon^1)) \\ & \quad\quad\quad\quad\quad\text{ and } \mathds{1}(x^2 = \texttt{obs}_X(z, \epsilon^2))]\\
\end{align}

The following is a bijection:
\begin{align}
    [x^1,x^2] &\triangleq \texttt{obs}(z, \epsilon^1, \epsilon^2)\\
    &\triangleq [\texttt{obs}_X(z, \epsilon^1)), \texttt{obs}_X(z, \epsilon^2))]
\end{align}
Therefore, 
\begin{align}
    P(x^1,x^2) = P(z, \epsilon^1, \epsilon^2)
\end{align}

\begin{align}
    p_{\theta}(x^1,x^2) = p_{\theta}(y^1,y^2) \label{eqn:matching_cooccurrences}
\end{align}
This implies that the kernels are the same. Plugging Eqn. \ref{eqn:matching_cooccurrences} into Eqn. \ref{eqn:coccurrence_model}, we have,
\begin{align}
    e^{-\langle f_X(x^1), f_X(x^2) \rangle} = e^{-\langle f_X(x^1), f_X(x^2) \rangle}
\end{align}
Then, by bijectivity of exponential function,
\begin{align}
    \langle f_X(x^1), f_X(x^2) \rangle = \langle f_Y(y^1), f_Y(y^2) \rangle
\end{align}
Then, by our definition of kernels, 
\begin{align}
    K_X(x^1, x^2) = K_Y(y^1,y^2)
\end{align}
All these arguments hold for any modality $X$ and any other modality $Y$. Therefore, we observe representational convergence to a common kernel across any two modalities in our world. We will call this kernel the \textit{cooccurrence kernel}. It is one concrete form of a platonic kernel, i.e. a kernel that a broad family of learners will converge to (the family we have defined above).

\subsection{Do real learners converge to the cooccurrence kernel?}
Is this idealized setting reflective of real learning problems? 

Many representation learners do indeed model cooccurrences, in one way or another. For example, contrastive learners try to predict whether or not two observations cooccur in the same context, where, for example, the observations might be image patches and the context is an image~\cite{SimCLR}, or the observations might be sentences and the context is a paragraph~\cite{SimSCE}. Other learners work by predicting the next observation in a sequence given previous observations, aiming to model $P(x^2 | x^1)$ where $[x^1, x^2]$ is a training sequence. For these predictive learners, the cooccurrence probability, $P(x^1, x^2)$, is a sufficient statistic of the prediction problem, since $P(x^2 | x^1) = \frac{P(x^1, x^2)}{\int_{x^2} P(x^1, x^2) dx^2}$. Therefore, we conjecture that predictive objectives also lead to internal representations of cooccurrence probability\footnote{Note that $x^1$ and $x^2$ in predictive learning are \textit{not} iid, rather $x^2$ is an observation that occurs \textit{after} $x^1$}.

In Appendix \ref{sec:analysis_contrastive_predictive}, we derive kernels for a few particular constrastive and predictive learners in idealized settings. The forms of these kernels are related to $K_{\texttt{coccur}}$ but not identical to it. Thus, our mathematical analysis does not precisely claim that all representations are converging to the \textit{exact} same thing, but it does suggest that they are converging to \textit{related} things. Our \textit{hypothesis} is that 1) these related representations are roughly the same in practical settings, 2) they are similar enough that we will observe continued convergence, even if the end points are slightly different, and 3) our idealized world is close enough to reality for these claims to hold in practice.

%First we will argue that the platonic learner we have described is a kind of contrastive learner. Then we will argue that other popular learners are also benefitted from modeling $p_{\theta}(x^1,x^2)$, which suggests they may learn an internal model of the platonic kernel we have defined.

\subsection{Do these results hold in practice?}
Here we show that modeling $P(x^1, x^2)$, $P(y^1, y^2)$, etc indeed results in a common kernel across modalities.

\paragraph{Demonstration: modeling color cooccurrences}


\paragraph{\textcolor{red}{Demonstration: modeling language cooccurrences}}




% $P(x^1, x^2)$ is non-negative and symmetric.
% Does it satisfy the triangle inequality?
% $P(x^1, x^3) \leq P(x^1, x^2) + P(x^2, x^3)$




\subsection{Limitations to this model}
This is all mathematical fact in the bijective case. However, real measurements are not bijcetive. In particular, they are typically highly lossy. A strong version of our hypothesis is that this doesn't matter all that much in practice. Even though a sentence may capture different information from a photograph, their mutual information is the information that matters, and the rest is details. Or we could say that the sweet spot of the U-shaped function is at a location where different modalities agree.

In the lossy case, can we say we have $P(f(x^1), f(x^2)) = P(g(y^1), g(y^2))$ if f and g are lossy ``in the same way''? And if they are not, then a frequency argument?


%There is some time-varying state variable $z(t)$. This corresponds to the platonic reality that generates all our observations. We have no access to this variable other than through observations. An observation is some function $\texttt{obs}: \mathcal{Z} \rightarrow \cdot$ that maps from state to an arbitrary measurement space, such as pixels, sounds, mass, force, torque, words, paragraphs, etc. 

\textit{All} data is considered to be the output of some observation function, which may be arbitrarily complex. This means that even properties that we might like to call ``physics'' are here thought of just as a different set of observed measurements. If we describe a ball being thrown as having a mass $m$ and an acceleration $a$, here we interpret that as meaning that a scale will observe the value $m$ and an accelerometer will measure the acceleration as $a$. These values are \textit{not} treated as state variables. They \textit{could} be part of a model of state, but it is not necessary for our argument. State is instead considered as an abstraction with the necessary properties to generate all our observations and we are agnostic as to whether it is physicists' model of reality or something else. Indeed, scholars have intrepreted Plato's arguments as \textit{rejecting} that physical properties are any more real than other kinds of observations. Philosophy aside, however, it may be convenient to consider $z(t)$ as physical state.


Suppose we have a learner whose objective is to model the function $P(A,B)$, where $A$ is a random variable that takes on discrete values (``events'') and $B$ likewise. 

To make our argument precise, we will define what we mean by joint probability. We assume the world consists of A joint event $\{a,b\}$ 

The function $P(A,B)$ is defined as follows: if $P(A=a, B=b) = p$ then $p$ is the number of joint events where $A$ takes on the value $a$ and $B$ takes on the value $b$ divided by the total number of events.

We will further assume that our model, $p_{\theta}$ has the form:
\begin{align}
    \log p_{\theta}(A,B) \propto \langle f_{\theta}(\texttt{obs}(A)), f_{\theta}(\texttt{obs}(B)) \rangle
\end{align}
where $\texttt{obs}(\cdot)$ is some measurement function (``observation'') of the event in its argument.



%\paragraph{Conjecture: this idealized scenario relates to real representation learning scenarios}


% \paragraph{Representation learners model cooccurrence probability}

% Many representation learning algorithms are models of cooccurrence. For example, contrastive learning tries to predict whether or not two events cooccur in the same context. GPT tries to predict which event follows which. To solve these problems, it is sufficient to have a model $P(Z1,Z2)$ (this is a sufficient statistic and also easily can be transformed to the relevant predictions such as $P(Z2|Z1)$ or $P(Z1,Z2)/P(Z1)(P(Z2)$), Therefore, we conjecture that these models result in an internal representation of $P(Z1,Z2)$.

% \paragraph{Cooccurrence probability is invariant to view}
% So far we have argued that different learners all result in $P(X1, X2)$ over their domain $X$. Why should this function be the same across different domains?

% It will be if discrete RV and bijective projection $f$:
% \begin{align}
%     P(X_1, X_2) = P(f(Y_1), f(Y_2))
% \end{align}
% with $f: \mathcal{Y} \rightarrow \mathcal{X}$. Note that it doesn't matter if $X$ is physics or just a measurement. There are only measurements.


% \paragraph{Relationship to kernels}

% Encoders can be used to extract unary embeddings $f(Z1)$ and $f(Z2)$. These are what are used for further inferences or as features. We argued above that encoders represent $P(Z1,Z2)$ and we point out here that they do so via some bivariate function over embeddings:
% \begin{align}
%     g(f(z_1), f(z_2))
% \end{align}
% Often $g$ is simple, a few layers, such as in contrastive learners like SimCLR. In autoregressive models, and diffusion models, it's more complicated.

% But anyway, for all these models, there is some way to get separate embeddings for two observations, $f(z1)$ and $f(z2)$, and we have argued that the full model has learned the joint distribution $P(Z1,Z2)$.

% Therefore, we argue that there should exist some $g$ such that $g(f(z_1), f(z_2)) = P(Z1,Z2)$. This is some kernel function.

% There should exist some kernel $K$ such that $K(f(z_1), f(z_2)) = P(Z1,Z2)$. Empirically, we and others have found some kernels that work (are aligned), and our conjecture is that this is because these kernels are measuring, roughly, $P(Z1,Z2)$.