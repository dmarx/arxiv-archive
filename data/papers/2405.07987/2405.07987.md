---
abstract: |
  We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato’s concept of an ideal reality. We term such a representation the *platonic representation* and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.

  <table>
  <tbody>
  <tr class="odd">
  <td style="text-align: left;"><strong>Project Page:</strong></td>
  <td style="text-align: right;"><p><a href="https://phillipi.github.io/prh/"><code>phillipi.github.io/prh</code></a></p></td>
  </tr>
  <tr class="even">
  <td style="text-align: left;"><span><strong>Code:</strong></span></td>
  <td style="text-align: right;"><p><span> <a href="https://github.com/minyoungg/platonic-rep/"><code>github.com/minyoungg/platonic-rep</code></a> </span></p></td>
  </tr>
  </tbody>
  </table>
bibliography:
- citations.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
---




# Introduction

AI systems are rapidly evolving into highly multifunctional entities. For example, whereas in the past we had special-purpose solutions for different language processing tasks (, sentiment analysis, parsing, dialogue), modern large language models (LLMs) are competent at all these tasks using a single set of weights . Unified systems are also being built across data modalities: instead of using a different architecture for processing images versus text, recent models, such as GPT4-V , Gemini , and LLaVA , handle both modalities with a combined architecture. More and more systems are built off of general-purpose pretrained backbones, sometimes called foundation models , that support a large range of tasks, including robotics , bioinformatics , and healthcare . In short, AI systems are becoming increasingly homogeneous in both their architectures and their capabilities.

<figure id="fig:platonic_rep">
<p><span class="image placeholder" data-original-image-src="figures/platonic_rep_less_space_v3.pdf" data-original-image-title="" width="0.85\linewidth">image</span></p>
<figcaption><strong>The Platonic Representation Hypothesis:</strong> Images (<span class="math inline">\(X\)</span>) and text (<span class="math inline">\(Y\)</span>) are projections of a common underlying reality (<span class="math inline">\(Z\)</span>). We conjecture that representation learning algorithms will converge on a shared representation of <span class="math inline">\(Z\)</span>, and scaling model size, as well as data and task diversity, drives this convergence. </figcaption>
</figure>

This paper explores one aspect of this trend: representational convergence. We argue that there is a growing similarity in how datapoints are represented in different neural network models. This similarity spans across different model architectures, training objectives, and even data modalities.

What has led to this convergence? Will it continue? And ultimately, where does it end?

Our central hypothesis, stated above in , is that there is indeed an endpoint to this convergence and a principle that drives it: different models are all trying to arrive at a *representation of reality*, meaning a representation of the joint distribution over events in the world that generate the data we observe. conveys this hypothesis: there exists a real world (labeled $Z$), which we measure with various sensors, such as the camera shown to the left ($X$). Other *projections* of these measurements, such as the textual description shown, can be produced from the first set of measurements or mediated by some other set of measurements, , touch or other camera views (dotted arrow from $X$ to $Y$)[^1]. Representation learning algorithms find vector embeddings that statistically model the various measurements and projections. The resulting vector embeddings are all derived from the underlying reality in $Z$ and thereby become aligned. As models are trained on more data and for more tasks, they require representations that capture more and more information about $Z$, and hence alignment toward $Z$ increases toward a convergent point as a function of scale.

We call this converged hypothetical representation the “platonic representation” in reference to Plato’s Allegory of the Cave , and his idea of an ideal reality that underlies our sensations. The training data for our algorithms are shadows on the cave wall, yet, we hypothesize, models are recovering ever better representations of the actual world outside the cave. This idea is not unique to Plato; our hypothesis is also related to the notion of “convergent realism”  in the philosophy of science (, that science is converging on truth), and to many arguments that have been put forth in the representation learning literature (, ).

Also closely related to our hypothesis is the “Anna Karenina scenario” described by , referring to the possibility that all well-performing neural nets represent the world in the same way. We discuss the evidence they give for this possibility in [^2]. The platonic representation hypothesis refers to the situation where we are in an Anna Karenina scenario *and* the “happy representation” that is converged upon is one that reflects a statistical model of the underlying reality. We discuss the potential nature of this statistical model in more detail in .

# Representations are converging

#### Preliminaries

We restrict our attention to representations that are *vector embeddings*. We characterize such a representation by the similarity structure it induces, referred to as its kernel. Kernels are commonly used to assess representations ; this can be justified by the fact that they capture the relative structures among data samples, which are also the learning signal for many machine learning algorithms  . Following prior literature, we define *representational alignment* as a measure of the similarity of the similarity structures induced by two representations, , a similarity metric over kernels. We give the mathematical definition of these concepts below:

- A **representation** is a function $f\colon \mathcal{X} \rightarrow \mathbb{R}^n$ that assigns a feature vector to each input in some data domain $\mathcal{X}$.

- A **kernel**, $K\colon \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, characterizes how a representation measures distance/similarity between datapoints. $K(x_i,x_j) = \langle f(x_i), f(x_j) \rangle$, where $\langle {{}\cdot{}},{{}\cdot{}}\rangle$ denotes inner product, $x_i, x_j \in \mathcal{X}$ and $K \in \mathcal{K}$.

- A **kernel-alignment metric**, $m\colon \mathcal{K} \times \mathcal{K} \rightarrow \mathbb{R}$, measures the similarity between two kernels, , how similar is the distance measure induced by one representation to the distance measure induced by another. Examples include Centered Kernel Distance (CKA) , SVCCA , and nearest-neighbor metrics .

In our experiments, we use a *mutual nearest-neighbor metric* that measures the mean intersection of the $k$-nearest neighbor sets induced by two kernels, $K_1$ and $K_2$, normalized by $k$. This metric is a variant of those proposed in , and . See  for the exact definition and  for comparisons with alternative alignment metrics.

Next, we explore several ways in which representations are converging. First, we argue that different neural networks are converging to aligned representations. Then, we show that this continues to hold across modalities, where image embeddings in vision models align with text embeddings in language models.

## Different models, with different architectures and objectives, can have aligned representations

One indication of representational convergence is the rising number of systems built on top of pre-trained foundation models. These models are becoming standard backbones across a growing spectrum of tasks. Their versatility across numerous applications implies a level of universality in the way they represent data.

While this trend implies convergence toward a relatively small set of foundation models, it does not imply that *different* foundation models will arrive at the same representation. Yet that is what has been observed by several recent papers.

conducted one such study, in which they measured representational similarity through a technique called *model stitching*. Given two models, $f$ and $g$, each composed of multiple layers ($f = f_1 \circ \cdots \circ f_n$, $g = g_1 \circ \cdots \circ g_m$), an intermediate representation from $f$ is integrated into $g$ via a learned affine stitching layer $h$, resulting in a new stitched model $F = f_1 \circ \cdots \circ f_k \circ h \circ g_{k+1} \circ \cdots \circ g_m$. If $F$ has good performance, it indicates that $f$ and $g$ have compatible representations at layer $k$, up to the transform $h$.

In their study,  made two notable findings: (1) A vision model trained on ImageNet  can be aligned with a model trained on Places-365  while maintaining good performance; (2) The early layers of these convolutional networks are more interchangeable than later layers. The first finding illustrates a level of data independence where distinct image datasets lead to similar representations. The second finding agrees with extensive research that oriented Gabor-like filters are common in both artificial and biological vision systems. This suggests a convergence to a similar initial layer of representation across various neural network architectures . expanded on the idea of model stitching, showing that models trained using self-supervised objectives align closely with their supervised counterparts.

further demonstrated the feasibility of “zero-shot” model stitching without learning a stitching layer. Despite the fact that different text models were trained on different modalities, they found that the models often embed data in remarkably similar ways. In particular, they considered the kernel $K$ defined by learned representations and showed that $K$ serves as a bridge between models, allowing an encoder trained in one language, like English, to work effectively with a decoder in another, like French.

extended this idea to individual neurons, and found “Rosetta Neurons” that are activated by the same pattern across a range of vision models. Such neurons form a common dictionary independently discovered by all models.

## Alignment increases with scale and performance

and observed model alignment not only exists but also increases with model scale and dataset size. On CIFAR-10 classification, found that larger models exhibit greater alignment with each other compared to smaller ones. Theoretically, showed that models with similar outputs (, as a result of having high performance) also have similar internal activations. With the continuing trend of models scaling up, this suggests model alignment will increase over time – we might expect that the next generation of bigger, better models will be even more aligned with each other.

We expand upon this observation by evaluating the transfer performance of $78$ vision models. These models were trained with varying architectures, training objectives, and datasets (detailed in ). In  (left), we bin these models based on their average transfer performance on the VTAB dataset , and then measure the average kernel alignment of the models within each bin. The results indicate that models with high transfer performance form a tightly clustered set of representations, while models with weak performance have more variable representations. We further visualize this structure with UMAP  over models representation in  (right). This suggests that models that are competent all represent data in a similar way. Echoing and , we might say: all strong models are alike, each weak model is weak in its own way.

The discussion so far indicates that various models are aligning toward a unified representation. But does the convergence extend to model weights? While models with different architectures might not have compatible weight spaces, there exists ample evidence that models with the same architecture will often converge to the same basin of weights . This holds even for models with different initializations, up to permutations over weight space . Because of this, it is possible to merge separately trained models of the same architecture, and achieve some of the capabilities of all models in the mixture .

## Representations are converging across modalities

Do models trained on different data modalities also converge? Several works indicate that the answer is *yes*.

extended model stitching to the cross-modal setting, finding that a single linear projection is sufficient to stitch a vision model to an LLM and achieve good performance on visual question answering and image captioning. showed that linear stitching can also work in the opposite direction, aligning text inputs to visual outputs. In fact, many recent language-vision models stitch pre-trained language and vision models together. For example, LLaVA  demonstrated state-of-the-art results by projecting visual features into a language model with a 2-layer MLP.

Other works show further kinds of evidence of cross-modal synergy. found that jointly training a language model with a vision model improves performance on language tasks, compared to training the language model on its own. show a setting in which word embeddings of visual concept names can be isometrically mapped to image embeddings for those same concepts. In work concurrent to ours, show well-trained vision encoders on large datasets exhibit high semantic similarity with language encoders regardless of the training paradigm (supervised, self-supervised, or language-supervised). probed the visual knowledge of LLMs trained *only* on language data, by converting images into code that an LLM can process. They found that LLMs have rich knowledge of visual structures, to the extent that decent visual representations can be trained on images generated solely by querying an LLM to produce code and rendering the response. In visual generation, LLMs show abilities to augment captions with visual structures (, bounding boxes) and improve generation quality . Over other modalities, showed auditory models are also roughly aligned with LLMs up to a linear transformation, and demonstrated the effectiveness of using pre-trained LLMs for facial motion prediction.

We set out to address these claims in a broader scope to determine whether models are indeed learning an increasingly modality-agnostic representation of the world. We sampled a variety of models trained either solely on vision or solely on language, and compared their representations as they became larger and more competent over many tasks.

In , we assess alignment between a suite of language models and vision models. So far we have only defined alignment for two kernels defined over the same input space. To measure cross-modal alignment, we use paired datasets to bridge the two modalities. For vision and text, we use the Wikipedia captions dataset $\{(x_i, y_i)\}_i$ , composed of images from Wikipedia ($x_i$) and their corresponding captions ($y_i$). We then measure alignment between a language model $f_{\texttt{text}}$ and a vision model $f_{\texttt{img}}$ as the alignment of the two following kernels: $$\begin{aligned}
    K_\texttt{img}(i, j) = \langle f_{\texttt{img}}(x_i), f_{\texttt{img}}(x_j) \rangle\\
    K_\texttt{text}(i, j) = \langle f_{\texttt{text}}(y_i), f_{\texttt{text}}(y_j) \rangle.
\end{aligned}$$ Using this analysis, we find that the better an LLM is at language modeling, the more it tends to aligns with vision models, as shown in . The converse effect also holds: the better a vision models is, the more it tends to align with LLMs. See for more details.

## Models are increasingly aligning to brains

Neural networks also show substantial alignment with biological representations in the brain . This commonality may be due to similarities in the task and data constraints both systems are confronted with. Even though the mediums may differ – silicon transistors versus biological neurons – the fundamental problem faced by brains and machines is the same: efficiently extracting and understanding the underlying structure in images, text, sounds,  . developed a theoretical framework for how the efficient extraction of novel concepts occurs for both the human visual system and deep networks. The tasks that the human visual system has been honed to perform through evolution – like segmentation, detection, and whole-image classification – are also the ones that we train our neural nets to perform. went as far as to title their work in the spirit that performance over such tasks implies brain alignment. posited that it is less the particular task and more the generality of the representations that explain their alignment with biological representations. Further, showed that training data plays a large role in alignment. Psychophysical studies have also shown agreement between how humans perceive visual similarity and how models do, even when the models are trained on tasks, such as self-supervised prediction, that are seemingly unrelated to mimicking human perception .

## Does alignment predict downstream performance?

If models are converging towards a more accurate representation of reality, we expect that alignment should correspond to improved performance on downstream tasks. supports this hypothesis by demonstrating improved performance on commonsense reasoning (Hellaswag; ) and mathematical problem solving (GSM8K; ) as alignment improves.

# Why are representations converging?

Modern machine learning models are generally trained to minimize the empirical risk with possible implicit and/or explicit regularization: $${
    \color{gray}
    \overbracket[1pt]{\color{black}f^*}^{\mathclap{\textsf{trained model}}}
    }{}= \mathbox[model]{\argmin}_{f \in {
    \color{gray}
    \underbracket[1pt]{\scriptsize\mathbox[model]{\color{black}\mathcal{F}}}_{\mathclap{\scriptstyle\textsf{function class}}}}
    }\mathbb{E}_{x \sim {\scriptsize\mathbox[task]{\mathsf{dataset}}}}[ {
    \color{gray}
    \overbracket[1pt]{\mathbox[task]{\color{black}\mathcal{L}}}^{\mathclap{\textsf{training objective}}}}(f, x)] + {
    \color{gray}
    \underbracket[1pt]{\mathbox[bias]{\color{black}\mathcal{R}}}_{\mathclap{\textsf{regularization}}}}(f)$$ In the following sections, we lay out how each colored component in this optimization process potentially plays a role in facilitating representational convergence.

<figure id="fig:multitask_hypothesis">
<p><span class="image placeholder" data-original-image-src="figures/multitask_hypothesis.pdf" data-original-image-title="" width="0.825\linewidth">image</span><br />
</p>
<figcaption><strong>The Multitask Scaling Hypothesis:</strong> Models trained with an increasing number of tasks are subjected to pressure to learn a representation that can solve all the tasks.</figcaption>
</figure>

## Convergence via 

Each training datapoint and objective (task) places an additional constraint on the model. As data and tasks scale, the volume of representations that satisfy these constraints must proportionately grow smaller, as visualized in Figure and stated below:

This has been previously termed as the Contravariance principle by , which states that the set of solutions to an easy goal is large, while the set of solutions to a challenging goal is comparatively smaller. Moreover, we argue that this narrower solution set also generalizes better. As data scales, models that optimize the empirical risk $\mathbb{E}_{x \sim {\scriptsize \mathbox[task]{\mathsf{dataset}}}}[ {{{\mathcal{L}}}}(f, x)]$ also improve on the population risk $\mathbb{E}_{x \sim {\scriptsize \mathbox[task]{\mathsf{reality}}}}[ {{{\mathcal{L}}}}(f, x)]$, and become better at capturing statistical structures of the true data generating process ($\mathsf{reality}$).

Recent work has demonstrated a power law relationship between data scale and model performance . This implies that with enough data (, consisting of the entire internet and all offline scientific measurements) one ought to converge to a very small solution set with irreducible error – the inherent epistemic uncertainty of the world. As more models are trained on internet-scale data, the set of solutions that satisfies all data constraints must become relatively small.

In addition to data-scaling, many modern representation learning objectives $\mathbox[task]{\mathcal{L}}(f, x)$ directly optimize for multi-task solving. Contrastive learning finds a distance structure over data samples that optimizes many classification tasks . Masked Autoencoders optimize randomly sampled reconstruction tasks. In fact, autoregressive language modeling can also be seen as optimizing a diverse set of tasks . Such multi-task objectives may be more effective than single-task ones (, ImageNet classification) due to the fact that they impose more task constraints on the representation, leading to a smaller and higher-quality solution space .

## Convergence via 

Suppose there is a globally optimal representation for standard learning objectives. Then, under sufficient data, *scaling* a model (, using larger function classes ), as well as , should be more effective at finding better approximations to this optimum, as illustrated in . With the same training objective, larger models, even of different architectures, will thus tend to converge toward this optimum. When different training objectives share similar minimizers, larger models are better at finding these minimizers, and will train to similar solutions over the training tasks. We summarize this hypothesis as follows:

## Convergence via 

Arriving at the same mapping on the *training data* does not prohibit the models from developing distinct internal representations. It is not unreasonable to posit that the representations used to detect a dog in a 1M parameter model could be quite different than that used by a 1B parameter model. What would stop a billion-parameter (and counting) model from learning an overly complicated and distinct representation? One key factor might be simplicity bias:

Such simplicity bias could be coming from explicit regularization $\mathbox[bias]{\mathcal{R}(f)}$ commonly used in deep learning (, weight decay and dropout). However, even in the absence of external influences, deep networks naturally adhere to Occam’s razor, that fit the data . Figure visualizes how simplicity bias can drive convergence.

<figure id="fig:simplicity_hypothesis">
<p><span class="image placeholder" data-original-image-src="figures/simplicity_hypothesis.pdf" data-original-image-title="" width="0.825\linewidth">image</span><br />
</p>
<figcaption><strong>The Simplicity Bias Hypothesis:</strong> Larger models have larger coverage of all possible ways to fit the same data. However, the implicit simplicity biases of deep networks encourage larger models to find the simplest of these solutions.</figcaption>
</figure>

# What representation are we converging to?

By now, we hope to have convinced the reader that task and data pressures, combined with increasing model capacity, can lead to convergence. We next turn our attention to *what* exactly is the endpoint of all this convergence.

Our central hypothesis, stated in , is that the representation we are converging toward is a statistical model of the underlying reality that generates our observations. Consistent with the multitask scaling hypothesis, such a representation would naturally be useful toward many tasks (or at least toward any task grounded in reality). Additionally, this representation might be relatively simple, assuming that scientists are correct in suggesting that the fundamental laws of nature are indeed simple functions , in line with the simplicity bias hypothesis.

But what exactly do we mean by “a statistical model of the underlying reality.” In this section, we formalize one definition with concrete mathematical statements. *Importantly*, this section should be read as just one concrete candidate for the form of the platonic representation; other candidates could be arrived at from other modeling assumptions.

## An idealized world

We consider a world that works as follows, consistent with the cartoon in . The world consists of a sequence of $T$ discrete events, denoted as $\mathbf{Z} \triangleq [z_1, \ldots, z_T]$, sampled from some unknown distribution $\mathbb{P}(\mathbf{Z})$. Each event can be observed in various ways. An observation is a bijective, deterministic function $\texttt{obs}: \mathcal{Z} \rightarrow \cdot{}\,$ that maps events to an arbitrary measurement space, such as pixels, sounds, mass, force, torque, words, etc. Later, in , we discuss limitations and potential extensions to continuous and unbounded worlds, and stochastic observations, that could yield a model that better reflects real learning scenarios.

One can think of an event as corresponding to the state of the world at some point in time[^3], but it is also fine to simply consider an event as any variable that indexes observations, with no further physical meaning[^4].

In this idealized world, knowing $\mathbb{P}(\mathbf{Z})$ would be useful for many kinds of predictions; this would constitute a world model over the events that cause our observations . We will next show that a particular representation of $\mathbb{P}(\mathbf{Z})$ is recovered by certain contrastive learners.

## A family of contrastive learners converge to a representation of $\mathbb{P}(\mathbf{Z})$

Consider a contrastive learner that models observations that *cooccur* together. For simplicity, we ground our discussion with the following definition of the *cooccurrence probability*, $\Pco$, of two observations $x_a$ and $x_b$ both occurring within some window $T_\mathsf{window}$: $$\begin{aligned}
    \Pco(x_a, x_b) \hspace{0.1in} \propto \hspace{-0in} \sum_{(t, t') \colon \abs{t-t'} \leq T_\mathsf{window}} \hspace{-0.2in} \mathbb{P}(X_t = x_a, X_{t'} = x_b).\nonumber
\end{aligned}$$ Analogously, we can define $\Pco$ for $\mathbf{Z}$ and other observation modalities. Note that $\Pco$ is symmetric.

Consider *positive pairs* as two observations nearby in time (sampled from $\Pco$) and *negative pairs* as observations drawn from any point in time (sampled independently from the marginal). Our contrastive learner tries to classify if a pair is positive or negative by learning a representation $f_X \colon X \rightarrow \mathbb{R}^d$ such that the dot-product kernel approximates the log odds ratio up to some offset: $$\begin{aligned}
    \langle f_X(x_a), f_X(x_b) \rangle 
    & \approx \log \frac{\mathbb{P}(\texttt{pos} \given x_a, x_b)}{\mathbb{P}(\texttt{neg} \given x_a, x_b)} + \tilde{c}_X(x_a) \\
    & = \log \frac{\Pco(x_a \given x_b)}{\Pco(x_a)} + c_X(x_a) \\
    & =  \Kpmi(x_a, x_b) + c_X(x_a), \label{eqn:contr-pmi}
\end{aligned}$$ where $\Kpmi$ is the pointwise mutual information (PMI) kernel, and $c_X(x_a)$ is constant in $x_b$. We note that this is a common setting for self-supervised contrastive learners with NCE objectives , including SimCLR  and SimCSE . (See and for detailed derivations.)

Under mild conditions that the world is smooth enough (see ), a choice of $f_X$ can exactly represent $\Kpmi$: $$\begin{aligned}
    \langle f_X(x_a), f_X(x_b) \rangle &= \Kpmi(x_a,x_b) + c_X,
\end{aligned}$$ where we observed that $c_X(x_a)$ from must be a constant since both sides are symmetric.

Therefore, the contrastive learners we consider are minimized by a representation $f_X$ whose kernel is $\Kpmi$ (up to a constant offset). With sufficient data and optimization, we will observe convergence to this point.

Thus we have convergence to a representation of the statistics of $X$, but what about $Z$? Recall that our idealized world consists of *bijective* observation functions, which, over discrete random variables, preserve probabilities. So we have: $$\begin{aligned}
    \Pco(x_a, x_b) &= \Pco(z_a, z_b)\\
    \Kpmi(x_a, x_b) &= \Kpmi(z_a, z_b),
\end{aligned}$$ where we use $\Pco$ and $\Kpmi$ in a modality-agnostic way to emphasize that different modalities share the same these quantities.

All these arguments hold not just for $X$ but also for $Y$ (or any other bijective, discrete modality), implying: $$\begin{aligned}
    \Kpmi(z_a, z_b) 
    & = \langle f_X(x_a), f_X(x_b) \rangle - c_X \\
    & =     \langle f_Y(y_a), f_Y(y_b) \rangle  - c_Y.
\end{aligned}$$ Therefore, for any modality in our idealized world, we observe representational convergence to the same kernel, which represents certain pairwise statistics of $\mathbb{P}(\mathbf{Z})$.

This analysis suggests that certain representation learning algorithms may boil down to a simple rule: *find an embedding in which similarity equals PMI*. We note that this idea is consistent with prior works that have used PMI as a similarity measure for clustering in vision and language (,  ).

#### A study in color

We conduct a case study to verify that convergence does happen on real data. discovered that color distances in learned language representations, when trained to predict cooccurrences in *text* , closely mirror human perception of these distances, which we reproduce in with both contrastive and predictive models. Interestingly, they noted an increasing similarity as models scale larger and become better at modeling *text* cooccurrences. In , we also learn representations of color based on $\Kpmi$ from cooccurrences in *images*. Indeed, learning cooccurrence statistics in either domain recovers roughly the *same* perceptual representation. Details of this experiment are described in .

We believe that our simple model encapsulates essential aspects of complex real-world systems, and offers a path toward understanding the representation that models are converging to—a unified model that is proficient across various domains and modalities, grounded in the statistical properties of the underlying world. further elaborates some limitations.

# What are the implications of convergence?

#### Scaling is sufficient, but not necessarily efficient

Our arguments are roughly in line with the claim that “scale is all you need” to reach high levels of intelligence. We have argued that as resources are scaled (# parameters, \# datapoints, \# flops), representations are converging, regardless of other modeling choices and even data modality. Does this mean that scale is all that matters? Not quite: different methods can scale with different levels of *efficiency* , and successful methods must still satisfy some general requirements (, be a consistent estimator, model pairwise statistics of $\mathbb{P}(\mathbf{Z})$).

#### Training data can be shared across modalities

Suppose you have access to $N$ images and $M$ sentences, and want to learn the best representation. If there is indeed a modality-agnostic platonic representation, then *both* image and language data should help find it. The implication is that if you want to train the best vision model, you should train not just on $N$ images but also on $M$ sentences. This is already becoming common practice . Many vision models are finetuned from pre-trained LLMs. The other direction is less common, but also is implied by our hypothesis: if you want to build the best LLM, *you should also train on image data*. Indeed, showed that training on images improved performance on text. In theory, there should be some conversion ratio: a pixel is worth $a$ words for training LLMs, and a word is worth $b$ pixels for training vision models.

#### Ease of translation and adaptation across modalities

When two representations are aligned, transitioning from one to the other should be a simple function that’s easily obtained. Our hypothesis could explain the phenomenon that conditional generation is easier than unconditional , as the data we condition on may have the same platonic structure as the data we are generating. In line with this, recent work has found that representation-conditioning is even easier . Similarly, representational convergence could act as a bridge that lets us find mappings between domains even without paired data; this may underlie the success of unpaired translation in vision and language . We emphasize that this doesn’t mean that models trained on a single modality (, language) can immediately process raw data from another (, vision). What makes them adaptable to the new modalities is that they share a common modality-agnostic representation, and can readily process *representations* of new modalities. Furthermore, this implies that language models would achieve some notion of grounding in the visual domain even in the absence of cross-modal data[^5]. The primary advantage of cross-modal data could then simply be sample efficiency.

#### Scaling may reduce hallucination and bias

A prominent shortcoming of current LLMs is their propensity to hallucinate, or output false statements. If models are indeed converging toward an accurate model of reality, and scale powers this convergence, then we may expect hallucinations to decrease with scale. Of course, our hypothesis is conditioned on the training data for future models constituting a sufficiently lossless and diverse set of measurements. This may not come to pass, but it is an implication of our hypothesis worth pointing out. A similar argument can be made about certain kinds of bias. It has been shown that large models can exacerbate existing biases present in their training data . Our hypothesis implies that, while this may be true, we should expect *larger* models to amplify bias *less*. This does not mean bias will be removed, rather that the model’s biases will more accurately reflect the data’s biases, rather than exacerbating them.

# Counterexamples and limitations

#### Different modalities may contain different information

One immediate objection to our hypothesis is: what about the information that is unique to a given modality? Can language really describe the ineffable experience of watching a total solar eclipse? Or, how could an image convey the a concept like “I believe in the freedom of speech,” which is easy to write in English? Two different models cannot converge to the same representation if they have access to fundamentally different information.

More precisely, our mathematical argument in only strictly holds for bijective projections of $\mathbf{Z}$, so that the information in all the projections is equivalent to the information in the underlying world. This will not hold true for either lossy or stochastic observation functions. Nonetheless, similar arguments have been made theoretically and empirically that cooccurrence relations are learned by practical contrastive and predictive learners . and also showed that models trained to autoregressively generate text also capture statistical relations in many other modalities, including symbolic reasoning, vision, protein folding, and robotics.

<figure id="fig:caption_density">
<p><span class="image placeholder" data-original-image-src="figures/caption_density_dci_combined.pdf" data-original-image-title="" width="\linewidth">image</span> </p>
<figcaption><strong>Increasing caption density improves alignment:</strong> We vary caption length using the Densely-Captioned-Images (DCI) dataset <span class="citation" data-cites="urbanek2023picture"></span>. Starting from a dense caption, we used LLaMA3-8B-Instruct <span class="citation" data-cites="meta2024llama3"></span> to summarize and generate coarse-grained captions. We compute the average alignment score across all vision and language models with standard deviation measured over the language models we evaluated. With denser captions, the mapping may become more bijective, leading to improved language-vision alignment scores.</figcaption>
</figure>

A more nuanced version of our hypothesis will need to be developed to handle the case of non-bijective observations and abstract concepts. A starting point could be: different models will converge to the same representation *when the input signals are sufficiently high information and the models are sufficiently high capacity*; when they are not, the lower-information representation will only align with the higher-information one up to a level capped by the mutual information between the input signals and by the capacity of each model. This cap might or might not be practically important. Popular representations like CLIP are explicitly optimized to only capture the shared information between vision and language, yet are highly successful on many pure vision tasks. We perform a preliminary test of the effect of information level in (detailed in ), and find that the more descriptive (higher information) a caption is, the better its LLM representation aligns with the visual representation of the corresponding image.

#### Not all representations are presently converging

Our argument has mainly focused on two modalities: vision and language. While we do expect other modalities will follow similar trends, we have yet to see the same level of convergence across all domains. For example, in robotics there is not yet a standardized approach to representing world states in the same way as there is for representing images and text. One limitation lies in the hardware used in robotics, which is often expensive and slow. This creates a bottleneck in the quantity and diversity of training data.

#### Sociological bias in producing AI models

Researcher bias and collective preferences within the AI community have shaped the trajectory of model development. There is often an explicit or implicit goal of designing AI systems that mimic human reasoning and performance, and this could lead to convergence toward human-like representations even if other kinds of intelligence are in fact possible. Additionally, the “hardware lottery”  suggests that the success of AI models can also depend on the compatibility of their design with available computational architectures, further contributing to convergent trends.

#### Special-purpose intelligences might not converge

Different intelligent systems can be designed to accomplish different tasks. For instance: A bioinformatics systems might predict protein structure; an autonomous vehicle might follow lanes on highways. It’s possible that not much is shared between these two narrow tasks. Our argument only holds for intelligences that are optimized to perform well on *many* tasks. We have argued that a representation of *reality* is a structure that is useful across many tasks, but for any special purpose there may be shortcuts, or even effective representations detached from reality. Such shortcuts may be more efficient and necessary for continued improvements in specific domains. This will become more relevant if continued scaling comes up against boundary conditions around resources like energy and compute.

####  How do we measure alignment? 

We focused on one particular alignment measure, mutual nearest-neighbor, in our experiments, and cited experiments using several others. However, there is active debate on the merits and deficiencies of all these ways of measuring alignment . We discuss our choice and show results for other alignment metrics in Appendix .

#### Lots left to explain

We have shown results where different models arrive at *similar* but not the *same* representations. For example, in , alignment clearly increases but only reaches a score of $0.16$, according to our mutual nearest-neighbor metric. The maximum theoretical value for this metric is $1$. Is a score of $0.16$ indicative of strong alignment with the remaining gap being “noise” or does it signify poor alignment with major differences left to explain? We leave this as an open question.

# Acknowledgements

We thank for sharing their data for our experiments shown in . We thank the anonymous reviewers for helpful feedback, and for providing the counterexample on how to visually convey “I believe in the freedom of speech.” Thanks for Yonglong Tian, Dilip Krishnan, Anna Decker, Yoon Kim, Jyo Pari, Ani Nrusimha, Dave Epstein, Victor Butoi, and Seungwook Han for helpful discussions and suggestions. We thank Mingzhong Sun for catching a typo. This work was supported by a Packard Fellowship and a Sloan Research Fellowship to P.I., by the MIT-IBM Watson AI Lab, by ONR MURI grant N00014-22-1-2740, by the Center for Brains, Minds, and Machines, the MIT Quest for Intelligence, NSF STC award CCF-1231216, the DARPA Knowledge Management at Scale and Speed (KMASS) program, and the DARPA Machine Common Sense (MCS) program.

[^1]: Touch could convey the shapes in this example but not the colors. This is an important limitation to our hypothesis that we discuss at several points in the paper: different sensors and views might capture different information, which may limit their potential to converge to identical representations.

[^2]: Borrowed from , similar analogies have been made in other domains, such as the “Anna Karenina principle” popularized by to explain animal domestication.

[^3]: Here we only analyze temporal sequences, but note that the same could be done with respect to events laid out in space instead.

[^4]: This latter interpretation may be more consistent with Plato’s intent. Scholars have argued that his allegory of the cave rejects any notion of a true world state . Instead, we could say that the joint distribution of observation indices is *itself* the platonic reality.

[^5]: In 1688, William Molyneux asked if a person born blind, upon gaining sight, could distinguish shapes by vision alone . Our arguments suggest they could not do so immediately, but after some visual experience, they could easily map shapes to their prior touch-based representations. Empirical data supports this, showing that congenitally blind children given sight can quickly learn these abilities .
