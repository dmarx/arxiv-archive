{
  "arxivId": "2412.13663",
  "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast, Memory Efficient, and Long Context Finetuning and Inference",
  "authors": "Benjamin Warner, Antoine Chaffin, Benjamin Clavi\u00e9, Orion Weller, Oskar Hallstr\u00f6m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli",
  "abstract": "Encoder-only transformer models such as BERT offer a great performance-size\ntradeoff for retrieval and classification tasks with respect to larger\ndecoder-only models. Despite being the workhorse of numerous production\npipelines, there have been limited Pareto improvements to BERT since its\nrelease. In this paper, we introduce ModernBERT, bringing modern model\noptimizations to encoder-only models and representing a major Pareto\nimprovement over older encoders. Trained on 2 trillion tokens with a native\n8192 sequence length, ModernBERT models exhibit state-of-the-art results on a\nlarge pool of evaluations encompassing diverse classification tasks and both\nsingle and multi-vector retrieval on different domains (including code). In\naddition to strong downstream performance, ModernBERT is also the most speed\nand memory efficient encoder and is designed for inference on common GPUs.",
  "url": "https://arxiv.org/abs/2412.13663",
  "issue_number": 113,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/113",
  "created_at": "2024-12-24T19:27:12.741544",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null
}