\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252, 2019.

\bibitem[Aronszajn(1950)]{aronszajn1950theory}
N.~Aronszajn.
\newblock Theory of reproducing kernels.
\newblock \emph{Transactions of the American mathematical society}, 68\penalty0
  (3):\penalty0 337--404, 1950.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8141--8150, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{36th International Conference on Machine Learning, ICML
  2019}, pages 477--502. International Machine Learning Society (IMLS),
  2019{\natexlab{b}}.

\bibitem[Atkinson and Han(2012)]{atkinson2012spherical}
K.~Atkinson and W.~Han.
\newblock \emph{Spherical harmonics and approximations on the unit sphere: an
  introduction}, volume 2044.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Bach(2017)]{bach2017breaking}
F.~Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Belkin et~al.(2018)Belkin, Ma, and Mandal]{belkin2018understand}
M.~Belkin, S.~Ma, and S.~Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  541--549, 2018.

\bibitem[Bietti and Bach(2021)]{bietti2020deep}
A.~Bietti and F.~Bach.
\newblock Deep equals shallow for relu networks in kernel regimes.
\newblock In \emph{ICLR}, 2021.

\bibitem[Bietti and Mairal(2019)]{bietti2019inductive}
A.~Bietti and J.~Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  12893--12904, 2019.

\bibitem[Bingham(1973)]{bingham1973positive}
N.~H. Bingham.
\newblock Positive definite functions on spheres.
\newblock In \emph{Mathematical Proceedings of the Cambridge Philosophical
  Society}, volume~73, pages 145--156. Cambridge University Press, 1973.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Y.~Cao and Q.~Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10836--10846, 2019.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019towards}
Y.~Cao, Z.~Fang, Y.~Wu, D.-X. Zhou, and Q.~Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:1912.01198}, 2019.

\bibitem[Cheney and Light(2009)]{cheney2009course}
E.~W. Cheney and W.~A. Light.
\newblock \emph{A course in approximation theory}, volume 101.
\newblock American Mathematical Soc., 2009.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Y.~Cho and L.~K. Saul.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  342--350, 2009.

\bibitem[Doetsch(1974)]{doetsch1974introduction}
G.~Doetsch.
\newblock \emph{Introduction to the theory and application of the Laplace
  transformation}.
\newblock Springer, 1974.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1675--1685, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Fan and Wang(2020)]{fan2020spectra}
Z.~Fan and Z.~Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock \emph{arXiv preprint arXiv:2005.11879}, 2020.

\bibitem[Flajolet and Sedgewick(2009)]{flajolet2009analytic}
P.~Flajolet and R.~Sedgewick.
\newblock \emph{Analytic combinatorics}.
\newblock cambridge University press, 2009.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and
  Basri]{geifman2020similarity}
A.~Geifman, A.~Yadav, Y.~Kasten, M.~Galun, D.~Jacobs, and R.~Basri.
\newblock On the similarity between the laplace and neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:2007.01580}, 2020.

\bibitem[Hui et~al.(2019)Hui, Ma, and Belkin]{hui2019kernel}
L.~Hui, S.~Ma, and M.~Belkin.
\newblock Kernel machines beat deep neural networks on mask-based
  single-channel speech enhancement.
\newblock \emph{Proc. Interspeech 2019}, pages 2748--2752, 2019.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Kuditipudi et~al.(2019)Kuditipudi, Wang, Lee, Zhang, Li, Hu, Ge, and
  Arora]{kuditipudi2019explaining}
R.~Kuditipudi, X.~Wang, H.~Lee, Y.~Zhang, Z.~Li, W.~Hu, R.~Ge, and S.~Arora.
\newblock Explaining landscape connectivity of low-cost solutions for
  multilayer nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14601--14610, 2019.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{liu2020toward}
C.~Liu, L.~Zhu, and M.~Belkin.
\newblock Toward a theory of optimization for over-parameterized systems of
  non-linear equations: the lessons of deep learning.
\newblock \emph{arXiv preprint arXiv:2003.00307}, 2020.

\bibitem[Minh et~al.(2006)Minh, Niyogi, and Yao]{minh2006mercer}
H.~Q. Minh, P.~Niyogi, and Y.~Yao.
\newblock Mercerâ€™s theorem, feature maps, and smoothing.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 154--168. Springer, 2006.

\bibitem[Pinelis(2020)]{pinelis}
I.~Pinelis.
\newblock Analyzing the decay rate of taylor series coefficients when
  high-order derivatives are intractable.
\newblock MathOverflow, 2020.
\newblock URL \url{https://mathoverflow.net/q/366252}.

\bibitem[Saitoh and Sawano(2016)]{saitoh2016theory}
S.~Saitoh and Y.~Sawano.
\newblock \emph{Theory of reproducing kernels and applications}.
\newblock Springer, 2016.

\bibitem[Schoenberg(1942)]{schoenberg1942positive}
I.~J. Schoenberg.
\newblock Positive definite functions on spheres.
\newblock \emph{Duke Mathematical Journal}, 9\penalty0 (1):\penalty0 96--108,
  1942.

\bibitem[Spiegel(1965)]{spiegel1965laplace}
M.~R. Spiegel.
\newblock \emph{Laplace transforms}.
\newblock McGraw-Hill New York, 1965.

\bibitem[Yang and Salman(2019)]{yang2019fine}
G.~Yang and H.~Salman.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
D.~Zou, Y.~Cao, D.~Zhou, and Q.~Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
