\newif\ifarxiv
\arxivtrue
\ifarxiv
\documentclass[11pt]{article}
\usepackage[paper=letterpaper,margin=1in]{geometry}
\usepackage[numbers,sort,square]{natbib}
\else
\documentclass{article} %
\usepackage{iclr2021_conference,times}
\iclrfinalcopy
\fi

\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\renewcommand \thepart{}
\renewcommand \partname{}
\input{lin_preamble}

\newcommand{\g}{\kappa_1}
\newcommand{\hlap}{\cH_{\textnormal{Lap}}}
\newcommand{\klap}{K_{\textnormal{Lap}}}
\newcommand{\kexp}{K_{\textnormal{exp}}}
\renewcommand{\i}{\mathbf{i}}
\newcommand{\la}{\mathscr{L}}

\newcommand{\sx}[1]{{\textcolor{blue}{\bf [SX: #1]}}}



\title{Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS }
\ifarxiv
\author{Lin Chen\thanks{Simons Institute for the Theory of Computing,  University of California, Berkeley. E-mail: \href{mailto:lin.chen@berkeley.com}{lin.chen@berkeley.edu}.
 } \and Sheng Xu\thanks{Department of Statistics and Data Science, Yale University. Email: 
 \href{mailto:sheng.xu@yale.edu}{sheng.xu@yale.edu}.
 }
 }
 \else
\author{Lin Chen\\
Simons Institute for the Theory of Computing \\
University of California, Berkeley\\
\href{mailto:lin.chen@berkeley.com}{lin.chen@berkeley.edu}
\And Sheng Xu \\
Department of Statistics and Data Science\\
Yale University\\
\href{mailto:sheng.xu@yale.edu}{sheng.xu@yale.edu}
}
 \fi
\date{}


\begin{document}

\maketitle 

\doparttoc %
\faketableofcontents %

\begin{abstract}
    We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions, when both kernels are restricted to the sphere $\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS, when it is restricted to the sphere $\mathbb{S}^{d-1}$ and when it is defined on the entire $\mathbb{R}^d$. 
\end{abstract}

\section{Introduction}
In the past few years, one of the most seminal discoveries in the theory of neural networks is the neural tangent kernel (NTK)~\citep{jacot2018neural}. The gradient flow on a normally initialized, fully connected neural network with a linear output layer
in the infinite-width limit turns out to be equivalent to kernel regression with respect to the NTK (This statement does not necessarily hold for a non-linear output layer, because the NTK is non-constant~\citep{liu2020toward}). Through the NTK, theoretical tools from kernel methods were introduced to the study of deep overparametrized neural networks. Theoretical results were thereby established regarding the convergence~\citep{allen2019convergence,du2018gradient,du2019gradient,zou2020gradient}, generalization~\citep{cao2019generalization,arora2019fine}, and loss landscape~\citep{kuditipudi2019explaining} of overparametrized neural networks in the NTK regime. 

While NTK has proved to be a powerful theoretical tool, a recent work \citep{geifman2020similarity} posed an important question whether the NTK is significantly different from our repertoire of standard kernels. Prior work provided empirical evidence that supports a negative answer. For example, \citet{belkin2018understand} showed experimentally that the Laplace kernel and neural networks had similar performance in fitting random labels. In the task of speech enhancement, exponential power kernels $
\kexp^{\gamma,\sigma}(x,y) = e^{-\|x-y\|^\gamma/\sigma}$, which include the Laplace kernel as a special case, outperform deep neural networks with even shorter training time~\citep{hui2019kernel}. The experiments in \citep{geifman2020similarity} also exhibited similar performance of the Laplace kernel and the NTK. 

The expressive power of a positive definite kernel can be characterized by its associated reproducing kernel Hilbert space (RKHS)~\citep{saitoh2016theory}. 
The work \citep{geifman2020similarity} considered the RKHS of the kernels restricted to the sphere $\bS^{d-1} \triangleq  \{x\in \bR^d \mid \|x\|_2=1\}$ and presented a partial answer to the question by showing the following subset inclusion relation \[
\cH_{\textnormal{Gauss}}(\bS^{d-1}) \subsetneq \hlap(\bS^{d-1}) = \cH_{N_1}(\bS^{d-1}) \subseteq \cH_{N_k}(\bS^{d-1})\,,
\] 
where the four spaces denote the RKHS associated with the Gaussian kernel, Laplace kernel, the NTK of two-layer and $(k+1)$-layer ($k\ge 1$) fully connected neural networks, respectively. All four kernels are restricted to $\bS^{d-1}$. 
However, the relation between $\hlap(\bS^{d-1})$ and $\cH_{N_k}(\bS^{d-1})$ remains open in \citep{geifman2020similarity}.

We make a final conclusion on this problem and show that the RKHS of the Laplace kernel and the NTK with any number of layers have the same set of functions, when they are both restricted to $\bS^{d-1}$. In other words, we prove the following theorem. 
\begin{restatable}{theorem}{main}\label{thm:main}
Let $\hlap(\bS^{d-1})$ and $\cH_{N_k}(\bS^{d-1})$ be the RKHS associated with the Laplace kernel $\klap(x,y) = e^{-c\|x-y\|}$ ($c>0$) and the neural tangent kernel of a $(k+1)$-layer fully connected ReLU network. Both kernels are restricted to the sphere $\bS^{d-1}$. Then the two spaces include the same set of functions:
\[
\hlap(\bS^{d-1}) = \cH_{N_k}(\bS^{d-1}),\quad \forall k\ge 1\,.
\]
\end{restatable}



Our second result is that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS, both when it is restricted to the sphere $\bS^{d-1}$ and when it is defined on the entire $\bR^d$. 

\begin{theorem}\label{thm:exp-rkhs}
Let $\cH_{\kexp^{\gamma,\sigma}}(\bS^{d-1})$ and $\cH_{\kexp^{\gamma,\sigma}}(\bR^d)$ be the RKHS associated with the exponential power kernel $\kexp^{\gamma,\sigma}(x,y) = \exp\left(-\frac{\|x-y\|^\gamma}{\sigma}\right)$ ($\gamma,\sigma>0$) when it is restricted to the unit sphere $\bS^{d-1}$ and defined on the entire $\bR^d$, respectively. Then we have the following RKHS inclusions:
\begin{compactenum}[(1)]
\item If $0<\gamma_1< \gamma_2 < 2$, \[
\cH_{\kexp^{\gamma_2,\sigma_2}}(\bS^{d-1}) \subseteq \cH_{\kexp^{\gamma_1,\sigma_1}}(\bS^{d-1})\,.
\]\label{it:exp-s}
\item If $0<\gamma_1< \gamma_2 < 2$ are rational, 
 \[\cH_{\kexp^{\gamma_2,\sigma_2}}(\bR^d) \subseteq \cH_{\kexp^{\gamma_1,\sigma_1}}(\bR^d)\,.\]\label{it:exp-r}
\end{compactenum}
\end{theorem}

If it is restricted to the unit sphere, the RKHS of the exponential power kernel with $\gamma<1$ is even larger than that of NTK.
This result partially explains the observation in \citep{hui2019kernel} that the best performance is attained by a highly non-smooth exponential power kernel with $\gamma < 1$. \citet{geifman2020similarity} applied the exponential power kernel and the NTK to classification and regression tasks on the UCI dataset and other large scale datasets.  Their experiment results also showed that the exponential power kernel slightly outperforms the NTK. 


\subsection{Further Related Work}

\citet{minh2006mercer} showed the complete spectrum of the polynomial and Gaussian kernels on $\bS^{d-1}$. They also gave a recursive relation for the eigenvalues of the polynomial kernel on the hypercube $\{-1,1\}^d$. 
Prior to the NTK~\citep{jacot2018neural}, \citet{cho2009kernel} presented a pioneering study on kernel methods for neural networks.
\citet{bach2017breaking} studied the eigenvalues of positively homogeneous activation functions of the form $\sigma_{\alpha}(u) = \max\{u,0\}^\alpha$ (e.g., the ReLU activation when $\alpha=1$) in their Mercer decomposition with Gegenbauer polynomials. Using the results in \citep{bach2017breaking},
\citet{bietti2019inductive} analyzed the two-layer NTK and its RKHS in order to  investigate the inductive bias in the NTK regime.  They studied the Mercer decomposition of two-layer NTK with ReLU activation on $\bS^{d-1}$ and characterized the corresponding RKHS by showing the asymptotic decay rate of the eigenvalues in the Mercer decomposition with Gegenbauer polynomials. 
In their derivation of a more concise expression of the ReLU NTK, they used the calculation of \citep{cho2009kernel} on arc-cosine kernels of degree $0$ and $1$. 
\citet{cao2019towards} improved the eigenvalue bound for the $k$-th eigenvalue derived in \citep{bietti2019inductive} when $d\gg k$. 
\citet{geifman2020similarity} used the results in \citep{bietti2019inductive} and considered the two-layer ReLU NTK with bias $\beta$ initialized with zero, rather than initialized with a normal distribution \citep{jacot2018neural}. However, neither \citep{bietti2019inductive} nor \citep{geifman2020similarity} went beyond two layers when they tried to characterize the RKHS of the ReLU NTK. This line of work \citep{bach2017breaking,bietti2019inductive,geifman2020similarity} is closely related to the Mercer decomposition with spherical harmonics. Interested readers are referred to \citep{atkinson2012spherical} for spherical harmonics on the unit sphere. The concurrent work \citep{bietti2020deep} analyzed the eigenvalues of the ReLU NTK. 

\citet{arora2019exact} presented a dynamic programming algorithm that computes convolutional NTK with ReLU activation. \citet{yang2019fine} analyzed the spectra of the conjugate kernel (CK) and NTK on the boolean cube. 
\citet{fan2020spectra} studied the spectrum of the gram matrix of training samples under the CK and NTK and showed that their eigenvalue distributions converge to a deterministic limit. The limit depends on the eigenvalue distribution of the training samples. 



\section{Preliminaries}

Let $\bC$ denote the set of all complex numbers and write $\i\triangleq \sqrt{-1}$. For $z\in \bC$, write $\Re z$, $\Im z$, $\arg z\in (-\pi,\pi]$ for its real part, imaginary part, and argument, respectively. Let $\bH^+ \triangleq \{z\in \bC \mid \Im z>0\}$ denote the upper half-plane and $\bH^- \triangleq \{z\in \bC \mid \Im z<0\}$ denote the lower half-plane. Write $B_z(r)$ for the open ball $\{w\in \bC \mid  |z-w|<r\}$ and $\bar{B}_z(r)$ for the closed ball $\{w\in \bC \mid  |z-w|\leq r\}$.

Suppose that $f(z)$ has a power series representation $f(z)=\sum_{n\ge 0} a_n z^n$ around $0$. Denote $[z^n]f(z)\triangleq a_n$ to be the coefficient of the $n$-th order term. 

For two sequences $\{a_n\}$ and $\{b_n\}$, write $a_n\sim b_n$ if $\lim_{n\to \infty} \frac{a_n}{b_n} = 1$. Similarly, for two functions $f(z)$ and $g(z)$, write $f(z)\sim g(z)$ as $z\to z_0$ if $\lim_{z\to z_0} \frac{f(z)}{g(z)}=1$. We also use big-$O$ and little-$o$ notation to characterize asymptotics.  

Write $\la\{f(t)\}(s) \triangleq \int_0^\infty f(t)e^{-st}dt$ for the Laplace transform of a function $f(t)$. The inverse Laplace transform of $F(s)$ is denoted by $\la^{-1}\{F(s)\}(t)$. 


\subsection{Positive Definite Kernels}
For any positive definite kernel function $K(x,y)$ defined for $x,y\in E$, denote $\cH_K(E)$ its associated reproducing kernel Hilbert space (RKHS). For any two positive definite kernel functions $K_1$ and $K_2$, we write $K_1  \preccurlyeq K_2$ if $K_2-K_1$ is a positive definite kernel. For a complete review of results on kernels and RKHS, please see \citep{saitoh2016theory}.

We will study positive definite zonal kernels on the sphere $\bS^{d-1} = \{x\in \bR^d  \mid  \|x\|=1\}$. 
For a zonal kernel $K(x,y)$, there exists a real function $\Tilde{K}:[-1,1]\to \bR$ such that $K(x,y)=\Tilde{K}(u)$, where $u = x^\top y$. 
We abuse the notation and use $K(u)$ to denote $\Tilde{K}(u)$, i.e., $K(u)$ here is real function on $[-1,1]$. 

In the sequel, we introduce two instances of the positive definite kernel that this paper will investigate.

\paragraph{Laplace Kernel}
The Laplace kernel $\klap(x,y) = e^{-c\|x-y\|}$ with $c>0$ restricted to the sphere $\bS^{d-1}$ is given by $
\klap(x,y) = e^{-c\sqrt{2(1-x^\top y)}} = e^{-\Tilde{c}\sqrt{1-u}} \triangleq \klap(u)
$,
where by our convention $u=x^\top y$ and $\Tilde{c}\triangleq \sqrt{2}c>0$\,. We denote its associated RKHS by $\hlap$. 

\paragraph{Exponential Power Kernel}
The exponential power kernel~\citep{hui2019kernel} with $\gamma>0$ and $\sigma>0$ is given by $
\kexp^{\gamma,\sigma}(x,y) = \exp\left(-\frac{\|x-y\|^\gamma}{\sigma}\right)
$.
If $x$ and $y$ are restricted to the sphere $\bS^{d-1}$, we have $
\kexp^{\gamma,\sigma}(x,y) =  \exp\left(-\frac{(2(1-x^\top y))^{\gamma/2}}{\sigma}\right)
$.

\paragraph{Neural Tangent Kernel}
Given the input $x\in \bR^{d}$ (we define $d_0\triangleq d$) and parameter $\theta$, this paper considers the following network model with $(k+1)$ layers
\begin{equation}\label{eq:net}
    \begin{split}
& f_{\theta}(x)\\ ={}& 
w^\top \sqrt{\frac{2}{d_k}} \sigma\left( W_{k} \sqrt{\frac{2}{d_{k-1}}} \sigma\left( \dots \sqrt{\frac{2}{d_2}}\sigma\left(  W_2 \sqrt{\frac{2}{d_1}}\sigma\left(W_1 x + \beta b_1\right) + \beta b_2 \right) \dots \right) + \beta b_k \right) + \beta b_{k+1} \,,
\end{split}
\end{equation}
where the parameter $\theta$ encodes $W_l\in \bR^{d_l\times d_{l-1}}$,  $b_l\in \bR^{d_l}$ ($l=1,\dots,k$), $w\in \bR^{d_k}$, and $b_{k+1} \in \bR$. 
The weight matrices $W_1,\dots,W_{k},w$ are initialized with $\cN(0,I)$ and the biases $b_1,\dots,b_{k+1}$ are initialized with zero, where $\cN(0,I)$ is the multivariate standard normal distribution. 
The activation function is chosen to be the ReLU function $\sigma(x)\triangleq \max\{x,0\}$. 


\citet{geifman2020similarity} and \citet{bietti2019inductive} presented the following recursive relations of the NTK $N_k(x,y)$ of the above ReLU network \eqref{eq:net}:

\begin{equation}\label{eq:recursive}
\begin{split}
    \Sigma_k(x,y) ={}& \sqrt{\Sigma_{k-1}(x,x)\Sigma_{k-1}(y,y)}\kappa_1\left(
    \frac{\Sigma_{k-1}(x,y)}{\sqrt{\Sigma_{k-1}(x,x)\Sigma_{k-1}(y,y)}}\right)\\
    N_k(x,y) ={}& \Sigma_k(x,y) + N_{k-1}(x,y)\kappa_0\left(\frac{\Sigma_{k-1}(x,y)}{\sqrt{\Sigma_{k-1}(x,x)\Sigma_{k-1}(y,y)}}\right) + \beta^2\,,
\end{split}
\end{equation}
 where $\kappa_0$ and $\g$ are the arc-cosine kernels of degree 0 and 1 \citep{cho2009kernel} given by \[
\kappa_0(u) = \frac{1}{\pi}(\pi-\arccos(u)),\quad \kappa_1(u) = \frac{1}{\pi}\left( u\cdot (\pi-\arccos(u)) + \sqrt{1-u^2} \right)\,.
\] 
The initial conditions are \begin{equation}\label{eq:init-cond}
    N_0(x,y) = u + \beta^2,\quad \Sigma_0(x,y) = u\,,
\end{equation}
where $u=x^\top y$ by our convention. 

The NTKs defined in \citep{bietti2019inductive} and \citep{geifman2020similarity} are slightly different. There is no bias term $\beta^2$ in \citep{bietti2019inductive}, while the bias term appears in \citep{geifman2020similarity}. We adopt the more general setup with the bias term. 

\begin{lemma}[Proof in \cref{sec:proof-sigma-1}]\label{lem:sigma-1}
$\Sigma_k(x,x) = 1$ for any $x\in \bS^{d-1}$ and $k\ge 0$.
\end{lemma}

\cref{lem:sigma-1} simplifies \eqref{eq:recursive} and gives 
\begin{equation}\label{eq:simplified-recursion}
\Sigma_k(u) ={} \g^{(k)}(u)\,, \qquad
N_k(u) ={} \g^{(k)}(u) + N_{k-1}(u)\kappa_0(\g^{(k-1)}(u)) + \beta^2\,,
\end{equation}
where $\g^{(k)}(u)\triangleq  \underbrace{\g( \g ( \cdots \g( \g}_{k}(u))\cdots ))$ is the $k$-th iterate of $\g(u)$. For example, $\kappa_1^{(0)}(u)=u$, $\kappa_1^{(1)}(u)=\kappa_1(u)$ and $\kappa_1^{(2)}(u) = \kappa_1(\kappa_1(u))$. We present a detailed derivation of \eqref{eq:simplified-recursion} in \cref{sec:proof-eq4}. 



\section{Results on Neural Tangent Kernel}

In this section, we present an overview of our proof for \cref{thm:main}. 
Since \citep{geifman2020similarity} showed $\hlap(\bS^{d-1})\subseteq \cH_{N_k}(\bS^{d-1})$, it suffices to prove the reverse inclusion $\cH_{N_k}(\bS^{d-1}) \subseteq \hlap(\bS^{d-1})$. We then relate positive definite kernels with their RKHS according to the following lemma.

\begin{lemma}[{\cite[p.~354]{aronszajn1950theory} and \cite[Theorem 2.17]{saitoh2016theory}}]\label{lem:inclusion}
Let $K_1,K_2:\Omega\times \Omega \to \bC$ be two positive definite kernels. Then the Hilbert space $\cH_{K_1}$ is a subset of $\cH_{K_2}$ if and only if there exists some constant $\gamma>0$ such that \[
K_1  \preccurlyeq \gamma^2 K_2\,.
\]
\end{lemma}

\cref{lem:inclusion} implies that in order to show $\cH_{N_k}(\bS^{d-1})\subseteq \hlap(\bS^{d-1})$, it suffices to show $\gamma^2 \klap - N_k$ is a positive definite kernel for some $\gamma>0$. Note that both $\klap$ and $N_k$ are positive definite kernels on the unit sphere. Then the Maclaurin series of $\klap(u)$ and $N_k(u)$ have all non-negative coefficients by the classical approximation theory; see \cite[Theorem 2]{schoenberg1942positive}, \cite{bingham1973positive}, and \cite[Chapter 17]{cheney2009course}. Conversely, if the Maclaurin series of $K(u)$ have all non-negative coefficients, $K(x,y) = K(x^\top y)$ is a positive definite kernel on the unit sphere. To be precise, we have the following lemma.
\begin{lemma}[\cite{schoenberg1942positive,bingham1973positive}]
Suppose that $K(x,y) = f(x^\top y)$ where $x,y\in \bS^{d-1}$ and $f$ is continuous on $[-1,1]$.\footnote{When $x$ and $y$ live on the unit sphere (i.e., $x^{\top}x=y^{\top}y=1$), their inner product $x^\top y$ can be any real number in $[-1,1]$.} Then $K$ is a positive definite kernel on $\bS^{d-1}$ for every $d$ if and only if $f(u)=\sum_{k=0}^\infty a_k u^k$, in which $a_k\ge 0$ and $\sum_{k=0}^\infty a_k < \infty$.
\end{lemma}
Thus, we turn to show that there exists $\gamma>0$ such that $\gamma^2 [z^n]\klap(z) \ge [z^n]N_k(z)$ holds for every $n\ge 0$.  

Exact calculation of the asymptotic rate of the Maclaurin coefficients is intractable for $N_k$ due to its recursive definition. Instead, we apply singularity analysis tools in analytic combinatorics. We refer the readers to \citep{flajolet2009analytic} for a systematic introduction. We treat all (zonal) kernels, $\klap(u)$, $N_k(u)$, $\kappa_0(u)$, and $\g(u)$, as complex functions of variable $u\in \bC$. To emphasize, we use $z\in \bC$ instead of $u$ to denote the variable.   The theory of analytic combinatorics states that the asymptotic of the coefficients of the Maclaurin series is determined by the local nature of the complex function at its dominant singularities (i.e., the singularities closest to $z=0$). 

To apply the methodology from \citep{flajolet2009analytic}, we introduce some additional definitions. For $R>1$ and $\phi\in (0,\pi/2)$, the \emph{$\Delta$-domain} $\Delta(\phi,R)$ is defined by \[
\Delta(\phi,R) \triangleq \{ z\in \bC \mid |z|<R, z\ne 1, |\arg(z-1)|>\phi \}\,.
\]
For a complex number $\zeta\not=0$, a $\Delta$-domain at $\zeta$ is the image by the mapping $z\mapsto \zeta z$ of $\Delta(\phi,R)$ for some $R>1$ and $\phi\in (0,\pi/2)$. A function is $\Delta$-analytic at $\zeta$ if it is analytic on a $\Delta$-domain at $\zeta$. 

Suppose the function $f(z)$ has only one dominant singularity and  without loss of generality assume that it lies at $z=1$. We then have the following lemma.

\begin{lemma}[{\cite[Corollary VI.1]{flajolet2009analytic}}]\label{lem:single-singularity}
 If $f$ is  $\Delta$-analytic at its dominant singularity $1$ and  \[
f(z)\sim (1-z)^{-\alpha},\quad {\rm as}~ z\to 1, z\in\Delta
\]
with $\alpha\notin \{0,-1,-2,\dots \}$, we have \[
[z^n]f(z) \sim \frac{n^{\alpha-1}}{\Gamma(\alpha)}\,.
\]
\end{lemma}
If the function has multiple dominant singularities, the influence of each singularity is added up (See \cite[Theorem VI.5]{flajolet2009analytic} for more details). Careful singularity analysis then gives
\[
[z^n]\klap(z)\sim C_1 n^{-3/2}, \quad [z^n]N_k(z)\leq C_2 n^{-3/2}\,,
\]
for some positive constants $C_1,C_2>0$. We refer to \cref{sec:AsympMac} and \cref{sec:NTK2} for more detailed steps. They are indeed of the same order of decay rate $n^{-3/2}$, which implies that such $\gamma$ exists. This shows $\cH_{N_k}(\bS^{d-1})\subseteq \hlap(\bS^{d-1})$. 








\subsection{$\Delta$-Analyticity of Neural Tangent Kernels}

We present the $\Delta$-analyticity of the NTKs here. In light of \eqref{eq:simplified-recursion}, the NTKs $N_k$ are compositions of arc-cosine kernels $\kappa_0$ and $\g$. 
We analytically extend $\kappa_0$ and $\kappa_1$ to a complex function of a complex variable $z\in \bC$. Both complex functions $\arccos(z)$ and $\sqrt{1-z^2}$ have branch points at $z=\pm 1$. Therefore, the branch cut of $\kappa_0(z)$ and $\kappa_1(z)$ is $[1,\infty)\cup (-\infty,-1]$. They have a single-valued analytic branch on \begin{equation}\label{eq:branch}
D = \bC\setminus [1,\infty) \setminus (-\infty,-1]\,.
\end{equation}
On this branch, we have \begin{align*}
\kappa_0(z) ={} & \frac{ \pi +\i\log(z+\i\sqrt{1-z^2})}{\pi}\,,\\
\g(z) ={} & \frac{1}{\pi}\left[ z\cdot \left( \pi +\i\log(z+\i\sqrt{1-z^2} \right) + \sqrt{1-z^2} \right]\,, 
\end{align*}
where we use the principal value of the logarithm and square root. We then show the dominant singularities of $\g^{(k)}(z)$ are $\pm 1$ and that $\g^{(k)}(z)$ is $\Delta$-analytic at $\pm 1$ for any $k\geq1$. We further have the following theorem on the $\Delta$-singularity for $N_k$. 

\begin{theorem}[Proof in \cref{sec:NTK1}]\label{thm:analytic_main}
  For each $k\geq 1$, the dominant singularities of $N_k$ are $\pm 1$. There exists $R_k>1$ such that $N_k$ is analytic on $\{z\in \bC~|~ |z|\le R_k\}\cap D$, where $D = \bC\setminus [1,\infty) \setminus (-\infty,-1] $.  
\end{theorem}









\subsection{Asymptotic Rates of Maclaurin Coefficients for $N_k$}\label{sec:AsympMac}
The following theorem demonstrates the asymptotic rates of Maclaurin coefficients for $N_k$. 

\begin{theorem}[Proof in \cref{sec:NTK2}]\label{thm:Kk}
The $n$-th order coefficient of the Maclaurin series of the $(k+1)$-layer NTK in \eqref{eq:recursive} satisfies $[z^n]N_k(z) = O(n^{-3/2})$.
\end{theorem}


In the proof of \cref{thm:Kk}, we show the following asymptotics \begin{align}
    N_k(z) ={}& (k+1)(z+\beta^2) -\left(\sqrt{2}(1+\beta^2)\frac{k(k+1)}{2\pi} + o(1)\right) \sqrt{1-z} \quad \textnormal{as~} z\to 1\,,\label{eq:N(1)}\\
    N_k(z) ={}& N_k(-1) + \left(\frac{\sqrt{2}(\beta^2-1)}{\pi} \prod_{j=1}^{k-1} \kappa_0(\g^{j}(-1)) + o(1)\right)\sqrt{1+z} \quad \textnormal{as~} z\to -1\,.\label{eq:N(-1)}
\end{align}

When $\beta=1$, the singularity at $z=-1$ will not provide a $\sqrt{1+z}$ term. The dominating term in \eqref{eq:N(-1)} is a higher power of $\sqrt{1+z}$. As a result, the contribution of the singularity at $-1$ to the Maclaurin coefficients is $o(n^{-3/2})$ and dominated by the contribution of the singularity at $1$. The singularity at $z=1$ provides a $\sqrt{1-z}$ term and thus contributes to $O(n^{-3/2})$ decay rate of $[z^n]N_k(z)$. In addition, from \eqref{eq:N(1)}, we deduce \begin{equation}\label{eq:coeff1}
  \frac{[z^n]N_k(z)}{n^{-3/2}} \sim -\frac{ 2\sqrt{2} k (k+1)}{(2 \pi ) \Gamma \left(-\frac{1}{2}\right)}
  = \frac{k (k+1)}{\sqrt{2} \pi ^{3/2}}\,.
\end{equation}

When $\beta\ne 1$, both singularities $\pm 1$ contribute $\Theta(n^{-3/2})$ to the Maclaurin cofficients. The contribution of $z=1$ is \begin{equation*}
-\frac{\sqrt{2} (1+\beta^2) k (k+1)}{2 \pi  \Gamma \left(-\frac{1}{2}\right)} n^{-3/2} =
\frac{\left(\beta ^2+1\right) k (k+1)}{2 \sqrt{2} \pi ^{3/2}} n^{-3/2}
\,.
\end{equation*}
The contribution of $z=-1$ is \[
\left(\frac{\sqrt{2}(\beta^2-1)}{\pi\Gamma(-1/2)} \prod_{j=1}^{k-1} \kappa_0(\g^{j}(-1))\right) n^{-3/2}
= \left(\frac{1-\beta^2}{\sqrt{2} \pi ^{3/2}}\prod_{j=1}^{k-1} \kappa_0(\g^{j}(-1)) \right)n^{-3/2}\,.
\]
Combining them gives \begin{equation}\label{eq:coeff-ne1}
\frac{[z^n]N_k(z)}{n^{-3/2}} \sim \frac{(\beta^2+1)k (k+1)}{2 \sqrt{2} \pi ^{3/2}} + (-1)^n\frac{1-\beta^2}{\sqrt{2} \pi ^{3/2}}\prod_{j=1}^{k-1} \kappa_0(\g^{j}(-1))\,.
\end{equation}




Based on \cref{thm:Kk}, we are ready to prove \cref{thm:main}. 

\begin{proof}
Let $\klap(z) = e^{-c\sqrt{1-z}}$, where $c>0$ is an arbitrary constant. We have $\cH_{\klap} = \hlap$.  The complex function $\klap$ is analytic on $\bC\setminus [1,\infty)$. As $z\to 1$, we have \[
\frac{\klap(z)-1}{-c} = \sqrt{1-z} + o(\sqrt{1-z}) \sim \sqrt{1-z} \,.
\]
By \cref{lem:single-singularity},
we obtain \begin{equation}\label{eq:asymp-klap}
[z^n]\klap(z) \sim \frac{c}{2\sqrt{\pi}} n^{-3/2}\,.
\end{equation}
Note that $[z^n]N_k(z) = O(n^{-3/2})$ from \cref{thm:Kk}. Therefore, there exists $\gamma>0$ such that $\gamma^2\cdot [z^n]\klap(z) - [z^n]N_k(z) > 0 $ for all $n\ge 0$. This further implies $\gamma^2 \klap(x^\top y) - N_k(x^\top y) $ 
is a positive definite kernel. According to 
 \cref{lem:inclusion}, we have $\cH_{N_k}(\bS^{d-1}) \subseteq \hlap(\bS^{d-1})$.
 Note that, due to \cite[Theorem 3]{geifman2020similarity}, we also have $\hlap(\bS^{d-1}) \subseteq \cH_{N_k}(\bS^{d-1})$.
 Therefore, for any $k\geq 1$, $\hlap(\bS^{d-1}) = \cH_{N_k}(\bS^{d-1})$.
 
 
\end{proof}



\section{Results on Exponential Power Kernel}
This section presents the proof of \cref{thm:exp-rkhs}. 
We first show part (\ref{it:exp-s}) below by singularity analysis. 

\begin{proof}[Proof of part (\ref{it:exp-s}) of \cref{thm:exp-rkhs}]
Recall that
the exponential power kernel restricted to the unit sphere with $\gamma>0$ and $\sigma>0$ is given by $
\kexp^{\gamma,\sigma}(x,y) = \exp\left(-\frac{\|x-y\|^\gamma}{\sigma}\right) = \exp\left(-\frac{(2(1-x^\top y))^{\gamma/2}}{\sigma}\right)
$.
Let us study the decay rate of the Maclaurin coefficients of $\kexp^{\gamma,\sigma}(z) \triangleq e^{-c(1-z)^{\gamma/2}}$, where $c = 2^{\gamma /2} / \sigma$. The dominant singularity lies at $z=1$. As $z\to 1$, we get \[
\kexp^{\gamma,\sigma}(z) = 1 - (c+o(1))(1-z)^{\gamma/2} \,.
\]
Applying \cref{lem:single-singularity} gives $
[z^n]\kexp^{\gamma,\sigma}(z) \sim \frac{cn^{-\gamma/2-1}}{-\Gamma(-\gamma/2)}
$.
Therefore, a smaller $\gamma$ results in a larger RKHS.
\end{proof}


Part (\ref{it:exp-r}) of \cref{thm:exp-rkhs} requires more technical preparation. Recall that $\la$ and $\la^{-1}$ denote the Laplace transform and inverse Laplace transform, respectively. We explicitly calculate the inverse Laplace transform $\la^{-1}\{\exp(-s^a)\}(t)$ using Bromwich contour integral and get the following lemma.

\begin{lemma}[Proof in \cref{sec:proof-series}]\label{lem:series}
For $a\in (0,1)$, $f(t)\triangleq \la^{-1}\{\exp(-s^a)\}(t)$ exists. Moreover, $f(t)$ is continuous in $-\infty < t< \infty$ and satisfies $f(0)=0$. If $t>0$, we have \begin{equation}\label{eq:series-repr}
f(t) = \frac{1}{\pi}\sum_{k=0}^\infty  \frac{(-1)^{k+1}\Gamma(ak+1)\sin(\pi ak)}{k! t^{ak+1}}\,.
\end{equation}
\end{lemma}

Based on the series representation \eqref{eq:series-repr}, we then analyze the asymptotic rate for $f(t)$ when $a$ is rational. Note that if $a\in (0,1)$, we have  $-\frac{1}{\Gamma(-a)}>0$. 
\begin{lemma}[Proof in \cref{sec:proof-tail}]\label{lem:tail}
Let $f(t)$ be as defined in \cref{lem:series}.
For $a = \frac{p}{q} \in (0,1)$ ($p$ and $q$ are co-prime), we have $ f(t) \sim -\frac{1}{t^{a+1}\Gamma(-a)}$ as $t\to +\infty$.
\end{lemma}

Thus, We have the following corollary for general exponential power kernel.

\begin{cor}\label{cor:tail}
For $a = \frac{p}{q} \in (0,1)$ ($p$ and $q$ are co-prime) and $\sigma>0$, 
$\la^{-1}\{\exp(-s^a/\sigma)\}(t)$ is  continuous in $t\in \bR$ and satisfies $\la^{-1}\{\exp(-s^a/\sigma)\}(0)=0$. Moreover, 
$\la^{-1}\{\exp(-s^a/\sigma)\}(t) \sim C t^{-a-1}$ as $t\to +\infty$, for some constant $C>0$.
\end{cor}
\begin{proof}
Use the property $\la^{-1} \{ F(cs) \}(t) = \frac{1}{c} f\left(\frac{t}{c}\right)$, where $c>0$ and $F(s) = \la\{f(t)\}(s) $.
\end{proof}

Before completing the proof for part (\ref{it:exp-r}), we need two additional lemmas from the classical approximation theory. Recall that a function $f(t)$ is \emph{completely monotone} if it is continuous on $[0,\infty)$, infinitely differentiable on $(0,\infty)$ and satisfies $(-1)^n\frac{d^n f(t)}{dt}\ge 0$ for every $n=0,1,2,\dots$ and $t>0$ \citep[Chapter~14]{cheney2009course}. 
\begin{lemma}[Schoenberg interpolation theorem {\cite[Theorem 1 of Chapter 15]{cheney2009course}}]\label{lem:schoenberg}
If $f$ is completely monotone but not constant on $[0,\infty)$, 
then for any $n$ distinct points $x_1,x_2,\dots,x_n$ in any inner-product space, the matrix $A_{ij} = f(\|x_i-x_j\|^2)$ is positive definite. 
\end{lemma}

\begin{lemma}[Bernstein-Widder {\cite[Theorem 1 of Chapter 14]{cheney2009course}}]\label{lem:bernstein}
A function $f:[0,\infty)\to [0,\infty)$ is completely monotone if and only if there is a nondecreasing bounded function $g$ such that $f(t) = \int_0^\infty e^{-st} dg(s)$. 
\end{lemma}

Now we are ready to prove part (\ref{it:exp-r}).

\begin{proof}[Proof of part (\ref{it:exp-r}) of \cref{thm:exp-rkhs}]
By \cref{lem:schoenberg} and \cref{lem:inclusion}, we need to show that 
\begin{equation}\label{eq:csquared-diff}
    c^2 \exp(-x^{\gamma_1/2}/\sigma_1) - \exp(-x^{\gamma_2/2}/\sigma_2)
\end{equation}
 is completely monotone but not constant on $[0,\infty)$ for some $c>0$. By \cref{lem:bernstein}, it suffices to check that \eqref{eq:csquared-diff} is the Laplace transform of a non-negative function on $[0,\infty)$. By \cref{cor:tail}, for rational $\gamma_1,\gamma_2\in (0,1]$, there exists $c>0$ such that \[
 c^2 \la^{-1}\{\exp(-x^{\gamma_1/2}/\sigma_1)\} - \la^{-1}\{\exp(-x^{\gamma_2/2}/\sigma_2)\}
 \]
 is continuous and positive on $[0,\infty)$, which completes the proof.
\end{proof}





\section{Numerical Results}
\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{.46\linewidth}
    \includegraphics[width=\linewidth]{pix/coeff.pdf}
    \caption{$\beta=1$}
    \end{subfigure}
    \begin{subfigure}[b]{.46\linewidth}
    \includegraphics[width=\linewidth]{pix/coeff0.pdf}
    \caption{$\beta=0$}
    \end{subfigure}
    \caption{We plot $[z^n]K(z)/n^{-3/2}$ versus $n$ for the Laplace kernel $\klap(u) = e^{-\sqrt{2(1-u)}}$ and NTKs $N_1,\dots,N_4$ with $\beta=0,1$. }
    \label{fig:coeff}
\end{figure}

\begin{table}[htb]
    \centering
    \begin{tabular}{c|l|l|l|l}
    \toprule
        \textbf{Kernel}  & $\frac{[z^{100}]K(z)}{100^{-3/2}}$ & \textbf{Theory} & $\frac{[z^{100}]K(z)}{100^{-3/2}}$ & \textbf{Theory} \\
       $K$ & ($\beta=1$) & ($\beta=1$)  & ($\beta = 0$) & ($\beta=0$)
        \\ \midrule
        $\klap$ & 0.28244 & $\frac{1}{2 \sqrt{\pi }} \approx 0.282095$ & &  \\
        $N_1$ & 0.261069 & $\frac{\sqrt{2}}{\pi ^{3/2}} \approx 0.253975 $ & 0.261069 & $\frac{\sqrt{2}}{\pi ^{3/2}}\approx 0.253975$\\
        $N_2$ & 0.776014 & $\frac{3 \sqrt{2}}{\pi ^{3/2}}\approx 0.761924$ &  0.457426 & $\frac{7}{2 \sqrt{2} \pi ^{3/2}} \approx 0.444455$\\
        $N_3$ & 1.54607 & $\frac{6 \sqrt{2}}{\pi ^{3/2}} \approx 1.52385$ &  0.821694 & $\frac{13 \pi -\arccos\left(\pi^{-1}\right)}{2 \sqrt{2} \pi ^{5/2}} \approx 0.800218$ \\
        $N_4$ & 2.56559 & $\frac{10 \sqrt{2}}{\pi ^{3/2}} \approx 2.53975 $ & 1.32472 & Equation~\eqref{eq:cell} $\approx 1.29531$
        \\
        \bottomrule
    \end{tabular}
    \caption{We report the numerical values of $ \frac{[z^{100}]K(z)}{100^{-3/2}}$ for the Laplace kernel $\klap(u) = e^{-\sqrt{2(1-u)}}$ and NTKs $N_1,\dots,N_4$ with $\beta=0,1$. These numerical values are the final values of the curves in \cref{fig:coeff}.   We present the theoretical prediction by the asymptotic of $[z^n]K(z)/n^{-3/2}$ alongside each numerical value. The choice of $\beta$ does not apply to the Laplace kernel. Therefore, we only show the results of the Laplace kernel in the columns for $\beta=1$ and leave blank the columns for $\beta=0$. }
    \label{tab:limit}
\end{table}

We verify the asymptotics of the Maclaurin coefficients of the Laplace kernel and NTKs through numerical results. 

\cref{fig:coeff} plots $\frac{[z^n]K(z)}{n^{-3/2}}$ versus $n$ for different kernels, including the Laplace kernel $\klap(u) = e^{-\sqrt{2(1-u)}}$ and NTKs $N_1,\dots,N_4$ with $\beta=0,1$. All curves converge to a constant as $n\to \infty$, which indicates that for every kernel $K(z)$ considered here, we have $[z^n]K(z) = \Theta(n^{-3/2})$. The numerical results agree with our theory in the proofs of \cref{thm:Kk} and \cref{thm:main}. 

Now we investigate the value of $[z^n]K(z)/n^{-3/2}$. \cref{tab:limit} reports $[z^{100}]K(z)/100^{-3/2}$ for the Laplace kernel and NTKs with $\beta=0,1$. These numerical values are the final values of the curves in \cref{fig:coeff}. The theoretical predictions are obtained through the asymptotic of $[z^n]K(z)/n^{-3/2}$, which we shall explain below. The theoretical prediction of $[z^{100}]N_4(z)/100^{-3/2}$ with $\beta=0$ is presented below due to the space limit in the table  \begin{equation}\label{eq:cell}
    \frac{20+\pi ^{-2}\left(\pi -\arccos\left(\pi^{-1}\right)\right) \left(\pi -\arccos\left(\frac{\sqrt{\pi ^2-1}+\pi -\arccos \left(\pi^{-1}\right)}{\pi ^2}\right)\right)}{2 \sqrt{2} \pi ^{3/2}} \approx 1.29531\,.
\end{equation}
We observe that the theoretical prediction by the asymptotic is close to the corresponding numerical value. There are two possible reasons that account for the minor discrepancy between them. First, the theoretical prediction reflects the situation for an infinitely large $n$ (so that the lower order terms become negligible), while $n=100$ is clearly finite. Second, the numerical results for the Maclaurin series are obtained by numerical Taylor expansion and therefore numerical errors could be present. 

In what follows, we explain how to obtain the theoretical predictions. First, \eqref{eq:asymp-klap} gives 
\ifarxiv
\[
[z^n]\klap(z)/n^{-3/2} \sim \frac{1}{2\sqrt{\pi}}\,.
\]
\else
$[z^n]\klap(z)/n^{-3/2} \sim \frac{1}{2\sqrt{\pi}}$.
\fi
As a result, the theoretical prediction for $[z^{100}]\klap(z)/100^{-3/2}$ is $\frac{1}{2\sqrt{\pi}}$. 
Now we explain the thereotical predictions for NTKs. 
When $\beta=1$, the theoretical prediction is given by \eqref{eq:coeff1}. 
We present it in the third column of \cref{tab:limit} for $N_1,\dots,N_4$. 
When $\beta=0$, we plug $\beta = 0$ into \eqref{eq:coeff-ne1} and obtain
\ifarxiv
\[
\frac{[z^n]N_k(z)}{n^{-3/2}} \sim \frac{k (k+1)}{2 \sqrt{2} \pi ^{3/2}} + \frac{(-1)^n}{\sqrt{2} \pi ^{3/2}}\prod_{j=1}^{k-1} \kappa_0(\g^{j}(-1))
\,.\]
\else
$
\frac{[z^n]N_k(z)}{n^{-3/2}} \sim \frac{k (k+1)}{2 \sqrt{2} \pi ^{3/2}} + \frac{(-1)^n}{\sqrt{2} \pi ^{3/2}}\prod_{j=1}^{k-1} \kappa_0(\g^{j}(-1))
$.
\fi
The above expression (when $n=100$ on the right-hand side) is the theoretical value presented in the fifth column of \cref{tab:limit} for NTKs. 

\section{Discussion}
Our result provides further evidence that the NTK is similar to the existing Laplace kernel. However, the following mysteries remain open. 
First, if we still restrict them to the unit sphere, do they have a similar learning dynamic when we perform kernelized gradient descent? 
Second, what is the behavior of the NTK and the Laplace kernel outside of $\bS^{d-1}$ and in the entire space $\bR^{d}$? Do they still share similarities in terms of the associated RKHS? If not, how far do they deviate from each other and is the difference significant? 
Third, this work along with \citep{bietti2019inductive,geifman2020similarity} focuses on the NTK with ReLU activation. It would be interesting to explore the influence of different activations upon the RKHS and other kernel-related quantities. We would like to remark that the ReLU NTK has a clean expression partly because the expectation over the Gaussian process in the general NTK can be computed exactly if the activation function is ReLU (which may not be true for other non-linearities, for example, it may require more work for sigmoid). 
Fourth, we showed that highly non-smooth exponential power kernels have an even larger RKHS than the NTK. It would be worthwhile comparing the performance of these non-smooth kernels and deep neural networks through more extensive experiments in a variety of machine learning tasks. 

Moreover, we show that a less smooth exponential power kernel leads to a larger RKHS and therefore greater expressive power. Its generalization capability is a related but different topic. Analyzing the generalization error requires more efforts in general. Researchers often use the RKHS norm to provide an upper bound for it. We will study its generalization in future work. 

\subsubsection*{Acknowledgements}
We gratefully acknowledge the support of the Simons Institute for the Theory of Computing.
We thank Peter Bartlett, Mikhail Belkin, Jason D.~Lee, 
and Iosif Pinelis for helpful discussions and thank Mikhail Belkin and Alexandre Eremenko for introducing to us the works \citep{hui2019kernel,liu2020toward} and \citep{flajolet2009analytic}, respectively.





























\newpage
\ifarxiv
\bibliographystyle{abbrvnat}
\else
\bibliographystyle{iclr2021_conference}
\fi
\bibliography{reference-list}

\newpage
\appendix
\part{Appendices}\label{Appendix}
\parttoc
\section{Proofs for Neural Tangent Kernel}\label{sec:NTK}

\subsection{Proof of \cref{lem:sigma-1}}\label{sec:proof-sigma-1}
\begin{proof}
We show it by induction. It holds when $k=0$ by the initial condition \eqref{eq:init-cond}. Assume that it holds for some $k\ge 0$, i.e., $\Sigma_k(x,x)=1$. Consider $k+1$. We have \[
\Sigma_{k+1}(x,x) = \kappa_1(\Sigma_{k}(x,x)) = \kappa_1(1) = 1\,.
\]
\end{proof}

\subsection{Proof of Equation~\eqref{eq:simplified-recursion}}\label{sec:proof-eq4}
\begin{proof}
We plug $\Sigma_k(x,x)=1$ into \eqref{eq:recursive} and obtain \begin{align*}
    \Sigma_k(x,y) ={}& \kappa_1(\Sigma_{k-1}(x,y)) \\
    N_k(x,y) ={}& \Sigma_k(x,y) + N_{k-1}(x,y)\kappa_0(\Sigma_{k-1}(x,y)) + \beta^2\,.
\end{align*}
Recall $\Sigma_0(x,y) = u$. By induction, we get \[
\Sigma_k(u) = \kappa_1^{(k)}(u)\,,
\]
where $\g^{(k)}(u)\triangleq \g^{(k)}(u) = \underbrace{\g( \g ( \cdots \g( \g}_{k}(u))\cdots ))$ is the $k$-th iterate of $\g(u)$. Then it follows \[
N_k(u) = \g^{(k)}(u) + N_{k-1}(u)\kappa_0(\g^{k-1}(u)) + \beta^2\,.
\]
\end{proof}

\subsection{Proof of \cref{thm:analytic_main}}\label{sec:NTK1}

\cref{lem:expansion-one} and \cref{lem:expansion-minus-one} demonstrate that $\pm 1$ are indeed singularities and analyze the asymptotics for $\kappa_1^{(k)}$ as $z$ tends to $\pm 1$, respectively. Our calculation is inspired by \citet{pinelis}, which only considers $k=2$.

\begin{lemma}\label{lem:expansion-one}
For every $k\ge 1$, there exists $c_k(z)$ such that \[\g^{(k)}(z) = z + c_k(z)(1-z)^{3/2}\,,\]
where
\[\lim_{z\to 1} c_k(z)=\frac{2\sqrt{2}k }{3\pi}\,.\]

\end{lemma}

\begin{proof}
We prove by induction on $k$. We first prove the statement for $k=1$. Let $z=1-r e^{\i\theta}$. Taylor's theorem around $1$ with integral form of remainder gives \[
\g(z) = z + \int_{\gamma} \frac{z-w}{\pi  \sqrt{1-w^2}} dw\,,
\]
where $\gamma:[0,1]\to\bC$ is the simple straight line connecting $1$ and $z$ taking the form $\gamma(t)=1-tre^{\i \theta}$. It follows
\begin{align*}
\g(z) =& z + \int_{\gamma} \frac{z-w}{\pi  \sqrt{1-w}}\cdot\frac{1}{\sqrt{1+w}} dw\\
=&z + \int_{\gamma} \frac{z-w}{\pi \sqrt{2} \sqrt{1-w}} dw+ \int_{\gamma} \frac{z-w}{\pi \sqrt{2} \sqrt{1-w}} \cdot(\frac{\sqrt{2}}{\sqrt{1+w}}-1) dw\,.
\end{align*}
Since \[\int_\gamma \frac{z-w}{\sqrt{1-w}} dw = \frac{2}{3} \sqrt{1-w} (w-3 z+2)\bigg\rvert_{w=1}^{w=z} = \frac{4}{3} (1-z)^{3/2} 
\,,
\]
we have
\begin{align*}
    \g(z) 
=z + \frac{2\sqrt{2}}{3\pi}(1-z)^{3/2}+ \int_{\gamma} \frac{z-w}{\pi \sqrt{2} \sqrt{1-w}} \cdot(\frac{\sqrt{2}}{\sqrt{1+w}}-1) dw\,.
\end{align*}
We then turn to show
\begin{align*}
    \lim_{z\to 1} \Big\{(1-z)^{-3/2}\cdot\int_{\gamma} \frac{z-w}{\pi \sqrt{2} \sqrt{1-w}} \cdot(\frac{\sqrt{2}}{\sqrt{1+w}}-1) dw\Big\}=0\,.
\end{align*}
Direct calculation gives 
\begin{align*}
    &\lim_{z\to 1} \Big\{(1-z)^{-3/2}\cdot\int_{\gamma} \frac{z-w}{ \sqrt{1-w}} \cdot(\frac{\sqrt{2}}{\sqrt{1+w}}-1) dw\Big\}\\
    ={}&\lim_{r\to 0}\Big\{(re^{\i\theta})^{-3/2}\cdot\int_0^1 \frac{(1-t)r^2 e^{2\i\theta}}{\sqrt{tre^{\i\theta}}}(\frac{\sqrt{2}}{\sqrt{2-tre^{\i\theta}}}-1)dt\Big\}\\
    ={}&\lim_{r\to 0}\Big\{\int_0^1 \frac{1-t}{\sqrt{t}}(\frac{1}{\sqrt{1-tre^{\i\theta}/2}}-1)dt\Big\}\\
    ={}& 0\,.
\end{align*}
Therefore, there exists $c_1(z)$ such that $\lim_{ z\to 1} c_1 (z) = \frac{2\sqrt{2}}{3\pi} \ne 0$ and \[
\g(z) = z + c_1(z)(1-z)^{3/2}\,.
\]

Next, assume that the desired equation holds for some $k\ge 1$. 
We then have \begin{align*}
    \g^{(k+1)}(z) ={}& \g(\g^{(k)}(z))\\
    ={}& \g(z+c_k(z)(1-z)^{3/2}) \\
    ={}& z + c_k(z)(1-z)^{3/2} + c_1\left(\g^{(k)}(z)\right)\cdot \left(1-z-c_k(z)(1-z)^{3/2}\right)^{3/2}\\
    ={}& z + c_{k+1}(z)(1-z)^{3/2}\,,
\end{align*}
where $c_{k+1}(z) \sim c_k(z) + c_1(k_1^{(k)}(z))$. Recall that when $ z\to 1$, we have $\g^{(k)}(z)\to 1$ as well. Therefore we deduce \[
\lim_{ z\to 1}c_{k+1}(z) = \lim_{ z\to 1} c_k(z) + \lim_{ z\to 1} c_1(k_1^{k}(z)) = \frac{2\sqrt{2}k}{3\pi} \ne 0\,.
\] 
\end{proof}


\begin{lemma}\label{lem:expansion-minus-one}
For every $k\ge 1$, there exist $a_k\in\bR$ and a complex function $b_k(z)$ such that \[\g^{(k)}(z) = a_k + b_k(z)(z+1)^{3/2}\,,\]
where \[a_k=\g^{(k)}(-1) ~~~{\rm and}~~~\lim_{z\to -1}b_k(z) = \frac{2\sqrt{2}}{3\pi} \prod_{j=1}^{k-1}\g'(\g^{(j)}(-1)) >0\,. \] 
\end{lemma}

\begin{proof}
We prove by induction on $k$. We first prove the statement for $k=1$. Let $z=-1+re^{\i\theta}$. Taylor's theorem around $-1$ with integral form of remainder gives \[
\g(z) = \int_\gamma \frac{z-w}{\pi\sqrt{1-w^2}}dw\,.
\]
where $\gamma:[0,1]\to\bC$ is the simple straight line connecting $-1$ and $z$ taking the form $\gamma(t)=-1+tre^{\i \theta}$. Similar arguments as in the proof of Lemma \ref{lem:expansion-one} give
\[\g(z)=b_1(z)(z+1)^{3/2}\,,\]
where $\lim_{z\to -1} b_1(z) = \frac{2\sqrt{2}}{3\pi} $.

Next, assume that the desired equation holds for some $k\geq 1$. Define $h_k\triangleq \g^{(k)}(-1)$. Since $\g$ is strictly increasing on $[-1,1]$,  $\g(-1)=0$ and $\g(1)=1$, we have $h_1 = 0$ and $h_k\in (0,1)$ for all $k>1$. Expanding $\g$ around $h_k$ yields
\[
\g(z) = \g(h_k) + p(z)(z-h_k) = h_{k+1} + p(z)(z-h_k)\,, 
\]
where $\lim_{z\to h_k}p(z)  = \g'(h_k)$. 
It follows that \begin{align*}
    \g^{k+1}(z) ={}& \g(a_k+b_k(z)(z+1)^{3/2}) 
    = h_{k+1} + p(\g^{(k)}(z))(a_k+b_k(z)(z+1)^{3/2}-h_k)\\
    ={}& a_{k+1} + b_{k+1}(z)(z+1)^{3/2}\,,
\end{align*}
where $a_{k+1} = h_{k+1} + \g'(h_k)(a_k-h_k)$ and $\lim_{z\to -1}b_{k+1}(z) = \g'(h_k)\lim_{z\to -1}b_k(z) $.
By induction, we can show that $a_k=h_k$ for all $k\ge 1$. 
Since $\g'$ is strictly increasing on $[-1,1]$, $\g'(-1)=0$, and $\g'(1)=1$, we have $\g'(h_k)\ge \g'(0)>0$. As a result, \[\lim_{z\to -1}b_{k+1}(z) = \frac{2\sqrt{2}}{3\pi} \prod_{j=1}^{k}\g'(\g^{(j)}(-1)) >0\,.\]

\end{proof}


In the sequel, we show that $\pm 1$ are the only dominant singularities of $\g^{(k)}$ and $\g^{(k)}$ is $\Delta$-analytic at $\pm 1$ (\cref{lem:analytic}). 

\begin{lemma}\label{lem:oct}
For any $z\in \bC$ with $\arg z \in (0,\pi/4)$, $\g(z)\in \bH^+$. For any $z\in \bC$ with $\arg z \in (-\pi/4,0)$, $\g(z)\in \bH^-$.
\end{lemma}
\begin{proof}
The second part of the statement follows from the first according to
the reflection principle. We only prove the first part here. Let $z=r e^{\i\theta}$ with $\theta\in (0,\pi/4)$. Taylor's theorem with integral form of the remainder and direct calculation give \[
\g(z) = \g(0) + \g'(0)z + \int_{\gamma} (z-w)\g''(w)dw = \frac{1}{\pi} + \frac{1}{2}z + \int_{\gamma} \frac{z-w}{\pi\sqrt{1-w^2}}dw\,,
\]
where $\gamma:[0,1]\to\bC$ is the simple straight line connecting $0$ and $z$ taking the form $\gamma(t)=t r e^{\i\theta}$. Then we have \[
\int_{\gamma} \frac{z-w}{\pi\sqrt{1-w^2}}dw = r^2 e^{2\i\theta}\int_0^1 \frac{1-t}{\pi\sqrt{1-r^2t^2 e^{2\i\theta}}} dt = e^{2\i\theta}\int_0^r \frac{r-t}{\pi\sqrt{1-t^2 e^{2\i\theta}}} dt\,.
\]
Since $\theta \in (0,\pi/4)$, we have $\arg(1-t^2 e^{2\i\theta})\in (-\pi,0)$. Further 
\[
\arg\left(\frac{1}{\sqrt{1-t^2 e^{2\i\theta}}}\right)\in (0,\pi/2)\qquad \textnormal{and}\qquad \arg \left(\int_0^r \frac{r-t}{\pi\sqrt{1-t^2 e^{2\i\theta}}} dt\right) \in (0,\pi/2)\,.
\]
Noting $\arg (e^{2\i\theta})\in (0,\pi/2)$, we get
\[\arg \left(\int_{\gamma} \frac{z-w}{\pi\sqrt{1-w^2}}dw\right) \in (0,\pi)\,,\]
which gives a positive imaginary part. Combining with $\Im(1/\pi+z/2) > 0$ yields the desired statement. 
\end{proof}

\begin{lemma}\label{lem:around-one}
For every $k\ge 1$ and $\eps>0$, there exists $\delta>0$ such that $\g^{(k)}$ is analytic on $B_1(\delta)\cap \bH^+$ and $B_1(\delta)\cap \bH^-$ with
\begin{align*}
    \g^{(k)}(B_1(\delta)\cap \bH^+) \subseteq{}& B_1(\eps)\cap \bH^+\,,\\
     \g^{(k)}(B_1(\delta)\cap \bH^-) \subseteq{}& B_1(\eps)\cap \bH^-\,.
\end{align*}
\end{lemma}
\begin{proof}
We present the proof for $\bH^+$ here and that for $\bH^-$ can be shown similarly. We adopt an induction argument on $k$.

For $k=1$, $\g$ is analytic on $\bH^+$. Since $\g$ is continuous at $z=1$, for any $\eps>0$, there exists $0<\delta<1/2$ such that \[
\g(B_1(\delta)\cap \bH^+)\subseteq B_1(\eps)\,.
\]
\cref{lem:oct} implies $\g(B_1(\delta)\cap \bH^+)\subseteq \bH^+$. Combining them yields \begin{equation}\label{eq:base-inclusion}
    \g(B_1(\delta)\cap \bH^+) \subseteq B_1(\eps)\cap \bH^+\,.
\end{equation}

Now assume that the statement holds true for some $k\ge 1$. Note that for any $\eps>0$, there exists $0<\delta<1/2$ such that \eqref{eq:base-inclusion} holds. Then by induction hypothesis, for this chosen $\delta$, there exists $\delta_1>0$ such that $\g^{(k)}$ is analytic on $B_1(\delta_1)\cap \bH^+$ and \[
\g^{(k)}(B_1(\delta_1)\cap \bH^+) \subseteq B_1(\delta)\cap \bH^+\,.
\]
It follows
\[
\g^{(k+1)}(B_1(\delta_1)\cap \bH^+) \subseteq \g(B_1(\delta)\cap \bH^+)\subseteq B_1(\eps)\cap \bH^+\,.
\]
This completes the proof.
\end{proof}


\begin{lemma}\label{lem:unit-disk}
$|\g(z)|\le 1$ for any $|z|\le 1$, where the equality holds if and only if $z=1$. 
\end{lemma}

\begin{proof}
The Taylor series of $\g$ around $z=0$ is 
\[
\g(z) = \frac{1}{\pi} + \frac{z}{2} + \sum_{n=1}^\infty \frac{(2n-3)!!}{(2n-1)n!2^n \pi} z^{2n}\,.
\]
Therefore, for $|z|\le 1$, we have \[
|\g(z)| \le \frac{1}{\pi} + \frac{|z|}{2} + \sum_{n=1}^\infty \frac{(2n-3)!!}{(2n-1)n!2^n \pi} |z|^{2n} \le \g(1) = 1\,.
\]
The equality holds if and only if $z=1$. 
\end{proof}

\begin{lemma}\label{lem:analytic}
  
For each $k\geq 1$, there exists $R>1$ such that $\g^{(k)}$ is analytic on $\{z\in \bC~|~ |z|\le R\}\cap D$, where $D = \bC\setminus [1,\infty) \setminus (-\infty,-1] $.  
\end{lemma}

\begin{proof}
For any $0<\theta < \pi/2$, there exists $\delta_\theta>0$ such that for all $|z|\le 1$ with $|\arg z| \ge \theta$, we have \[
|\g(z)| \le 1 - \delta_\theta\,.
\]
To see this, we use an argument similar to \citep{pinelis}.
If 
we define $\phi \triangleq \arg z$, we have \begin{align*}
& \left| \frac{1}{\pi} + \frac{z}{2} \right| 
= \sqrt{\frac{|z|^2}{4}+\frac{|z| \cos \phi }{\pi }+\frac{1}{\pi ^2}}\\
\le{}& \sqrt{\frac{1}{4}+\frac{ \cos \theta}{\pi }+\frac{1}{\pi ^2}}
= \sqrt{\left(\frac{1}{2} + \frac{1}{\pi}\right)^2-\frac{1-\cos \theta}{\pi}}
= \frac{1}{2} + \frac{1}{\pi} - \delta_{\theta}
\,,
\end{align*}
for some $\delta_\theta>0$. 
Consider the Taylor series of $\g$ around $z=0$
\[
\g(z) = \frac{1}{\pi} + \frac{z}{2} + \sum_{n=1}^\infty \frac{(2n-3)!!}{(2n-1)n!2^n \pi} z^{2n}\,.
\]
We obtain \[
|\g(z)| \le \left|\frac{1}{\pi} + \frac{z}{2} \right| + \sum_{n=1}^\infty \frac{(2n-3)!!}{(2n-1)n!2^n \pi} |z|^{2n} \le \frac{1}{2} + \frac{1}{\pi}-\delta_\theta + \sum_{n=1}^\infty \frac{(2n-3)!!}{(2n-1)n!2^n \pi} = 1 - \delta_\theta\,.
\]

\cref{lem:around-one} shows that there exists $0<\delta'<1$ such that $\g^{(k)}$ is analytic on $B_1(\delta')\cap D$.
From the argument above, we know that $\g$ maps $ A\triangleq  \{z\in \bC \mid |z|=1,|\arg z|\ge \theta\}$ to inside of the open unit ball $B_0(1)$. Since $A$ is compact and \cref{lem:unit-disk} implies that $g$ maps $B_0(1)$ to $B_0(1)$, there exists $1< R_\theta < 1 + \delta'$ such that $\g$ maps 
\[
A_{\theta}\triangleq \left(\{z\in \bC\mid  |z|\le R_\theta, |\arg z| \ge \theta\} \cap D\right) \cup B_0(1)
\]
to $B_0(1)$. It follows that $\g^{(k)}$ is analytic on $A_\theta$. Let us pick $\theta \in (0,\pi/2)$ such that $ e^{\i\theta}\in B_1(\delta')$. Then we conclude that $\g^{(k)}$ is analytic on $\{z\in \bC \mid |z|\le R_\theta\}\cap D$. \end{proof}

Now we are ready to prove \cref{thm:analytic_main}.
\begin{proof}
Since $\kappa_0$ and $\g$ are both analytic on $D = \bC\setminus [1,\infty) \setminus (-\infty,-1] $, similar arguments as in the proof of \cref{lem:analytic} shows that $\kappa_0(\g^{(k)}(z))$ is analytic on $\{z\in \bC\mid  |z|\le R\}\cap D$ for all $k\ge 1$ and some $R>1$. We then show, for any $k\geq 1$, there exists some $R_k>1$ such that $N_k(z)$ is analytic on $\{z\in \bC  \mid |z|\le R_k\}\cap D$ by induction. The function $N_0(z) = z+\beta^2$ is analytic on $D$. Assume $N_{k-1}(z)$ is analytic on $\{z\in \bC \mid |z|\le R_{k-1}\} \cap D$ for some $R_{k-1}>1$. Recall that \[
N_k(z) ={} \g^{(k)}(z) + N_{k-1}(z)\kappa_0(\g^{(k-1)}(z)) + \beta^2\,.
\]
Then we can find some $R_k>1$ such that $N_k(z)$ is analytic on $\{z \in \bC \mid |z|\le R_k\}\cap D$. 
\end{proof}






\subsection{Proof of \cref{thm:Kk}}\label{sec:NTK2}
\begin{proof}



We first analyze the behavior of $N_k(z)$ as $z\to 1$ for any $k\geq 1$. We aim to show, for any $k\geq 1$, there exists a sequence of complex functions $p_k(z)$ with $\lim_{z\to 1} p_k(z)=-\sqrt{2}(1+\beta^2)k(k+1)/2\pi$ such that
\begin{equation}\label{eq:Kk-around1}
    N_k(z) = (k+1)(z+\beta^2) + p_k(z)\sqrt{1-z}\,.
\end{equation}
We prove by induction on $k$. Recall 
\[\kappa_0(z) = \frac{ \pi +\i\log(z+\i\sqrt{1-z^2})}{\pi}\,.\]
The fundamental theorem of calculus then gives for any $z\in D$ \begin{equation*}
    \kappa_0(z) = 1 + \int_{\gamma} \frac{1}{\pi  \sqrt{1-w^2}} dw\,,
\end{equation*}
where $\gamma:[0,1]\to\bC$ is the simple straight line connecting $1$ and $z$. As $z\to 1$, we have $\frac{1}{\sqrt{1-z^2}}\sim \frac{1}{\sqrt{2}\sqrt{1-z}}$. Therefore, similar arguments as in the proof of \cref{lem:expansion-one} give \[
\kappa_0(z) = 1 +h(z) \sqrt{1- z}\,,
\]
where $\lim_{z\to 1} h(z) = -\frac{\sqrt{2}}{\pi }$. Combining with \cref{lem:expansion-one} further gives, for any $k\ge 1$
\[
\kappa_0(\g^{(k)}(z)) = 1 + h(\g^{(k)}(z))\sqrt{1-z-c_{k}(z)(1-z)^{3/2}} = 1 + h_k(z)\sqrt{1-z}\,,
\]
where $\lim_{z\to 1} h_k(z) = -\frac{\sqrt{2}}{\pi}$. For $k=1$, we then have
\begin{align*}
    N_1(z) ={}& \g(z) + (z+\beta^2)\kappa_0(z) + \beta^2
= z + d_1(z)(1-z)^{3/2} + (z + \beta^2)(1+ h(z)\sqrt{1-z}) + \beta^2\\
={}&  2(z+\beta^2) + p_1(z)\sqrt{1-z} \,,
\end{align*}
where $\lim_{z\to 1} d_1(z) = \frac{2\sqrt{2} }{3\pi} $ and $\lim_{z\to 1} p_1(z) = -\sqrt{2}(1+\beta^2)/\pi$. Assume $N_{k-1}(z) = k(z+\beta^2) + p_{k-1}(z)\sqrt{1-z}$ with $\lim_{z\to 1}p_{k-1}(z)=-\sqrt{2}(1+\beta^2)k(k-1)/(2\pi)$. We further have
\begin{align*}
    N_k(z) ={}& \g^{(k)}(z) + N_{k-1}(z)\kappa_0(\g^{(k-1)}(z)) + \beta^2\\
={}& z + d_k(z)(1-z)^{3/2} + \left(k(z+\beta^2) + p_{k-1}(z)\sqrt{1-z}\right)(1+h_{k-1}(z)\sqrt{1-z}) + \beta^2\\
={}& (k+1)(z+\beta^2) + (p_{k-1}(z) + k \cdot h_{k-1}(z)(z+\beta^2))\sqrt{1-z}\\
={}& (k+1)(z+\beta^2) + p_k(z)\sqrt{1-z}\,.
\end{align*}
where we set $p_k(z)=p_{k-1}(z) + k \cdot h_{k-1}(z)(z+\beta^2)$ and $d_k(z)\to \frac{2\sqrt{2}k}{3\pi}$, $h_{k-1}(z)\to -\frac{\sqrt{2}}{\pi}$ as $z\to 1$. Moreover, we have 
\begin{align*}
    \lim_{z\to 1}p_k(z)&=\lim_{z\to 1}\Big\{ p_{k-1}(z) + k \cdot h_{k-1}(z)(z+\beta^2)\Big\}\\
    &=-\frac{\sqrt{2}(1+\beta^2)k(k-1)}{2\pi}-k\cdot\frac{\sqrt{2}}{\pi}(1+\beta^2)\\
    &=-\frac{\sqrt{2}(1+\beta^2)k(k+1)}{2\pi}\,,
\end{align*}
which is desired. This proves \eqref{eq:Kk-around1}.





Next we study the behavior of $N_k(z)$ as $z\to -1$ for any $k\geq 1$. We aim to show, for any $k\geq 1$, there exists a sequence of complex functions $q_k(z)$ with $\lim_{z\to -1}q_k(z)=\sqrt{2}(\beta^2-1)\prod_{j=1}^{k-1}\kappa_0(a_j)/\pi$ and $a_k \triangleq \g^{(k)}(-1)$ as defined in \cref{lem:expansion-minus-one} such that 
\begin{equation}\label{eq:Kk-around-1}
    N_k(z) = N_k(-1) + q_k(z)\sqrt{1+z}\,.
\end{equation}
We again adopt induction on $k$. Taylor's theorem gives
\[
\kappa_0(z) = \kappa_0(a_k) + r_k(z)(z- a_k)
\,,
\]
where $\lim_{z\to a_k} r_k(z)= \kappa'_0(a_k)>0$. Combining with \cref{lem:expansion-minus-one} further gives, for any $k\geq 1$
\[
\kappa_0(\g^{(k)}(z)) = \kappa_0(a_k) + r_k(\g^{(k)}(z))b_k(z)(z+1)^{3/2}
= \kappa_0(a_k) + \Tilde{r}_k(z)(z+1)^{3/2}
\,,
\]
where $b_k(z)\to \frac{2\sqrt{2}}{3\pi} \prod_{j=1}^{k-1}\g'(a_k)$ and $\Tilde{r}_k(z)\to \frac{2\sqrt{2}}{3\pi} \kappa'_0(a_k)  \prod_{j=1}^{k-1}\g'(a_k) > 0 $ as $z\to -1$ by \cref{lem:expansion-minus-one}. For $k=1$, the fundamental theorem of calculus gives for any $z\in D$ \begin{equation*}
    \kappa_0(z) = \int_{\gamma} \frac{1}{\pi  \sqrt{1-w^2}} dw\,,
\end{equation*}
where $\gamma:[0,1]\to\bC$ is the simple straight line connecting $-1$ and $z$. As $z\to -1$, we have $\frac{1}{\sqrt{1-z^2}}\sim \frac{1}{\sqrt{2}\sqrt{1+z}}$. Therefore, similar arguments as in the proof of \cref{lem:expansion-one} give
\[
\kappa_0(z) =  g(z)\sqrt{1+z}\,,
\]
where $g(z)\to \frac{\sqrt{2}}{\pi}$ as $z\to -1$. We then have
 \begin{align*}
    N_1(z) ={}& \g(z) + (z+\beta^2)\kappa_0(z) + \beta^2\\
    ={}& a_1 + b_1(z)(z+1)^{3/2} + (z+\beta^2)g(z)\sqrt{1+z} + \beta^2\\
    ={}& (a_1+\beta^2) + q_1(z)\sqrt{1+z}\\
    ={}& N_1(-1) + q_1(z)\sqrt{1+z}\,,
\end{align*}
where $N_1(-1)=a_1+\beta^2$ $\lim_{z\to -1} q_1(z)= \frac{\sqrt{2}}{\pi}(\beta^2-1)$. Assume $N_{k-1}(z)=N_{k-1}(-1)+q_{k-1}(z)\sqrt{1+z}$ with $\lim_{z\to -1}q_{k-1}(z)=\sqrt{2}(\beta^2-1)\prod_{j=1}^{k-2}\kappa_0(a_j)/\pi$. We further have 
\begin{align*}
     N_k(z)
    ={}& \g^{(k)}(z) + N_{k-1}(z)\kappa_0(\g^{(k-1)}(z)) + \beta^2\\
    ={}& a_k + b_k(z)(z+1)^{3/2} + N_{k-1}(z)\left( \kappa_0(a_{k-1}) + \Tilde{r}_{k-1}(z)(z+1)^{3/2} \right) + \beta^2\\
    ={}& \left(a_k+\beta^2+N_{k-1}(z)\kappa_0(a_{k-1})\right)+\left( b_k(z)+N_{k-1}(z)\Tilde{r}_{k-1}(z) \right)(z+1)^{3/2}\\
    ={}& \left(a_k+\beta^2+N_{k-1}(-1)\kappa_0(a_{k-1})\right)+q_{k-1}(z)\kappa_0(a_{k-1})\sqrt{z+1}\\
    &+\left( b_k(z)+N_{k-1}(z)\Tilde{r}_{k-1}(z) \right)(z+1)^{3/2}\\
    ={}& N_k(-1)+q_{k-1}(z)\kappa_0(a_{k-1})\sqrt{z+1}+\left( b_k(z)+N_{k-1}(z)\Tilde{r}_{k-1}(z) \right)(z+1)^{3/2}\\
    ={}& N_k(-1) + q_k(z)\sqrt{1+z}\,,
\end{align*}
where we use the induction assumption in the fourth equation, use the fact $N_k(-1)=a_k+\beta^2+N_{k-1}(-1)\kappa_0(a_{k-1})$ in the fifth equation and define \[q_k(z)=q_{k-1}(z)\kappa_0(a_{k-1})+\left( b_k(z)+N_{k-1}(z)\Tilde{r}_{k-1}(z) \right)(z+1)\] in the last equation. We also have 
\begin{align*}
    \lim_{z\to -1} q_k(z)=&\lim_{z\to -1}\Big\{q_{k-1}(z)\kappa_0(a_{k-1})+\left( b_k(z)+N_{k-1}(z)\Tilde{r}_{k-1}(z) \right)(z+1)\Big\}\\
    =&\lim_{z\to -1}\Big\{q_{k-1}(z)\kappa_0(a_{k-1})\Big\}\\
    =&\frac{\sqrt{2}(\beta^2-1)}{\pi}\prod_{j=1}^{k-1}\kappa_0(a_j)\,,
\end{align*}
which is desired. This proves \eqref{eq:Kk-around-1}.


Finally, according to \cref{thm:analytic_main}, combining \eqref{eq:Kk-around1} and \eqref{eq:Kk-around-1}, applying 
 \cite[Theorem VI.5]{flajolet2009analytic} with $\rho=1$, $r=2$, $\tau(z) = (1-z)^{1/2}$, $\zeta_1=1$, $\zeta_2=-1$, $\sigma_1(z) = (k+1)(z+\beta^2)$, $\sigma_2(z) = N_k(-1)$, $\bfD = \{z\in \bC\mid |z|\le R_k\}\cap D$,
we conclude $[z^n]N_k(z) = O(n^{-3/2})$. 
\end{proof}



\section{Proofs for Exponential Power Kernel}
\subsection{Proof of \cref{lem:series}}\label{sec:proof-series}
\begin{proof}

According to \cite[Theorem 28.2]{doetsch1974introduction}, we have, for $0<a<1$,
\[
f(t) = \frac{1}{2\pi \i}\lim_{T\to +\infty}\int_{x_0-\i T}^{x_0+\i T} \exp(ts-s^a) ds\qquad (x_0\ge 0)\,.
\]
Also \cite[Theorem 28.2]{doetsch1974introduction} implies that $f(t)$ is continuous in $-\infty<t<+\infty$ and $f(0)=0$. 

\begin{figure}[htb]
    \centering
    \includegraphics[]{pix/contour.pdf}
    \caption{Bromwich contour that circumvents the branch cut $(-\infty,0]$}
    \label{fig:contour}
\end{figure}

Next we explicitly calculate $f(t)$ using Bromwich contour integral. We denote each part of the Bromwich contour by $\Gamma_0,\ldots,\Gamma_5$ as depicted in \cref{fig:contour}. Denote the radius of the outer and inner arc by $R$ and $r$. When $T\to\infty$, we have $R=\sqrt{T^2+x_0^2}\to\infty$. Also we let $r\to 0$ and $\Gamma_2,\Gamma_4$ tend to $(-\infty,0]$ from above and below respectively in the limit. By the residue theorem, we have
\[\left(\int_{\Gamma_0}+\ldots+\int_{\Gamma_5}\right)\exp(ts-s^a)  ds=0\,,\]
which implies
\begin{align*}
    \lim_{T\to\infty}\int_{x_0-\i T}^{x_0+\i T} \exp(ts-s^a) ds &=\lim_{T\to\infty}\int_{\Gamma_0} \exp(ts-s^a) ds\\
    &=-\lim\Big(\int_{\Gamma_1}+\ldots+\int_{\Gamma_5}\Big)\exp(ts-s^a) ds\\
    &\triangleq -\lim(I_1+\ldots+I_5)\,,
\end{align*}
where the last two limits are taken as $R\to\infty$, $r\to 0$, and $\Gamma_2,\Gamma_4$ tend to $(-\infty,0]$. We then calculate each part separately.

\bigskip

{\bf Part I:} We calculate the parts for $\Gamma_1$ and $\Gamma_5$. We follow the similar idea as in the proof of \cite[Theorem 7.1]{spiegel1965laplace}. Along $\Gamma_1$, since $s=Re^{\i\theta}$ with $\theta_0\leq \theta\leq \pi$, $\theta_0=\arccos(x_0/R)$,
\begin{align*}
    I_1&=\int_{\theta_0}^{\pi/2} e^{Re^{\i\theta}t}e^{-R^a e^{\i a\theta}}\i Re^{\i\theta}d\theta+\int_{\pi/2}^{\pi} e^{Re^{\i\theta}t}e^{-R^a e^{\i a\theta}}\i Re^{\i\theta}d\theta\\
    &\triangleq I_{11}+I_{12}\,.
\end{align*}

For $I_{11}$,
\begin{align*}
    |I_{11}|&\leq \int_{\theta_0}^{\pi/2} |e^{Rt\cos\theta}|\cdot |e^{-R^a \cos(a\theta)}| R d\theta\\
    &\leq \int_{\theta_0}^{\pi/2} e^{Rt\cos\theta}\cdot e^{-R^a \cos(a\pi/2)} R d\theta\\
    &\leq \frac{R}{R^{a\cos(a\pi/2)} }\int_{\theta_0}^{\pi/2} e^{Rt\cos\theta} d\theta\\
    &= \frac{R}{R^{a\cos(a\pi/2)} }\int_{0}^{\phi_0} e^{Rt\sin\phi} d\phi\,,
\end{align*}
where $\phi_0=\pi/2-\theta_0=\arcsin(x_0/R)$. Since $\sin\phi\leq\sin\phi_0\leq x_0/R$, we have
\begin{align*}
    |I_{11}|\leq \frac{R}{R^{a\cos(a\pi/2)}}\phi_0 e^{x_0 t}=\frac{R}{R^{a\cos(a\pi/2)}} e^{x_0 t}\arcsin(x_0/R)\,.
\end{align*}
As $R\to \infty$, we have $\lim_{R\to \infty}I_{11}=0$. 

For $I_{12}$,
\begin{align*}
    |I_{12}|\leq \int_{\pi/2}^{\pi} e^{Rt\cos\theta}\cdot e^{-R^a \cos(a\theta)} R d\theta\,.
\end{align*}
First, we consider the case $0<a<1/2$. We have $a\theta \le a\pi < \pi/2$ and $\cos(a\theta) \ge \cos(a\pi) > 0$. It follows
\begin{align*}
& \int_{\pi/2}^{\pi} e^{Rt\cos\theta}\cdot e^{-R^a \cos(a\theta)} R d\theta\\
\le{}& R e^{-R^a \cos(a\pi)} \int_{\pi/2}^{\pi} e^{Rt\cos\theta}  d\theta\\
={}& R e^{-R^a \cos(a\pi)} \int_{0}^{\pi/2} e^{-Rt\sin \phi}  d\phi\\
\le{}& R e^{-R^a \cos(a\pi)} \int_{0}^{\pi/2} e^{-2Rt \phi/\pi}  d\phi\\
={}&  e^{-R^a \cos(a\pi)}\frac{\pi(1 -  e^{-R t})}{2  t}\,,
\end{align*}
where in the last inequality we use the fact $\sin\phi \ge 2\phi/\pi$ for $\phi\in [0,\pi/2]$. Thus, $\lim_{R\to\infty} I_{12}=0$. Next, we consider $1/2\le a< 1$. Define \[p(\theta)\triangleq Rt\cos\theta-R^a \cos(a\theta)\,.\] We then have its second derivative as follows \[p''(\theta) = a^2 R^a \cos (a \theta )-R t \cos (\theta )\,.\] 
Choose $\delta$ to be a fixed constant in $(0,\frac{\pi}{2}(\frac{1}{a}-1))$.
Since $a\ge 1/2$, then $\delta < \pi/2$.
If $\pi/2+\delta \le \theta \le \pi$,
\[
p''(\theta) \ge -a^2 R^a - R t \cos(\pi/2+\delta)
=  -a^2 R^a + R t \sin(\delta)\,.
\]
Since $a<1$, there exists some large $R_1>0$ such that $p''(\theta)\ge -a^2 R^a + R t \sin(\delta)>0$ holds for all $R>R_1$. 
If $\pi/2\le \theta < \pi/2 + \delta$,
\[
p''(\theta) \ge a^2 R^a \cos(a(\pi/2+\delta))\,.
\] 
Since $a(\pi/2+\delta) < \pi/2$ by the choice of $\delta$, we get $\cos(a(\pi/2+\delta))>0$. Then we also have $p''(\theta)>0$. Therefore, if $R>R_1$, $p(\theta)$ is convex in $\theta\in [\pi/2,\pi]$. As a result, we get \[
\max_{\theta\in [\pi/2,\pi]} p(\theta) \le \max \{ p(\pi/2),p(\pi) \}\,.
\]
Write \[h(R,\theta)\triangleq R e^{Rt\cos\theta}\cdot e^{-R^a \cos(a\theta)}
= R e^{p(\theta)}\,.\]
Then we have \begin{align*}
\max_{\theta\in [\pi/2,\pi]} h(R,\theta) \le{}& \max\{ h(R,\pi/2),h(R,\pi) \}\\ 
={}& R \max\{ e^{-R^a \cos \left(\frac{\pi  a}{2}\right)},  e^{-R^a \cos (\pi  a)-R t}\}\\
\le{}& R \max\{ e^{-R^a \cos \left(\frac{\pi  a}{2}\right)},  e^{R^a -R t}\} \,,
\end{align*}
which goes to $0$ as $R\to\infty$.
Therefore, $h(R,\theta)$ converges to $0$ uniformly (as a function of $\theta\in [\pi/2,\pi]$ with index $R$), which implies
\[\lim_{R\to \infty} \int_{\pi/2}^{\pi} h(R,\theta) d\theta = 0\,.\] 
Hence, we establish $\lim_{R\to \infty} I_{12}=0$ for all $a\in(0,1)$. 

 
Combining these above, we conclude $\lim_{R\to\infty} I_1=0$. Similarly, $\lim_{R\to\infty} I_5=0$.

{\bf Part II:} We calculate the parts for $\Gamma_2$ and $\Gamma_4$. By the dominated convergence theorem, we have, for $y>0$
\begin{align*}
     \lim_{\substack{R\to\infty\\r\to 0 \\ y\to 0^+}} I_2 ={}& \lim_{\substack{R\to\infty\\r\to 0 \\ y\to 0^+}}\int_{-R+\i y}^{-r+\i y} \exp(ts) \exp(-s^a) ds \\
    ={}&  \lim_{\substack{R\to\infty\\r\to 0 \\ y\to 0^+}}\int_{-R+\i y}^{-r+\i y} \exp(ts) \sum_{k=0}^\infty \frac{(-1)^k s^{ak}}{k!}ds \\
    ={}&   \lim_{\substack{R\to\infty\\r\to 0 \\ y\to 0^+}}\sum_{k=0}^\infty \frac{(-1)^k }{k!}  \int_{-R+\i y}^{-r+\i y} \exp(ts) s^{ak} ds \,.
\end{align*}
We then calculate the limit of the summand. 
\begin{align*}
    \lim_{\substack{R\to\infty\\r\to 0 \\ y\to 0^+}} \int_{-R+\i y}^{-r+\i y} \exp(ts) s^{ak} ds&=\int_{-\infty}^0 e^{tx}\cdot [(-x)e^{\i\pi}]^{ak} dx\\
    &=\int_0^{\infty} e^{-tx} x^{ak} e^{\i\pi ak}dx\\
    &=\frac{1}{t^{ak+1}}\Gamma(ak+1)e^{\i\pi ak}\,.
\end{align*}
Similarly, we obtain the corresponding part in $\Gamma_4$:
\begin{align*}
    \lim_{\substack{R\to\infty\\r\to 0 \\ y\to 0^-}} \int_{-r+\i y}^{-R+\i y} \exp(ts) s^{ak} ds&=-\int_{-\infty}^0 e^{tx}\cdot [(-x)e^{-\i\pi}]^{ak} dx\\
    &=-\frac{1}{t^{ak+1}}\Gamma(ak+1)e^{-\i\pi ak}\,.
\end{align*}
Combining the parts of $\Gamma_2$ and $\Gamma_4$ together, we get
\begin{align*}
    \lim(I_2+I_4)=\sum_{k=0}^\infty \frac{(-1)^k }{k!} \frac{2\i\Gamma(ak+1)\sin(\pi ak)}{t^{ak+1}}\,.
\end{align*}

{\bf Part III:} We get the limit for $\Gamma_3$ is $0$ as $r\to 0$.
\bigskip

Combining the three parts above, we conclude
\begin{align*}
    f(t)&=\frac{1}{2\pi\i} \sum_{k=0}^\infty \frac{(-1)^{k+1} }{k!} \frac{2\i\Gamma(ak+1)\sin(\pi ak)}{t^{ak+1}}\\
    &=\frac{1}{\pi}\sum_{k=0}^\infty  \frac{(-1)^{k+1}\Gamma(ak+1)\sin(\pi ak)}{k! t^{ak+1}}\,.
\end{align*}

\end{proof}

\subsection{Proof of \cref{lem:tail}}\label{sec:proof-tail}
\begin{proof}
Euler's reflection formula gives \[
\Gamma(1+k a)\Gamma(-k a) = \frac{-\pi}{\sin(\pi k a)},\quad k a\notin \bZ\,.
\]
According to \cref{lem:series}, we have
\begin{align}
    f(t) ={}& \frac{1}{\pi}\sum_{k=0}^\infty  \frac{(-1)^{k+1}\Gamma(ak+1)\sin(\pi ak)}{k! t^{ak+1}}\nonumber\\
    ={}& \sum_{k=0}^{\infty} \frac{(-1)^k}{k! t^{ak+1} \Gamma(-ak) }\nonumber\\
={}& \sum_{j=1}^{q-1}\sum_{n=0}^\infty  \frac{(-1)^{nq+j}}{(nq+j)! t^{a(nq+j)+1}\Gamma(-a(nq+j))} \,.\label{eq:decompose-series}
\end{align}

First, we show that the series in \eqref{eq:decompose-series}
converges absolutely:
\begin{align}
& \sum_{j=1}^{q-1} \sum_{n=0}^\infty \frac{ |t|^{-a (nq+j)-1}}{(nq+j)! |\Gamma (-a (nq+j))|} \nonumber \\
={}& \sum_{j=1}^{q-1} \frac{1}{|t|^{aj+1}} \sum_{n=0}^\infty \frac{ |t|^{-np}}{(nq+j)! |\Gamma (-a (nq+j))|}\nonumber  \\
={}& \sum_{j=1}^{q-1} \frac{1}{|t|^{aj+1} |\Gamma (-a j)|} \sum_{n=0}^\infty \frac{ |t|^{-np} \prod_{i=1}^{n p} (aj+i)}{(nq+j)! }\,. \label{eq:power-series} 
\end{align}
The inner summation in \eqref{eq:power-series} is a power series in $|t|^{-p}$. We would like to show that its radius of convergence is $\infty$. Define \[
b_n = \frac{ \prod_{i=1}^{n p} (aj+i)}{(nq+j)! }\,.
\]
We have \begin{align*}
    \frac{b_{n+1}}{b_n} ={}&  \frac{\prod_{np<i\le (n+1)p} (aj+i)}{\prod_{nq<i\le (n+1)q} (j+i)} = \frac{\prod_{i=1}^p \frac{aj+np+i}{j+nq+i} }{\prod_{i=nq+p+1}^{(n+1)q} (j+i) }\\
    \le{}& \frac{1 }{\prod_{i=nq+p+1}^{(n+1)q} (j+i) } \le \frac{1}{(j+nq+p+1)^{q-p}}\to 0 \,.
\end{align*}
As a result, the radius of convergence is $\infty$. Then we have 
\begin{align*}
    f(t)
    ={}& \sum_{j=1}^{q-1} \frac{1}{t^{aj+1} \Gamma (-a j)}\sum_{n=0}^\infty \frac{ (-1)^{n(p+q)+j} t^{-pn} \prod_{i=1}^{np}(aj+i)}{(nq+j)! }\\
    ={}& \sum_{j=1}^{q-1} \frac{1}{t^{aj+1} \Gamma (-a j)}\left(
    \frac{(-1)^j}{j!}+ \underbrace{\sum_{n=1}^\infty \frac{ (-1)^{n(p+q)+j} t^{-pn} \prod_{i=1}^{np}(aj+i)}{(nq+j)! }}_{A}\right)
\end{align*}
Notice that the quantity $A$ goes to $0$ 
as $t\to +\infty$. Therefore we deduce \[
f(t)
\sim 
\sum_{j=1}^{q-1} \frac{(-1)^j}{t^{aj+1}j! \Gamma (-a j)} \sim -\frac{1}{t^{a+1}\Gamma(-a)}\,, 
\]
as $t\to+\infty$.
\end{proof}

\end{document}
