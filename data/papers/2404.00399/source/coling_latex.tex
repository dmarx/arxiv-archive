% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{coling}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


\usepackage{tablefootnote}

% \usepackage[table,x11names]{xcolor}
\usepackage{color, colortbl}
\definecolor{verylightgray}{rgb}{0.9,0.9,0.9}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{pygreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{pyred}{rgb}{0.7, 0.0, 0.0}
\definecolor{pyblue}{rgb}{0.0, 0.0, 0.7}
\definecolor{pygray}{rgb}{0.5, 0.5, 0.5}
\definecolor{pydarkgray}{rgb}{0.3, 0.3, 0.3}

\definecolor{color1}{HTML}{006EB8}
\definecolor{color2}{HTML}{009B55}
\newcommand{\myblue}[1]{\textcolor{azureblue}{~#1}}
\newcommand{\myorange}[1]{\textcolor{orange}{~#1}}

\usepackage{wrapfig}

\usepackage[symbol]{footmisc}

\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{microtype}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=color2,linkcolor=color2,citecolor=color2}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[whole]{bxcjkjatype}
\usepackage{amsmath} 
\usepackage[inline]{enumitem}
% For Sonny's comment
\newcommand{\SVX}[1]{\textcolor{red}{[SVX: #1]}}
\newcommand{\Q}[1]{\textcolor{red}{[#1]}}


%% for non-anonymous version
% \newcommand{\system}{\textsc{Aurora-M}} 

% for submission
\newcommand{\system}{\textsc{Aurora-M}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{\system}: Open Source Continual Pre-training for Multilingual Language and Code}

% \title{\textsc{\system}: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

% \author{
%    Aurora Team\\
%    Ontocord.AI
% }
\input{authors}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\setlength\titlebox{35\baselineskip}
\maketitle
\begin{abstract}
Pretrained language models are an integral part of AI applications, but their high computational cost for training limits accessibility. 
Initiatives such as \textsc{Bloom} and \textsc{StarCoder} aim to democratize access to pretrained models for collaborative community development. 
Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks.
%However, existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. 
%
This paper presents \textbf{\textcolor{violet}{\system}}, a \texttt{15B} parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from \textsc{StarCoderPlus} on \texttt{435B} additional tokens, \system\ surpasses \texttt{2T} tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.
%
We evaluate \system\ across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source \system\ and its variants to encourage responsible open-source development of large language models at \url{https://huggingface.co/aurora-m}.

% \begin{center}
%    % \includegraphics[width=0.03\textwidth]{img/huggingface_logo_svg-tex.pdf}
%    \url{https://huggingface.co/aurora-m}
% \end{center}
\end{abstract}

% To encourage responsible open-source development of large language models, \system\ and its variants are available at \url{https://huggingface.co/aurora-m}.
% \system\ is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source LLM development, \system\ and its variants are released at \url{https://huggingface.co/aurora-m}.
% \href{https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407}{here}.
% \system\ is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source LLM development, \system\ and its variants will be made publicly available. 



\section{Introduction}
\input{paper/introduction}

\section{Datasets}

\input{paper/datasets}


\section{Model Training}
\input{paper/methodology}

\input{paper/training}

\section{Safety}\label{sec:safety}
\input{paper/safety}

\section{Evaluation}\label{sec:experiments}
\input{paper/experiments}



\section{Related Work}
\input{paper/related-work}


\section{Conclusion}
\input{paper/conclusions}

% \newpage

\section*{Ethical Consideration}
We believe that transparency and accessibility are fundamental principles in the development and deployment of artificial intelligence technologies. Closed-source LLMs limit public scrutiny, hinder collaboration, and potentially reinforce biases inherent in their development process. 
In contrast, our commitment to open source models fosters a culture of accountability, collaboration, and inclusivity. By making \system\ accessible to all, we promote innovation, empower diverse voices, and strive for equitable outcomes in AI applications. We firmly believe that openness in AI development is essential for creating solutions that truly serve the needs and values of society. To this end, we prioritized safety guardrails in alignment with the Biden-Harris Executive Order on AI. Furthermore, the multilingual capability of \system\ enhances its usability for users across the world.

On the other hand, each promise comes with peril, and improved technological access through \system\ might also increase the potential number of malicious actors. We overall believe that the general benefit far outweighs the potential misuse and want to emphasize the importance of a considered and ethical use of this technology and thus also of \system.

Lastly, we recognize that safety and lawfulness can be contextual to different cultures and laws. We recognize that in our work we focused on a U.S. centric standard, and we believe future work should also explore multi-jurisdictional redteaming.


\section*{Acknowledgments}
This work was supported by the ``R\&D Hub Aimed at Ensuring Transparency and Reliability of Generative AI Models'' project of the Ministry of Education, Culture, Sports, Science and Technology, and used resources of LUMI supercomputer under project\_462000316.

% This document has been adapted by Emily Allaway from the instructions for earlier ACL and NAACL proceedings, including those for NAACL 2024 by Steven Bethard, Ryan Cotterell and Rui Yan,
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% % Bibliography entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
% % Custom bibliography entries only
\bibliography{custom}

\appendix

\input{paper/appendix}



\end{document}
