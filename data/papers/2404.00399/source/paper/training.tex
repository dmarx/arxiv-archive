% \section{Continuous Training}
% We utilized 32 nodes on the LUMI supercomputer for continuous training, implementing a two-stage training process. Throughout this training, we experienced relatively few gradient spikes.

% \subsection{Training Library}
% For our codebase, we chose Megatron-LM, originally designed for NVIDIA GPUs. Due to our hardware configuration, we used a version of Megatron-LM that had been modified to support AMD GPUs.

% \subsection{Training Environment}
% Our training was conducted on the LUMI supercomputer, located in Finland. We employed mixed precision training using bfloat16 and distributed parallel training across 32 nodes. Each node was equipped with 4 AMD MI250X GPUs. Notably, from both a software and Slurm perspective, a single MI250X GPU is recognized as two GPUs, effectively providing 8 GPUs per node.f

% \subsection{Training Settings}

% \subsubsection{Batch Size}
% For improved training efficiency, we utilized a batch size of 2048. Given the sequence length was set to 2048, this resulted in a token batch size of approximately 4,000,000 tokens, comparable to the size used in the training of Llama 2.

% \subsubsection{Learning Rate}
% We set our learning rate with a maximum of $1e-4$ and a minimum of $1e-5$. This is slightly lower than the maximum learning rate of $3e-4$ used for the 13B model in Code Llama, aiming to prevent learning divergence.

% \subsubsection{Learning Rate Scheduling}
% A cosine learning rate scheduler was utilized, with the learning rate reaching its maximum after 2,000 warmup steps and eventually decaying to one-tenth of its maximum value. The schedule was designed with an aim to train for 120,000 steps.

% \subsubsection{Sequence Length}
% Initially, Starcoderplus was trained with a sequence length of 8192. However, due to memory limitations on AMD GPUs within LUMI, which prevented the use of FlashAttention, we reduced the sequence length to 2048.

% \subsubsection{Optimizer}
% We chose the Adam optimizer with hyperparameters $\beta_1 = 0.9$, $\beta_2 = 0.95$, and $\epsilon = 1.0 \times 10^{-8}$.

% \subsubsection{Distributed Optimizer}
% The Distributed Optimizer from Megatron-LM was used to distribute the optimizer state variables across data-parallel processes. This strategy helped eliminate redundancy and reduce memory usage.

% \subsubsection{Tokenizer}
% The tokenizer from Starcoderplus, with a vocabulary size of 49,152 tokens trained using byte-level Byte-Pair-Encoding, was inherited without any modifications.

% \subsubsection{Training Data}
% The training was structured into two stages, with Stage 2 focusing on adjusting the model to produce safer outputs. This stage utilized data primarily sampled from Stage 1, enriched with a higher proportion of safety data. The instruction data was formatted for next-word prediction.

% \subsection{Preliminary Experiments}
% Given our limited computational resources and available training time, we investigated optimal settings for 3D parallelism and batch size. The findings are summarized in Table 5.2, with notations for tensor parallelism (TP), pipeline parallelism (PP), data parallelism (DP), micro-batch size (MBS), global batch size (GBS), TFLOPS per GCD (TFLOPS/GCD), tokens per second per GCD (TPS/GCD), and recomputation of activation (RA).

% \begin{table}[h]
% \centering
% \begin{tabular}{@{}ccccccccc@{}}
% \toprule
% Nodes & TP & PP & DP & MBS & GBS & TFLOPS/GCD & TPS/GCD & RA \\ \midrule
% % Your data here
% \bottomrule
% \end{tabular}
% \caption{Investigation of optimal settings for 3D parallelism and batch size.}
% \end{table}

% Our experiments revealed that increasing the number of nodes enhances overall tokens per second and FLOPS, though these metrics decrease per GCD. Therefore, we proceeded with the training using 32 nodes, balancing between computational efficiency and resource limitations.

% \subsection{Continual Pre-training}

% \paragraph{Training Setup old.}
% We trained our model on the LUMI supercomputer. Each server node on LUMI is equipped with 4x AMD MI250X 128GB GPUs. We used a modified version of Megatron-LM \citep{narayanan2021efficient} that supports AMD-compatible kernels. We ported a portion of the Bigcode fork of Megatron-LM using the HIP runtime and made this code public as part of promoting open science. We used Megatron-LM's distributed optimizer with mixed precision training \citep{mixed-precision} in BF16 with gradient all-reduce and gradient accumulation in FP32 for training stability. We limit our context lengths for training to 2048 tokens due to the unavailability of FlashAttention \citep{dao2022flashattention} for AMD GPUs at the time of training our model. 


% To train \system\ we use a constant batch size of 2048 samples (4M tokens with sequence length 2048). The learning rate is decayed from $10^{-4}$ to $10^{-5}$ following a cosine scheduler over the course of training. We use the AdamW optimizer \citep{adam, weight-decay} with coefficients $\beta_1=0.9$ and $\beta_2=0.95$.

% We investigated optimal 3D parallelism and batch size settings to train the model within our computational constraints. We perform extensive scaling experiments as set forth in Appendix [X]. 
% \Q{Taishi to add table} Increasing the number of nodes resulted in increased training throughput but with sublinear scaling performance and thus to remain efficient, while making use of our compute budget to the fullest, we opted to use a maximum of 32 nodes, even though it took longer train.

% We train our model on 32 nodes for 74 days with server downtime. It should also be noted that LUMI uses 100\% hydro-powered energy. LUMI's waste heat is also used to heat hundreds of households in the city of Kajaani. 

% The cosine learning rate scheduling was set up to reduce the maximum learning rate of $1e-4$ to 1/10 of its value after completing approximately 500B tokens. This scheduling approach allows for a gradual decrease in the learning rate, which can help the model converge to a better solution during the later stages of training.

% \subsubsection{Preliminary Experiments}


% \subsubsection{How to Finetune Experts}
% Using LoRA experts is highly efficient for training and allows experts to be trained on consumer hardware. The weights are hosted on HuggingFace here and can be finetuned using the PEFT library. Using the Starcoder finetuning script found here (but using the Aurora weights) you can train on a single A100 GPU. We are working with Wolfram Research to train the first 3rd party MDEL expert on a variety of data from their platform.

% \paragraph{Curriculum Training.}

% \Q{Huu: What was the LR or hyper parameters for the second stage @Taishi}

% The continual pretraining process followed a carefully designed two-stage curriculum.  The first stage included aimed to expose the model to a diverse range of data and establish a strong foundation for the subsequent stage. In the second stage, we employed a strategic data-mixing approach to enhance the model's performance in specific areas and align it with our desired objectives. Following \cite{taylor2022galactica, Li2021Colossal}, we also mixed in publicly available instruction tuning datasets in both stages of training as described below. As seen in Appendix \ref{analysis_extra}, we see a steeper decline in training loss after 90k steps, which could be attributed to much cleaner data in the second stage. We leave this to future investigations.

\system\ was trained on the LUMI supercomputer\footnote{\url{https://www.lumi-supercomputer.eu/}}, utilizing 128 AMD MI250X GPUs for 48 days. The training process operated entirely on 100\% hydro-powered energy and included waste heat recycling. For orchestration, we adapted a segment of the Bigcode fork of Megatron-LM~\citep{narayanan2021efficient} using the HIP runtime. 
% We use sequence parallelism \cite{sequence-parallel}. 
% We also use Megatron's distributed optimizer with mixed precision training \cite{mixed-precision} in BF16 \cite{bf16} with gradient all-reduce and gradient accumulation in FP32 for training stability. We intend to publicly release the code to foster open science.
For training, we distributed the model using 4-way Tensor Parallelism and 4-way Pipeline Parallelism using the 1F1B schedule to reduce the pipeline bubble \citep{narayanan2021efficient}. We also used Megatron's distributed optimizer \citep{narayanan2021efficient} to distribute the optimizer states across data-parallel processes and eliminate redundancy, reducing the required memory usage.

% To efficiently perform the training process, we adopted 3D parallelism, which integrates data parallelism, tensor parallelism, and pipeline parallelism, Efficient Memory Consumption

For the training of \system, we maintained a consistent batch size of 2048 and a sequence length of 2048 tokens. The learning rate was linearly warmed up to $10^{-4}$ over 2,000 steps, followed by a cosine decay scheduler set to decay the learning rate to $10^{-5}$ by 120,000 steps. 
while optimization utilized the AdamW optimizer \citep{adam, weight-decay} with coefficients $\beta_1=0.9$ and $\beta_2=0.95$. Additionally, Megatron-LM's distributed optimizer with mixed precision training \citep{mixed-precision} was used. Further training details can be found in the Appendix \ref{training_setup_apdx}.


\iffalse
The first stage contains 377B tokens of processed and filtered web data from the Stack, RefinedWeb~\citep{refinedweb}, RedPajama~\citep{together2023redpajama} and the Pile~\citep{gao2020pile} along with a multilingual dataset that we created, containing text in Japanese, English, Vietnamese, Hindi and Finnish. For the distribution of languages, please see Figure \ref{fig:distribution}. We also mix in lower-quality instruction tuning datasets during this stage of training.

For the second stage, we chose to use an expanded mix of high-quality instructions, code, reasoning, and domain-specific datasets. The latter was included for the purpose of future work on expert models \citep{li2022branchtrainmerge, feng2024mixtureofloras}. Additionally, we included a safety instructions dataset called BidenHarrisRedteam, which is described in section \ref{bidenharris}. For the full list of datasets, please see Appendix \ref{datasets}. The total data size for this stage is 58B tokens.
\fi