

% \paragraph{Multilingual LLMs} Early work on large language models has mostly focused on English~\cite{brown2020language}. However, there has been a surge in multilingual models and accompanying datasets to address the need to use these models in non-English languages.

\textbf{Expanding Multilingual Language Models.} Initially, the development of LLMs has predominantly targeted the English language~\citep{brown2020language}, leveraging the extensive corpus of English data available on the Web and the broad applicability of models trained on English text. However, this emphasis has often come at the cost of accommodating the linguistic diversity found across various language demographics~\citep{zhu2023extrapolating,bang2023multitask,zhang2024m3exam}. Recognizing this significant limitation~\citep{robinson2023chatgpt,peng-etal-2024-humaneval}, recent research has proposed foundational LLMs equipped with multilingual capabilities~\citep{chai2023ernie, scao2022bloom,wei2023polylm,shliazhko2022mgpt}, or has explicitly concentrated on addressing the challenges posed by low-resource languages~\citep{ustun2024aya,singh2024aya,gala2023indictrans2}. To integrate multilingual capabilities into existing LLMs, researchers have proposed a variety of methods to enhance multilingual adaptation. These approaches range from continual pretraining techniques~\citep{ibrahim2024simple,gupta2023continual} to initial training on extensive multilingual datasets~\citep{scao2022bloom,chai2023ernie} and then subsequent specialized fine-tuning on a target language~\citep{yang2023bigtranslate, han-etal-2022-x}, and even adaptation through instruction tuning~\citep{shaham2024multilingual,kew2023turning,gala2024airavata}. Critical aspects in multilingual adaptation remain on the availability of high-quality diverse multilingual corpus~\citep{correa2024teenytinyllama} and further the scope of vocabulary of the specific language.

% \SVX{need to clearly state the difference from BLOOM and ERNIE-Code, LLMs on both multilingual NLs and multilingual code}

\textbf{Continual Pretraining.}  Static datasets are impractical for adapting to evolving real-world data, making continual learning essential~\citep{ring1998child,thrun1998lifelong}. Continual pretraining~\citep{gururangan2020dont} allows models to incorporate new knowledge without retraining from scratch, a costly endeavor. As curated datasets like RedPajama~\citep{together2023redpajama} and Dolma~\citep{soldaini2024dolma} become available, integrating them efficiently is crucial. This also enables the extension of models to new modalities, such as code (e.g., StableCode). Previous approaches focus on replay techniques, optimizing learning schedules~\citep{ibrahim2024simple}, soft masking~\citep{ke2023adapting}, and forward/backward transfer~\citep{yildiz2024investigating}.

% The notion of large static datasets becomes impractical when confronted with the dynamic nature of real-time events, evolving facts, and the introduction of new data or concepts within a domain. In such scenarios, continual learning~\citep{ring1998child,thrun1998lifelong,Kirkpatrick2018overcoming,zenke2017continual,rebuffi2017icarl,lopez2017gradient} becomes imperative for large pretrained models to swiftly adapt to these shifting environments. The drive for continual pretraining~\citep{gururangan2020dont} extends beyond the dynamic nature of real-world data; it is also fueled by the prohibitive expense associated with training current foundation models from scratch. As more curated datasets become accessible (e.g. RedPajama~\citep{together2023redpajama}, Dolma~\citep{soldaini2024dolma}, CommonCorpus\footnote{\url{https://huggingface.co/blog/Pclanglais/common-corpus}}, ToolQA~\citep{zhuang2023toolqa}) the idea of incorporating knowledge from these datasets through retraining on the union of all available data sets becomes inherently unfeasible. This also encompasses the integration of new capabilities into foundational models trained on specific data distributions. For example, it involves expanding natural language-based models to include the structured modality of code (e.g. StableCode\footnote{\url{https://stability.ai/news/stable-code-2024-llm-code-completion-release}}). Previous studies on continual pretraining have primarily concentrated on replay, optimizing the learning rate schedule~\citep{ibrahim2024simple}, preserving general knowledge through soft masking attention heads~\citep{ke2023adapting}, exploring the effects of domain similarity and model capacity on forward and backward transfer~\citep{yildiz2024investigating}, and continual post-training for few-shot adaptation~\citep{ke2022continual}.

% \citep{scao2022bloom}
% \citep{chai2023ernie}
% \citep{ustun2024aya}
% \citep{lin2021few}
% \citep{muennighoff2022crosslingual}
% \citep{shliazhko2022mgpt}
% \citep{scao2022language}

% \citep{singh2024aya}
% \citep{kudugunta2024madlad}
% \citep{longpre2023data}
% \citep{laurenccon2022bigscience}
% \citep{albalak2024survey}
% \citep{nguyen2023culturax}

% \textbf{LLM Compliance}
% The extensive utilization of Large Language Models (LLMs) across various applications underscores the necessity for their operation to uphold user privacy, mitigate risks such as misinformation or biased outputs, and ensure transparency regarding their functionality and utilization, all while adhering to local regulations. Independent evaluations and red teaming play crucial roles in assessing these risks. However, conflicts of interest within major AI companies may impede such safety evaluations, underscoring the necessity for a safe harbor for safety research~\citep{longpre2024safe}. Benchmark datasets facilitating such research efforts include those proposed by \cite{zhang2023safetybench} and \cite{sun2023safety}. Studies aimed at enhancing the security, safety, and legal compliance of Large Language Models (LLMs) have encompassed various approaches. These include the creation of datasets based on FAIR data principles~\citep{raza2024fair}, the development of structured LLM auditing mechanisms~\citep{M_kander_2023}, risk assessment of LLM alignment through personalized feedback~\citep{kirk2023personalisation}, structured evaluation of risks associated with LLM deployment~\citep{derczynski2023assessing}, and profiling foundation model transparency~\citep{bommasani2023foundation,bommasani2024foundation}.