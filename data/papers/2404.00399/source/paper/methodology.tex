\iffalse
The continued pretraining process followed a carefully designed two-stage curriculum. In the first stage, the model was trained on a randomly shuffled corpus comprising all datasets for a total of approximately 380B tokens. This extensive training aimed to expose the model to a diverse range of data and establish a strong foundation for the subsequent stage.

In the second stage, the training continued for an additional approximately 60B tokens. During this stage, we employed a strategic data-mixing approach to enhance the model's performance in specific areas and align it with our desired objectives. Safety instructions were intermixed to reinforce the model's adherence to ethical guidelines. Wikipedia data was oversampled to strengthen the model's knowledge acquisition and factual understanding. To improve the model's language generation capabilities, English data such as stories were subsampled. Furthermore, Python coding data and markdown text were oversampled to steer the model towards producing well-formatted and code-friendly outputs, ultimately enhancing its performance in tasks related to Python programming.

This two-stage curriculum, with its targeted data mixing strategy, aimed to fine-tune the model's abilities in key areas while maintaining a balanced exposure to diverse data sources. By allocating more tokens to specific data types, such as Python coding and Wikipedia, we sought to improve the model's proficiency in these domains and align its outputs with the desired format and style.

[Discuss learning rate and total flops]

We then subsample certain domains and oversampled other datasets in order to guide the model to produce more factual data and improve it’s abilities on python.

We also noticed that intermediate checkpoints produced answers that had undesirable formatting such as “$>>>$” which was present in one of the datasets (gorilla), so we reformatted that dataset to have more regular formatting with brackets. 

Moreover, we noticed that the original Stack dataset used the tag <NAME> and this caused our model to generate this tag in some instances. Thus we augmented Stack text by adding fake names.

We also noticed that the fill-in-the-middle tags from Starcoder appeared in random places when outputting text so we continued the 20,000 steps without the fill-in-the-middle objective. Instead, we augmented certain text such as Python code with instructions and the fill-in-the-middle tags as control tokens.

We then finish the continued pretraining for a total of 100,000 steps on 400b tokens.

We then adversarially prompted the 100K step model for safety and where the model failed we created additional training examples.

Next we performed a positional encoding removing as described in Section \_. 

We then performed a finetune with the improved safety instructions and along high quality instruction data in multiple domains and languages, including instructions that produced long output, [Enhanced multilingual domain specific caption dataset], and the MEMIT-IT instruction dataset and the CLAP-INST dataset.  In this stage we added an embedding projection to the beginning of the instructions in some cases, along with a special tag <embed>. The projection is a normalized clip tower embedding. This is to align the model to be able to understand multimodal input. The training in this section totaled roughly 10B tokens.

Lastly, we performed 16+256+4096 LoRA finetune to create LoRA for our experts as described in Section \_.  The training in this section totaled roughy 90B tokens. 

After tuning, we have a model that has approximately 20B parameters (with the 16B base model, clip image towers, clap audio towers, and the 4096 LoRa). Using our router at inference, the model can route an instruction to a specific domain.

Collectively the model was trained on approximately 2T tokens and performs comparable to other english models of this size but also able to perform in other languages, domains, and modals. 

Our model is able to see and hear and understand multiple languages and is adapted to 4096 domains of tasks, while being safety aligned. 

We release all of our data except books3 and all of our models open source.


\fi