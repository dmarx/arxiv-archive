{
  "arxivId": "2404.00399",
  "title": "Aurora-M: Open Source Continual Pre-training for Multilingual Language\n  and Code",
  "authors": "Taishi Nakamura, Mayank Mishra, Simone Tedeschi, Yekun Chai, Jason T Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao, Adalberto Junior, Alpay Ariyak, Aleksandr Drozd, Jordan Clive, Kshitij Gupta, Liangyu Chen, Qi Sun, Ken Tsui, Noah Persaud, Nour Fahmy, Tianlong Chen, Mohit Bansal, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Huu Nguyen, Sampo Pyysalo",
  "abstract": "Pretrained language models are an integral part of AI applications, but their\nhigh computational cost for training limits accessibility. Initiatives such as\nBloom and StarCoder aim to democratize access to pretrained models for\ncollaborative community development. Despite these efforts, such models\nencounter challenges such as limited multilingual capabilities, risks of\ncatastrophic forgetting during continual pretraining, and the high costs of\ntraining models from scratch, alongside the need to align with AI safety\nstandards and regulatory frameworks.\n  This paper presents Aurora-M, a 15B parameter multilingual open-source model\ntrained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually\npretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T\ntokens in total training token count. It is the first open-source multilingual\nmodel fine-tuned on human-reviewed safety instructions, thus aligning its\ndevelopment not only with conventional red-teaming considerations, but also\nwith the specific concerns articulated in the Biden-Harris Executive Order on\nthe Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence.\n  We evaluate Aurora-M across a wide range of tasks and languages, showcasing\nits robustness against catastrophic forgetting and its superior performance in\nmultilingual settings, particularly in safety evaluations. We open-source\nAurora-M and its variants to encourage responsible open-source development of\nlarge language models at https://huggingface.co/aurora-m.",
  "url": "https://arxiv.org/abs/2404.00399",
  "issue_number": 619,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/619",
  "created_at": "2024-12-30T20:00:53.262437",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-30T20:00:09.707Z",
  "main_tex_file": null,
  "published_date": "2024-03-30T15:38:54Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ]
}