---
abstract: |
  Pretrained language models are an integral part of AI applications, but their high computational cost for training limits accessibility. Initiatives such as <span class="smallcaps">Bloom</span> and <span class="smallcaps">StarCoder</span> aim to democratize access to pretrained models for collaborative community development. Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks. This paper presents **<span style="color: violet"><span class="smallcaps">Aurora-M</span></span>**, a `15B` parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from <span class="smallcaps">StarCoderPlus</span> on `435B` additional tokens, <span class="smallcaps">Aurora-M</span> surpasses `2T` tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We evaluate <span class="smallcaps">Aurora-M</span> across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source <span class="smallcaps">Aurora-M</span> and its variants to encourage responsible open-source development of large language models at <https://huggingface.co/aurora-m>.
bibliography:
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "<span class=\"smallcaps\"><span class=\"smallcaps\">Aurora-M</span></span>: Open Source Continual Pre-training for Multilingual Language and Code"
---





# Introduction

# Datasets

# Model Training

# Safety

# Evaluation

# Related Work

# Conclusion

# Ethical Consideration

We believe that transparency and accessibility are fundamental principles in the development and deployment of artificial intelligence technologies. Closed-source LLMs limit public scrutiny, hinder collaboration, and potentially reinforce biases inherent in their development process. In contrast, our commitment to open source models fosters a culture of accountability, collaboration, and inclusivity. By making <span class="smallcaps">Aurora-M</span> accessible to all, we promote innovation, empower diverse voices, and strive for equitable outcomes in AI applications. We firmly believe that openness in AI development is essential for creating solutions that truly serve the needs and values of society. To this end, we prioritized safety guardrails in alignment with the Biden-Harris Executive Order on AI. Furthermore, the multilingual capability of <span class="smallcaps">Aurora-M</span> enhances its usability for users across the world.

On the other hand, each promise comes with peril, and improved technological access through <span class="smallcaps">Aurora-M</span> might also increase the potential number of malicious actors. We overall believe that the general benefit far outweighs the potential misuse and want to emphasize the importance of a considered and ethical use of this technology and thus also of <span class="smallcaps">Aurora-M</span>.

Lastly, we recognize that safety and lawfulness can be contextual to different cultures and laws. We recognize that in our work we focused on a U.S. centric standard, and we believe future work should also explore multi-jurisdictional redteaming.

# Acknowledgments

This work was supported by the “R&D Hub Aimed at Ensuring Transparency and Reliability of Generative AI Models” project of the Ministry of Education, Culture, Sports, Science and Technology, and used resources of LUMI supercomputer under project_462000316.
