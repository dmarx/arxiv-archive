---
abstract: |
  *Query Optimization* (QO) refers to techniques aimed at enhancing the efficiency and quality of Large Language Models (LLMs) in understanding and answering queries, especially complex ones in scenarios like Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the limitations of LLMs by dynamically retrieving and leveraging up-to-date relevant information, which provides a cost-effective solution to the challenge of LLMs producing plausible but potentially inaccurate responses. Recently, as RAG evolves and incorporates multiple components that influence its performance, QO has emerged as a critical element, playing a pivotal role in determining the effectiveness of RAG’s retrieval stage in accurately sourcing the necessary multiple pieces of evidence to answer queries correctly. In this paper, we trace the evolution of QO techniques by summarizing and analyzing significant studies. Through an organized framework and categorization, we aim to consolidate existing QO techniques in RAG, elucidate their technological foundations, and highlight their potential to enhance the versatility and applications of LLMs.
author:
- |
  Mingyang Song, Mao Zheng  
  Machine Learning Platform Department, Tencent  
  `nickmysong@tencent.com`  
bibliography:
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: A Survey of Query Optimization in Large Language Models
---





# Introduction

Large Language Models (LLMs) have made impressive achievements , yet they still encounter notable challenges, particularly in tasks that are domain-specific or heavily reliant on specialized knowledge . One prominent issue is their tendency to produce "hallucinations" when dealing with queries that surpass their training data or necessitate up-to-date information . To mitigate these challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant segments, effectively diminishing the production of factually incorrect content. The widespread integration of RAG into LLMs has established it as a crucial technology for the advancement of query solvers and has improved the suitability of LLMs for practical, real-world applications.

<figure id="intro">
<span class="image placeholder" data-original-image-src="figure/QueryOptimizationAbs.pdf" data-original-image-title=""></span>
<figcaption>Four atomic operations in QO.</figcaption>
</figure>

Since introduced RAG, the field has advanced rapidly, particularly with the emergence of models like ChatGPT. Despite these developments, there is a significant gap in the literature—a thorough analysis of RAG’s underlying mechanisms and the progress made in subsequent studies is lacking. Furthermore, the field is characterized by fragmented research focuses and inconsistent terminology for similar methods, which leads to confusion.

RAG typically involves several core concepts, including but not limited to query optimization, information retrieval, and response generation . Among these, query optimization plays a crucial role in directly determining the relevance of the retrieved information and consequently impacts the quality of the final response. Although query optimization in retrieval-augmented large language models (LLMs) has experienced rapid growth, there has been a lack of systematic synthesis to clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the query optimization process in retrieval-augmented LLMs, charting its evolution, and anticipating future developments. We consider both technical paradigms and research methods, summarizing four main approaches identified in recent LLM-based RAG studies: *Expansion*, *Disambiguation*, *Decomposition*, and *Abstraction*, as shown in Figure , and then categorize the corresponding atomic operations for query optimization and map them accordingly. We classify the difficulty of most queries into four types: those that can be solved with a single piece of explicit evidence, those requiring multiple pieces of explicit evidence, those solvable with a single piece of implicit evidence, and those needing multiple pieces of implicit evidence. We then map these queries to different optimization operations respectively for ease of explanation, as shown in Figure . Next, we briefly introduce each type of query and the corresponding optimization method, as illustrated in Figure .

Overall, this paper aims to meticulously compile and categorize the foundational technical concepts, historical developments, and the range of query optimization methodologies and applications that have emerged since the advent of LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of query optimization in retrieval-augmented LLMs, illuminating the evolution of these techniques and speculating on upcoming trends and innovations.

Query optimization techniques summarized in this paper may involve multiple scenarios, including but not limited to retrieval-augmented generation, question answering, etc. Therefore, we uniformly adopt the term "query" to represent terms such as "query", "question", and "problem" in the subsequent content.

Additionally, this survey is organized as follows: Section  introduces the stratification of query optimization. The subsequent sections delve into key techniques in query optimization: Section  explores query expansion, which is further divided into internal expansion (Section ) and external expansion (Section ). Section  discusses query decomposition. Section  and Section  focus on disambiguation and abstraction. Section  addresses the challenges and future directions in this field. Finally, the conclusion and limitations are presented in Section  and Section .

# Stratification of Query Optimization

Query optimization is crucial for enhancing the effectiveness and precision of retrieval-augmented generation using large language models. By refining users’ original queries, this process addresses several challenges, including ambiguous semantics, complex requirements, and discrepancies in relevance between the query and target documents. Effective query optimization demands a profound understanding of user intent and query context, especially when dealing with intricate or multifaceted inquiries. When implemented successfully, it significantly improves problem-solving performance, substantially impacting the quality of the model’s generated outputs. Ultimately, this enhancement in query processing leads to more accurate and contextually appropriate responses, elevating the overall user experience and increasing the utility of LLMs across various applications.

## Query Expansion

Query Expansion techniques are critical in enhancing the performance of retrieval-augmented generation, particularly when integrated with LLMs . Based on the different sources of knowledge, we broadly categorize it into internal expansion and external expansion. The former focuses on maximizing the value of existing information in the original query or the used LLM without relying on external knowledge sources., while the latter introduces supplementary data from outside sources (e.g., Web or Knowledge base) to fill gaps, provide additional context, or broaden the scope of the content.

### Internal Expansion

In recent years, researchers have developed various query expansion techniques to enhance information retrieval systems by leveraging LLMs. One of the early approaches is <span class="smallcaps">GenRead</span> , which employs a well-designed instruction to prompt LLMs to generate contextual documents based on the initial query. These generated documents are then read by the LLM to produce the final response, effectively bridging the gap between query understanding and answer generation.

Building upon the concept of query expansion, <span class="smallcaps">Query2Doc</span> introduces a simple yet effective approach to improve both sparse and dense retrieval systems. By generating pseudo-documents through the few-shot prompting of LLMs, the original query is expanded with these generated documents. Since LLMs are trained on web-scale text corpora, these pseudo-documents often contain highly relevant information that aids in disambiguating queries and guiding retrievers toward more pertinent results.

In a similar vein, <span class="smallcaps">ReFeed</span> tackles LLM limitations efficiently and cost-effectively by first generating initial outputs. It then retrieves relevant information from large document collections using a retrieval model and incorporates this information into the in-context demonstration to refine the output. This iterative process enhances the quality of the final response by grounding it in retrieved data.

<span class="smallcaps">InteR</span> presents an interactive retrieval framework where retrieval models expand the knowledge within queries by utilizing LLM-generated knowledge collections. Concurrently, LLMs enhance prompt formulation by leveraging retrieved documents, creating a synergistic loop between the retrieval models and the LLMs for improved information access.

Approaching the challenge from a different angle, <span class="smallcaps">HyDE</span> employs a zero-shot prompt with a language model to generate a hypothetical document that captures relevant patterns, even if it contains "hallucinations." An unsupervised contrastive encoder then encodes this document into an embedding vector to identify a neighborhood in the corpus embedding space. By retrieving similar real documents based on vector similarity, <span class="smallcaps">HyDE</span> grounds the generated content to the actual corpus, with the encoder’s dense bottleneck filtering out inaccuracies. <span class="smallcaps">FLARE</span> introduces an iterative anticipation mechanism where, based on the original query, it predicts future content and retrieves relevant information to enhance retrieval performance. If the generated temporary next sentence contains low-confidence tokens, <span class="smallcaps">FLARE</span> treats it as a new query to retrieve additional documents, repeating this process until a satisfactory answer is obtained.

Expanding on query generation, <span class="smallcaps">MILL</span> proposes a query–query–document generation approach that leverages the zero-shot reasoning capabilities of LLMs to produce diverse sub-queries and corresponding documents. A mutual verification process then synergizes the generated and retrieved documents, leading to optimal expansion and comprehensive retrieval results.

To further refine retrieval performance, <span class="smallcaps">GenQREnsemble</span> suggests an ensemble-based prompting technique that uses paraphrases of a zero-shot instruction to generate multiple sets of keywords. By combining these keyword sets, the method enhances retrieval efficacy through diversity and redundancy.

Lastly, <span class="smallcaps">ERRR</span> emphasizes the extraction of parametric knowledge from LLMs and the refinement of these queries using a specialized query optimizer. This approach ensures that only the most pertinent information is retrieved, which is essential for generating accurate and relevant responses.

### External Expansion

External Expansion is a sophisticated process that significantly enhances document content by seamlessly integrating pertinent information from diverse external sources. This methodology augments the overall context, depth, and accuracy of the document corpus. The enrichment process involves strategically incorporating authoritative facts, up-to-date data points, and relevant contextual knowledge derived from a wide array of external datasets, knowledge bases, and curated information repositories.

LameR augments a query with its potential answers by prompting LLMs with a combination of the query and the question’s in-domain candidates. These candidates, regardless of whether they are correct or incorrect, are obtained through a standard retrieval procedure on the target collection. GuideCQR refines queries for conversational query reformulation by leveraging key information from the initially retrieved documents. CSQE promotes the incorporation of knowledge embedded within the corpus and leverages the relevance-assessing capabilities of LLMs to systematically identify pivotal sentences in the initially retrieved documents. These corpus-derived texts are then used to expand the query, along with LLM-empowered expansions, enhancing the relevance prediction between the query and the target documents. MUGI explores and leverages LLMs to generate multiple pseudo references, integrating them with queries to enhance both sparse and dense retrievers.

## Question Decomposition

For complex queries, simply searching with the original query often fails to retrieve adequate information. It is crucial for LLMs to first decompose such queries into simpler, answerable sub-queries, and then search for information relevant to these sub-components. By integrating the responses to these sub-queries, LLMs are able to construct a comprehensive response to the original query.

One such method is the Demonstrate Search Predict (<span class="smallcaps">DSP</span>) framework , which relies on passing natural language texts through sophisticated pipelines between an LLM and a retrieval model (RM). <span class="smallcaps">DSP</span> can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LLM and RM can handle more reliably.

Similarly, techniques like <span class="smallcaps">Least-to-Most</span> prompting utilize few-shot prompts to first decompose a complex problem into a series of simpler subproblems and then solve them in sequence. <span class="smallcaps">Plan-and-Solve</span> prompting involves devising a plan to divide the entire task into smaller subtasks and then carrying out these subtasks according to the plan. These approaches emphasize the importance of decomposition in handling complex queries, allowing models to process each component effectively.

<span class="smallcaps">SELF-ASK</span> introduces the concept of the compositionality gap, which describes the fraction of compositional queries that the model answers incorrectly out of all the compositional queries for which the model answers the sub-queries correctly. This highlights the challenges LLMs face in integrating answers from sub-queries to solve more complex queries.

To address retrieval challenges, approaches like <span class="smallcaps">EAR</span> apply a query expansion model to generate a diverse set of queries, using a query reranker to select those that could lead to better retrieval results. Correction of Knowledge (<span class="smallcaps">CoK</span>) first proposes and prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers, <span class="smallcaps">CoK</span> corrects the rationales step by step by adapting knowledge from the identified domains, serving as a better foundation for the final response consolidation.

In the realm of transferring abilities to LLMs, <span class="smallcaps">ICAT</span> induces reasoning capabilities without any LLM fine-tuning or manual annotation of in-context samples. It transfers the ability to decompose complex queries into simpler ones or generate step-by-step rationales by carefully selecting from available data sources of related tasks.

<span class="smallcaps">ReAct</span> introduces a paradigm to combine reasoning and acting with LLMs for solving diverse language reasoning and decision-making tasks. <span class="smallcaps">ReAct</span> prompts LLMs to generate both verbal reasoning traces and actions on a task in an interleaved manner. This allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting ("reason to act"), while also interacting with external environments (e.g., Wikipedia) to incorporate additional information into reasoning ("act to reason").

Approaches like <span class="smallcaps">AutoPRM</span> and <span class="smallcaps">RA-ISF</span> employ query decomposition to handle complex queries. first decomposes complex problems into more manageable sub-queries with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the sub-query solver. mitigates the impact of irrelevant prompts by iteratively addressing sub-queries while integrating text relevance with self-knowledge answering capabilities. The process involves breaking down the initial multi-turn query into single-turn queries, addressing each sub-task independently, and then synthesizing these responses to resolve the initial query.

Other methods enhance models by equipping them with capabilities for explicit rewriting, decomposition, and disambiguation, such as <span class="smallcaps">RQ-RAG</span>. <span class="smallcaps">LPKG</span> enhances the query planning capabilities of LLMs by grounding predefined patterns in an open-domain knowledge graph to extract numerous instances, which are then verbalized into complex queries and corresponding sub-queries in natural language.

Techniques like <span class="smallcaps">ALTER</span> and <span class="smallcaps">IM-RAG</span> focus on enhancing retrieval and reasoning processes. Specifically, <span class="smallcaps">ALTER</span> employs a question augmentor to enhance the original question by generating multiple sub-queries, each examining the original question from different perspectives, for handling complex table reasoning tasks. <span class="smallcaps">IM-RAG</span> introduces a Refiner that improves the outputs from the Retriever, effectively bridging the gap between the Reasoner and information retrieval modules with varying capabilities and fostering multi-round communications.

<span class="smallcaps">REAPER</span> , a reasoning-based planner, is designed for efficient retrieval required for complex queries. Using a single and smaller LLM, REAPER generates a plan that includes the tools to call, the order in which they should be called, and the arguments for each tool.

Building on previous studies, <span class="smallcaps">HiRAG</span> decomposes the original query into multi-hop queries. Each sub-query is answered based on retrieved knowledge, and the answers are then integrated using the Chain-of-Thought (CoT) approach to derive the final answer. <span class="smallcaps">MQA-KEAL</span> stores knowledge edits as structured knowledge units in external memory. To solve multi-hop queries, it first uses task decomposition to break the query into smaller sub-problems. For each sub-problem, it iteratively queries the external memory and/or the target LLM to generate the final response.

Recent methods like <span class="smallcaps">RichRAG</span> and <span class="smallcaps">ConTReGen</span> further improve the retrieval process. <span class="smallcaps">RichRAG</span> introduces a sub-aspect explorer to dissect input queries and uncover their latent facets. This is integrated with a multi-faceted retriever, which curates a diverse corpus of external documents pertinent to these identified sub-aspects to answer queries. <span class="smallcaps">ConTReGen</span> proposes a context-driven, tree-structured retrieval approach to enhance the depth and relevance of retrieved content. It incorporates a hierarchical, top-down in-depth exploration of query facets with a systematic bottom-up synthesis, ensuring comprehensive coverage and coherent integration of multifaceted information.

<span class="smallcaps">Plan×RAG</span> formulates a comprehensive reasoning plan represented as a directed acyclic graph (DAG). This reasoning DAG decomposes the main query into interrelated atomic sub-queries, providing a computational structure that enables efficient information sharing between sub-queries. <span class="smallcaps">RAG-Star</span> seamlessly integrates retrieved information to guide a tree-based deliberative reasoning process, leveraging the inherent knowledge of LLMs. By utilizing Monte Carlo Tree Search, <span class="smallcaps">RAG-Star</span> iteratively plans intermediate sub-queries and generates answers for reasoning based on the capabilities of the LLM itself.

## Query Disambiguation

For ambiguous queries with multiple possible answers, relying solely on the original query for information retrieval is inadequate. To deliver complete and nuanced responses, LLMs must learn to clarify the query by identifying the user’s intent and then formulate a more targeted search query. After gathering relevant information, LLMs can provide a detailed and comprehensive response. There are mainly two types of approaches for query disambiguation. One is when the query itself is ambiguous, and the other is for multi-turn queries, where it’s necessary to rewrite the query by incorporating historical dialogue content to achieve disambiguation .

early introduces a deductive reasoning format based on the natural language that decomposes the reasoning verification process into a series of step-by-step subprocesses. Each subprocess receives only the necessary context and premises, allowing LLMs to generate precise reasoning steps that are rigorously grounded on prior ones. This approach empowers language models to conduct reasoning self-verification sequentially, significantly enhancing the rigor and trustworthiness of the generated reasoning steps.

Building upon the refinement of model reasoning, <span class="smallcaps">EchoPrompt</span> introduces a query-rephrasing subtask by employing prompts like “*Let’s repeat the query and also think step by step.*”. This encourages the model to restate the query in its own words before engaging in reasoning, ensuring better understanding and consistency. Importantly, the prompt used for answer extraction remains consistent across all zero-shot methodologies. <span class="smallcaps">ToC</span> recursively builds a tree of disambiguations for ambiguous queries by utilizing few-shot prompting and external knowledge. It retrieves relevant facts to generate a comprehensive long-form answer based on this tree, thus providing more accurate and detailed responses. <span class="smallcaps">InfoCQR</span> introduces a novel "rewrite-then-edit" framework, where LLMs first rewrite the original query and then revise the rewritten query to eliminate ambiguities. The well-designed instructions independently guide the LLMs through the rewriting and editing tasks, resulting in more informative and unambiguous queries.

To further manipulate the disambiguated query, <span class="smallcaps">AdaQR</span> proposes a novel preference optimization approach, which aims to tailor rewriters to better suit retrievers by utilizing conversation answers to model retrievers’ preferences. Specifically, the trained rewriter generates several rewrites, which are then used as queries to retrieve passages from a target retriever. Then, <span class="smallcaps">AdaQR</span> calculates the conditional probability of the answer given each retrieved passage and the conversation, obtaining the marginal probability of the answer by marginalizing over the set of passages. This marginal probability serves as a reward that quantifies the retrievers’ preferences over rewrites and pairs these rewrites based on their rewards to optimize the trained rewriter using direct preference optimization.

<span class="smallcaps">MaFeRw</span> improves the RAG performance by integrating multi-aspect feedback from both the retrieved documents and the generated responses as rewards to explore the optimal query rewriting strategy. This approach leverages comprehensive feedback to enhance the effectiveness of query rewriting. <span class="smallcaps">CHIQ</span> leverages the NLP capabilities of LLMs, such as resolving coreference relations and expanding context, to reduce ambiguity in conversational history. This enhancement improves the relevance of the generated search queries. We investigate various methods for integrating refined conversational history into existing frameworks, including ad-hoc query rewriting, generating pseudo-supervision signals for fine-tuning query rewriting models, and combining both approaches.

# Query Abstraction

For complex multi-hop queries, sequential decomposition may not yield accurate answers and can even complicate the query further. Humans often step back and perform abstractions to arrive at high-level principles to solve complex queries, reducing the chance of making errors in the intermediate reasoning steps .

<span class="smallcaps">Step-Back</span> manipulates the initial query using meticulously designed prompts that steer the LLM’s reasoning process. This ensures that the outputs are more closely aligned with the idea behind the original query, especially for tasks requiring complex reasoning. Building upon the idea of guiding reasoning through abstraction, requires LLMs to engage in conceptual reasoning with abstract queries, producing solutions within a verifiable symbolic space. This promotes a deeper understanding and handling of abstract concepts. <span class="smallcaps">CoA</span> abstracts the general CoT reasoning into a reasoning chain with abstract variables. This enables LLMs to solve queries by utilizing domain-specialized tools, such as calculation results from a calculator or relevant articles retrieved from web search engines.

Similarly, <span class="smallcaps">AoT</span> utilizes an abstract skeletal framework to structure the entire reasoning process, potentially unlocking the key to eliciting abstract reasoning. Unlike the unconstrained CoT, the <span class="smallcaps">AoT</span> format explicitly integrates different levels of abstraction throughout the reasoning process. At each higher level, the abstraction is a distilled version of the lower level, containing fewer concrete details while clearly stating the objective and functionality of each reasoning step. In addition, generates a higher level of abstraction information that serves as the contextual background for the existing query. This approach enriches the direct information about the query object within the initial query, providing a more comprehensive understanding.

Focusing on multi-faceted queries, <span class="smallcaps">MA-RIR</span> defines a query aspect as a sub-span of a multi-aspect query that represents a distinct topic or facet within the query. This allows for more focused and effective reasoning across different aspects of a complex query. To further improve reasoning efficiency and accuracy, <span class="smallcaps">Meta-Reasoning</span> seeks to deconstruct the semantics of entities and operations within each query into generic symbolic representations. This methodology allows LLMs to learn generalized reasoning patterns across a variety of semantically complex scenarios.

Recently, recognizing the role of explicit logical guidance, <span class="smallcaps">RuleRAG</span> observes that widespread logical rules can guide people to accomplish given tasks. It proposes a new approach that can recall documents supporting queries logically in the directions of these rules, generating final responses based on retrieved information and attributable rules. <span class="smallcaps">SimGRAG</span> effectively tackles the challenge of aligning query texts with knowledge graph (KG) structures through a two-stage process. First, in the query-to-pattern stage, it uses a large language model to transform queries into desired graph patterns. Second, in the pattern-to-subgraph stage, it quantifies the alignment between these patterns and candidate subgraphs using a graph semantic distance (GSD).

# Challenges and Future Directions

## Query-Centric Process Reward Model

A promising approach to improving reasoning in LLMs is the use of process reward models (PRMs) . PRMs provide feedback at each step of a multi-step reasoning process, potentially enhancing credit assignment compared to outcome reward models (ORMs) that only provide feedback at the final step. However, the processes in PRMs generated by chain-of-thought (CoT) prompting methods are usually unpredictable and make it difficult to find the optimal path. Utilizing the optimal path for optimizing complex queries to construct query-centric process reward models may be a simpler and more effective strategy, which means rewards are provided at each sub-query of a multi-step reasoning process.

## Query Optimization Benchmark

Currently, the notable lack of benchmarks for query optimization hinders the consistent assessment and comparison of different query optimization techniques across various scenarios. Typically, the issue is especially prominent in complex contexts, such as optimizing queries for search within multi-turn retrieval-augmented dialogues and in the decomposition of intricate problems. Developing comprehensive evaluation frameworks and benchmarks may significantly benefit advancements in query optimization techniques, such as existing benchmarks in RAG .

## Improving Query Optimization Efficiency and Quality

Many existing methods fail to pursue the most optimal query optimization paths, relying instead on strategies akin to exhaustive enumeration. This kind of strategy leads to increased computational time and higher search costs, as the system expends resources exploring numerous non-optimal paths. Additionally, it may introduce inconsistent or irrelevant search information, potentially impacting the overall quality and reliability of the results.

Future research should focus on designing efficient algorithms capable of identifying optimal optimization pathways without the need for exhaustive search. Such advancements would reduce time and resource expenditures while enhancing the consistency and accuracy of query optimization outcomes. For example, query decomposition can further be categorized into parallel decomposition and sequential decomposition. Sequential decomposition typically corresponds to multi-hop queries. The reason for this classification is that parallel decomposition usually does not increase additional search time, while sequential decomposition requires iterative searching to solve dependent queries one by one, which typically increases search time as the number of hops increases.

## Enhancing Query Optimization via Post-Performance

A typical paradigm of prompting-based methods involves providing LLMs with several ground-truth optimizing cases (optional) and a task description for the query optimizer. Although LLMs are capable of identifying the potential user intents of a query, they lack awareness of the retrieval quality resulting from the optimized query. This disconnect can result in optimized queries that appear correct but produce unsatisfactory ranking results. While some existing studies have utilized reinforcement learning to adjust the query optimization process based on generation results, a substantial realm of research remains unexplored concerning the integration of ranking results.

# Conclusion

This in-depth analysis explores the domain of query optimization techniques, with a focus on their application to retrieval-augmented LLMs. Our study encompasses a broad range of optimization methods, providing a comprehensive understanding of the field. By examining the complexities of query optimization, we identify the key challenges and opportunities that arise in this area. As research in this field continues to advance, the development of specialized methodologies tailored to the needs of retrieval-augmented LLMs is crucial for unlocking their full potential across various domains. This survey aims to serve as a valuable resource for retrieval-augmented LLMs, providing a detailed overview of the current landscape and encouraging further investigation into this vital topic.

# Limitations

The main goal of this paper is to provide a survey of the existing retrieval-augmented LLMs. Since we do not propose new models, there are no potential social risks to the best of our knowledge. Our work may benefit the research community by providing more introspection into the current state-of-the-art retrieval-augmented LLMs.
