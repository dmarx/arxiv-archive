\begin{thebibliography}{277}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and
  Morcos]{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic
  deduplication.
\newblock \emph{arXiv preprint arXiv:2303.09540}, 2023.

\bibitem[Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla,
  Bach, Bahree, Bakhtiari, Behl, et~al.]{abdin2024phi}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,
  Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl,
  et~al.
\newblock Phi-3 technical report: A highly capable language model locally on
  your phone.
\newblock \emph{arXiv preprint arXiv:2404.14219}, 2024.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy,
  Lebr{\'o}n, and Sanghai]{ainslie2023gqa}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico
  Lebr{\'o}n, and Sumit Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from
  multi-head checkpoints.
\newblock \emph{arXiv preprint arXiv:2305.13245}, 2023.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei,
  Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski,
  Barreira, Vinyals, Zisserman, and Simonyan]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
  Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew
  Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo
  Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.14198}, 2022.

\bibitem[Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli,
  Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic,
  et~al.]{almazrouei2023falcon}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
  Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow,
  Julien Launay, Quentin Malartic, et~al.
\newblock The falcon series of open language models.
\newblock \emph{arXiv preprint arXiv:2311.16867}, 2023.

\bibitem[Alzahrani et~al.(2024)Alzahrani, Alyahya, Alnumay, Alrashed, Alsubaie,
  Almushaykeh, Mirza, Alotaibi, Al{-}Twairesh, Alowisheq, Bari, and
  Khan]{alzahrani2024when}
Norah Alzahrani, Hisham~Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed,
  Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora
  Al{-}Twairesh, Areeb Alowisheq, M.~Saiful Bari, and Haidar Khan.
\newblock When benchmarks are targets: Revealing the sensitivity of large
  language model leaderboards.
\newblock \emph{CoRR}, abs/2402.01781, 2024.
\newblock \doi{10.48550/ARXIV.2402.01781}.
\newblock \url{https://doi.org/10.48550/arXiv.2402.01781}.

\bibitem[Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and
  Hajishirzi]{amini2019mathqa}
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and
  Hannaneh Hajishirzi.
\newblock Mathqa: Towards interpretable math word problem solving with
  operation-based formalisms.
\newblock \emph{arXiv preprint arXiv:1905.13319}, 2019.

\bibitem[An et~al.(2023{\natexlab{a}})An, Gong, Zhong, Li, Zhang, Kong, and
  Qiu]{an2023eval}
Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and
  Xipeng Qiu.
\newblock L-eval: Instituting standardized evaluation for long context language
  models.
\newblock \emph{arXiv preprint arXiv:2307.11088}, 2023{\natexlab{a}}.

\bibitem[An et~al.(2023{\natexlab{b}})An, Ma, Lin, Zheng, Lou, and
  Chen]{an2023learning}
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu
  Chen.
\newblock Learning from mistakes makes llm better reasoner.
\newblock \emph{arXiv preprint arXiv:2310.20689}, 2023{\natexlab{b}}.

\bibitem[Anil et~al.(2024)Anil, Durmus, Sharma, Benton, Kundu, Batson, Rimsky,
  Tong, Mu, Ford, et~al.]{anil2024many}
Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua
  Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et~al.
\newblock Many-shot jailbreaking.
\newblock \emph{Anthropic, April}, 2024.

\bibitem[Ansel et~al.(2024)Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao,
  Bell, Berard, Burovski, et~al.]{ansel2024pytorch}
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael
  Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et~al.
\newblock Pytorch 2: Faster machine learning through dynamic python bytecode
  transformation and graph compilation.
\newblock In \emph{Proceedings of the 29th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems, Volume
  2}, pages 929--947, 2024.

\bibitem[Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and
  Parikh]{vqav2}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
  C.~Lawrence Zitnick, and Devi Parikh.
\newblock {VQA}: {V}isual {Q}uestion {A}nswering.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2015.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang,
  Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu,
  Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan,
  Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
  Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
  Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
  Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
  Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang,
  Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,
  Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou,
  Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen,
  Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli,
  Li, Tran{-}Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosiute,
  Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson,
  Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen{-}Lawton, Conerly,
  Henighan, Hume, Bowman, Hatfield{-}Dodds, Mann, Amodei, Joseph, McCandlish,
  Brown, and Kaplan]{constitutional-ai-bai}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain,
  Deep Ganguli, Dustin Li, Eli Tran{-}Johnson, Ethan Perez, Jamie Kerr, Jared
  Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute,
  Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer,
  Noem{\'{\i}} Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam
  Ringer, Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera
  Lanham, Timothy Telleen{-}Lawton, Tom Conerly, Tom Henighan, Tristan Hume,
  Samuel~R. Bowman, Zac Hatfield{-}Dodds, Ben Mann, Dario Amodei, Nicholas
  Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional {AI:} harmlessness from {AI} feedback.
\newblock \emph{CoRR}, abs/2212.08073, 2022.
\newblock \doi{10.48550/ARXIV.2212.08073}.
\newblock \url{https://doi.org/10.48550/arXiv.2212.08073}.

\bibitem[Barrault et~al.(2023)Barrault, Chung, Meglioli, Dale, Dong,
  Duppenthaler, Duquenne, Ellis, Elsahar, Haaheim, Hoffman, Hwang, Inaguma,
  Klaiber, Kulikov, Li, Licht, Maillard, Mavlyutov, Rakotoarison, Sadagopan,
  Ramakrishnan, Tran, Wenzek, Yang, Ye, Evtimov, Fernandez, Gao, Hansanti,
  Kalbassi, Kallet, Kozhevnikov, Gonzalez, Roman, Touret, Wong, Wood, Yu,
  Andrews, Balioglu, Chen, Costa-jussà, Elbayad, Gong, Guzmán, Heffernan,
  Jain, Kao, Lee, Ma, Mourachko, Peloquin, Pino, Popuri, Ropers, Saleem,
  Schwenk, Sun, Tomasello, Wang, Wang, Wang, and
  Williamson]{barrault2023seamless}
Loïc Barrault, Yu-An Chung, Mariano~Coria Meglioli, David Dale, Ning Dong,
  Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin
  Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber,
  Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov,
  Alice Rakotoarison, Kaushik~Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran,
  Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez,
  Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom
  Kozhevnikov, Gabriel~Mejia Gonzalez, Robin~San Roman, Christophe Touret,
  Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen
  Chen, Marta~R Costa-jussà, Maha Elbayad, Hongyu Gong, Francisco Guzmán,
  Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko,
  Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah
  Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang,
  Skyler Wang, and Mary Williamson.
\newblock Seamless: Multilingual expressive and streaming speech translation.
\newblock \emph{arXiv preprint arXiv:2312.05187}, 2023.

\bibitem[Battey and Gupta(2024)]{battey2024storage}
Robin Battey and Sumit Gupta.
\newblock Training llama: A storage perspective, 2024.
\newblock
  \url{https://atscaleconference.com/videos/training-llama-a-storage-perspective/}.

\bibitem[Bellagente et~al.(2024)Bellagente, Tow, Mahan, Phung, Zhuravinskyi,
  Adithyan, Baicoianu, Brooks, Cooper, Datta, et~al.]{bellagente2024stable}
Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi,
  Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta,
  et~al.
\newblock Stable lm 2 1.6 b technical report.
\newblock \emph{arXiv preprint arXiv:2402.17834}, 2024.

\bibitem[Benchekroun et~al.(2023)Benchekroun, Dervishi, Ibrahim, Gaya,
  Martinet, Mialon, Scialom, Dupoux, Hupkes, and
  Vincent]{benchekroun2023worldsense}
Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean{-}Baptiste Gaya, Xavier
  Martinet, Gr{\'{e}}goire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke
  Hupkes, and Pascal Vincent.
\newblock Worldsense: {A} synthetic benchmark for grounded reasoning in large
  language models.
\newblock \emph{CoRR}, abs/2311.15930, 2023.
\newblock \doi{10.48550/ARXIV.2311.15930}.
\newblock \url{https://doi.org/10.48550/arXiv.2311.15930}.

\bibitem[Berant et~al.(2013)Berant, Chou, Frostig, and
  Liang]{berant-etal-2013-semantic}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
\newblock Semantic parsing on {F}reebase from question-answer pairs.
\newblock In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and
  Steven Bethard, editors, \emph{Proceedings of the 2013 Conference on
  Empirical Methods in Natural Language Processing}, pages 1533--1544, Seattle,
  Washington, USA, October 2013. Association for Computational Linguistics.
\newblock \url{https://aclanthology.org/D13-1160}.

\bibitem[Bhatt et~al.(2023)Bhatt, Chennabasappa, Nikolaidis, Wan, Evtimov,
  Gabi, Song, Ahmad, Aschermann, Fontana, et~al.]{bhatt2023purple}
Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan
  Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann,
  Lorenzo Fontana, et~al.
\newblock Purple llama cyberseceval: A secure coding benchmark for language
  models.
\newblock \emph{arXiv preprint arXiv:2312.04724}, 2023.

\bibitem[Bhatt et~al.(2024)Bhatt, Chennabasappa, Li, Nikolaidis, Song, Wan,
  Ahmad, Aschermann, Chen, Kapil, et~al.]{bhatt2024cyberseceval}
Manish Bhatt, Sahana Chennabasappa, Yue Li, Cyrus Nikolaidis, Daniel Song,
  Shengye Wan, Faizan Ahmad, Cornelius Aschermann, Yaohui Chen, Dhaval Kapil,
  et~al.
\newblock Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for
  large language models.
\newblock \emph{arXiv preprint arXiv:2404.13161}, 2024.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley,
  O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff,
  et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley,
  Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit,
  USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages
  2397--2430. PMLR, 2023.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem[Bizzoni et~al.(2020)Bizzoni, Juzek, Espa{\~n}a-Bonet, Dutta~Chowdhury,
  van Genabith, and Teich]{bizzoni-etal-2020-human}
Yuri Bizzoni, Tom~S Juzek, Cristina Espa{\~n}a-Bonet, Koel Dutta~Chowdhury,
  Josef van Genabith, and Elke Teich.
\newblock How human is machine translationese? comparing human and machine
  translations of text and speech.
\newblock In Marcello Federico, Alex Waibel, Kevin Knight, Satoshi Nakamura,
  Hermann Ney, Jan Niehues, Sebastian St{\"u}ker, Dekai Wu, Joseph Mariani, and
  Francois Yvon, editors, \emph{Proceedings of the 17th International
  Conference on Spoken Language Translation}, pages 280--290, Online, July
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.iwslt-1.34}.
\newblock \url{https://aclanthology.org/2020.iwslt-1.34}.

\bibitem[Blakeney et~al.(2024)Blakeney, Paul, Larsen, Owen, and
  Frankle]{blakeney2024doesdatasparkjoy}
Cody Blakeney, Mansheej Paul, Brett~W. Larsen, Sean Owen, and Jonathan Frankle.
\newblock Does your data spark joy? performance gains from domain upsampling at
  the end of training, 2024.
\newblock \url{https://arxiv.org/abs/2406.03476}.

\bibitem[Bordes et~al.(2024)Bordes, Pang, Ajay, Li, Bardes, Petryk, Mañas,
  Lin, Mahmoud, Jayaraman, Ibrahim, Hall, Xiong, Lebensold, Ross, Jayakumar,
  Guo, Bouchacourt, Al-Tahan, Padthe, Sharma, Xu, Tan, Richards, Lavoie,
  Astolfi, Hemmat, Chen, Tirumala, Assouel, Moayeri, Talattof, Chaudhuri, Liu,
  Chen, Garrido, Ullrich, Agrawal, Saenko, Celikyilmaz, and
  Chandra]{bordes2024vlm}
Florian Bordes, Richard~Yuanzhe Pang, Anurag Ajay, Alexander~C. Li, Adrien
  Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav
  Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold,
  Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider
  Al-Tahan, Karthik Padthe, Vasu Sharma, Hu~Xu, Xiaoqing~Ellen Tan, Megan
  Richards, Samuel Lavoie, Pietro Astolfi, Reyhane~Askari Hemmat, Jun Chen,
  Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika
  Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya
  Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra.
\newblock An introduction to vision-language modeling.
\newblock 2024.

\bibitem[Broder(1997)]{666900}
A.Z. Broder.
\newblock On the resemblance and containment of documents.
\newblock In \emph{Proceedings. Compression and Complexity of SEQUENCES 1997
  (Cat. No.97TB100171)}, pages 21--29, 1997.
\newblock \doi{10.1109/SEQUEN.1997.666900}.

\bibitem[Cai et~al.(2024)Cai, Liu, Mustikovela, Meyer, Chai, Park, and
  Lee]{cai2023vipllava}
Mu~Cai, Haotian Liu, Siva~Karthik Mustikovela, Gregory~P. Meyer, Yuning Chai,
  Dennis Park, and Yong~Jae Lee.
\newblock Making large multimodal models understand arbitrary visual prompts.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2024.

\bibitem[Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tram\`er, and
  Zhang]{carlini2022quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
  Tram\`er, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock \emph{arXiv:2202.07646}, 2022.
\newblock \url{https://arxiv.org/abs/2202.07646}.

\bibitem[Carlini et~al.(2023)Carlini, Hayes, Nasr, Jagielski, Sehwag, Tramer,
  Balle, Ippolito, and Wallace]{carlini2023extracting}
Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,
  Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace.
\newblock Extracting training data from diffusion models.
\newblock In \emph{32nd USENIX Security Symposium (USENIX Security 23)}, pages
  5253--5270, 2023.

\bibitem[Cassano et~al.(2023)Cassano, Gouwar, Nguyen, Nguyen, {Phipps-Costin},
  Pinckney, Yee, Zi, Anderson, Feldman, Guha, Greenberg, and
  Jangda]{cassano2022multiple}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna
  {Phipps-Costin}, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane
  Anderson, Molly~Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
\newblock {MultiPL-E}: A scalable and polyglot approach to benchmarking neural
  code generation.
\newblock \emph{{IEEE} Trans. Software Eng.}, 49\penalty0 (7):\penalty0
  3675--3691, 2023.

\bibitem[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and
  Wong]{2023pair}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J. Pappas,
  and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock \emph{arXiv preprint arXiv:2310.08419}, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2023)Chen, Zheng, Wu, Gong, Song, Zhang, and
  Li]{chen2023breakinglanguagebarriersmultilingual}
Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Yangqiu Song, Dongmei Zhang, and Jia
  Li.
\newblock Breaking language barriers in multilingual mathematical reasoning:
  Insights and observations, 2023.
\newblock \url{https://arxiv.org/abs/2310.20246}.

\bibitem[Chen et~al.(2022)Chen, Ma, Wang, and Cohen]{chen2022program}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W Cohen.
\newblock Program of thoughts prompting: Disentangling computation from
  reasoning for numerical reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.12588}, 2022.

\bibitem[Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang,
  Zhu, Jordan, Gonzalez, et~al.]{chiang2024chatbot}
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios~Nikolas Angelopoulos,
  Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph~E
  Gonzalez, et~al.
\newblock Chatbot arena: An open platform for evaluating llms by human
  preference.
\newblock \emph{arXiv preprint arXiv:2403.04132}, 2024.

\bibitem[Chiu et~al.(2022)Chiu, Qin, Zhang, Yu, and Wu]{chiu2022self}
Chung-Cheng Chiu, James Qin, Yu~Zhang, Jiahui Yu, and Yonghui Wu.
\newblock Self-supervised learning with random-projection quantizer for speech
  recognition.
\newblock In \emph{International Conference on Machine Learning}, pages
  3915--3924. PMLR, 2022.

\bibitem[Choi et~al.(2018)Choi, He, Iyyer, Yatskar, Yih, Choi, Liang, and
  Zettlemoyer]{choi-etal-2018-quac}
Eunsol Choi, He~He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy
  Liang, and Luke Zettlemoyer.
\newblock {Q}u{AC}: Question answering in context.
\newblock In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun{'}ichi
  Tsujii, editors, \emph{Proceedings of the 2018 Conference on Empirical
  Methods in Natural Language Processing}, pages 2174--2184, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1241}.
\newblock \url{https://aclanthology.org/D18-1241}.

\bibitem[Chou et~al.(2023)Chou, Chien, Hsu, Livescu, Babu, Conneau, Baevski,
  and Auli]{chou2023sutlm}
Ju-Chieh Chou, Chung-Ming Chien, Wei-Ning Hsu, Karen Livescu, Arun Babu, Alexis
  Conneau, Alexei Baevski, and Michael Auli.
\newblock Toward joint language modeling for speech units and text.
\newblock 2023.

\bibitem[Choudhury et~al.(2024)Choudhury, Wang, Pelkonen, Srinivasan, Jain,
  Lin, David, Soleimanifard, Chen, Yadav, Tijoriwala, Samoylov, and
  Tang]{choudhury2024mast}
Arnab Choudhury, Yang Wang, Tuomas Pelkonen, Kutta Srinivasan, Abha Jain,
  Shenghao Lin, Delia David, Siavash Soleimanifard, Michael Chen, Abhishek
  Yadav, Ritesh Tijoriwala, Denis Samoylov, and Chunqiang Tang.
\newblock {MAST}: Global scheduling of ml training across geo-distributed
  datacenters at hyperscale.
\newblock In \emph{Proceedings from 18th USENIX Symposium on Operating Systems
  Design and Implementation}, 2024.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (240):\penalty0 1--113, 2023.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Narang, Mishra,
  Yu, Zhao, Huang, Dai, Yu, Petrov, Chi, Dean, Devlin, Roberts, Zhou, Le, and
  Wei]{chung2022scalinginstruction}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent~Y. Zhao, Yanping
  Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob
  Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{CoRR}, abs/2210.11416, 2022.
\newblock \doi{10.48550/ARXIV.2210.11416}.
\newblock \url{https://doi.org/10.48550/arXiv.2210.11416}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Conneau et~al.(2023)Conneau, Ma, Khanuja, Zhang, Axelrod, Dalmia,
  Riesa, Rivera, and Bapna]{conneau2023fleurs}
Alexis Conneau, Min Ma, Simran Khanuja, Yu~Zhang, Vera Axelrod, Siddharth
  Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna.
\newblock Fleurs: Few-shot learning evaluation of universal representations of
  speech.
\newblock In \emph{2022 IEEE Spoken Language Technology Workshop (SLT)}, pages
  798--805, 2023.
\newblock \doi{10.1109/SLT54892.2023.10023141}.

\bibitem[Costa-jussà et~al.(2023)Costa-jussà, Meglioli, Andrews, Dale,
  Hansanti, Kalbassi, Mourachko, Ropers, and Wood]{mutox}
Marta~R. Costa-jussà, Mariano~Coria Meglioli, Pierre Andrews, David Dale,
  Prangthip Hansanti, Elahe Kalbassi, Alex Mourachko, Christophe Ropers, and
  Carleigh Wood.
\newblock Mutox: Universal multilingual audio-based toxicity dataset and
  zero-shot detector.
\newblock 2023.

\bibitem[Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and
  Hoi]{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with
  instruction tuning.
\newblock 2023.

\bibitem[Databricks(2024)]{databricksmpt}
Databricks.
\newblock {Introducing MPT-7B: A New Standard for Open-Source, Commercially
  Usable LLMs} blog.
\newblock \url{https://www.databricks.com/blog/mpt-7b}, 2024.

\bibitem[DeepSeek-AI et~al.(2024)DeepSeek-AI, Zhu, Guo, Shao, Yang, Wang, Xu,
  Wu, Li, Gao, Ma, Zeng, Bi, Gu, Xu, Dai, Dong, Zhang, Piao, Gou, Xie, Hao,
  Wang, Song, Chen, Xie, Guan, You, Liu, Du, Gao, Lu, Chen, Wang, Deng, Li,
  Zhao, Ruan, Luo, and
  Liang]{deepseekai2024deepseekcoderv2breakingbarrierclosedsource}
DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin
  Xu, Y.~Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui
  Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou,
  Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang
  Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen,
  Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo,
  and Wenfeng Liang.
\newblock Deepseek-coder-v2: Breaking the barrier of closed-source models in
  code intelligence, 2024.
\newblock \url{https://arxiv.org/abs/2406.11931}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Didolkar et~al.(2024)Didolkar, Goyal, Ke, Guo, Valko, Lillicrap,
  Rezende, Bengio, Mozer, and Arora]{didolkar2024metacognitive}
Aniket Didolkar, Anirudh Goyal, Nan~Rosemary Ke, Siyuan Guo, Michal Valko,
  Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, and Sanjeev
  Arora.
\newblock Metacognitive capabilities of llms: An exploration in mathematical
  problem solving.
\newblock \emph{arXiv preprint arXiv:2405.12205}, 2024.

\bibitem[Dong et~al.(2019)Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and
  Hon]{dong2019unified}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv:2010.11929}, 2020.

\bibitem[Dua et~al.(2019)Dua, Wang, Dasigi, Stanovsky, Singh, and
  Gardner]{dua-etal-2019-drop}
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and
  Matt Gardner.
\newblock {DROP}: A reading comprehension benchmark requiring discrete
  reasoning over paragraphs.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
  \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of
  the Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 2368--2378, Minneapolis, Minnesota,
  June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1246}.
\newblock \url{https://aclanthology.org/N19-1246}.

\bibitem[Esser et~al.(2024)Esser, Kulal, Blattmann, Entezari, M{\"u}ller,
  Saini, Levi, Lorenz, Sauer, Boesel, et~al.]{esser2024scaling}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas
  M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic
  Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image
  synthesis.
\newblock \emph{arXiv preprint arXiv:2403.03206}, 2024.

\bibitem[Farid(2021)]{farid2021overview}
Hany Farid.
\newblock An overview of perceptual hashing.
\newblock \emph{Journal of Online Trust and Safety}, 1\penalty0 (1), 2021.

\bibitem[Fathullah et~al.(2024)Fathullah, Wu, Lakomkin, Li, Jia, Shangguan,
  Mahadeokar, Kalinli, Fuegen, and Seltzer]{fathullah2024audiochatllama}
Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Ke~Li, Junteng Jia, Yuan
  Shangguan, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, and Mike Seltzer.
\newblock Audiochatllama: Towards general-purpose speech abilities for llms.
\newblock In \emph{Proceedings of the 2024 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies (Volume 1: Long Papers)}, pages 5522--5532, 2024.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (120):\penalty0 1--39, 2022.

\bibitem[Gangidi et~al.(2024)Gangidi, Miao, Zheng, Bondu, Goes, Morsy, Puri,
  Riftadi, Shetty, Yang, Zhang, Fernandez, Gandham, and Zeng]{gangidi2024rmda}
Adithya Gangidi, Rui Miao, Shengbao Zheng, Sai~Jayesh Bondu, Guilherme Goes,
  Hany Morsy, Rohit Puri, Mohammad Riftadi, Ashmitha~Jeevaraj Shetty, Jingyi
  Yang, Shuqiang Zhang, Mikel~Jimenez Fernandez, Shashidhar Gandham, and Hongyi
  Zeng.
\newblock {RDMA over Ethernet for Distributed AI Training at Meta Scale}.
\newblock In \emph{ACM Special Interest Group on Data Communication (SIGCOMM)},
  2024.
\newblock \url{https://doi.org/10.1145/3651890.3672233}.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and
  Neubig]{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie
  Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock In \emph{International Conference on Machine Learning}, pages
  10764--10799. PMLR, 2023.

\bibitem[Gekhman et~al.(2024)Gekhman, Yona, Aharoni, Eyal, Feder, Reichart, and
  Herzig]{gekhman2024does}
Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart,
  and Jonathan Herzig.
\newblock Does fine-tuning llms on new knowledge encourage hallucinations?,
  2024.

\bibitem[Geng and Liu(2023)]{openlm2023openllama}
Xinyang Geng and Hao Liu.
\newblock Openllama: An open reproduction of llama, 2023.
\newblock \url{https://github.com/openlm-research/open_llama}.

\bibitem[Girdhar et~al.(2023)Girdhar, Singh, Brown, Duval, Azadi, Rambhatla,
  Shah, Yin, Parikh, and Misra]{girdhar2023emu}
Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi,
  Sai~Saketh Rambhatla, Akbar Shah, Xi~Yin, Devi Parikh, and Ishan Misra.
\newblock Emu video: Factorizing text-to-video generation by explicit image
  conditioning.
\newblock \emph{arXiv preprint arXiv:2311.10709}, 2023.

\bibitem[Google(2023)]{gemini2023gemini}
Gemini~Team Google.
\newblock Gemini: A family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Gou et~al.(2023)Gou, Shao, Gong, Yang, Huang, Duan, Chen,
  et~al.]{gou2023tora}
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan,
  Weizhu Chen, et~al.
\newblock Tora: A tool-integrated reasoning agent for mathematical problem
  solving.
\newblock \emph{arXiv preprint arXiv:2309.17452}, 2023.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney,
  Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu,
  Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik,
  Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Strubell, Subramani,
  Wortsman, Dasigi, Lambert, Richardson, Zettlemoyer, Dodge, Lo, Soldaini,
  Smith, and Hajishirzi]{groeneveld2024olmoacceleratingsciencelanguage}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind
  Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane
  Arora, David Atkinson, Russell Authur, Khyathi~Raghavi Chandu, Arman Cohan,
  Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William
  Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam,
  Matthew~E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk,
  Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell
  Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer,
  Jesse Dodge, Kyle Lo, Luca Soldaini, Noah~A. Smith, and Hannaneh Hajishirzi.
\newblock Olmo: Accelerating the science of language models, 2024.
\newblock \url{https://arxiv.org/abs/2402.00838}.

\bibitem[Gulati et~al.(2020)Gulati, Qin, Chiu, Parmar, Zhang, Yu, Han, Wang,
  Zhang, Wu, et~al.]{gulati2020conformer}
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu~Zhang, Jiahui Yu,
  Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et~al.
\newblock Conformer: Convolution-augmented transformer for speech recognition.
\newblock \emph{arXiv preprint arXiv:2005.08100}, 2020.

\bibitem[Guo et~al.(2023)Guo, Leng, Wu, Zhao, and Tan]{guo2023prompttts}
Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu~Tan.
\newblock Prompttts: Controllable text-to-speech with text descriptions.
\newblock In \emph{ICASSP 2023-2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 1--5. IEEE, 2023.

\bibitem[Gupta et~al.(2024)Gupta, Pantoja, Ross, Williams, and
  Ung]{gupta2024changinganswerorderdecrease}
Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, and Megan Ung.
\newblock Changing answer order can decrease mmlu accuracy.
\newblock \emph{arXiv preprint:2406.19470}, 2024.
\newblock \url{https://arxiv.org/abs/2406.19470}.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovic, Swayamdipta, Lo,
  Beltagy, Downey, and Smith]{gururangan2024dontstoppretraining}
Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug
  Downey, and Noah~A. Smith.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel~R. Tetreault,
  editors, \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pages
  8342--8360. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/V1/2020.ACL-MAIN.740}.
\newblock \url{https://doi.org/10.18653/v1/2020.acl-main.740}.

\bibitem[Hardalov et~al.(2020)Hardalov, Mihaylov, Zlatkova, Dinkov, Koychev,
  and Nakov]{hardalov-etal-2020-exams}
Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan
  Koychev, and Preslav Nakov.
\newblock {EXAMS}: A multi-subject high school examinations dataset for
  cross-lingual and multilingual question answering.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,
  \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, pages 5427--5444, Online, November 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.438}.
\newblock \url{https://aclanthology.org/2020.emnlp-main.438}.

\bibitem[Hartvigsen et~al.(2022)Hartvigsen, Gabriel, Palangi, Sap, Ray, and
  Kamar]{hartvigsen2022toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
  and Ece Kamar.
\newblock Toxigen: A large-scale machine-generated dataset for adversarial and
  implicit hate speech detection.
\newblock \emph{arXiv preprint arXiv:2203.09509}, 2022.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Burns, Basart, Zou,
  Mazeika, Song, and Steinhardt]{hendrycks2021mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net,
  2021{\natexlab{a}}.
\newblock \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Kadavath, Arora,
  Basart, Tang, Song, and Steinhardt]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the {MATH} dataset.
\newblock In Joaquin Vanschoren and Sai{-}Kit Yeung, editors, \emph{Proceedings
  of the Neural Information Processing Systems Track on Datasets and Benchmarks
  1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual},
  2021{\natexlab{b}}.
\newblock
  \url{https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland,
  Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae,
  Vinyals, and Sifre]{hoffmann2022chinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den
  Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
  Elsen, Jack~W Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, and Chen]{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia~Xu Chen, Dehao
  Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, Yonghui Wu, and Zhifeng Chen.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism, 2019.

\bibitem[Inan et~al.(2023)Inan, Upasani, Chi, Rungta, Iyer, Mao, Tontchev, Hu,
  Fuller, Testuginne, and Khabsa]{inan2023llamaguard}
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer,
  Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuginne, and
  Madian Khabsa.
\newblock Llama guard: Llm-based input-output safeguard for human-ai
  conversations.
\newblock 2023.

\bibitem[Ippolito et~al.(2023)Ippolito, Tramer, Nasr, Zhang, Jagielski, Lee,
  Choquette~Choo, and Carlini]{ippolito-etal-2023-preventing}
Daphne Ippolito, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski,
  Katherine Lee, Christopher Choquette~Choo, and Nicholas Carlini.
\newblock Preventing generation of verbatim memorization in language models
  gives a false sense of privacy.
\newblock In C.~Maria Keet, Hung-Yi Lee, and Sina Zarrie{\ss}, editors,
  \emph{Proceedings of the 16th International Natural Language Generation
  Conference}, pages 28--53, Prague, Czechia, September 2023. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2023.inlg-main.3}.
\newblock \url{https://aclanthology.org/2023.inlg-main.3}.

\bibitem[Izmailov et~al.(2019)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2019averagingweightsleadswider}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization,
  2019.
\newblock \url{https://arxiv.org/abs/1803.05407}.

\bibitem[Jaegle et~al.(2021)Jaegle, Gimeno, Brock, Zisserman, Vinyals, and
  Carreira]{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and
  Joao Carreira.
\newblock Perceiver: General perception with iterative attention.
\newblock \emph{arXiv preprint arXiv:2103.03206}, 2021.

\bibitem[Ji et~al.(2023)Ji, Ji, Bouillon, and
  Seligman]{Ji_Ji_Bouillon_Seligman_2023}
Meng Ji, Meng Ji, Pierrette Bouillon, and Mark Seligman.
\newblock \emph{Cultural and Linguistic Bias of Neural Machine Translation
  Technology}, page 100–128.
\newblock Studies in Natural Language Processing. Cambridge University Press,
  2023.

\bibitem[Jia and Liang(2017)]{jia-liang-2017-adversarial}
Robin Jia and Percy Liang.
\newblock Adversarial examples for evaluating reading comprehension systems.
\newblock In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors,
  \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural
  Language Processing}, pages 2021--2031, Copenhagen, Denmark, September 2017.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D17-1215}.
\newblock \url{https://aclanthology.org/D17-1215}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock,
  Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux,
  Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,
  and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford,
  Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
  Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou
  Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Johnson et~al.(2019)Johnson, Douze, and J{\'e}gou]{johnson2019billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with gpus.
\newblock \emph{IEEE Transactions on Big Data}, 7\penalty0 (3):\penalty0
  535--547, 2019.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi-etal-2017-triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset
  for reading comprehension.
\newblock In Regina Barzilay and Min-Yen Kan, editors, \emph{Proceedings of the
  55th Annual Meeting of the Association for Computational Linguistics (Volume
  1: Long Papers)}, pages 1601--1611, Vancouver, Canada, July 2017. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1147}.
\newblock \url{https://aclanthology.org/P17-1147}.

\bibitem[Joulin et~al.(2017)Joulin, Grave, Bojanowski, and
  Mikolov]{joulin2017bag}
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.
\newblock Bag of tricks for efficient text classification.
\newblock In \emph{Proceedings of the 15th Conference of the European Chapter
  of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages 427--431. Association for Computational Linguistics, April 2017.

\bibitem[Kalchbrenner et~al.(2018)Kalchbrenner, Elsen, Simonyan, Noury,
  Casagrande, Lockhart, Stimberg, Oord, Dieleman, and
  Kavukcuoglu]{kalchbrenner2018efficient}
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande,
  Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray
  Kavukcuoglu.
\newblock Efficient neural audio synthesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  2410--2419. PMLR, 2018.

\bibitem[Kamradt(2023)]{niah}
Gregory Kamradt.
\newblock Llmtest\_needleinahaystack.
\newblock
  \url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md},
  2023.

\bibitem[Kang et~al.(2024)Kang, Wang, Zhang, Hinsvark, and He]{kang2024tn}
Wonjune Kang, Yun Wang, Shun Zhang, Arthur Hinsvark, and Qing He.
\newblock Multi-task learning for front-end text processing in tts.
\newblock In \emph{ICASSP 2024 - 2024 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 10796--10800, 2024.
\newblock \doi{10.1109/ICASSP48485.2024.10446241}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kassem et~al.(2024)Kassem, Mahmoud, Mireshghallah, Kim, Tsvetkov,
  Choi, Saad, and Rana]{kassem2024alpacavicunausingllms}
Aly~M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia
  Tsvetkov, Yejin Choi, Sherif Saad, and Santu Rana.
\newblock Alpaca against vicuna: Using llms to uncover memorization of llms,
  2024.
\newblock \url{https://arxiv.org/abs/2403.04801}.

\bibitem[Kaufmann et~al.(2023)Kaufmann, Weng, Bengs, and
  H{\"u}llermeier]{kaufmann2023survey}
Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H{\"u}llermeier.
\newblock A survey of reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2312.14925}, 2023.

\bibitem[Kembhavi et~al.(2016)Kembhavi, Salvato, Kolve, Seo, Hajishirzi, and
  Farhadi]{Kembhavi2016ADI}
Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh
  Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock \emph{ArXiv}, abs/1603.07396, 2016.
\newblock \url{https://api.semanticscholar.org/CorpusID:2682274}.

\bibitem[Kharitonov et~al.(2021)Kharitonov, Lee, Polyak, Adi, Copet, Lakhotia,
  Nguyen, Rivi{\`e}re, Mohamed, Dupoux, et~al.]{kharitonov2021text}
Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal
  Lakhotia, Tu-Anh Nguyen, Morgane Rivi{\`e}re, Abdelrahman Mohamed, Emmanuel
  Dupoux, et~al.
\newblock Text-free prosody-aware generative spoken language modeling.
\newblock \emph{arXiv preprint arXiv:2109.03264}, 2021.

\bibitem[Kiela et~al.(2021)Kiela, Bartolo, Nie, Kaushik, Geiger, Wu, Vidgen,
  Prasad, Singh, Ringshia, Ma, Thrush, Riedel, Waseem, Stenetorp, Jia, Bansal,
  Potts, and Williams]{kiela-etal-2021-dynabench}
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,
  Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia,
  Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp,
  Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams.
\newblock Dynabench: Rethinking benchmarking in {NLP}.
\newblock In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
  Hakkani-Tur, Iz~Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
  and Yichao Zhou, editors, \emph{Proceedings of the 2021 Conference of the
  North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies}, pages 4110--4124, Online, June 2021.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.324}.
\newblock \url{https://aclanthology.org/2021.naacl-main.324}.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, Allal, Li, Mou, Ferrandis, Jernite,
  Mitchell, Hughes, Wolf, Bahdanau, von Werra, and
  de~Vries]{kocetkov2022stack3tbpermissively}
Denis Kocetkov, Raymond Li, Loubna~Ben Allal, Jia Li, Chenghao Mou,
  Carlos~Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes,
  Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de~Vries.
\newblock The stack: 3 tb of permissively licensed source code, 2022.
\newblock \url{https://arxiv.org/abs/2211.15533}.

\bibitem[Koncel-Kedziorski et~al.(2016)Koncel-Kedziorski, Roy, Amini, Kushman,
  and Hajishirzi]{koncel2016mawps}
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh
  Hajishirzi.
\newblock Mawps: A math word problem repository.
\newblock In \emph{Proceedings of the 2016 conference of the north american
  chapter of the association for computational linguistics: human language
  technologies}, pages 1152--1157, 2016.

\bibitem[Korthikanti et~al.(2023)Korthikanti, Casper, Lym, McAfee, Andersch,
  Shoeybi, and Catanzaro]{korthikanti2023reducing}
Vijay~Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael
  Andersch, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Reducing activation recomputation in large transformer models.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5, 2023.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{NIPS2012_c399862d}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In F.~Pereira, C.J. Burges, L.~Bottou, and K.Q. Weinberger, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~25. Curran
  Associates, Inc., 2012.
\newblock
  \url{https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang,
  and Stoica]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu,
  Joseph~E. Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with
  pagedattention, 2023.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai-etal-2017-race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock {RACE}: Large-scale {R}e{A}ding comprehension dataset from
  examinations.
\newblock In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors,
  \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural
  Language Processing}, pages 785--794, Copenhagen, Denmark, September 2017.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D17-1082}.
\newblock \url{https://aclanthology.org/D17-1082}.

\bibitem[Lamy-Poirier(2023)]{lamy2023breadth}
Joel Lamy-Poirier.
\newblock Breadth-first pipeline parallelism.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5:\penalty0
  48--67, 2023.

\bibitem[Le et~al.(2024)Le, Vyas, Shi, Karrer, Sari, Moritz, Williamson,
  Manohar, Adi, Mahadeokar, et~al.]{le2024voicebox}
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz,
  Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et~al.
\newblock Voicebox: Text-guided multilingual universal speech generation at
  scale.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Lee et~al.(2021)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch,
  and Carlini]{lee2021deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
  Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock \emph{arXiv preprint arXiv:2107.06499}, 2021.

\bibitem[Lee et~al.(2023)Lee, Joshi, Turc, Hu, Liu, Eisenschlos, Khandelwal,
  Shaw, Chang, and Toutanova]{Pix2Struct}
Kenton Lee, Mandar Joshi, Iulia~Raluca Turc, Hexiang Hu, Fangyu Liu,
  Julian~Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang,
  and Kristina Toutanova.
\newblock Pix2struct: Screenshot parsing as pretraining for visual language
  understanding.
\newblock In \emph{International Conference on Machine Learning}, pages
  18893--18912. PMLR, 2023.

\bibitem[Lee and Sengupta(2022)]{Lee22RSC}
Kevin Lee and Shubho Sengupta.
\newblock {Introducing the AI Research SuperCluster --- Meta’s cutting-edge
  AI supercomputer for AI research}, 2022.
\newblock \url{https://ai.meta.com/blog/ai-rsc/}.

\bibitem[Lee et~al.(2024)Lee, Gangidi, and Oldham]{lee2024building}
Kevin Lee, Adi Gangidi, and Mathew Oldham.
\newblock Building meta’s genai infrastructure.
\newblock 2024.

\bibitem[Lei et~al.(2018)Lei, Yu, Bansal, and Berg]{lei2018tvqa}
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara~L Berg.
\newblock Tvqa: Localized, compositional video question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and
  Zettlemoyer]{lewis2021base}
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.
\newblock Base layers: Simplifying training of large, sparse models.
\newblock In \emph{International Conference on Machine Learning}, pages
  6265--6274. PMLR, 2021.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Wang, Hu, Wei, Zheng, Hu, Zhang, and
  Peng]{li2024common}
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng
  Zhang, and Houwen Peng.
\newblock Common 7b language models already possess strong math capabilities.
\newblock \emph{arXiv preprint arXiv:2403.04706}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Fang, Smyrnis, Ivgi, Jordan, Gadre,
  Bansal, Guha, Keh, Arora, Garg, Xin, Muennighoff, Heckel, Mercat, Chen,
  Gururangan, Wortsman, Albalak, Bitton, Nezhurina, Abbas, Hsieh, Ghosh,
  Gardner, Kilian, Zhang, Shao, Pratt, Sanyal, Ilharco, Daras, Marathe,
  Gokaslan, Zhang, Chandu, Nguyen, Vasiljevic, Kakade, Song, Sanghavi, Faghri,
  Oh, Zettlemoyer, Lo, El-Nouby, Pouransari, Toshev, Wang, Groeneveld,
  Soldaini, Koh, Jitsev, Kollar, Dimakis, Carmon, Dave, Schmidt, and
  Shankar]{li2024datacomplmsearchgenerationtraining}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre,
  Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin,
  Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin
  Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna
  Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej
  Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco,
  Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu,
  Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi,
  Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby,
  Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca
  Soldaini, Pang~Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros~G. Dimakis,
  Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Datacomp-lm: In search of the next generation of training sets for
  language models, 2024{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/2406.11794}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, He, Wang, Li, Wang, Luo, Wang, Wang,
  and Qiao]{li2023videochat}
KunChang Li, Yinan He, Yi~Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,
  Limin Wang, and Yu~Qiao.
\newblock Videochat: Chat-centric video understanding.
\newblock \emph{arXiv preprint arXiv:2305.06355}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2022)Li, Gururangan, Dettmers, Lewis, Althoff, Smith, and
  Zettlemoyer]{li2022branchtrainmergeembarrassinglyparalleltraining}
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah~A.
  Smith, and Luke Zettlemoyer.
\newblock Branch-train-merge: Embarrassingly parallel training of expert
  language models, 2022.
\newblock \url{https://arxiv.org/abs/2208.03306}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhao, Yu, Song, Li, Yu, Li, Huang,
  and Li]{li2023api}
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun
  Li, Fei Huang, and Yongbin Li.
\newblock Api-bank: A comprehensive benchmark for tool-augmented llms.
\newblock \emph{arXiv preprint arXiv:2304.08244}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Cui, Zhao, Kong, and Bi]{li2024gsm}
Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi.
\newblock Gsm-plus: A comprehensive benchmark for evaluating the robustness of
  llms as mathematical problem solvers.
\newblock \emph{arXiv preprint arXiv:2402.19255}, 2024{\natexlab{c}}.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, Newman, Yuan, Yan, Zhang, Cosgrove, Manning,
  R{\'{e}}, Acosta{-}Navas, Hudson, Zelikman, Durmus, Ladhak, Rong, Ren, Yao,
  Wang, Santhanam, Orr, Zheng, Y{\"{u}}ksekg{\"{o}}n{\"{u}}l, Suzgun, Kim,
  Guha, Chatterji, Khattab, Henderson, Huang, Chi, Xie, Santurkar, Ganguli,
  Hashimoto, Icard, Zhang, Chaudhary, Wang, Li, Mai, Zhang, and
  Koreeda]{liang2022holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  Benjamin Newman, Binhang Yuan, Bobby Yan, Ce~Zhang, Christian Cosgrove,
  Christopher~D. Manning, Christopher R{\'{e}}, Diana Acosta{-}Navas, Drew~A.
  Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren,
  Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel~J. Orr, Lucia Zheng, Mert
  Y{\"{u}}ksekg{\"{o}}n{\"{u}}l, Mirac Suzgun, Nathan Kim, Neel Guha,
  Niladri~S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi,
  Sang~Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,
  Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li,
  Yifan Mai, Yuhui Zhang, and Yuta Koreeda.
\newblock Holistic evaluation of language models.
\newblock \emph{CoRR}, abs/2211.09110, 2022.
\newblock \doi{10.48550/ARXIV.2211.09110}.
\newblock \url{https://doi.org/10.48550/arXiv.2211.09110}.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Lin et~al.(2023)Lin, Zhu, Ye, Ning, Jin, and Yuan]{lin2023video}
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li~Yuan.
\newblock Video-llava: Learning united visual representation by alignment
  before projection.
\newblock \emph{arXiv preprint arXiv:2311.10122}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Zaharia, and Abbeel]{liu2023ring}
Hao Liu, Matei Zaharia, and Pieter Abbeel.
\newblock Ring attention with blockwise transformers for near-infinite context.
\newblock \emph{arXiv preprint arXiv:2310.01889}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Li, and
  Lee]{liu2023improvedllava}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning,
  2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Li, Wu, and Lee]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock In \emph{NeurIPS}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Xia, Wang, and Zhang]{liu2024your}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation
  of large language models for code generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 36,
  2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Wei, Liu, Si, Zhang, Rao, Zheng,
  Peng, Yang, Zhou, and Dai]{liu2024bestpractices}
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao,
  Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, and Andrew~M. Dai.
\newblock Best practices and lessons learned on synthetic data for language
  models.
\newblock \emph{CoRR}, abs/2404.07503, 2024{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2404.07503}.
\newblock \url{https://doi.org/10.48550/arXiv.2404.07503}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Zeng, He, Jiang, and
  He]{liu2024makesgooddataalignment}
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
\newblock What makes good data for alignment? a comprehensive study of
  automatic data selection in instruction tuning, 2024{\natexlab{c}}.
\newblock \url{https://arxiv.org/abs/2312.15685}.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Ott, Goyal, Du, Joshi, Chen, Levy,
  Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Ott, Goyal, Du, Joshi, Chen, Levy,
  Lewis, Zettlemoyer, and Stoyanov]{liu_ott_roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{CoRR}, abs/1907.11692, 2019{\natexlab{b}}.
\newblock \url{http://arxiv.org/abs/1907.11692}.

\bibitem[Llama-Team(2024)]{metallamaguard2}
Llama-Team.
\newblock Meta llama guard 2.
\newblock
  \url{https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md},
  2024.

\bibitem[Lu et~al.(2023)Lu, Yuan, Yuan, Lin, Lin, Tan, Zhou, and
  Zhou]{lu2023instag}
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang
  Zhou, and Jingren Zhou.
\newblock Instag: Instruction tagging for analyzing supervised fine-tuning of
  large language models, 2023.

\bibitem[Lu et~al.(2022)Lu, Bartolo, Moore, Riedel, and
  Stenetorp]{lu-etal-2022-fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 8086--8098, Dublin,
  Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.556}.
\newblock \url{https://aclanthology.org/2022.acl-long.556}.

\bibitem[Luo et~al.(2023)Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and
  Zhang]{luo2023wizardmath}
Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo
  Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.
\newblock Wizardmath: Empowering mathematical reasoning for large language
  models via reinforced evol-instruct.
\newblock \emph{arXiv preprint arXiv:2308.09583}, 2023.

\bibitem[Maaz et~al.(2024)Maaz, Rasheed, Khan, and Khan]{Maaz2023VideoChatGPT}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision
  and language models.
\newblock In \emph{ACL}, 2024.

\bibitem[Madaan et~al.(2024{\natexlab{a}})Madaan, Tandon, Gupta, Hallinan, Gao,
  Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2024self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 36,
  2024{\natexlab{a}}.

\bibitem[Madaan et~al.(2024{\natexlab{b}})Madaan, Singh, Schaeffer, Poulton,
  Koyejo, Stenetorp, Narang, and Hupkes]{madaan2024quantifying}
Lovish Madaan, Aaditya~K Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo,
  Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes.
\newblock Quantifying variance in evaluation benchmarks.
\newblock \emph{arXiv preprint arXiv:2406.10229}, 2024{\natexlab{b}}.

\bibitem[Madan et~al.(2024)Madan, Moegelmose, Modi, Rawat, and
  Moeslund]{madan2024foundation}
Neelu Madan, Andreas Moegelmose, Rajat Modi, Yogesh~S. Rawat, and Thomas~B.
  Moeslund.
\newblock Foundation models for video understanding: A survey.
\newblock 2024.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and van~der Maaten]{Mahajan_2018_ECCV}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens van~der Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem[Maiti et~al.(2023)Maiti, Peng, Choi, weon Jung, Chang, and
  Watanabe]{maiti2023voxtlm}
Soumi Maiti, Yifan Peng, Shukjae Choi, Jee weon Jung, Xuankai Chang, and Shinji
  Watanabe.
\newblock Voxtlm: unified decoder-only models for consolidating speech
  recognition/synthesis and speech/text continuation tasks.
\newblock 2023.

\bibitem[Masry et~al.(2022)Masry, Do, Tan, Joty, and
  Hoque]{masry-etal-2022-chartqa}
Ahmed Masry, Xuan~Long Do, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.
\newblock {C}hart{QA}: A benchmark for question answering about charts with
  visual and logical reasoning.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
  \emph{Findings of the Association for Computational Linguistics: ACL 2022},
  pages 2263--2279, Dublin, Ireland, May 2022. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2022.findings-acl.177}.
\newblock \url{https://aclanthology.org/2022.findings-acl.177}.

\bibitem[Mathew et~al.(2020)Mathew, Karatzas, Manmatha, and
  Jawahar]{Mathew2020DocVQAAD}
Minesh Mathew, Dimosthenis Karatzas, R.~Manmatha, and C.~V. Jawahar.
\newblock Docvqa: A dataset for vqa on document images.
\newblock \emph{2021 IEEE Winter Conference on Applications of Computer Vision
  (WACV)}, pages 2199--2208, 2020.
\newblock \url{https://api.semanticscholar.org/CorpusID:220280200}.

\bibitem[Matt~Bowman(2022)]{various2022grandteton}
Jeremy~Baumgartner Matt~Bowman.
\newblock Meta open compute project, grand teton ai platform, 2022.
\newblock
  \url{https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/}.

\bibitem[Mehta et~al.(2024)Mehta, Sekhavat, Cao, Horton, Jin, Sun, Mirzadeh,
  Najibi, Belenko, Zatloukal, et~al.]{mehta2024openelm}
Sachin Mehta, Mohammad~Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi
  Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter
  Zatloukal, et~al.
\newblock Openelm: An efficient language model family with open-source training
  and inference framework.
\newblock \emph{arXiv preprint arXiv:2404.14619}, 2024.

\bibitem[Mekala et~al.(2024)Mekala, Weston, Lanchantin, Raileanu, Lomeli,
  Shang, and Dwivedi-Yu]{mekala2024toolverifier}
Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli,
  Jingbo Shang, and Jane Dwivedi-Yu.
\newblock Toolverifier: Generalization to new tools via self-verification.
\newblock \emph{arXiv preprint arXiv:2402.14158}, 2024.

\bibitem[Mialon et~al.(2023{\natexlab{a}})Mialon, Dess{\`\i}, Lomeli,
  Nalmpantis, Pasunuru, Raileanu, Rozi{\`e}re, Schick, Dwivedi-Yu, Celikyilmaz,
  et~al.]{mialon2023augmented}
Gr{\'e}goire Mialon, Roberto Dess{\`\i}, Maria Lomeli, Christoforos Nalmpantis,
  Ram Pasunuru, Roberta Raileanu, Baptiste Rozi{\`e}re, Timo Schick, Jane
  Dwivedi-Yu, Asli Celikyilmaz, et~al.
\newblock Augmented language models: a survey.
\newblock \emph{arXiv preprint arXiv:2302.07842}, 2023{\natexlab{a}}.

\bibitem[Mialon et~al.(2023{\natexlab{b}})Mialon, Fourrier, Swift, Wolf, LeCun,
  and Scialom]{mialon2023gaia}
Gr{\'e}goire Mialon, Cl{\'e}mentine Fourrier, Craig Swift, Thomas Wolf, Yann
  LeCun, and Thomas Scialom.
\newblock Gaia: a benchmark for general ai assistants.
\newblock \emph{arXiv preprint arXiv:2311.12983}, 2023{\natexlab{b}}.

\bibitem[Mielke et~al.(2020)Mielke, Szlam, Boureau, and
  Dinan]{mielke2020metacognition}
Sabrina~J. Mielke, Arthur Szlam, Y{-}Lan Boureau, and Emily Dinan.
\newblock Linguistic calibration through metacognition: aligning dialogue agent
  responses with expected correctness.
\newblock \emph{CoRR}, abs/2012.14983, 2020.
\newblock \url{https://arxiv.org/abs/2012.14983}.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{mihaylov-etal-2018-suit}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun{'}ichi
  Tsujii, editors, \emph{Proceedings of the 2018 Conference on Empirical
  Methods in Natural Language Processing}, pages 2381--2391, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1260}.
\newblock \url{https://aclanthology.org/D18-1260}.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Mishra et~al.(2022)Mishra, Khashabi, Baral, Choi, and
  Hajishirzi]{mishra-etal-2022-reframing}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh
  Hajishirzi.
\newblock Reframing instructional prompts to {GPT}k{'}s language.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
  \emph{Findings of the Association for Computational Linguistics: ACL 2022},
  pages 589--612, Dublin, Ireland, May 2022. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2022.findings-acl.50}.
\newblock \url{https://aclanthology.org/2022.findings-acl.50}.

\bibitem[Mitra et~al.(2024)Mitra, Khanpour, Rosset, and
  Awadallah]{mitra2024orca}
Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah.
\newblock Orca-math: Unlocking the potential of slms in grade school math.
\newblock \emph{arXiv preprint arXiv:2402.14830}, 2024.

\bibitem[Mouret and Clune(2015)]{mouret2015illuminatingsearchspacesmapping}
Jean-Baptiste Mouret and Jeff Clune.
\newblock Illuminating search spaces by mapping elites, 2015.
\newblock \url{https://arxiv.org/abs/1504.04909}.

\bibitem[Muennighoff et~al.(2023)Muennighoff, Wang, Sutawika, Roberts,
  Biderman, Le~Scao, Bari, Shen, Yong, Schoelkopf,
  et~al.]{muennighoff2023crosslingual}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
  Biderman, Teven Le~Scao, M~Saiful Bari, Sheng Shen, Zheng~Xin Yong, Hailey
  Schoelkopf, et~al.
\newblock Crosslingual generalization through multitask finetuning.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 15991--16111,
  2023.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse,
  Jain, Kosaraju, Saunders, et~al.]{nakano2021webgpt}
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
  Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
  et~al.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock \emph{arXiv preprint arXiv:2112.09332}, 2021.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary,
  Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, Phanishayee, and
  Zaharia‡]{narayanan2021efficient}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
  Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia‡.
\newblock {Efficient Large-Scale Language Model Training on GPU Clusters Using
  Megatron-LM}.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--15, 2021.

\bibitem[Nasr et~al.(2023)Nasr, Carlini, Hayase, Jagielski, Cooper, Ippolito,
  Choquette-Choo, Wallace, Tram{\`e}r, and Lee]{nasr2023scalableextraction}
Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A.~Feder
  Cooper, Daphne Ippolito, Christopher~A. Choquette-Choo, Eric Wallace, Florian
  Tram{\`e}r, and Katherine Lee.
\newblock Scalable extraction of training data from (production) language
  models.
\newblock \emph{ArXiv}, abs/2311.17035, 2023.
\newblock \url{https://api.semanticscholar.org/CorpusID:265466445}.

\bibitem[Nguyen et~al.(2024)Nguyen, Muller, Yu, Costa-jussa, Elbayad, Duquenne,
  Algayres, Mavlyutov, Gat, Synnaeve, Pino, Sagot, and
  Dupoux]{nguyen2024spirit}
Tu~Anh Nguyen, Benjamin Muller, Bokai Yu, Marta~R. Costa-jussa, Maha Elbayad,
  Sravya Popuri Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai
  Gat, Gabriel Synnaeve, Juan Pino, Benoît Sagot, and Emmanuel Dupoux.
\newblock Spirit-lm: Interleaved spoken and written language model.
\newblock 2024.

\bibitem[NLLB~Team et~al.(2022)NLLB~Team, Cross, Çelebi, Elbayad, Heafield,
  Heffernan, Kalbassi, Lam, Licht, Maillard, Sun, Wang, Wenzek, Youngblood,
  Akula, Barrault, Gonzalez, Hansanti, Hoffman, Jarrett, Sadagopan, Rowe,
  Spruit, Tran, Andrews, Ayan, Bhosale, Edunov, Fan, Gao, Goswami, Guzmán,
  Koehn, Mourachko, Ropers, Saleem, Schwenk, and Wang]{nllb2022}
Marta R. Costa-jussà NLLB~Team, James Cross, Onur Çelebi, Maha Elbayad,
  Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
  Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al~Youngblood, Bapi
  Akula, Loic Barrault, Gabriel~Mejia Gonzalez, Prangthip Hansanti, John
  Hoffman, Semarley Jarrett, Kaushik~Ram Sadagopan, Dirk Rowe, Shannon Spruit,
  Chau Tran, Pierre Andrews, Necip~Fazil Ayan, Shruti Bhosale, Sergey Edunov,
  Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn,
  Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and
  Jeff Wang.
\newblock No language left behind: Scaling human-centered machine translation.
\newblock 2022.

\bibitem[OpenAI(2023{\natexlab{a}})]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023{\natexlab{a}}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openai2023gpt4blog}
OpenAI.
\newblock {GPT-4} blog.
\newblock \url{https://openai.com/index/gpt-4-research/}, 2023{\natexlab{b}}.

\bibitem[OpenAI(2024)]{simpleevals}
OpenAI.
\newblock simple-evals.
\newblock \url{https://github.com/openai/simple-evals}, 2024.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe]{ouyang2022instructgpt}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Pal et~al.(2024)Pal, Karkhanis, Dooley, Roberts, Naidu, and
  White]{pal2024smaug}
Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and
  Colin White.
\newblock Smaug: Fixing failure modes of preference optimisation with
  dpo-positive.
\newblock \emph{arXiv preprint arXiv:2402.13228}, 2024.

\bibitem[Pan et~al.(2024)Pan, Saxon, Xu, Nathani, Wang, and
  Wang]{pan2024selfcorrection}
Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and
  William~Yang Wang.
\newblock Automatically correcting large language models: \emph{Surveying the
  Landscape of Diverse Automated Correction Strategies}.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 12:\penalty0 484--506,
  2024.
\newblock \doi{10.1162/TACL\_A\_00660}.
\newblock \url{https://doi.org/10.1162/tacl\_a\_00660}.

\bibitem[Pan et~al.(2021)Pan, Stavrinos, Zhang, Sikaria, Zakharov, Sharma,
  Shankar, Shuey, Wareing, Gangapuram, Cao, Preseau, Singh, Patiejunas, Tipton,
  Katz-Bassett, and Lloyd]{pan2021tectonicfs}
Satadru~Pan Pan, Theano Stavrinos, Yunqiao Zhang, Atul Sikaria, Pavel Zakharov,
  Abhinav Sharma, Shiva Shankar, Mike Shuey, Richard Wareing, Monika
  Gangapuram, Guanglei Cao, Christian Preseau, Pratap Singh, Kestutis
  Patiejunas, JR~Tipton, Ethan Katz-Bassett, and Wyatt Lloyd.
\newblock Facebook’s tectonic filesystem: Efficiency from exascale.
\newblock In \emph{Proceedings of the 19th USENIX Conference on File and
  Storage Technologies}, pages 217--231, 2021.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and
  Khudanpur]{panayotov2015librispeech}
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In \emph{2015 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 5206--5210. IEEE, 2015.

\bibitem[Pang et~al.(2022)Pang, Parrish, Joshi, Nangia, Phang, Chen,
  Padmakumar, Ma, Thompson, He, and Bowman]{pang-etal-2022-quality}
Richard~Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang,
  Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He~He, and
  Samuel Bowman.
\newblock {Q}u{ALITY}: Question answering with long input texts, yes!
\newblock In Marine Carpuat, Marie-Catherine de~Marneffe, and Ivan~Vladimir
  Meza~Ruiz, editors, \emph{Proceedings of the 2022 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies}, pages 5336--5358, Seattle, United States, July 2022.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.391}.
\newblock \url{https://aclanthology.org/2022.naacl-main.391}.

\bibitem[Pang et~al.(2024)Pang, Yuan, Cho, He, Sukhbaatar, and
  Weston]{pang2024iterative}
Richard~Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He~He, Sainbayar Sukhbaatar,
  and Jason Weston.
\newblock Iterative reasoning preference optimization.
\newblock \emph{arXiv preprint arXiv:2404.19733}, 2024.

\bibitem[Parisi et~al.(2022)Parisi, Zhao, and Fiedel]{parisi2022talm}
Aaron Parisi, Yao Zhao, and Noah Fiedel.
\newblock Talm: Tool augmented language models.
\newblock \emph{arXiv preprint arXiv:2205.12255}, 2022.

\bibitem[Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez]{patil2023gorilla}
Shishir~G Patil, Tianjun Zhang, Xin Wang, and Joseph~E Gonzalez.
\newblock Gorilla: Large language model connected with massive apis.
\newblock \emph{arXiv preprint arXiv:2305.15334}, 2023.

\bibitem[Pizzi et~al.(2022)Pizzi, Roy, Ravindra, Goyal, and
  Douze]{pizzi2022self}
Ed~Pizzi, Sreya~Dutta Roy, Sugosh~Nagavara Ravindra, Priya Goyal, and Matthijs
  Douze.
\newblock A self-supervised descriptor for image copy detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 14532--14542, 2022.

\bibitem[Polyak(1991)]{polyak1991averaging}
B.T. Polyak.
\newblock New stochastic approximation type procedures.
\newblock \emph{Automation and Remote Control}, 7\penalty0 (7), 1991.

\bibitem[Pratap et~al.(2020)Pratap, Xu, Sriram, Synnaeve, and
  Collobert]{pratap2020mls}
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan
  Collobert.
\newblock Mls: A large-scale multilingual dataset for speech research.
\newblock \emph{arXiv preprint arXiv:2012.03411}, 2020.

\bibitem[Prokopidis et~al.(2016)Prokopidis, Papavassiliou, and
  Piperidis]{PROKOPIDIS16.778}
Prokopis Prokopidis, Vassilis Papavassiliou, and Stelios Piperidis.
\newblock Parallel global voices: a collection of multilingual corpora with
  citizen media stories.
\newblock In Nicoletta Calzolari~(Conference Chair), Khalid Choukri, Thierry
  Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene
  Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors,
  \emph{Proceedings of the Tenth International Conference on Language Resources
  and Evaluation (LREC 2016)}, Paris, France, may 2016. European Language
  Resources Association (ELRA).
\newblock ISBN 978-2-9517408-9-1.

\bibitem[Pătrăucean et~al.(2023)Pătrăucean, Smaira, Gupta, Continente,
  Markeeva, Banarse, Koppula, Heyward, Malinowski, Yang, Doersch, Matejovicova,
  Sulsky, Miech, Frechette, Klimczak, Koster, Zhang, Winkler, Aytar, Osindero,
  Damen, Zisserman, and Carreira]{patraucean2023perception}
Viorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià~Recasens Continente,
  Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz
  Malinowski, Yi~Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine
  Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang,
  Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman,
  and João Carreira.
\newblock Perception test: A diagnostic benchmark for multimodal video models.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Radford et~al.(2023)Radford, Kim, Xu, Brockman, Mcleavey, and
  Sutskever]{radford23whisper}
Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and
  Ilya Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of
  \emph{Proceedings of Machine Learning Research}, pages 28492--28518. PMLR,
  23--29 Jul 2023.
\newblock \url{https://proceedings.mlr.press/v202/radford23a.html}.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri,
  Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy,
  Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart,
  Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis,
  Kavukcuoglu, and Irving]{Rae2021ScalingLM}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John F.~J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese,
  Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden,
  Esme Sutherland, Karen Simonyan, Michela Paganini, L.~Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, N.~K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
  de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
  Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris Jones, James
  Bradbury, Matthew~G. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason
  Gabriel, William~S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell,
  Chris Dyer, Oriol Vinyals, Kareem~W. Ayoub, Jeff Stanway, L.~L. Bennett,
  Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{ArXiv}, abs/2112.11446, 2021.
\newblock \url{https://api.semanticscholar.org/CorpusID:245353475}.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and
  Finn]{rafailov2023dpo}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano
  Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and
  Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano
  Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0
  (140):\penalty0 1--67, 2020.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2020zeromemoryoptimizationstraining}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models,
  2020.
\newblock \url{https://arxiv.org/abs/1910.02054}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar-etal-2016-squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In Jian Su, Kevin Duh, and Xavier Carreras, editors,
  \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural
  Language Processing}, pages 2383--2392, Austin, Texas, November 2016.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D16-1264}.
\newblock \url{https://aclanthology.org/D16-1264}.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and
  Liang]{rajpurkar-etal-2018-know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}.
\newblock In Iryna Gurevych and Yusuke Miyao, editors, \emph{Proceedings of the
  56th Annual Meeting of the Association for Computational Linguistics (Volume
  2: Short Papers)}, pages 784--789, Melbourne, Australia, July 2018.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P18-2124}.
\newblock \url{https://aclanthology.org/P18-2124}.

\bibitem[Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael,
  and Bowman]{rein2023gpqagraduatelevelgoogleproofqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe
  Pang, Julien Dirani, Julian Michael, and Samuel~R. Bowman.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark, 2023.
\newblock \url{https://arxiv.org/abs/2311.12022}.

\bibitem[Ren et~al.(2021)Ren, Rajbhandari, Aminabadi, Ruwase, Yang, Zhang, Li,
  and He]{ren2021zerooffloaddemocratizingbillionscalemodel}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
  Yang, Minjia Zhang, Dong Li, and Yuxiong He.
\newblock Zero-offload: Democratizing billion-scale model training, 2021.
\newblock \url{https://arxiv.org/abs/2101.06840}.

\bibitem[Robinson and Wingate(2023)]{robison2023leveraging}
Joshua Robinson and David Wingate.
\newblock Leveraging large language models for multiple choice question
  answering.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net,
  2023.
\newblock \url{https://openreview.net/pdf?id=yKbprarjc5B}.

\bibitem[R{\"o}ttger et~al.(2023)R{\"o}ttger, Kirk, Vidgen, Attanasio, Bianchi,
  and Hovy]{rottger2023xstest}
Paul R{\"o}ttger, Hannah~Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico
  Bianchi, and Dirk Hovy.
\newblock Xstest: A test suite for identifying exaggerated safety behaviours in
  large language models.
\newblock \emph{arXiv preprint arXiv:2308.01263}, 2023.

\bibitem[Rozi{\`{e}}re et~al.(2023)Rozi{\`{e}}re, Gehring, Gloeckle, Sootla,
  Gat, Tan, Adi, Liu, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt,
  Canton{-}Ferrer, Grattafiori, Xiong, D{\'{e}}fossez, Copet, Azhar, Touvron,
  Martin, Usunier, Scialom, and Synnaeve]{codellama}
Baptiste Rozi{\`{e}}re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
  Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'{e}}r{\'{e}}my
  Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt,
  Cristian Canton{-}Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre
  D{\'{e}}fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
  Usunier, Thomas Scialom, and Gabriel Synnaeve.
\newblock Code llama: Open foundation models for code.
\newblock \emph{CoRR}, abs/2308.12950, 2023.
\newblock \doi{10.48550/ARXIV.2308.12950}.
\newblock \url{https://doi.org/10.48550/arXiv.2308.12950}.

\bibitem[Rubenstein et~al.(2023)Rubenstein, Asawaroengchai, Nguyen, Bapna,
  Borsos, de~Chaumont~Quitry, Chen, Badawy, Han, Kharitonov, Muckenhirn,
  Padfield, Qin, Rozenberg, Sainath, Schalkwyk, Sharifi, Ramanovich,
  Tagliasacchi, Tudor, Velimirović, Vincent, Yu, Wang, Zayats, Zeghidour,
  Zhang, Zhang, Zilka, and Frank]{rubenstein2023audiopalm}
Paul~K. Rubenstein, Chulayuth Asawaroengchai, Duc~Dung Nguyen, Ankur Bapna,
  Zalán Borsos, Félix de~Chaumont~Quitry, Peter Chen, Dalia~El Badawy, Wei
  Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny
  Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle~Tadmor
  Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien
  Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu~Zhang,
  Zhishuai Zhang, Lukas Zilka, and Christian Frank.
\newblock Audiopalm: A large language model that can speak and listen.
\newblock 2023.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106,
  2021.

\bibitem[Samvelyan et~al.(2024)Samvelyan, Raparthy, Lupu, Hambro, Markosyan,
  Bhatt, Mao, Jiang, Parker-Holder, Foerster, Rocktäschel, and
  Raileanu]{samvelyan2024rainbowteamingopenendedgeneration}
Mikayel Samvelyan, Sharath~Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram~H.
  Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob
  Foerster, Tim Rocktäschel, and Roberta Raileanu.
\newblock Rainbow teaming: Open-ended generation of diverse adversarial
  prompts, 2024.
\newblock \url{https://arxiv.org/abs/2402.16822}.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim,
  Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, Fevry, Fries, Teehan, Scao,
  Biderman, Gao, Wolf, and Rush]{sanh2022multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
  Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful
  Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla,
  Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
  Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong,
  Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
  Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan Fries, Ryan
  Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M
  Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and
  Choi]{sap-etal-2019-social}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le~Bras, and Yejin Choi.
\newblock Social {IQ}a: Commonsense reasoning about social interactions.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
  \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing (EMNLP-IJCNLP)}, pages 4463--4473, Hong Kong, China,
  November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1454}.
\newblock \url{https://aclanthology.org/D19-1454}.

\bibitem[Savoldi et~al.(2021)Savoldi, Gaido, Bentivogli, Negri, and
  Turchi]{10.1162/tacl_a_00401}
Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco
  Turchi.
\newblock {Gender Bias in Machine Translation}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 845--874, 08 2021.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00401}.
\newblock \url{https://doi.org/10.1162/tacl\_a\_00401}.

\bibitem[Schick et~al.(2024)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli,
  Hambro, Zettlemoyer, Cancedda, and Scialom]{schick2024toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria
  Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[{Seamless Communication} et~al.(2023){Seamless Communication},
  Barrault, Chung, Meglioli, Dale, Dong, Duquenne, Elsahar, Gong, Heffernan,
  Hoffman, Klaiber, Li, Licht, Maillard, Rakotoarison, Sadagopan, Wenzek, Ye,
  Akula, Chen, Hachem, Ellis, Gonzalez, Haaheim, Hansanti, Howes, Huang, Hwang,
  Inaguma, Jain, Kalbassi, Kallet, Kulikov, Lam, Li, Ma, Mavlyutov, Peloquin,
  Ramadan, Ramakrishnan, Sun, Tran, Tran, Tufanov, Vogeti, Wood, Yang, Yu,
  Andrews, Balioglu, Costa-juss\`{a}, Onur~\, Elbayad, Gao, Guzm\'an, Kao, Lee,
  Mourachko, Pino, Popuri, Ropers, Saleem, Schwenk, Tomasello, Wang, Wang, and
  Wang]{seamlessm4t2023}
{Seamless Communication}, Loic Barrault, Yu-An Chung, Mariano~Cora Meglioli,
  David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong,
  Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht,
  Jean Maillard, Alice Rakotoarison, Kaushik~Ram Sadagopan, Guillaume Wenzek,
  Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji~El Hachem, Brian Ellis,
  Gabriel~Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes,
  Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi,
  Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan
  Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna
  Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin
  Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta~R. Costa-juss\`{a},
  {C}elebi Onur~\, Maha Elbayad, Cynthia Gao, Francisco Guzm\'an, Justine Kao,
  Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers,
  Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang,
  and Skyler Wang.
\newblock Seamlessm4t—massively multilingual \& multimodal machine
  translation.
\newblock \emph{ArXiv}, 2023.

\bibitem[Shaham et~al.(2023)Shaham, Ivgi, Efrat, Berant, and Levy]{zeroscrolls}
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.
\newblock Zeroscrolls: A zero-shot benchmark for long text understanding.
\newblock \emph{arXiv preprint arXiv:2305.14196}, 2023.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and
  Guo]{shao2024deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang,
  YK~Li, Yu~Wu, and Daya Guo.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open
  language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017moe}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Shi et~al.(2022)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung,
  Tay, Ruder, Zhou, Das, and
  Wei]{shi2022languagemodelsmultilingualchainofthought}
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush
  Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,
  and Jason Wei.
\newblock Language models are multilingual chain-of-thought reasoners, 2022.
\newblock \url{https://arxiv.org/abs/2210.03057}.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism, 2019.
\newblock \url{http://arxiv.org/abs/1909.08053}.

\bibitem[Singh et~al.(2024)Singh, Kocyigit, Poulton, Esiobu, Lomeli, Szilvasy,
  and Hupkes]{singh2024contamination}
Aaditya Singh, Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli,
  Gergely Szilvasy, and Dieuwke Hupkes.
\newblock Evaluation data contamination in llms: how do we measure it and
  (when) does it matter?
\newblock 2024.

\bibitem[Singh et~al.(2019)Singh, Natarjan, Shah, Jiang, Chen, Parikh, and
  Rohrbach]{singh2019towards}
Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu~Jiang, Xinlei Chen, Devi Parikh,
  and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8317--8326, 2019.

\bibitem[Snowflake(2024)]{snowflakearctic}
Snowflake.
\newblock {Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently
  Intelligent, Truly Open} blog.
\newblock
  \url{https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/},
  2024.

\bibitem[Somepalli et~al.(2023)Somepalli, Singla, Goldblum, Geiping, and
  Goldstein]{somepalli2023diffusion}
Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom
  Goldstein.
\newblock Diffusion art or digital forgery? investigating data replication in
  diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 6048--6058, 2023.

\bibitem[Srinivasan et~al.(2023)Srinivasan, Dong, Zhu, Yu, Mosk-Aoyama,
  Keutzer, Jiao, and Zhang]{srinivasan2023nexusraven}
Venkat~Krishna Srinivasan, Zhen Dong, Banghua Zhu, Brian Yu, Damon Mosk-Aoyama,
  Kurt Keutzer, Jiantao Jiao, and Jian Zhang.
\newblock Nexusraven: a commercially-permissive language model for function
  calling.
\newblock In \emph{NeurIPS 2023 Foundation Models for Decision Making
  Workshop}, 2023.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Suzgun et~al.(2023)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung,
  Chowdhery, Le, Chi, Zhou, and Wei]{suzgun-etal-2023-challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc Le, Ed~Chi, Denny Zhou, and Jason
  Wei.
\newblock Challenging {BIG}-bench tasks and whether chain-of-thought can solve
  them.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,
  \emph{Findings of the Association for Computational Linguistics: ACL 2023},
  pages 13003--13051, Toronto, Canada, July 2023. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.824}.
\newblock \url{https://aclanthology.org/2023.findings-acl.824}.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and
  Berant]{talmor-etal-2019-commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {C}ommonsense{QA}: A question answering challenge targeting
  commonsense knowledge.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
  \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of
  the Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 4149--4158, Minneapolis, Minnesota,
  June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1421}.
\newblock \url{https://aclanthology.org/N19-1421}.

\bibitem[Tang et~al.(2015)Tang, Kooburat, Venkatachalam, Chander, Wen,
  Narayanan, Dowell, and Karl]{configerator}
Chunqiang Tang, Thawan Kooburat, Pradeep Venkatachalam, Akshay Chander, Zhe
  Wen, Aravind Narayanan, Patrick Dowell, and Robert Karl.
\newblock {Holistic Configuration Management at Facebook}.
\newblock In \emph{{Proceedings of the 25th Symposium on Operating Systems
  Principles}}, pages 328--343, 2015.

\bibitem[Team(2024)]{chameleon2024}
Chameleon Team.
\newblock Chameleon: Mixed-modal early-fusion foundation models.
\newblock 2024.

\bibitem[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,
  Sifre, Rivi{\`e}re, Kale, Love, et~al.]{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
  Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,
  Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.

\bibitem[Thiel(2023)]{thiel2023csam}
David Thiel.
\newblock Identifying and eliminating csam in generative ml training data and
  models.
\newblock Technical report, Stanford Internet Observatory, 2023.

\bibitem[Thoppilan et~al.(2022)Thoppilan, Freitas, Hall, Shazeer, Kulshreshtha,
  Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang,
  Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhao, Zhou, Chang,
  Krivokon, Rusch, Pickett, Srinivasan, Man, Meier-Hellstern, Morris, Doshi,
  Santos, Duke, Soraker, Zevenbergen, Prabhakaran, Diaz, Hutchinson, Olson,
  Molina, Hoffman-John, Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton,
  Cohen, Bernstein, Kurzweil, Aguera-Arcas, Cui, Croak, Chi, and
  Le]{thoppilan2022lamdalanguagemodelsdialog}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo
  Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
  Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao,
  Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,
  Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel
  Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben
  Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen
  Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi
  Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron
  Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,
  Marian Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications, 2022.
\newblock \url{https://arxiv.org/abs/2201.08239}.

\bibitem[Tiedemann(2012)]{Tiedemann2012ParallelDT}
J{\"o}rg Tiedemann.
\newblock Parallel data, tools and interfaces in opus.
\newblock In \emph{International Conference on Language Resources and
  Evaluation}, 2012.
\newblock \url{https://api.semanticscholar.org/CorpusID:15453873}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave,
  and Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher,
  Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami,
  Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann,
  Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov,
  Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva,
  Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang,
  Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and
  Scialom]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang,
  Creswell, Irving, and Higgins]{uesato2022solving}
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa
  Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.
\newblock Solving math word problems with process-and outcome-based feedback.
\newblock \emph{arXiv preprint arXiv:2211.14275}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Łukasz Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Łukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Vidgen et~al.(2024)Vidgen, Agrawal, Ahmed, Akinwande, Al-Nuaimi,
  Alfaraj, Alhajjar, Aroyo, Bavalatti, Blili-Hamelin,
  et~al.]{vidgen2024introducing}
Bertie Vidgen, Adarsh Agrawal, Ahmed~M Ahmed, Victor Akinwande, Namir
  Al-Nuaimi, Najla Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti,
  Borhane Blili-Hamelin, et~al.
\newblock Introducing v0.5 of the ai safety benchmark from mlcommons.
\newblock \emph{arXiv preprint arXiv:2404.12241}, 2024.

\bibitem[Vigraham and Leonhardi(2024)]{leonhardi2024maintenance}
Saranyan Vigraham and Benjamin Leonhardi.
\newblock Maintaining large-scale ai capacity at meta.
\newblock 2024.

\bibitem[Wallace et~al.(2024)Wallace, Xiao, Leike, Weng, Heidecke, and
  Beutel]{wallace2024instructionhierarchytrainingllms}
Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex
  Beutel.
\newblock The instruction hierarchy: Training llms to prioritize privileged
  instructions, 2024.
\newblock \url{https://arxiv.org/abs/2404.13208}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Rivière, Lee, Wu, Talnikar,
  Haziza, Williamson, Pino, and Dupoux]{wang2021voxpopuli}
Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel
  Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux.
\newblock Voxpopuli: A large-scale multilingual speech corpus for
  representation learning, semi-supervised learning and interpretation.
\newblock \emph{arXiv preprint arXiv:2101.00390}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Wu, and Pino]{wang2021covost}
Changhan Wang, Anne Wu, and Juan Pino.
\newblock Covost 2 and massively multilingual speech-to-text translation.
\newblock \emph{arXiv preprint arXiv:2007.10310}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Zhao, Qiang, Qin, and
  Liu]{wang2024beyond}
Haochun Wang, Sendong Zhao, Zewen Qiang, Bing Qin, and Ting Liu.
\newblock Beyond the answers: Reviewing the rationality of multiple choice
  question answering for the evaluation of large language models.
\newblock \emph{CoRR}, abs/2402.01349, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2402.01349}.
\newblock \url{https://doi.org/10.48550/arXiv.2402.01349}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Rubinstein, and
  Cohn]{wang-etal-2022-measuring}
Jun Wang, Benjamin Rubinstein, and Trevor Cohn.
\newblock Measuring and mitigating name biases in neural machine translation.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 2576--2590, Dublin,
  Ireland, May 2022{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.184}.
\newblock \url{https://aclanthology.org/2022.acl-long.184}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Li, Shao, Xu, Dai, Li, Chen, Wu,
  and Sui]{wang2023math}
Peiyi Wang, Lei Li, Zhihong Shao, RX~Xu, Damai Dai, Yifei Li, Deli Chen, Y~Wu,
  and Zhifang Sui.
\newblock Math-shepherd: Verify and reinforce llms step-by-step without human
  annotations.
\newblock \emph{CoRR, abs/2312.08935}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhou, Zhang, Wu, Liu, Gaur, Chen,
  Li, and Wei]{wang2023viola}
Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu~Wu, Shujie Liu, Yashesh Gaur, Zhuo
  Chen, Jinyu Li, and Furu Wei.
\newblock Viola: Unified codec language models for speech recognition,
  synthesis, and translation.
\newblock 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Naik, Ashok, Dhanasekaran, Arunkumar, Stap, et~al.]{wang2022super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, et~al.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 5085--5109, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Ma, Zhang, Ni, Chandra, Guo, Ren,
  Arulraj, He, Jiang, et~al.]{wang2024mmlu}
Yubo Wang, Xueguang Ma, Ge~Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,
  Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et~al.
\newblock Mmlu-pro: A more robust and challenging multi-task language
  understanding benchmark.
\newblock \emph{arXiv preprint arXiv:2406.01574}, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2017)Wang, Hamza, and Florian]{quoraFirstQuora}
Zhiguo Wang, Wael Hamza, and Radu Florian.
\newblock Bilateral multi-perspective matching for natural language sentences.
\newblock \emph{arXiv preprint arXiv:1702.03814}, 2017.

\bibitem[Weber et~al.(2023{\natexlab{a}})Weber, Bruni, and
  Hupkes]{weber-etal-2023-mind}
Lucas Weber, Elia Bruni, and Dieuwke Hupkes.
\newblock Mind the instructions: a holistic evaluation of consistency and
  interactions in prompt-based learning.
\newblock In Jing Jiang, David Reitter, and Shumin Deng, editors,
  \emph{Proceedings of the 27th Conference on Computational Natural Language
  Learning (CoNLL)}, pages 294--313, Singapore, December 2023{\natexlab{a}}.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.conll-1.20}.
\newblock \url{https://aclanthology.org/2023.conll-1.20}.

\bibitem[Weber et~al.(2023{\natexlab{b}})Weber, Bruni, and
  Hupkes]{weber2023icl}
Lucas Weber, Elia Bruni, and Dieuwke Hupkes.
\newblock The icl consistency test.
\newblock \emph{arXiv preprint arXiv:2312.04945}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du,
  Dai, and Le]{weifinetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang,
  Dean, and Fedus]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{Transactions on Machine Learning Research}, 2022{\natexlab{b}}.
\newblock \url{https://openreview.net/forum?id=yzkSU5zdwD}.

\bibitem[Wei et~al.(2022{\natexlab{c}})Wei, Wang, Schuurmans, Bosma, Xia, Chi,
  Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 24824--24837, 2022{\natexlab{c}}.

\bibitem[Wei et~al.(2024)Wei, Wang, Liu, Ding, and
  Zhang]{wei2024magicoderempoweringcodegeneration}
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.
\newblock Magicoder: Empowering code generation with oss-instruct, 2024.
\newblock \url{https://arxiv.org/abs/2312.02120}.

\bibitem[Welleck et~al.(2022)Welleck, Lu, West, Brahman, Shen, Khashabi, and
  Choi]{welleck2022generating}
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel
  Khashabi, and Yejin Choi.
\newblock Generating sequences by learning to self-correct.
\newblock \emph{arXiv preprint arXiv:2211.00053}, 2022.

\bibitem[Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzmán,
  Joulin, and Grave]{wenzek2019ccnetextractinghighquality}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary,
  Francisco Guzmán, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl
  data, 2019.
\newblock \url{https://arxiv.org/abs/1911.00359}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, and
  Schmidt]{wortsman2022modelsoupsaveragingweights}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs,
  Raphael Gontijo-Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair
  Carmon, Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time, 2022.
\newblock \url{https://arxiv.org/abs/2203.05482}.

\bibitem[Wu et~al.(2021)Wu, Xiu, Shi, Kalinli, Fuegen, Koehler, and
  He]{wu2021transformer}
Chunyang Wu, Zhiping Xiu, Yangyang Shi, Ozlem Kalinli, Christian Fuegen, Thilo
  Koehler, and Qing He.
\newblock Transformer-based acoustic modeling for streaming speech synthesis.
\newblock In \emph{Interspeech}, pages 146--150, 2021.

\bibitem[Wu et~al.(2023)Wu, Hui, Chen, Wu, Tu, and
  Zhou]{wu2023conic10kchallengingmathproblem}
Haoyi Wu, Wenyang Hui, Yezeng Chen, Weiqi Wu, Kewei Tu, and Yi~Zhou.
\newblock Conic10k: A challenging math problem understanding and reasoning
  dataset, 2023.
\newblock \url{https://arxiv.org/abs/2311.05113}.

\bibitem[Wu and Palmer(1994)]{wu1994verb}
Zhibiao Wu and Martha Palmer.
\newblock Verb semantics and lexical selection.
\newblock In \emph{ACL}, 1994.

\bibitem[XAI(2024)]{xaigrok}
XAI.
\newblock {Open Release of Grok-1} blog.
\newblock \url{https://x.ai/blog/grok-os}, 2024.

\bibitem[Xiao et~al.(2024{\natexlab{a}})Xiao, Wu, Xu, Dai, Hu, Lu, Zeng, Liu,
  and Yuan]{xiao2024florence}
Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael
  Zeng, Ce~Liu, and Lu~Yuan.
\newblock Florence-2: Advancing a unified representation for a variety of
  vision tasks.
\newblock 2024{\natexlab{a}}.

\bibitem[Xiao et~al.(2024{\natexlab{b}})Xiao, Lin, Seznec, Wu, Demouth, and
  Han]{xiao2024smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models, 2024{\natexlab{b}}.

\bibitem[Xiao et~al.(2021)Xiao, Shang, Yao, and Chua]{xiao2021next}
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
\newblock Next-qa: Next phase of question-answering to explaining temporal
  actions.
\newblock In \emph{CVPR}, 2021.

\bibitem[Xie et~al.(2024)Xie, Goyal, Zheng, Kan, Lillicrap, Kawaguchi, and
  Shieh]{xie2024monte}
Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy~P Lillicrap, Kenji
  Kawaguchi, and Michael Shieh.
\newblock Monte carlo tree search boosts reasoning via iterative preference
  learning.
\newblock \emph{arXiv preprint arXiv:2405.00451}, 2024.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin,
  Rungta, Sankararaman, Oguz, Khabsa, Fang, Mehdad, Narang, Malik, Fan,
  Bhosale, Edunov, Lewis, Wang, and Ma]{xiong2023effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui
  Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz,
  Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela
  Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{arXiv preprint arXiv:2309.16039}, 2023.

\bibitem[Xu et~al.(2023)Xu, Xie, Tan, Huang, Howes, Sharma, Li, Ghosh,
  Zettlemoyer, and Feichtenhofer]{xu2023demystifying}
Hu~Xu, Saining Xie, Xiaoqing~Ellen Tan, Po-Yao Huang, Russell Howes, Vasu
  Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph
  Feichtenhofer.
\newblock Demystifying clip data.
\newblock \emph{arXiv preprint arXiv:2309.16671}, 2023.

\bibitem[Yan et~al.(2024)Yan, Mao, Ji, Zhang, Patil, Stoica, and
  Gonzalez]{berkeley-function-calling-leaderboard}
Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir~G. Patil,
  Ion Stoica, and Joseph~E. Gonzalez.
\newblock Berkeley function calling leaderboard.
\newblock
  \url{https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html},
  2024.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Zhang, Li, Zou, Li, and
  Gao]{yang2023set}
Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao.
\newblock Set-of-mark prompting unleashes extraordinary visual grounding in
  gpt-4v.
\newblock \emph{arXiv preprint arXiv:2310.11441}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Li, Wang, Lin, Azarnasab, Ahmed,
  Liu, Liu, Zeng, and Wang]{yang2023mmreact}
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal
  Ahmed, Zicheng Liu, Ce~Liu, Michael Zeng, and Lijuan Wang.
\newblock Mm-react: Prompting chatgpt for multimodal reasoning and action.
\newblock 2023{\natexlab{b}}.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and
  Cao]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
  and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Ye et~al.(2023)Ye, Xu, Xu, Ye, Yan, Zhou, Wang, Hu, Shi, Shi, Li, Xu,
  Chen, Tian, Qian, Zhang, Huang, and Zhou]{ye2023mplug}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
  Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong
  Chen, Junfeng Tian, Qi~Qian, Ji~Zhang, Fei Huang, and Jingren Zhou.
\newblock mplug-owl: Modularization empowers large language models with
  multimodality.
\newblock 2023.

\bibitem[Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and
  Liu]{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang,
  James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large
  language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023.

\bibitem[Yu et~al.(2019)Yu, Xu, Yu, Yu, Zhao, Zhuang, and
  Tao]{yu2019activityqa}
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng
  Tao.
\newblock Activitynet-qa: A dataset for understanding complex web videos via
  question answering.
\newblock In \emph{AAAI}, 2019.

\bibitem[Yue et~al.(2023)Yue, Qu, Zhang, Fu, Huang, Sun, Su, and
  Chen]{yue2023mammoth}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and
  Wenhu Chen.
\newblock Mammoth: Building math generalist models through hybrid instruction
  tuning.
\newblock \emph{arXiv preprint arXiv:2309.05653}, 2023.

\bibitem[Yue et~al.(2024{\natexlab{a}})Yue, Ni, Zhang, Zheng, Liu, Zhang,
  Stevens, Jiang, Ren, Sun, Wei, Yu, Yuan, Sun, Yin, Zheng, Yang, Liu, Huang,
  Sun, Su, and Chen]{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel
  Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin
  Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao
  Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock Mmmu: A massive multi-discipline multimodal understanding and
  reasoning benchmark for expert agi.
\newblock In \emph{Proceedings of CVPR}, 2024{\natexlab{a}}.

\bibitem[Yue et~al.(2024{\natexlab{b}})Yue, Zheng, Zhang, and
  Chen]{yue2024mammoth2}
Xiang Yue, Tuney Zheng, Ge~Zhang, and Wenhu Chen.
\newblock Mammoth2: Scaling instructions from the web.
\newblock \emph{arXiv preprint arXiv:2405.03548}, 2024{\natexlab{b}}.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 15476--15488, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Li, and Bing]{zhang2023videollama}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for
  video understanding.
\newblock \emph{arXiv preprint arXiv:2306.02858}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Chen, Hu, Xu, Chen, Hao, Han, Thai, Wang,
  Liu, et~al.]{zhang2024infty}
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo~Khai Hao,
  Xu~Han, Zhen~Leng Thai, Shuo Wang, Zhiyuan Liu, et~al.
\newblock $\infty$ bench: Extending long context evaluation beyond 100k tokens.
\newblock \emph{arXiv preprint arXiv:2402.13718}, 2024.

\bibitem[Zhang et~al.(2021)Zhang, Colbert, Kreutz-Delgado, and
  Das]{zhang2021training}
Xinyu Zhang, Ian Colbert, Ken Kreutz-Delgado, and Srinjoy Das.
\newblock Training deep neural networks with joint quantization and pruning of
  weights and activations, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Baldridge, and He]{zhang-etal-2019-paws}
Yuan Zhang, Jason Baldridge, and Luheng He.
\newblock {PAWS}: Paraphrase adversaries from word scrambling.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
  \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of
  the Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 1298--1308, Minneapolis, Minnesota,
  June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1131}.
\newblock \url{https://aclanthology.org/N19-1131}.

\bibitem[Zhao et~al.(2023{\natexlab{a}})Zhao, Zhou, Li, Tang, Wang, Hou, Min,
  Zhang, Zhang, Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu,
  Nie, and Wen]{LLMSurvey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
  Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
  Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023{\natexlab{a}}.
\newblock \url{http://arxiv.org/abs/2303.18223}.

\bibitem[Zhao et~al.(2023{\natexlab{b}})Zhao, Gu, Varma, Luo, Huang, Xu,
  Wright, Shojanazeri, Ott, Shleifer, Desmaison, Balioglu, Damania, Nguyen,
  Chauhan, Hao, Mathews, and Li]{zhao2023pytorch}
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less
  Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can
  Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit
  Mathews, and Shen Li.
\newblock Pytorch fsdp: Experiences on scaling fully sharded data parallel,
  2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2022)Zhao, Misra, Kr{\"a}henb{\"u}hl, and
  Girdhar]{zhao2022lavila}
Yue Zhao, Ishan Misra, Philipp Kr{\"a}henb{\"u}hl, and Rohit Girdhar.
\newblock Learning video representations from large language models.
\newblock In \emph{arXiv preprint arXiv:2212.04501}, 2022.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh]{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning, {ICML} 2021, 18-24 July
  2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning
  Research}, pages 12697--12706. {PMLR}, 2021.
\newblock \url{http://proceedings.mlr.press/v139/zhao21c.html}.

\bibitem[Zheng et~al.(2023)Zheng, Zhou, Meng, Zhou, and Huang]{zheng2023large}
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang.
\newblock Large language models are not robust multiple choice selectors.
\newblock \emph{CoRR}, abs/2309.03882, 2023.
\newblock \doi{10.48550/ARXIV.2309.03882}.
\newblock \url{https://doi.org/10.48550/arXiv.2309.03882}.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and
  Duan]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin
  Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: A human-centric benchmark for evaluating foundation models.
\newblock \emph{arXiv preprint arXiv:2304.06364}, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu,
  et~al.]{zhou2024lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao,
  Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and
  Hou]{zhou2023instruction}
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu,
  Yi~Luan, Denny Zhou, and Le~Hou.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2311.07911}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Lei, Liu, Du, Huang, Zhao, Dai, Le, Laudon,
  et~al.]{zhou2022mixture}
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew~M
  Dai, Quoc~V Le, James Laudon, et~al.
\newblock Mixture-of-experts with expert choice routing.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 7103--7114, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced
  large language models.
\newblock 2023.

\end{thebibliography}
