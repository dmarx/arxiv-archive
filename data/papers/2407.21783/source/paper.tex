\documentclass[dvipsnames]{fairmeta}

\title{The Llama 3 Herd of Models}

\author[1]{Llama Team, AI @ Meta}
\affiliation[1]{A detailed contributor list can be found in the appendix of this paper.}

\abstract{
Modern artificial intelligence (AI) systems are powered by foundation models.
This paper presents a new set of foundation models, called Llama 3.
It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage.
Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens.
This paper presents an extensive empirical evaluation of Llama 3.
We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks.
We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety.
The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach.
We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks.
The resulting models are not yet being broadly released as they are still under development.
}

\date{July 23, 2024}

\metadata[Website]{\url{https://llama.meta.com/}}

\usepackage{makecell}
\usepackage{siunitx}
\usepackage{framed}
\usepackage{pifont}
\usepackage{graphicx,subcaption}
\usepackage{pifont}
\usepackage{xspace}
\usepackage{nicematrix}
\usepackage{nicefrac}
\usepackage{wrapfig}
\newcommand{\rulesep}{\unskip\ \vrule\ }
\newcommand{\cmark}{\textcolor{ForestGreen}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}

\begin{document}

\maketitle

\providecommand{\llama}{Llama\xspace}
\providecommand{\llamatwo}{Llama~2\xspace}
\providecommand{\llamathree}{Llama~3\xspace}
\providecommand{\TODO}[1]{{\color{red}[\textbf{TODO}: #1]}}
\providecommand{\mlc}{Multilingual\xspace}
\providecommand{\gpt}{GPT-4\xspace}
\providecommand{\gptp}{GPT-4\xspace}
\providecommand{\gpto}{GPT-4o\xspace}
\providecommand{\gptfourturbo}{GPT-4 Turbo\xspace}
\providecommand{\sonnet}{Claude 3.5 Sonnet\xspace}
\providecommand{\nemotron}{Nemotron 4 340B\xspace}
\providecommand{\mixtralbig}{Mixtral 8$\times$22B\xspace}
\providecommand{\gptthreedotfivet}{GPT-3.5 Turbo\xspace}
\providecommand{\gemmatwo}{Gemma 2 9B\xspace}
\providecommand{\mistralsmall}{Mistral 7B\xspace}
\providecommand*{\acc}[1]{\num[round-mode=places,round-precision=2]{#1}}


\input{introduction.tex}
\input{overview.tex}
\input{pretraining.tex}
\input{posttraining.tex}
\input{results.tex}
\input{inference.tex}
\input{vision.tex}
\input{speech.tex}
\input{related_work.tex}
\input{conclusion.tex}

\clearpage
\input{contributors.tex}

\clearpage
\newpage
\bibliographystyle{assets/plainnat}
\bibliography{paper,anthology}



\end{document}
