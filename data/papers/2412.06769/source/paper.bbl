\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Amalric and Dehaene(2019)]{amalric2019distinct}
Marie Amalric and Stanislas Dehaene.
\newblock A distinct cortical network for mathematical knowledge in the human brain.
\newblock \emph{NeuroImage}, 189:\penalty0 19--31, 2019.

\bibitem[Biran et~al.(2024)Biran, Gottesman, Yang, Geva, and Globerson]{biran2024hopping}
Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson.
\newblock Hopping too late: Exploring the limitations of large language models on multi-hop queries.
\newblock \emph{arXiv preprint arXiv:2406.12775}, 2024.

\bibitem[Chen et~al.(2023)Chen, Yin, Ku, Lu, Wan, Ma, Xu, Wang, and Xia]{chen2023theoremqa}
Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia.
\newblock Theoremqa: A theorem-driven question answering dataset.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 7889--7901, 2023.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[DeepMind(2024)]{alphaproof2024}
Google DeepMind.
\newblock Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024.
\newblock \url{https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/}.

\bibitem[Deng et~al.(2023)Deng, Prasad, Fernandez, Smolensky, Chaudhary, and Shieber]{deng2023implicit}
Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber.
\newblock Implicit chain of thought reasoning via knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2311.01460}, 2023.

\bibitem[Deng et~al.(2024)Deng, Choi, and Shieber]{deng2024explicit}
Yuntian Deng, Yejin Choi, and Stuart Shieber.
\newblock From explicit cot to implicit cot: Learning to internalize cot step by step.
\newblock \emph{arXiv preprint arXiv:2405.14838}, 2024.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Fan et~al.(2024)Fan, Du, Ramchandran, and Lee]{fan2024looped}
Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee.
\newblock Looped transformers for length generalization.
\newblock \emph{arXiv preprint arXiv:2409.15647}, 2024.

\bibitem[Fedorenko et~al.(2011)Fedorenko, Behr, and Kanwisher]{fedorenko2011functional}
Evelina Fedorenko, Michael~K Behr, and Nancy Kanwisher.
\newblock Functional specificity for high-level linguistic processing in the human brain.
\newblock \emph{Proceedings of the National Academy of Sciences}, 108\penalty0 (39):\penalty0 16428--16433, 2011.

\bibitem[Fedorenko et~al.(2024)Fedorenko, Piantadosi, and Gibson]{fedorenko2024language}
Evelina Fedorenko, Steven~T Piantadosi, and Edward~AF Gibson.
\newblock Language is primarily a tool for communication rather than thought.
\newblock \emph{Nature}, 630\penalty0 (8017):\penalty0 575--586, 2024.

\bibitem[Feng et~al.(2023)Feng, Zhang, Gu, Ye, He, and Wang]{feng2023towards}
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di~He, and Liwei Wang.
\newblock Towards revealing the mystery behind chain of thought: a theoretical perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Gandhi et~al.(2024)Gandhi, Lee, Grand, Liu, Cheng, Sharma, and Goodman]{gandhi2024stream}
Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah~D Goodman.
\newblock Stream of search (sos): Learning to search in language.
\newblock \emph{arXiv preprint arXiv:2404.03683}, 2024.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and Papailiopoulos]{giannou2023looped}
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason~D Lee, and Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock In \emph{International Conference on Machine Learning}, pages 11398--11442. PMLR, 2023.

\bibitem[Gloeckle et~al.(2024)Gloeckle, Idrissi, Rozi{\`e}re, Lopez-Paz, and Synnaeve]{gloeckle2024better}
Fabian Gloeckle, Badr~Youbi Idrissi, Baptiste Rozi{\`e}re, David Lopez-Paz, and Gabriel Synnaeve.
\newblock Better \& faster large language models via multi-token prediction.
\newblock \emph{arXiv preprint arXiv:2404.19737}, 2024.

\bibitem[Goyal et~al.(2023)Goyal, Ji, Rawat, Menon, Kumar, and Nagarajan]{goyal2023think}
Sachin Goyal, Ziwei Ji, Ankit~Singh Rawat, Aditya~Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan.
\newblock Think before you speak: Training language models with pause tokens.
\newblock \emph{arXiv preprint arXiv:2310.02226}, 2023.

\bibitem[Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu]{hao2023reasoning}
Shibo Hao, Yi~Gu, Haodi Ma, Joshua~Jiahua Hong, Zhen Wang, Daisy~Zhe Wang, and Zhiting Hu.
\newblock Reasoning with language model is planning with world model.
\newblock \emph{arXiv preprint arXiv:2305.14992}, 2023.

\bibitem[Hao et~al.(2024)Hao, Gu, Luo, Liu, Shao, Wang, Xie, Ma, Samavedhi, Gao, et~al.]{hao2024llm}
Shibo Hao, Yi~Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et~al.
\newblock Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models.
\newblock \emph{arXiv preprint arXiv:2404.05221}, 2024.

\bibitem[Havrilla et~al.(2024)Havrilla, Du, Raparthy, Nalmpantis, Dwivedi-Yu, Zhuravinskyi, Hambro, Sukhbaatar, and Raileanu]{havrilla2024teaching}
Alex Havrilla, Yuqing Du, Sharath~Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu.
\newblock Teaching large language models to reason with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2403.04642}, 2024.

\bibitem[Khot et~al.(2022)Khot, Trivedi, Finlayson, Fu, Richardson, Clark, and Sabharwal]{khot2022decomposed}
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.
\newblock Decomposed prompting: A modular approach for solving complex tasks.
\newblock \emph{arXiv preprint arXiv:2210.02406}, 2022.

\bibitem[LeCun(2022)]{lecun2022path}
Yann LeCun.
\newblock A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.
\newblock \emph{Open Review}, 62\penalty0 (1):\penalty0 1--62, 2022.

\bibitem[Lehnert et~al.(2024)Lehnert, Sukhbaatar, Mcvay, Rabbat, and Tian]{lehnert2024beyond}
Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian.
\newblock Beyond a*: Better planning with transformers via search dynamics bootstrapping.
\newblock \emph{arXiv preprint arXiv:2402.14083}, 2024.

\bibitem[Li et~al.(2024)Li, Liu, Zhou, and Ma]{li2024chain}
Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma.
\newblock Chain of thought empowers transformers to solve inherently serial problems.
\newblock \emph{arXiv preprint arXiv:2402.12875}, 2024.

\bibitem[Madaan and Yazdanbakhsh(2022)]{madaan2022text}
Aman Madaan and Amir Yazdanbakhsh.
\newblock Text and patterns: For effective chain of thought, it takes two to tango.
\newblock \emph{arXiv preprint arXiv:2209.07686}, 2022.

\bibitem[Merrill and Sabharwal(2023)]{merrill2023expresssive}
William Merrill and Ashish Sabharwal.
\newblock The expresssive power of transformers with chain of thought.
\newblock \emph{arXiv preprint arXiv:2310.07923}, 2023.

\bibitem[Monti et~al.(2007)Monti, Osherson, Martinez, and Parsons]{monti2007functional}
Martin~M Monti, Daniel~N Osherson, Michael~J Martinez, and Lawrence~M Parsons.
\newblock Functional neuroanatomy of deductive inference: a language-independent distributed network.
\newblock \emph{Neuroimage}, 37\penalty0 (3):\penalty0 1005--1016, 2007.

\bibitem[Monti et~al.(2009)Monti, Parsons, and Osherson]{monti2009boundaries}
Martin~M Monti, Lawrence~M Parsons, and Daniel~N Osherson.
\newblock The boundaries of language and thought in deductive inference.
\newblock \emph{Proceedings of the National Academy of Sciences}, 106\penalty0 (30):\penalty0 12554--12559, 2009.

\bibitem[Monti et~al.(2012)Monti, Parsons, and Osherson]{monti2012thought}
Martin~M Monti, Lawrence~M Parsons, and Daniel~N Osherson.
\newblock Thought beyond language: neural dissociation of algebra and natural language.
\newblock \emph{Psychological science}, 23\penalty0 (8):\penalty0 914--922, 2012.

\bibitem[Pfau et~al.(2024)Pfau, Merrill, and Bowman]{pfau2024let}
Jacob Pfau, William Merrill, and Samuel~R Bowman.
\newblock Let's think dot by dot: Hidden computation in transformer language models.
\newblock \emph{arXiv preprint arXiv:2404.15758}, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Saparov and He(2022)]{saparov2022language}
Abulhair Saparov and He~He.
\newblock Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.
\newblock \emph{arXiv preprint arXiv:2210.01240}, 2022.

\bibitem[Shalev et~al.(2024)Shalev, Feder, and Goldstein]{shalev2024distributional}
Yuval Shalev, Amir Feder, and Ariel Goldstein.
\newblock Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning.
\newblock \emph{arXiv preprint arXiv:2406.13858}, 2024.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo]{shao2024deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK~Li, Yu~Wu, and Daya Guo.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Su et~al.(2024)Su, Sukhbaatar, Rabbat, Tian, and Zheng]{su2024dualformer}
DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng.
\newblock Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces.
\newblock \emph{arXiv preprint arXiv:2410.09918}, 2024.

\bibitem[Turpin et~al.(2024)Turpin, Michael, Perez, and Bowman]{turpin2024language}
Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman.
\newblock Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Wang et~al.(2022)Wang, Min, Deng, Shen, Wu, Zettlemoyer, and Sun]{wang2022towards}
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.
\newblock Towards understanding chain-of-thought prompting: An empirical study of what matters.
\newblock \emph{arXiv preprint arXiv:2212.10001}, 2022.

\bibitem[Wang et~al.(2024)Wang, Li, Shao, Xu, Dai, Li, Chen, Wu, and Sui]{wang2024math}
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu~Wu, and Zhifang Sui.
\newblock Math-shepherd: Verify and reinforce llms step-by-step without human annotations.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 9426--9439, 2024.

\bibitem[Wang et~al.(2023)Wang, Caccia, Ostapenko, Yuan, Wang, and Sordoni]{wang2023guiding}
Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William~Yang Wang, and Alessandro Sordoni.
\newblock Guiding language model reasoning with planning tokens.
\newblock \emph{arXiv preprint arXiv:2310.05707}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Xie et~al.(2023)Xie, Kawaguchi, Zhao, Zhao, Kan, He, and Xie]{xie2023self}
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James~Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.
\newblock Self-evaluation guided beam search for reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Yang et~al.(2024)Yang, Gribovskaya, Kassner, Geva, and Riedel]{yang2024large}
Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel.
\newblock Do large language models latently perform multi-hop reasoning?
\newblock \emph{arXiv preprint arXiv:2402.16837}, 2024.

\bibitem[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Yu et~al.(2024{\natexlab{a}})Yu, Jiang, Kang, Hao, and Qin]{yu2024flow}
Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin.
\newblock Flow of reasoning: Efficient training of llm policy with divergent thinking.
\newblock \emph{arXiv preprint arXiv:2406.05673}, 2024{\natexlab{a}}.

\bibitem[Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu]{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023.

\bibitem[Yu et~al.(2024{\natexlab{b}})Yu, Xu, Weston, and Kulikov]{yu2024distilling}
Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov.
\newblock Distilling system 2 into system 1.
\newblock \emph{arXiv preprint arXiv:2407.06023}, 2024{\natexlab{b}}.

\bibitem[Yue et~al.(2023)Yue, Qu, Zhang, Fu, Huang, Sun, Su, and Chen]{yue2023mammoth}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock Mammoth: Building math generalist models through hybrid instruction tuning.
\newblock \emph{arXiv preprint arXiv:2309.05653}, 2023.

\bibitem[Zelikman et~al.(2024)Zelikman, Harik, Shao, Jayasiri, Haber, and Goodman]{zelikman2024quiet}
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah~D Goodman.
\newblock Quiet-star: Language models can teach themselves to think before speaking.
\newblock \emph{arXiv preprint arXiv:2403.09629}, 2024.

\bibitem[Zhou et~al.(2022)Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le, et~al.]{zhou2022least}
Denny Zhou, Nathanael Sch{\"a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et~al.
\newblock Least-to-most prompting enables complex reasoning in large language models.
\newblock \emph{arXiv preprint arXiv:2205.10625}, 2022.

\end{thebibliography}
