\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarap(2018)]{agarap2018deep}
Abien~Fred Agarap.
\newblock Deep learning using rectified linear units ({ReLU}).
\newblock \emph{arXiv preprint arXiv:1803.08375}, 2018.
\newblock URL \url{https://arxiv.org/pdf/1803.08375.pdf}.

\bibitem[Ahmad \& Scheinkman(2019)Ahmad and Scheinkman]{ahmad2019can}
Subutai Ahmad and Luiz Scheinkman.
\newblock How can we be so dense? {The} benefits of using highly sparse representations.
\newblock \emph{arXiv preprint arXiv:1903.11257}, 2019.
\newblock URL \url{https://arxiv.org/pdf/1903.11257.pdf}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{mbpp}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2108.07732.pdf}.

\bibitem[Besiroglu et~al.(2024)Besiroglu, Erdil, Barnett, and You]{besiroglu2024chinchilla}
Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You.
\newblock Chinchilla scaling: A replication attempt.
\newblock \emph{arXiv preprint arXiv:2404.10102}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2404.10102}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock {PIQA}: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pp.\  7432--7439, 2020.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/6239/6095}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{humaneval}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2107.03374.pdf}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock {BoolQ}: Exploring the surprising difficulty of natural yes/no questions.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  2924--2936, 2019.
\newblock URL \url{https://aclanthology.org/N19-1300.pdf}.

\bibitem[Clark et~al.(2020)Clark, Choi, Collins, Garrette, Kwiatkowski, Nikolaev, and Palomaki]{tydiqa}
Jonathan~H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.
\newblock {TyDi QA}: A benchmark for information-seeking question answering in typologically diverse languages.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 454--470, 2020.
\newblock URL \url{https://aclanthology.org/2020.tacl-1.30.pdf}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2110.14168.pdf}.

\bibitem[Colombo et~al.(2024)Colombo, Pires, Boudiaf, Culver, Melo, Corro, Martins, Esposito, Raposo, Morgado, et~al.]{colombo2024saullm}
Pierre Colombo, Telmo~Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre~FT Martins, Fabrizio Esposito, Vera~L{\'u}cia Raposo, Sofia Morgado, et~al.
\newblock {SaulLM-7B}: A pioneering large language model for law.
\newblock \emph{arXiv preprint arXiv:2403.03883}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2403.03883}.

\bibitem[Cuadros et~al.(2022)Cuadros, Zappella, and Apostoloff]{cuadros2022self}
Xavier~Suau Cuadros, Luca Zappella, and Nicholas Apostoloff.
\newblock Self-conditioning pre-trained language models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  4455--4473. PMLR, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/cuadros22a/cuadros22a.pdf}.

\bibitem[Dai et~al.(2022)Dai, Dong, Hao, Sui, Chang, and Wei]{dai2022knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.
\newblock Knowledge neurons in pretrained {Transformers}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  8493--8502, 2022.
\newblock URL \url{https://aclanthology.org/2022.acl-long.581.pdf}.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  933--941. PMLR, 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf}.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2305.14233.pdf}.

\bibitem[Elfwing et~al.(2018)Elfwing, Uchibe, and Doya]{elfwing2018sigmoid}
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
\newblock Sigmoid-weighted linear units for neural network function approximation in reinforcement learning.
\newblock \emph{Neural networks}, 107:\penalty0 3--11, 2018.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0893608017302976}.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch {Transformers}: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (120):\penalty0 1--39, 2022.
\newblock URL \url{https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf}.

\bibitem[Frantar et~al.(2023)Frantar, Ruiz, Houlsby, Alistarh, and Evci]{frantar2023scaling}
Elias Frantar, Carlos~Riquelme Ruiz, Neil Houlsby, Dan Alistarh, and Utku Evci.
\newblock Scaling laws for sparsely-connected foundation models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/pdf?id=i9K2ZWkYIP}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.
\newblock URL \url{https://arxiv.org/pdf/2101.00027.pdf}.

\bibitem[Gao et~al.(2024)Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu]{gao2024scaling}
Leo Gao, Tom~Dupr{\'e} la~Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu.
\newblock Scaling and evaluating sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:2406.04093}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2406.04093}.

\bibitem[He(2024)]{he2024mixture}
Xu~Owen He.
\newblock Mixture of a million experts.
\newblock \emph{arXiv preprint arXiv:2407.04153}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2407.04153}.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.
\newblock URL \url{https://arxiv.org/pdf/2009.03300.pdf}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock In \emph{Proceedings of the 36th International Conference on Neural Information Processing Systems}, pp.\  30016--30030, 2022.
\newblock URL \url{https://papers.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf}.

\bibitem[Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, et~al.]{hu2024minicpm}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al.
\newblock {MiniCPM}: Unveiling the potential of small language models with scalable training strategies.
\newblock \emph{arXiv preprint arXiv:2404.06395}, 2024.

\bibitem[Huang et~al.(2024)Huang, An, Zhuang, Tao, Zhang, Jin, Xu, Chen, Huang, and Feng]{huang2024harder}
Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, and Yansong Feng.
\newblock Harder tasks need more experts: Dynamic routing in {MoE} models.
\newblock \emph{arXiv preprint arXiv:2403.07652}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2403.07652}.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, Jia, Mou, Jernite, Mitchell, Ferrandis, Hughes, Wolf, Bahdanau, et~al.]{kocetkov2022stack}
Denis Kocetkov, Raymond Li, LI~Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos~Mu{\~n}oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et~al.
\newblock The {Stack}: 3 {TB} of permissively licensed source code.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock URL \url{https://openreview.net/pdf?id=pxpbTdUEpD}.

\bibitem[Krajewski et~al.(2024)Krajewski, Ludziejewski, Adamczewski, Pi{\'o}ro, Krutul, Antoniak, Ciebiera, Kr{\'o}l, Odrzyg{\'o}{\'z}d{\'z}, Sankowski, et~al.]{krajewski2024scaling}
Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pi{\'o}ro, Micha{\l} Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Kr{\'o}l, Tomasz Odrzyg{\'o}{\'z}d{\'z}, Piotr Sankowski, et~al.
\newblock Scaling laws for fine-grained mixture of experts.
\newblock \emph{arXiv preprint arXiv:2402.07871}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2402.07871}.

\bibitem[Kurtz et~al.(2020)Kurtz, Kopinsky, Gelashvili, Matveev, Carr, Goin, Leiserson, Moore, Shavit, and Alistarh]{kurtz2020inducing}
Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Nir Shavit, and Dan Alistarh.
\newblock Inducing and exploiting activation sparsity for fast inference on deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5533--5543. PMLR, 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf}.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock {StarCoder}: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2305.06161.pdf}.

\bibitem[Li et~al.(2022)Li, You, Bhojanapalli, Li, Rawat, Reddi, Ye, Chern, Yu, Guo, et~al.]{li2022lazy}
Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit~Singh Rawat, Sashank~J Reddi, Ke~Ye, Felix Chern, Felix Yu, Ruiqi Guo, et~al.
\newblock The lazy neuron phenomenon: On emergence of activation sparsity in {Transformers}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/pdf?id=TJ2nxciYCk-}.

\bibitem[Liu et~al.(2024)Liu, Kim, Wang, Liang, Shen, Cheng, Liu, Tanaka, Wu, Hu, Chaudhary, Lin, Zhang, Xue, Awadalla, Gao, and Chen]{liu2024gringradientinformedmoe}
Liyuan Liu, Young~Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, and Weizhu Chen.
\newblock {GRIN}: {GRadient-INformed MoE}, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.12136}.

\bibitem[Liu et~al.(2023)Liu, Wang, Dao, Zhou, Yuan, Song, Shrivastava, Zhang, Tian, Re, et~al.]{liu2023deja}
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce~Zhang, Yuandong Tian, Christopher Re, et~al.
\newblock {Deja Vu}: Contextual sparsity for efficient {LLMs} at inference time.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22137--22176. PMLR, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/liu23am/liu23am.pdf}.

\bibitem[Marquardt(1963)]{marquardt1963algorithm}
Donald~W Marquardt.
\newblock An algorithm for least-squares estimation of nonlinear parameters.
\newblock \emph{Journal of the society for Industrial and Applied Mathematics}, 11\penalty0 (2):\penalty0 431--441, 1963.

\bibitem[Mirzadeh et~al.(2023)Mirzadeh, Alizadeh, Mehta, Del~Mundo, Tuzel, Samei, Rastegari, and Farajtabar]{mirzadeh2023relu}
Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo~C Del~Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, and Mehrdad Farajtabar.
\newblock {ReLU} strikes back: Exploiting activation sparsity in large language models.
\newblock \emph{arXiv preprint arXiv:2310.04564}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2310.04564.pdf}.

\bibitem[Muthukumar \& Sulam(2023)Muthukumar and Sulam]{muthukumar2023adversarial}
Ramchandran Muthukumar and Jeremias Sulam.
\newblock Adversarial robustness of sparse local {Lipschitz} predictors.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 5\penalty0 (4):\penalty0 920--948, 2023.
\newblock URL \url{https://epubs.siam.org/doi/full/10.1137/22M1478835}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1525--1534, 2016.
\newblock URL \url{https://aclanthology.org/P16-1144.pdf}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text {Transformer}.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.
\newblock URL \url{https://www.jmlr.org/papers/volume21/20-074/20-074.pdf}.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, and Gordon]{copa}
Melissa Roemmele, Cosmin~Adrian Bejan, and Andrew~S Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
\newblock In \emph{2011 AAAI Spring Symposium Series}, 2011.
\newblock URL \url{https://cdn.aaai.org/ocs/2418/2418-10878-1-PB.pdf}.

\bibitem[Sajjad et~al.(2022)Sajjad, Durrani, and Dalvi]{sajjad2022neuron}
Hassan Sajjad, Nadir Durrani, and Fahim Dalvi.
\newblock Neuron-level interpretation of deep {NLP} models: {A survey}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 1285--1303, 2022.
\newblock URL \url{https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00519/2060745/tacl_a_00519.pdf}.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Le~Bras, Bhagavatula, and Choi]{winogrande}
Keisuke Sakaguchi, Ronan Le~Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock {WinoGrande}: An adversarial winograd schema challenge at scale.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~34, pp.\  8732--8740, 2020.
\newblock URL \url{https://cdn.aaai.org/ojs/6399/6399-13-9624-1-10-20200517.pdf}.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi]{siqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le~Bras, and Yejin Choi.
\newblock {SocialIQA}: Commonsense reasoning about social interactions.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4463--4473, 2019.
\newblock URL \url{https://aclanthology.org/D19-1454.pdf}.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock {GLU} variants improve {Transformer}.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.
\newblock URL \url{https://arxiv.org/pdf/2002.05202.pdf}.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2402.00159}.

\bibitem[Song et~al.(2024{\natexlab{a}})Song, Han, Zhang, Hu, Shi, Li, Chen, Liu, Li, Yang, and Sun]{song2024prosparse}
Chenyang Song, Xu~Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, and Maosong Sun.
\newblock {ProSparse}: Introducing and enhancing intrinsic activation sparsity within large language models.
\newblock \emph{arXiv preprint arXiv:2402.13516}, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/pdf/2402.13516.pdf}.

\bibitem[Song et~al.(2023)Song, Mi, Xie, and Chen]{song2023powerinfer}
Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen.
\newblock {PowerInfer}: Fast large language model serving with a consumer-grade {GPU}.
\newblock \emph{arXiv preprint arXiv:2312.12456}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2312.12456.pdf}.

\bibitem[Song et~al.(2024{\natexlab{b}})Song, Xie, Zhang, Wen, Ma, Mi, and Chen]{song2024turbo}
Yixin Song, Haotong Xie, Zhengyan Zhang, Bo~Wen, Li~Ma, Zeyu Mi, and Haibo Chen.
\newblock Turbo {Sparse}: Achieving {LLM} {SOTA} performance with minimal activated parameters.
\newblock \emph{arXiv preprint arXiv:2406.05955}, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/pdf/2406.05955}.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{bbh}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2210.09261.pdf}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2302.13971.pdf}.

\bibitem[Wei et~al.(2024)Wei, Wang, Liu, Ding, and Zhang]{wei2024magicoder}
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.
\newblock Magicoder: Empowering code generation with {OSS-Instruct}.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2312.02120}.

\bibitem[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
\newblock {WizardLM}: Empowering large language models to follow complex instructions.
\newblock \emph{arXiv preprint arXiv:2304.12244}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2304.12244}.

\bibitem[Xue et~al.(2024)Xue, Song, Mi, Chen, Xia, and Chen]{xue2024powerinfer}
Zhenliang Xue, Yixin Song, Zeyu Mi, Le~Chen, Yubin Xia, and Haibo Chen.
\newblock {PowerInfer-2}: Fast large language model inference on a smartphone.
\newblock \emph{arXiv preprint arXiv:2406.06282}, 2024.
\newblock URL \url{https://arxiv.org/pdf/2406.06282}.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Greg Yang, Edward~J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs {V}: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2203.03466}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pp.\  4791--4800, 2019.
\newblock URL \url{https://aclanthology.org/P19-1472.pdf}.

\bibitem[Zhang et~al.(2022)Zhang, Lin, Liu, Li, Sun, and Zhou]{zhang2022moefication}
Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.
\newblock {MoEfication}: {Transformer} feed-forward layers are mixtures of experts.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pp.\  877--890, 2022.
\newblock URL \url{https://aclanthology.org/2022.findings-acl.71.pdf}.

\bibitem[Zhang et~al.(2023)Zhang, Zeng, Lin, Xiao, Wang, Han, Liu, Xie, Sun, and Zhou]{zhang2023emergent}
Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xiaozhi Wang, Xu~Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, and Jie Zhou.
\newblock Emergent modularity in pre-trained {Transformers}.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  4066--4083, 2023.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.250.pdf}.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Song, Yu, Han, Lin, Xiao, Song, Liu, Mi, and Sun]{zhang2024relu}
Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu~Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, and Maosong Sun.
\newblock {ReLU}$^2$ wins: Discovering efficient activation functions for sparse {LLMs}.
\newblock \emph{arXiv preprint arXiv:2402.03804}, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/pdf/2402.03804.pdf}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Xiao, Qin, Lin, Zeng, Han, Liu, Xie, Sun, and Zhou]{zhang2024exploring}
Zhengyan Zhang, Chaojun Xiao, Qiujieli Qin, Yankai Lin, Zhiyuan Zeng, Xu~Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, and Jie Zhou.
\newblock Exploring the benefit of activation sparsity in pre-training.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/pdf?id=KfXXPCcobh}.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan]{agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
\newblock {AGIEval}: A human-centric benchmark for evaluating foundation models.
\newblock \emph{arXiv preprint arXiv:2304.06364}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2304.06364.pdf}.

\bibitem[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and Fedus]{zoph2022st}
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus.
\newblock {ST-MoE}: Designing stable and transferable sparse expert models.
\newblock \emph{arXiv preprint arXiv:2202.08906}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2202.08906}.

\end{thebibliography}
