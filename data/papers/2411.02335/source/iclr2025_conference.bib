% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021},
  url={https://arxiv.org/pdf/2109.01652},
}

@article{touvron2023llama,
  title={{LLaMA}: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/pdf/2302.13971.pdf},
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
}

@misc{chatgpt,
  author = {OpenAI},
  title = {{ChatGPT}},
  year = {2023},
  url = {https://openai.com/blog/chatgpt},
}

@article{touvron2023llama2,
  title={{LLaMA} 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023},
  url={https://arxiv.org/pdf/2307.09288.pdf},
}

@article{achiam2023gpt,
  title={{GPT-4} technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023},
  url={https://arxiv.org/pdf/2303.08774.pdf},
}

@article{pope2023efficiently,
  title={Efficiently scaling {Transformer} inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023},
  url={https://proceedings.mlsys.org/paper_files/paper/2023/file/523f87e9d08e6071a3bbd150e6da40fb-Paper-mlsys2023.pdf},
}

@inproceedings{aminabadi2022deepspeed,
  title={{DeepSpeed-Inference}: enabling efficient inference of {Transformer} models at unprecedented scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2022},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/10046087},
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf},
}

@article{bai2022towards,
  title={Towards efficient post-training quantization of pre-trained language models},
  author={Bai, Haoli and Hou, Lu and Shang, Lifeng and Jiang, Xin and King, Irwin and Lyu, Michael R},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1405--1418},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/file/096347b4efc264ae7f07742fea34af1f-Paper-Conference.pdf},
}

@article{yao2023comprehensive,
  title={A comprehensive study on post-training quantization for large language models},
  author={Yao, Zhewei and Li, Cheng and Wu, Xiaoxia and Youn, Stephen and He, Yuxiong},
  journal={arXiv preprint arXiv:2303.08302},
  year={2023},
  url={https://arxiv.org/pdf/2303.08302.pdf},
}

@article{sun2023simple,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023},
  url={https://arxiv.org/pdf/2306.11695.pdf},
}

@inproceedings{frantar2023sparsegpt,
  title={{SparseGPT}: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v202/frantar23a/frantar23a.pdf},
}

@article{xia2023sheared,
  title={Sheared {LLaMA}: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023},
  url={https://arxiv.org/pdf/2310.06694.pdf},
}

@inproceedings{leviathan2023fast,
  title={Fast inference from {Transformers} via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v202/leviathan23a/leviathan23a.pdf},
}

@inproceedings{liu2023deja,
  title={{Deja Vu}: Contextual sparsity for efficient {LLMs} at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  booktitle={International Conference on Machine Learning},
  pages={22137--22176},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v202/liu23am/liu23am.pdf}
}

@article{song2023powerinfer,
  title={{PowerInfer}: Fast Large Language Model Serving with a Consumer-grade {GPU}},
  author={Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  journal={arXiv preprint arXiv:2312.12456},
  year={2023},
  url={https://arxiv.org/pdf/2312.12456.pdf},
}

@article{muthukumar2023adversarial,
  title={Adversarial robustness of sparse local {Lipschitz} predictors},
  author={Muthukumar, Ramchandran and Sulam, Jeremias},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={5},
  number={4},
  pages={920--948},
  year={2023},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/full/10.1137/22M1478835},
}

@article{ahmad2019can,
  title={How can we be so dense? {The} benefits of using highly sparse representations},
  author={Ahmad, Subutai and Scheinkman, Luiz},
  journal={arXiv preprint arXiv:1903.11257},
  year={2019},
  url={https://arxiv.org/pdf/1903.11257.pdf}
}

@inproceedings{correia2019adaptively,
  title={Adaptively Sparse {Transformers}},
  author={Correia, Gon{\c{c}}alo M and Niculae, Vlad and Martins, Andr{\'e} FT},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2174--2184},
  year={2019},
  url={https://aclanthology.org/D19-1223.pdf}
}

@inproceedings{li2022lazy,
  title={The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in {Transformers}},
  author={Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/pdf?id=TJ2nxciYCk-}
}

@article{zhang2022opt,
  title={{OPT}: Open pre-trained {Transformer} language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022},
  url={https://arxiv.org/pdf/2205.01068.pdf},
}

@article{agarap2018deep,
  title={Deep learning using rectified linear units ({ReLU})},
  author={Agarap, Abien Fred},
  journal={arXiv preprint arXiv:1803.08375},
  year={2018},
  url={https://arxiv.org/pdf/1803.08375.pdf}
}

@article{chowdhery2023palm,
  title={{PaLM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023},
  url={https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf}
}

@article{almazrouei2023falcon,
  title={The {Falcon} series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023},
  url={https://arxiv.org/pdf/2311.16867.pdf}
}

@article{mirzadeh2023relu,
  title={{ReLU} strikes back: Exploiting activation sparsity in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Sachin and Del Mundo, Carlo C and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2310.04564},
  year={2023},
  url={https://arxiv.org/pdf/2310.04564.pdf}
}

@article{li2020efficacy,
  title={The Efficacy of $ L_1 $ Regularization in Two-Layer Neural Networks},
  author={Li, Gen and Gu, Yuantao and Ding, Jie},
  journal={arXiv preprint arXiv:2010.01048},
  year={2020},
  url={https://arxiv.org/pdf/2010.01048.pdf}
}

@article{ma2019transformed,
  title={Transformed $ L_1 $ regularization for learning sparse deep neural networks},
  author={Ma, Rongrong and Miao, Jianyu and Niu, Lingfeng and Zhang, Peng},
  journal={Neural Networks},
  volume={119},
  pages={286--298},
  year={2019},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S0893608019302321},
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units ({GELUs})},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016},
  url={https://arxiv.org/pdf/1606.08415.pdf}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S0893608017302976}
}

@inproceedings{ng2004feature,
  title={Feature selection, $ L_1 $ vs. $ L_2 $ regularization, and rotational invariance},
  author={Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={78},
  year={2004},
  url={https://dl.acm.org/doi/abs/10.1145/1015330.1015435}
}

@article{zhang2015survey,
  title={A survey of sparse representation: algorithms and applications},
  author={Zhang, Zheng and Xu, Yong and Yang, Jian and Li, Xuelong and Zhang, David},
  journal={IEEE access},
  volume={3},
  pages={490--530},
  year={2015},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/7102696}
}

@inproceedings{ying2019overview,
  title={An overview of overfitting and its solutions},
  author={Ying, Xue},
  booktitle={Journal of physics: Conference series},
  volume={1168},
  pages={022022},
  year={2019},
  organization={IOP Publishing},
  url={https://iopscience.iop.org/article/10.1088/1742-6596/1168/2/022022/pdf}
}

@inproceedings{loshchilov2016sgdr,
  title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2016},
  url={https://arxiv.org/pdf/1608.03983.pdf}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International Conference on Machine Learning},
  pages={933--941},
  year={2017},
  organization={PMLR},
  url={https://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf}
}

@article{shazeer2020glu,
  title={{GLU} variants improve {Transformer}},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020},
  url={https://arxiv.org/pdf/2002.05202.pdf}
}

@inproceedings{han2022bminf,
  title={{BMInf}: An efficient toolkit for big model inference and tuning},
  author={Han, Xu and Zeng, Guoyang and Zhao, Weilin and Liu, Zhiyuan and Zhang, Zhengyan and Zhou, Jie and Zhang, Jun and Chao, Jia and Sun, Maosong},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={224--230},
  year={2022},
  url={https://aclanthology.org/2022.acl-demo.22.pdf}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single GPU},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v202/sheng23a/sheng23a.pdf}
}

@article{humaneval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021},
  url={https://arxiv.org/pdf/2107.03374.pdf}
}

@article{mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021},
  url={https://arxiv.org/pdf/2108.07732.pdf}
}

@inproceedings{piqa,
  title={{PIQA}: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={7432--7439},
  year={2020},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/6239/6095}
}

@inproceedings{siqa,
  title={{SocialIQA}: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4463--4473},
  year={2019},
  url={https://aclanthology.org/D19-1454.pdf}
}

@inproceedings{hellaswag,
  title={{HellaSwag}: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019},
  url={https://aclanthology.org/P19-1472.pdf}
}

@inproceedings{winogrande,
  title={{WinoGrande}: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={8732--8740},
  year={2020},
  url={https://cdn.aaai.org/ojs/6399/6399-13-9624-1-10-20200517.pdf}
}

@article{arc,
  title={Think you have Solved Question Answering? {Try ARC}, the {AI2} Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018},
  url={https://arxiv.org/pdf/1803.05457.pdf}
}

@inproceedings{commonsenseqa,
  title={{CommonsenseQA}: A Question Answering Challenge Targeting Commonsense Knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4149--4158},
  year={2019},
  url={https://aclanthology.org/N19-1421.pdf}
}

@inproceedings{copa,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI Spring Symposium Series},
  year={2011},
  url={https://cdn.aaai.org/ocs/2418/2418-10878-1-PB.pdf}
}

@inproceedings{boolq,
  title={{BoolQ}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019},
  url={https://aclanthology.org/N19-1300.pdf}
}

@inproceedings{lambada,
  title={The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1525--1534},
  year={2016},
  url={https://aclanthology.org/P16-1144.pdf}
}

@article{tydiqa,
  title={{TyDi QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
  author={Clark, Jonathan H and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and Nikolaev, Vitaly and Palomaki, Jennimaria},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={454--470},
  year={2020},
  url={https://aclanthology.org/2020.tacl-1.30.pdf}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021},
  url={https://arxiv.org/pdf/2110.14168.pdf}
}

@article{mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020},
  url={https://arxiv.org/pdf/2009.03300.pdf}
}

@article{bbh,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022},
  url={https://arxiv.org/pdf/2210.09261.pdf}
}

@article{agieval,
  title={{AGIEval}: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023},
  url={https://arxiv.org/pdf/2304.06364.pdf}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020},
  url={https://arxiv.org/abs/2001.08361}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015},
  url={https://arxiv.org/pdf/1510.00149.pdf}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018},
  url={https://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf}
}

@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1325--1334},
  year={2019},
  url={https://openaccess.thecvf.com/content_ICCV_2019/papers/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.pdf}
}

@inproceedings{zhao2019improving,
  title={Improving neural network quantization without retraining using outlier channel splitting},
  author={Zhao, Ritchie and Hu, Yuwei and Dotzel, Jordan and De Sa, Chris and Zhang, Zhiru},
  booktitle={International Conference on Machine Learning},
  pages={7543--7552},
  year={2019},
  organization={PMLR},
  url={http://proceedings.mlr.press/v97/zhao19c/zhao19c.pdf}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015},
  url={https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf}
}

@article{ma2023llm,
  title={{LLM-Pruner}: On the Structural Pruning of Large Language Models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={arXiv preprint arXiv:2305.11627},
  year={2023},
  url={https://arxiv.org/pdf/2305.11627.pdf}
}

@inproceedings{molchanov2016pruning,
  title={Pruning Convolutional Neural Networks for Resource Efficient Inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  booktitle={International Conference on Learning Representations},
  year={2016},
  url={https://openreview.net/pdf?id=SJGCiw5gl}
}

@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={10882--11005},
  year={2021},
  publisher={JMLRORG},
  url={https://dl.acm.org/doi/pdf/10.5555/3546258.3546499}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015},
  url={https://arxiv.org/pdf/1503.02531.pdf}
}

@article{tang2019distilling,
  title={Distilling task-specific knowledge from bert into simple neural networks},
  author={Tang, Raphael and Lu, Yao and Liu, Linqing and Mou, Lili and Vechtomova, Olga and Lin, Jimmy},
  journal={arXiv preprint arXiv:1903.12136},
  year={2019},
  url={https://arxiv.org/pdf/1903.12136.pdf}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image {Transformers} \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR},
  url={http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf}
}

@article{gu2023knowledge,
  title={Knowledge Distillation of Large Language Models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal={arXiv preprint arXiv:2306.08543},
  year={2023},
  url={https://arxiv.org/pdf/2306.08543.pdf}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023},
  url={https://arxiv.org/pdf/2305.02301.pdf}
}

@inproceedings{wang2023tabi,
  title={Tabi: {An} Efficient Multi-Level Inference System for Large Language Models},
  author={Wang, Yiding and Chen, Kai and Tan, Haisheng and Guo, Kun},
  booktitle={Proceedings of the Eighteenth European Conference on Computer Systems},
  pages={233--248},
  year={2023},
  url={https://dl.acm.org/doi/abs/10.1145/3552326.3587438}
}

@article{chen2023accelerating,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023},
  url={https://arxiv.org/pdf/2302.01318.pdf}
}

@article{miao2023specinfer,
  title={{SpecInfer}: Accelerating Generative {LLM} Serving with Speculative Inference and Token Tree Verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Wong, Rae Ying Yee and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
  journal={arXiv preprint arXiv:2305.09781},
  year={2023},
  url={https://arxiv.org/pdf/2305.09781}
}

@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale {Transformers}},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf}
}

@article{frantar2023massive,
  title={Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2301.00774},
  year={2023},
  url={https://arxiv.org/pdf/2301.00774.pdf}
}

@inproceedings{zheng2023pit,
  title={{PIT}: Optimization of dynamic sparse deep learning models via permutation invariant transformation},
  author={Zheng, Ningxin and Jiang, Huiqiang and Zhang, Quanlu and Han, Zhenhua and Ma, Lingxiao and Yang, Yuqing and Yang, Fan and Zhang, Chengruidong and Qiu, Lili and Yang, Mao and others},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={331--347},
  year={2023},
  url={https://dl.acm.org/doi/abs/10.1145/3600006.3613139}
}

@article{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016},
  url={https://proceedings.neurips.cc/paper_files/paper/2016/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf}
}

@inproceedings{narang2016exploring,
  title={Exploring Sparsity in Recurrent Neural Networks},
  author={Narang, Sharan and Diamos, Greg and Sengupta, Shubho and Elsen, Erich},
  booktitle={International Conference on Learning Representations},
  year={2016},
  url={https://openreview.net/pdf?id=BylSPv9gx}
}

@article{changpinyo2017power,
  title={The power of sparsity in convolutional neural networks},
  author={Changpinyo, Soravit and Sandler, Mark and Zhmoginov, Andrey},
  journal={arXiv preprint arXiv:1702.06257},
  year={2017},
  url={https://arxiv.org/abs/1702.06257}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: {Transformers} for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/pdf?id=YicbFdNTTy}
}

@article{alizadeh2023llm,
  title={{LLM} in a flash: Efficient large language model inference with limited memory},
  author={Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2312.11514},
  year={2023},
  url={https://arxiv.org/pdf/2312.11514.pdf}
}

@inproceedings{kurtz2020inducing,
  title={Inducing and exploiting activation sparsity for fast inference on deep neural networks},
  author={Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Shavit, Nir and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={5533--5543},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf}
}

@article{song2021training,
  title={Training multi-layer over-parametrized neural network in subquadratic time},
  author={Song, Zhao and Zhang, Lichen and Zhang, Ruizhe},
  journal={arXiv preprint arXiv:2112.07628},
  year={2021},
  url={https://arxiv.org/pdf/2112.07628.pdf}
}

@inproceedings{dai2022knowledge,
  title={Knowledge Neurons in Pretrained {Transformers}},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8493--8502},
  year={2022},
  url={https://aclanthology.org/2022.acl-long.581.pdf}
}

@inproceedings{cuadros2022self,
  title={Self-conditioning pre-trained language models},
  author={Cuadros, Xavier Suau and Zappella, Luca and Apostoloff, Nicholas},
  booktitle={International Conference on Machine Learning},
  pages={4455--4473},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v162/cuadros22a/cuadros22a.pdf}
}

@article{sajjad2022neuron,
  title={Neuron-level interpretation of deep {NLP} models: {A survey}},
  author={Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={1285--1303},
  year={2022},
  publisher={MIT Press},
  url={https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00519/2060745/tacl_a_00519.pdf}
}

@inproceedings{zhang2022moefication,
  title={{MoEfication}: {Transformer} Feed-forward Layers are Mixtures of Experts},
  author={Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={877--890},
  year={2022},
  url={https://aclanthology.org/2022.findings-acl.71.pdf}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional {Transformers} for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018},
  url={https://arxiv.org/pdf/1810.04805.pdf}
}

@article{han2016eie,
  title={{EIE}: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/abs/10.1145/3007787.3001163}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the {Lasso}},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Oxford University Press},
  url={https://watermark.silverchair.com/jrsssb_58_1_267.pdf}
}

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer},
  url={https://link.springer.com/content/pdf/10.1007/978-0-387-21606-5.pdf}
}

@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017},
  url={https://arxiv.org/pdf/1710.09282.pdf}
}

@article{yuan2006model,
  title={Model selection and estimation in regression with grouped variables},
  author={Yuan, Ming and Lin, Yi},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={68},
  number={1},
  pages={49--67},
  year={2006},
  publisher={Oxford University Press},
  url={https://academic.oup.com/jrsssb/article-pdf/68/1/49/49794691/jrsssb_68_1_49.pdf}
}

@article{zhao2016loss,
  title={Loss functions for image restoration with neural networks},
  author={Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
  journal={IEEE Transactions on computational imaging},
  volume={3},
  number={1},
  pages={47--57},
  year={2016},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/7797130}
}

@article{scardapane2017group,
  title={Group sparse regularization for deep neural networks},
  author={Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
  journal={Neurocomputing},
  volume={241},
  pages={81--89},
  year={2017},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S0925231217302990}
}

@article{zhu2021vision,
  title={Vision {Transformer} pruning},
  author={Zhu, Mingjian and Tang, Yehui and Han, Kai},
  journal={arXiv preprint arXiv:2104.08500},
  year={2021},
  url={https://arxiv.org/pdf/2104.08500.pdf}
}

@article{prasetyo2023sparse,
  title={Sparse then Prune: Toward Efficient Vision {Transformers}},
  author={Prasetyo, Yogi and Yudistira, Novanto and Widodo, Agus Wahyu},
  journal={arXiv preprint arXiv:2307.11988},
  year={2023},
  url={https://arxiv.org/pdf/2307.11988.pdf}
}

@inproceedings{georgiadis2019accelerating,
  title={Accelerating convolutional neural networks via activation map compression},
  author={Georgiadis, Georgios},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7085--7095},
  year={2019},
  url={https://openaccess.thecvf.com/content_CVPR_2019/papers/Georgiadis_Accelerating_Convolutional_Neural_Networks_via_Activation_Map_Compression_CVPR_2019_paper.pdf}
}

@inproceedings{wang2019structured,
  title={Structured pruning for efficient {ConvNets} via incremental regularization},
  author={Wang, Huan and Zhang, Qiming and Wang, Yuehai and Yu, Lu and Hu, Haoji},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE},
  url={https://arxiv.org/pdf/1804.09461.pdf}
}

@article{li2023starcoder,
  title={{StarCoder}: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023},
  url={https://arxiv.org/pdf/2305.06161.pdf}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023},
  url={https://arxiv.org/pdf/2305.14233.pdf}
}

@inproceedings{sanh2021multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/pdf?id=9Vrb9D0WI4}
}

@misc{wikidump,
    author = {{Wikimedia Foundation}},
    title  = {Wikimedia Downloads},
    year   = {2022},
    url    = {https://dumps.wikimedia.org}
}

@article{gao2020pile,
  title={The {P}ile: An 800{GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020},
  url={https://arxiv.org/pdf/2101.00027.pdf}
}

@article{kim2022soda,
    title={{SODA}: Million-scale Dialogue Distillation with Social Commonsense Contextualization},
    author={Hyunwoo Kim and Jack Hessel and Liwei Jiang and Peter West and Ximing Lu and Youngjae Yu and Pei Zhou and Ronan Le Bras and Malihe Alikhani and Gunhee Kim and Maarten Sap and Yejin Choi},
    journal={ArXiv},
    year={2022},
    volume={abs/2212.10465},
    url={https://arxiv.org/pdf/2212.10465.pdf}
}

@article{lewis2021paq,
  title={{PAQ}: 65 Million Probably-Asked Questions and What You Can Do With Them},
  author={Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini, Pasquale and K{\"u}ttler, Heinrich and Piktus, Aleksandra and Stenetorp, Pontus and Riedel, Sebastian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1098--1115},
  year={2021},
  url={https://aclanthology.org/2021.tacl-1.65.pdf}
}

@article{honovich2022unnatural,
  title={Unnatural instructions: Tuning language models with (almost) no human labor},
  author={Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  journal={arXiv preprint arXiv:2212.09689},
  year={2022},
  url={https://arxiv.org/pdf/2212.09689.pdf}
}

@inproceedings{longpre2023flan,
    author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
    title={The {Flan} collection: designing data and methods for effective instruction tuning},
    year={2023},
    publisher={JMLR.org},
    booktitle={Proceedings of the 40th International Conference on Machine Learning},
    articleno={941},
    numpages={18},
    url={https://openreview.net/pdf?id=ZX4uS605XV}
}

@inproceedings{wang2022super,
  title={{Super-NaturalInstructions}: Generalization via Declarative Instructions on 1600+ {NLP} Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022},
  url={https://aclanthology.org/2022.emnlp-main.340.pdf}
}

@article{zhang2024relu,
  title={{ReLU}$^2$ Wins: Discovering Efficient Activation Functions for Sparse {LLMs}},
  author={Zhang, Zhengyan and Song, Yixin and Yu, Guanghui and Han, Xu and Lin, Yankai and Xiao, Chaojun and Song, Chenyang and Liu, Zhiyuan and Mi, Zeyu and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.03804},
  year={2024},
  url={https://arxiv.org/pdf/2402.03804.pdf}
}

@article{shen2023study,
  title={A Study on {ReLU} and {Softmax} in {Transformer}},
  author={Shen, Kai and Guo, Junliang and Tan, Xu and Tang, Siliang and Wang, Rui and Bian, Jiang},
  journal={arXiv preprint arXiv:2302.06461},
  year={2023},
  url={https://arxiv.org/pdf/2302.06461.pdf}
}

@article{wortsman2023replacing,
  title={Replacing softmax with {ReLU} in vision {Transformers}},
  author={Wortsman, Mitchell and Lee, Jaehoon and Gilmer, Justin and Kornblith, Simon},
  journal={arXiv preprint arXiv:2309.08586},
  year={2023},
  url={https://arxiv.org/pdf/2309.08586.pdf}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021},
  url={https://arxiv.org/pdf/2108.07258.pdf}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023},
  url={https://arxiv.org/pdf/2303.18223.pdf}
}

@article{song2024prosparse,
  title={{ProSparse}: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models},
  author={Song, Chenyang and Han, Xu and Zhang, Zhengyan and Hu, Shengding and Shi, Xiyu and Li, Kuai and Chen, Chen and Liu, Zhiyuan and Li, Guangli and Yang, Tao and Sun, Maosong},
  year={2024},
  journal={arXiv preprint arXiv:2402.13516},
  url={https://arxiv.org/pdf/2402.13516.pdf}
}

@article{hu2024minicpm,
  title={{MiniCPM}: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@article{he2024ultraeval,
  title={{UltraEval}: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs},
  author={He, Chaoqun and Luo, Renjie and Hu, Shengding and Zhao, Yuanqian and Zhou, Jie and Wu, Hanghao and Zhang, Jiajie and Han, Xu and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2404.07584},
  year={2024}
}

@article{ji2024featurebased,
    title={Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization}, 
    author={Yixin Ji and Yang Xiang and Juntao Li and Wei Chen and Zhongyi Liu and Kehai Chen and Min Zhang},
    year={2024},
    journal={arXiv preprint arXiv:2405.10616},
    url={https://arxiv.org/pdf/2405.10616}
}

@article{xue2024powerinfer,
  title={{PowerInfer-2}: Fast Large Language Model Inference on a Smartphone},
  author={Xue, Zhenliang and Song, Yixin and Mi, Zeyu and Chen, Le and Xia, Yubin and Chen, Haibo},
  journal={arXiv preprint arXiv:2406.06282},
  year={2024},
  url={https://arxiv.org/pdf/2406.06282}
}

@inproceedings{zhang2023emergent,
  title={Emergent Modularity in Pre-trained {Transformers}},
  author={Zhang, Zhengyan and Zeng, Zhiyuan and Lin, Yankai and Xiao, Chaojun and Wang, Xiaozhi and Han, Xu and Liu, Zhiyuan and Xie, Ruobing and Sun, Maosong and Zhou, Jie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4066--4083},
  year={2023},
  url={https://aclanthology.org/2023.findings-acl.250.pdf}
}

@inproceedings{zhang2024exploring,
  title={Exploring the Benefit of Activation Sparsity in Pre-training},
  author={Zhang, Zhengyan and Xiao, Chaojun and Qin, Qiujieli and Lin, Yankai and Zeng, Zhiyuan and Han, Xu and Liu, Zhiyuan and Xie, Ruobing and Sun, Maosong and Zhou, Jie},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/pdf?id=KfXXPCcobh},
}

@article{singh2024rethinking,
  title={Rethinking interpretability in the era of large language models},
  author={Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.01761},
  year={2024},
  url={https://arxiv.org/pdf/2402.01761},
}

@article{wan2023efficient,
  title={Efficient Large Language Models: A Survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others},
  journal={Transactions on Machine Learning Research},
  year={2023},
  url={https://openreview.net/pdf?id=bsCCJHbO8A}
}

@article{fedus2022switch,
  title={Switch {Transformers}: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022},
  url={https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf}
}

@article{zoph2022st,
  title={{ST-MoE}: Designing stable and transferable sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022},
  url={https://arxiv.org/pdf/2202.08906}
}

@article{huang2024harder,
  title={Harder Tasks Need More Experts: Dynamic Routing in {MoE} Models},
  author={Huang, Quzhe and An, Zhenwei and Zhuang, Nan and Tao, Mingxu and Zhang, Chen and Jin, Yang and Xu, Kun and Chen, Liwei and Huang, Songfang and Feng, Yansong},
  journal={arXiv preprint arXiv:2403.07652},
  year={2024},
  url={https://arxiv.org/pdf/2403.07652}
}

@misc{liu2024gringradientinformedmoe,
      title={{GRIN}: {GRadient-INformed MoE}}, 
      author={Liyuan Liu and Young Jin Kim and Shuohang Wang and Chen Liang and Yelong Shen and Hao Cheng and Xiaodong Liu and Masahiro Tanaka and Xiaoxia Wu and Wenxiang Hu and Vishrav Chaudhary and Zeqi Lin and Chenruidong Zhang and Jilong Xue and Hany Awadalla and Jianfeng Gao and Weizhu Chen},
      year={2024},
      eprint={2409.12136},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12136}, 
}

@inproceedings{frantar2023scaling,
  title={Scaling Laws for Sparsely-Connected Foundation Models},
  author={Frantar, Elias and Ruiz, Carlos Riquelme and Houlsby, Neil and Alistarh, Dan and Evci, Utku},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/pdf?id=i9K2ZWkYIP}
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024},
  url={https://arxiv.org/pdf/2406.04093}
}

@article{krajewski2024scaling,
  title={Scaling laws for fine-grained mixture of experts},
  author={Krajewski, Jakub and Ludziejewski, Jan and Adamczewski, Kamil and Pi{\'o}ro, Maciej and Krutul, Micha{\l} and Antoniak, Szymon and Ciebiera, Kamil and Kr{\'o}l, Krystian and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Sankowski, Piotr and others},
  journal={arXiv preprint arXiv:2402.07871},
  year={2024},
  url={https://arxiv.org/pdf/2402.07871}
}

@article{song2024turbo,
  title={Turbo {Sparse}: Achieving {LLM} {SOTA} Performance with Minimal Activated Parameters},
  author={Song, Yixin and Xie, Haotong and Zhang, Zhengyan and Wen, Bo and Ma, Li and Mi, Zeyu and Chen, Haibo},
  journal={arXiv preprint arXiv:2406.05955},
  year={2024},
  url={https://arxiv.org/pdf/2406.05955}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text {Transformer}},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  url={https://www.jmlr.org/papers/volume21/20-074/20-074.pdf}
}

@article{yang2022tensor,
  title={Tensor programs {V}: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022},
  url={https://arxiv.org/pdf/2203.03466}
}

@article{soldaini2024dolma,
  title={Dolma: An open corpus of three trillion tokens for language model pretraining research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  journal={arXiv preprint arXiv:2402.00159},
  year={2024},
  url={https://arxiv.org/pdf/2402.00159}
}

@article{kocetkov2022stack,
  title={The {Stack}: 3 {TB} of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Jia, LI and Mou, Chenghao and Jernite, Yacine and Mitchell, Margaret and Ferrandis, Carlos Mu{\~n}oz and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and others},
  journal={Transactions on Machine Learning Research},
  year={2022},
  url={https://openreview.net/pdf?id=pxpbTdUEpD}
}

@article{colombo2024saullm,
  title={{SaulLM-7B}: A pioneering large language model for law},
  author={Colombo, Pierre and Pires, Telmo Pessoa and Boudiaf, Malik and Culver, Dominic and Melo, Rui and Corro, Caio and Martins, Andre FT and Esposito, Fabrizio and Raposo, Vera L{\'u}cia and Morgado, Sofia and others},
  journal={arXiv preprint arXiv:2403.03883},
  year={2024},
  url={https://arxiv.org/pdf/2403.03883}
}

@inproceedings{wei2024magicoder,
  title={Magicoder: Empowering code generation with {OSS-Instruct}},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://arxiv.org/pdf/2312.02120}
}

@article{xu2023wizardlm,
  title={{WizardLM}: Empowering Large Language Models to Follow Complex Instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023},
  url={https://arxiv.org/pdf/2304.12244}
}

@article{he2024mixture,
  title={Mixture of A Million Experts},
  author={He, Xu Owen},
  journal={arXiv preprint arXiv:2407.04153},
  year={2024},
  url={https://arxiv.org/pdf/2407.04153}
}

@inproceedings{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  booktitle={Proceedings of the 36th International Conference on Neural Information Processing Systems},
  pages={30016--30030},
  year={2022},
  url={https://papers.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf}
}

@article{besiroglu2024chinchilla,
  title={Chinchilla Scaling: A replication attempt},
  author={Besiroglu, Tamay and Erdil, Ege and Barnett, Matthew and You, Josh},
  journal={arXiv preprint arXiv:2404.10102},
  year={2024},
  url={https://arxiv.org/pdf/2404.10102}
}

@article{marquardt1963algorithm,
  title={An algorithm for least-squares estimation of nonlinear parameters},
  author={Marquardt, Donald W},
  journal={Journal of the society for Industrial and Applied Mathematics},
  volume={11},
  number={2},
  pages={431--441},
  year={1963},
  publisher={SIAM}
}
