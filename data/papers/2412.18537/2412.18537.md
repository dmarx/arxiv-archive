---
abstract: |
  Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an <u>A</u>daptive <u>M</u>ulti-<u>A</u>spect <u>R</u>etrieval-augmented over KGs (<span class="smallcaps">Amar</span>) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The <span class="smallcaps">Amar</span> framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMsâ€™ output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9% improvement in accuracy over its best competitor and a 6.6% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of <span class="smallcaps">Amar</span> in improving the reasoning of LLMs.
author:
- Derong Xu<sup>12</sup> Xinhang Li<sup>1</sup>, Ziheng Zhang<sup>3</sup>, Zhenxi Lin<sup>3</sup>, Zhihong Zhu<sup>4</sup>, Zhi Zheng<sup>1</sup>, Xian Wu<sup>3</sup>[^1], Xiangyu Zhao<sup>2</sup>, Tong Xu<sup>1</sup>, Enhong Chen<sup>1</sup>
bibliography:
- aaai25.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation
---




[^1]: Corresponding authors.
