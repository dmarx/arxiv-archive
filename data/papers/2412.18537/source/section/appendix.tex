
\newpage
\section{Appendix}

\subsection{Comparison in Oracle Entities setting}


In this section, we present the results of our experiments conducted under the Oracle Entity Linking Annotations setting. This setting assumes that the model has prior knowledge of the correct entity mentioned in the question. The outcomes of this experiment can be seen in Table \ref{tab:orcale setting result}.
The results of our experiment indicate that our method achieves state-of-the-art (SOTA) improvements. Interestingly, our approach performs exceptionally well even when the Oracle setting is not utilized. In fact, our method outperforms several baseline models that rely on the Oracle setting.
For example, when evaluating the WebQSP F1 metric, RoG obtains a score of 70.8. However, our method surpasses this score significantly, achieving a score of 81.2. This remarkable achievement demonstrates the effectiveness of our approach.
\begin{table*}[t]
% \setlength\tabcolsep{3pt}  %可以控制列间距
% \renewcommand{\arraystretch}{1.1} %可以控制行间距
% \footnotesize
\centering
% \fontsize{9}{11}\selectfont
\setlength{\tabcolsep}{2.6mm}{
\begin{tabular}{l|cccccc}
\toprule
  \multicolumn{1}{l|}{\multirow{2}{*} {Model}}   & \multicolumn{3}{c}{WebQSP}   & \multicolumn{3}{c}{CWQ} \\
\cline{2-7}
 \multicolumn{1}{c|}{}   &F1&Hits@1& Acc &F1&Hits@1& Acc\\ 
\hline
\hline
TIARA* \cite{shu-etal-2022-tiara} & 78.9& 75.2& - &- &- &- \\
EmbedKGQA* & - & 66.6& - & - & 45.9& -  \\
ProgramTransfer* \cite{ProgramTransfer}& 76.5& 74.6& - &58.7& 58.1 &- \\
ChatKBQA* \cite{chatkbqa}& 83.5 &86.4 &77.8 &81.3 &86.0 &76.8\\
RoG* \cite{RoG}& 70.8 &85.7& -  &56.2& 62.6& - \\
FiDeLiS-GPT4* \cite{sui2024fidelis}&  78.32&  84.39&  -&  64.32&  71.47&  - \\
\rowcolor{gray!10}  \model (Ours) & 81.2\scriptsize{$\pm$0.15} & 84.3\scriptsize{$\pm$0.16}  & 75.2\scriptsize{$\pm$0.10} & 78.5\scriptsize{$\pm$0.11} & 83.1\scriptsize{$\pm$0.09} & 74.5\scriptsize{$\pm$0.07} \\
\rowcolor{gray!10}  \model* (Ours) & \textbf{84.1}\scriptsize{$\pm$0.13} & \textbf{87.0}\scriptsize{$\pm$0.11} & \textbf{78.4}\scriptsize{$\pm$0.09}  & \textbf{82.0}\scriptsize{$\pm$0.10}& \textbf{86.4}\scriptsize{$\pm$0.11} &  \textbf{78.2}\scriptsize{$\pm$0.09} \\
\bottomrule
\end{tabular}}
\caption{Performance comparison of KGQA methods in oracle entity linking annotations setting. \textbf{*} means methods with oracle entity.}
\label{tab:orcale setting result}
\end{table*}

\subsection{Logical Form}
In this study, we have chosen the S-expression $\mathcal{F}$ as our logical expression, which has been previously used in \cite{chatkbqa,decaf}. The S-expression, as exemplified by `\textit{(AND (JOIN base.biblioness.bibs\_location.loc\_type "Country") (JOIN (R location.location.contains) Oceania))}', utilizes functions such as \textit{JOIN} and \textit{AND} to operate on set-based semantics. This approach strikes a balance between readability and compactness, making it suitable for Knowledge Graph Question Answering (KGQA) \cite{gu2021beyond}.
The \textit{JOIN} operation is used to query a triple (h, r, t) on either h or t. For instance, (?, r, t) is denoted as \textit{(JOIN r t)}, and (h, r, ?) is denoted as \textit{(JOIN (R r) h)}.
E1 or E2 denote a sublayer logical form. Various operators include:
\begin{itemize}
    \item \textit{`AND' (AND E1 E2)}: represents the intersection of E1 and E2.
    \item \textit{`COUNT' (COUNT E1)}: denotes the count of E1.
    \item \textit{`ARGMAX' (ARGMAX E1 r)}: represents the maximum literal obtained after projecting E1 onto the r relation.
    \item \textit{`ARGMIN' (ARGMIN E1 r)}: denotes the minimum literal obtained after projecting E1 onto the r relation.
    \item \textit{`GT' (GT E1 l)}: represents the portion of E1 that is greater than l.
    \item \textit{`GE' (GE E1 l)}: denotes the part of E1 that is greater than or equal to l.
    \item \textit{`LT' (LT E1 l)}: represents the part of E1 that is less than l.
    \item \textit{`LE' (LE E1 l)}: denotes the part of E1 that is less than or equal to l.
\end{itemize}


\subsection{Retrieval of Multi-Aspect Knowledge}
\textbf{Entity Retrieval.} 
One effective approach for retrieving candidate entities $k_e$ is to conduct entity linking with question $q$. Following \citet{GMT-KBQA}, we employ the ELQ \cite{ELQ} for question entity linking. ELQ is an efficient and precise entity linking system designed specifically for questions. The system aim to identify the boundaries of entity mentions within a question and link them to their corresponding Wikipedia entities. The system uses a bi-encoder based on BERT \cite{devlin2018bert} to achieve this. The entity encoder calculates entity embeddings for all entities in Wikipedia \cite{wikidata} using their short descriptions. Simultaneously, the question encoder generates token-level embeddings for the input question. These embeddings are then used to detect mention boundaries and disambiguate each entity mention. This is done by calculating an inner product between the mention embeddings (which are an average of the mention tokens) and the entity embeddings.
Then FACC1 \cite{facc1} (a comprehensive Freebase annotation of corpora) is employed to identify entities that were not linked by ELQ, to enhance the range of candidate entities. The entity recall score on WebQSP and CWQ datasets are shown in Table \ref{tab:Recall entity}.


\textbf{Relation Retrieval.}
As we have stated in main body, in large-scale KG (e.g. Freebase), relations are typically organized hierarchically. Therefore, directly using question-based dense retrieval for similarity may not be effective. To address this, we propose masking entity mentions detected during the candidate entity retrieval stage with a [BLANK] token for each question $q$. For example, `\textit{(AND (JOIN base.biblioness.bibs\_location.loc\_type Country) (JOIN (R location.location.contains) Oceania))}' is replaced by `\textit{(AND (JOIN base.biblioness.bibs\_location.loc\_type [BLANK]) (JOIN (R location.location.contains) [BLANK]))}'. Following \citet{GMT-KBQA,CBR-KBQA}, we train two separate BERT \cite{devlin2018bert} models that encode questions $q$ and relations $r$ into a shared dense space. And we calculate the similarity score by dot-product:
 \begin{equation}
\begin{aligned}
    &\boldsymbol{v}_{q} = BERT(q), \\
   &\boldsymbol{v}_{r} = BERT(r), \\
    &\boldsymbol{s}(q,r) = \boldsymbol{v}_{q} \cdot \boldsymbol{v}_{r}. \\
\end{aligned}
\end{equation}
To construct a training batch, we randomly select negative relations that are not part of the logical form of a given question. The objective of optimization is to maximize the score of the relevant relation compared to the randomly sampled relations. 
 To retrieve the nearest relations, we employ FAISS \cite{douze2024faiss}, a highly efficient vector database, which allows us to speed up the search process and obtain the most relevant results. 
Next, we proceed to train a ranker that assigns scores to question and relation pairs. To achieve this, we utilize a cross-encoder, which is a single BERT model. The input to cross-encoder is a combination of question and candidate relation. By employing a linear layer, we project the representation of combined input ([$q$; $r$]) to a binary probability distribution. This allows us to calculate score between $q$ and $r$. During training, we employ cross-entropy loss to optimize this process. Finally, we retain the top-k candidate relations based on their rankings.
The relation recall score on WebQSP and CWQ datasets are shown in Table \ref{tab:Recall relation}.

\textbf{Subgraph Retrieval.}
To better utilize structural and semantic information contained within KG, we linearize triplets by combining the head entity, relation, and tail entity with spaces for retrieval. Drawing inspiration from \cite{decaf}, we propose grouping linearized sentences with the same head entity into a document. To save computing resources, we only focus on 1-hop subgraphs to capture structural information. For example, the triple \textit{(Oceania, location.location.contains, Australia)} can be linearized to `\textit{Oceania location location contains Australia}'.  All linearized texts from triplet connected to the same entity will be concatenated together. Each document is truncated with a maximum of 100 words.
Furthermore, concerning the potential information loss when converting long documents into vectors, we employ sparse retrieval approaches that rely on keyword dependencies. Specifically, we employ techniques like BM25 \cite{robertson2009probabilistic}, The BM25 algorithm is widely employed for scoring search relevance. In essence, it involves analyzing the query to generate morphemes $q_i$ through morphological analysis. For each search result $D$, the algorithm computes the relevance score between each morpheme $q_i$ and $D$. These relevance scores are then weighted and combined to determine the overall relevance score between the query and $D$.
 We follow BM25 to calculates TF-IDF scores based on sparse word matches between input questions and KB-linearized passages, and obtain the fine rerieved subgraphs.
The subgraph recall score on WebQSP and CWQ datasets are shown in Table \ref{tab:Recall subgraph}.


\begin{table}[t]
% \setlength\tabcolsep{3pt}  %可以控制列间距
% \renewcommand{\arraystretch}{0.8} %可以控制行间距
\centering
\begin{tabular}{lcc}
\toprule
Retrieved Entities & WebQSP  & CWQ  \\
\midrule 
Top 1 &67.7 &48.4 \\
Top 2 &76.1 &72.0 \\
Top 3 &77.8 &76.9 \\
Top 4 &78.8 &78.2 \\
Top 5 &79.4 &78.9 \\
Top 6 &80.0 &79.3 \\
Top 7 &80.1 &79.6 \\
Top 8 &80.3 &79.9 \\
Top 9 &80.3 &80.0 \\
Top 10 &80.6 &80.2 \\
\bottomrule 
\end{tabular}
\caption{Recall score (\%) on WebQSP and CWQ datasets. This metric measures the recall of groundtruth entities in the retrieved entities information.
}
\label{tab:Recall entity}
\end{table}

\begin{table}[t]
% \setlength\tabcolsep{3pt}  %可以控制列间距
% \renewcommand{\arraystretch}{0.8} %可以控制行间距
\centering
\begin{tabular}{lcc}
\toprule
Retrieved Relations & WebQSP  & CWQ  \\
\midrule 
Top 1& 41.4& 29.1 \\
Top 2& 63.1& 54.1 \\
Top 3& 75.7& 71.0\\
Top 4& 82.3& 80.2\\
Top 5& 86.0& 85.4\\
Top 6& 88.1& 87.9\\
Top 7& 89.3& 89.5\\
Top 8& 90.3& 90.6\\
Top 9& 91.2& 91.4\\
Top 10& 92.0& 92.0\\
\bottomrule 
\end{tabular}
\caption{Recall score (\%) on WebQSP and CWQ datasets. This metric measures the recall of groundtruth relations in the retrieved relations information.
}
\label{tab:Recall relation}
\end{table}

\begin{table}[t]
% \setlength\tabcolsep{3pt}  %可以控制列间距
% \renewcommand{\arraystretch}{0.8} %可以控制行间距
\centering
\begin{tabular}{lcc}
\toprule
Retrieved Subgraphs & WebQSP  & CWQ  \\
\midrule 
Top 5 &30.7 &27.9  \\
Top 10 &39.8 &34.8  \\
Top 20 &48.5 &41.0  \\
Top 100 &68.4 &57.4  \\
\bottomrule 
\end{tabular}
\caption{Recall score (\%) on WebQSP and CWQ datasets. This metric measures the recall answers in the retrieved subgraph information.
}
\label{tab:Recall subgraph}
\end{table}


\begin{table*}[htbp]
% \setlength\tabcolsep{3pt}  %可以控制列间距
% \renewcommand{\arraystretch}{0.8} %可以控制行间距
\centering
% \footnotesize
\begin{tabular}{lrrrrrccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*} {Dataset}}  & \multicolumn{1}{l}{\multirow{2}{*} {\#Question }} & \multicolumn{1}{l}{\multirow{2}{*} {\#Skeleton}} & \multicolumn{1}{l}{\multirow{2}{*} {\#Train}} & \multicolumn{1}{l}{\multirow{2}{*} {\#Valid}} & \multicolumn{1}{l}{\multirow{2}{*} {\#Test }} & \multicolumn{3}{c}{\#Average Token Length}  \\
 \multicolumn{6}{c}{} & $k_e$ & $k_r$ & $k_s$ \\
\midrule 
WebQSP  & 4,737 &34 &3,098 &- &1,639 & 11.4 & 4.2 & 145.9 \\
CWQ &34,689 &174 &27,639 &3,519 &3,531& 11.0 & 4.1 &147.4 \\
\bottomrule 
\end{tabular}
\caption{Dataset statistics.}
\label{tab:all Statistics}
\end{table*}

\subsection{Query Execution}
After generating S-expressions using LLMs, we need to refine them further. Let's take the S-expression \textit{(AND (JOIN base.biblioness.bibs\_location.loc\_type Country) (JOIN (R location.location.contains) Oceania))} as an example. In this case, our refined targets consist of entities such as \textit{Oceania} and \textit{Country}, as well as relations like \textit{base.biblioness.bibs\_location.loc\_type} and \textit{location.location.contains}. It's important to note that the logical structure of the S-expression remains unchanged.

To refine the S-expressions, we follow the approach proposed by \citet{chatkbqa}. Firstly, we extract the entire set of entities from the KG. Then, we employ unsupervised technique SimCSE \cite{gao2021simcse} to calculate similarity scores between the extracted entities. By applying a certain threshold, we obtain a subset of entities that have a similarity score above the threshold.
Regarding relations, we extract all connected relation neighbors from the subset of entities. Also, we utilize SimCSE to calculate similarity scores, enabling us to identify the candidate relation with the highest similarity score. All settings of executing query follow \citet{chatkbqa}.

After refining the S-expression, we directly convert it into a SPARQL expression. For example, the SPARQL expression for the above example is as follows:
\lstset{language=C}
\begin{lstlisting}
"PREFIX ns: http://rdf.freebase.com/ns/
SELECT DISTINCT ?x
WHERE 
{
FILTER (?x != ns:m.05nrg)
FILTER (!isLiteral(?x) OR lang(?x) = '' OR langMatches(lang(?x), 'en'))
ns:m.05nrg ns:location.location.contains ?x .
?x ns:base.biblioness.bibs_location.loc_type ?sk0 .
FILTER (str(?sk0) = "Country")
}"
\end{lstlisting}
We then use SPARQL expression to query KG, obtaining final results. This process of refining and querying allows us to ensure accuracy and relevance of information retrieved from KG, thereby enhancing the effectiveness of our models.


\subsection{Experiment Settings}
\subsubsection{Datasets}
In our work, we utilized two well-known and commonly used datasets: the WebQuestions Semantic Parses Dataset (WebQSP) \cite{webqsp} and ComplexWebQuestions (CWQ) \cite{CWQ}. The statistics for these datasets are presented in table \ref{tab:all Statistics}. $k_e$, $k_r$ and $k_s$ denote the average token length of entities, relations and subgraphs in the retrieval knowledge.  Both of these datasets are based on Freebase \cite{bollacker2008freebase} as the KG database.

The WebQSP dataset contains full semantic parses in SPARQL queries for 4,737 questions, and partial annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer.

The CWQ dataset is a dataset designed for answering complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language.


\subsubsection{Baselines}

In this study, we compare the performance of \model against 22 baselines, including 4 embedding-based (EM-based) KGQA methods, 5 information retrieval-based (IR-based) KGQA methods, 6 semantic parsing-based (SP-based) KGQA methods, and 7 LLM-based KGQA methods.
In the comparative experiments in the Oracle entities setting, we compare \model against another 6 baselines.
The details of each baseline are given as follows.

\textbf{EM-based KGQA methods}.
KV-Mem~\cite{miller2016key} firstly stores the facts in a key-value structured memory, utilizes different encodings on the reading operation, and reasons on these facts to obtain the answer.
NSM$_{+h}$~\cite{NSM} proposes a teacher-student framework for multi-hop KGQA, in which the student network aims to find the answers using a neural state machine (NSM) and the teacher network tries to learn intermediate supervision signals for enhance the student work.
TransferNet~\cite{shi2021transfernet} begins with the topic entity (within the question) and obtains the answer by transferring entity scores along relation scores of multiple steps.
KGT5~\cite{KGT5} unifies knowledge graph completion and KGQA as the seq-to-seq tasks and performs question answering after fine-tuning using QA pairs.

\textbf{IR-based KGQA methods}.
GraftNet~\cite{sun-etal-2018-open} adopts a graph convolutional network (GCN) to operate on both the KG facts and text sentences and extract answers from a question-specific subgraph.
PullNet~\cite{sun-etal-2019-pullnet} involves training a graph convolutional network (GCN) to improve the retrieval process and multi-hop question answering.
SR+NSM~\cite{zhang2022subgraph} proposes a trainable subgraph retriever to improve the retrieval module, and SR+NSM+E2E~\cite{zhang2022subgraph} trains both the retrieval and reasoning modules in an end-to-end manner based on SR+NSM.
UniKGQA~\cite{unikgqa} proposes to unify the retrieval and reasoning in both model architecture and parameter learning for multi-hop KGQA.
EmbedKGQA~\cite{EmbedKGQA} leverages KG embeddings to perform missing link prediction, thus reducing KG sparsity and improving multi-hop question-answering.

\textbf{SP-based KGQA methods}.
CBR-KBQA~\cite{CBR-KBQA} proposes a neuro-symbolic case-based reasoning (CBR) approach and reuses existing cases to improve reasoning on unseen cases.
GMT-KBQA~\cite{GMT-KBQA} improves logical form generation with multi-task learning and better utilization of auxiliary information.
UnifiedSKG~\cite{xie-etal-2022-unifiedskg} unifies a total of 21 structured knowledge grounding tasks into a text-to-text format and improves the performance via multi-task prefix tuning on T5.
RnG-KBQA~\cite{rng-kbqa} enumerates all the candidate logical forms from KG, ranks these candidates using the contrastive ranker, and then obtains the target logical form with the tailored generator.
DecAF~\cite{decaf} proposes to jointly generate both logical forms and direct answers and then combine them to obtain the final answer.
FC-KBQA~\cite{fc-kbqa} introduces a fine-to-coarse composition framework to improve both the generalization and execution abilities of the generated logical forms.
TIARA~\cite{shu-etal-2022-tiara} applies multi-grained retrieval to help language models concentrate on relevant KG contexts, including entities, schemas, and logical forms.
ProgramTransfer~\cite{ProgramTransfer} proposes a two-stage parsing framework and an ontology-guided pruning strategy to program transfer, leveraging program annotations as supervision signals to assist program induction.

\textbf{LLM-based KGQA methods}.
KD-CoT~\cite{wang2023knowledge} introduces formulating chain-of-thought (CoT) into a multi-round QA format and during the interaction, LLMs can retrieve external knowledge for faithful reasoning.
Pangu~\cite{pangu} proposes to leverage LLMs' discriminative abilities rather than generative abilities for performance improvement on complex KGQA.
StructGPT~\cite{structgpt} is an iterative reading-then-reasoning framework to improve LLMs' reasoning when handling structured data, such as KG.
ChatKBQA~\cite{chatkbqa} follows the generate-then-retrieve KGQA framework, which firstly probes LLMs to generate the logical forms and refines the entities and relations within the logical forms.
ToG-R~\cite{TOG} proposes to explore paths and reasoning interactively on KGs using LLM as an agent.
RoG~\cite{RoG} presents a planning-retrieval-reasoning pipeline to generate relation paths, retrieve reasoning paths, and conduct faithful reasoning on these paths.
G-Retriever~\cite{G-retriever} introduces a retrieval-augmented method for general textual graphs and it integrates GNNs, LLMs, and RAG to improve the question-answering abilities via soft prompting of LLMs.
GNN-RAG~\cite{mavromatis2024gnn} focuses on combining the language understanding abilities of LLMs with the reasoning abilities of GNNs in an RAG style.
FiDeLiS~\cite{sui2024fidelis} presents a retrieval-exploration interactive method, which addresses intermediate reasoning steps on KGs and utilizes deductive reasoning capabilities of LLMs to guide the reasoning process in a step-wise and generalizable manner.

\subsubsection{Implementation Details}
Our implementation is based on NVIDIA A6000 GPUs, requiring approximately 48GB of VRAM for training and testing. However, executing querying on KG only requires 3GB of VRAM. Below, we provide the selected hyperparameters for the WebQSP and CWQ datasets. We only searched for the number of retrieved data.

\textbf{For the WebQSP dataset:}
LoRA target weight: gate\_proj down\_proj up\_proj
Learning rate: 5e-5,
LoRA rank: 8,
LoRA alpha: 32,
LoRA dropout: 0.1,
Training epochs: 80,
Training batch size: 4,
Number of retrieved data: \{4, 8, 16, 32, 64, 100\},
Soft prompt length: 7,
Beam search number: 8,
Max new tokens: 256.

\textbf{For the CWQ dataset:}
LoRA target weight: gate\_proj down\_proj up\_proj,
Learning rate: 5e-5,
LoRA rank: 8,
LoRA alpha: 32,
LoRA dropout: 0.1,
Training epochs: 10,
Training batch size: 4,
Number of retrieved data: \{4, 8, 16, 32, 64, 100\},
Soft prompt length: 16,
Beam search number: 15,
Max new tokens: 256.

For more code details, please refer to the source code in the supplementary materials.



\subsubsection{Experiments on training time and space consumption}
These results shown in Table \ref{tab:time}, obtained using an A6000 GPU with LLaMA2-7b on webQSP (batch\_size=4), demonstrate that (1) time and space consumption are within acceptable range; (2) our method significantly reduces both time and space costs compared to directly using text as the context.

\begin{table}[t]
\setlength\tabcolsep{2pt}  %可以控制列间距
% \renewcommand{\arraystretch}{1.1} %可以控制行间距
\centering
\begin{tabular}{lcc}
\toprule
 & Training Time & GPU Memory  \\
\midrule 
Context Prompt&	7.24 min/epoch	&31 GB \\
Ours	&4.87 min/epoch	&18 GB\\
\bottomrule 
\end{tabular}
\caption{Training time and space consumption.
}
\label{tab:time}
\end{table}
