---
abstract: |
  For many machine learning problem settings, particularly with structured inputs such as sequences or sets of objects, a distance measure between inputs can be specified more naturally than a feature representation. However, most standard machine models are designed for inputs with a vector feature representation. In this work, we consider the estimation of a function $f:\mathcal{X} \rightarrow \mathbb{R}$ based solely on a dissimilarity measure $d:\mathcal{X}\times\mathcal{X} \rightarrow \mathbb{R}$ between inputs. In particular, we propose a general framework to derive a family of *positive definite kernels* from a given dissimilarity measure, which subsumes the widely-used *representative-set method* as a special case, and relates to the well-known *distance substitution kernel* in a limiting case. We show that functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are Lipschitz-continuous w.r.t. the given distance metric. We provide a tractable algorithm to estimate a function from this RKHS, and show that it enjoys better generalizability than Nearest-Neighbor estimates. Our approach draws from the literature of Random Features, but instead of deriving feature maps from an existing kernel, we construct novel kernels from a random feature map, that we specify given the distance measure. We conduct classification experiments with such disparate domains as strings, time series, and sets of vectors, where our proposed framework compares favorably to existing distance-based learning methods such as $k$-nearest-neighbors, distance-substitution kernels, pseudo-Euclidean embedding, and the representative-set method.
author:
- |
  Lingfei Wu [^1]  
  IBM Research  
  Yorktown Heights, NY 10598  
  `wuli@us.ibm.com`  
  Ian En-Hsu Yen $^\ast$  
  Carnegie Mellon University  
  Pittsburgh, PA 15213  
  `eyan@cs.cmu.edu`  
  Fangli Xu  
  College of William and Mary  
  Williamsburg, VA 23185  
  `fxu02@email.wm.edu`  
  Pradeep Ravikumar  
  Carnegie Mellon University  
  Pittsburgh, PA 15213  
  `pradeepr@cs.cmu.edu`  
  Michael Witbrock  
  IBM Research  
  Yorktown Heights, NY 10598  
  `witbrock@us.ibm.com`  
bibliography:
- IEEEabrv.bib
- D2KE_NIPS18.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "D2KE: From Distance to Kernel and Embedding"
---





# Introduction

In many problem domains, it is easier to specify a reasonable dissimilarity (or similarity) function between instances, than to construct a feature representation. This is particularly the case with structured inputs, such as sets, sequences, or networks of objects, where it is typically less than clear how to construct the representation of the whole structured input, even when given a good feature representation of each individual object. On the other hand, even for complex structured inputs, there are many well-developed dissimilarity measures, such as the Edit Distance (Levenshtein distance) between sequences, Dynamic Time Warping measure between time series, Hausdorff distance between sets, and Wasserstein distance between distributions.

However, standard machine learning methods are designed for vector representations, and classically there has been far less work on distance-based methods for either classification or regression. The most common distance-based method is Nearest-Neighbor Estimation (NNE), which predicts the outcome for an instance using an average of its nearest neighbors in the input space, with nearness measured by the given dissimilarity measure. Estimation from nearest neighbors, however, is unreliable, specifically having high variance when the neighbors are far apart, which is typically the case when the intrinsic dimension implied by the distance is large.

To address this issue, a line of research has focused on developing global distance-based (or similarity-based) machine learning methods , in large part by drawing upon connections to kernel methods or directly learning with similarity functions ; we refer the reader in particular to the survey in . Among these, the most direct approach treats the data similarity matrix (or a transformed dissimilarity matrix) as a kernel Gram matrix, and then uses standard kernel-based methods such as Support Vector Machines (SVM) or kernel ridge regression with this Gram matrix. A key caveat with this approach however is that most similarity (or dissimilarity) measures do not provide a *positive-definite (PD)* kernel, so that the empirical risk minimization problem is not well-defined, and moreover becomes non-convex .

A line of work has therefore focused on estimating a positive-definite (PD) Gram matrix that merely approximates the similarity matrix. This could be achieved for instance by clipping, or flipping, or shifting eigenvalues of the similarity matrix , or explicitly learning a PD approximation of the similarity matrix . Equivalently, one could also find a Euclidean embedding (also known as dissimilarity representation) approximating the dissimilarity matrix as in Multidimensional Scaling  Such modifications of the similarity matrix however often leads to a loss of information, and moreover, the enforced PD property is typically guaranteed to hold only on the training data, resulting in an inconsistency between the set of test and training samples  provide conditions under which one can obtain a PD kernel through simple transformations of the distance measure, but which are not satisfied for many commonly used dissimilarity measures such as Dynamic Time Warping, Hausdorff distance and Earth Mover’s distance .

Another common approach is to select a subset of training samples as a held-out representative set, and use distances or similarities to points in the set as the feature function . As we show, with proper scaling, this approach can be interpreted as a special instance of our framework. On the other hand, our framework provides a more general and richer family of kernels, many of which significantly outperform the representative-set method in a variety of application domains.

In this paper, we propose a general framework that constructs a family of PD kernels from a dissimilarity measure. The kernel satisfies the property that functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are Lipschitz-continuous w.r.t. the given distance measure. We also provide a tractable estimator for a function from this RKHS which enjoys much better generalization properties than nearest-neighbor estimation . Our approach draws from the literature of Random Features , but instead of deriving feature maps from an existing kernel, we derive novel kernels from a random feature map specifically designed given the distance measure. Our framework produces a feature embedding and consequently a vector representation of each instance that can be employed by any classification and regression models. In classification experiments in disparate domains as strings, time series, and sets of vectors, our proposed framework compares favorably to existing distance-based algorithms such as $k$-nearest-neighbors, distance-substitution kernels, pseudo-Euclidean embedding, and representative-set method.

# Problem Setup

We consider the estimation of a target function $f:\mathcal{X}\rightarrow \mathbb{R}$ from a collection of samples $\{(\boldsymbol{x}_i,y_i)\}_{i=1}^n$, where $\boldsymbol{x}_i\in\mathcal{X}$ is the input object, and $y_i\in\mathcal{Y}$ is the output observation associated with the target function $f(\boldsymbol{x}_i)$. For instance, in a regression problem, $y_i\sim f(\boldsymbol{x}_i)+\omega_i \in \mathbb{R}$ for some random noise $\omega_i$, and in binary classification, we have $y_i\in\{0,1\}$ with $P(y_i=1|\boldsymbol{x}_i)=f(\boldsymbol{x}_i)$. We are given a dissimilarity measure $d:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ between input objects instead of a feature representation of $\boldsymbol{x}$. For some of the analyses, we would require the dissimilarity measure to be a *metric* as follows.

<div class="assumption">

**Assumption 1** (Distance Metric).  * $d:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ is a distance metric, that is, it satisfies (i) $d(\boldsymbol{x}_1,\boldsymbol{x}_2)\geq 0$, (ii) $d(\boldsymbol{x}_1,\boldsymbol{x}_2)=0 \Leftarrow \Rightarrow \boldsymbol{x}_1=\boldsymbol{x}_2$, (iii) $d(\boldsymbol{x}_1,\boldsymbol{x}_2)=d(\boldsymbol{x}_2,\boldsymbol{x}_1)$, and (iv) $d(\boldsymbol{x}_1,\boldsymbol{x}_2)\leq d(\boldsymbol{x}_1,\boldsymbol{x}_3) + d(\boldsymbol{x}_3,\boldsymbol{x}_2)$.*

</div>

## Function Continuity and Space Covering

An ideal feature representation for the learning task is (i) compact and (ii) such that the target function $f(\boldsymbol{x})$ is a simple (e.g. linear) function of the resulting representation. Similarly, an ideal dissimilarity measure $d(\boldsymbol{x}_1,\boldsymbol{x}_2)$ for learning a target function $f(\boldsymbol{x})$ should satisfy certain properties. On the one hand, a small dissimilarity $d(\boldsymbol{x}_1,\boldsymbol{x}_2)$ between two objects should imply small difference in the function values $|f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)|$. On the other hand, we want a small expected distance among samples, so that the data lies in a compact space of small intrinsic dimension. We next build up some definitions to formalize these properties.

<div class="assumption">

**Assumption 2** (Lipschitz Continuity).  * For any $\boldsymbol{x}_1, \boldsymbol{x}_2 \in \mathcal{X}$, there exists some constant $L>0$ such that $$\label{lips}
|f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)| \leq L \, d(\boldsymbol{x}_1,\boldsymbol{x}_2),$$*

</div>

We would prefer the target function to have a small Lipschitz-continuity constant $L$ with respect to the dissimilarity measure $d(.,.)$. Such Lipschitz-continuity alone however might not suffice. For example, one can simply set $d(\boldsymbol{x}_1,\boldsymbol{x}_2)=\infty$ for any $\boldsymbol{x}_1\neq \boldsymbol{x}_2$ to satisfy Eq. . We thus need the following quantity that measures the size of the space implied by a given dissimilarity measure.

<div class="definition">

**Definition 1** (Covering Number).  * Assuming $d$ is a *metric*. A $\delta$-cover of $\mathcal{X}$ w.r.t. $d(.,.)$ is a set $\mathcal{E}$ s.t. $$\forall \boldsymbol{x}\in\mathcal{X}, \exists \boldsymbol{x}_i\in\mathcal{E}, d(\boldsymbol{x},\boldsymbol{x}_i)\leq \delta.$$ Then the covering number $N(\delta; \mathcal{X},d)$ is the size of the smallest $\delta$-cover for $\mathcal{X}$with respect to $d$.*

</div>

Assuming the input domain $\mathcal{X}$ is compact, the covering number $N(\delta; \mathcal{X},d)$ measures its size w.r.t. the distance measure $d$. We show how the two quantities defined above affect the estimation error of a Nearest-Neighbor Estimator.

## Effective Dimension and Nearest Neighbor Estimation

We extend the standard analysis of the estimation error of $k$-nearest-neighbor from finite-dimensional vector spaces to any input space $\mathcal{X}$, with an associated distance measure $d$, and a finite covering number $N(\delta; \mathcal{X},d)$, by defining the *effective dimension* as follows.

<div class="assumption">

**Assumption 3** (Effective Dimension).  * Let the effective dimension $p_{\mathcal{X},d}>0$ be the minimum $p$ satisfying $$\exists c>0, \forall \delta: 0< \delta < 1, \;\;N(\delta;\mathcal{X},d)\leq c\left(\frac{1}{\delta}\right)^p.$$*

</div>

Here we provide an example of effective dimension in case of measuring the space of *Multiset*.

**Multiset with Hausdorff Distance.** A multiset is a set that allows duplicate elements. Consider two multisets $\boldsymbol{x}_1=\{\boldsymbol{u}_i\}_{i=1}^M$, $\boldsymbol{x}_2=\{\boldsymbol{v}_j\}_{j=1}^N$. Let $\Delta(\boldsymbol{u}_i,\boldsymbol{v}_j)$ be a *ground distance* that measures the distance between two elements $\boldsymbol{u}_i,\boldsymbol{v}_j\in\mathcal{V}$ in a set. The (modified) *Hausdorff Distance* can be defined as $d(\boldsymbol{x}_1,\boldsymbol{x}_2):=$ $$\label{HD}
\max\{\frac{1}{N}\sum_{i=1}^N \min_{j\in[M]}\Delta(\boldsymbol{u}_i,\boldsymbol{v}_j),\frac{1}{M}\sum_{j=1}^M\min_{i\in[N]}\Delta(\boldsymbol{v}_j,\boldsymbol{u}_i)\}$$ Let $N(\delta;\mathcal{V},\Delta)$ be the covering number of $\mathcal{V}$ under the ground distance $\Delta$. Let $\mathcal{X}$ denote the set of all sets of size bounded by $L$. By constructing a covering of $\mathcal{X}$ containing any set of size less or equal than $L$ with its elements taken from the covering of $\mathcal{V}$, we have $N(\delta;\mathcal{X},d)\leq  N(\delta;\mathcal{V};\Delta)^L.$ Therefore, $p_{\mathcal{X},d}\leq L\log N(\delta;\mathcal{V},\Delta).$ For example, if $\mathcal{V}:=\{\boldsymbol{v}\in\mathbb{R}^{p}\mid \|\boldsymbol{v}\|_2\leq 1\}$ and $\Delta$ is Euclidean distance, we have $N(\delta;\mathcal{V},\Delta)=(1+\frac{2}{\delta})^p$ and $p_{\mathcal{X},d}\leq Lp.$

Equipped with the concept of *effective dimension*, we can obtain the following bound on the estimation error of the $k$-Nearest-Neighbor estimate of $f(\boldsymbol{x})$.

<div class="theorem">

**Theorem 1**.  * Let $Var(y|f(x))\leq \sigma^2$, and $\hat f_n$ be the $k$-Nearest Neighbor estimate of the target function $f$ constructed from a training set of size $n$. Denote $p:=p_{\mathcal{X},d}$. We have $$\mathbb{E}_{\boldsymbol{x}}\biggl[\left(\hat f_n(\boldsymbol{x})-f(\boldsymbol{x})\right)^2\biggr] \leq \frac{\sigma^2}{k} + cL^2\left(\frac{k}{n}\right)^{2/p}$$ for some constant $c>0$. For $\sigma>0$, minimizing RHS w.r.t. the parameter $k$, we have $$\label{knn_est}
\mathbb{E}_{\boldsymbol{x}}\biggl[\left(\hat f_n(\boldsymbol{x})-f(\boldsymbol{x})\right)^2\biggr] \leq c_2\sigma^{\frac{4}{p+2}}L^{\frac{2p}{2+p}} \left(\frac{1}{n}\right)^{\frac{2}{2+p}}$$ for some constant $c_2>0$.*

</div>

<div class="proof">

*Proof.* The proof is almost the same to a standard analysis of $k$-NN’s estimation error in, for example, , with the *space partition number* replaced by the *covering number*, and *dimension* replaced by the *effective dimension* in Defition . ◻

</div>

When $p_{\mathcal{X},d}$ is reasonably large, the estimation error of $k$-NN decreases quite slowly with $n$. Thus, for the estimation error to be bounded by $\epsilon$, requires the number of samples to scale exponentially in $p_{\mathcal{X},d}$. In the following sections, we develop an estimator $\hat{f}$ based on a RKHS derived from the distance measure, with a considerably better sample complexity for problems with higher effective dimension.

# From Distance to Kernel

We aim to address the long-standing problem of how to convert a distance measure into a positive-definite kernel. Existing approaches either require strict conditions on the distance function (e.g. that the distance be isometric to the square of the Euclidean distance) , or construct empirical PD Gram matrices that do not necessarily generalize to the test samples . There are also some approaches specific to some structured inputs such as sequences, such as that modify a distance function over sequences to a kernel by replacing the minimization over possible alignments into a summation over all possible alignments. This type of kernel, however, results in a diagonal-dominance problem, where the diagonal entries of the kernel Gram matrix are orders of magnitude larger than the off-diagonal entries, due to the summation over a huge number of alignments with a sample itself.

Here we introduce a simple but effective approach *D2KE* that constructs a family of *positive-definite* kernels from a given distance measure. Given an input domain $\mathcal{X}$ and a distance measure $d(.,.)$, we construct a family of kernels as $$\label{DKernel}
k(\boldsymbol{x},\boldsymbol{y}):=\int p(\boldsymbol{\omega}) \phi_{\boldsymbol{\omega}}(\boldsymbol{x})\phi_{\boldsymbol{\omega}}(\boldsymbol{y}) d\boldsymbol{\omega}, \text{where}\;\; \phi_{\boldsymbol{\omega}}(\boldsymbol{x}):=\exp(-\gamma d(\boldsymbol{x},\boldsymbol{\omega})),$$ where $\boldsymbol{\omega}\in \Omega$ is a random structured object, $p(\boldsymbol{\omega})$ is a distribution over $\Omega$, and $\phi_{\boldsymbol{\omega}}(\boldsymbol{x})$ is a feature map derived from the distance of $\boldsymbol{x}$ to all objects $\boldsymbol{\omega}\in\Omega$. The kernel is parameterized by both $p(\boldsymbol{\omega})$ and $\gamma$.

**Relationship to Distance Substitution Kernel.** An insightful interpretation of the kernel can be obtained by expressing the kernel as $$\label{DKernel2}
\exp\left( -\gamma\mathit{softmin}_{p(\boldsymbol{\omega})}\{ d(\boldsymbol{x},\boldsymbol{\omega})+d(\boldsymbol{\omega},\boldsymbol{y}) \} \right)$$ where the soft minimum function, parameterized by $p(\boldsymbol{\omega})$ and $\gamma$, is defined as $$\label{softmin}
\mathit{softmin}_{p(\boldsymbol{\omega})}\;f(\boldsymbol{\omega}):= -\frac{1}{\gamma}\log \int p(\boldsymbol{\omega}) e^{-\gamma f(\boldsymbol{\omega})} d\boldsymbol{\omega}.$$ Therefore, the kernel $k(\boldsymbol{x},\boldsymbol{y})$ can be interpreted as a soft version of the *distance substitution kernel* , where instead of substituting $d(\boldsymbol{x},\boldsymbol{y})$ into the exponent, it substitutes a soft version of the form $$\label{soft_dist}
\mathit{softmin}_{p(\omega)}\{ d(\boldsymbol{x},\boldsymbol{\omega})+d(\boldsymbol{\omega},\boldsymbol{y}) \}.$$ Note when $\gamma\rightarrow\infty$, the value of is determined by $\min_{\boldsymbol{\omega}\in\Omega}\;  d(\boldsymbol{x},\boldsymbol{\omega})+d(\boldsymbol{\omega},\boldsymbol{y})$, which equals $d(\boldsymbol{x},\boldsymbol{y})$ if $\mathcal{X}\subseteq\Omega$, since it cannot be smaller than $d(\boldsymbol{x},\boldsymbol{y})$ by the triangle inequality. In other words, when $\mathcal{X}\subseteq \Omega$, $$k(\boldsymbol{x},\boldsymbol{y}) \rightarrow \exp(-\gamma d(\boldsymbol{x},\boldsymbol{y})) \;\;\text{as}\;\; \gamma\rightarrow \infty.$$ On the other hand, unlike the distance-substituion kernel, our kernel in Eq. is always PD by construction.

**Random Feature Approximation.** The reader might have noticed that the kernel cannot be evaluated analytically in general. However, this does not prohibit its use in practice, so long as we can approximate it via *Random Features (RF)* , which in our case is particularly natural as the kernel itself is defined via a random feature map. Thus, our kernel with the RF approximation can not only be used in small problems but also in large-scale settings with a large number of samples, where standard kernel methods with $O(n^2)$ complexity are no longer efficient enough and approximation methods, such as Random Features, must be employed . Given the RF approximation, one can then directly learn a target function as a linear function of the RF feature map, by minimizing a domain-specific empirical risk. It is worth noting that a recent work that learns to select a set of random features by solving an optimization problem in an supervised setting is orthogonal to our D2KE approach and could be extended to develop a supervised D2KE method. We outline this overall *RF* based empirical risk minimization for our class of D2KE kernels in Algorithm . We will provide a detailed analysis of our estimator in Algorithm  in Section , and contrast its statistical performance to that of $K$-nearest-neighbor.

**Relationship to Representative-Set Method.** A naive choice of $p(\boldsymbol{\omega})$ relates our approach to the *representative-set method (RSM)*: setting $\Omega=\mathcal{X}$, with $p(\boldsymbol{\omega})=p(\boldsymbol{x})$. This gives us a kernel that depends on the data distribution. One can then obtain a Random-Feature approximation to the kernel in by holding out a part of the training data $\{\hat\boldsymbol{x}_j\}_{j=1}^R$ as samples from $p(\boldsymbol{\omega})$, and creating an $R$-dimensional feature embedding of the form: $$\label{feature_embed}
\hat{\phi}_{j}(\boldsymbol{x}):=\frac{1}{\sqrt{R}}\exp\left(-\gamma d(\boldsymbol{x},\hat\boldsymbol{x}_j)\right),\; j\in[R],$$ as in Algorithm . This is equivalent to a $1/\sqrt{R}$-scaled version of the embedding function in the *representative-set method* (or *similarity-as-features method*) , where one computes each sample’s similarity to a set of representatives as its feature representation. However, here by interpreting as a random-feature approximation to the kernel , we obtain a much nicer generalization error bound even in the case $R\rightarrow \infty$. This is in contrast to the analysis of RSM in , where one has to keep the size of the representative set small (of the order $o(n)$) in order to have reasonable generalization performance.

**Effect of $p(\boldsymbol{\omega})$.** The choice of $p(\boldsymbol{\omega})$ plays an important role in our kernel. Surprisingly, we found that many “close to uniform” choices of $p(\boldsymbol{\omega})$ in a variety of domains give better performance than for instance the choice of the data distribution $p(\boldsymbol{\omega})=p(\boldsymbol{x})$ (as in the representative-set method). Here are some examples from our experiments: i) In the *time-series* domain with dissimilarity computed via Dynamic Time Warping (DTW), a distribution $p(\boldsymbol{\omega})$ corresponding to random time series of length uniform in $\in [2,10]$, and with Gaussian-distributed elements, yields much better performance than the Representative-Set Method (RSM); ii) In *string* classification, with edit distance, a distribution $p(\boldsymbol{\omega})$ corresponding to random strings with elements uniformly drawn from the alphabet $\Sigma$ yields much better performance than RSM; iii) When classifying sets of vectors with the Hausdorff distance in Eq. , a distribution $p(\boldsymbol{\omega})$ corresponding to random sets of size uniform in $\in [3,15]$ with elements drawn uniformly from a unit sphere yields significantly better performance than RSM.

We conjecture two potential reasons for the better performance of the chosen distributions $p(\boldsymbol{\omega})$ in these cases, though a formal theoretical treatment is an interesting subject we defer to future work. Firstly, as $p(\boldsymbol{\omega})$ is synthetic, one can generate unlimited number of random features, which results in a much better approximation to the exact kernel . In contrast, RSM requires held-out samples from the data, which could be quite limited for a small data set. Second, in some cases, even with a small or similar number of random features to RSM, the performance of the selected distribution still leads to significantly better results. For those cases we conjecture that the selected $p(\boldsymbol{\omega})$ generates objects that capture semantic information more relevant to the estimation of $f(\boldsymbol{x})$, *when coupled* with our feature map under the dissimilarity measure $d(\boldsymbol{x},\boldsymbol{\omega})$.

# Analysis

In this section, we analyze the proposed framework from the perspectives of error decomposition. Let $\mathcal{H}$ be the RKHS corresponding to the kernel . Let $$\label{risk_minimizer}
f_{C}:=\underset{f\in \mathcal{H}}{argmin} \mathbb{E}[ \ell( f(\boldsymbol{x}), y ) ] \ \ \
s.t.           \|f\|_{\mathcal{H}}\leq C$$ be the population risk minimizer subject to the RKHS norm constraint $\|f\|_{\mathcal{H}}\leq C$. And let $$\label{erm_minimizer}
\hat{f}_{n}:=\underset{f\in \mathcal{H}}{argmin} \frac{1}{n}\sum_{i=1}^n \ell( f(\boldsymbol{x}_i), y_i )  \ \ \
s.t.           \|f\|_{\mathcal{H}}\leq C$$ be the corresponding empirical risk minimizer. In addition, let $\tilde{f}_R$ be the estimated function from our *random feature approximation* (Algorithm ). Then denote the population and empirical risks as $L(f)$ and $\hat{L}(f)$ respectively. We have the following risk decomposition $L(\tilde{f}_R)-L(f)=$ $$\begin{aligned}
\underbrace{(L(\tilde{f}_R)-L(\hat{f}_n))}_{random feature} + \underbrace{(L(\hat{f}_n)-L(f_C))}_{estimation} + \underbrace{(L(f_C)-L(f))}_{approximation}
\end{aligned}$$

In the following, we will discuss the three terms from the rightmost to the leftmost.

**Function Approximation Error.** The RKHS implied by the kernel is $$\mathcal{H}:=\left\{f \;\middle|\; f(\boldsymbol{x})=\sum_{j=1}^m \alpha_{j} k(\boldsymbol{x}_j,\boldsymbol{x}),\; \boldsymbol{x}_j\in\mathcal{X},\forall j\in [m],\; m \in \mathbb{N}\right\},$$ which is a smaller function space than the space of Lipschitz-continuous function w.r.t. the distance $d(\boldsymbol{x}_1,\boldsymbol{x}_2)$. As we show, any function $f\in\mathcal{H}$ is Lipschitz-continous w.r.t. the distance $d(.,.)$.

<div class="proposition">

**Proposition 1**.  * Let $\mathcal{H}$ be the RKHS corresponding to the kernel derived from some metric $d(.,.)$. For any $f\in\mathcal{H}$, $$|f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)|\leq L_f d(\boldsymbol{x}_1,\boldsymbol{x}_2)$$ where $L_f=\gamma C$.*

</div>

While any $f$ in the RKHS is Lipschitz-continuous w.r.t. the given distance $d(.,.)$, we are interested in imposing additional smoothness via the RKHS norm constraint $\|f\|_{\mathcal{H}}\leq C$, and by the kernel parameter $\gamma$. The hope is that the best function $f_C$ within this class approximates the true function $f$ well in terms of the approximation error $L(f_C)-L(f).$ The stronger assumption made by the RKHS gives us a qualitatively better estimation error, as discussed below.

**Estimation Error.** Define $D_{\lambda}$ as $$D_{\lambda}:=\sum_{j=1}^{\infty} \frac{1}{1+\lambda/\mu_j}$$ where $\{\mu_j\}_{j=1}^{\infty}$ is the eigenvalues of the kernel and $\lambda$ is a tuning parameter. It holds that for any $\lambda\geq D_{\lambda}/n$, with probability at least $1-\delta$, $L(\hat{f}_n)-L(f_C) \leq c(\log\frac{1}{\delta})^2C^2 \lambda$ for some universal constant $c$ . Here we would like to set $\lambda$ as small as possible (as a function of $n$). By using the following kernel-independent bound: $D_{\lambda} \leq 1/\lambda,$ we have $\lambda=1/\sqrt{n}$ and thus a bound on the estimation error

$$\label{est_error}
L(\hat{f}_n)-L(f_C) \leq c(\log\frac{1}{\delta})^2C^2 \sqrt{\frac{1}{n}}.$$

The estimation error is quite standard for a RKHS estimator. It has a much better dependency w.r.t. $n$ (i.e. $n^{-1/2}$) compared to that of *$k$-nearest-neighbor method* (i.e. $n^{-2/(2+p_{\mathcal{X},d})}$) especially for higher effective dimension. A more careful analysis might lead to tighter bound on $D_{\lambda}$ and also a better rate w.r.t. $n$. However, the analysis of $D_{\lambda}$ for our kernel is much more difficult than that of typical cases as we do not have an analytic form of the kernel.

**Random Feature Approximation.** Denote $\hat L(.)$ as the empirical risk function. The error from RF approximation $L(\tilde f_R)-L(\hat f_n)$ can be further decomposed as $$(L(\tilde f_R)-\hat L(\tilde f_R)) + (\hat L(\tilde f_R)-\hat L(\hat f_n)) + (\hat L(\hat f_n)-L(\hat f_n))$$ where the first and third terms can be bounded via the same estimation error bound in , as both $\tilde f_R$ and $\hat f_n$ have RKHS norm bounded by $C$. Therefore, in the following, we focus only on the second term of empirical risk. We start by analyzing the approximation error of the kernel $\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)=\tilde{k}_R(\boldsymbol{x}_1,\boldsymbol{x}_2)-k(\boldsymbol{x}_1,\boldsymbol{x}_2)$ where $$\label{RF}
\tilde{k}_R(\boldsymbol{x}_1,\boldsymbol{x}_2):=\frac{1}{R}\sum_{j=1}^R \phi_j(\boldsymbol{x}_1)\phi_j(\boldsymbol{x}_2).$$

<div class="proposition">

**Proposition 2**.  * Let $\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)=k(\boldsymbol{x}_1,\boldsymbol{x}_2)-\tilde{k}(\boldsymbol{x}_1,\boldsymbol{x}_2)$, we have uniform convergence of the form $$\label{converge_result}
P\left\{ \max_{\boldsymbol{x}_1,\boldsymbol{x}_2\in\mathcal{X}} |\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)| > 2t\right\} \leq 2\left(\frac{12\gamma}{t}\right)^{2p_{\mathcal{X},d}}e^{-Rt^2/2},$$ where $p_{\mathcal{X},d}$ is the effective dimension of $\mathcal{X}$ under metric $d(.,.)$. In other words, to guarantee $|\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)|\leq \epsilon$ with probability at least $1-\delta$, it suffices to have $$R = \Omega\biggl(\frac{p_{\mathcal{X},d}}{\epsilon^2}\log(\frac{\gamma}{\epsilon})+\frac{1}{\epsilon^2}\log(\frac{1}{\delta}) \biggr).$$*

</div>

Proposition  gives an approximation error in terms of kernel evaluation. To get a bound on the empirical risk $\hat{L}(\tilde{f}_R)-\hat{L}(\hat{f}_n)$, consider the optimal solution of the empirical risk minimization. By the Representer theorem we have $\hat{f}_n(\boldsymbol{x})=\frac{1}{n}\sum_{i}\alpha_i k(\boldsymbol{x}_i,\boldsymbol{x})$ and $\tilde{f}_R(\boldsymbol{x})=\frac{1}{n}\sum_{i}\tilde{\alpha}_i \tilde{k}(\boldsymbol{x}_i,\boldsymbol{x}).$ Therefore, we have the following corollary.

<div class="corollary">

**Corollary 1**.  * To guarantee $\hat{L}(\tilde{f}_R)-\hat{L}(\hat{f}_n) \leq \epsilon,$ with probability $1-\delta$, it suffices to have $$R = \Omega\biggl(\frac{p_{\mathcal{X},d}M^2A^2}{\epsilon^2}\log(\frac{\gamma}{\epsilon})+\frac{M^2A^2}{\epsilon^2}\log(\frac{1}{\delta}) \biggr).$$ where $M$ is the Lipschitz-continuous constant of the loss function $\ell(.,y)$, and $A$ is a bound on $\|\boldsymbol{\alpha}\|_1/n$.*

</div>

For most of loss functions, $A$ and $M$ are typically small constants. Therefore, Corollary states that it suffices to have number of Random Features proportional to the *effective dimension* $O(p_{\mathcal{X},d}/\epsilon^2)$ to achieve an $\epsilon$ approximation error.

Combining the three error terms, we can show that the proposed framework can achieve $\epsilon$-suboptimal performance.

<div class="claim">

**Claim 1**. *Let $\tilde{f}_R$ be the estimated function from our *random feature approximation* based ERM estimator in Algorithm , and let $f^*$ denote the desired target function. Suppose further that for some absolute constants $c_1, c_2 > 0$ (up to some logarithmic factor of $1/\epsilon$ and $1/\delta$):*

1.  *The target function $f^*$ lies close to the population risk minimizer $f_C$ lying in the RKHS spanned by the D2KE kernel: $L(f_C)-L(f) \le \epsilon/2$.*

2.  *The number of training samples $n \ge c_1 \, C^4/\epsilon^2$.*

3.  *The number of random features $R \ge c_2 p_{\mathcal{X},d}/\epsilon^2$.*

*We then have that: $L(\tilde{f}_R) - L(f^*) \le \epsilon$ with probability $1-\delta$.*

</div>

# Experiments

In this section, we evaluate the proposed method in three different domains involving time-series, strings, and images. Firstly, we discuss the dissimilarity measures and data characteristics for each set of experiments. Then we introduce compared distance-based methods and report their results.

**Distance Measures.** We have chosen three well-known dissimilarity measures: 1) Dynamic Time Warping (DTW) for time-series ; 2) Edit Distance (Levenshtein distance) for string ; 3) (Modified) Hausdorff distance for measuring the closeness of two cloud of points for images. Since most distance measures are computationally demanding, having quadratic complexity, we adapted or implemented C-MEX programs for them; other codes were written in Matlab.

**Datasets.** For each domain, we selected 4 datasets for our experiments. For time-series data, all are multivariate time-series; three are from the UCI Machine Learning repository , the other is generated from the IQ (In-phase and Quadrature components) samples from a wireless line-of-sight communication system from GMU. For string data, the size of alphabet is between 4 and 8; two of them are from the UCI Machine Learning repository and the other two from the LibSVM Data Collection . All image datasets derived from Kaggle; we computed a set of SIFT-descriptors to represent each image. We divided each dataset into 70/30 train and test subsets (if there was no predefined train/test split). Properties of these datasets are summarized in Table in Appendix .

**Baselines.** We compare D2KE against KNN, DSK_RBF , DSK_ND , GDK_LED , and RSM . Among these baselines, KNN, DSK_RBF, DSK_ND, and GDK_LED have quadratic complexity $O(N^2L^2)$ in both the number of data samples and the length of the sequences, while RSM has computational complexity $O(NRL^2)$, linear in the number of data samples but still quadratic in the length of the sequence. These compare to our method, D2KE, which has complexity $O(NRL)$, linear in both the number of data samples and the length of the sequence. For each method, we search for the best parameters on the training set by performing 10-fold cross validation. For our new method D2KE, since we generate random samples from the distribution, we can use as many as needed to achieve performance close to an exact kernel. We report the best number in the range $R = [4, 4096]$ (typically the larger $R$ is, the better the accuracy). We employ a linear SVM implemented using LIBLINEAR (Fan et al., 2008) for all embedding-based methods (GDK_LED, RSM, and D2KE) and use LIBSVM for precomputed dissimilairty kernels (DSK_RBF and DSK_ND). More details of experimental setup is provided in Appendix .

**Results.** As shown in Tables , , and , D2KE can consistently outperform or match all other baselines in terms of classification accuracy while requiring far less computation time. There are several observations worth noting here. First, D2KE performs much better than KNN, supporting our claim that D2KE can be a strong alternative to KNN across applications. Second, compared to the two distance substitution kernels DSK_RBF and DSK_ND, our method can achieve much better performance, suggesting that a representation induced from a truly p.d. kernel makes significantly better use of the data than indefinite kernels. Among all methods, RSM is closest to our method in terms of practical construction of the feature matrix. However, the random objects (time-series, strings, or sets) sampled by D2KE performs significantly better, as we discussed in section . GDK_LED performs best in image domain, which may be contributed to both by transductive training and by the SVD operation which allow it to directly access features of the test set and denoise unwanted information from the raw images. More detailed discussions of the experimental results for each domain are given in Appendix .

# Conclusion and Future Work

In this work, we propose a general framework for deriving a *positive-definite* kernel and a feature embedding function from a given dissimilarity measure between input objects. The framework is especially useful for structured input domains such as sequences, time-series, and sets, where many well-established dissimilarity measures have been developed. Our framework subsumes a couple of existing approaches as special or limiting cases, and also opens up a new direction for creating embeddings of structured objects based on distance to random objects. A promising future direction is to develop such distance-based embeddings within a deep architecture to handle structured inputs in an end-to-end learning system.

# Proof of Theorem  and Theorem 

## Proof of Theorem 

<div class="proof">

*Proof.* Note the function $g(t)=exp(-\gamma t)$ is Lipschitz-continuous with Lipschitz constant $\gamma$. Therefore, $$\begin{aligned}
&|f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)|=|\langle f, \phi(\boldsymbol{x}_1)-\phi(\boldsymbol{x}_2)\rangle|\\
&\leq \|f\|_{\mathcal{H}} \|\phi(\boldsymbol{x}_1)-\phi(\boldsymbol{x}_2)\|_{\mathcal{H}}\\
&= \|f\|_{\mathcal{H}} \sqrt{ \int_{\boldsymbol{\omega}} p(\boldsymbol{\omega}) ( \phi_{\boldsymbol{\omega}}(\boldsymbol{x}_1)-\phi_{\boldsymbol{\omega}}(\boldsymbol{x}_2))^2 d\boldsymbol{\omega}}\\
&\leq  \|f\|_{\mathcal{H}} \sqrt{ \int_{\boldsymbol{\omega}} p(\boldsymbol{\omega}) \gamma^2 |d(\boldsymbol{x}_1,\boldsymbol{\omega})-d(\boldsymbol{x}_2,\boldsymbol{\omega})|^2 d\boldsymbol{\omega}}\\
&\leq \gamma \|f\|_{\mathcal{H}}  \sqrt{ \int_{\boldsymbol{\omega}} p(\boldsymbol{\omega})  d(\boldsymbol{x}_1,\boldsymbol{x}_2)^2 d\boldsymbol{\omega}}\\
&\leq \gamma  \|f\|_{\mathcal{H}}  d(\boldsymbol{x}_1,\boldsymbol{x}_2)\leq \gamma  C  d(\boldsymbol{x}_1,\boldsymbol{x}_2)
\end{aligned}$$ ◻

</div>

## Proof of Theorem 

<div class="proof">

*Proof.* Our goal is to bound the magnitude of $\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)=\tilde{k}_R(\boldsymbol{x}_1,\boldsymbol{x}_2)-k(\boldsymbol{x}_1,\boldsymbol{x}_2)$. Since $E[\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)]=0$ and $|\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)|\leq 1$, from Hoefding’s inequality, we have $$P\left\{ |\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)|\geq t \right\} \leq 2 \exp(-Rt^2/2)$$ a given input pair $(\boldsymbol{x}_1,\boldsymbol{x}_2)$. To get a unim bound that holds $\forall (\boldsymbol{x}_1,\boldsymbol{x}_2)\in\mathcal{X}\times\mathcal{X}$, we find an $\epsilon$-covering $\mathcal{E}$ of $\mathcal{X}$ w.r.t. $d(.,.)$ of size $N(\epsilon,\mathcal{X},d)$. Applying union bound over the $\epsilon$-covering $\mathcal{E}$ for $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$, we have $$\label{tmp1}
P\left\{ \max_{\boldsymbol{x}_1'\in\mathcal{E},\boldsymbol{x}_2'\in\mathcal{E}} |\Delta_R(\boldsymbol{x}_1',\boldsymbol{x}_2')| > t \right\} \leq 2|\mathcal{E}|^2\exp(-Rt^2/2).$$ Then by the definition of $\mathcal{E}$ we have $|d(\boldsymbol{x}_1,\boldsymbol{\omega})-d(\boldsymbol{x}_1',\boldsymbol{\omega})|\leq d(\boldsymbol{x}_1,\boldsymbol{x}_1')\leq \epsilon$. Together with the fact that $\exp(-\gamma t)$ is Lipschitz-continuous with parameter $\gamma$ for $t\geq0$, we have $$|\phi_{\boldsymbol{\omega}}(\boldsymbol{x}_1)-\phi_{\boldsymbol{\omega}}(\boldsymbol{x}_1')|\leq \gamma\epsilon$$ and thus $$|\tilde{k}_R(\boldsymbol{x}_1,\boldsymbol{x}_2)-\tilde{k}_R(\boldsymbol{x}_1',\boldsymbol{x}_2')|\leq 3\gamma\epsilon,$$ $$|k(\boldsymbol{x}_1,\boldsymbol{x}_2)-k(\boldsymbol{x}_1',\boldsymbol{x}_2')|\leq 3\gamma\epsilon$$ for $\gamma\epsilon$ chosen to be $\leq 1$. This gives us $$\label{tmp2}
|\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)-\Delta_R(\boldsymbol{x}_1',\boldsymbol{x}_2')|\leq 6\gamma\epsilon$$ Combining and , we have $$\label{tmp3}
\begin{aligned}
&P\left\{ \max_{\boldsymbol{x}_1'\in\mathcal{E},\boldsymbol{x}_2'\in\mathcal{E}} |\Delta_R(\boldsymbol{x}_1',\boldsymbol{x}_2')| > t + 6\gamma \epsilon\right\} \\ 
&\leq 2\left(\frac{2}{\epsilon}\right)^{2p_{\mathcal{X},d}}\exp(-Rt^2/2).
\end{aligned}$$ Choosing $\epsilon=t/6\gamma$ yields the result. ◻

</div>

## Proof for Corollary 

<div class="proof">

*Proof.* First of all, we have $$\begin{aligned}
&\frac{1}{n}\sum_{i=1}^n \ell(\frac{1}{n}\sum_{j=1}^n \tilde{\alpha}_j\tilde{k}(\boldsymbol{x}_j,\boldsymbol{x}_i),y_i)\\
&\leq \frac{1}{n}\sum_{i=1}^n \ell(\frac{1}{n}\sum_{j=1}^n \alpha_j\tilde{k}(\boldsymbol{x}_j,\boldsymbol{x}_i),y_i)
\end{aligned}$$ by the optimality of $\{\tilde{\alpha}_j\}_{j=1}^n$ w.r.t. the objective using the approximate kernel. Then we have $$\begin{aligned}
&\hat{L}(\tilde{f}_R)-\hat{L}(\hat{f}_n)\\ 
&\leq \frac{1}{n}\sum_{i=1}^n \ell(\frac{1}{n}\sum_{j=1}^n\alpha_j\tilde{k}(\boldsymbol{x}_j,\boldsymbol{x}_i),y_i)-\ell(\frac{1}{n}\sum_{j=1}^n\alpha_j k(\boldsymbol{x}_j,\boldsymbol{x}_i),y_i) \\
&\leq M \frac{\|\boldsymbol{\alpha}\|_1}{n}\left(\max_{\boldsymbol{x}_1,\boldsymbol{x}_2\in\mathcal{X}}|\tilde{k}(\boldsymbol{x}_1,\boldsymbol{x}_2)-k(\boldsymbol{x}_1,\boldsymbol{x}_2)|\right) \\
&\leq MA\left(\max_{\boldsymbol{x}_1,\boldsymbol{x}_2\in\mathcal{X}}|\tilde{k}(\boldsymbol{x}_1,\boldsymbol{x}_2)-k(\boldsymbol{x}_1,\boldsymbol{x}_2)|\right) 
\end{aligned}$$ where $A$ is a bound on $\|\boldsymbol{\alpha}\|_1/n$. Therefore to guarantee $$\hat{L}(\tilde{f}_R)-\hat{L}(\hat{f}_n) \leq \epsilon$$ we would need $\left(\max_{i,j\in[n]} |\Delta_R(\boldsymbol{x}_1,\boldsymbol{x}_2)|\right)\leq \hat{\epsilon}:=\epsilon/MA$. Then applying Theorem leads to the result. ◻

</div>

# General Experimental Settings

**Baselines.** We compare with the following methods:  
**KNN**: a simple yet universal method to apply any distance measure to classification tasks.  
**DSK_RBF** : distance substitution kernels, a general framework for kernel construction by substituting a problem specific distance measure for the Euclidean distance used in ordinary kernel functions. We use a Gaussian RBF kernel.  
**DSK_ND** : another class of distance substitution kernels with negative distance.  
**GDK_LED** : learning a pseudo-Euclidean linear embedding from the dissimilarity matrix followed by performing singular value decomposition .  
**RSM** : building an embedding by computing distances from randomly selected representative samples.

Among these baselines, KNN, DSK_RBF, DSK_ND, and GDK_LED have quadratic complexity $O(N^2L^2)$ in both the number of data samples and the length of the sequences, while RSM has computational complexity $O(NRL^2)$, linear in the number of data samples but still quadratic in the length of the sequence. These compare to our method, D2KE, which has complexity $O(NRL)$, linear in both the number of data samples and the length of the sequence.

**General Setup.** For each method, we search for the best parameters on the training set by performing 10-fold cross validation. Following , we use an exact RBF kernel for DSK_RBF while choosing squared distance for DSK_ND. Since there no clear indication how many singular vectors should be computed for the GDK_LED method after construction of the dissimilarity matrix, we compute $R = [4, 512]$ singular vectors and report the best performance. Importantly, we also perform SVD transductively on both train and test data for GDK_LED; we will show below that this is beneficial. Similarly, we adopted a simple method – random selection – to obtain $R = [4, 512]$ data samples as the representative set for GDK_LW. For our new method D2KE, since we generate random samples from the distribution, we can use as many as needed to achieve performance close to an exact kernel. We report the best number in the range $R = [4, 4096]$ (typically the larger $R$ is, the better the accuracy). We employ a linear SVM implemented using LIBLINEAR (Fan et al., 2008) for all embedding-based methods (GDK_LED, RSM, and D2KE) and use LIBSVM for precomputed dissimilairty kernels (DSK_RBF and DSK_ND).

All computations were carried out on a DELL dual-socket system with Intel Xeon processors at 2.93GHz for a total of 16 cores and 250 GB of memory, running the SUSE Linux operating system. To accelerate the computation of all methods, we used multithreading with 12 threads total for various distance computations in all experiments.

<div class="center">

<div id="tb:info of datasets">

| Domain |    Name    | Var/Alpb | Classes | Train | Test  | length  |
|:------:|:----------:|:--------:|:-------:|:-----:|:-----:|:-------:|
|   TS   |   Auslan   |    22    |   95    | 1795  |  770  | 45-136  |
|   TS   |   pentip   |    3     |   20    | 2000  |  858  | 109-205 |
|   TS   |  ActRecog  |    3     |    7    | 1837  |  788  |  2-151  |
|   TS   |  IQ_radio  |    4     |    5    | 6715  | 6715  |   512   |
|  Str   |  bit-str4  |    4     |   10    |  140  |  60   | 44/158  |
|  Str   |   splice   |    4     |    3    | 2233  |  957  |   60    |
|  Str   | mnist-str4 |    4     |   10    | 60000 | 10000 | 34/198  |
|  Str   | mnist-str8 |    8     |   10    | 60000 | 10000 |  17/99  |
|  Img   |   flower   |   128    |   10    |  147  |  63   | 66/429  |
|  Img   |   decor    |   128    |    7    |  340  |  144  | 35/914  |
|  Img   |   style    |   128    |    7    |  625  |  268  |  6/530  |
|  Img   |  letters2  |   128    |   33    | 3277  | 1404  |  1/22   |

Properties of the datasets. TS, Str and Img stand for Time-Series, String, and Image respectively. Var/Alpb stands for the number of variables for time-series or image SIFT-descriptors, and for the size of the alphabet for strings.

</div>

</div>

# Detailed Experimental Results on Time-Series, Strings, and Images

## Results on multivariate time-series 

**Setup.** For time-series data, we employed the most successful distance measure - DTW - for all methods. For all datasets, a Gaussian distribution was found to be applicable, parameterized by its bandwidth $\sigma$. The best values for $\sigma$ and for the length of random time series were searched in the ranges and , respectively.

**Results.** As shown in Table , D2KE can consistently outperform or match all other baselines in terms of classification accuracy while requiring far less computation time for multivariate time-series. The first interesting observation is that our method performs substantially better than KNN, often by a large margin, i.e., D2KE achieves 26.62% higher performance than KNN on IQ_radio. This is because KNN is sensitive to the data noise common in real-world applications like IQ_radio, and has notoriously poor performance for high-dimensional data sets like Auslan. Moreover, compared to the two distance substitution kernels DSK_RBF and DSK_ND, our method can achieve much better performance, suggesting that a representation induced from a truly p.d. kernel makes significantly better use of the data than indefinite kernels. However, GDK_LED slightly outperforms D2KE on Auslan, probably due to the embedding matrix (singular vectors) being computed transductively on both train and test data. Among all methods, RSM is closest to our method in terms of practical construction of the feature matrix. However, the random time series sampled by D2KE performs significantly better, as we discussed in section .

## Results on strings 

**Setup.** For string data, there are various well-known edit distances. Here, we choose Levenshtein distance as our distance measure since it can capture global alignments of the underlying strings. We first compute the alphabet from the original data and then uniformly sample characters from this alphabet to generate random strings. We search for the best parameters for $\gamma$ in the range , and for the length of random strings in the range , respectively.

**Results.** As shown in Table , D2KE consistently performs better than or similarly to other distance-based baselines. Unlike the previous experiments where DTW is not a distance metric, Levenshtein distance is indeed a distance metric; this helps improve the performance of our baselines. However, D2KE still offers a clear advantage over baseline. It is interesting to note that the performance of DSK_RBF is quite close to our method’s, which may be due to DKS_RBF with Levenshtein distance producing a c.p.d. kernel which can essentially be converted into a p.d. kernel. Notice that on relatively large datasets, our method, D2KE, can achieve better performance, and often with far less computation than other baselines with quadratic complexity in both number and length of data samples. For instance, on mnist-str4 D2KE obtains higher accuracy with an order of magnitude less runtime compared to DSK_RBF and DSK_ND, and two orders of magnitude less than GDK_LED, due to higher computational costs both for kernel matrix construction and for eigendecomposition.

## Results on Sets of SIFT-descriptors for images 

**Setup.** For image data, following we use the modified Hausdorff distance (MHD) as our distance measure between images, since this distance has shown excellent performance in the literature . We first applied the open-source OpenCV library to generate a sequence of SIFT-descriptors with dimension 128, then MHD to compute the distance between sets of SIFT-descriptors. We generate random images of each SIFT-descriptor uniformly sampled from the unit sphere of the embedding vector space $\mathbb{R}^{128}$. We search for the best parameters for $\gamma$ in the range , and for length of random SIFT-descriptor sequence in the range .

**Results.** As shown in Table , D2KE performance is near other baselines in most cases. First, GDK_LED performs best in three cases, which may be contributed to both by transductive training and by the SVD operation which allow it to directly access features of the test set and denoise unwanted information from the raw images. Nevertheless, the quadratic complexity of GDK_LED in terms of both the number of images and the length of SIFT descriptor sequences makes it hard to scale to large data. Interestingly, D2KE still performs much better than KNN, again supports our claim that D2KE can be a strong alternative to KNN across applications.

[^1]: Both authors contributed equally to this work
