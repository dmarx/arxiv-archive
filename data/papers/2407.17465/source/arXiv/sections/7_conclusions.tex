\section{Conclusions}

We introduce \umup, a modified and improved version of \mup\ that satisfies Unit Scaling. Through careful analysis guided by first principles we identify an interpretable set of HPs that has minimal interdependencies and facilitates an efficient independent sweeping strategy. We show that the stability properties of \mup\ combined with Unit Scaling enable a simple and robust FP8 mixed precision scheme that works in a realistic large scale training scenario.
\umup\ provides further evidence that the principle of Unit Scaling is beneficial for model design.

\paragraph{Limitations and future work} Some choices like the modified embedding LR rule are only justified by empirical observations, and currently lack a theoretical explanation. Additionally, neither \mup\ nor Unit Scaling give guarantees for network quantities to be well-behaved over the course of training. In particular we would like to understand the issue (or feature) of scale growth in the critical layers better and look into possible mitigations. We also believe that low-precision training techniques can be pushed further, with \umup\ offering an ideal starting point for future optimizations.

\section{Acknowledgments}

We would like to thank Paul Balan√ßa, Andrew Fitzgibbon, Steve Barlow, Mark Pupilli, Jeremy Bernstein, Tim Large and Lucas Lingle for their feedback and discussions around \umup\ at the various stages of its development.