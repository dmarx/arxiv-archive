\section{A guide to the \texttt{unit scaling} library} \label{app:us_lib_guide}

Our PyTorch \citep{pytorch} extension library, released under an open source license at \url{https://github.com/graphcore-research/unit-scaling}, accompanies this paper to provide standard and reference implementations of \umup{} operations and optimizers.

This section provides an overview of the functionality of the library; please consult the repository documentation for details. A good place to start is our demo of a simple \umup\ training implementation: \url{https://github.com/graphcore-research/unit-scaling/blob/main/examples/demo.ipynb}.

\subsection{Standard usage}

Compared with SP, \umup{} requires the insertion of appropriate scaling factors in the forward and backward pass, a different parameter initialization scheme and the application of learning rate scaling rules based on the role and shape of each parameter.

The library provides implementations of ops in \texttt{unit\_scaling.functional} with appropriate scaling rules (\Cref{tab:ops_compendium}). Non-homogeneous ops (\Cref{app:non-homogeneous}) have optional \emph{mults}, which are hyperparameters controlling shape of non-linear operations and the interpolation between mutiple inputs. Ops may also specify \emph{constraints}, which are used to satisfy the cut-edge rule (\Cref{app:cut_edge_rule}). Although this rule could be automated as a global graph transformation, the library makes constraint selection an explicit step for the user, while providing sensible defaults. For example, weight gradients are generally cut-edges, so are unconstrained.

Parameters are \emph{tagged} with their role in the model (as a ``bias'', ``norm'' parameter, ``weight'' or ``output''). The library achieves this by extending \texttt{torch.nn.Parameter} with an additional property \texttt{mup\_type}. This property is required for every parameter in a \umup{} model. Given this, and information on the overall depth of the model, the library applies the learning rate rules of \Cref{table:mup_umup_schemes} as a pre-optimizer transformation that modifies the learning rate for each parameter. This allows standard PyTorch optimizers to be used without modification.

PyTorch uses \emph{modules} to encapsulate parameter declaration, initialization and the calling of ops. The library makes available \umup{} versions of common modules, which declare tagged parameters, apply unit-scale initialization, and call unit-scaled ops, with appropriate default settings.

With these components, user code for training using \umup{} is very close to that of vanilla PyTorch (see an example in \Cref{fig:umup_example}).

\begin{figure}[h]
\codefig{umup_example.py}
\caption{Using the \texttt{unit scaling} library given the tensors \texttt{input\_} \& \texttt{target}.}
\label{fig:umup_example}
\end{figure}

\subsection{Extending the library}

As the set of deep learning ops of interest is always growing, the unit-scaling library is open for extension. For example, consider the possible implementation of unit-scaled \texttt{hardtanh(x) = clip(x, -1, 1)} in \Cref{fig:umup_extension_hardtanh}.

\begin{figure}[h]
\codefig{umup_extension_hardtanh.py}
\caption{Implementing new unit-scaled operations.}
\label{fig:umup_extension_hardtanh}
\end{figure}

The implementation follows a standard pattern:

\begin{enumerate}
\item Calculate the theoretical or empirical scaling factors for each forward and backward pass independently, based on independent unit-scaled Gaussian inputs.
\item Apply the optional constraint to select or combine these scaling factors, using the helper function \texttt{apply\_constraint}.
\item Call \texttt{scale\_bwd} on the inputs, and \texttt{scale\_fwd} on the outputs to compensate for scaling after the op or grad-op is executed.
\end{enumerate}

It can be checked empirically using random inputs and gradients (example in \Cref{fig:umup_testing_hardtanh}).

\begin{figure}[h]
\codefig{umup_testing_hardtanh.py}
\caption{Testing unit-scaled operations, using \texttt{constraint=None} to allow independent fwd and bwd scaling. The default constraint \texttt{"to\_output\_scale"} preserves forward-pass scale while constraining the forward and backward scales to be equal.}
\label{fig:umup_testing_hardtanh}
\end{figure}

\subsection{As a reference implementation}

The core technique of \umup{} is readily implementable in most deep learning frameworks; the primary requirement is for custom gradient operations in order to provide equivalents of \texttt{scale\_fwd} and \texttt{scale\_bwd}. We hope that the library provides a useful reference, as well as a set of tools and techniques for developing custom \umup{} support in other libraries and projects.
