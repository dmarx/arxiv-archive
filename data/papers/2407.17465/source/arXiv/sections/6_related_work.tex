\section{Related Work}

\paragraph{Low-precision training}

Techniques introduced to facilitate FP8 training include those covered in \Cref{app:low_precision_and_its_trade_offs} and more \citep{Training_And_Inference_Using_8_Bit,DNNs_With_8_Bit,Mixed_Precision_8_Bit}. These largely concern the quantizing of activations, weights and gradients, though \citep{FP8-LM} also explore FP8 optimizer states and cross-device communication, which we consider interesting avenues of further exploration. Recently, stable training has been demonstrated for the MX family of formats which use a shared block-exponent \citep{MX1,MX2}, and even for the ternary BitNet format \citep{BitNet1,BitNet2,BitNet3}. Again, we consider these formats for follow-up work.
% , though evaluation of these formats and hardware/software support is nascent. Again, we consider examining these formats for \umup\ valuable follow-up work.

\paragraph{Stability features}

Another recent research trend has been the analysis of features that contribute to (or resolve) numerical and algorithmic instability. \citep{Small_Scale_Proxies} show that unstable training dynamics can result from attention logit growth (fixed by QK-norm \citep{Scaling_Vision_Transformers}) and from divergent output logits (fixed by z-loss \citep{Palm}). \citep{Stable_And_Low_Precision_Training} find large feature magnitudes can be avoided by zero-initialization, and loss spikes avoided via a modified AdamW, specifically for low-precision training. \citep{Intriguing_Properties} investigate how pre-training settings affect instabilities revealed during post-training quantization. \citep{Dynamics_Of_Diffusion_Models} apply a similar philosophy to Unit Scaling for the training of diffusion models, to address uncontrolled magnitude changes. Extreme activation values seen in large models \citep{LLM_INT8,SmoothQuant} have been addressed by softmax-clipping \citep{Quantizable_Transformers}, and by the addition of extra terms \citep{Massive_Activations} or tokens \citep{Vision_Transformers_Need_Registers} to bias the attention computation. We do not adopt these features in our experiments to avoid confounding effects, but we expect them to benefit \umup\ and hope to explore their usage.

\paragraph{Learning dynamics}

Several recent efforts have tried to improve \mup\ from different angles.
\citep{Modula} introduces the notion of the \textit{modular norm} over the full weight-space, which like \mup\ aims to ensure stable updates that provide LR transfer, and like \umup\ is implemented via modules designed to ensure stable training.
Challenging the assumptions underpinning \mup, \citep{Scaling_Exponents} explores the notion of \textit{alignment} between parameters and data, demonstrating that other parametrizations with per-layer learning rates can outperform standard \mup. We consider comparing these parametrizations against \umup\ and trying unit-scaled versions valuable future work. Recent applications of \mup\ to the problems of weight sparsity \citep{Sparse_Mup} and structured matrices \citep{Compute_Better_Spent} are also interesting candidates for \umup.
