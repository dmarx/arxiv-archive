\section{Background} \label{sec:background}

\subsection{The Maximal Update Parametrization} \label{sec:background:the_maximal_update_parameterization}

% As the width of a neural network tends to infinity, its learning dynamics depend on the \textit{parametrization}.
Tensor Programs V \citep{Tensor_Programs_V} defines a parametrization as `a rule for how to change hyperparameters when the widths of a neural network change'. They show that \mup\ is the only parametrization that gives `maximal feature learning' in the limit,
% (i.e. the order of all layers' weight updates is independent of width)
whereas standard parametrization (SP) has imbalanced learning (parts of the network blow up or cease to learn).
% Due to this instability, the training performance of \mup\ models is expected to exceed that of SP models at large widths.

One consequence of this improved stability is that learning dynamics under \mup\ are ideally independent of model-size, as are optimal HPs. This facilitates a method known as \mut, which describes the process of training many smaller proxy models to evaluate candidate HP values, then using the best-performing ones to train a larger target model. An HP is said to be \mutable\ if its optimal value is the same across model-sizes.
% The benefit of \mut\ is that sweeping HP values for the proxy model is substantially cheaper than doing so for the target model. 

\paragraph{ABC-parametrizations} \mup, SP, and the Neural Tangent Kernel (NTK) \citep{NTK} are all instances of abc-parametrizations. This assumes a model under training where weights are defined as:
\begin{align} \label{eq:abc}
    w_0 &\sim \mathcal{N}(0,B_W^2),
    \\
    W_t &= A_W \cdot w_t, \nonumber
    \\
    w_{t+1} &= w_t + C_W \cdot \Phi_t(\nabla \mathcal{L}_0,...,\nabla \mathcal{L}_t), \nonumber
\end{align}
with $t$ a time-step and $\Phi_t(\nabla \mathcal{L}_0,...,\nabla \mathcal{L}_t)$ the weight update based on previous loss gradients.

% \begin{align}
%     A_W = \alpha_W \frac{a_W}{a_{W_\mathrm{base}}}, \quad B = \sigma_W \frac{b_W}{b_{W_\mathrm{base}}}, \quad C = \eta_W \frac {c_W}{c_{W_\mathrm{base}}}
% \end{align}

A parametrization scheme such as \mup\ is then defined specifying how scalars $A_W,B_W,C_W$ change with model width. This can be expressed in terms of width-dependent factors $a_W, b_W, c_W$, such that $A_W \propto a_W$, $B_W \propto b_W$, $C_W \propto c_W$. The values these factors take are what characterize a particular scheme. For \mup\ these are given in \Cref{table:mup}.
% Note that because \mup\ only defines a parametrization, it doesn't specify \textit{absolute} values for $A_W,B_W,C_W$.
%, which also depend on the $\initstd$ and $\baseshapes$ HPs (see \Cref{eq:abc_mup_absolute})
% \mup\ is provably the only abc-parametrization that allows all model features to evolve non-trivially in the infinite-width limit without blowing up.
For depth a similar result has been proved using \depthmup\ \citep{Tensor_Programs_VI}, albeit in a restricted setting. When we refer to \mup\ in the paper we assume the \depthmup\ scaling rules (\Cref{table:mup_umup_schemes}, `Residual' column).

A key property of the abc-parametrization is that one can shift scales between $A_W,B_W,C_W$ in a way that preserves learning dynamics (i.e. the activations computed during training are unchanged). We term this \textit{abc-symmetry}. For a fixed $\theta > 0$, the behavior of a network trained with Adam is invariant to changes of the kind:
\begin{align} \label{eq:abc_symmetry}
    A_W \leftarrow A_W \cdot \theta, \quad B_W \leftarrow B_W / \theta, \quad C_W \leftarrow C_W / \theta
\end{align}
(reproduced from Tensor Programs V, Section~J.2.1). This means that parametrizations like \mup\ can be presented in different but equivalent ways. ABC-symmetry is a key component in developing \umup.

\begin{table}[b]
  \vspace{-0.5em}
  \centering
  \caption{The scaling rules defining \mup. The type of a weight is determined by whether $\fanin$ \& $\fanout$ both depend on width (hidden), only $\fanout$ does (input), or only $\fanin$ (output). Hence $\fanin$ is always a multiple of width here.}
  \vspace{0.5em}
  \label{table:mup}
  \begin{tabular}{rl @{\hspace{0.8\tabcolsep}} lccc}
      \toprule
      & \multicolumn{2}{c}{\multirow{2}{*}{ABC-multiplier}} & \multicolumn{3}{c}{Weight ($W$) Type}
      \\
      \rule{0pt}{1em} & & & Input & Hidden & Output
      \\  % \& Bias
      \midrule
      \multirow{3}{*}{\textbf{\mup}} & parameter & ($a_W$) & $\one$ & $\one$ & $\nicefrac \one {\fanin(W)}$
      \\
      & initialization & ($b_W$) & $\one$ & $\nicefrac{\one}{\sqrt{\fanin(W)}}$ & $\one$
      \\
      & Adam LR & ($c_W$) & $\one$ & $\nicefrac \one {\fanin(W)}$ & $\one$
      \\
      \bottomrule
      \\
  \end{tabular}
\end{table}

\paragraph{Transferable HPs} \mup\ focuses on the subset of HPs whose optimal values we expect to \textit{transfer across} axes such as width and depth. We term these \mutable\ HPs. All \mutable\ HPs function as multipliers and can be split into three kinds, which contribute to the three (non-HP) multipliers given by the abc-parametrization: $\alpha_W, \sigma_W, \eta_W$ where $A_W \propto \alpha_W, B_W \propto \sigma_W, C_W \propto \eta_W$. The difference between these multipliers and the ones that define a parametrization is that they are specified by the user, rather than being a function of width. $\alpha_W$ and $ \eta_W$ are rarely introduced outside of the \mup\ literature, but can be valuable to tune for both \mup\ and SP models. In the \mup\ literature the term `HPs' often implicitly refers to \mutable\ HPs. We adopt this convention here, unless specified otherwise.

% \footnotetext{Other optimizer-related HPs (e.g. Adam $\beta$s) may also have \mut, but these are rarely modified so we ignore them here.}

\paragraph{Base shape} Two additional (non-\mutable) HPs introduced by \mup\ are the $\basewidth$ and $\basedepth$. This refers to a mechanism where a user specifies a particular shape for the model, where its behavior under \mup\ and SP are the same. The \mup\ model still \textit{scales} according to the abc-rules, so for all other shapes the two models will be different. This is implemented by dividing the \mup\ scaling rules for the given model by those of a fixed-shape model at the $\basewidth$ and $\basedepth$.

Putting this together with our abc-parametrization given in \Cref{eq:abc}, and the \mutable\ HPs outlined above, we now derive our final, absolute expressions for $A_W,B_W,C_W$:
\begin{align} \label{eq:abc_mup_absolute}
    A_W \leftarrow \alpha_W \frac{a_W}{a_{W_\textrm{base}}},
    \quad
    B_W \leftarrow \sigma_W \frac{b_W}{b_{W_\textrm{base}}},
    \quad
    C_W \leftarrow \eta_W \frac{c_W}{c_{W_\textrm{base}}}
\end{align}
Though base shapes are necessary for \mup, they are not typically swept. Rather, they are considered a preference of the user, who may wish to retain the behavior of an existing SP model at a given shape.

\paragraph{Choosing HPs to sweep} 

In theory, the search space of \mutable\ HPs includes $\alpha_W, \sigma_W, \eta_W$ for every parameter tensor $W$ in the model. In practice far fewer HPs are swept, with global grouping often used for $\sigma_W$ and $\eta_W$, and many $\alpha_W$s dropped or grouped across layers.

The sets of HPs chosen for sweeps in the \mup\ literature is explored in \Cref{app:additional_background:mup}. Tensor Programs V uses a random search to identify the best HP values, which has become the standard approach to sweeping. The number of runs in a sweep is typically in the low 100s, incurring a non-negligible cost (though usually less than a single training run of the target model). This high number partly owes to dependencies between HPs (shown in \Cref{sec:experiments:hp_independence}), making the search space hard to explore.

\subsection{Low-precision training} \label{sec:background:low_precision_training}

% Quantizing tensors into low-precision number formats during training offers substantial speedups.
All the major potential bottlenecks of model training---compute, communication and storage---see roughly linear improvements as the bit-width of their number format is reduced.
In modern LLM training, the compute cost of large matrix multiplications (matmuls) means that substantial gains are available if these can be done in low-precision ($<32$ bit) formats.
With the ending of Dennard scaling and Moore's law \citep{Moores_Law_A,Moores_Law_B}, the use of low-precision formats represents one of the most promising avenues towards increased efficiency in deep learning.

Recent AI hardware offers substantial acceleration for the 8-bit FP8 E4 and E5 formats. However the reduced range of these formats means that they cannot directly represent some values generated during training. Various methods have been introduced to address this, such as the per-tensor dynamic re-scaling in Transformer Engine \citep{Transformer_Engine}. However, this comes at the cost of added complexity and potential overheads. For a more in-depth treatment of low-precision formats, see \Cref{app:low_precision_and_its_trade_offs}.

\subsection{Unit Scaling} \label{sec:background:unit_scaling}

An alternative approach to low-precision training is Unit Scaling \citep{Unit_Scaling}, which also uses fine-grained scaling factors to control range, but instead finds these factors via an analysis of expected tensor statistics at initialization. These are fixed factors, calculated independently of the contents of a tensor, at the beginning of training. As such, the method is easy to use and only adds the overhead of applying static scaling factors (which we show to be negligible in \Cref{app:scaled_mm_benchmarking}).

These factors are chosen to ensure the unit variance of activations, weights and gradients at initialization.
% \footnote{We can also think of Unit Scaling as ensuring unit standard deviation, or even (approximately) unit root-mean-square as typically $\mu \ll \sigma$ and $\operatorname{RMS}^2 = \sigma^2 + \mu^2$.}
This is a useful criterion as it places values around the center of floating-point formats' absolute range. This applies to all tensors, meaning every operation in the network requires a scaling factor that ensures unit-scaled outputs, assuming unit-scaled inputs. Unit Scaling does not provide a mechanism for re-scaling tensors dynamically during training, but due to its ideal starting scale for gradients, activations and weights this may not be required. Empirically this is shown to be true across multiple architectures, though it is not guaranteed.

We provide an example of deriving the Unit Scaling rule for a matmul op in \Cref{app:additional_background:us}, resulting in the scaling factor: $1 / \sqrt{d_\fanin}$. We accompany this example with a full recipe for applying Unit Scaling to an arbitrary model.
