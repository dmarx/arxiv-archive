\section{A guide to using \umup} \label{app:using_umup_guide}

We bring together our \umup\ scheme presented in \Cref{sec:umup} to form a simple recipe for applying it to a model. The \umup\ scheme is designed and validated on a Llama-style architecture, so it may not be applicable or effective on other models, particularly those with substantially different architectures. Exploring this question is an important avenue for future work.

Before applying our scheme, users are encouraged to apply the following pre-requisites to their training setup, based on our analysis of effective \mut\ in \Cref{sec:challenges:mut}:

\begin{itemize}
    \item Remove trainable parameters from normalization layers
    \item Use the \textit{independent} form of AdamW
    \item Ensure training is in the under-fitting regime (i.e. avoid excessive data repetition)
\end{itemize}

Having done this, our recipe for using \umup\ is as follows:

\begin{enumerate}
    \item \textbf{Replace operations \& optimizers with \umup\ versions:}
    Each operation should be replaced by a unit-scaled version (these wrap the existing operations, with added static scales in the forward and backward passes). We have pre-calculated scales for common operations in \Cref{app:additional_unit_scaled_ops}. Parameters should be initialized with unit variance, and Adam(W) adjusted to use the scaling rules defined in \Cref{sec:umup:emb_lr_rule} (we refer to the optimizer as Adam in this section, but AdamW should be used if weight decay is required. Other optimizer scaling rules can be determined by the same process we outline). These features are all implemented in our library (see \Cref{app:us_lib_guide}).

    \item \textbf{Choose a set of HPs to sweep:}
    From the set of HPs outlined in \Cref{table:hp_sets}, select those to be swept. We recommend the extended set, though a basic LR sweep can be effective.
    
    \item \textbf{Decide on proxy model config:}
    The cost of proxy model training should be such that the sweeping process is much less than target model training, while still being as representative as possible. We base our recommendations on the results in \Cref{fig:lr_transfer}. In general, width is the most reliable feature to transfer. Training steps and batch size also give good transfer, so moderate changes here are permissible. Depth is the least reliable feature for transfer, so we only recommend modest changes in depth. We keep the number of warmup steps constant, but always decay to the same final LR when varying the number of steps.
    
    \item \textbf{Perform independent HP search:}
    Following the process outlined in \Cref{sec:experiments:hp_independence} and \Cref{app:umup_hp_search_algorithm}.
    
    \item \textbf{Train the target model:}
    This can be done in FP8 simply by placing casts on matmul inputs (though for our large-scale experiments we found the scales of two operations drifted enough over time that some lightweight dynamic re-scaling was required).
\end{enumerate}

The above functionality is provided in the Unit Scaling library, to avoid users having to implement it themselves, and to provide a reference implementation. We provide a guide to using this library in the following section.