\section{The cut-edge rule} \label{app:cut_edge_rule}

In the section we review the notion of \textit{constraints} used for scaling operations in a computational graph. For a more thorough, generalized treatment, please refer to Section 5.1 and Appendix E.4 of the original Unit Scaling paper \cite{Unit_Scaling}.

For simplicity, we will only discuss the cut-edge rule in the context of a typical neural network. For each operation $f$, parametrized by $\theta$ taking input $x$ and emitting output $y$, a user must choose how to scale $y$, $\nabla_x$ and $\nabla_\theta$ (gradient of loss w.r.t. $x$ and $\theta$ respectively). In the simplest case, where there are no further data dependencies, we can simply choose factors that preserve unit scale. In more complex scenarios, we must balance the need for each tensor to be unit-scaled and for gradients to be correct up to a constant factor.

In particular, a problem emerges in the presence of residual blocks in which $y = x + f(x; \theta)$. In these circumstances, $\nabla_x$ is computed as the sum of residual gradient $\nabla_f$ $\frac{\partial{f}}{\partial{x}}$ and  skip gradient $\nabla_y$. If we choose not to insert scaling factors into our graph, $\nabla_f$ $\frac{\partial{f}}{\partial{x}}$ and $\nabla_y$ will have some ratio of scale $r$. However, if we have chosen to rescale the gradient of operations in $f$, then $\nabla_f$ $\frac{\partial{f}}{\partial{x}}$ will have been rescaled by some $s$. This means the new ratio of $\nabla_f$ $\frac{\partial{f}}{\partial{x}}$ and $\nabla_y$ will be $r \cdot s$. Therefore,  when adding these together, $\nabla_x$ is no longer a correct gradient up to a constant factor.

How do you remedy this? If we can ensure that the scaling factors are the same for both the input gradients and outputs of an op, we will have $s=1$. This ensures that gradients for inputs to residual blocks are correct up to a constant factor.

How do you decide when you are free to preserve unit scale, and when to constrain scaling factors to be the same? We previously define the \textit{cut-edge rule} \cite{Unit_Scaling} for computational graphs where nodes represent forward pass operations and edges represent operation outputs. If an input edge is a \textit{cut-edge}, i.e., the number of connected components in the graph would increase upon deletion (examples in a typical transformer model: output of embedding gather, output of a residual add, output of final norm, output token logits, weights), there is no need to constrain the scales of the operation's output edge and the input edge gradient. For all other input edges (e.g., inputs to a residual add, intermediates computed along a residual branch), the scales of gradients and outputs should be constrained.