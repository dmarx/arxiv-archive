\section{Unit-scaled pre-norm residual layers}  \label{sec:us_residuals}

The popular pre-norm residual network architecture is simple to implement, but problematic to combine with Unit Scaling. It exhibits scale-growth in the skip-stream at initialization, due to the repeated addition of residual connections without subsequent normalization. Here we present a surprising and useful finding: that for any pre-norm model there exists a mathematically-equivalent model where this scale-growth is eliminated, through the careful re-scaling of residual connections.

Note that this section focuses on applying Unit Scaling to \textit{standard} pre-norm models. Only once we have addressed this problem are we able to do the same for \umup\ models, as shown in \Cref{subsec:residual_branch_multipliers}. Readers only interested in our final \umup\ residual implementation may skip ahead to \Cref{subsubsec:umup_residual_in_full}.

%\begin{enumerate}
    %\item \textbf{Unit scale:} They satisfy unit scaling, meaning that the variance at initialization throughout the skip-stream is 1.
    %\item \textbf{Interpretable HPs:} Each HP value determines a dynamic of the model at initialization that we consider important, giving them a clear interpretation.
    %\item \textbf{Fully expressive:} They result in a model which is as expressive as a standard residual network, meaning that for any model expressed by ~\Cref{eq:resnet_a,eq:resnet_b,eq:resnet_c}, there is a choice of HPs that forms a mathematically-equivalent \umup\ residual model.
%\end{enumerate}

% For post-norm models, which place normalisation layers in the skip-stream, Unit Scale is automatically satisfied, and interpretable HPs can be derived via a similar analysis to the one provided below. The \umup\ set of HPs is also specific to transformer models, and incorporates the scaling rules of \depthmup. We recommend the \umup\ residual scaling rules over those presented in \citet{Unit_Scaling} (which are themselves derived from \citet{NF_ResNets}).

% We begin by explaining how to unit-scale pre-norm residual networks, then outline our new set of HPs (first in abstract, and then the specific case of a transformer), and finally present the combination, which is our \umup\ residual scheme.

\subsection{Scale growth in pre-norm residual networks}

Let's consider a pre-norm residual network of depth $L$:
\begin{align}
    R_0(x) &= r_0 x, \label{eq:resnet_a}
    \\
    R_{l}(x) &= r_{l}f_l(R_{l-1}(x)) + R_{l-1}(x), \quad l=1,..,L \label{eq:resnet_b}
    \\
    R_{L+1}(x) &= f_{L+1}(R_L(x)) \label{eq:resnet_c}
\end{align}
with embedding multiplier $r_0$ and residual branch multipliers $r_l$ for $l=1,..,L$. To satisfy pre-norm, all $f_l$ are zero-homogeneous functions, i.e. $f_l(\lambda x) = f_l(x)$.
% Unit Scaling in a network is always ensured inductively, i.e. if the $l-1$-th layer satisfies Unit Scaling, then the same should be true for the $l$-th layer. For residual networks, this does not work, since \Cref{eq:resnet_b} gives the scale

The scale of the skip-stream at initialization as a result of \Cref{eq:resnet_b} is
\begin{align}
    \sigma(R_l) &= \sqrt{r_l^2 \sigma(f_l)^2 + \sigma(R_{l-1})^2}
    > \sigma(R_{l-1}), \quad l=1,..,L
    % = \sqrt{r_l^2 + 1} > 1,
    \label{eq:residual_non_unit_scale}
\end{align}
assuming $r_l^2 \sigma(f_l)^2 > 0$. This shows that scale inevitably grows with the addition of each residual layer.

This scale-growth is clearly incompatible with unit scaling, which aims for $\sigma(R_l) = 1$ for all $l=0,..,L+1$.
In the following we present an elegant solution to this problem making use of a symmetry transformation available in pre-norm residual architectures.

\subsection{Residual symmetry in pre-norm architectures}

To resolve the problem of scale shift in residual networks demonstrated by \Cref{eq:residual_non_unit_scale}, we try a slightly more general ansatz:
\begin{align}
    \hat{R}_0(x) &= x,  \label{eq:general_resnet_a}
    \\
    \hat{R}_{l}(x) &= a_l f_l(\hat{R}_{l-1}(x)) + b_l\hat{R}_{l-1}(x),  \label{eq:general_resnet_b}
    \\
    \hat{R}_{L+1}(x) &= f_{L+1}(\hat{R}_L(x)) \label{eq:general_resnet_c}
\end{align}
with coefficients $a_l, b_l$. We want to choose these coefficients so that the outputs of $\hat{R}_{l}$ are unit-scaled if the outputs $f_l, \hat{R}_{l-1}$ are. A similar calculation as in \Cref{eq:residual_non_unit_scale} leads to the sufficient condition
\begin{align}
    a_l^2 + b_l^2 = 1, \label{eq:a_b_sq_sum}
\end{align}
which can be easily satisfied. Having restored Unit Scale, we are faced with another issue. It seems that \Cref{eq:general_resnet_a,eq:general_resnet_b,eq:general_resnet_c} describe a different network than \Cref{eq:resnet_a,eq:resnet_b,eq:resnet_c}, whereas ideally the relation from input to final output should be unchanged when converting the network to Unit Scaling. 

Note that the coefficients $a_l, b_l$ are not uniquely defined yet, so our mathematical intuition tells us that we should find an additional constraint to get a unique solution. To find this constraint, let us consider our original residual network in \Cref{eq:resnet_a,eq:resnet_b,eq:resnet_c} and analyze how the variance propagates through the network if we assume all the $f_l$ satisfy Unit Scaling and $\sigma(x) = 1$. Let $\sigma_{l-1}^2$ denote the variance of $R_{l-1}$. Then a simple inductive calculation shows that
\begin{align*}
    \sigma_{l-1}^2 =  \sum_{i=0}^{l-1}r_i^2.
\end{align*}
By \Cref{eq:resnet_b} the output of $R_l$ adds a quantity of scale $r_l$ from the residual connection and a quantity of scale $\sigma_{l-1}$ from the skip connection. Intuitively, the \textit{ratio} of these scales should be more important for the overall network dynamics than their absolute values. Thus our constraint becomes preserving the ratio of scales from the original model, through our choice of $a_l, b_l$:
\begin{align*}
    \frac{a_l}{b_l} = \frac{\sigma(r_l f_l)}{\sigma_{l-1}} = \frac{r_l}{\sqrt{\sum_{i=0}^{l-1}r_i^2}} =: \tau_l,
\end{align*}
which, recalling \Cref{eq:a_b_sq_sum}, (up to sign) uniquely defines our multipliers $a_l, b_l$ as 
\begin{align}
    a_l = \frac{\tau_l}{\sqrt{\tau_l^2 + 1}}, \quad b_l = \frac{1}{\sqrt{\tau_l^2 + 1}}
\end{align}
In summary, we propose the modified residual network
\begin{align}
    \hat{R}_0(x) &= x, \label{eq:umup_resnet_a}
    \\
    \hat{R}_{l}(x) &= \frac{\tau_l}{\sqrt{\tau_l^2 + 1}}f_l(\hat{R}_{l-1}(x)) + \frac{1}{\sqrt{\tau_l^2 + 1}}\hat{R}_{l-1}(x), \label{eq:umup_resnet_b}
    \\
    \hat{R}_{L+1}(x) &= f_{L+1}(\hat{R}_L(x)),  \label{eq:umup_resnet_c}
    \\
    \tau^2_l &= \frac{r^2_l}{\sum_{i=0}^{l-1}r_i^2}. \label{eq:umup_resnet_d}
\end{align}
% Alternatively we can re-write \Cref{eq:umup_resnet_b} as
% \begin{align}
%     \hat{R}_{l}(x) &= \frac{r_l}{\sqrt{\sum_{i=0}^{l}r_i^2}}f_l(\hat{R}_{l-1}(x)) + \frac{\sqrt{\sum_{i=0}^{l-1}r_i^2}}{\sqrt{\sum_{i=0}^{l}r_i^2}}\hat{R}_{l-1}(x)
% \end{align}
Our main result of this section is that this network is indeed mathematically equivalent to the network defined in \Cref{eq:resnet_a,eq:resnet_b,eq:resnet_c}, under a simple additional structural assumption:
\begin{lem} \label{lem:residual_symmetry}
    Consider $R_l$, $\hat{R}_l$ defined as in \Cref{eq:resnet_b,eq:umup_resnet_b} respectively. Then $\hat{R}_l = R_l / \sqrt{\sum_{i=0}^l r_i^2}$ for all $l=0,..,L$.
\end{lem}
% Remarkably, this result holds independent of Unit Scaling. However, only under Unit Scaling can the factors $\tau_l$ be interpreted as the ratio of variances between skip and residual branch.
Remarkably, this result does not assume the individual network operations $f_l$ actually satisfy Unit Scaling. It is purely a consequence of the pre-norm residual structure. However, only under Unit Scaling can the factors $\tau_l$ be interpreted as the ratio of scales between skip and residual branch.

As a consequence of the lemma, the final residual output $R_L(x)$ is the same as in our original network up to a fixed multiplier. Due to the zero-homogeneity of the final output function $f_{L+1}$ this gives $\hat{R}_{L+1} = f_{L+1}\left(R_L(x)/\sqrt{\sum_{i=0}^l r_i^2}\right) = f_{L+1}(R_L(x)) = R_{L+1}$, proving the mathematical equivalence of our residual scheme.
% \begin{cor}
%      Functions given by a residual network and zero-homogeneous output layer are invariant under the reparametrization defined in~\eqref{eq:umup_resnet_a}, \eqref{eq:umup_resnet_b}, i.e. $F = \hat{F}$.
% \end{cor}
Modern LLM architectures like Llama~\citep{Llama} are pre-norm residual networks of this kind. Hence they admit a faithful unit-scaled reparametrization.

\subsection{Proof of Lemma~\ref{lem:residual_symmetry}}
\begin{proof}
    This is proved by induction. For the base-case $l=1$, we have $\tau_1 = r_1/r_0$, giving
    \begin{align*}
        \hat{R}_{1}(x) &= \frac{\tau_l}{\sqrt{\tau_l^2 + 1}}f_1(x) + \frac{1}{\sqrt{\tau^2_l + 1}}x
        \\
        &= (r_1f_1(x) + r_0x) / \sqrt{r_0^2 + r_1^2}
        \\
        &= R_1 / \sqrt{r_0^2 + r_1^2}.
    \end{align*}
    Then if the statement holds for $l-1$ we have
    \begin{align*}
        \hat{R}_{l}(x) &= \frac{\tau_l}{\sqrt{\tau^2_l + 1}}f_l(\hat{R}_{l-1}(x)) + \frac{1}{\sqrt{\tau^2_l + 1}}\hat{R}_{l-1}(x)
        \\
        &= \frac{r_l}{\sqrt{\sum_{i=0}^{l}r_i^2}}f_l(\hat{R}_{l-1}(x)) + \frac{\sqrt{ \sum_{i=0}^{l-1}r_i^2}}{\sqrt{\sum_{i=0}^{l}r_i^2}}\hat{R}_{l-1}(x)
        \\
        &= \left(r_l f_l(\hat{R}_{l-1}(x)) + \sqrt{\sum_{i=0}^{l-1}r_i^2} \hat{R}_{l-1}(x)\right) / \sqrt{\sum_{i=0}^{l}r_i^2}
        \\
        &= \left(r_l f_l({R}_{l-1}(x)) + \sqrt{\sum_{i=0}^{l-1}r_i^2} \frac{{R}_{l-1}(x)}{\sqrt{\sum_{i=0}^{l-1}r_i^2}}\right) / \sqrt{\sum_{i=0}^{l}r_i^2}
        \\
        &= \left(r_l f_l({R}_{l-1}(x)) + {R}_{l-1}(x)\right) / \sqrt{\sum_{i=0}^{l}r_i^2}
        \\
        &= R_l(x) / \sqrt{\sum_{i=0}^{l}r_i^2}
    \end{align*}
\end{proof}

\subsection{Unit Scaling for transformer residuals} \label{subsec:unit_scaled_transformer_residuals}

The above scheme describes Unit Scaling for arbitrary pre-norm residual networks. We now apply it to the case of pre-norm transformer residual layers.

We can describe a transformer in terms of the residual network given in \Cref{eq:resnet_a,eq:resnet_b,eq:resnet_c}. Our $f_l$ functions alternate between self-attention layers and feed-forward layers. Implementations differ in the handling of how residual multipliers $r_l$ correspond to HPs. In many cases practitioners simply ignore these $r_l$, but for the sake of expressivity we assume the two types of residual layer each have their own HP, as well as the embedding. In other words,

\begin{equation*}
    r_l = \begin{cases}
        \alpha_{\mathrm{emb}} & l = 0
        \\
        \alpha_{\mathrm{attn{\text -}residual}} & l \textrm{ is odd }
        \\
        \alpha_{\mathrm{ffn{\text -}residual}} & l \textrm{ is even, and } l > 0.
    \end{cases}
\end{equation*}

To convert this to a Unit Scaled network we apply \Cref{eq:umup_resnet_a,eq:umup_resnet_b,eq:umup_resnet_c,eq:umup_resnet_d}, from which can derive the following closed-form expression for $\tau_l$:

\begin{equation*}
    \tau^2_l = \begin{dcases}
        \frac{\alpha^2_{\mathrm{attn{\text -}residual}}}{\alpha^2_{\mathrm{emb}} + \ell \alpha^2_{\mathrm{attn{\text -}residual}} + \ell \alpha^2_{\mathrm{ffn{\text -}residual}}} & l \textrm{ is odd }
        \\
        \frac{\alpha^2_{\mathrm{ffn{\text -}residual}}}{\alpha^2_{\mathrm{emb}} + (\ell + 1) \alpha^2_{\mathrm{attn{\text -}residual}} + \ell \alpha^2_{\mathrm{ffn{\text -}residual}}} & l \textrm{ is even}.
    \end{dcases}
\end{equation*}

where $\ell = \lfloor{\frac{l-1}{2}}\rfloor$.

This gives us a unit-scaled pre-norm residual implementation for a \textit{standard} transformer, which is mathematically equivalent to a non-unit-scaled version. In the next section we augment this by adding in two HPs, in a carefully-designed manner that satisfies our criteria for \umup\ HPs, giving us our full residual implementation.
