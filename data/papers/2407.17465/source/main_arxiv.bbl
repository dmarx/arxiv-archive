\begin{thebibliography}{10}

\bibitem{Tensor_Programs_IV}
Greg Yang and Edward~J. Hu.
\newblock Tensor programs {IV:} {Feature} learning in infinite-width neural networks.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of {\em Proceedings of Machine Learning Research}, pages 11727--11737. {PMLR}, 2021.

\bibitem{Tensor_Programs_V}
Greg Yang, Edward~J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs {V:} {Tuning} large neural networks via zero-shot hyperparameter transfer.
\newblock {\em CoRR}, abs/2203.03466, 2022.

\bibitem{Cerebras_GPT}
Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness.
\newblock {Cerebras-GPT}: Open compute-optimal language models trained on the cerebras wafer-scale cluster.
\newblock {\em CoRR}, abs/2304.03208, 2023.

\bibitem{BTLM}
Nolan Dey, Daria Soboleva, Faisal Al{-}Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming Chen, Robert Myers, Jacob~Robert Steeves, Natalia Vassilieva, Marvin Tom, and Joel Hestness.
\newblock {BTLM-3B-8K:} {7B} parameter performance in a {3B} parameter model.
\newblock {\em CoRR}, abs/2309.11568, 2023.

\bibitem{LLM360}
Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi~Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric~P. Xing.
\newblock {LLM360:} {Towards} fully transparent open-source {LLMs}.
\newblock {\em CoRR}, abs/2312.06550, 2023.

\bibitem{MiniCPM}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen~Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock {MiniCPM}: Unveiling the potential of small language models with scalable training strategies.
\newblock {\em CoRR}, abs/2404.06395, 2024.

\bibitem{GPT4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock {\em CoRR}, abs/2303.08774, 2023.

\bibitem{Grok}
xAI.
\newblock Grok-1.
\newblock \url{https://github.com/xai-org/grok-1}, 2024.

\bibitem{Exploration_Of_Mu_Transfer}
Lucas~D. Lingle.
\newblock A large-scale exploration of {\(\mu\)}-transfer.
\newblock {\em CoRR}, abs/2404.05728, 2024.

\bibitem{Falcon}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'{e}}rouane Debbah, {\'{E}}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.
\newblock The {Falcon} series of open language models.
\newblock {\em CoRR}, abs/2311.16867, 2023.

\bibitem{Unit_Scaling}
Charlie Blake, Douglas Orr, and Carlo Luschi.
\newblock Unit scaling: Out-of-the-box low-precision training.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 2548--2576. {PMLR}, 2023.

\bibitem{us_library}
Graphcore.
\newblock Unit scaling.
\newblock \url{https://github.com/graphcore-research/unit-scaling}, 2023.

\bibitem{NTK}
Arthur Jacot, Cl{\'{e}}ment Hongler, and Franck Gabriel.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol{\`{o}} Cesa{-}Bianchi, and Roman Garnett, editors, {\em Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada}, pages 8580--8589, 2018.

\bibitem{Tensor_Programs_VI}
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.
\newblock Tensor programs {VI:} {Feature} learning in infinite-depth neural networks.
\newblock {\em CoRR}, abs/2310.02244, 2023.

\bibitem{Moores_Law_A}
Thomas~N. Theis and H.{-}S.~Philip Wong.
\newblock The end of {Moore's} law: {A} new beginning for information technology.
\newblock {\em Comput. Sci. Eng.}, 19(2):41--50, 2017.

\bibitem{Moores_Law_B}
Hadi Esmaeilzadeh, Emily~R. Blem, Ren{\'{e}}e~St. Amant, Karthikeyan Sankaralingam, and Doug Burger.
\newblock Dark silicon and the end of multicore scaling.
\newblock In Ravi~R. Iyer, Qing Yang, and Antonio Gonz{\'{a}}lez, editors, {\em 38th International Symposium on Computer Architecture {(ISCA} 2011), June 4-8, 2011, San Jose, CA, {USA}}, pages 365--376. {ACM}, 2011.

\bibitem{Transformer_Engine}
NVIDIA.
\newblock {Transformer Engine}.
\newblock \url{https://github.com/NVIDIA/TransformerEngine}, 2024.

\bibitem{Small_Scale_Proxies}
Mitchell Wortsman, Peter~J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John~D. Co{-}Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl{-}Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock {\em CoRR}, abs/2309.14322, 2023.

\bibitem{AdamW_Weight_Decay}
Xi~Wang and Laurence Aitchison.
\newblock How to set adamw's weight decay as you scale model and dataset size.
\newblock {\em CoRR}, abs/2405.13698, 2024.

\bibitem{Mup_Library}
Microsoft.
\newblock Maximal update parametrization ($\mu${P}) and hyperparameter transfer ($\mu${T}ransfer).
\newblock \url{https://github.com/microsoft/mup}, 2024.

\bibitem{Llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie{-}Anne Lachaux, Timoth{\'{e}}e Lacroix, Baptiste Rozi{\`{e}}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur{\'{e}}lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em CoRR}, abs/2302.13971, 2023.

\bibitem{WikiText103}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In {\em 5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.

\bibitem{SlimPajama}
Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric~P. Xing.
\newblock {SlimPajama-DC}: Understanding data combinations for {LLM} training.
\newblock {\em CoRR}, abs/2309.10818, 2023.

\bibitem{Pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der Wal.
\newblock Pythia: A suite for analyzing large language models across training and scaling, 2023.

\bibitem{LLAMA3}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian~Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric~Michael Smith, Filip Radenovic, and Frank~Zhang et~al.
\newblock The llama 3 herd of models, 2024.

\bibitem{Training_And_Inference_Using_8_Bit}
Sergio~P. Perez, Yan Zhang, James Briggs, Charlie Blake, Josh Levy{-}Kramer, Paul Balanca, Carlo Luschi, Stephen Barlow, and Andrew Fitzgibbon.
\newblock Training and inference of large language models using 8-bit floating point.
\newblock {\em CoRR}, abs/2309.17224, 2023.

\bibitem{DNNs_With_8_Bit}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia{-}Yu Chen, and Kailash Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol{\`{o}} Cesa{-}Bianchi, and Roman Garnett, editors, {\em Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada}, pages 7686--7695, 2018.

\bibitem{Mixed_Precision_8_Bit}
Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul.
\newblock Mixed precision training with 8-bit floating point.
\newblock {\em CoRR}, abs/1905.12334, 2019.

\bibitem{FP8-LM}
Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze~Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng.
\newblock {FP8-LM:} training {FP8} large language models.
\newblock {\em CoRR}, abs/2310.18313, 2023.

\bibitem{MX1}
Bita~Darvish Rouhani, Ritchie Zhao, Venmugil Elango, Rasoul Shafipour, Mathew Hall, Maral Mesmakhosroshahi, Ankit More, Levi Melnick, Maximilian Golub, Girish Varatkar, Lai Shao, Gaurav Kolhe, Dimitry Melts, Jasmine Klar, Renee L'Heureux, Matt Perry, Doug Burger, Eric~S. Chung, Zhaoxia~(Summer) Deng, Sam Naghshineh, Jongsoo Park, and Maxim Naumov.
\newblock With shared microexponents, a little shifting goes a long way.
\newblock In Yan Solihin and Mark~A. Heinrich, editors, {\em Proceedings of the 50th Annual International Symposium on Computer Architecture, {ISCA} 2023, Orlando, FL, USA, June 17-21, 2023}, pages 83:1--83:13. {ACM}, 2023.

\bibitem{MX2}
Bita~Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, Dusan Stosic, Venmugil Elango, Maximilian Golub, Alexander Heinecke, Phil James{-}Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao, Michael~Y. Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verilli, Ralph Wittig, Doug Burger, and Eric~S. Chung.
\newblock Microscaling data formats for deep learning.
\newblock {\em CoRR}, abs/2310.10537, 2023.

\bibitem{BitNet1}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi~Wu, and Furu Wei.
\newblock Bitnet: Scaling 1-bit transformers for large language models.
\newblock {\em CoRR}, abs/2310.11453, 2023.

\bibitem{BitNet2}
Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li~Dong, Ruiping Wang, Jilong Xue, and Furu Wei.
\newblock The era of 1-bit {LLMs}: All large language models are in 1.58 bits.
\newblock {\em CoRR}, abs/2402.17764, 2024.

\bibitem{BitNet3}
Rui{-}Jie Zhu, Yu~Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, and Jason~K. Eshraghian.
\newblock Scalable matmul-free language modeling.
\newblock {\em CoRR}, abs/2406.02528, 2024.

\bibitem{Scaling_Vision_Transformers}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos~Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin~Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey~A. Gritsenko, Vighnesh Birodkar, Cristina~Nader Vasconcelos, Yi~Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah~J. Harmsen, and Neil Houlsby.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 7480--7512. {PMLR}, 2023.

\bibitem{Palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur{-}Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier{-}Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em J. Mach. Learn. Res.}, 24:240:1--240:113, 2023.

\bibitem{Stable_And_Low_Precision_Training}
Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt.
\newblock Stable and low-precision training for large-scale vision-language models.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, {\em Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023.

\bibitem{Intriguing_Properties}
Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Stephen~Zhen Gou, Phil Blunsom, Ahmet {\"{U}}st{\"{u}}n, and Sara Hooker.
\newblock Intriguing properties of quantization at scale.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, {\em Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023.

\bibitem{Dynamics_Of_Diffusion_Models}
Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock {\em CoRR}, abs/2312.02696, 2023.

\bibitem{LLM_INT8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Gpt3.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, {\em Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem{SmoothQuant}
Guangxuan Xiao, Ji~Lin, Micka{\"{e}}l Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 38087--38099. {PMLR}, 2023.

\bibitem{Quantizable_Transformers}
Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
\newblock Quantizable transformers: Removing outliers by helping attention heads do nothing.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, {\em Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023.

\bibitem{Massive_Activations}
Mingjie Sun, Xinlei Chen, J.~Zico Kolter, and Zhuang Liu.
\newblock Massive activations in large language models.
\newblock {\em CoRR}, abs/2402.17762, 2024.

\bibitem{Vision_Transformers_Need_Registers}
Timoth{\'{e}}e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski.
\newblock Vision transformers need registers.
\newblock In {\em The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.

\bibitem{Modula}
Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, and Jeremy Bernstein.
\newblock Scalable optimization in the modular norm.
\newblock {\em CoRR}, abs/2405.14813, 2024.

\bibitem{Scaling_Exponents}
Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander~A. Alemi, Roman Novak, Peter~J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie~Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington.
\newblock Scaling exponents across parameterizations and optimizers, 2024.

\bibitem{Sparse_Mup}
Nolan Dey, Shane Bergsma, and Joel Hestness.
\newblock Sparse maximal update parameterization: {A} holistic approach to sparse training dynamics.
\newblock {\em CoRR}, abs/2405.15743, 2024.

\bibitem{Compute_Better_Spent}
Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, and Andrew~Gordon Wilson.
\newblock Compute better spent: Replacing dense layers with structured matrices.
\newblock {\em CoRR}, abs/2406.06248, 2024.

\bibitem{Independent_WD}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em 7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem{Zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock {ZeRO}: {Memory} optimizations toward training trillion parameter models.
\newblock In Christine Cuicchi, Irene Qualters, and William~T. Kramer, editors, {\em Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020}, page~20. {IEEE/ACM}, 2020.

\bibitem{Flash_Attention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'{e}}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, {\em Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem{GLU}
Noam Shazeer.
\newblock {GLU} variants improve transformer.
\newblock {\em CoRR}, abs/2002.05202, 2020.

\bibitem{SiLU}
Chao Yu and Zhiguo Su.
\newblock Symmetrical {Gaussian} error linear units ({SGELUs}).
\newblock {\em CoRR}, abs/1911.03925, 2019.

\bibitem{Swish}
Prajit Ramachandran, Barret Zoph, and Quoc~V. Le.
\newblock Searching for activation functions.
\newblock In {\em 6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings}. OpenReview.net, 2018.

\bibitem{RoPE}
Jianlin Su, Murtadha H.~M. Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock {\em Neurocomputing}, 568:127063, 2024.

\bibitem{RMS_Norm}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pages 12360--12371, 2019.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K{\"{o}}pf, Edward~Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pages 8024--8035, 2019.

\bibitem{Tensor_Programs_I}
Greg Yang.
\newblock Tensor programs {I:} {Wide} feedforward or recurrent neural networks of any architecture are {Gaussian} processes.
\newblock {\em CoRR}, abs/1910.12478, 2019.

\bibitem{Tensor_Programs_II}
Greg Yang.
\newblock Tensor programs {II:} {Neural} tangent kernel for any architecture.
\newblock {\em CoRR}, abs/2006.14548, 2020.

\bibitem{Tensor_Programs_IIb}
Greg Yang and Etai Littwin.
\newblock Tensor programs {IIb}: Architectural universality of neural tangent kernel training dynamics.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of {\em Proceedings of Machine Learning Research}, pages 11762--11772. {PMLR}, 2021.

\bibitem{Tensor_Programs_III}
Greg Yang.
\newblock Tensor programs {III:} {Neural} matrix laws.
\newblock {\em CoRR}, abs/2009.10685, 2020.

\bibitem{Tensor_Programs_IVb}
Greg Yang and Etai Littwin.
\newblock Tensor programs {IVb}: Adaptive optimization in the infinite-width limit.
\newblock {\em CoRR}, abs/2308.01814, 2023.

\bibitem{Spectral_Condition}
Greg Yang, James~B. Simon, and Jeremy Bernstein.
\newblock A spectral condition for feature learning.
\newblock {\em CoRR}, abs/2310.17813, 2023.

\bibitem{Supar}
Nolan Dey, Shane Bergsma, and Joel Hestness.
\newblock Sparse maximal update parameterization: {A} holistic approach to sparse training dynamics.
\newblock {\em CoRR}, abs/2405.15743, 2024.

\bibitem{IEEE_754}
{IEEE Computer Society}.
\newblock {IEEE} standard for floating-point arithmetic.
\newblock pages 1--84, July 2019.

\bibitem{HFP8}
Xiao Sun, Jungwook Choi, Chia{-}Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan.
\newblock Hybrid 8-bit floating point {(HFP8)} training and inference for deep neural networks.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pages 4901--4910, 2019.

\bibitem{8_Bit_Numerical_Formats}
Badreddine Noune, Philip Jones, Daniel Justus, Dominic Masters, and Carlo Luschi.
\newblock 8-bit numerical formats for deep neural networks.
\newblock {\em CoRR}, abs/2206.02915, 2022.

\bibitem{FP8_Formats}
Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart~F. Oberman, Mohammad Shoeybi, Michael~Y. Siu, and Hao Wu.
\newblock {FP8} formats for deep learning.
\newblock {\em CoRR}, abs/2209.05433, 2022.

\bibitem{Mixed_Precision}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory~F. Diamos, Erich Elsen, David Garc{\'{\i}}a, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu.
\newblock Mixed precision training.
\newblock In {\em 6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.

\bibitem{Megatron-LM_Cluster}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.
\newblock Efficient large-scale language model training on {GPU} clusters using {Megatron-LM}.
\newblock In Bronis~R. de~Supinski, Mary~W. Hall, and Todd Gamblin, editors, {\em International Conference for High Performance Computing, Networking, Storage and Analysis, {SC} 2021, St. Louis, Missouri, USA, November 14-19, 2021}, page~58. {ACM}, 2021.

\bibitem{OpenSeq2Seq}
Oleksii Kuchaiev, Boris Ginsburg, Igor Gitman, Vitaly Lavrukhin, Carl Case, and Paulius Micikevicius.
\newblock {OpenSeq2Seq}: {Extensible} toolkit for distributed and mixed precision training of sequence-to-sequence models.
\newblock {\em CoRR}, abs/1805.10387, 2018.

\bibitem{FP8_Transformer_Engine}
NVIDIA.
\newblock Using {FP8} with transformer engine.
\newblock \url{https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html}, 2024.

\end{thebibliography}
