{
  "arxivId": "2407.17465",
  "title": "u-$\u03bc$P: The Unit-Scaled Maximal Update Parametrization",
  "authors": "Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Bj\u00f6rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, Douglas Orr",
  "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal\nhyperparameters (HPs) of a model independent of its size, allowing them to be\nswept using a cheap proxy model rather than the full-size target model. We\npresent a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with\nUnit Scaling, a method for designing models that makes them easy to train in\nlow-precision. The two techniques have a natural affinity: $\\mu$P ensures that\nthe scale of activations is independent of model size, and Unit Scaling ensures\nthat activations, weights and gradients begin training with a scale of one.\nThis synthesis opens the door to a simpler scheme, whose default values are\nnear-optimal. This in turn facilitates a more efficient sweeping strategy, with\nu-$\\mu$P models reaching a loss that is equal to or lower than comparable\n$\\mu$P models and working out-of-the-box in FP8.",
  "url": "https://arxiv.org/abs/2407.17465",
  "issue_number": 76,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/76",
  "created_at": "2024-12-25T08:53:20.576075",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-19T21:12:13.293Z"
}