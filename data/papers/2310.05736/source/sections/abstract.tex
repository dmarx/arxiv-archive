% The remarkable capabilities of large language models (LLMs) have led to their rapid implementation across various applications.
% The remarkable capabilities of large language models (LLMs) have resulted in their swift adoption in a wide range of applications.
Large language models (LLMs) have been applied in various applications due to their astonishing capabilities.
% However, 
With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts % LLMs need to handle
fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens.
To accelerate model inference and reduce cost, 
this paper presents \textit{\sysname{}},
a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. 
% that compresses prompts iteratively alongside a demonstration-level budget controller,
% \textcolor{blue}{It can achieve up to a 20x prompt compression ratio.
% This approach employs a smaller language model, aware of black-box LLMs, to assess the original prompt's token-level perplexity and compress the corresponding prompt accordingly. Evaluation results across four scenarios - Chat, Summarization, ICL, and Reasoning - showcase the generalizability of this method and its ability to maintain the reasoning and ICL capabilities of LLMs.}
We conduct experiments and analysis over four datasets from different scenarios, \ie, GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss.\footnote{Our code is available at \url{https://aka.ms/LLMLingua}.}