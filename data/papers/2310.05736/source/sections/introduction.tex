\section{Introduction}

% 1. prompt compression 的必要性；随着LLMs技术的发展，越来越多的场景需要long prompt，举例子：chat context,
% 
% 2. Prompt compression 的合理性；LLMs language
% 3. 现有工作总结和缺点，select context，引出我们的方法
% 4. 总结Contribution

% 随着ChatGPT的风靡，无数个场景被LLMs强大的泛化能力和推理能力所改变。在实际应用场景中往往需要设计精心设计对应的prompt，例如使用Chain-of-thought, In-context learning, retrieve related document, chat conversation,这些操作能激活LLMs关于特定domain和问题的知识,使得生成的结果效果非常好，但也同样意味着需要更长的prompt。
% 如何平衡LLMs所需要的huge 计算量和对更长 prompt需求之间的关系，成为了当前急切需要被解决的一个问题。
% 模型推理加速，例如模型量化，模型压缩，系统优化等是其实一种方式.但这些方法都需要依赖白盒 LLMs.


% With the popularity of ChatGPT, numerous scenarios have been transformed by the powerful generalization and reasoning capabilities of large language models (LLMs). In practical applications, it is often necessary to meticulously design corresponding prompts, employing techniques such as Chain-of-thought, In-context learning, retrieving related documents, or chat conversations.
The widespread adoption of ChatGPT has transformed numerous scenarios by harnessing the powerful generalization and reasoning capabilities of large language models (LLMs).
In practical applications, crafting suitable prompts is crucial and usually involves techniques such as chain-of-thought, in-context learning, and retrieving related documents or historical conversations~\cite{wei2022chain,Chase_LangChain_2022}. 
% These methods activate LLMs' knowledge about specific domains and questions, resulting in highly effective generated responses.
While these methods can elicit highly effective generations by activating LLMs' domain-specific knowledge, they often require longer prompts. 
% However, this also necessitates the use of longer prompts.
Therefore, striking a balance between the massive computational demands of LLMs and the need for longer prompts has become an urgent issue. %  that requires attention.
Some studies attempt to accelerate model inference by modifying the parameters of LLMs through quantization \cite{dettmers2022gptint,xiao2022smoothquant}, compression~\cite{frantar2023sparsegpt}, etc.
However, these approaches may be not suitable when the LLMs can be accessed via APIs only.
% One approach to addressing this issue is model inference acceleration, which includes techniques like model quantization, model compression, and system optimization. However, these methods all depend on white-box LLMs.


% 另外一方面，尝试在保留信息的同时，缩短prompt成为了自然而然的想法。从信息论角度，实际上自然语言是极具冗余的，也存在着压缩空间。而大模型能很好的理解压缩之后的信息。从信息熵角度来看，ppl越低的token对于Language model而言，其信息熵增益较小，去除这些token对于LLM理解上下文影响较小。
%On the other hand,
% Approaches that attempt to shorten the original prompts while preserving necessary information have emerged recently.
Approaches that attempt to reduce the length of original prompts while preserving essential information have emerged lately. % \cite{li2023unlocking, Chase_LangChain_2022, zhang2023mlcopilot}. %as a natural approach.
These approaches are grounded in the concept that natural language is inherently redundant~\cite{shannon1951prediction} and thus can be compressed. 
% From the perspective of information theory, natural language is inherently redundant~\cite{shannon1951prediction}, providing ample opportunities for compression.
\citet{gilbert2023semantic} also indicate that LLMs can effectively reconstruct source code from compressed text descriptions while maintaining a high level of functional accuracy.
% Moreover, large models are capable of effectively understanding the compressed information~\cite{gilbert2023semantic}
% \footnote{\href{https://gist.github.com/VictorTaelin/d293328f75291b23e203e9d9db9bd136}{https://gist.github.com/VictorTaelin/d293328f75291b23e2 03e9d9db9bd136}}
% Previous work on prompt compression largely necessitates fine-tuning models, and in some cases, even requires full-scale fine-tuning of LLMs, which makes practical application challenging.
Therefore, we follow this line of studies to compress a long prompt into a shorter one without any gradient flow through the LLMs to support applications based on a larger range of LLMs.


In terms of information entropy, tokens with lower perplexity (PPL) contribute less to the overall entropy gains of the language model.
In other words, removing tokens with lower perplexity has a relatively minor impact on the LLM's comprehension of the context.
Motivated by this, \citet{li2023unlocking} propose Selective-Context, which first employs a small language model to compute the self-information of each lexical unit (such as sentences, phrases, or tokens) in original prompts, and then drops the less informative content for prompt compression.
%conditional relationships between lexical units 
However, this method not only ignores the interdependence between the compressed contents but also neglects the correspondence between the LLM being targeted and the small language model used for prompt compression. %, both of which are important factors that could impact its effectiveness.
% disregards the alignment between the target LLM and the small base language model.


% 本文提出一种基于ppl的interative prompt compression算法，\sysname{}.来解决上述问题。其利用一个小的语言模型计算ppl估计给定prompt对于black-box LLMs的重要程度，共分为Budget Controller, Interative Prompt Compression 和Alignment三个模块。其中Budget Controller负责给prompt中不同成分分配不同压缩率，并丢弃一些ppl较少的demonstration和sentence以达到更高的压缩率。Interative Prompt Compression则通过将prompt按segment为粒度依次计算ppl进行压缩，从而缓解drop token间dependcy的问题，从而避免丢失一些低频但是在当前prompt多次出现的keywords。此外我们还引入Alignment，用于对齐small language model 与black-box LLMs的distrubution。

% In this paper, we propose an iterative prompt compression algorithm based on perplexity(PPL), \textit{\sysname{}}, to address the aforementioned issues.
% which utilizes a small language model to estimate the importance of each token in a given prompt for the target LLMs.
This paper proposes \textit{\sysname{}},
a coarse-to-fine prompt compression method, to address the aforementioned issues.
Specifically, we first present a budget controller to dynamically allocate different compression ratios to various components in original prompts such as the instruction, demonstrations, and the question, and meanwhile, perform coarse-grained, demonstration-level compression to maintain semantic integrity under high compression ratios.
We further introduce a token-level iterative  algorithm for fine-grained prompt compression.
Compared with \textit{Selective Context}, it can better preserve the key information within the prompt by taking into account the conditional dependencies between tokens.
% It consists of three modules: a budget controller, Iterative Prompt Compression, and Alignment.
% The Budget Controller is responsible for allocating different compression ratios to different components(such as instruction, demonstrations, and question) of the prompt, and discarding some demonstrations and sentences with lower PPL to achieve higher compression ratio.
% Iterative Prompt Compression compresses the prompt by calculating PPL for each segment in sequence, thereby mitigating the dependency issue when dropping tokens and preventing the loss of low-frequency keywords that appear multiple times in the prompt.
Additionally, we pose the challenge of distribution discrepancy between the target LLM and the small language model used for prompt compression, and further propose an instruction tuning based method to align the distribution of both language models.
% Additionally, we introduce Alignment to align the distribution of the small language model with that of the black-box LLMs.

We validate the effectiveness of our approach on four datasets from different domains, \ie, GSM8K and BBH for reasoning and ICL, ShareGPT for conversation, and Arxiv-March23 for summarization.
The results show that our method yields state-of-the-art performance across the board.
Furthermore, we conduct extensive experiments and discussions to analyze why our approach attains superior performance.
To our best knowledge, we are the first to evaluate reasoning and ICL capabilities in the domain of efficient LLMs.

% Our major contributions can be summarized as:
% \begin{itemize}
%     \item We present a budget controller to dynamically allocate different compression ratios to various components in original prompts, and perform demonstration-level compression to maintain semantic integrity under high compression ratios.
%     % drop some demonstrations or sentences based on PPL, thereby achieving higher compression ratios.
%     \item We introduce a token-level iterative prompt compression algorithm, % based on PPL, 
%     which can better preserve the key information within the prompt by taking into account the conditional dependencies between tokens. % compressed tokens.
%     % which considers the conditional dependencies between compressed tokens and better preserves the key information within the prompt.
%     \item We pose the challenge of distribution discrepancy between the target LLM and the small language model used for prompt compression, and further propose an instruction tuning based method to align both.
%     % We incorporate Alignment, enabling the small language model to perceive black-box LLMs and attempt to align with a closer distribution.
%     \item We validate the effectiveness of our approach on four datasets from different domains, \ie, GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that our method yields state-of-the-art performance across the board.
%     \item We conduct extensive experiments and discussions to analyze why our approach attains superior performance.
%     To our best knowledge, we are the first to evaluate reasoning and ICL capabilities in the domain of efficient LLMs.
%     % \item To the best of our knowledge, we are the first to evaluate reasoning and ICL capabilities within the domain of efficient LLMs. Our experiments show that we can attain up to a 20x compression ratio with only a 1.5 performance drop.
%     % \item Furthermore, we investigate the relationship between generation length and compression ratio, as well as analyze the reasoning information present in the recovered compressed prompts using LLMs.
% \end{itemize}
% 我们提出Budget Controller用于动态分配给不同成分不同压缩比，并根据ppl drop 部分demonstration或sentence，从而达到更高的压缩比；
% 我们提出一种token-level 基于ppl的Iterative prompt compression算法，其考虑compressed token间的条件依赖，能够更好的保留prompt内的关键信息；
% 我们引入Alignment，使得small language model能够感知black-box LLMs，并尝试更为接近的distrubution。
% 根据我们的知识，我们是第一个在高效LLMs领域内评测Reasoning和ICL能力的工作。且实验表明，我们最多能做到20x压缩率且仅drop 1.5 performance。
% 此外，我们还对被压缩prompt进行恢复，分析了generation 长度与compression ratio之间的关系

% This can be seen as a method that uses the Out-of-Distribution (OoD) Detection method to detect the OD in the original prompt for the purpose of prompt compression.

% Motivation: 
% 1. Context in prompt is huge and expensive;
% 2. 
% Prompt Compression 

% Description of high PPL => more knowledge (the basic idea of this paper should make sense.)


% Motivation for the design of budget controller (two perspectives)
% 两个功能
% i) 不同成分的 contribution 不同
% ii) 如何去应对比较大的 compression ratio: 信息丢失严重。Controller: 尽量保证留下来的 content 的语义。
% This module serves a dual purpose:
% % 使得我们可以执行更加 fine-grained?(个性化的；对症下药的) 预算分配：为不同的 prompt component 分配不同的预算，
% firstly, it addresses the differing tolerance levels for information loss among distinct prompt components, necessitating fine-grained control in accordance with the target compression ratio; 
% % 其次，在目标压缩率和困惑的基础上，通过消除信息含量较低的演示或句子级信息，避免了因压缩率过高而导致的重要信息丢失。
% secondly, it averts the loss of vital information from the original text due to excessive compression rates by eliminating demonstrations or sentence-level information with lower informational content, based on the target compression rate and perplexity. 
% This approach allows for achieving higher compression rates while preserving a certain degree of linguistic integrity.

% Some paragraphs to motivate the writing:
% i) Prompting is now the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and re-encoding the same prompt is computationally inefficient.
% ii) We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general sentiments can be retained with surprisingly few parameters, which can be useful in the context of decode-time algorithms for controllability and toxicity reduction. We explore contrastive conditioning to steer language model generation towards desirable text and away from undesirable text, and find that some complex prompts can be effectively compressed into a single token to guide generation. We also show that compressed prompts are largely compositional, and can be constructed such that they can be used to control independent aspects of generated text.

% We utilize GSM8K and BBH to evaluate the inference capability and ICL capacity of the compressed prompts. Both of these tasks are highly challenging inference tasks, where small LLMs fall significantly short compared to larger LLMs. For instance, Alpaca-7B achieves only an 11.0 score on GSM8K. To attain higher performance, previous state-of-the-art (SoTA) methods typically employ complex, multi-step CoT prompts designed manually. However, this also implies an increase in prompt length. For example, the intricate 9-step CoT prompt with 2366 tokens designed by \citet allows GPT-3.5-Turbo to achieve a 3.95 improvement on the GSM8K dataset.

% 我们利用GSM8K, BBH 来评测压缩后的prompt的推理能力和ICL能力。这两个任务都是十分困难的推理任务，small LLMs远弱于LLMs，例如Alpaca-7B在GSM8K只有11.0. 为达到更高的performance，之前的SoTA方法普遍使用人工进行设计的复杂多步CoT prompt，但这也意味着prompt长度会随之增加，例如\citet设计的长达2366个token的9步复杂CoT prompt 能让gpt-3.5-Turbo在GSM8K数据集上提升3.95.