% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{arora2021types,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Arora, Udit  and
Huang, William  and
He, He},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.835},
 pages = {10687--10701},
 publisher = {Association for Computational Linguistics},
 title = {Types of Out-of-Distribution Texts and How to Detect Them},
 url = {https://aclanthology.org/2021.emnlp-main.835},
 year = {2021}
}

@article{mai2022self,
 author = {Mai, Kimberly T and Davies, Toby and Griffin, Lewis D},
 journal = {ArXiv preprint},
 title = {Self-Supervised Losses for One-Class Textual Anomaly Detection},
 url = {https://arxiv.org/abs/2204.05695},
 volume = {abs/2204.05695},
 year = {2022}
}

@inproceedings{wu2023multi,
 author = {Wu, Qianhui  and
Jiang, Huqiang  and
Yin, Haonan and
Karlsson, B\"orje F. and
Lin, Chin-Yew},
 booktitle = {Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Long Papers)},
 title = {Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text},
 year = {2023}
}

@inproceedings{wingate-etal-2022-prompt,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Wingate, David  and
Shoeybi, Mohammad  and
Sorensen, Taylor},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
 pages = {5621--5634},
 publisher = {Association for Computational Linguistics},
 title = {Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models},
 url = {https://aclanthology.org/2022.findings-emnlp.412},
 year = {2022}
}

@article{yang2023inference,
 author = {Yang, Nan and Ge, Tao and Wang, Liang and Jiao, Binxing and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Inference with reference: Lossless acceleration of large language models},
 url = {https://arxiv.org/abs/2304.04487},
 volume = {abs/2304.04487},
 year = {2023}
}

@article{mu2023learning,
 author = {Mu, Jesse and Li, Xiang Lisa and Goodman, Noah},
 journal = {ArXiv preprint},
 title = {Learning to compress prompts with gist tokens},
 url = {https://arxiv.org/abs/2304.08467},
 volume = {abs/2304.08467},
 year = {2023}
}

@article{ge2022extensible,
 author = {Ge, Tao and Hu, Jing and Dong, Li and Mao, Shaoguang and Xia, Yan and Wang, Xun and Chen, Si-Qing and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Extensible Prompts for Language Models},
 url = {https://arxiv.org/abs/2212.00616},
 volume = {abs/2212.00616},
 year = {2022}
}

@article{chevalier2023adapting,
 author = {Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
 journal = {ArXiv preprint},
 title = {Adapting Language Models to Compress Contexts},
 url = {https://arxiv.org/abs/2305.14788},
 volume = {abs/2305.14788},
 year = {2023}
}

@article{ge2023context,
 author = {Ge, Tao and Hu, Jing and Wang, Xun and Chen, Si-Qing and Wei, Furu},
 journal = {ArXiv preprint},
 title = {In-context Autoencoder for Context Compression in a Large Language Model},
 url = {https://arxiv.org/abs/2307.06945},
 volume = {abs/2307.06945},
 year = {2023}
}

@article{li2023unlocking,
 author = {Li, Yucheng},
 journal = {ArXiv preprint},
 title = {Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering},
 url = {https://arxiv.org/abs/2304.12102},
 volume = {abs/2304.12102},
 year = {2023}
}

@article{zhou2023efficient,
 author = {Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cotterell, Ryan and Sachan, Mrinmaya},
 journal = {ArXiv preprint},
 title = {Efficient Prompting via Dynamic In-Context Learning},
 url = {https://arxiv.org/abs/2305.11170},
 volume = {abs/2305.11170},
 year = {2023}
}

@inproceedings{yang2019xlnet,
 author = {Zhilin Yang and
Zihang Dai and
Yiming Yang and
Jaime G. Carbonell and
Ruslan Salakhutdinov and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/YangDYCSL19.bib},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
 pages = {5754--5764},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html},
 year = {2019}
}

@article{cobbe2021training,
 author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
 journal = {ArXiv preprint},
 title = {Training verifiers to solve math word problems},
 url = {https://arxiv.org/abs/2110.14168},
 volume = {abs/2110.14168},
 year = {2021}
}

@article{suzgun2022challenging,
 author = {Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and and Wei, Jason},
 journal = {ArXiv preprint},
 title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
 url = {https://arxiv.org/abs/2210.09261},
 volume = {abs/2210.09261},
 year = {2022}
}

@misc{sharegpt,
 howpublished = {\url{https://sharegpt.com/}},
 title = {ShareGPT},
 year = {2023}
}

@article{fu2023chain,
 author = {Fu, Yao and Ou, Litu and Chen, Mingyu and Wan, Yuhao and Peng, Hao and Khot, Tushar},
 journal = {ArXiv preprint},
 title = {Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance},
 url = {https://arxiv.org/abs/2305.17306},
 volume = {abs/2305.17306},
 year = {2023}
}

@inproceedings{papineni2002bleu,
 address = {Philadelphia, Pennsylvania, USA},
 author = {Papineni, Kishore  and
Roukos, Salim  and
Ward, Todd  and
Zhu, Wei-Jing},
 booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.3115/1073083.1073135},
 pages = {311--318},
 publisher = {Association for Computational Linguistics},
 title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
 url = {https://aclanthology.org/P02-1040},
 year = {2002}
}

@inproceedings{lin2004rouge,
 address = {Barcelona, Spain},
 author = {Lin, Chin-Yew},
 booktitle = {Text Summarization Branches Out},
 pages = {74--81},
 publisher = {Association for Computational Linguistics},
 title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
 url = {https://aclanthology.org/W04-1013},
 year = {2004}
}

@inproceedings{zhang2020BERTScore,
 author = {Tianyi Zhang and
Varsha Kishore and
Felix Wu and
Kilian Q. Weinberger and
Yoav Artzi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Wed, 03 Jun 2020 01:00:00 +0200},
 title = {BERTScore: Evaluating Text Generation with {BERT}},
 url = {https://openreview.net/forum?id=SkeHuCVFDr},
 year = {2020}
}

@inproceedings{xiao2022smoothquant,
 author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Demouth, Julien and Han, Song},
 booktitle = {International Conference on Machine Learning},
 title = {Smoothquant: Accurate and efficient post-training quantization for large language models},
 year = {2023}
}

@inproceedings{dettmers2022gptint,
 author = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
 title = {{GPT}3.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
 url = {https://openreview.net/forum?id=dXiGWqBoxaD},
 year = {2022}
}

@inproceedings{frantar2023optq,
 author = {Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {{OPTQ}: Accurate Quantization for Generative Pre-trained Transformers},
 url = {https://openreview.net/forum?id=tcbBPnfwxS},
 year = {2023}
}

@inproceedings{frantar2023sparsegpt,
 author = {Elias Frantar and Dan Alistarh},
 booktitle = {International Conference on Machine Learning},
 title = {{SparseGPT}: Massive Language Models Can Be Accurately Pruned in One-Shot},
 year = {2023}
}

@misc{alpaca,
 author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
 howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
 journal = {GitHub repository},
 publisher = {GitHub},
 title = {Stanford Alpaca: An Instruction-following LLaMA model},
 year = {2023}
}

@misc{vicuna2023,
 author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
 title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
 url = {https://lmsys.org/blog/2023-03-30-vicuna/},
 year = {2023}
}

@article{xu2023wizardlm,
 author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
 journal = {ArXiv preprint},
 title = {Wizardlm: Empowering large language models to follow complex instructions},
 url = {https://arxiv.org/abs/2304.12244},
 volume = {abs/2304.12244},
 year = {2023}
}

@inproceedings{hu2022lora,
 author = {Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
 booktitle = {International Conference on Learning Representations},
 title = {Lo{RA}: Low-Rank Adaptation of Large Language Models},
 url = {https://openreview.net/forum?id=nZeVKeeFYf9},
 year = {2022}
}

@inproceedings{fu2023complexitybased,
 author = {Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {Complexity-Based Prompting for Multi-step Reasoning},
 url = {https://openreview.net/forum?id=yf1icZHC-l9},
 year = {2023}
}

@inproceedings{goyal2020power,
 author = {Saurabh Goyal and
Anamitra Roy Choudhury and
Saurabh Raje and
Venkatesan T. Chakaravarthy and
Yogish Sabharwal and
Ashish Verma},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/GoyalCRCSV20.bib},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {3690--3699},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
 title = {PoWER-BERT: Accelerating {BERT} Inference via Progressive Word-vector
Elimination},
 url = {http://proceedings.mlr.press/v119/goyal20a.html},
 volume = {119},
 year = {2020}
}

@inproceedings{kim2021length,
 address = {Online},
 author = {Kim, Gyuwan  and
Cho, Kyunghyun},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.508},
 pages = {6501--6511},
 publisher = {Association for Computational Linguistics},
 title = {Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search},
 url = {https://aclanthology.org/2021.acl-long.508},
 year = {2021}
}

@inproceedings{kim2022learned,
 author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
 booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
 pages = {784--794},
 title = {Learned token pruning for transformers},
 year = {2022}
}

@inproceedings{rao2021dynamicvit,
 author = {Yongming Rao and Wenliang Zhao and Benlin Liu and Jiwen Lu and Jie Zhou and Cho-Jui Hsieh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
 title = {DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification},
 url = {https://openreview.net/forum?id=jB0Nlbwlybm},
 year = {2021}
}

@inproceedings{bolya2023token,
 author = {Daniel Bolya and Cheng-Yang Fu and Xiaoliang Dai and Peizhao Zhang and Christoph Feichtenhofer and Judy Hoffman},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {Token Merging: Your ViT But Faster},
 url = {https://openreview.net/forum?id=JroZRaRw7Eu},
 year = {2023}
}

@inproceedings{loshchilov2018decoupled,
 author = {Ilya Loshchilov and
Frank Hutter},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Decoupled Weight Decay Regularization},
 url = {https://openreview.net/forum?id=Bkg6RiCqY7},
 year = {2019}
}

@inproceedings{modarressi2022adapler,
 address = {Dublin, Ireland},
 author = {Modarressi, Ali  and
Mohebbi, Hosein  and
Pilehvar, Mohammad Taher},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.1},
 pages = {1--15},
 publisher = {Association for Computational Linguistics},
 title = {{A}dap{L}e{R}: Speeding up Inference by Adaptive Length Reduction},
 url = {https://aclanthology.org/2022.acl-long.1},
 year = {2022}
}

@article{shannon1951prediction,
 author = {Shannon, Claude E},
 journal = {Bell system technical journal},
 number = {1},
 pages = {50--64},
 publisher = {Wiley Online Library},
 title = {Prediction and entropy of printed English},
 volume = {30},
 year = {1951}
}

@article{gilbert2023semantic,
 author = {Gilbert, Henry and Sandborn, Michael and Schmidt, Douglas C and Spencer-Smith, Jesse and White, Jules},
 journal = {ArXiv preprint},
 title = {Semantic Compression With Large Language Models},
 url = {https://arxiv.org/abs/2304.12512},
 volume = {abs/2304.12512},
 year = {2023}
}

@software{Chase_LangChain_2022,
 author = {Chase, Harrison},
 title = {{LangChain}},
 url = {https://github.com/hwchase17/langchain},
 year = {2022}
}

@article{zhang2023mlcopilot,
 author = {Zhang, Lei and Zhang, Yuge and Ren, Kan and Li, Dongsheng and Yang, Yuqing},
 journal = {ArXiv preprint},
 title = {MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks},
 url = {https://arxiv.org/abs/2304.14979},
 volume = {abs/2304.14979},
 year = {2023}
}

@inproceedings{wei2022chain,
 author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
 title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
 url = {https://openreview.net/forum?id=_VjQlMeSB_J},
 year = {2022}
}

@misc{ilya2023unsupervised,
 author = {Ilya Sutskever},
 howpublished = {\url{https://simons.berkeley.edu/talks/ilya-sutskever-openai-2023-08-14}},
 title = {A theory of unsupervised learning},
 year = {2023}
}

@article{deletang2023language,
 author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and others},
 journal = {ArXiv preprint},
 title = {Language Modeling Is Compression},
 url = {https://arxiv.org/abs/2309.10668},
 volume = {abs/2309.10668},
 year = {2023}
}

@article{rissanen1976generalized,
 author = {Rissanen, Jorma J},
 journal = {IBM Journal of research and development},
 number = {3},
 pages = {198--203},
 publisher = {IBM},
 title = {Generalized Kraft inequality and arithmetic coding},
 volume = {20},
 year = {1976}
}

@phdthesis{pasco1976source,
 author = {Pasco, Richard Clark},
 school = {Citeseer},
 title = {Source coding algorithms for fast data compression},
 year = {1976}
}
