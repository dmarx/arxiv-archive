\begin{thebibliography}{44}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{sha(2023)}]{sharegpt}
 2023.
\newblock Sharegpt.
\newblock \url{https://sharegpt.com/}.

\bibitem[{Arora et~al.(2021)Arora, Huang, and He}]{arora2021types}
Udit Arora, William Huang, and He~He. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.835} {Types of
  out-of-distribution texts and how to detect them}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 10687--10701, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Bolya et~al.(2023)Bolya, Fu, Dai, Zhang, Feichtenhofer, and
  Hoffman}]{bolya2023token}
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph
  Feichtenhofer, and Judy Hoffman. 2023.
\newblock \href {https://openreview.net/forum?id=JroZRaRw7Eu} {Token merging:
  Your vit but faster}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Chase(2022)}]{Chase_LangChain_2022}
Harrison Chase. 2022.
\newblock \href {https://github.com/hwchase17/langchain} {{LangChain}}.

\bibitem[{Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and
  Chen}]{chevalier2023adapting}
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023.
\newblock \href {https://arxiv.org/abs/2305.14788} {Adapting language models to
  compress contexts}.
\newblock \emph{ArXiv preprint}, abs/2305.14788.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing. 2023.
\newblock \href {https://lmsys.org/blog/2023-03-30-vicuna/} {Vicuna: An
  open-source chatbot impressing gpt-4 with 90\%* chatgpt quality}.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al. 2021.
\newblock \href {https://arxiv.org/abs/2110.14168} {Training verifiers to solve
  math word problems}.
\newblock \emph{ArXiv preprint}, abs/2110.14168.

\bibitem[{Del{\'e}tang et~al.(2023)Del{\'e}tang, Ruoss, Duquenne, Catt,
  Genewein, Mattern, Grau-Moya, Wenliang, Aitchison, Orseau
  et~al.}]{deletang2023language}
Gr{\'e}goire Del{\'e}tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt,
  Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li~Kevin Wenliang,
  Matthew Aitchison, Laurent Orseau, et~al. 2023.
\newblock \href {https://arxiv.org/abs/2309.10668} {Language modeling is
  compression}.
\newblock \emph{ArXiv preprint}, abs/2309.10668.

\bibitem[{Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer}]{dettmers2022gptint}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022.
\newblock \href {https://openreview.net/forum?id=dXiGWqBoxaD} {{GPT}3.int8():
  8-bit matrix multiplication for transformers at scale}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Frantar and Alistarh(2023)}]{frantar2023sparsegpt}
Elias Frantar and Dan Alistarh. 2023.
\newblock {SparseGPT}: Massive language models can be accurately pruned in
  one-shot.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and
  Alistarh}]{frantar2023optq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023.
\newblock \href {https://openreview.net/forum?id=tcbBPnfwxS} {{OPTQ}: Accurate
  quantization for generative pre-trained transformers}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Fu et~al.(2023{\natexlab{a}})Fu, Ou, Chen, Wan, Peng, and
  Khot}]{fu2023chain}
Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot.
  2023{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2305.17306} {Chain-of-thought hub: A
  continuous effort to measure large language models' reasoning performance}.
\newblock \emph{ArXiv preprint}, abs/2305.17306.

\bibitem[{Fu et~al.(2023{\natexlab{b}})Fu, Peng, Sabharwal, Clark, and
  Khot}]{fu2023complexitybased}
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot.
  2023{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=yf1icZHC-l9} {Complexity-based
  prompting for multi-step reasoning}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Ge et~al.(2022)Ge, Hu, Dong, Mao, Xia, Wang, Chen, and
  Wei}]{ge2022extensible}
Tao Ge, Jing Hu, Li~Dong, Shaoguang Mao, Yan Xia, Xun Wang, Si-Qing Chen, and
  Furu Wei. 2022.
\newblock \href {https://arxiv.org/abs/2212.00616} {Extensible prompts for
  language models}.
\newblock \emph{ArXiv preprint}, abs/2212.00616.

\bibitem[{Ge et~al.(2023)Ge, Hu, Wang, Chen, and Wei}]{ge2023context}
Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
\newblock \href {https://arxiv.org/abs/2307.06945} {In-context autoencoder for
  context compression in a large language model}.
\newblock \emph{ArXiv preprint}, abs/2307.06945.

\bibitem[{Gilbert et~al.(2023)Gilbert, Sandborn, Schmidt, Spencer-Smith, and
  White}]{gilbert2023semantic}
Henry Gilbert, Michael Sandborn, Douglas~C Schmidt, Jesse Spencer-Smith, and
  Jules White. 2023.
\newblock \href {https://arxiv.org/abs/2304.12512} {Semantic compression with
  large language models}.
\newblock \emph{ArXiv preprint}, abs/2304.12512.

\bibitem[{Goyal et~al.(2020)Goyal, Choudhury, Raje, Chakaravarthy, Sabharwal,
  and Verma}]{goyal2020power}
Saurabh Goyal, Anamitra~Roy Choudhury, Saurabh Raje, Venkatesan~T.
  Chakaravarthy, Yogish Sabharwal, and Ashish Verma. 2020.
\newblock \href {http://proceedings.mlr.press/v119/goyal20a.html} {Power-bert:
  Accelerating {BERT} inference via progressive word-vector elimination}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 3690--3699. {PMLR}.

\bibitem[{Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang,
  and Chen}]{hu2022lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen. 2022.
\newblock \href {https://openreview.net/forum?id=nZeVKeeFYf9} {Lo{RA}: Low-rank
  adaptation of large language models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Kim and Cho(2021)}]{kim2021length}
Gyuwan Kim and Kyunghyun Cho. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-long.508}
  {Length-adaptive transformer: Train once with length drop, use anytime with
  search}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 6501--6511,
  Online. Association for Computational Linguistics.

\bibitem[{Kim et~al.(2022)Kim, Shen, Thorsley, Gholami, Kwon, Hassoun, and
  Keutzer}]{kim2022learned}
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph
  Hassoun, and Kurt Keutzer. 2022.
\newblock Learned token pruning for transformers.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 784--794.

\bibitem[{Li(2023)}]{li2023unlocking}
Yucheng Li. 2023.
\newblock \href {https://arxiv.org/abs/2304.12102} {Unlocking context
  constraints of llms: Enhancing context efficiency of llms with
  self-information-based content filtering}.
\newblock \emph{ArXiv preprint}, abs/2304.12102.

\bibitem[{Lin(2004)}]{lin2004rouge}
Chin-Yew Lin. 2004.
\newblock \href {https://aclanthology.org/W04-1013} {{ROUGE}: A package for
  automatic evaluation of summaries}.
\newblock In \emph{Text Summarization Branches Out}, pages 74--81, Barcelona,
  Spain. Association for Computational Linguistics.

\bibitem[{Loshchilov and Hutter(2019)}]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter. 2019.
\newblock \href {https://openreview.net/forum?id=Bkg6RiCqY7} {Decoupled weight
  decay regularization}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[{Mai et~al.(2022)Mai, Davies, and Griffin}]{mai2022self}
Kimberly~T Mai, Toby Davies, and Lewis~D Griffin. 2022.
\newblock \href {https://arxiv.org/abs/2204.05695} {Self-supervised losses for
  one-class textual anomaly detection}.
\newblock \emph{ArXiv preprint}, abs/2204.05695.

\bibitem[{Modarressi et~al.(2022)Modarressi, Mohebbi, and
  Pilehvar}]{modarressi2022adapler}
Ali Modarressi, Hosein Mohebbi, and Mohammad~Taher Pilehvar. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.1} {{A}dap{L}e{R}:
  Speeding up inference by adaptive length reduction}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1--15, Dublin,
  Ireland. Association for Computational Linguistics.

\bibitem[{Mu et~al.(2023)Mu, Li, and Goodman}]{mu2023learning}
Jesse Mu, Xiang~Lisa Li, and Noah Goodman. 2023.
\newblock \href {https://arxiv.org/abs/2304.08467} {Learning to compress
  prompts with gist tokens}.
\newblock \emph{ArXiv preprint}, abs/2304.08467.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock \href {https://doi.org/10.3115/1073083.1073135} {{B}leu: a method for
  automatic evaluation of machine translation}.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics}, pages 311--318, Philadelphia, Pennsylvania,
  USA. Association for Computational Linguistics.

\bibitem[{Pasco(1976)}]{pasco1976source}
Richard~Clark Pasco. 1976.
\newblock \emph{Source coding algorithms for fast data compression}.
\newblock Ph.D. thesis, Citeseer.

\bibitem[{Rao et~al.(2021)Rao, Zhao, Liu, Lu, Zhou, and
  Hsieh}]{rao2021dynamicvit}
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.
  2021.
\newblock \href {https://openreview.net/forum?id=jB0Nlbwlybm} {Dynamicvit:
  Efficient vision transformers with dynamic token sparsification}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Rissanen(1976)}]{rissanen1976generalized}
Jorma~J Rissanen. 1976.
\newblock Generalized kraft inequality and arithmetic coding.
\newblock \emph{IBM Journal of research and development}, 20(3):198--203.

\bibitem[{Shannon(1951)}]{shannon1951prediction}
Claude~E Shannon. 1951.
\newblock Prediction and entropy of printed english.
\newblock \emph{Bell system technical journal}, 30(1):50--64.

\bibitem[{Sutskever(2023)}]{ilya2023unsupervised}
Ilya Sutskever. 2023.
\newblock A theory of unsupervised learning.
\newblock
  \url{https://simons.berkeley.edu/talks/ilya-sutskever-openai-2023-08-14}.

\bibitem[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung,
  Chowdhery, Le, Chi, Zhou, , and Wei}]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, , and
  Jason Wei. 2022.
\newblock \href {https://arxiv.org/abs/2210.09261} {Challenging big-bench tasks
  and whether chain-of-thought can solve them}.
\newblock \emph{ArXiv preprint}, abs/2210.09261.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto}]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi,
  Le, and Zhou}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
  Ed~H. Chi, Quoc~V Le, and Denny Zhou. 2022.
\newblock \href {https://openreview.net/forum?id=_VjQlMeSB_J} {Chain of thought
  prompting elicits reasoning in large language models}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wingate et~al.(2022)Wingate, Shoeybi, and
  Sorensen}]{wingate-etal-2022-prompt}
David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022.
\newblock \href {https://aclanthology.org/2022.findings-emnlp.412} {Prompt
  compression and contrastive conditioning for controllability and toxicity
  reduction in language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 5621--5634, Abu Dhabi, United Arab Emirates. Association
  for Computational Linguistics.

\bibitem[{Wu et~al.(2023)Wu, Jiang, Yin, Karlsson, and Lin}]{wu2023multi}
Qianhui Wu, Huqiang Jiang, Haonan Yin, B\"orje~F. Karlsson, and Chin-Yew Lin.
  2023.
\newblock Multi-level knowledge distillation for out-of-distribution detection
  in text.
\newblock In \emph{Proceedings of the 61th Annual Meeting of the Association
  for Computational Linguistics (Long Papers)}.

\bibitem[{Xiao et~al.(2023)Xiao, Lin, Seznec, Demouth, and
  Han}]{xiao2022smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Julien Demouth, and Song Han. 2023.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and
  Jiang}]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang. 2023.
\newblock \href {https://arxiv.org/abs/2304.12244} {Wizardlm: Empowering large
  language models to follow complex instructions}.
\newblock \emph{ArXiv preprint}, abs/2304.12244.

\bibitem[{Yang et~al.(2023)Yang, Ge, Wang, Jiao, Jiang, Yang, Majumder, and
  Wei}]{yang2023inference}
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan
  Majumder, and Furu Wei. 2023.
\newblock \href {https://arxiv.org/abs/2304.04487} {Inference with reference:
  Lossless acceleration of large language models}.
\newblock \emph{ArXiv preprint}, abs/2304.04487.

\bibitem[{Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le}]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime~G. Carbonell, Ruslan Salakhutdinov,
  and Quoc~V. Le. 2019.
\newblock \href
  {https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html}
  {Xlnet: Generalized autoregressive pretraining for language understanding}.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 5754--5764.

\bibitem[{Zhang et~al.(2023)Zhang, Zhang, Ren, Li, and
  Yang}]{zhang2023mlcopilot}
Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang. 2023.
\newblock \href {https://arxiv.org/abs/2304.14979} {Mlcopilot: Unleashing the
  power of large language models in solving machine learning tasks}.
\newblock \emph{ArXiv preprint}, abs/2304.14979.

\bibitem[{Zhang et~al.(2020)Zhang, Kishore, Wu, Weinberger, and
  Artzi}]{zhang2020BERTScore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q. Weinberger, and Yoav Artzi.
  2020.
\newblock \href {https://openreview.net/forum?id=SkeHuCVFDr} {Bertscore:
  Evaluating text generation with {BERT}}.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[{Zhou et~al.(2023)Zhou, Jiang, Cotterell, and
  Sachan}]{zhou2023efficient}
Wangchunshu Zhou, Yuchen~Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan.
  2023.
\newblock \href {https://arxiv.org/abs/2305.11170} {Efficient prompting via
  dynamic in-context learning}.
\newblock \emph{ArXiv preprint}, abs/2305.11170.

\end{thebibliography}
