\begin{table}[ht]
    \centering
	\setlength{\tabcolsep}{0.5mm}
	% \scalebox{0.8}{
     \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
         & 1x & 2x & 3x & 4x \\
         \midrule
        % Original & 14.47 & - &  \\
        Windows & 14.47 & 96.19 & 121.13 & 124.00\\
        \textbf{Ours} w/o Iterative & - & 22.19 & 55.03 & 86.57 \\
        \textbf{Ours} & - & 16.08 & 22.94 & 25.69 \\
        \bottomrule
    \end{tabular}
    }
    % }
    \caption{Using the iterative token-level prompt compression method for KV cache compression, we evaluate the perplexity on the raw-WikiText2 dataset with the LLaMA-7B model.}
    \label{tab:kv_cache}
\end{table}