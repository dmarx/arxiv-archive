{
  "arxivId": "2310.05736",
  "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large\n  Language Models",
  "authors": "Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu",
  "abstract": "Large language models (LLMs) have been applied in various applications due to\ntheir astonishing capabilities. With advancements in technologies such as\nchain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed\nto LLMs are becoming increasingly lengthy, even exceeding tens of thousands of\ntokens. To accelerate model inference and reduce cost, this paper presents\nLLMLingua, a coarse-to-fine prompt compression method that involves a budget\ncontroller to maintain semantic integrity under high compression ratios, a\ntoken-level iterative compression algorithm to better model the interdependence\nbetween compressed contents, and an instruction tuning based method for\ndistribution alignment between language models. We conduct experiments and\nanalysis over four datasets from different scenarios, i.e., GSM8K, BBH,\nShareGPT, and Arxiv-March23; showing that the proposed approach yields\nstate-of-the-art performance and allows for up to 20x compression with little\nperformance loss. Our code is available at https://aka.ms/LLMLingua.",
  "url": "https://arxiv.org/abs/2310.05736",
  "issue_number": 299,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/299",
  "created_at": "2024-12-26T22:56:50.391505",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 38,
  "last_read": "2024-12-26T23:13:03.210307",
  "last_visited": "2024-12-26T22:57:57.379000+00:00",
  "main_tex_file": null
}