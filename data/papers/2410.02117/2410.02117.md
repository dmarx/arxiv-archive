---
abstract: |
  Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts focused on a small number of hand-crafted structured matrices and neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch, along with many novel structures. To analyze the framework, we develop a taxonomy of all such operators based on their computational and algebraic properties and show that differences in the compute-optimal scaling laws are mostly governed by a small number of variables that we introduce. Namely, a small $\omega$ (which measures parameter sharing) and large $\psi$ (which measures the rank) reliably led to better scaling laws. Guided by the insight that full-rank structures that maximize parameters per unit of compute perform the best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture obtained by sparsifying computation in the BTT structure. In contrast to the standard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE in every single linear layer of the model, including the projection matrices in the attention blocks. We find BTT-MoE provides a substantial compute-efficiency gain over dense layers and standard MoE.
author:
- |
  Andres Potapczynski[^1]  
  New York University  
  Shikai Qiu$^{*}$  
  New York University  
  Marc Finzi  
  Carnegie Mellon University  
  Christopher Ferri  
  Capital One  
  Zixi Chen  
  New York University  
  Micah Goldblum  
  Columbia University  
  Bayan Bruss  
  Capital One  
  Christopher De Sa  
  Cornell University  
  Andrew Gordon Wilson  
  New York University  
bibliography:
- refs.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices
---





# Introduction

Neural networks primarily consist of interleaved linear layers and simple non-linearities. In large foundation models such as GPT-3 , these linear layers consume the vast majority of the parameters and computation , and are commonly represented by dense matrices. Substituting these dense matrices with structured matrices with fast matrix-vector multiplies (MVMs) has the potential to significantly improve the computational efficiency of these models.

Recent work by , , and demonstrated that incorporating certain structured matrices into neural network architectures, including transformers, can improve performance over dense models of the same size trained for equal number of epochs on problems such as ImageNet classification. However, these success cases do not reflect the current paradigm of large-scale training, where the models 1) are typically not trained for multiple epochs, making the expressiveness of dense matrices particularly appealing since the generalization gap vanishes, and 2) are heavily bottlenecked by compute cost, making it infeasible to train the models until convergence , unlike in image classification. These attributes of large-scale training make the compute-optimal scaling rather than scaling in model size alone more relevant.

In this work, we investigate how different structures perform in a compute-optimal setting, which characterizes performance as a function of training compute when allocated optimally between using larger models versus training on more data . In language modeling and many other tasks, the compute-optimal scaling law has been shown to take the form $\mathcal{L}= \mathcal{L}_\infty + b C^{-a}$ as a function of training compute $C,$ where $\mathcal{L}_\infty$ is the minimal achievable loss . Quantifying the compute-optimal scaling laws of various structures is essential for understanding their practical value for training large-scale neural networks.

In addition to investigating the scaling laws of existing structures, we expand the set of matrix structures beyond what has been previously considered. We do so by introducing a continuous parameterization of the space of all possible structures whose matrix-vector-multiplication (MVM) can be expressed as an Einstein summation (Einsum).[^2] This space contains many known structures such as low-rank, Tensor-Train , Kronecker product , Monarch and Block Tensor-Train , but also includes many novel hardware-efficient structures. Indeed, all structures in this space are hardware-efficient in the sense that they are computed through a series of batch matrix multiplication primitives, which we implement through the `Linear Operator` abstractions available in `CoLA` . Moreover, this space lends itself to an intuitive exploration as we can analyze how different parameters of the Einsum affect a structure’s performance and scaling laws. We make our code available [<u>here</u>](https://github.com/AndPotap/einsum-search).

<figure id="fig:goal">
<p><span class="image placeholder" data-original-image-src="figs/cases.eps" data-original-image-title="" width="0.5\linewidth">image</span> <span class="image placeholder" data-original-image-src="figs/exmaple_scaling.pdf" data-original-image-title="" width="0.37\linewidth">image</span></p>
<figcaption> <strong>We use Einsums to parameterize a wide range of structured matrices and search for the most efficient structure for compute-optimal training.</strong> <strong>Left:</strong> A diagrammatic representation of a general two-factor Einsum. We parameterize the space of Einsums through a real-valued vector <span class="math inline">\({\boldsymbol{\mathbf{\theta}}}=(\theta_\alpha,\theta_\beta,\theta_\gamma,\theta_\delta,\theta_\epsilon,\theta_\phi,\theta_\rho) \in [0,1]^{7}\)</span>. This space captures many well-known structures through specific values of <span class="math inline">\({\boldsymbol{\mathbf{\theta}}}\)</span>. <strong>Middle:</strong> Example of well-known structures with their <span class="math inline">\({\boldsymbol{\mathbf{\theta}}}\)</span> values. Any omitted line implies the value of the entry in the vector is 0. <strong>Right:</strong> Compute-optimal scaling laws of example structures for GPT-2 on OpenWebText when substituting its dense layers (see details in ). </figcaption>
</figure>

We summarize our main contributions as follows:

- We introduce a continuous parameterization of the space of structured matrices whose matrix-vector-multiplication can be implemented via an Einstein summation (Einsum). This parameterization allows us to search a wide range of hardware-efficient structured linear layers for neural network architectures beyond a handful of well-known cases identified in prior work .

- We develop a taxonomy of the space of Einsums based on its computational and algebraic properties. We identify three key scalar quantities that characterize this space $(\omega, \psi, \nu)$. (1) $\omega \geq 0$, which reflects the extent of parameter sharing in a matrix. (2) $\psi \in [0, 1]$, which characterizes to the rank of the structure ($\psi=1$ meaning full-rank). (3) $\nu \in [0,1)$, which relates to compute per dimension in an MVM, where the upper-bound $\nu = 1$ is achieved by dense matrices. Intuitively, $\nu$ measures how much a structure resembles dense.

- We investigate the scaling laws of different Einsums on language modeling, autoregressive image generation, and a synthetic regression task. We find that the best-performing structures are the ones that do not share parameters ($\omega=0$), are full-rank ($\psi=1$), while $\nu$ can be varied with often negligible impact to their scaling laws. In contrast to previous findings, we demonstrate that structures shown in prior work to outperform dense matrices in non-compute-optimal settings can yield similar but not significantly better compute-optimal scaling laws on these tasks.

- Building on prior work in structure-aware learning rates , we show how to properly initialize generic Einsum layers and transfer learning rates from the original dense layers and across model sizes, leveraging insights from $\mu$P  and manifold optimization .

- Based on the observed relation between the taxonomy variables and the scaling laws, we propose a new structured Mixture of Experts (MoE) architecture implementing a sparse mixture of multiple structure matrices. This block tensor-train (BTT) MoE provides a sparse MoE in every single layer of each feedforward network (FFN) and attention project matrices, compared to standard MoE which operates over entire FFNs. We show BTT-MoE is significantly more compute-efficient than dense matrices and standard MoE for training GPT-2 language models.

# Parameterizing the Space of Einsums

We now present a unifying framework that parameterizes all linear operators ${\boldsymbol{\mathbf{W}}} \in \mathbb{R}^{{d_\mathrm{out}}\times {d_\mathrm{in}}}$ whose matrix-vector-multiply ${\boldsymbol{\mathbf{y}}} = {\boldsymbol{\mathbf{Wx}}}$ can be expressed as an Einsum over the tensors ${\boldsymbol{\mathbf{x}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, \ldots,$ where ${\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, \ldots$ defines the operator ${\boldsymbol{\mathbf{W}}}$ and contains all its learnable parameters. To simplify the presentation, throughout this paper we assume ${\boldsymbol{\mathbf{W}}}$ is defined using only two factors ${\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}},$ but we show generalization to more than two factors is straightforward in .

We consider the following general expression of such an Einsum $$\label{eq:general}
    \begin{split}
      Y_{\delta \epsilon \phi}
      =
      \sum_{\alpha \beta \gamma \rho}^{}
      B_{\beta \gamma \epsilon \phi \rho} A_{\alpha \gamma \delta \phi \rho} X_{\alpha \beta \gamma},
    \end{split}$$ where the vectors ${\boldsymbol{\mathbf{x}}}$ and ${\boldsymbol{\mathbf{y}}}$ are written as tensors with multiple indexes to allow them to interact differently with each other, and the factors ${\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}.$ Each index $x \in \{\alpha, \beta, \gamma, \delta, \epsilon, \phi, \rho\}$ ranges from $1$ to $d_x.$ Given this general expression, we obtain different structures via different factorizations of ${d_\mathrm{in}}$ into $d_{\alpha} d_{\beta} d_{\gamma}$, and ${d_\mathrm{out}}$ into $d_{\delta} d_{\epsilon} d_{\phi},$ and separately a choice of $d_{\rho}$. For example, for a low-rank matrix, we have $d_{\alpha} = {d_\mathrm{in}}, d_{\beta} = d_{\gamma} = 1$, $d_{\epsilon} = {d_\mathrm{out}}, d_{\delta} = d_{\phi} = 1$ and $d_{\rho} = r$. For a Kronecker product, we have $d_{\alpha} = d_{\beta} = \sqrt{{d_\mathrm{in}}}$, $d_{\delta} = d_{\epsilon} = \sqrt{{d_\mathrm{out}}}$ and $d_{\gamma} = d_{\phi} = d_{\rho} = 1$. We provide an extended list of examples in .

The general expression and the above two examples can be more conveniently and intuitively represented as a diagram shown in (left), where each index $\alpha, \beta, \gamma, \ldots$ corresponds to a (hyper)edge among the input, output, and weight factors: $\alpha \leftrightarrow \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}\}$, $\beta \leftrightarrow \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{B}}}\}$, $\gamma \leftrightarrow \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\}$, $\delta \leftrightarrow \{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{A}}}\}$, $\epsilon \leftrightarrow \{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{B}}}\}$, $\phi \leftrightarrow \{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\}$ and $\rho \leftrightarrow \{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\}$. This set of edges can be written succinctly as $\mathcal{E} = \{S \subseteq \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{Y}}}\}: \left|S \right| \geq 2
\text{  and  } \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{Y}}}\} \not\subseteq S\}$. We exclude subsets that contain ${\boldsymbol{\mathbf{X}}}$ and ${\boldsymbol{\mathbf{Y}}}$ simultaneously, as adding them simply produces an already included structure but repeated multiple times along one of the input and output axes. The structure of a particular Einsum is fully specified by the the vector $(d_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}}, d_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{B}}}}, d_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}}, d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}}, d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{B}}}}, d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}},
d_{{\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}}})$, which specifies the range of the indices $\alpha, \beta, \gamma, \delta, \epsilon, \phi, \rho$. When the range of an index is of size 1, the corresponding edge effectively disappears from the diagram and the expression simplfies.

As we will build models of varying sizes, it is more natural to think about how these entries scale with ${d_\mathrm{in}}$ and ${d_\mathrm{out}}$. We therefore assign a real-valued vector ${\boldsymbol{\mathbf{\theta}}} \in \left[0, 1\right]^{7}$ to each structure indicating that $d_{i} = d_\mathrm{in}^{\theta_{i}}$ for $i \in \{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}}\},$ $d_{j} = {d_\mathrm{out}^{\theta_{j}}}$ for $j \in \{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}}\},$ and $d_{{\boldsymbol{\mathbf{AB}}}} = \min({d_\mathrm{in}}, {d_\mathrm{out}})^{\theta_{{\boldsymbol{\mathbf{AB}}}}},$ with the restriction that $\theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}} + \theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{B}}}} + \theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} = \theta_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}} + \theta_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{B}}}} + \theta_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} = 1$. For example, a low-rank matrix whose rank scales as the dimension it operates on to the $1/2$-th power is represented as ${\boldsymbol{\mathbf{\theta}}}=(1, 0, 0, 0, 1, 0, 1/2),$ and a Kronecker product of two factors of equal sizes is represented as ${\boldsymbol{\mathbf{\theta}}}=(1/2, 1/2, 0, 1/2, 1/2, 0, 0)$. We round all $d_i$ to its nearest integer when instantiating the Einsum. Note this rounding only quantizes the the values of $d_i,$ but leaves the space of meaningfully different ${\boldsymbol{\mathbf{\theta}}}$s continuous as we consider arbitrarily large matrices.

# A Taxonomy of the Space of Einsum Linear Structures

<figure id="fig:taxonomy">
<span class="image placeholder" data-original-image-src="figs/3d_v2.eps" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption> <strong>Illustrating the Einsum taxonomy.</strong> The 3D graph represents relevant quantities of the Einsum structure such as the amount of parameter sharing <span class="math inline">\(\omega\)</span> (x-axis), its rank <span class="math inline">\(\psi\)</span> (y-axis), and its compute intensity <span class="math inline">\(\nu\)</span> (z-axis). The structures on the left of the figure appear as dots on the graph based on their coordinates <span class="math inline">\({\boldsymbol{\mathbf{\theta}}}\)</span>. We highlight two key subspaces. (a) The BTT subspace, characterized by no parameter sharing <span class="math inline">\(\omega=0,\)</span> learning the maximum number of parameters per FLOP. (b) The full-rank BTT subspace where <span class="math inline">\(\omega=0\)</span> and <span class="math inline">\(\psi=1\)</span>. In  we show that the full-rank BTT subspace contains the most performant structures across multiple tasks. </figcaption>
</figure>

The space of all Einsums is a high-dimensional space containing a wide range of possible structures. *A priori*, it is difficult to reason about the properties of a particular point in this space given its coordinates ${\boldsymbol{\mathbf{\theta}}} \in [0,1]^{7}$. In this section, we develop a taxonomy of the space of Einsums based on their computational and algebraic properties. We introduce three key scalar quantities that characterize this space: (1) $\psi \in [0, 1]$, which is related to the rank of the structure, (2) $\nu \in [0, 1]$, which is related to the compute intensity of the structure, i.e. FLOPs per MVM divided by the dimension, and (3) $\omega \geq0$, which is related to the number of learnable parameters divided by the FLOPs per MVM. Each of these three quantities can be expressed in terms of the entries of ${\boldsymbol{\mathbf{\theta}}},$ which we derive in . To simplify the presentation, we assume ${d_\mathrm{in}}= {d_\mathrm{out}}= d$ in the rest of the section. Without loss of generality, we assume $\min(\theta_{{\boldsymbol{\mathbf{XA}}}}, \theta_{\boldsymbol{\mathbf{YB}}}) \geq \min(\theta_{{\boldsymbol{\mathbf{YA}}}}, \theta_{{\boldsymbol{\mathbf{BX}}}}),$ so that it is more efficient to first multiply by ${\boldsymbol{\mathbf{A}}}$ instead of ${\boldsymbol{\mathbf{B}}}.$

**Rank Exponent, $\psi$.** For a given ${\boldsymbol{\mathbf{\theta}}}$, we have that $\text{rank}({\boldsymbol{\mathbf{W}}}) = \Theta\left(d^{\psi}\right)$ where $\psi=\min(1, 2 + \theta_{{\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}}} - \theta_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}}} - \theta_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{B}}}})$. Thus, $\psi=1$ implies full-rank and decreasing values of $\psi$ imply lower ranks until the limit of $0$. In other words, the rank decreases when an Einsum increasingly allocates part of the input and output to factors that are *only* connected the input or output. The limit being a low-rank structure as seen in where $\theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}}=1=\theta_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{B}}}}$ and which creates a bottleneck on $\{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\}$. The opposite trend is exhibited by dense, which allocates the full dimension of the input and output to both factors as seen in . Nonetheless, it is still possible to achieve a full-rank when allocating dimension to only single factors as long as those values do not constitute most of the allocation, meaning $0<\theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}} \leq 1/2$ and $0<\theta_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{B}}}} \leq 1/2$, as seen in Monarch, Tensor-Train, and Kronecker but not for the particular Block Tensor-Train (BTT) structure in . In we show that structures with $\psi=1$ perform best when training neural networks.

**Compute Intensity Exponent, $\nu$.** Let $F$ denote the FLOPs required to perform a MVM and define the compute intensity as $F/d = \Theta\left(d^{\nu}\right)$. The upper bound is achieved by dense, which requires quadratic compute for an MVM and thus $\nu=1$. In general, we have $\nu=1 + \theta_{{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} - \min(\theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}}, \theta_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{B}}}})$. Thus, in order to achieve lower compute intensity than dense, a structure has to allocate dimensionality to factors that *only* connect to the input and output. As seen in , the BTT example is able to achieve the lowest compute intensity by allocating a substantial part of the input dimension to the first factor and a substantial part of the output dimension to the second factor. $\psi$ and $\nu$ are not completely independent, e.g. $\psi=\nu$ for low-rank matrices $W_{ij} = \sum_{k=1}^{r} B_{ik} A_{kj}$, though exceptions exist such as for the Kronecker product where $\nu$ can be arbitrarily low while maintaining $\psi = 1$. In , we show there exists a wide range of structures with varying $\nu$ that perform as well as dense matrices.

**Parameters-Sharing Exponent, $\omega$.** Let $N$ denote the number of parameters in the structure then $N / F = \Theta\left(d^{\omega}\right)$. Clearly, $\omega=1$ for dense matrices where each parameter is used exactly once in an MVM. In general, we can show that $\omega = \min(\theta_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}}} + \theta_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}}, \theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{B}}}} + \theta_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{B}}}})
- \min(\theta_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}}}, \theta_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{B}}}})$. In we find that structures that share parameters, that is $\omega > 0$, have worse scaling laws that structures that do not ($\omega=0$). To achieve $\omega=0,$ we have to avoid introducing edges that *skip* some factors, that is $\theta_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{B}}}} = \theta_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}} = 0$. Structures that skip factors in are Tensor-Train and Kronecker where there exists an edge that connects ${\boldsymbol{\mathbf{X}}}$ to ${\boldsymbol{\mathbf{B}}}$ while skipping ${\boldsymbol{\mathbf{A}}}$. In contrast, in Monarch and BTT, the edge connecting ${\boldsymbol{\mathbf{X}}}$ with ${\boldsymbol{\mathbf{B}}}$ also touches ${\boldsymbol{\mathbf{A}}}$.

# Scaling Laws of Einsums

While prior works have shown that certain structured matrices such as Monarch and BTT have better scaling laws than dense matrices as a function of model size on datasets such as CIFAR-10, CIFAR-100, and ImageNet , their experimental setups do not reflect today’s large-scale training, where the models 1) typically do not train for multiple epochs on the training set, and 2) are heavily compute-bottlenecked such that we care primarily about performance as a function of training compute rather than model size (omitting the cost of training). These attributes of large-scale training make the compute-optimal scaling rather than scaling in model size alone more relevant.

In this section, we investigate the compute-optimal scaling laws of a wide range of Einsums — how their performance scales as a function of training compute. We will show that we can understand the systematic differences in the scaling laws of various Einsums by leveraging the taxonomy we have developed. While we do not find a structure that achieves noticeably better scaling laws compared to dense matrices, we identify the set of common properties shared across a wide range of structures that match the performance of dense matrices, based on which we will propose a significantly more efficient alternative to dense layers in .

## Main Experimental Setup

We train GPT-2  language models on the OpenWebText dataset. To make our measurement of the scaling laws more robust and our experiments more affordable, we reduce the vocabulary of the original GPT-2 to $96$ commonly used alphanumeric symbols. Using a small vocabulary limits the compute and parameters consumed by the language modeling head, which would otherwise obscure the scaling laws measured at small scales . We train models of varying sizes from $120$k to $76$M parameters, with model dimension $d \in [256, 4096]$ and number of transformer blocks $L \in \{3, 6\}.$ Each model is trained for $100$k steps with a batch size of $65536$ tokens and a sequence length of $128.$ All linear layers except the language modeling head are replaced with Einsums. We use the Adam optimizer with a base learning rate of $0.003$ for a $L=3, d=256$ dense model, and scale it using $\mu$P  and structure-aware learning rates  to larger models and models using Einsums in place of dense layers. We discuss learning rate scaling in detail in , showing it is crucial for the performance of Einsums. We use weight normalization to stabilize the training of Einsums following .

In Appendix , we show our main conclusions derived from this simplified setup translate to the more standard GPT-2 evaluation with a longer sequence length of 512 and its original vocabulary of 50,257 tokens.

## Analyzing the Compute-Optimal Scaling Laws

<figure id="fig:scaling_laws_analysis">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/omega.pdf" data-original-image-title="" height="0.28\linewidth"></span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/psi.pdf" data-original-image-title="" height="0.28\linewidth"></span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/nu.pdf" data-original-image-title="" height="0.28\linewidth"></span></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<figcaption> <strong>The taxonomy parameters <span class="math inline">\((\omega, \psi)\)</span> explain differences in the scaling laws.</strong> (<strong>Left</strong>): parameter sharing (<span class="math inline">\(\omega &gt; 0\)</span>) leads to worse scaling. (<strong>Middle</strong>): among structures without parameter sharing (<span class="math inline">\(\omega = 0\)</span>), full-rank structures (<span class="math inline">\(\psi=1\)</span>) scale better than low-rank structures (<span class="math inline">\(\psi&lt;1\)</span>). (<strong>Right</strong>): in the <span class="math inline">\((\omega = 0, \psi = 1)\)</span> subspace, various structures have nearly indistinguishable scaling laws compared to dense matrices, suggesting that not implementing parameter sharing and being full-rank are the necessary and sufficient conditions for a compute-efficient linear layer for GPT-2. </figcaption>
</figure>

**Einsum Performance Obeys Power Law Scaling.** When replacing the standard dense layers with Einsums, we find the resulting model’s loss continues to follow the usual $\mathcal{L}= \mathcal{L}_\infty + b C^{-a}$ compute-optimal scaling laws except with possibly different constants $a,b$. In , we visualize the compute-optimal scaling laws of various Einsums on our language modeling task, including those corresponding to previously proposed structures such as TT , Low-rank and BTT , as well as a generic Einsum with all entries of ${\boldsymbol{\mathbf{\theta}}}$ strictly positive. We report the reducible loss with an estimated $\mathcal{L}_\infty = 0.75$ subtracted. This finding suggests that all Einsums can be scaled to reach arbitrarily low reducible loss, each with a different rate that can be estimated from small-scale experiments.

**Parameter Sharing Leads to Worse Scaling.** As discussed in , the vast majority of Einsums implement some kind of parameter sharing, where the number of parameters $N$ in the Einsum relates to its MVM FLOPs $F$ via $N/F = \Theta(d^{-\omega}),$ for some $\omega > 0.$ In (left), we show the scaling laws of a wide range of Einsums (only including points on the compute-optimal frontier) colored by $\omega.$ We find larger values of $\omega$ lead to significantly worse scaling laws. To search for compute-efficient structures, we should therefore focus on the subspace with $\omega = 0.$

**Full-Rank Performs Best.** Within the $\omega = 0$ subspace, we find that $\psi$ becomes the next most important parameter. Recall $\psi \in [0, 1]$ is defined such that the rank of the Einsum scales as $\Theta(d^\psi).$ Einsums with $\psi < 1$ introduce information bottlenecks in the model by preventing the linear layers from accessing information from all the feature dimensions. The smaller $\psi$ is, the more severe this effect. In (middle), we show that small values of $\psi$ indeed lead to worse scaling laws. This observation further narrows down our search to the subspace with $\omega=0$ and $\psi=1,$ i.e. the space of full-rank BTT matrices.

**Any Full-Rank BTT Scales Similarly as Dense.** The $\omega=0$ and $\psi=1$ subspace contains the Monarch matrices and its generalization BTT matrices[^3]. From a computational perspective, a primary distinguishing factor among these structures is how close they resemble a dense matrix, which we characterize by their compute intensity $\nu \in [0, 1)$ defined so that $F/d = \Theta(d^{\nu}).$ $\nu$ is large whenever their exists large values (close to $1$) in the remaining allowed entries $(\theta_{\boldsymbol{\mathbf{AX}}}, \theta_{\boldsymbol{\mathbf{ABX}}}, \theta_{\boldsymbol{\mathbf{YB}}}, \theta_{\boldsymbol{\mathbf{YAB}}}, \theta_{\boldsymbol{\mathbf{AB}}}).$ In (right), we show that, somewhat surprisingly, $\nu$ has minimal effect on the scaling laws of these structures. Structures with different $\nu$ have almost indistinguishable scaling laws compared to each other and dense matrices, which has $\nu = 1$. This result shows that while dense matrices perform well compared to the vast majority of possible Einsums, their good performance does not arise from being dense, but rather from not sharing parameters and being full-rank.

**Reconciling with Results from Prior Work.** Our findings do not mean that structured matrices cannot outperform dense in other settings. Rather, it highlights that the relative performance between structures depends on what resource is controlled, as prior work has shown that low rank, Tensor-Train, Monarch, and BTT can significantly outperform dense in other settings such as controlling for memory, model size, or inference compute , rather than training compute. For example, when training dataset size instead of training compute is the primary bottleneck, such as on conventional vision datasets like CIFAR-10 and ImageNet, structured matrices have shown considerable advantage over dense as a function of model size and inference compute . In those settings, observe the benefits of structure likely arise through enabling computationally efficient wider layers.

<figure id="fig:more_scaling_laws_analysis">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/rebuttal/c5m/omega.pdf" data-original-image-title="" height="0.27\linewidth"></span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/rebuttal/c5m/psi.pdf" data-original-image-title="" height="0.27\linewidth"></span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/rebuttal/c5m/nu.pdf" data-original-image-title="" height="0.27\linewidth"></span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/rebuttal/mlp/omega.pdf" data-original-image-title="" height="0.28\linewidth">image</span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/rebuttal/mlp/psi.pdf" data-original-image-title="" height="0.28\linewidth"></span></td>
<td style="text-align: center;"><span class="image placeholder" data-original-image-src="figs/rebuttal/mlp/nu.pdf" data-original-image-title="" height="0.28\linewidth"></span></td>
</tr>
</tbody>
</table>
<figcaption><strong>Our findings about the effect of <span class="math inline">\((\omega, \psi, \nu)\)</span> on the scaling laws generalize to other settings.</strong> (<strong>Top row</strong>) Transformers trained with cross-entropy for autoregressive pixel generation on CIFAR-5M. (<strong>Bottom row</strong>) MLP trained with mean-squared-error loss on synthetic data generated by a large and randomly initialized MLP. </figcaption>
</figure>

## Our Findings Generalize to Other Settings

We now test if our findings derived from the GPT-2 experiments can generalize to other settings. We evaluate on the following two additional tasks where there is sufficient data to measure the compute-optimal scaling laws without repeating training data. We provide additional experiment details in .

**Autoregressive Pixel Modeling on CIFAR-5M.** We train transformers to autoregressively predict the RGB pixel values of images in the CIFAR-5M dataset , downsampled to $8\times8\times3$ resolution. (top row) shows qualitatively the same results as our GPT-2 experiments, where $\omega$ and $\psi$ have the most significant impact on the scaling laws, while varying $\nu$ yield only slight variations. In this particular case, having $\nu = 0.75$ (BTT with BTT-rank scaling as $d^{1/4}$) is better than having $\nu=0.5$ (Monarch matrices). One notable trend in this setup is that most Einsums, regardless of $\omega$ or $\psi$, outperform dense at small scales. We hypothesize this improved performance is due to Einsums having larger embedding dimensions than dense layers for a fixed parameter budget and can thus preserve more information about the input pixels at smaller model sizes.

**MLP Regression on Synthetic Data.** We train MLPs on a synthetic regression dataset where the target is a scalar-valued function defined by a large randomly initialized MLP, similar to the student-teacher setting in . In (bottom row), we observe qualitatively the same results as in the GPT-2 experiments.

Together, these additional results suggest there is some degree of universality associated with our findings on the effect of $\omega, \psi,$ and $\nu$ on the compute-optimal scaling laws of neural networks that use Einsums in place of dense matrices.

# Structured Mixture of Experts

In , we identified that Einsums with $\omega=0$ and $\psi=1$ perform the best, and $\omega > 0$ or $\psi < 1$ lead to worse-than-dense performance. The most impactful parameter on the scaling laws is $\omega,$ which measures how many parameters an Einsum has compared to the FLOPs for an MVM. Einsums that learn one parameter per FLOP perform significantly better than those that learn less than one parameter per FLOP. Therefore, a natural question arises: can we design structures that learn more than one parameter per FLOP, which we might expect will have even better scaling laws? Doing so requires that not all parameters are used in an MVM, which necessitates a sparse Mixture-of-Experts (MoE) like architecture . Furthermore, we would like the structure to be full-rank, i.e. $\psi = 1.$ In the following section, we introduce such a structure and demonstrate significant improvement over dense layers and the standard MoE architecture for training GPT-2.

<figure id="fig:moe">
<p><span class="image placeholder" data-original-image-src="figs/moe_scaling_loss_8.pdf" data-original-image-title="" height="0.26\linewidth">image</span> <span class="image placeholder" data-original-image-src="figs/moe_compute_mul_across_loss_8.pdf" data-original-image-title="" height="0.26\linewidth">image</span>  <span class="image placeholder" data-original-image-src="figs/compute_moe_summary.pdf" data-original-image-title="" height="0.26\linewidth">image</span>  <br />
<span class="image placeholder" data-original-image-src="figs/moe_compute_mul_across_loss_legend.pdf" data-original-image-title="">image</span> </p>
<figcaption> <strong>BTT Mixture-of-Experts has significantly better compute-optimal scaling laws than dense GPT-2 and its standard MoE variant.</strong> (<strong>Left</strong>): Compute-optimal frontier with <span class="math inline">\(8\)</span>. (<strong>Middle</strong>): 8 experts compute multiplier of BTT-MoE and standard MoE relative to dense as a function of FLOPs required by the dense model to achieve the same loss. (<strong>Right</strong>): Increasing the number of experts improves computational savings. Mean and standard deviation of the compute multiplier over all compute observations for 8 and 16 experts. </figcaption>
</figure>

## More Parameters than FLOPs via Mixture of Experts

One natural candidate for constructing such a layer via an Einsum is to turn a BTT with BTT-rank $E$, which involves a sum over the rank index $\rho=1, \ldots, E:$ $$Y_{\epsilon \phi}
      =
      \sum_{\alpha \gamma \rho}
         B_{\gamma \epsilon \phi \rho} A_{\alpha \gamma \phi \rho} X_{\alpha \gamma}$$ into a $k$-sparse sum: $$Y_{\epsilon \phi}
      =
      \sum_\rho g_\rho \underbrace{{\sum_{\alpha \gamma}
         B_{\gamma \epsilon \phi \rho} A_{\alpha \gamma \phi \rho} X_{\alpha \gamma}}}_{\text{output of } \rho \text{-th expert }},$$

where ${\boldsymbol{\mathbf{g}}} \in \mathbb{R}^{E}$ is a $k$-sparse vector so that only $k$ out of $E$ terms need to be computed. We interpret $k$ as the number of active experts and $E$ as the total number of experts. We compute ${\boldsymbol{\mathbf{g}}}$ via a softmax over the top-$k$ entries of the logits ${\boldsymbol{\mathbf{e}}}$ produced by a (dense) linear gating function ${\boldsymbol{\mathbf{e}}} = \mathrm{Linear}(X) \in \mathbb{R}^{E}.$ There is no need to make this gating function structured because its cost is negligible. We choose $\theta_{\boldsymbol{\mathbf{AX}}} = \theta_{\boldsymbol{\mathbf{ABX}}} = \theta_{\boldsymbol{\mathbf{YB}}} = \theta_{\boldsymbol{\mathbf{YAB}}} = 1/2$ so that each expert is full-rank. We follow the common practice of using $k=2.$ The resulting BTT-MoE layer is a BTT with BTT-rank $2$ (sum of two Monarch matrices) with input-dependent parameters. It is similarly straightforward to construct structured MoE from other structures by sparsifying the sum over $\rho$ with a gate. We use the load-balancing loss to encourage equal utilization of all experts .

In contrast to the standard MoE architecture used in transformer language models, which uses a sparse MoE for each entire feed-forward network (FFN) : $$\begin{aligned}
    {\boldsymbol{\mathbf{Y}}} = \sum_{i=1}^{E} g_i \underbrace{{\boldsymbol{\mathbf{W}}}^\downarrow_i \mathrm{ReLU}({\boldsymbol{\mathbf{W}}}^\uparrow_i {\boldsymbol{\mathbf{X}}})}_{\text{output of } i \text{-th FFN expert }},
\end{aligned}$$

BTT-MoE learns an MoE in every single linear layer of the model (except the language modeling head) and treats them equally, including the projection matrices ${\boldsymbol{\mathbf{W^Q}}}, {\boldsymbol{\mathbf{W^K}}}, {\boldsymbol{\mathbf{W^V}}}, {\boldsymbol{\mathbf{W^O}}}$ in the attention blocks . It learns more fine-grained routing decisions among the experts, with $\qty(\frac{1}{2}E(E-1))^{6M}$ possible combinations of the experts in a transformer with $M$ blocks, compared to $\qty(\frac{1}{2}E(E-1))^{M}$ for the standard MoE architecture.

## Compute Efficiency Gains

In , we show GPT-2 with BTT-MoE achieves better compute-optimal scaling laws compared to the dense model as well as the standard MoE, with $k=2$ and $E \in \{8, 16\}$. BTT-MoE consistently outperforms the standard MoE and the dense model. We quantify and compare the compute efficiency gains of BTT-MoE and standard MoE over dense models via the *compute multiplier*. A model with a compute multiplier of $\lambda$ means with $C$ training FLOPs it achieves the same loss as a dense model with $\lambda C$ training FLOPs. In , we show BTT-MoE is significantly more compute-efficient than the standard MoE for both $E=8$ and $E=16$. In particular, with $E=16$ experts, BTT-MoE achieves a compute multiplier of $\lambda=5.3_{\pm0.3},$ compared to $\lambda=4.1_{\pm0.3}$ for a standard MoE.

## Effect of Structures

In we show that replacing BTT-MoE with a sparse ($k=2$) sum of low-rank matrices (low-rank-MoE) or dense matrices (dense-MoE) also yields a nontrivial compute multiplier ($\sim2\times$) over the dense model, but is significantly less effective than BTT-MoE or even the standard MoE.

While the poor relative performance of low-rank-MoE is expected, this result shows that in addition to $\omega=0$ and $\psi=1,$ $\nu < 1$ is a desirable property for the base structure in a structured MoE. Using a dense structure with $\nu = 1$ means the experts are not complementary to each other since each one is able to represent the entire space of dense matrices.

<figure id="fig:lr_landscapes">
<p><span class="image placeholder" data-original-image-src="figs/lr_landscape.pdf" data-original-image-title="" width="0.8\linewidth">image</span><br />
 <span class="image placeholder" data-original-image-src="figs/lr_landscape_legend.pdf" data-original-image-title="" width="0.9\linewidth">image</span> </p>
<figcaption> <strong>Einsums trained with <span class="math inline">\(\mu\)</span>P achieve lower error and share an optimal base learning rate.</strong> We plot test error of 4 layered MLP models on CIFAR-10, where the hidden layers are Einsums. We vary the model widths in 64, 256, 1024 and 4096. The naive approach uses a global learning rate independent of width or structure and initializes the Einsums parameters with unit variance. </figcaption>
</figure>

# Scaling Optimization for Einsums

As prior work has shown, the optimal initialization scales and learning rate depend heavily on the structure of the linear layers and are critical for successfully training models with structured layers. Fortunately, the theory of the Maximal Update Parameterization ($\mu$P) and its application to various structured matrices in provides a template on how to reason about the appropriate initialization and learning rate scales for all Einsums.

In short, $\mu$P states that for a dense matrix ${\boldsymbol{\mathbf{W}}} \in \mathbb{R}^{{d_\mathrm{in}}\times {d_\mathrm{out}}}$, the optimal initialization standard deviation scales as $\sigma = \Theta(\sqrt{\min({d_\mathrm{in}}, {d_\mathrm{out}}) / d^{2}_{\text{in}}})$ and its learning rate as $\eta=\Theta(1/ {d_\mathrm{in}})$ if using Adam. Furthermore, shows we can apply $\mu$P to structured matrices as long as we can cast the MVM as a series of batched matrix multiplications (BMM). As shown in , we can indeed cast any Einsum as a series of BMMs and show that $d_{\text{in}}^{{\boldsymbol{\mathbf{A}}}} = d_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}},
d_{\text{out}}^{{\boldsymbol{\mathbf{A}}}} = d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}} d_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}}},$ $d_{\text{in}}^{{\boldsymbol{\mathbf{B}}}} = d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}}$ and $d_{\text{out}}^{{\boldsymbol{\mathbf{B}}}} = d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{B}}}}$. As a result, we can compute the optimal scaling of $\sigma_{{\boldsymbol{\mathbf{A}}}}, \sigma_{{\boldsymbol{\mathbf{B}}}}, \eta^{{\boldsymbol{\mathbf{A}}}}$, and $\eta^{{\boldsymbol{\mathbf{B}}}}.$ In particular, for Adam we have $\eta^{{\boldsymbol{\mathbf{A}}}} = \Theta(\frac{1}{d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}}}})$ and $\eta^{{\boldsymbol{\mathbf{B}}}} = \Theta(\frac{1}{d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}}})$. shows using $\mu$P leads to a stable optimal learning rate and better performance compared to naively using a constant global learning rate and unit initialization variance. This property allows us to transfer the learning rate between structures and model sizes, saving substantial compute for hyperparameter tuning. For $\mu$P, the learning rate refers to that used by a dense model with width $64$, which we transfer to the Einsum models of different widths and structures via the scaling rule identified earlier (see details in ).

Finally, we discuss in an alternative way to reason about the optimal learning rates of Einsums via Riemannian SGD (RSGD) . We analyze the effective learning rate prescribed by RSGD at initialization for asymptotically large Einsums and find it often agrees with the $\mu$P prescription derived above.

# Conclusion

Going beyond prior works that study hand-crafted structured matrices on a case-by-case basis, we introduce a continuous parameterization over the space of all structured matrices expressible as Einsums. Using this parameterization, we measure and compare the compute-optimal scaling laws of a wide range of known and novel structures, with the following key takeaways:

- *Compute-optimal scaling laws of Einsums are primarily governed by the parameter-sharing exponent $\omega$ and the rank exponent $\psi.$* Across tasks, we find all full-rank Einsums without parameter sharing (i.e. full-rank BTTs) scale similar to dense, while the remaining vast majority of Einsums consistently underperform dense as $\omega$ increases or $\psi$ decreases.

- *Existing structured matrices do not significantly outperform dense in the compute-optimal setting.* While low rank, Tensor-Train, Monarch, and BTT have shown advantages over dense in other settings, such as controlling for memory or model size, they generally perform worse or similar to dense when controlling for training compute. However, there are also instances in the compute-optimal regime where a full-rank structured representation with no parameter sharing can outperform dense layers. This advantage is most likely due to the ability to make wider structured layers for the same computational budget as narrower dense layers, which can particularly benefit smaller vision models, as we show on CIFAR-5M.

- *$\mu$P prescribes effective initialization and learning rate scaling for Einsums.* Breaking an Einsum down to a sequence of batched matrix multiplications, we extend prior work on structure-aware initialization and learning rate based on $\mu$P to arbitrary Einsums.

- *MoE over structured matrices is more efficient than standard MoE over entire FFNs.* By replacing every single dense linear layer with a sparse sum of structured matrices like BTT, compared to standard MoE which operates over entire FFNs, we create a more efficient MoE architecture, achieving over $5\times$ savings in compute on language modeling relative to dense. Scaling and improving the proposed structured MoE architecture are exciting directions for future work.

**Acknowledgements:** We thank Alan Amin, Nate Gruver, and Hoang Phan for helpful discussions. This work is supported by NSF CAREER IIS-2145492, NSF CDS&E-MSS 2134216, NSF HDR-2118310, BigHat Biosciences, Capital One, and an Amazon Research Award.

# Appendix Outline

-  shows specific examples of Einsums and their MVM computations.

-  shows the derivation for the taxonomy variables $\omega$, $\psi$ and $\nu.$

-  discusses learning rate scaling details and connections between $\mu$P and Riemannian SGD.

-  provides extended experiment results and experiment details.

-  discusses the generalization of our parameterization to Einsums of more than two factors.

-  shows how to further improve performance by structuring the Einsum axes to be compatible with the attention head structure in transformers.

-  comments on the hardware used in our experiments.

# Examples of Einsums

We explicitly show the expression for some common structures expressed as Einsums. Any index not appearing in the expression has a size of 1 and is thus omitted.

#### Dense

A one-factor Einsum would have been enough to represent dense, though one way to represent it within a two-factor Einsum is as: $$Y_{\phi}
      =
      \sum_{\gamma\phi}^{}
      B_{\gamma\phi} A_{\gamma \phi} X_{\gamma},$$ which is an over-parameterization due to the Hadamard product $B_{\gamma\phi} A_{\gamma \phi}.$

#### Low Rank

$$Y_{\epsilon}
      =
      \sum_{\alpha\phi}^{}
      B_{\epsilon\rho} A_{\rho \alpha} X_{\alpha},$$

#### Kronecker Product

$$Y_{\delta\epsilon}
      =
      \sum_{\alpha\beta}^{}
      B_{\beta\epsilon} A_{\alpha\delta} X_{\alpha\beta},$$

#### Tensor-Train

$$Y_{\delta\epsilon}
      =
      \sum_{\alpha\beta\rho}^{}
      B_{\beta\epsilon\rho} A_{\alpha\delta\rho} X_{\alpha\beta},$$

#### Monarch

$$Y_{\epsilon \phi}
      =
      \sum_{\alpha \gamma}
         B_{\gamma \epsilon \phi} A_{\alpha \gamma \phi} X_{\alpha \gamma}$$

#### Block Tensor-Train

$$Y_{\epsilon \phi}
      =
      \sum_{\alpha \gamma \rho}
         B_{\gamma \epsilon \phi \rho} A_{\alpha \gamma \phi \rho} X_{\alpha \gamma}$$

# Taxonomy Derivation

Here we present the derivation for the taxonomy variables $(\omega,\psi,\nu).$ It is helpful to first reason about the amount of compute required for calculating an Einsum, which will allow us to exclude some uninteresting Einsums from consideration and thereby simplify our analysis. We only consider two-factor Einsums representing a square matrix, but generalization to more factors and non-square matrices is straightforward.

For reference, we show the general expression again: $$\label{eq:2core}
      Y_{\delta \epsilon \phi}
      =
      \sum_{\alpha \beta \gamma \rho}
         B_{\beta \gamma \epsilon \phi \rho} A_{\alpha \gamma \delta \phi \rho} X_{\alpha \beta \gamma}$$

#### Computational Complexity.

There are three possible ways to compute , depending on whether the first contraction happens between $\{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\}$, $\{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{X}}}\}$, or $\{{\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{X}}}\}.$ The required FLOPs

[^4]

are $\Theta\qty(d^{2 + \theta_\rho}), \Theta\qty(d^{2 + \theta_\rho - \min(\theta_\alpha, \theta_\epsilon)}),$ and $\Theta\qty(d^{2 + \theta_\rho - \min(\theta_\beta, \theta_\delta)})$ respectively. The optimal computational path depends on the entries of ${\boldsymbol{\mathbf{\theta}}},$ and the optimal FLOPs $F$ required is given by $F=\Theta\qty(d^{2 + \theta_\rho - \max\qty{\min(\theta_\alpha, \theta_\epsilon), \min(\theta_\beta, \theta_\delta)}}).$

#### Removing the Exchange Redundancy.

The factors ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$ play an equivalent role in the Einsum expression, so each distinct structure is represented by two vectors that correspond to relabelling ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$ to each other. To remove this redundancy, we can require that first summing with ${\boldsymbol{\mathbf{A}}}$ is more computationally efficient. Thus, we require $$\min(\theta_\alpha, \theta_\epsilon) \geq \min(\theta_\beta, \theta_\delta),$$ which also simplifies the FLOPs to $F = \Theta\qty(d^{2 + \theta_\rho - \min(\theta_\alpha, \theta_\epsilon)}).$ To be more exact, we have $F = \rho d^2 \qty(\frac{1}{d_{\alpha}} + \frac{1}{d_{\epsilon}}).$

#### Degenerate Einsums.

Since the overall Einsum is a linear operator on $\mathbb{R}^d,$ any Einsum that requires more than $\Theta\qty(d^2)$ FLOPs to compute is degenerate in the sense that unnecessary computations are performed. For example, one such Einsum could correspond to a factorization ${\boldsymbol{\mathbf{U}}}{\boldsymbol{\mathbf{V}}}^\intercal, {\boldsymbol{\mathbf{U}}} \in \mathbb{R}^{d\times r}, {\boldsymbol{\mathbf{V}}} \in \mathbb{R}^{r \times d}$ where $r \gg d.$ For convenience, we will also define an Einsum whose cost is equal to $\Theta\qty(d^2)$ as degenerate since it is no more efficient than a dense matrix. Given the expression for FLOPs, we conclude that non-degenerate Einsum are those where $$\theta_\rho < \min(\theta_\alpha, \theta_\epsilon).$$ Intuitively, this requirement means that the rank dimension $d_\rho$, i.e. the range of the index $\rho$ connecting ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$, cannot be set too high, since after some point it becomes more efficient to simply use a dense matrix in place of the Einsum. In particular, we must have $d_\alpha > 1$ and $d_\epsilon > 1$ (except when $d = 1$).

#### Compute Intensity Exponent, $\nu$.

One defining characteristic of structured matrices is that the FLOPs $F$ for performing matrix-vector-multiplication (MVM) is sub-quadratic in $d.$ Equivalently, the compute intensity $F / d,$ i.e. FLOPs for an MVM normalized by the dimension, is sublinear. In our case, all non-degenerate Einsum have sublinear compute intensity. More precisely, we have $F / d = \Theta\qty(d^{\nu})$, where $\nu = 1 + \theta_\rho - \min(\theta_\alpha, \theta_\epsilon)$ takes value in $[0, 1)$ assuming non-degeneracy. The closer $\nu$ is to $1$, the more a Einsum resembles a dense matrix and vice versa. $\nu = 0$ corresponds to low rank matrices with $\Theta(1)$ rank.

#### Rank Exponent, $\psi$.

As we show in , the Einsum can be computed via two batched matrix multiplications where ${\boldsymbol{\mathbf{A}}}$ acts as a matrix consisting of $d_{\beta} d_{\gamma}$ blocks of $d_{\alpha} \times d_{\delta} d_{\phi} d_{\rho}$ matrices. Thus, $\text{rank}({\boldsymbol{\mathbf{A}}}) = \min(d, d_{\beta} d_{\gamma} d_{\delta} d_{\phi} d_{\rho}) =
d^{\min(1, \theta_{\beta} + \theta_{\gamma} + \theta_{\delta} + \theta_{\phi} + \theta_{\rho})}$ as $d_{\alpha} d_{\beta} d_{\gamma} = d$. Similarly, we have that ${\boldsymbol{\mathbf{B}}}$ acting as a matrix consisting of $d_{\delta} d_{\phi}$ blocks of $d_{\beta} d_{\gamma} d_{\rho} \times d_{\epsilon}$ matrices. Therefore we have that $\text{rank}({\boldsymbol{\mathbf{B}}}) =
d^{\min(1, \theta_{\beta} + \theta_{\gamma} + \theta_{\delta} + \theta_{\phi} + \theta_{\rho})}$. Since ${\boldsymbol{\mathbf{W}}}$ is the product of ${\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}},$ and some reshape operations (which are full-rank), $\text{rank}({\boldsymbol{\mathbf{W}}})= d^{\psi}$ is the minimum of $\text{rank}({\boldsymbol{\mathbf{A}}})$ and $\text{rank}({\boldsymbol{\mathbf{B}}}):$ $$\begin{split}
      \psi
      &=
      \min(
      1,
      \theta_{\beta} + \theta_{\gamma} + \theta_{\delta} + \theta_{\phi} + \theta_{\rho}
      )
      \\
      &=
      \min(
      1,
      2 + \theta_{\rho} - \theta_{\alpha} - \theta_{\epsilon},
      )
      .
    \end{split}$$ Note that, technically, when we write $\text{rank}({\boldsymbol{\mathbf{M}}}), {\boldsymbol{\mathbf{M}}} \in \{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{W}}}\},$ we mean the maximum possible rank of ${\boldsymbol{\mathbf{M}}}$ when its parameters are learned. Otherwise the equalities will become upperbounds. For low-rank we have ${\boldsymbol{\mathbf{\theta}}}=(1, 0, 0, 0, 1, 0, \theta_{\rho})$ and hence $\psi = \theta_{\rho}$. For dense we have ${\boldsymbol{\mathbf{\theta}}}=(0, 0, 1, 0, 0, 1, 0)$ and hence $\psi= 1$. For Kronecker we have ${\boldsymbol{\mathbf{\theta}}}=(1/2, 1/2, 0, 1/2, 1/2, 0, 0)$ and hence $\psi = 1$. For BTT we have $\psi = \min(1, \theta_{\gamma} + \theta_{\phi} + \theta_{\rho})$.

#### Parameter Sharing Exponent $\omega$.

The number of learnable parameters $N$ in a Einsum is simply the sum of the elements in ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}},$ which works out to be $N  =d_{\rho} d^{2} \qty(\frac{1}{d_{\alpha} d_{\delta}} + \frac{1}{d_{\beta} d_{\epsilon}})$. Since the FLOPs is $F = d_{\rho} d^2 \qty(\frac{1}{d_{\alpha}} + \frac{1}{d_{\epsilon}})$, only Einsums with $d_\delta = d_\beta = 1$, or equivalently $\theta_\delta = \theta_\beta = 0,$ have parameters matching FLOPs. In general, the number of parameters per FLOP $N / F = \Theta(d^{-\omega})$ with $\omega = \min(\theta_\alpha + \theta_\delta, \theta_\beta + \theta_\epsilon) - \min(\theta_\alpha, \theta_\epsilon) \geq 0.$

The $\omega = 0$ subspace is of particular interest because any Einsum outside of this subspace has an artificially limited expressivity per FLOP because it reuses each parameter multiple times. In this sense, the $\omega = 0$ subspace is the space of maximally expressive Einsums that maximizes expressivity per FLOP. We note that this subspace precisely corresponds to the Block Tensor-Train (BTT) structure proposed in , provided we allow a minor generalization of the original BTT structure so that an axis connects the first factor back to the last factor when there are more than 2 factors, exactly analogous to the generalization of the Tensor-Train structure to the Tensor Ring structure.

# Scaling Optimization for Einsums

## Learning Rate Scaling

In , we claimed that the Adam learning rates should be scaled as $\eta^{{\boldsymbol{\mathbf{A}}}} = \Theta(\frac{1}{d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}}}})$ and $\eta^{{\boldsymbol{\mathbf{B}}}} = \Theta(\frac{1}{d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}}})$. Arriving at these scaling rules requires (1) expressing an Einsum as a sequence of batched matrix multiplies (BMMs) and showing $d_{\text{in}}^{{\boldsymbol{\mathbf{A}}}} = d_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}},
d_{\text{out}}^{{\boldsymbol{\mathbf{A}}}} = d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}} d_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}}},$ $d_{\text{in}}^{{\boldsymbol{\mathbf{B}}}} = d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}}$ and $d_{\text{out}}^{{\boldsymbol{\mathbf{B}}}} = d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{B}}}}$, and (2) applying results from on learning rate scaling for structured matrices expressible in terms of BMMs. We now justify (1).

The general 2-factor Einsum $$Y_{\delta \epsilon \phi}
      =
      \sum_{\alpha \beta \gamma \rho}
         B_{\beta \gamma \epsilon \phi \rho} A_{\alpha \gamma \delta \phi \rho} X_{\alpha \beta \gamma}$$ can be computed in two steps. In step 1, $$Z_{\beta \gamma \delta \phi \rho}
      = \sum_{\alpha} A_{\alpha \gamma \delta \phi \rho} X_{\alpha \beta \gamma},$$ which is a BMM with $\beta\gamma$ acting as batch dimensions, $\alpha$ the input dimension, and $\delta\phi\rho$ the output dimensions. So $d_{\text{in}}^{{\boldsymbol{\mathbf{A}}}} = d_{{\boldsymbol{\mathbf{X}}}{\boldsymbol{\mathbf{A}}}},
d_{\text{out}}^{{\boldsymbol{\mathbf{A}}}} = d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{A}}}} d_{{\boldsymbol{\mathbf{Y}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}}}.$ In step 2, $$Y_{\delta \epsilon \phi}
      = \sum_{\beta \gamma \rho} B_{\beta \gamma \epsilon \phi \rho} Z_{\beta \gamma \delta \phi \rho},$$ which is a BMM with $\delta\phi$ acting batch dimensions, $\beta\gamma\rho$ input dimensions, and $\epsilon$ the output dimension. So $d_{\text{in}}^{{\boldsymbol{\mathbf{B}}}} = d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}}$ and $d_{\text{out}}^{{\boldsymbol{\mathbf{B}}}} = d_{{\boldsymbol{\mathbf{Y}}}{\boldsymbol{\mathbf{B}}}},$ as wanted.

In our experiments, we apply the above scaling rule to transfer from a learning rate $\eta$ used by a dense matrix with width $d_0$ to learning rates for ${\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}$ as $$\eta^{{\boldsymbol{\mathbf{A}}}} = \frac{d_0}{2d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}}}}\eta,\quad \eta^{{\boldsymbol{\mathbf{B}}}} = \frac{d_0}{2d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{X}}} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}} d_{{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}}}\eta,$$ where the additional factor of two is to account for both ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$ contributing updates to the output of each layer, following .

## Connections between $\mu$P and Riemannian SGD

Riemannian SGD (RSGD) is an optimization technique that allows us to perform the equivalent of SGD on a Riemannian manifold consisting of points $\{{\boldsymbol{\mathbf{q}}}\}$ with a metric $g_{\boldsymbol{\mathbf{q}}}$ . The updates of RSGD is almost identical to SGD: ${\boldsymbol{\mathbf{q}}}^{(t+1)} = {\boldsymbol{\mathbf{q}}}^{(t)} - \eta_{{\boldsymbol{\mathbf{q}}}} \, g_{{\boldsymbol{\mathbf{q}}}^{(t)}}^{-1} \nabla_{{\boldsymbol{\mathbf{q}}}}\mathcal{L}({\boldsymbol{\mathbf{q}}}^{(t)}),$ except that the gradient is multiplied by the inverse of the metric $g_{{\boldsymbol{\mathbf{q}}}}$. In our case, we want to mimic training the Einsum parameters ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$ as if we were training ${\boldsymbol{\mathbf{W}}}$ with SGD directly, even though we will never represent ${\boldsymbol{\mathbf{W}}}$ explicitly. Thus, we identify (flattened) $({\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}})$ with ${\boldsymbol{\mathbf{q}}}$ and specify its metric as the pull-back metric of the Euclidean metric $g_{{\boldsymbol{\mathbf{W}}}} = {\boldsymbol{\mathbf{I}}}$ on ${\boldsymbol{\mathbf{W}}}$, which is given by $g_{{\boldsymbol{\mathbf{q}}}} = {\boldsymbol{\mathbf{J}}}({\boldsymbol{\mathbf{q}}})^{\intercal} {\boldsymbol{\mathbf{J}}}({\boldsymbol{\mathbf{q}}})$ where ${\boldsymbol{\mathbf{J}}}({\boldsymbol{\mathbf{q}}}) = \pdv{{\boldsymbol{\mathbf{W}}}}{{\boldsymbol{\mathbf{q}}}}.$ The RSGD updates to ${\boldsymbol{\mathbf{q}}}$ is therefore $$\begin{split}
      {\boldsymbol{\mathbf{q}}}^{(t+1)}
      =
      {\boldsymbol{\mathbf{q}}}^{(t)}
      - \eta_{{\boldsymbol{\mathbf{q}}}} \,
      \qty({\boldsymbol{\mathbf{J}}}({\boldsymbol{\mathbf{q}}}^{(t)})^{\intercal} {\boldsymbol{\mathbf{J}}}({\boldsymbol{\mathbf{q}}}^{(t)}))^{-1} \nabla_{{\boldsymbol{\mathbf{q}}}}\mathcal{L}({\boldsymbol{\mathbf{q}}}^{(t)}).
    \end{split}$$

Exactly computing the inverse metric would require $O(P^3)$ time, where $P$ is the number of parameters in ${\boldsymbol{\mathbf{A}}},{\boldsymbol{\mathbf{B}}}$. It is, therefore, too expensive to run RSGD during training. However, we can efficiently approximate the inverse metric at initialization in a way that is exact for asymptotically large matrices. We compute ${\boldsymbol{\mathbf{J}}}^{\intercal}{\boldsymbol{\mathbf{J}}}$ through three blocks ${\boldsymbol{\mathbf{J}}}_{{\boldsymbol{\mathbf{A}}}}^{\intercal} {\boldsymbol{\mathbf{J}}}^{\vphantom{\intercal}}_{{\boldsymbol{\mathbf{A}}}}$, ${\boldsymbol{\mathbf{J}}}^{\intercal}_{{\boldsymbol{\mathbf{B}}}} {\boldsymbol{\mathbf{J}}}^{\vphantom{\intercal}}_{{\boldsymbol{\mathbf{B}}}}$, and ${\boldsymbol{\mathbf{J}}}_{{\boldsymbol{\mathbf{A}}}}^{\intercal} {\boldsymbol{\mathbf{J}}}^{\vphantom{\intercal}}_{{\boldsymbol{\mathbf{B}}}}$ and note that for a fixed ${\boldsymbol{\mathbf{\theta}}}$ as we increase both ${d_\mathrm{in}}$ and ${d_\mathrm{out}}$, we have that ${\boldsymbol{\mathbf{J}}}_{{\boldsymbol{\mathbf{A}}}}^{\intercal} {\boldsymbol{\mathbf{J}}}^{\vphantom{\intercal}}_{{\boldsymbol{\mathbf{A}}}} \approx d_{\beta} d_{\epsilon} \sigma_{{\boldsymbol{\mathbf{B}}}}^{2} {\boldsymbol{\mathbf{I}}}$, ${\boldsymbol{\mathbf{J}}}_{{\boldsymbol{\mathbf{B}}}}^{\intercal} {\boldsymbol{\mathbf{J}}}^{\vphantom{\intercal}}_{{\boldsymbol{\mathbf{B}}}} \approx d_{\alpha} d_{\delta} \sigma_{{\boldsymbol{\mathbf{A}}}}^{2} {\boldsymbol{\mathbf{I}}}$, and ${\boldsymbol{\mathbf{J}}}_{{\boldsymbol{\mathbf{A}}}}^{\intercal} {\boldsymbol{\mathbf{J}}}^{\vphantom{\intercal}}_{{\boldsymbol{\mathbf{B}}}} \approx {\boldsymbol{\mathbf{0}}}$ where $\sigma_{{\boldsymbol{\mathbf{A}}}}$ and $\sigma_{{\boldsymbol{\mathbf{B}}}}$ denote the initialization scales of ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$. Therefore we have $$\begin{split}
      ({\boldsymbol{\mathbf{J}}}^{\intercal} {\boldsymbol{\mathbf{J}}})^{-1}
      \approx
      \begin{pmatrix}
        {\boldsymbol{\mathbf{I}}} / (d_{\beta} d_{\epsilon} \sigma_{{\boldsymbol{\mathbf{B}}}}^{2})
        &
        {\boldsymbol{\mathbf{0}}}
        \\
        {\boldsymbol{\mathbf{0}}} &
        {\boldsymbol{\mathbf{I}}} / (d_{\alpha} d_{\delta} \sigma_{{\boldsymbol{\mathbf{A}}}}^{2})
      \end{pmatrix}
      .
    \end{split}$$ We can now identify $\Theta\qty(1/ (d_{\beta} d_{\epsilon} \sigma_{{\boldsymbol{\mathbf{B}}}}^{2}))$ as $\eta_{\boldsymbol{\mathbf{A}}}$ and $\Theta\qty(1 / (d_{\alpha} d_{\delta} \sigma_{{\boldsymbol{\mathbf{A}}}}^{2}))$ as $\eta_{\boldsymbol{\mathbf{B}}}.$ If we further use the initialization scales prescribed by $\mu$P as given in , we have $$\begin{split}
      \eta^{{\boldsymbol{\mathbf{A}}}}_{\text{RSGD}}
      =
      \Theta\left(\frac{1}{d_{\beta} d_{\epsilon}} \frac{d_{\beta}^{2} d_{\gamma}^{2} d_{\rho}^{2}}{\min(d_{\epsilon}, d_{\beta} d_{\gamma} d_{\rho})}\right)
      \quad \text{and} \quad
      \eta^{{\boldsymbol{\mathbf{B}}}}_{\text{RSGD}}
      =
      \Theta\left(\frac{1}{d_{\alpha} d_{\delta}} \frac{d_{\alpha}^{2}}{\min(d_{\alpha}, d_{\delta} d_{\phi} d_{\rho})}\right).
    \end{split}$$ It is now interesting to compare these scalings to the prescriptions of $\mu$P SGD. For SGD, $\mu$P proposes to scale the learning rate as $\Theta\qty({d_\mathrm{out}}/{d_\mathrm{in}})$ for dense matrices. Unlike in Adam, to extend $\mu$P to Einsums, we need not only to replace ${d_\mathrm{in}}, {d_\mathrm{out}}$ with $d_{\text{in}}^{{\boldsymbol{\mathbf{A}}}}, d_{\text{out}}^{{\boldsymbol{\mathbf{A}}}}$, or $d_{\text{in}}^{{\boldsymbol{\mathbf{B}}}}$ and $d_{\text{out}}^{{\boldsymbol{\mathbf{B}}}},$ but also scale down the learning rate as $\Theta\qty(1/d_\beta)$ for ${\boldsymbol{\mathbf{A}}}$ and $\Theta\qty(1/d_\delta)$ for ${\boldsymbol{\mathbf{B}}}.$ The final $\mu$P learning rates have two possibilities depending on whether we consider ${\boldsymbol{\mathbf{A}}}$ or ${\boldsymbol{\mathbf{B}}}$ as the first layer. If we consider ${\boldsymbol{\mathbf{A}}}$ as the first layer, then $$\begin{split}
      \eta^{{\boldsymbol{\mathbf{A}}}}_{\mu\text{P}}
      =
      \Theta
      \left(\frac{1}{d_{\beta}} \frac{d_{\beta} d_{\gamma} d_{\rho}}{d_{\alpha}}\right)
      \quad \text{and} \quad
      \eta^{{\boldsymbol{\mathbf{B}}}}_{\mu\text{P}}
      =
      \Theta\left(\frac{1}{d_{\delta}} \frac{d_{\epsilon}}{d_{\delta} d_{\phi} d_{\rho}}\right)
      .
    \end{split}$$ Evidently, the two approaches don’t always agree. However, they are identical when $d_\alpha = d_\epsilon, d_\beta d_\gamma d_\rho \leq d_\epsilon,$ and $d_\delta d_\phi d_\rho \leq d_\alpha.$ In fact, many structures satisfy this condition. If we assume the structure is symmetric, meaning its transpose can be represented with the same ${\boldsymbol{\mathbf{\theta}}},$ i.e. $d_\alpha=d_\epsilon, d_\beta = d_\delta$, and $d_\gamma = d_\phi,$ then the remaining conditions simplify to only $d_\rho \leq \frac{d^2_\alpha}{d}.$ Therefore, any symmetric Einsum with $\theta_\alpha \geq 1/2$ and $\theta_\rho \leq 2\theta_\alpha - 1$ satisfy these conditions. Thus, for these structures, $\mu$P SGD can be viewed as an approximation to RSGD that is valid at initialization in the infinite-width limit. While we cannot establish a direct connection between RSGD and $\mu$P Adam, which is what we use in our experiments and broadly in large-scale training, $\mu$P Adam is similar to $\mu$P SGD in that it maximizes feature learning per layer . The connection between $\mu$P SGD and RSGD therefore indirectly provides an alternative justification for $\mu$P Adam.

# Experiments

## GPT-2 with Original Vocabulary and Longer Context

In , we show our findings in translate to the more standard GPT-2 evaluation with a longer sequence length of 512 and its original vocabulary of 50,257 tokens. We train models with $L=12$ layers up to the GPT-2 Small size by increasing width $d$. We use Adam with a base learning rate of $0.002$ for a $L=3, d=256$ dense model, which is scaled to other models via $\mu$P. Since the language modeling head contains a significant fraction of the parameters for models of this scale, we replace all layers, including the head, with Einsums.

Qualitatively, differs from in two ways: 1) the scaling laws are less power law like and show some curvature on a log-log scale, and 2) BTT with $\nu > 0$ seems to perform better than $\nu=0.$ We believe 1) is due to the increased context length and vocabulary size, making the loss no longer follow a clean power law at the small scales we tested . This was an important motivation for performing experiments with a smaller vocabulary size and context length in . Similarly, we believe the increased vocabulary size and context length contributed to 2), as a larger $\nu$ implies at small scales a higher fraction of compute are in the transformer blocks rather than the language modeling head, which likely improves performance. By contrast, in our setup in , the model dimension $d$ dominates the vocabulary size and context length, leading to less significant finite-size effects.

<figure id="fig:original_gpt_scaling_laws_analysis">
<p><span class="image placeholder" data-original-image-src="figs/rebuttal/full_vocab/omega.pdf" data-original-image-title="" height="0.23\linewidth">image</span> <span class="image placeholder" data-original-image-src="figs/rebuttal/full_vocab/psi.pdf" data-original-image-title="" height="0.23\linewidth">image</span> <span class="image placeholder" data-original-image-src="figs/rebuttal/full_vocab/nu.pdf" data-original-image-title="" height="0.23\linewidth">image</span></p>
<figcaption> <strong>The taxonomy parameters <span class="math inline">\((\omega, \psi)\)</span> explain differences in the Einsum scaling laws for 12-layer GPT-2 models with standard vocabulary (50,257 tokens) and a context length of 512.</strong> Small <span class="math inline">\(\omega\)</span> (no parameter sharing) and large <span class="math inline">\(\psi\)</span> (full-rank) are necessary for a structure to perform well, while variation in <span class="math inline">\(\nu\)</span> has a much smaller impact on performance. </figcaption>
</figure>

## Autoregressive Pixel Modeling on CIFAR-5M

We train 2 and 3 layer transformers with Adam using a base learning rate of 3e-3 for a width 64 dense model. The width ranges from 32 to 512 for dense and 32 to 1024 for Einsums. All models are trained for 2 epochs with a batch size of 64. We subtract an estimated irreducible loss of 3.25 before reporting the loss in (top row).

## MLP Regression on Synthetic Data.

We train 3-layer MLP models with width $d \in [64, 4096]$ for a maximum of $10^6$ steps and a batch size of 4096 on an effectively infinite synthetic dataset. The synthetic dataset is obtained by querying a scalar-valued target function on $\mathbb{R}^8$ with inputs drawn from a Gaussian distribution. The target function is a randomly initialized target MLP with 6 layers and a hidden dimension of 1024. We minimize the Mean-Squared-Error (MSE) loss. We train with Adam using a base learning rate of 1e-3 for a width 64 dense model. We report the raw MSE loss in (bottom row).

# Generalization to More than Two Factors

The generalization to more factors is easy to understand if we consider the set of edges that define the Einsum $\mathcal{E} = \{S \subseteq \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{Y}}}\}: \left|S \right| \geq 2 \text{  and  } \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{Y}}}\} \not\subseteq S\}$. For example, for three factors, the edges that connect to ${\boldsymbol{\mathbf{X}}}$ are $\{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}\} \leftrightarrow i_{1}, \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{B}}}\} \leftrightarrow i_{2}, \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow i_{3},
\{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\} \leftrightarrow i_{4}, \{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow i_{5}$ $\{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow i_{6}$ and $\{{\boldsymbol{\mathbf{X}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow i_{7}$. The edges that connect to ${\boldsymbol{\mathbf{Y}}}$ are $\{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{A}}}\} \leftrightarrow j_{1}, \{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{B}}}\} \leftrightarrow j_{2}, \{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow j_{3},
\{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\} \leftrightarrow j_{4}, \{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow j_{5}$ $\{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow j_{6}$ and $\{{\boldsymbol{\mathbf{Y}}}, {\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow j_{7}$. The edges between the factors are $\{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}\} \leftrightarrow r_{1}, \{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow r_{2}$ and $\{{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}, {\boldsymbol{\mathbf{C}}}\} \leftrightarrow r_{3}$. The expression for a three factor would be $$\begin{split}
      Y_{{\boldsymbol{\mathbf{j}}}}
      =
      \sum_{{\boldsymbol{\mathbf{i}}}, {\boldsymbol{\mathbf{r}}}}
      A_{i_{1}i_{4}i_{6}i_{7}j_{1}j_{4}j_{6}j_{7}r_{1}r_{3}}
      B_{i_{2}i_{4}i_{5}i_{7}j_{2}j_{4}j_{5}j_{7}r_{1}r_{3}}
      C_{i_{3}i_{5}i_{6}i_{7}j_{3}j_{5}j_{6}j_{7}r_{2}r_{3}}
      X_{{\boldsymbol{\mathbf{i}}}}
      .
    \end{split}$$ For more factors we follow the combinatorial procedure of listing the sets and adding an index based on each edge in $\mathcal{E}$.

# Exploiting the Attention Structure

In self-attention, given an embedding vector ${\boldsymbol{\mathbf{x}}} \in \mathbb{R}^{d}$, we compute ${\boldsymbol{\mathbf{q}}} = {\boldsymbol{\mathbf{W}}}^{{\boldsymbol{\mathbf{Q}}}}{\boldsymbol{\mathbf{x}}}$, ${\boldsymbol{\mathbf{v}}} = {\boldsymbol{\mathbf{W}}}^{{\boldsymbol{\mathbf{V}}}}{\boldsymbol{\mathbf{x}}}$ and ${\boldsymbol{\mathbf{k}}} = {\boldsymbol{\mathbf{W}}}^{{\boldsymbol{\mathbf{K}}}}{\boldsymbol{\mathbf{x}}}$. After computing each of the ${\boldsymbol{\mathbf{q}}}, {\boldsymbol{\mathbf{k}}}, {\boldsymbol{\mathbf{v}}} \in \mathbb{R}^{d}$ vectors, they are reshaped to produce one smaller vector per attention head: $q_{i} \to q_{hj}, k_{i} \to k_{hj}, v_{i} \to v_{hj}$ where $h$ is an axis of size $H,$ the number of attention heads, and $j$ is an axis of size $d/H.$ When replacing ${\boldsymbol{\mathbf{W}}}^{{\boldsymbol{\mathbf{Q}}}}, {\boldsymbol{\mathbf{W}}}^{{\boldsymbol{\mathbf{K}}}}, {\boldsymbol{\mathbf{W}}}^{{\boldsymbol{\mathbf{V}}}}$ with BTTs, it is therefore more natural to have the BTT output axes $\epsilon\phi$ coincide with $hj$ so the MVM is aware of the attention head structure rather than potentially mixing different heads. Similarly, when replacing ${\boldsymbol{\mathbf{W}}}^{{\boldsymbol{\mathbf{O}}}}$ with a BTT, it is most natural to have the BTT input axes $\alpha\gamma$ coincide with $hj.$ In , we show doing so slightly improves compute efficiency by an average of 17% over naively replacing all attention and FFN matrices with BTT, which corresponds to ${\boldsymbol{\mathbf{\theta}}}=(1/2,0,1/2,0,1/2,1/2,0)$ in .

# Hardware specifications

Our experiments in are run on on A100 and H100 GPUs. The CIFAR-10 experiments in were run on RTX2080 Ti and RTX Titan GPUs.

[^1]: Equal contribution. Correspondence to `ap6604@nyu.edu`, `sq2129@nyu.edu`, `andrewgw@cims.nyu.edu`.

[^2]: Technically, we consider everything that can be expressed via `torch.einsum`, which is slightly more general than the Einstein summation, which allows an index to appear at most twice.

[^3]: In the 2-factor case, this space corresponds to all Block Low-Rank matrices that are full-rank.

[^4]: We use the more familiar term FLOPs as a stand-in for MACs (Multiply-Accumulate), even though $1$ MAC is technically $2$ FLOPs.
