\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amestoy et~al.(2015)Amestoy, Ashcraft, Boiteau, Buttari, l'Excellent,
  and Weisbecker]{amestoy2015improving}
Patrick Amestoy, Cleve Ashcraft, Olivier Boiteau, Alfredo Buttari, Jean-Yves
  l'Excellent, and Cl{\'e}ment Weisbecker.
\newblock Improving multifrontal methods by means of block low-rank
  representations.
\newblock \emph{SIAM Journal on Scientific Computing}, 37\penalty0
  (3):\penalty0 A1451--A1474, 2015.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and
  Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Bonnabel(2011)]{bonnabel2011rsgd}
Silvere Bonnabel.
\newblock {Stochastic gradient descent on Riemannian manifolds}.
\newblock \emph{arXiv 1111.5280}, 2011.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and
  R\'{e}]{dao2019butterfly}
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\'{e}.
\newblock {Learning Fast Algorithms for Linear Transforms Using Butterfly
  Factorizations}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Dao et~al.(2022)Dao, Chen, Sohoni, Desai, Poli, Grogan, Liu, Rao,
  Rudra, and R\'{e}]{dao2022monarch}
Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
  Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\'{e}.
\newblock {Monarch: Expressive Structured Matrices for Efficient and Accurate
  Training}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (120):\penalty0 1--39, 2022.

\bibitem[Fu et~al.(2023)Fu, Arora, Grogan, Johnson, Eyuboglu, Thomas, Spector,
  Poli, Rudra, and R\'{e}]{fu2023mixer}
Daniel~Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu,
  Armin~W. Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher
  R\'{e}.
\newblock {Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2023.

\bibitem[Grosse and Martens(2016)]{grosse2016kfac}
Roger Grosse and James Martens.
\newblock {A Kronecker-Factored Approximate Fisher Matrix for Convolution
  Layers}.
\newblock \emph{arXiv 1602.01407}, 2016.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,
  Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scaling}
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
  Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{arXiv preprint arXiv:2010.14701}, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford,
  Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
  Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou
  Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Lee and Kim(2023)]{lee2023differentiable}
Changwoo Lee and Hun-Seok Kim.
\newblock Differentiable learning of generalized structured matrices for
  efficient deep neural networks.
\newblock \emph{arXiv preprint arXiv:2310.18882}, 2023.

\bibitem[Lialin et~al.(2023)Lialin, Muckatira, Shivagunde, and
  Rumshisky]{lialin2023relora}
Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky.
\newblock Relora: High-rank training through low-rank updates.
\newblock In \emph{Workshop on Advancing Neural Network Training: Computational
  Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)},
  2023.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Neyshabur, and
  Sedghi]{nakkiran2020deep}
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi.
\newblock The deep bootstrap framework: Good online learners are good offline
  generalizers.
\newblock \emph{arXiv preprint arXiv:2010.08127}, 2020.

\bibitem[Oseledets(2011)]{oseledets2011tt}
I.~V. Oseledets.
\newblock {Tensor-Train Decomposition}.
\newblock \emph{SIAM Journal on Scientific Computing}, 2011.

\bibitem[Potapczynski et~al.(2023)Potapczynski, Finzi, Pleiss, and
  Wilson]{potapczynski2023cola}
Andres Potapczynski, Marc Finzi, Geoff Pleiss, and Andrew~Gordon Wilson.
\newblock {CoLA: Exploiting Compositional Structure for Automatic and Efficient
  Numerical Linear Algebra}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2023.

\bibitem[Qiu et~al.(2024)Qiu, Potapczynski, Finzi, Goldblum, and
  Wilson]{qiu2024compute}
Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, and Andrew~Gordon
  Wilson.
\newblock Compute better spent: Replacing dense layers with structured
  matrices.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock {Language Models are Unsupervised Multitask Learners}.
\newblock \emph{OpenAI}, 2019.

\bibitem[Saat{\c{c}}i(2012)]{saatcci2012scalable}
Yunus Saat{\c{c}}i.
\newblock \emph{Scalable inference for structured Gaussian process models}.
\newblock PhD thesis, Citeseer, 2012.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Shen et~al.(2024)Shen, Guo, Cai, and Qin]{shen2024jetmoe}
Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin.
\newblock Jetmoe: Reaching llama2 performance with 0.1 m dollars.
\newblock \emph{arXiv preprint arXiv:2404.07413}, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wilson et~al.(2014)Wilson, Gilboa, Nehorai, and
  Cunningham]{wilson2014fast}
Andrew~Gordon Wilson, Elad Gilboa, Arye Nehorai, and John~P. Cunningham.
\newblock {Fast Kernel Learning for Multidimensional Pattern Extrapolation}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2014.

\bibitem[Yang and Hu(2021)]{yang2021infty}
Greg Yang and Edward~J. Hu.
\newblock {Feature Learning in Infinite-Width Neural Networks}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Yang and Littwin(2023)]{yang2023iv}
Greg Yang and Etai Littwin.
\newblock {Tensor Programs IVb: Adaptive Optimization in the Infinite-Width
  Limit}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2023.

\bibitem[Yang et~al.(2021)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder,
  Pachocki, Chen, and Gao]{yang2021v}
Greg Yang, Edward~J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David
  Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot
  Hyperparameter Transfer}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Yang et~al.(2023)Yang, Simon, and Bernstein]{yang2023spectral}
Greg Yang, James~B. Simon, and Jeremy Bernstein.
\newblock {A Spectral Condition for Feature Learning}.
\newblock \emph{Preprint arXiv:2310.17813}, 2023.

\bibitem[Zhao et~al.(2016)Zhao, Zhou, Xie, Zhang, and Cichocki]{zhao2016tensor}
Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki.
\newblock Tensor ring decomposition.
\newblock \emph{arXiv preprint arXiv:1606.05535}, 2016.

\end{thebibliography}
