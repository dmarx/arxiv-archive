{
  "arxivId": "2410.02117",
  "title": "Searching for Efficient Linear Layers over a Continuous Space of\n  Structured Matrices",
  "authors": "Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen, Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson",
  "abstract": "Dense linear layers are the dominant computational bottleneck in large neural\nnetworks, presenting a critical need for more efficient alternatives. Previous\nefforts focused on a small number of hand-crafted structured matrices and\nneglected to investigate whether these structures can surpass dense layers in\nterms of compute-optimal scaling laws when both the model size and training\nexamples are optimally allocated. In this work, we present a unifying framework\nthat enables searching among all linear operators expressible via an Einstein\nsummation. This framework encompasses many previously proposed structures, such\nas low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,\nalong with many novel structures. To analyze the framework, we develop a\ntaxonomy of all such operators based on their computational and algebraic\nproperties and show that differences in the compute-optimal scaling laws are\nmostly governed by a small number of variables that we introduce. Namely, a\nsmall $\\omega$ (which measures parameter sharing) and large $\\psi$ (which\nmeasures the rank) reliably led to better scaling laws. Guided by the insight\nthat full-rank structures that maximize parameters per unit of compute perform\nthe best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture\nobtained by sparsifying computation in the BTT structure. In contrast to the\nstandard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE\nin every single linear layer of the model, including the projection matrices in\nthe attention blocks. We find BTT-MoE provides a substantial compute-efficiency\ngain over dense layers and standard MoE.",
  "url": "https://arxiv.org/abs/2410.02117",
  "issue_number": 131,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/131",
  "created_at": "2024-12-24T19:26:42.741915",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null
}