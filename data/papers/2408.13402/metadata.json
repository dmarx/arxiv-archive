{
  "arxivId": "2408.13402",
  "title": "LLaVaOLMoBitnet1B: Ternary LLM goes Multimodal!",
  "authors": "Jainaveen Sundaram, Ravi Iyer",
  "abstract": "Multimodal Large Language Models (MM-LLMs) have seen significant advancements\nin the last year, demonstrating impressive performance across tasks. However,\nto truly democratize AI, models must exhibit strong capabilities and be able to\nrun efficiently on small compute footprints accessible by most. Part of this\nquest, we introduce LLaVaOLMoBitnet1B - the first Ternary Multimodal LLM\ncapable of accepting Image(s)+Text inputs to produce coherent textual\nresponses. The model is fully open-sourced along with training scripts to\nencourage further research in this space. This accompanying technical report\nhighlights the training process, evaluation details, challenges associated with\nternary models and future opportunities. Link to the model:\nhttps://huggingface.co/IntelLabs/LlavaOLMoBitnet1B",
  "url": "https://arxiv.org/abs/2408.13402",
  "issue_number": 624,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/624",
  "created_at": "2024-12-30T20:04:21.779289",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 16,
  "last_read": "2024-12-30T21:20:33.349696",
  "last_visited": "2024-12-30T21:19:26.474000+00:00",
  "main_tex_file": null,
  "published_date": "2024-08-23T23:00:19Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}