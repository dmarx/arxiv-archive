@article{lowe2017multi,
	title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive 
	Environments},
	author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, 
	Pieter and Mordatch, Igor},
	journal={arXiv preprint arXiv:1706.02275},
	year={2017}
}

@article{das2017learning,
  title={Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning},
  author={Das, Abhishek and Kottur, Satwik and Moura, Jos{\'e} MF and Lee, Stefan and Batra, Dhruv},
  journal={arXiv preprint arXiv:1703.06585},
  year={2017}
}

@article{lazaridou2016multi,
  title={Multi-agent cooperation and the emergence of (natural) language},
  author={Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
  journal={arXiv preprint arXiv:1612.07182},
  year={2016}
}

@article{mordatch2017emergence,
  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},
  author={Mordatch, Igor and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1703.04908},
  year={2017}
}

@article{omidshafiei2017deep,
  title={Deep Decentralized Multi-task Multi-Agent RL under Partial Observability},
  author={Omidshafiei, Shayegan and Pazis, Jason and Amato, Christopher and How, Jonathan P and Vian, John},
  journal={arXiv preprint arXiv:1703.06182},
  year={2017},
}

@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{guptacooperative,
  title={Cooperative Multi-Agent Control Using Deep Reinforcement Learning},
  author={Gupta, Jayesh K and Egorov, Maxim and Kochenderfer, Mykel},
  year={2017}
}

@inproceedings{foerster2017stabilising,
  title={Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning},
  author={Foerster, Jakob and Nardelli, Nantas and Farquhar, Gregory and Torr, Philip and Kohli, Pushmeet and Whiteson, Shimon and others},
    booktitle={Proceedings of The 34th International Conference on Machine Learning},
  year={2017}
}


@article{peng2017multiagent,
  title={Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games},
  author={Peng, Peng and Yuan, Quan and Wen, Ying and Yang, Yaodong and Tang, Zhenkun and Long, Haitao and Wang, Jun},
  journal={arXiv preprint arXiv:1703.10069},
  year={2017}
}

@Article{Oliehoek08JAIR,
    author =        {Frans A. Oliehoek and Matthijs T. J. Spaan and Nikos
                    Vlassis},
    title =         {Optimal and Approximate {Q}-value Functions for
                    Decentralized {POMDPs}},
    journal =       JAIR,
    year =          2008,
    volume =        {32},
    pages =         {289--353},
    abstract = {
    Decision-theoretic planning is a popular approach to sequential
    decision making problems, because it treats uncertainty in sensing
    and acting in a principled way. In single-agent frameworks like
    MDPs and POMDPs, planning can be carried out by resorting to
    Q-value functions: an optimal Q-value function Q* is computed in a
    recursive manner by dynamic programming, and then an optimal
    policy is extracted from Q*.  In this paper we study whether
    similar Q-value functions can be defined for decentralized POMDP
    models (Dec-POMDPs), and how policies can be extracted from such
    value functions. We define two forms of the optimal Q-value
    function for Dec-POMDPs: one that gives a normative description as
    the Q-value function of an optimal pure joint policy and another
    one that is sequentially rational and thus gives a recipe for
    computation. This computation, however, is infeasible for all but
    the smallest problems. Therefore, we analyze various approximate
    Q-value functions that allow for efficient computation. We
    describe how they relate, and we prove that they all provide an
    upper bound to the optimal Q-value function Q*. Finally, unifying
    some previous approaches for solving Dec-POMDPs, we describe a
    family of algorithms for extracting policies from such Q-value
    functions, and perform an experimental evaluation on existing test
    problems, including a new firefighting benchmark problem.
    }
}
@article{jim2000talking,
	title={Talking helps: Evolving communicating agents for the predator-prey pursuit problem},
	author={Jim, Kam-Chuen and Giles, C Lee},
	journal={artificial life},
	volume={6},
	number={3},
	pages={237--254},
	year={2000},
	publisher={MIT Press}
}

@article{kraemer2016multi,
  title={Multi-agent reinforcement learning as a rehearsal for decentralized planning},
  author={Kraemer, Landon and Banerjee, Bikramjit},
  journal={Neurocomputing},
  volume={190},
  pages={82--94},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{VanDerPol16LICMAS,
    title =     {Coordinated Deep Reinforcement Learners for Traffic Light Control},
    correctAuthor =    {{\noopsort{Pol}van der Pol}, Elise and Oliehoek, Frans A. },
    author =    {Van der Pol, Elise and Oliehoek, Frans A. },
    year =      2016,
    booktitle = {NIPS'16 Workshop on Learning, Inference and Control of Multi-Agent Systems}
}

@inproceedings{kuyer:ecml08,
  title = "Multiagent Reinforcement Learning for Urban Traffic Control using Coordination Graphs",
  author = "Lior Kuyer and Shimon Whiteson and Bram Bakker and Nikos Vlassis",
  year = "2008",
  booktitle = "ECML 2008: Proceedings of the Nineteenth European Conference on Machine Learning",
  month = "September",
  pages = "656-671",
}

@inproceedings{huang2016emergence,
  title={Emergence of communication among reinforcement learning agents under coordination environment},
  author={Huang, Qiong and Uchibe, Eiji and Doya, Kenji},
  booktitle={Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2016 Joint IEEE International Conference on},
  pages={57--58},
  year={2016},
  organization={IEEE}
}

@phdthesis{hausknecht2017cooperation,
  title={Cooperation and communication in multiagent deep reinforcement learning},
  author={Hausknecht, Matthew John},
  year={2017}
}

@article{ghosh2016message,
  title={Message Passing Multi-Agent GANs},
  author={Ghosh, Arnab and Kulharia, Viveka and Namboodiri, Vinay},
  journal={arXiv preprint arXiv:1612.01294},
  year={2016}
}

@article{michel2017communication,
  title={Communication in Collaborative Multi-Agent Reinforcement Learning},
  author={Michel, {\'E}lie},
  year={2017}
}

@inproceedings{tan1993multi,
  title={Multi-agent reinforcement learning: Independent vs. cooperative agents},
  author={Tan, Ming},
  booktitle={Proceedings of the tenth international conference on machine learning},
  pages={330--337},
  year={1993}
}

@article{ye2015multi,
  title={A multi-agent framework for packet routing in wireless sensor networks},
  author={Ye, Dayong and Zhang, Minjie and Yang, Yun},
  journal={sensors},
  volume={15},
  number={5},
  pages={10026--10047},
  year={2015},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{conitzer2007awesome,
  title={AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents},
  author={Conitzer, Vincent and Sandholm, Tuomas},
  journal={Machine Learning},
  volume={67},
  number={1-2},
  pages={23--43},
  year={2007},
  publisher={Springer}
}

@article{leibo2017multi,
  title={Multi-agent Reinforcement Learning in Sequential Social Dilemmas},
  author={Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  journal={arXiv preprint arXiv:1702.03037},
  year={2017}
}

@inproceedings{da2006dealing,
  title={Dealing with non-stationary environments using context detection},
  author={Da Silva, Bruno C and Basso, Eduardo W and Bazzan, Ana LC and Engel, Paulo M},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={217--224},
  year={2006},
  organization={ACM}
}

@article{mataric1998using,
  title={Using communication to reduce locality in distributed multiagent learning},
  author={Mataric, Maja J},
  journal={Journal of experimental \& theoretical artificial intelligence},
  volume={10},
  number={3},
  pages={357--369},
  year={1998},
  publisher={Taylor \& Francis}
}

@inproceedings{lauer2000algorithm,
  title={An algorithm for distributed reinforcement learning in cooperative multi-agent systems},
  author={Lauer, Martin and Riedmiller, Martin},
  booktitle={In Proceedings of the Seventeenth International Conference on Machine Learning},
  year={2000},
  organization={Citeseer}
}

@inproceedings{tesauro2003extending,
  title={Extending Q-Learning to General Adaptive Multi-Agent Systems.},
  author={Tesauro, Gerald},
  booktitle={NIPS},
  volume={4},
  year={2003}
}

@inproceedings{makar2001hierarchical,
  title={Hierarchical multi-agent reinforcement learning},
  author={Makar, Rajbala and Mahadevan, Sridhar and Ghavamzadeh, Mohammad},
  booktitle={Proceedings of the fifth international conference on Autonomous agents},
  pages={246--253},
  year={2001},
  organization={ACM}
}

@article{matignon2012independent,
  title={Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems},
  author={Matignon, Laetitia and Laurent, Guillaume J and Le Fort-Piat, Nadine},
  journal={The Knowledge Engineering Review},
  volume={27},
  number={01},
  pages={1--31},
  year={2012},
  publisher={Cambridge Univ Press}
}

@article{wang2016sample,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@book{rubinstein2016simulation,
  title={Simulation and the Monte Carlo method},
  author={Rubinstein, Reuven Y and Kroese, Dirk P},
  year={2016},
  publisher={John Wiley \& Sons}
}

@incollection{weyns2005packet,
  title={The packet-world: A test bed for investigating situated multi-agent systems},
  author={Weyns, Danny and Helleboogh, Alexander and Holvoet, Tom},
  booktitle={Software Agent-Based Applications, Platforms and Development Kits},
  pages={383--408},
  year={2005},
  publisher={Springer}
}

@techreport{yang2004multiagent,
  title={Multiagent reinforcement learning for multi-robot systems: A survey},
  author={Yang, Erfu and Gu, Dongbing},
  year={2004},
  institution={tech. rep}
}

@inproceedings{hausknecht2016half,
  title={Half field offense: an environment for multiagent learning and ad hoc teamwork},
  author={Hausknecht, Matthew and Mupparaju, Prannoy and Subramanian, Sandeep and Kalyanakrishnan, S and Stone, P},
  booktitle={AAMAS Adaptive Learning Agents (ALA) Workshop},
  year={2016}
}

@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews},
  volume={38},
  number={2},
  pages={156},
  year={2008},
  publisher={IEEE SYSTEMS MAN \& CYBERNETICS SOCIETY}
}

@article{jorge2016learning,
  title={Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence},
  author={Jorge, Emilio and K{\aa}geb{\"a}ck, Mikael and Gustavsson, Emil},
  journal={arXiv preprint arXiv:1611.03218},
  year={2016}
}

@article{kok2006collaborative,
  title={Collaborative multiagent reinforcement learning by payoff propagation},
  author={Kok, Jelle R and Vlassis, Nikos},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={Sep},
  pages={1789--1828},
  year={2006}
}

@inproceedings{he2016opponent,
  title={Opponent Modeling in Deep Reinforcement Learning},
  author={He, He and Boyd-Graber, Jordan and Kwok, Kevin and Daum{\'e} III, Hal},
  booktitle={Proceedings of The 33rd International Conference on Machine Learning},
  pages={1804--1813},
  year={2016}
}

@inproceedings{tumer2007distributed,
	title={Distributed agent-based air traffic flow management},
	author={Tumer, Kagan and Agogino, Adrian},
	booktitle={Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems},
	pages={255},
	year={2007},
	organization={ACM}
}
@article{cao2013overview,
	title={An overview of recent progress in the study of distributed multi-agent coordination},
	author={Cao, Yongcan and Yu, Wenwu and Ren, Wei and Chen, Guanrong},
	journal={IEEE Transactions on Industrial informatics},
	volume={9},
	number={1},
	pages={427--438},
	year={2013},
	publisher={IEEE}
}
@article{ying2005multi,
	title={Multi-agent framework for third party logistics in E-commerce},
	author={Ying, Wang and Dayong, Sang},
	journal={Expert Systems with Applications},
	volume={29},
	number={2},
	pages={431--436},
	year={2005},
	publisher={Elsevier}
}

@inproceedings{chang2003all,
	title={All learning is Local: Multi-agent Learning in Global Reward Games.},
	author={Chang, Yu-Han and Ho, Tracey and Kaelbling, Leslie Pack},
	booktitle={NIPS},
	pages={807--814},
	year={2003}
}

@incollection{wolpert2002optimal,
	title={Optimal payoff functions for members of collectives},
	author={Wolpert, David H and Tumer, Kagan},
	booktitle={Modeling complexity in economic and social systems},
	pages={355--369},
	year={2002},
	publisher={World Scientific}
}
%explains aristocrat utility/related utility fn's

@inproceedings{proper2012modeling,
	title={Modeling difference rewards for multiagent learning},
	author={Proper, Scott and Tumer, Kagan},
	booktitle={Proceedings of the 11th International Conference on Autonomous 
	Agents and Multiagent Systems-Volume 3},
	pages={1397--1398},
	year={2012},
	organization={International Foundation for Autonomous Agents and Multiagent 
	Systems}
}
% estimated difference-reward fns

@inproceedings{colby2015approximating,
	title={Approximating difference evaluations with local information},
	author={Colby, Mitchell K and Curran, William and Tumer, Kagan},
	booktitle={Proceedings of the 2015 International Conference on Autonomous 
	Agents and Multiagent Systems},
	pages={1659--1660},
	year={2015},
	organization={International Foundation for Autonomous Agents and Multiagent 
	Systems}
}
% decentralised estimated difference-reward fn