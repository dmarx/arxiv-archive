@article{lyu2024centralizedcriticsmultiagentreinforcement,
      title={On Centralized Critics in Multi-Agent Reinforcement Learning}, 
      author={Xueguang Lyu and Andrea Baisero and Yuchen Xiao and Brett Daley and Christopher Amato},
      year={2024},
      journal={arXiv 2408.14597},
      url={https://arxiv.org/abs/2408.14597}, 
}

@article{lyu2021contrasting,
      title={Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning}, 
      author={Xueguang Lyu and Yuchen Xiao and Brett Daley and Christopher Amato},
      year={2021},
      journal={arXiv 2102.04402},
}

@inproceedings{lyu2022deeper,
  title={A deeper understanding of state-based critics in multi-agent reinforcement learning},
  author={Lyu, Xueguang and Baisero, Andrea and Xiao, Yuchen and Amato, Christopher},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={36},
  number={9},
  pages={9396--9404},
  year={2022}
}

@article{lyu2023centralized,
  title={On Centralized Critics in Multi-Agent Reinforcement Learning},
  author={Lyu, Xueguang and Baisero, Andrea and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
  journal={Journal of Artificial Intelligence Research},
  volume={77},
  pages={295--354},
  year={2023}
}

@article{ciosek2017offer,
  title={OFFER: Off-Environment Reinforcement Learning},
  author={Ciosek, Kamil and Whiteson, Shimon},
  year={2017}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@inproceedings{konda1999actor,
	title={Actor-Critic Algorithms.},
	author={Konda, Vijay R and Tsitsiklis, John N},
	booktitle={NIPS},
	volume={13},
	pages={1008--1014},
	year={1999}
}
@inproceedings{weaver2001optimal,
	title={The optimal reward baseline for gradient-based reinforcement learning},
	author={Weaver, Lex and Tao, Nigel},
	booktitle={Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence},
	pages={538--545},
	year={2001},
	organization={Morgan Kaufmann Publishers Inc.}
}
@article{kimura2000analysis,
	title={An analysis of actor-critic algorithms using eligibility traces: reinforcement learning with imperfect value functions},
	author={Kimura, Hajime and Kobayashi, Shigenobu and others},
	journal={Journal of Japanese Society for Artificial Intelligence},
	volume={15},
	number={2},
	pages={267--275},
	year={2000}
}
@article{wang2016sample,
	title={Sample Efficient Actor-Critic with Experience Replay},
	author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
	journal={arXiv preprint arXiv:1611.01224},
	year={2016}
}
@article{hafner2011reinforcement,
	title={Reinforcement learning in feedback control},
	author={Hafner, Roland and Riedmiller, Martin},
	journal={Machine learning},
	volume={84},
	number={1},
	pages={137--169},
	year={2011},
	publisher={Springer}
}


@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{DBLP:journals/corr/SchaulQAS15,
  author    = {Tom Schaul and
               John Quan and
               Ioannis Antonoglou and
               David Silver},
  title     = {Prioritized Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1511.05952},
  year      = {2015},
  timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SchaulQAS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{sukhbaatar2016learning,
  title={Learning multiagent communication with backpropagation},
  author={Sukhbaatar, Sainbayar and Fergus, Rob and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2244--2252},
  year={2016}
}

@article{usunier2016episodic,
  title={Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks},
  author={Usunier, Nicolas and Synnaeve, Gabriel and Lin, Zeming and Chintala, Soumith},
  journal={arXiv preprint arXiv:1609.02993},
  year={2016}
}

@article{moravvcik2017deepstack,
  title={DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker},
  author={Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\`y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  journal={arXiv preprint arXiv:1701.01724},
  year={2017}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{synnaeve2016torchcraft,
  title={TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games},
  author={Synnaeve, Gabriel and Nardelli, Nantas and Auvolat, Alex and Chintala, Soumith and Lacroix, Timoth{\'e}e and Lin, Zeming and Richoux, Florian and Usunier, Nicolas},
  journal={arXiv preprint arXiv:1611.00625},
  year={2016}
}

@inproceedings{foerster2016learning,
  title={Learning to communicate with deep multi-agent reinforcement learning},
  author={Foerster, Jakob and Assael, Yannis M and de Freitas, Nando and Whiteson, Shimon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2137--2145},
  year={2016}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Research}
}

@article{tampuu2015multiagent,
  title={Multiagent cooperation and competition with deep reinforcement learning},
  author={Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
  journal={arXiv preprint arXiv:1511.08779},
  year={2015}
}


@article{Zawadzki:2014,
  author    = {E. Zawadzki and A. Lipson and K. {Leyton-Brown}},
  title     = {Empirically Evaluating Multiagent Learning Algorithms},
  journal   = {arXiv preprint 1401.8074},
  year      = {2014}
}

@BOOK{MASfoundations09,
  author = "Y. Shoham and K. {Leyton-Brown}",
  title = "Multiagent Systems: {Algorithmic}, Game-Theoretic, and Logical Foundations",
  PUBLISHER =    {Cambridge University Press},
  YEAR =         {2009},
  address =      {New York}
}

@article{wang2015dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
  journal={arXiv preprint arXiv:1511.06581},
  year={2015}
}

@ARTICLE{2017arXiv170203274W,
   author = {{Williams}, J.~D. and {Asadi}, K. and {Zweig}, G.},
    title = "{Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1702.03274},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
     year = 2017,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170203274W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={39},
  pages={1--40},
  year={2016}
}

@article{gu2016deep,
  title={Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.00633},
  year={2016}
}

@phdthesis{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  school={University of Cambridge England}
}

@article{hausknecht2015deep,
  title={Deep recurrent q-learning for partially observable mdps},
  author={Hausknecht, Matthew and Stone, Peter},
  journal={arXiv preprint arXiv:1507.06527},
  year={2015}
}

@inproceedings{torch,
  title = {Torch7: A Matlab-like Environment for Machine Learning},
  author = {R. Collobert and K. Kavukcuoglu and C. Farabet},
  booktitle = {BigLearn, NIPS Workshop},
  year = {2011}
}

@article{DBLP:journals/corr/SchulmanMLJA15,
	author    = {John Schulman and
	Philipp Moritz and
	Sergey Levine and
	Michael I. Jordan and
	Pieter Abbeel},
	title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
	journal   = {CoRR},
	volume    = {abs/1506.02438},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.02438},
	timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SchulmanMLJA15},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{sutton1999policy,
	title={Policy gradient methods for reinforcement learning with function approximation.},
	author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay and others},
	booktitle={NIPS},
	volume={99},
	pages={1057--1063},
	year={1999}
}

@article{williams1992simple,
	title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	author={Williams, Ronald J},
	journal={Machine learning},
	volume={8},
	number={3-4},
	pages={229--256},
	year={1992},
	publisher={Springer}
}

@article{sutton1988learning,
	title={Learning to predict by the methods of temporal differences},
	author={Sutton, Richard S},
	journal={Machine learning},
	volume={3},
	number={1},
	pages={9--44},
	year={1988},
	publisher={Springer}
}

@inproceedings{konda2000actor,
	title={Actor-critic algorithms},
	author={Konda, Vijay R and Tsitsiklis, John N},
	booktitle={Advances in neural information processing systems},
	pages={1008--1014},
	year={2000}
}

@inproceedings{sutton2000policy,
	title={Policy gradient methods for reinforcement learning with function 
	approximation},
	author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and 
	Mansour, Yishay},
	booktitle={Advances in neural information processing systems},
	pages={1057--1063},
	year={2000}
}
