\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Busoniu, Babuska, and De~Schutter}{2008}]{busoniu2008comprehensive}
Busoniu, L.; Babuska, R.; and De~Schutter, B.
\newblock 2008.
\newblock A comprehensive survey of multiagent reinforcement learning.
\newblock {\em IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews} 38(2):156.

\bibitem[\protect\citeauthoryear{Cao \bgroup et al\mbox.\egroup }{2013}]{cao2013overview}
Cao, Y.; Yu, W.; Ren, W.; and Chen, G.
\newblock 2013.
\newblock An overview of recent progress in the study of distributed multi-agent coordination.
\newblock {\em IEEE Transactions on Industrial informatics} 9(1):427--438.

\bibitem[\protect\citeauthoryear{Chang, Ho, and Kaelbling}{2003}]{chang2003all}
Chang, Y.-H.; Ho, T.; and Kaelbling, L.~P.
\newblock 2003.
\newblock All learning is local: Multi-agent learning in global reward games.
\newblock In {\em NIPS},  807--814.

\bibitem[\protect\citeauthoryear{Cho \bgroup et al\mbox.\egroup }{2014}]{cho2014properties}
Cho, K.; van Merri{\"e}nboer, B.; Bahdanau, D.; and Bengio, Y.
\newblock 2014.
\newblock On the properties of neural machine translation: Encoder-decoder approaches.
\newblock {\em arXiv preprint arXiv:1409.1259}.

\bibitem[\protect\citeauthoryear{Colby, Curran, and Tumer}{2015}]{colby2015approximating}
Colby, M.~K.; Curran, W.; and Tumer, K.
\newblock 2015.
\newblock Approximating difference evaluations with local information.
\newblock In {\em Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},  1659--1660.
\newblock International Foundation for Autonomous Agents and Multiagent Systems.

\bibitem[\protect\citeauthoryear{Collobert, Kavukcuoglu, and Farabet}{2011}]{torch}
Collobert, R.; Kavukcuoglu, K.; and Farabet, C.
\newblock 2011.
\newblock Torch7: A matlab-like environment for machine learning.
\newblock In {\em BigLearn, NIPS Workshop}.

\bibitem[\protect\citeauthoryear{Das \bgroup et al\mbox.\egroup }{2017}]{das2017learning}
Das, A.; Kottur, S.; Moura, J.~M.; Lee, S.; and Batra, D.
\newblock 2017.
\newblock Learning cooperative visual dialog agents with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.06585}.

\bibitem[\protect\citeauthoryear{Foerster \bgroup et al\mbox.\egroup }{2016}]{foerster2016learning}
Foerster, J.; Assael, Y.~M.; de~Freitas, N.; and Whiteson, S.
\newblock 2016.
\newblock Learning to communicate with deep multi-agent reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems},  2137--2145.

\bibitem[\protect\citeauthoryear{Foerster \bgroup et al\mbox.\egroup }{2017}]{foerster2017stabilising}
Foerster, J.; Nardelli, N.; Farquhar, G.; Torr, P.; Kohli, P.; Whiteson, S.; et~al.
\newblock 2017.
\newblock Stabilising experience replay for deep multi-agent reinforcement learning.
\newblock In {\em Proceedings of The 34th International Conference on Machine Learning}.

\bibitem[\protect\citeauthoryear{Gupta, Egorov, and Kochenderfer}{2017}]{guptacooperative}
Gupta, J.~K.; Egorov, M.; and Kochenderfer, M.
\newblock 2017.
\newblock Cooperative multi-agent control using deep reinforcement learning.

\bibitem[\protect\citeauthoryear{Hausknecht and Stone}{2015}]{hausknecht2015deep}
Hausknecht, M., and Stone, P.
\newblock 2015.
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock {\em arXiv preprint arXiv:1507.06527}.

\bibitem[\protect\citeauthoryear{Hochreiter and Schmidhuber}{1997}]{hochreiter1997long}
Hochreiter, S., and Schmidhuber, J.
\newblock 1997.
\newblock Long short-term memory.
\newblock {\em Neural computation} 9(8):1735--1780.

\bibitem[\protect\citeauthoryear{Jorge, K{\aa}geb{\"a}ck, and Gustavsson}{2016}]{jorge2016learning}
Jorge, E.; K{\aa}geb{\"a}ck, M.; and Gustavsson, E.
\newblock 2016.
\newblock Learning to play guess who? and inventing a grounded language as a consequence.
\newblock {\em arXiv preprint arXiv:1611.03218}.

\bibitem[\protect\citeauthoryear{Konda and Tsitsiklis}{2000}]{konda2000actor}
Konda, V.~R., and Tsitsiklis, J.~N.
\newblock 2000.
\newblock Actor-critic algorithms.
\newblock In {\em Advances in neural information processing systems},  1008--1014.

\bibitem[\protect\citeauthoryear{Kraemer and Banerjee}{2016}]{kraemer2016multi}
Kraemer, L., and Banerjee, B.
\newblock 2016.
\newblock Multi-agent reinforcement learning as a rehearsal for decentralized planning.
\newblock {\em Neurocomputing} 190:82--94.

\bibitem[\protect\citeauthoryear{Lazaridou, Peysakhovich, and Baroni}{2016}]{lazaridou2016multi}
Lazaridou, A.; Peysakhovich, A.; and Baroni, M.
\newblock 2016.
\newblock Multi-agent cooperation and the emergence of (natural) language.
\newblock {\em arXiv preprint arXiv:1612.07182}.

\bibitem[\protect\citeauthoryear{Leibo \bgroup et al\mbox.\egroup }{2017}]{leibo2017multi}
Leibo, J.~Z.; Zambaldi, V.; Lanctot, M.; Marecki, J.; and Graepel, T.
\newblock 2017.
\newblock Multi-agent reinforcement learning in sequential social dilemmas.
\newblock {\em arXiv preprint arXiv:1702.03037}.

\bibitem[\protect\citeauthoryear{Lowe \bgroup et al\mbox.\egroup }{2017}]{lowe2017multi}
Lowe, R.; Wu, Y.; Tamar, A.; Harb, J.; Abbeel, P.; and Mordatch, I.
\newblock 2017.
\newblock Multi-agent actor-critic for mixed cooperative-competitive environments.
\newblock {\em arXiv preprint arXiv:1706.02275}.

\bibitem[\protect\citeauthoryear{Lyu \bgroup et al\mbox.\egroup }{2021}]{lyu2021contrasting}
Lyu, X.; Xiao, Y.; Daley, B.; and Amato, C.
\newblock 2021.
\newblock Contrasting centralized and decentralized critics in multi-agent reinforcement learning.
\newblock {\em arXiv 2102.04402}.

\bibitem[\protect\citeauthoryear{Lyu \bgroup et al\mbox.\egroup }{2022}]{lyu2022deeper}
Lyu, X.; Baisero, A.; Xiao, Y.; and Amato, C.
\newblock 2022.
\newblock A deeper understanding of state-based critics in multi-agent reinforcement learning.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~36,  9396--9404.

\bibitem[\protect\citeauthoryear{Lyu \bgroup et al\mbox.\egroup }{2023}]{lyu2023centralized}
Lyu, X.; Baisero, A.; Xiao, Y.; Daley, B.; and Amato, C.
\newblock 2023.
\newblock On centralized critics in multi-agent reinforcement learning.
\newblock {\em Journal of Artificial Intelligence Research} 77:295--354.

\bibitem[\protect\citeauthoryear{Lyu \bgroup et al\mbox.\egroup }{2024}]{lyu2024centralizedcriticsmultiagentreinforcement}
Lyu, X.; Baisero, A.; Xiao, Y.; Daley, B.; and Amato, C.
\newblock 2024.
\newblock On centralized critics in multi-agent reinforcement learning.
\newblock {\em arXiv 2408.14597}.

\bibitem[\protect\citeauthoryear{Mnih \bgroup et al\mbox.\egroup }{2015}]{mnih2015human}
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.~A.; Veness, J.; Bellemare, M.~G.; Graves, A.; Riedmiller, M.; Fidjeland, A.~K.; Ostrovski, G.; et~al.
\newblock 2015.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature} 518(7540):529--533.

\bibitem[\protect\citeauthoryear{Mordatch and Abbeel}{2017}]{mordatch2017emergence}
Mordatch, I., and Abbeel, P.
\newblock 2017.
\newblock Emergence of grounded compositional language in multi-agent populations.
\newblock {\em arXiv preprint arXiv:1703.04908}.

\bibitem[\protect\citeauthoryear{Oliehoek, Spaan, and Vlassis}{2008}]{Oliehoek08JAIR}
Oliehoek, F.~A.; Spaan, M. T.~J.; and Vlassis, N.
\newblock 2008.
\newblock Optimal and approximate {Q}-value functions for decentralized {POMDPs}.
\newblock 32:289--353.

\bibitem[\protect\citeauthoryear{Omidshafiei \bgroup et al\mbox.\egroup }{2017}]{omidshafiei2017deep}
Omidshafiei, S.; Pazis, J.; Amato, C.; How, J.~P.; and Vian, J.
\newblock 2017.
\newblock Deep decentralized multi-task multi-agent rl under partial observability.
\newblock {\em arXiv preprint arXiv:1703.06182}.

\bibitem[\protect\citeauthoryear{Peng \bgroup et al\mbox.\egroup }{2017}]{peng2017multiagent}
Peng, P.; Yuan, Q.; Wen, Y.; Yang, Y.; Tang, Z.; Long, H.; and Wang, J.
\newblock 2017.
\newblock Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games.
\newblock {\em arXiv preprint arXiv:1703.10069}.

\bibitem[\protect\citeauthoryear{Proper and Tumer}{2012}]{proper2012modeling}
Proper, S., and Tumer, K.
\newblock 2012.
\newblock Modeling difference rewards for multiagent learning.
\newblock In {\em Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 3},  1397--1398.
\newblock International Foundation for Autonomous Agents and Multiagent Systems.

\bibitem[\protect\citeauthoryear{Schulman \bgroup et al\mbox.\egroup }{2015}]{DBLP:journals/corr/SchulmanMLJA15}
Schulman, J.; Moritz, P.; Levine, S.; Jordan, M.~I.; and Abbeel, P.
\newblock 2015.
\newblock High-dimensional continuous control using generalized advantage estimation.
\newblock {\em CoRR} abs/1506.02438.

\bibitem[\protect\citeauthoryear{Shoham and {Leyton-Brown}}{2009}]{MASfoundations09}
Shoham, Y., and {Leyton-Brown}, K.
\newblock 2009.
\newblock {\em Multiagent Systems: {Algorithmic}, Game-Theoretic, and Logical Foundations}.
\newblock New York: Cambridge University Press.

\bibitem[\protect\citeauthoryear{Sukhbaatar, Fergus, and others}{2016}]{sukhbaatar2016learning}
Sukhbaatar, S.; Fergus, R.; et~al.
\newblock 2016.
\newblock Learning multiagent communication with backpropagation.
\newblock In {\em Advances in Neural Information Processing Systems},  2244--2252.

\bibitem[\protect\citeauthoryear{Sutton \bgroup et al\mbox.\egroup }{1999}]{sutton1999policy}
Sutton, R.~S.; McAllester, D.~A.; Singh, S.~P.; Mansour, Y.; et~al.
\newblock 1999.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock In {\em NIPS}, volume~99,  1057--1063.

\bibitem[\protect\citeauthoryear{Sutton}{1988}]{sutton1988learning}
Sutton, R.~S.
\newblock 1988.
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning} 3(1):9--44.

\bibitem[\protect\citeauthoryear{Synnaeve \bgroup et al\mbox.\egroup }{2016}]{synnaeve2016torchcraft}
Synnaeve, G.; Nardelli, N.; Auvolat, A.; Chintala, S.; Lacroix, T.; Lin, Z.; Richoux, F.; and Usunier, N.
\newblock 2016.
\newblock Torchcraft: a library for machine learning research on real-time strategy games.
\newblock {\em arXiv preprint arXiv:1611.00625}.

\bibitem[\protect\citeauthoryear{Tampuu \bgroup et al\mbox.\egroup }{2015}]{tampuu2015multiagent}
Tampuu, A.; Matiisen, T.; Kodelja, D.; Kuzovkin, I.; Korjus, K.; Aru, J.; Aru, J.; and Vicente, R.
\newblock 2015.
\newblock Multiagent cooperation and competition with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1511.08779}.

\bibitem[\protect\citeauthoryear{Tan}{1993}]{tan1993multi}
Tan, M.
\newblock 1993.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative agents.
\newblock In {\em Proceedings of the tenth international conference on machine learning},  330--337.

\bibitem[\protect\citeauthoryear{Tumer and Agogino}{2007}]{tumer2007distributed}
Tumer, K., and Agogino, A.
\newblock 2007.
\newblock Distributed agent-based air traffic flow management.
\newblock In {\em Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems},  255.
\newblock ACM.

\bibitem[\protect\citeauthoryear{Usunier \bgroup et al\mbox.\egroup }{2016}]{usunier2016episodic}
Usunier, N.; Synnaeve, G.; Lin, Z.; and Chintala, S.
\newblock 2016.
\newblock Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks.
\newblock {\em arXiv preprint arXiv:1609.02993}.

\bibitem[\protect\citeauthoryear{Weaver and Tao}{2001}]{weaver2001optimal}
Weaver, L., and Tao, N.
\newblock 2001.
\newblock The optimal reward baseline for gradient-based reinforcement learning.
\newblock In {\em Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence},  538--545.
\newblock Morgan Kaufmann Publishers Inc.

\bibitem[\protect\citeauthoryear{Weyns, Helleboogh, and Holvoet}{2005}]{weyns2005packet}
Weyns, D.; Helleboogh, A.; and Holvoet, T.
\newblock 2005.
\newblock The packet-world: A test bed for investigating situated multi-agent systems.
\newblock In {\em Software Agent-Based Applications, Platforms and Development Kits}. Springer.
\newblock  383--408.

\bibitem[\protect\citeauthoryear{Williams}{1992}]{williams1992simple}
Williams, R.~J.
\newblock 1992.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock {\em Machine learning} 8(3-4):229--256.

\bibitem[\protect\citeauthoryear{Wolpert and Tumer}{2002}]{wolpert2002optimal}
Wolpert, D.~H., and Tumer, K.
\newblock 2002.
\newblock Optimal payoff functions for members of collectives.
\newblock In {\em Modeling complexity in economic and social systems}. World Scientific.
\newblock  355--369.

\bibitem[\protect\citeauthoryear{Yang and Gu}{2004}]{yang2004multiagent}
Yang, E., and Gu, D.
\newblock 2004.
\newblock Multiagent reinforcement learning for multi-robot systems: A survey.
\newblock Technical report, tech. rep.

\bibitem[\protect\citeauthoryear{Ye, Zhang, and Yang}{2015}]{ye2015multi}
Ye, D.; Zhang, M.; and Yang, Y.
\newblock 2015.
\newblock A multi-agent framework for packet routing in wireless sensor networks.
\newblock {\em sensors} 15(5):10026--10047.

\bibitem[\protect\citeauthoryear{Ying and Dayong}{2005}]{ying2005multi}
Ying, W., and Dayong, S.
\newblock 2005.
\newblock Multi-agent framework for third party logistics in e-commerce.
\newblock {\em Expert Systems with Applications} 29(2):431--436.

\end{thebibliography}
