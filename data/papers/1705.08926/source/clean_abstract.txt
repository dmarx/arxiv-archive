Many real-world problems, such as network packet routing  and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems.  There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems.  To this end, we propose a new multi-agent actor-critic method called <i>counterfactual multi-agent</i> (COMA) policy gradients.  COMA uses a centralised critic to estimate the <i>Q</i>-function and decentralised actors to optimise the agents' policies.  In addition, to address the challenges of multi-agent credit assignment, it uses a <i>counterfactual baseline</i> that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of <i>StarCraft unit micromanagement</i>, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with  state-of-the-art centralised controllers that get access to the full state.