{
  "arxivId": "1705.08926",
  "title": "Counterfactual Multi-Agent Policy Gradients",
  "authors": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, Shimon Whiteson",
  "abstract": "Cooperative multi-agent systems can be naturally used to model many real\nworld problems, such as network packet routing and the coordination of\nautonomous vehicles. There is a great need for new reinforcement learning\nmethods that can efficiently learn decentralised policies for such systems. To\nthis end, we propose a new multi-agent actor-critic method called\ncounterfactual multi-agent (COMA) policy gradients. COMA uses a centralised\ncritic to estimate the Q-function and decentralised actors to optimise the\nagents' policies. In addition, to address the challenges of multi-agent credit\nassignment, it uses a counterfactual baseline that marginalises out a single\nagent's action, while keeping the other agents' actions fixed. COMA also uses a\ncritic representation that allows the counterfactual baseline to be computed\nefficiently in a single forward pass. We evaluate COMA in the testbed of\nStarCraft unit micromanagement, using a decentralised variant with significant\npartial observability. COMA significantly improves average performance over\nother multi-agent actor-critic methods in this setting, and the best performing\nagents are competitive with state-of-the-art centralised controllers that get\naccess to the full state.",
  "url": "https://arxiv.org/abs/1705.08926",
  "issue_number": 127,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/127",
  "created_at": "2024-12-27T10:14:40.745766",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-22T07:25:41.427Z",
  "main_tex_file": null,
  "published_date": "2017-05-24T18:52:17Z",
  "arxiv_tags": [
    "cs.AI",
    "cs.MA"
  ]
}