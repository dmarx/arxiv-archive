\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry} % Adjust margins to match style
\usepackage{float}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{natbib}
\usepackage{amsthm, amssymb, amsmath}
\usepackage{authblk} % For author formatting
\usepackage{caption}
\usepackage[toc,page]{appendix} % For appendix formatting
\usepackage[dvipsnames]{xcolor} % Allow for colored text

\usepackage{tcolorbox}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} % For including figures
\usepackage{caption}  % For figure captions
\usepackage{subcaption} % For subfigures


\newcommand{\niklas}[1]{\textcolor{purple}{Niklas: #1}}

\newtheorem{theorem}{Theorem}   
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\title{Scaling Laws for Precision}


\author[ ]{\small 
    Tanishq Kumar\thanks{Equal contribution. Correspondence to \texttt{tkumar@college.harvard.edu}}\textsuperscript{1}\ \ Zachary Ankner\footnotesize{*}\textsuperscript{3, 4} \ \ Benjamin F. Spector\textsuperscript{2} \ \  Blake Bordelon\textsuperscript{1} \ \ Niklas Muennighoff\textsuperscript{2} \ \ Mansheej Paul\textsuperscript{4} \ \ Cengiz Pehlevan\textsuperscript{1} \ \ Christopher RÃ©\textsuperscript{2} \ \ Aditi Raghunathan\textsuperscript{5}
}

\affil[1]{Harvard University}
\affil[2]{Stanford University}
\affil[3]{MIT}
\affil[4]{Databricks}
\affil[5]{Carnegie Mellon University}

\date{}


\begin{document}

\maketitle
\vspace{-1.25cm}

\begin{abstract}
Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise ``precision-aware'' scaling laws for both training and inference. We propose that training in lower precision reduces the model's \textit{effective parameter count}, allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training \textit{larger} models in \textit{lower} precision may be compute optimal.  We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.
\end{abstract}

\vspace{-0.1cm}
\section{Introduction}


Scale has emerged as a central driver of progress in deep learning \citep{brown2020languagemodelsfewshotlearners}. Key work on scaling \citep{kaplan2020scaling, hoffmann2022training} studied tradeoffs between model/dataset size to balance performance and compute. However, the precision in which models are trained and served is an important third factor that contributes to both cost and performance. Deep learning is trending towards lower precision: current frontier models like the Llama-3 series are trained in BF16 \citep{dubey2024llama}, and there is widespread effort to move the pretraining paradigm to FP8 \citep{micikevicius2022fp8}. The next generation of hardware will support FP4, and advances in weight-only quantization have led to training in binary and ternary at scale \citep{ma2024era, wang2023bitnet}. How far will these paradigms go? Specifically, we ask: 
\begin{center}
\textit{What are the tradeoffs between precision, parameters, and data?\\How do they compare for pretraining and inference?}
\end{center}

Studying scaling in precision is challenging because work on scaling laws generally aims to drop fine-grained implementation details in pursuit of universal functional forms while work on quantization generally does the opposite, focuses on the details: how quantization is done, with what type, to what part of the model. In seeking a balance, we consider a variety of plausible functional forms, and choose one that abstracts implementation details of quantization away from loss scaling, allowing us to predict loss scaling in many situations of practical interest. This functional form that posits bit precision and parameter count interchangeably contribute to a model's ``effective parameter count," $N_\text{eff}$, and implementation details like which parts of a model are quantized to what precision, interact with loss scaling only through their effect on this quantity. 

\begin{figure}[t] % Use [b] to place figures at the bottom
    \centering
    \includegraphics[width=\linewidth]{test_fig1_new_evals.pdf} % test-fig1 
    \caption{ Schematic of key findings. (Left) Training a fixed model size to various data budgets in BF16 and quantizing weights at the end. We find that degradation due to post-train quantization increases with tokens seen during pretraining, so that eventually \textbf{additional pretraining data can be harmful}. (Right) Our scaling suggests\textbf{ training \textit{larger} models in \textit{lower} precision can be compute-optimal} according to the cost model in Section \ref{section:implications-pretraining}. Weights, activations, attention quantized, all models trained on the same data budget, details in Appendix \ref{appdx:main-fig}.
    }
    \label{fig:main_figures}
\end{figure}

Overall, we study the scaling of the effects of precision on loss as we vary data and parameters, both \textit{during} and \textit{after} training. We first study how the degradation induced by post-train quantization scales with parameters and data. We find that the degradation increases with data, so that for a fixed model, training on additional data after a certain point can be actively harmful if the model will be quantized after training. We then shift our focus to quantized training, examining both the quantization-aware-training (weights only) and low-precision training (weights, activations, attention all quantized) settings. Our scaling laws for pretraining suggest that the compute-optimal pretraining precision is in general independent of compute budget. Surprisingly, however, this independence ceases to be true if model size is constrained, in which case the compute-optimal precision grows slowly in compute. 

In all, we pretrain a suite of 465 language models in 3 to 16 bit precisions, as well as post-train quantize each to multiple precisions. For a language model with $N$ parameters, trained on $D$ tokens with training precision $P_\text{train}$, and post-train weight precision $P_\text{post}$, we ultimately find a unified scaling law that takes the following form:  
    \begin{equation}
        L(N, D, P_\text{train}, P_\text{post}) = \underbrace{\underbrace{AN_\text{eff}^{-\alpha}}_{\text{Training-time Effects}} + B D^{-\beta} + 
E}_\text{Usual Chinchilla form} + \underbrace{\delta_\text{PTQ}(N_\text{eff}, D, P_\text{train}, P_\text{post})}_{\text{Post-Training Effects}}
\end{equation}
where $A, B, E, \alpha, \beta$ are positive fitted constants, and $\delta_\text{PTQ}$ refers to the loss degradation induced by post-training quantization before inference. Altogether, our results for post-train quantization illustrate how \textbf{more pretraining FLOPs do not always lead to better models} at inference-time, and our results for low-precision pretraining suggest \textbf{that both the standard practice of training models in 16-bit, and the race to extremely low (sub 4-bit) pretraining precision, may be suboptimal. }

\section{Background, Related Work, and Setup}

\textbf{Notation.}
Throughout, $D$ denotes dataset size in tokens and $N$ denotes model size in parameters. $P_\text{w}, P_\text{a}, P_\text{kv}$ refer to the bit precision, in integer-type, of the weights, activations, and key-value cache (``attention")\footnote{We study KV, rather than QKV, because understanding scaling in the KV cache alone is important for many inference settings. For pretraining claims in Section \ref{section:implications-pretraining}, we quantize the entire attention computation, including queries, finding additionally quantizing the query vectors makes a negligible difference to scaling.} during \textit{training}, and $P_\text{post}$ refers to the precision we post-train quantize (PTQ) weights to at the end for model inference. When $P$ or $P_\text{train}$ is used without reference to a part of the model, all three model parts are tied to the same precision. The inference-time loss degradation induced by post-train quantization will be denoted $\delta_\text{PTQ}(N, D, P_\text{train}, P_\text{post})$, and it is defined as the change in loss from performing post-training quantization compared to the end of pretraining. We use ``high precision" to mean 16-bit or above. 

\subsection{Quantization Fundamentals: How, What, When}


\textbf{The Problem: Compute vs Memory-Bound Workloads.} Most deep learning workloads are bottlenecked by either \textit{compute}, in the form of matrix multiplications, or \textit{memory bandwidth}, in the form of data movement between different parts of the GPU. Different types of workloads have different bottlenecks: most time is spent doing large matrix multiplications during pretraining, so it is compute-bound; in contrast, small-batch inference is bandwidth-bound by model weights; long-sequence decoding is bandwidth-bound by KV cache, etc. \textbf{This motivates studying scaling in the training precision of the (weights, activations, KV cache) both in isolation and in combination. }

% \za{
% Motivation for quantization.
% LLM workloads are typically either \emph{compute-bound}, where the run time is dominated by performing matrix multiplications, or \emph{memory-bandwidth-bound}, where the run time is dominated by data movement on the GPU.
% Pretraining is an example compute-bound workload, while small-batch inference is an example of a memory-bandwidth-bound workload.
% Quantization, a method for reducing the precision of model data, is a technique that can be leveraged to lessen both types of bottlenecks.
% }

\textbf{Quantization: How.} Quantization of an operation typically refers to rounding of values in matrices involved in some computation on the forward or backward pass, depending on what is quantized, and when. Quantization is usually done to integer or floating-point type.

\textbf{Quantization: What.} \textit{Only weights. ``Quantization-aware training"} Quantizing only weights during training does not offer any compute savings because matrix multiplications are still done in high precision. However, this is commonly done to allow weights to adapt to low precision so they can be served at very low precision at inference-time, thereby alleviating memory bottlenecks \citep{ma2024era, wang2023bitnet}. We will refer to this as ``quantization-aware-training" and defer additional discussion to Appendix \ref{appdx:types}. 


\textit{Weights, activations, attention. ``Low-precision training"} Quantizing and activations and attention in addition to weights allows for compute gains because matrix multiplications can be done in low precision (if the hardware supports it) since everything is in the same precision. We will refer to this setting as ``low-precision training" to distinguish it from quantization-aware training.  

\textbf{Quantization: When.} Quantization can be done \textit{during} or \textit{after} training. In practice, when seeking to reduce inference-time memory costs, one first attempts post-train quantization. If that degrades the model too much, quantization-aware-training is used. Post-train quantization is typically only applied to model weights \citep{frantar2022gptq, dettmers2022gpt3, lin2023awq, xiao2023smoothquant}. To reduce pretraining costs, low-precision-training is needed. We will study scaling laws for post-training quantization in Section \ref{section:ptq}, for quantized training in Section \ref{section:training} (examining both quantization-aware training and low precision training) and unify the two in Section \ref{section:unifying}. The numerical values of all our fitted constants can be found in Appendix \ref{appdx: fits}.

\subsection{Scaling Laws and Parametric Fits}


\textbf{Scaling Laws.}  \citet{hoffmann2022training} model loss scaling using the functional form $L(N, D) = AN^{-\alpha} + BD^{-\beta} + E$ where $A, B, \alpha, \beta, E$ are positive fitted constants, finding that data and parameters should be scaled in roughly equal proportion as more compute becomes available. We will refer to the scaling of \citep{hoffmann2022training} as ``Chinchilla-optimal" or just ``Chinchilla" and note this is often used colloquially as $D/N \approx 20$ being pretraining compute-optimal. On the theoretical front, work on scaling laws \citep{bahri2024explaining, bordelon2024dynamical, lin2024scaling} finds that noise to various parts of model or data affects loss in a predictable way. While previous works have explored the scaling behavior of post-training quantization in terms of total model bits~\citep{dettmers2023case} and knowledge capacity~\citep{allen2024physics}, we focus instead on data scaling. We note that in general the exact fitted values of all coefficients and exponents can vary drastically based on small implementation differences: \citet{besiroglu2024chinchilla} find different constants when attempting to replicate \citep{hoffmann2022training}, \citet{sardana2023beyond} fit coefficients $A, B$ of different orders of magnitude. For this reason, we emphasize our contribution is not the numerical values we fit, but the trends and functional forms we identify. 

\textbf{Overtraining.} In practice, accounting for inference costs means training smaller models for substantially longer than Chinchilla-optimal \citep{sardana2023beyond, gadre2024language}. For instance, Llama-3-8B is trained to $D/N \approx 2000$ \citep{dubey2024llama} and the Gemma-2 series up to $D/N > 1000$ \citep{team2024gemma}. We refer to such models as ``overtrained" in this paper, with the token/parameter ratio $D/N$ being a key quantity throughout. Work on inference-time compute \citep{snell2024scaling, brown2024large} and on synthetic and multimodal data \citep{yang2024synthetic, fan2024scaling, bauer2024comprehensive} suggests future models may be even more overtrained. Therefore, modern work on scale must consider ratios much larger than Chinchilla-optimal, and in this work we perform experiments up to $D/N \approx 10^3$ and analyze the predictions found by our scaling law for up to $D/N \approx 10^5$. See Appendix \ref{appdx: addrel} for additional related work.

\subsection{Setup}

We train and evaluate a suite of OLMo-style models on the Dolma V1.7 dataset \citep{groeneveld2024olmo, soldaini2024dolma}, using a standard Transformer++ implementation; see Appendix \ref{appdx: hypers} for hyperparameters and ablations. Our experiments consist of a sweep of language model pretraining runs over $N \in [30, 60, 110, 220]$ million parameters (non-embedding) and $D \in [1.5, 3, 6, 13, 26]$ billion tokens. Our model sizes are relatively small because we train up to a very high $D/N \approx 10^3$ to study data scaling and set off over 20 runs at every $(N, D)$: we sweep 8 values of precision for each of the (weights, activations, attention). 


\section{Scaling Laws for Post-Train Quantization}
\label{section:ptq}

The easiest and most common quantization technique is post-train quantizing a model off-the-shelf \citep{chee2024quip, huang2024billm, dettmers2022gpt3, lin2023awq, xiao2023smoothquant}. In this section, we consider models trained in BF16 and use GPTQ \citep{frantar2022gptq} to post-train quantize them, replicating our findings with two other methods in Appendix \ref{appdx:rtn}. We quantify the resulting loss degradation $\delta_\text{PTQ}$, finding that post-train quantization scales poorly in data. 

\subsection{Overtrained Models Degrade more when Post-Train Quantized}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{tpr_gptq_new_evals.pdf}
    \caption{Loss degradation from PTQ increases with data. Top row is loss after PTQ, bottom row is loss degradation compared to end of training, before PTQ. The top row is thus the gray line in each plot plus the corresponding value in the bottom row. We can see that degradation grows with data, bottom row is fitted with Equation \ref{eqn:degrade}. For $D/N $ sufficiently large (left), loss can increase in data. Even at lower $D/N$, where post-quant loss continues to decrease with data, the value of data is reduced compare to the baseline. $R^2 = 0.97$ over all fitted points (bottom row).}
    \label{fig:tpr}
\end{figure}

We consider different model sizes (columns) trained on various data budgets (x-axis of each plot) and plot in Figure \ref{fig:tpr} both the loss after post-train quantization (top row) and the degradation incurred relative to end of training (bottom row). We find that the degradation $\delta_\text{PTQ}$ increases in training data size across all model sizes, but that for a fixed dataset size larger models incur a smaller degradation. We additionally observe that $\delta_{\text{PTQ}}$ increases exponentially as we decrease the precision we quantize to.
Based on these observations we model $\delta_\text{PTQ}$ as taking the form:
\begin{equation}
\label{eqn:degrade}
    \delta_\text{PTQ}(N, D, P_\text{post}) = C_T \left(\frac{D^{\gamma_D}}{N^{\gamma_N}}\right) e^{-P_\text{post}/\gamma_\text{post}}
\end{equation}
where $C_T, \gamma_D, \gamma_N, \gamma_\text{post}$ are positive fitted constants. As we find the fitted values of $\gamma_D$ and $\gamma_N$ to be similar (see Appendix \ref{appdx: fits} for numerical values), we can think of this as an approximate power law in the token/parameter ratio $D/N$. The intuition for this poor data scaling might be that as models train on more data, they compress more information into their weights, so that perturbations to weights in the form of quantization are more harmful to loss, all else equal. We discuss formal theoretical interpretations in Appendix \ref{appdx:sharpness}. 

This finding implies that for models that will be post-train quantized, \textit{there exists an amount of pretraining data beyond which additional data is actively harmful to performance at inference-time} (see top-left, Figure \ref{fig:tpr}). This can be defined as the point where additional data increases post-train degradation more than it decreases loss during pretraining. We solve analytically for this critical data size in Appendix \ref{appdx: derivations}, as well analyze a cost model for workloads where inference-cost is the primary concern. We thus summarize our first scaling finding as follows. 

\begin{tcolorbox}[colback=lightblue!10, colframe=lightblue!50!black, boxrule=0.5mm, arc=2mm]
\label{finding:1}
    \textbf{Finding 1.} Overtrained language models are more sensitive to post-training quantization. For models trained in BF16 or above, we can model this loss degradation as
    $$\delta_\text{PTQ}(N, D, P_\text{post}) = C_T \left(\frac{D^{\gamma_D}}{N^{\gamma_N}}\right) e^{-P_\text{post}/\gamma_\text{post}}$$ where $C_T, \gamma_D, \gamma_N, \gamma_\text{post}$ are positive fitted constants. This implies that when $D/N$ is sufficiently large, or $P_\text{post}$ sufficiently small, loss after quantization can increase as models are pretrained for longer, as in Figure \ref{fig:tpr}. We will revisit and modify Equation \ref{eqn:degrade} in Section \ref{section:unifying} to account for the effects of \textit{training} in low-precision on $\delta_\text{PTQ}$.
\end{tcolorbox}


\section{Scaling Laws for Quantized Training}
\label{section:training}

In this section we study pretraining with weights, activations, and KV cache in various precisions. Importantly, only training precision, not test-time precision, is varied in this section; we discuss the interaction between train and test-time precision in Section \ref{section:unifying}. We sweep the training precisions of the weights, activations, and KV cache $P_\text{w}, P_\text{a}, P_\text{kv} \in [3, 12]$ individually, as well as training BF16 baselines. We also pretrain models with arbitrary combinations of $P_\text{w}, P_\text{a}, P_\text{kv}$ to validate our scaling laws. To perform quantization during training, we quantize the forward pass in integer type unless otherwise noted, see Appendix \ref{appdx:types} for implementation details.

\subsection{Quantization-Aware-Training: Quantizing Weights During Training has a Consistent and Predictable Effect}

We first examine the trade-off between weight precision $P_\text{w}$ and parameters $N$ while holding $P_\text{a} = P_\text{kv}$ fixed at high precision. We fix $D=13$B tokens and perform a grid sweep over combinations of $N$ and $P_\text{w}$. We plot the resulting IsoLoss contours where we linearly interpolate the final loss values in Figure~\ref{fig:isoloss}. We observe that the bit precision of the weights can be traded off for the number of parameters, i.e., a model with smaller $N$ but larger $P_\text{w}$ can achieve the same loss as a model with larger $N$ but smaller $P_\text{w}$.
Additionally, we find that the gains from increasing the bit precision of the weights are large at lower precisions but saturate at higher precisions (typically around 6-7 bits per weight).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{isoloss_N_eff_new_evals.pdf} % isoloss_contours_comparison_with_Neff_over_N
    \caption{ (Left) $N_\text{eff}/N$ from our final scaling law. Our fit of $N_\text{eff}(N, P_\text{w})$ in this section is the first step towards this (blue). Empirical (center) and predicted (right) IsoLoss contours illustrating the precision-parameter tradeoff. Y-axis is weight precision during quantized training. All runs plotted trained on $D = 13$B tokens. Predictions from a fitted version of Equation \ref{eqn:our-chinchilla}, darker lines correspond to lower loss. }
    \label{fig:isoloss}
\end{figure}

In line with the empirical trends in Figure \ref{fig:isoloss}, we find the best fit for the tradeoff between weight precision and parameters is $N_\text{eff}(N, P_\text{w}) = N(1-e^{-P_\text{w}/\gamma_\text{w}})$, where $\gamma_\text{w}$ is a fitted constant measuring the sensitivity of model weights (alternative fits explored in Appendix \ref{appdx: fits}). We therefore modify Chinchilla scaling to account for $N_\text{eff}$ by making the substitution $N \mapsto N_\text{eff}(N, P_\text{w})$, giving the modified form:

\begin{equation}
\label{eqn:our-chinchilla}
    L(N, D) = A[N(1-e^{-P_\text{w}/\gamma_\text{w}})]^{-\alpha} + BD^{-\beta} + E
\end{equation}

where we recall that $A, B, E, \alpha, \beta$ are fitted positive constants in the usual Chinchilla scaling form, and $\gamma_\text{w}$ is a fitted constant we introduce.  We plot the predictions of our fit compared to observed values in Figure \ref{fig:pw-fit} for a range of $(N, D)$. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{loss_vs_w_new_evals.pdf} % loss_vs_w_fit_Dolma
    \caption{Predicting final validation losses $L(N, D, P_\text{w})$ for various $N, D, P_\text{w}$ to test our proposed functional form. Points are experimental values, lines are predictions of a single parametric fit of the form in Equation \ref{eqn:our-chinchilla}. We train only two model sizes at 26B due to compute constraints. }
    \label{fig:pw-fit}
\end{figure}

\subsection{Low-Precision-Training: The Effects of Quantizing Weights, Activations, and Attention are Compositional and Multiplicative}

Quantization-aware training does not change the cost of pretraining. This is because modern GPUs require inputs to a matrix multiplication to have the same precision, i.e. $P_\text{w} = P_\text{a}=P_\text{kv}$ \citep{micikevicius2022fp8}. To understand the interplay between precision and pretraining compute we must now analyze the scaling behavior of $P_\text{a}$ and $P_\text{kv}$ as well. Note that in our training experiments, we only quantize on the forward pass to ensure a fair comparison between quantization-aware-training (weights only) and the additional quantization to activations/KV cache, see Appendix \ref{appdx:types}. 

\textbf{Precision of activations and KV cache affect loss in a similar way.} We first verify in Appendix Figure~\ref{fig:more-marginals} that varying $P_\text{a}$ and $P_\text{kv}$ in isolation give rise to scaling behavior that is best fit by a functional form analogous to the form for $P_\text{w}$ (Equation~\ref{eqn:our-chinchilla}, Figure \ref{fig:validate-qat-law}, left).

We refer to the scaling coefficients computed by varying the precision of just one part of the model at a time as \textit{marginally fitted constants}, and those found by fitting on runs that include multiple model components in low precision at the same time as \textit{jointly fitted constants}.  

\textbf{Constants fitted marginally and jointly make similarly good predictions}.
We now turn our attention to understanding the interactions between weights, activations, and attention. If the effects of quantizing weights, activations, and attention are independent, then a factorized, multiplicative interaction of the following form is a natural proposal. 
\begin{equation}
\label{eqn: full}
    N_\text{eff}(P) = N(1-e^{-P_\text{w}/\gamma_\text{w}})(1-e^{-P_\text{a}/\gamma_\text{a}})(1-e^{-P_\text{kv}/\gamma_\text{kv}})
\end{equation}
We test whether this independence approximately holds by comparing the predictive power of a model with marginally fitted constants and a model with jointly fitted constants.
We show the predictive power of both models in Figure \ref{fig:validate-qat-law}(b, c), finding that both methods for fitting constants have approximately the same predictive power.
These results suggest that the independence assumption is reasonable.
We both present further evidence that this ``factorized" functional form is a strong fit to the data as well as discuss alternative factorization schemes in Appendix \ref{appdx: empirical-neff}. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{predictions_comparison_new_evals.pdf}
    \caption{(Left) Predicted loss based on fitted values with Equation \ref{eqn: full}. (center) Fitting $\gamma$ parameters jointly on sweeps with combinations of precisions vs (right) fitting them on ``marginal" sweeps where only one model part is in low precision at a time. Outliers are those at extremely low precision whose training runs are sometimes unstable.}
    \label{fig:validate-qat-law}
\end{figure}


\begin{tcolorbox}[colback=lightblue!10, colframe=lightblue!50!black, boxrule=0.5mm, arc=2mm]
\label{finding:3}
    \textbf{Finding 2.} The effects of quantizing the weights, activations, and KV cache during training are well modeled as independent and multiplicative so that $$L(N, D, P_\text{w}, P_\text{a}, P_\text{kv}) = 
AN_\text{eff}^{-\alpha} + B D^{-\beta} + E$$ where $$N_\text{eff}(P_\text{w}, P_\text{a}, P_\text{kv}) = N(1-e^{-P_\text{w}/\gamma_\text{w}})(1-e^{-P_\text{a}/\gamma_\text{a}})(1-e^{-P_\text{kv}/\gamma_\text{kv}})$$ for which we fit constants $\gamma_\text{w}, \gamma_\text{a}, \gamma_\text{kv}$ that reflect the different sensitivities of weights, activations, and KV cache. If the three precisions are set to the same value $P$, as in pretraining, this simplifies to $N_\text{eff}(P) \approx N(1-e^{-P/\bar{\gamma}})^3$ where $\bar{\gamma}$ is the average of the three parameters. We visualize this functional form with our fitted values in Figure \ref{fig:isoloss} (left). 
\end{tcolorbox}

\subsection{Implications For Pretraining}
\label{section:implications-pretraining}

% Modern GPUs usually require all inputs to matrix multiplication to be in the same precision, so we fix $P_\text{w}=P_\text{a}=P_\text{kv}=P$. Then, compute costs scale linear in $P$ \citep{abdelkhalik2022demystifying}\footnote{In practice, the gains are less than linear due to systems overhead.}.
When training in a precision $P$, meaning $P_\text{w}=P_\text{a}=P_\text{kv}=P$, compute cost scales linearly in~$P$ \citep{abdelkhalik2022demystifying}\footnote{In practice, the gains are less than linear due to systems overhead.}.
\citet{hoffmann2022training} performed all experiments in 16-bit precision and use a cost model of $C = 6ND$ FLOPs. We generalize this to $C = \frac{6}{16}NDP$ to account for the linear relation between compute and precision, which reduces to the Chinchilla cost function for $P=16$. We now examine three practically relevant variants of the following optimization problem. 
%
\begin{equation}
    \min_{N, D, P} L(N, D, P) = A[N(1-e^{-P/\gamma})^3]^{-\alpha} + BD^{-\beta} + E \text { subject to } C = \frac{6}{16} NDP 
\end{equation}
%
Since derivations are algebraically involved, we will work up to proportionality and verify proposed solutions numerically. See Appendix \ref{appdx: derivations} for mathematical details. We note that the implications of our functional form are true no matter the scale at which future experiments are done, but the numerical values we predict depend on our fitted constants which are fitted on smaller-scale, integer-type experiments. 

\begin{figure}
    \centering
    % \includegraphics[width=\linewidth]{pretraining-precision-scaling.pdf}
    \includegraphics[width=\linewidth]{new_fig_6_new_evals.pdf} % new-fig-6
    \caption{Scaling law predictions (left, fitted on integer type) vs empirical values (right, floating-point type). Precision of weights, activations, attention fixed to $P_\text{train}$. Predictions closely match the empirical trend, but are shifted up by a small amount since floating-point is a more expressive type and will incur lower loss at the same precision. (Right) When $N$ is held fixed, compute-optimal precision increases approximately logarithmically with data. Markers correspond to predicted compute-optimal precision for Llama-3 (8b, 70b, 405b), denoted by (circle, triangle, star) at each IsoFLOP (lines), illustrating how compute-optimal precision increases in data when model size is held fixed.}
    \label{fig:pretraining-plots}
\end{figure}

\subsubsection{If You Must Train in Low Precision, Increase Parameters Before Data}

\textbf{Minimizing $L(N, D)$ with $P$ fixed, subject to $C \propto NDP$.} We get with some algebra that at precision $P$ and compute budget $C$, the optimal allocations $N^*, D^*$ of parameters and data relative to Chinchilla-optimal $N_{\text{Ch}}, D_{\text{Ch}}$ will be given by 
    % \begin{equation}
    % \label{eqn: N(P)}
    %     \frac{N^*(P, C)}{N_{\text{Ch}}(C)} \propto \left[1-e^{-P/\bar{\gamma}}\right]^{-\frac{3\alpha}{\alpha + \beta}} \text{  and  } \frac{D^*(P, C)}{D_{\text{Ch}}(C)} \propto \left[1-e^{-P/\bar{\gamma}}\right]^{\frac{3\alpha}{\alpha + \beta}}
    % \end{equation}
    \begin{equation}
    \label{eqn: N(P)-top}
        \frac{N^*(P, C)}{N_{\text{Ch}}(C)} \propto \left[1-e^{-P/\bar{\gamma}}\right]^{-\frac{3\alpha}{\alpha + \beta}} P^{-\frac{\beta}{\alpha + \beta}} \text{  and  } \frac{D^*(P, C)}{D_{\text{Ch}}(C)} \propto \left[1-e^{-P/\bar{\gamma}}\right]^{\frac{3\alpha}{\alpha + \beta}} P^{ \frac{\beta}{\alpha + \beta}}
    \end{equation}
%
    \textbf{which suggests as precision of training decreases at fixed compute, we should increase parameters and decrease data}. The interpretation of this is that at very low precisions, our effective parameter count vanishes so that increasing parameter count is compute-optimal since data egregiously outstrips effective parameters.
    
\subsubsection{Compute-Optimal Pretraining Precision is in General Independent of Compute} 

\textbf{Jointly minimizing $L(N, D, P)$ with $C \propto NDP$}. This is the setting of pretraining without constraints on $N, D, P$ except for a fixed compute budget. Solving this joint minimization problem gives an implicit equation for $P^*(C)$. Denoting $u(P) = [1-e^{-P/\bar{\gamma}}]^{-3\alpha}$, we find (see Appendix \ref{appdx: derivations}) that this equation takes the form
%
\begin{align}
    \frac{3\alpha}{\bar{\gamma}}\ u(P)^{\frac{3\alpha+1}{3\alpha}} e^{- P/\bar{\gamma}} = P^{-1 } u(P)
\end{align}
%
 which reveals that in general the optimal pretraining precision is independent of compute budget. This suggests that compute-optimal precision should be held fixed to $P^*$ while $N, D$ are scaled according to Equation $\ref{eqn: N(P)-top}$. We find this $P^*$ to be around 7-8 bits when fitting our scaling law on runs with quantization done to integer type. This has two consequences: first, this means \textbf{the de-facto practice of training models in 16-bit may be suboptimal.} Second,\textbf{ the race to low-precision training may have to stop before going below 4-bits}, since this would force model sizes to become disproportionately (more than 4x) larger to maintain loss scaling (see Figure \ref{fig:isoloss}, left). 
 
 We test our predictions in Figure \ref{fig:pretraining-plots} at a larger scale. We train compute-matched models at various parameter count and precision ranging from FP4 to FP32 and 220M to 1.6B parameters. We train in floating-point type since that is standard in pretraining \citep{groeneveld2024olmo, deitke2024molmo}, though our scaling laws are fitted on integer type. We plot our predicted trend in Figure \ref{fig:pretraining-plots} (left) and the empirical values in the middle. We find that scaling fits on integer type are a strong fit until 4-bit precision, at which points the difference between the two types becomes more apparent. The matching of qualitative trends throughout, with the optimum being close to the predicted optimum of $P^*$ near 7-8 bits suggests that similar scaling laws may exist across types. We initiate a similar analysis for floating-point type in Appendix \ref{appdx:fp}. 
 
\subsubsection{But Compute-Optimal Pretraining Precision Can Increase in Compute if Model Size \texorpdfstring{$N$}{N} is Constrained}


\textbf{Minimizing $L(D, P)$ with $N$ fixed, subject to $C \propto NDP$.} A common use case in practice is to train a suite of models of various sizes on similar data. The Llama-3 and Gemma-2 series \citep{dubey2024llama, team2024gemma} are examples. In this setting, $N$ is fixed in advance and only $D, P$ are jointly optimized. Surprisingly, our scaling laws predict that models of differing sizes should \textit{not} necessarily be trained in the same precision, and that compute-optimal precision scales as $P^*(C) \propto \log C$. Since $N$ is held constant and we show in Appendix \ref{appdx: derivations} that $\log C \approx \log D$ in proportion, we can write $P^*(C) \propto \log(D/N)$. The intuition for this is that, for a fixed $N$, precision acts as a new lever to bring highly overtrained models closer to pretraining optimality\footnote{An important subtlety here is that since models are overtrained for inference, we want to keep the cost of a forward pass---which is proportional to $NP$---fixed, not just $N$. While $NP$ is the same for both a model of $N_0$ parameters in 16-bit and one with $2N_0$ parameters in 8-bit, the latter has higher $N_\text{eff}$ with our $\bar{\gamma}$, so will reach a lower pretraining loss on the same data with the same training/inference costs.} by reducing $D/N_\text{eff}$. 
% There is a subtlety here: for settings where inference costs are high, it is crucial the cost of a forward pass remain fixed: this is proportional to $NP$. While quantity is the same for both a model of $N_0$ parameters in 16-bit and one with $2N_0$ parameters in 8-bit, the latter has higher $N_\text{eff}$ using our fitted $\bar{\gamma}$, and will therefore reach lower pretraining loss on the same data, and cost the same amount to train and serve using our cost model.}. 
    
\begin{tcolorbox}[colback=lightblue!10, colframe=lightblue!50!black, boxrule=0.5mm, arc=2mm]
    \textbf{Finding 3.} When $N, D, P$ are optimized jointly, compute-optimal pretraining precision is independent of compute. 16-bit has many unnecessary bits, and 4-bit requires increasing the model size disproportionately to maintain loss scaling. Our fits imply that 7-8 bits are compute-optimal. In contrast, when $N$ is fixed in advance, such as when training a model family on similar data, $P^*(C) \propto \log C$. This suggests that for models that will be significantly overtrained, higher precision during training may be compute-optimal.
\end{tcolorbox}

\section{A Unified Scaling Law for Precision}\label{section:unifying}

In this section, we combine the two scaling laws presented into a unified functional form that predicts both training/post-training effects, including interactions between the two. We now treat $\delta_\text{PTQ}$ as a function $\delta_\text{PTQ}(N, D, P_\text{train}, P_\text{post})$ rather than just $\delta_\text{PTQ}(N, D, P_\text{post})$ as we did earlier in Section \ref{section:ptq}. We find two competing effects at play when predicting $\delta_\text{PTQ}$, but \textbf{overall, models trained in lower precision are more robust to post-train quantization in the sense of incurring lower degradation. }

\textbf{Two competing effects at play during post-train quantization.} Intuitively, training any of $P_\text{w}, P_\text{a}, P_\text{kv}$ in low precision forces the model to learn weights that are robust to ``quantization noise," so they degrade less under PTQ. But the reduced $N \mapsto N_\text{eff}$ implies that models trained in low precision will degrade \textit{more} because $\delta_\text{PTQ}$ increases with $N^{-\gamma_N}$ (Section \ref{section:ptq}). We call this second effect the ``overtraining" effect. In practice, the first ``robustification" effect is larger, so that models trained in lower precision overall degrade \textit{less} when post-train quantized. We confirm using $N_\text{eff}$ rather than $N$ to predict degradation given various training precisions leads to a substantially stronger fit in Figure \ref{fig:ptq-tax-unifying}(top left, top center), to verify the competing overtraining effect. 

    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{combined_plots_new_evals.pdf}  % combined_plots
    \caption{Combined plots for predicting degradation. (Left) demonstrates the quality of our fit on all our runs, including all combinations of pre and post-training precisions. (Center, right) illustrate visually that our unified degradation form can predict degradation when training and serving in any precision. Plots (center, right) vary $P_\text{w}$ only, but fits in (left) include runs where $P_\text{a}, P_\text{kv}$ are also jointly varied. 
    }
    \label{fig:ptq-tax-unifying-main}
    \end{figure}

\textbf{Modifying $\delta_\text{PTQ}$ to account for training precision.} We assume training precision is strictly greater than inference precision, and define degradation as identically zero if they are equal. We begin by studying how degradation scales with just weight-precision during training, $P_\text{w}$. 

Consider Figure \ref{fig:ptq-tax-unifying-main}(center). We fix $(N,D)$ and each cell of the heatmap represents the empirical degradation $\delta_\text{PTQ}(P_\text{w}, P_\text{post})$. We observe that degradation very quickly increases to its exponentially large value from Section \ref{section:ptq} if there is any gap between training and inference-time precision. This motivates modifying our initial functional form fitted in Section \ref{section:ptq} to 
%
\begin{equation}\label{eqn:final-ptqw}
    \delta_\text{PTQ}(N, D, P_\text{w}, P_\text{post}) = 
    C_T e^{-P_\text{post}/\gamma_\text{post}} 
    \underbrace{\left(\frac{D^{\gamma_D}}{N_\text{eff}^{\gamma_N}}\right)}_{\text{Overtraining effect}}
    \underbrace{[1 - e^{-C_\text{w}(P_\text{w} - P_\text{post})}]}_{\text{Robustification effect}}
\end{equation}
%
where $C_\text{w}$ is the only new fitted value. Then, we can extend this to include the precision effects of activations/attention in the natural way:

\begin{equation}
    \label{eqn:final-ptq}
    \delta_\text{PTQ}(N, D, P_\text{w},P_\text{a},P_\text{kv}, P_\text{post}) = 
    C_T e^{-P_\text{post}/\gamma_\text{post}} 
    \left(\frac{D^{\gamma_D}}{N_\text{eff}^{\gamma_N}}\right)
    \prod_{\text{x} \in \{\text{w}, \text{a}, \text{kv}\}} [1 - e^{-C_\text{x}(P_\text{x} - P_\text{post})}]
\end{equation}

We measure the fit to the data of such a functional form in Figure \ref{fig:ptq-tax-unifying-main}, and find a strong fit with $R^2 = 0.90$ on over 1000 data points (each of 465 pretraining runs post-train quantized to multiple precisions). 

\textbf{An interpretable, unified functional form.} Now we simplify and interpret the resulting functional form. Consider training with only weights in low precision and take $C_\text{w} =1$ for illustrative purposes so we can simplify Equation \ref{eqn:final-ptq}. Denote $\sigma^2_\text{tr} := e^{-P_\text{w}/\gamma_\text{w}}$ as ``training noise" reflecting the decrease in effective parameter count due to training weights in lower precision. Then, Equation \ref{eqn:final-ptq} simplifies to
\begin{equation}
    \delta_\text{PTQ}(N, D, P_\text{train}, P_\text{post}) = C_T 
    \underbrace{(\sigma^2_{\text{PTQ}} - \sigma^2_\text{tr})}_{\text{Robustification effect}} \cdot 
    \underbrace{\left(\frac{D^{\gamma_D}}{N_\text{eff}^{\gamma_N}}\right)}_{\text{Overtraining effect}}
\end{equation}
%
% C_\text{w} = 0.45, C_\text{a} = 0.20, C_\text{kv} = 0.33      
which we note is the intuitive modification one might make to the form of the initial post-training quantization degradation we fitted in Section \ref{section:ptq}, in Finding \ref{finding:1}, with a small competing effects factor from $N_\text{eff}$ pushing in the opposite direction. \textit{It cleanly reflects the intuition that models are robustified to PTQ noise to the extent they were trained with similar noise}. 

\begin{tcolorbox}[colback=lightblue!10, colframe=lightblue!50!black, boxrule=0.5mm, arc=2mm]
    \textbf{Finding 4 (Unified Scaling Laws).} Modeling low-precision effects during pretraining as independent and multiplicative noise that accumulates, and including post-training quantization degradation, the predicted loss for a language model with $N$ parameters, trained on $D$ tokens, with training precision $P_\text{w}, P_\text{a}, P_\text{kv}$ to end-time weight-precision $P_\text{post}$, can be predicted as 

    \begin{equation}
        L(N, D, P_\text{w}, P_\text{a}, P_\text{kv}, P_\text{post}) = AN_\text{eff}^{-\alpha} + BD^{-\beta} + E + \delta_\text{PTQ}
    \end{equation}

    where $\delta_\text{PTQ}(N, D, P_\text{w}, P_\text{a}, P_\text{kv}, P_\text{post})$ is in general as in Equation \ref{eqn:final-ptq} and $N_\text{eff}(N, P_\text{w}, P_\text{a}, P_\text{kv})$ as in Finding \ref{finding:3}.
    
\end{tcolorbox}

\section{Conclusion and Limitations}

We find that the common inference-time technique of post-train quantization can incur large degradation at very high data budgets, demonstrating a striking example of how more pretraining compute does not always imply stronger models at inference-time. Seeking better data scaling, we study quantization-aware and low precision training. We find that parameters and bit precision are well modeled as interchangeably controlling an ``effective parameter count" of the model allows us to predict finite-precision loss effects accurately during both training and inference. 

There are limitations to our analysis. First, we use a fixed architecture throughout to examine the effects of precision, parameters, and tokens in a controlled manner. In contrast, low precision training often involves architectural tweaks \citep{ma2024era, zhu2024scalable} that can close much of the gap from a vanilla full precision model. Second, while compute costs do scale linearly with precision, the gains from halving precision are usually less than 2x due to systems overhead. Third, we only consider loss scaling without downstream model evaluations. We emphasize that the trends we find aim to be suggestive rather than prescriptive, and hope future work can more comprehensively examine these effects at larger model scale. In all, we find that the effects of precision on loss are predictable and consistent, with important and surprising implications.

\newpage 
\section{Acknowledgements}

Tanishq Kumar thanks Tim Dettmers, Chris De Sa, Neil Band and Luke Bailey for helpful comments and discussion, as well as Ludwig Schmidt for spotting an early typo. Blake Bordelon is supported by a Google PhD Fellowship. Cengiz Pehlevan is supported by NSF grant DMS-2134157, NSF CAREER Award IIS-2239780, and a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. Aditi Raghunathan acknowledges support from AI2050 program by Schmidt Sciences (Grant G2264481), Google Research Scholar, Apple, NSF, Cisco. 

We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-2-0184 (Long-context) and W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under Nos. N000142312633 (Deep Signal Processing); Stanford HAI under No. 247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI). Benjamin F. Spector is supported by a Hertz Fellowship. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.

\bibliographystyle{unsrtnat}
\bibliography{main}

\appendix
\newpage 
\part*{Appendix} % * avoids it being numbered

\section{Hyperparameter Details and Ablations}
\label{appdx: hypers}
 We launch over 20 runs for each $(N, D)$ combination to study scaling in precision, trained and validated on the common crawl split of the Dolma dataset \citep{soldaini2024dolma}. We use a standard causal Transformer++ implementation: SwiGLU activations \citep{shazeer2020glu}, RoPE embeddings \citep{su2021roformer}, RMSLayerNorm, Adam $\beta$ values of $(0.9, 0.95)$. We adopt a cosine learning rate schedule with 10\% warmup period and peak learning rate of 6e-4 for the smallest model and learning rates scaled with width and depth according to depth-$\mu$P for the larger models \citep{yang2022tensor, bordelon2023depthwise}. We use a sequence length of 1024 and batch size of 256 throughout, with Adam $\epsilon$ 1e-15, following \citep{wortsman2023small}. We use weight decay of $0.1$, as \citep{ahmadian2023intriguing} find some results in the quantization literature may be artifacts of insufficient weight decay. We follow \citep{ma2024era} in including a LayerNorm before projections because they find it is important for low precision training to be stable. These are the hyperparameters and settings used for the main scaling law experiments. 

To check robustness, we then ablate these hyperparameter choices, with results in Figure \ref{fig:hyper-ablations}. In our ablation we use a sequence length of 512 with batch size 128, weight decay of 1e-3, Adam $\epsilon$ of 1e-10, a peak learning rate of 1e-4 and a warmup period of duration 3\%. We train models with these alternative hyperparameters at various weight, activation, and KV cache precisions. We train and val on C4~\citep{raffel2020exploring,dodge2021documenting} instead. Though these ablations are at rather smaller scale due to compute constraints, the loss curves follow the same trends -- rapid decrease in final loss with an initial increase in precision from 4 bits, then diminishing returns as we approach higher precision -- as in the main text, suggesting the trends are robust to hyperparameter choices. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{hyper_ablations.pdf}
    \caption{$L(P_\text{w}), L(P_\text{a}), L(P_\text{kv})$ for ablated hyperparameters, $N=30$M, $D = 1.5$B. We can see the trends persist, where the first few bits reduce final val loss significantly, with diminishing/saturating returns quickly setting in at higher precision. We do not fit constants on these ablated runs. }
    \label{fig:hyper-ablations}
\end{figure}

\section{Additional Related Work}
\label{appdx: addrel}

\paragraph{Efficient training and inference} Low precision has been key to improving the efficiency of training and using LLMs~\citep{micikevicius2017mixed,shoeybi2019megatron,wortsman2023stable,zhu2023survey}. Prior works generally study either precision during training~\citep{courbariaux2014training,dettmers2024qlora,dettmers20218,sun2020ultra,liu2023llm} or the effects of changing the precision after training (post-training quantization)~\citep{frantar2022gptq,lin2024awq,dettmers2022gpt3,xiao2023smoothquant,sheng2023flexgen,dettmers2023spqr}. In this work we study both, the precision during training and after, and unify them from a scaling perspective. Other important works include recent popular work on quantization-aware-training \citep{ma2024era} where weights are quantized to extreme precisions (ternary) on the forward pass during training. This work is consistent with ours in that they can quantize weights so aggressively because weights are less sensitive than activations or KV cache. Further, while we use a fixed architecture throughout to maintain a controlled comparison across precision, they use a nonstandard architecture, learning rate, and weight decay schedule specifically to make training with ternary weights stable. Finally, \citep{Keller2018} find similar tradeoffs for floating-point type with weights-only quantization. 

\paragraph{Large language models and scaling} By scaling up the transformer architecture~\citep{vaswani2017attention} a variety of large language models have been proposed~\citep{brown2020languagemodelsfewshotlearners,rae2021scaling,touvron2023llama,touvron2023llama2,dubey2024llama,le2023bloom,muennighoff2022crosslingual,muennighoff2024olmoe,groeneveld2024olmo,jiang2023mistral,zhang2022opt,allal2023santacoder,li2023starcoder,lozhkov2024starcoder,luukkonen2023fingpt,bai2023qwen,chowdhery2023palm,team2023gemini,ustun2024aya,deitke2024molmo}. To improve our understanding of these models, various works have investigated their scaling properties~\citep{ruan2024observational,allen2024physics,hagele2024scaling}. Many aspects are relevant to scaling including the architecture~\citep{tay2022scaling,krajewski2024scaling,tao2024scaling,clark2022unified,tay2022transcending,scao2022language,peng2024eagle}, the modalities considered~\citep{aghajanyan2023scaling,alabdulmohsin2022revisiting,cherti2023reproducible}, the performance metrics~\citep{wei2022emergent,srivastava2022beyond,isik2024scaling}, the data composition~\citep{li2024datacomp,liu2024regmix,albalak2024survey} and data repetitions~\citep{muennighoff2024scaling}. Our work analyzes one such aspect, which is key to better scaling: the numeric precision during and after training.




\section{Alternative Functional Forms}
\label{appdx: forms}

There are several plausible functional forms to try a priori. The key junctions are whether a form is 1) additive or multiplicative and 2) interacts with parameters/data or is independent, 3) a power law or exponential. We try a variety of combinations of these three and find the formulation in the main text one of the best fits, notably with the fewest fitted parameters. We emphasize that several fitted forms are likely to be reasonable fits to the data, and an important desiderata for choosing a functional fit is interpretability. Several scaling law papers find multiple fits plausible in terms of predictive power \citep{muennighoff2024scaling, kaplan2020scaling}, and ultimately make a decision based on interpretability. 

We make these fit choices on sweeps of the form $L(N, D, P_\text{W})$ and discuss alternatives to the decomposition/factorization to account for activations and KV cache in Appendix Section \ref{appdx: empirical-neff}, which assumes an effective parameter count formulation. In this section, a power law refers to a term of the form $C_\text{w} \cdot P^{-\alpha_
\text{w}}$ where $C_\text{w}, \alpha_
\text{w}$ are fitted. In general, we find modeling precision effects with power law fits on their own causes the fitted constants $A, B$ to blow up, whereas this does not happen with exponential fits, suggesting the power law does not change sharply enough to match the change in loss induced by precision. We note that while fitting parameters using a double notion of effective parameters \textit{and} effective data leads to a slightly better fit, it requires more fitted parameters so we stick with the $N_\text{eff}$ formulation for simplicity and interpretability. When choosing between fits we validate on held-out data and the $R^2$ values below reflect the fit on the held out data. This is in contrast to our plots in the main text, where we have chosen a functional form and we  fit and plot on the same data, as is standard in scaling laws \citep{muennighoff2024scaling}. 

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Functional Form} & \textbf{Val  R\textsuperscript{2}} & \textbf{Number of Fitted Parameters} \\
\hline
$N_\text{eff}$ & 0.82 & 3 \\
\hline
Additive/independent power law & 0.71 & 2 \\
\hline
$D_\text{eff}$ & 0.74 & 3 \\
\hline
$N_\text{eff}$ and $D_\text{eff}$ (tied) & 0.79 & 3 \\
\hline
$N_\text{eff}$ and $D_\text{eff}$ (not tied) & 0.84 & 4 \\
\hline
Multiplicative power law, $N, P$ & 0.75 & 2 \\
\hline
\end{tabular}
\caption{Comparison of Functional Forms with $R^2$,and Number of Fitted Parameters}
\label{table:functions}
\end{table}


\section{Quantization Implementation Details and Types}
\label{appdx:types}

Two canonical types for neural network quantization are floating-point (FP) and integer (INT) quantization. Despite their differences in representation, we hypothesize the scaling behavior between floating-point and integer quantization can be described by similar functional forms, where \ref{fig:main_figures}(b) provides preliminary evidence for this. 

\subsection{Integer Quantization and Implementation Details}

In integer quantization, continuous values are mapped to discrete integer values. Typically, this is done by scaling the original values according to a fixed scale factor. Mathematically, for a real number \( x \), the quantized integer value \( x_{\text{int}} \) is computed as:
\[
x_{\text{int}} = \left\lfloor \frac{x}{s} \right\rceil
\]
where \( s \) is the scaling factor, and \( \left\lfloor \cdot \right\rceil \) denotes rounding to the nearest integer specified by the number of bits. The value can then be dequantized back to an approximate real value by multiplying by \( s \):
\[
x_{\text{dequant}} = s \cdot x_{\text{int}}
\]
This process introduces quantization error, defined as the difference between the original value \( x \) and the dequantized value \( x_{\text{dequant}} \). The goal of quantization is to minimize this error while still reducing the precision. One can think of this as rounding to the nearest point on a uniform lattice. More complicated quantization schemes involve selecting the lattice points in a data or model-dependent manner. Integer quantization, as implemented, uses a fixed-point scaling based on the maximum absolute value of the tensor, and then scales the values within the range \([Q_n, Q_p]\), where \( Q_n = -2^{(b-1)} \) and \( Q_p = 2^{(b-1)} - 1 \), with \( b \) being the number of bits.

Integer quantization first rescales the inputs into the range specified by the number of bits by 
\[
s = \frac{Q_p}{\max(|x|)}
\]
for tensor-based scaling, or
\[
s = \frac{Q_p}{\max(|x|, \text{dim}=k)}
\]
for channel-based scaling. After scaling, the result is rounded to the nearest integer and then clamped to the range \([Q_n, Q_p]\). After matrix multiplication, the result is rescaled back into the original range. We quantize only the forward pass in this work, to ensure fair comparison between quantization-aware-training (weights only) and low-precision training (weights, activations, KV cache). This is because the backward pass is not usually quantized during quantization-aware-training \citep{ma2024era}, so comparing sensitivities of weights (forward only) to activations/KV cache (forward and backward) would not be a principled comparison. In production pretraining in low precision, the matrix multiplications on the backward pass are also quantized, leading to further compute savings. We leave a detailed analysis of how our observations change when accounting for the backward pass to future work. We use integer quantization throughout to fit our scaling laws for simplicity. 

\subsection{Floating-Point Quantization}

Floating-point quantization is slightly more sophisticated, aiming to make a non-uniform lattice roughly matching the distribution of the weights, which are assumed to be Gaussian. A floating-point number is in general represented as:
\[
x_{\text{fp}} = (-1)^s \cdot m \cdot 2^e
\]
where \( s \) is the sign bit, \( m \) is the mantissa, and \( e \) is the exponent. In floating-point quantization, both the mantissa and exponent are quantized to reduce the bit width. For exponent-mantissa allocations of bits and details of exponent bias, we follow the guidelines from \citep{micikevicius2022fp8} and quantize weights per channel and activations per-tensor. 

Making a full scaling law for floating-point quantization is more involved than our integer treatment, because the effects of scaling mantissa vs exponent bits are not the same. In contrast, in integer quantization, each additional bit simply causes us to round into a finer-grained lattice after rescaling, thereby reducing quantization error by a predictable amount. In floating-point quantization, altering the exponent affects the dynamic range, while altering the mantissa changes the precision within that range. This flexibility at once makes floating-point quantization more suitable for model training, but harder to analyze. We leave a commensurately detailed analysis of mantissa vs exponent -- and more generally floating point -- scaling to future work.

\subsection{Hardware Details}

Weight-only quantization can accelerate inference because software can be written to accommodate moving data between GPU parts (HBM-SRAM) in smaller units (types), so that a given bandwidth can move more data per second. This reduces memory (IO) bottlenecks that often dominate during inference, even with high-batch workloads. However, we emphasize that the type and therefore speed at which the GPU can do matrix multiplications in natively is determined by the hardware provider, so that even when $P_\text{w}=P_\text{a}=P_\text{qkv}$ (including queries), compute savings are only achieved when these correspond with both a bit-width and type that the GPU supports. We aim to study scaling in a fairly hardware-agnostic manner so that our work may be useful in the future, and make no claims about hardware details or optimality. We train all our models with fake (simulated) quantization on NVidia H100 GPUs to remain hardware agnostic, not taking advantage of any true low-precision computation. The only assumption is that when hardware does implement support for integer quantization, it is done in a way that involves some combination of rescaling and rounding, as is standard at the time of writing \citep{dettmers2023case, dettmers2022gpt3, wu2020integer, jacob2018quantization}. 


\section{Derivations}
\label{appdx: derivations}

\subsection{Critical Dataset Size for PTQ}

We seek a $D_\text{crit}$ that satisfies $\frac{\partial L(D_\text{crit})}{\partial D} = \frac{\partial \delta_\text{PTQ}(D_\text{crit})}{\partial D}$. Taking both derivatives for the functional forms presented in the main text and equating their opposing effects, we get the equation 

\begin{equation}
    BD_\text{crit}^{-\beta - 1} = \gamma_D C_T N^{-\gamma_N}e^{-P_\text{post}/\gamma_\text{post}}D_\text{crit}^{\gamma_D-1}
\end{equation}

which implies 

\begin{equation}
    D_\text{crit}=\left(\frac{\beta B N^{\gamma_N} e^{P_\text{post} / \gamma_\text{post}}}{\gamma_D C_T}\right)^{\frac{1}{\gamma_D+\beta}}
\end{equation}

is the predicted point after which pretraining on more data can increase loss of a model that is post-train quantized. Note that this quantity explodes in $P$, so that a truly unreasonable amount of data is required for longer pretraining to be harmful at commonly used precisions (eg. 8-bit). However, we find that on overtrained models $D/N \gg 10^3$, these overtraining-degradation effects become nontrivial around 5-bits, and dominant below that. 

\subsection{Compute-optimality calculations}

We set a constraint $C \propto NDP$ throughout. Working up to proportionality is essentially rescaling the compute constraint, so it doesn't affect the scaling trends we identify, which is our focus. 

\subsubsection{Fixed Precision Compute Optimal Scaling}

Under fixed precision, the loss takes the form
\begin{align}
    L = u(P) A  N^{- \alpha} + B D^{-\beta} 
\end{align}
where $u(P) = [1-e^{-P/\gamma}]^{-3\alpha}$ is a fixed constant. The compute optimal scaling when minimizing the loss over $N,D$ gives
\begin{align}
    L = u(P) A N^{- \alpha} + B C^{-\beta} N^\beta P^\beta
\end{align}
by replacing $D = \frac{C}{NP}$. Optimizing over $N$, we see that this is equivalent to the original chinchilla optimization problem but with $A \to A u(P)$ and $B \to B P^\beta$. Performing this optimization, we find
\begin{align}
    N^*(P,C) = \left( \frac{u(P) A \alpha}{B P^\beta \beta}  \right)^{\frac{1}{\alpha+\beta}} C^{\frac{\beta}{\alpha + \beta}} \quad ,  \ D^*(P,C) = \left( \frac{u(P) A \alpha}{B P^\beta \beta}  \right)^{- \frac{1}{\alpha+\beta}} C^{\frac{\alpha}{\alpha + \beta}}
\end{align}
We can relate the above expressions to the original Chinchilla-optimal $N, D$ at full precision $N_{\text{Ch}}(C), D_{\text{Ch}}(C)$. 
   \begin{equation}
    \label{eqn: N(P)}
        \frac{N^*(P, C)}{N_{\text{Ch}}(C)} \propto \left[1-e^{-P/\bar{\gamma}}\right]^{-\frac{3\alpha}{\alpha + \beta}} P^{-\frac{\beta}{\alpha + \beta}} \text{  and  } \frac{D^*(P, C)}{D_{\text{Ch}}(C)} \propto \left[1-e^{-P/\bar{\gamma}}\right]^{\frac{3\alpha}{\alpha + \beta}} P^{ \frac{\beta}{\alpha + \beta}}
    \end{equation}



\subsubsection{Fixed model size \texorpdfstring{$N$}{N}}

Now, we investigate the case where model size $N$ is fixed but precision and data are jointly optimized at fixed compute $C= N D P$. This optimization problem takes the form
\begin{align}
    L = u(P) A N^{-\alpha} + B D^{-\beta} 
\end{align}
Under fixed compute, we have $D = \frac{C}{NP}$ so replacing the second term, we have
\begin{align}
    L = u(P) A N^{-\alpha} + B C^{-\beta} N^\beta P^\beta  
\end{align}
where $N$ is a constant. We therefore have a single variable $P$ to minimize the above formula over
\begin{align}
    \frac{\partial L}{\partial P} =  u'(P) A N^{-\alpha} + B C^{-\beta} N^\beta \ \beta \  P^{\beta - 1} = 0
\end{align}
First, we note that $u'(P)$ has the following form
\begin{align}
    u'(P) = - 3\alpha [1-e^{-P/\gamma}]^{-3\alpha - 1} \times \frac{1}{\gamma} e^{-P/\gamma} = - \frac{3\alpha }{\gamma} e^{-P/\gamma} \times u(P)^{ \frac{3\alpha+1}{3\alpha} }
\end{align}
We thus desire a solution to the implicit equation
\begin{align}
    \frac{ 3 \alpha }{\gamma} e^{-P/\gamma} \times u(P)^{ \frac{3\alpha+1}{3\alpha} }  A N^{-\alpha} =  B C^{-\beta} N^\beta \ \beta \  P^{\beta - 1} 
\end{align}
We now aim to find an approximate asymptotic relationship between $P$ and $C$ as $C \to \infty$. Taking a logarithm of both sides, we find (neglecting additive constants that are independent of $C, P$)
\begin{align}
    - (3\alpha+1)\ln( 1 -e^{-P/\gamma} ) - \frac{1}{\gamma} P \approx - \beta \ln C 
\end{align}
The correct dominant balance at large $C$ is to take $P^\star \sim \beta\gamma \ln C$, as can be verified numerically. With the constraint that $C = N P D$ we have that $D^\star \approx \frac{C}{N \beta \gamma 
\ln C}$. 


\subsubsection{Minimization over \texorpdfstring{$N$}{N}, \texorpdfstring{$D$}{D}, \texorpdfstring{$P$}{P} with Fixed Compute}


Recall our three-way loss function is given as below. We separate $N_\text{eff}$ into terms involving $(N, P)$ explicitly here as it makes the math easier to follow. 

\begin{align}
    L(N, D, P) = A N^{-\alpha} u(P) + B D^{-\beta} \ , \ u(P) = [1-e^{-P/\gamma}]^{-3\alpha} 
\end{align}
Under the constraint $C \propto NDP$, we can replace $D$ in terms of $C, N, P$ giving the loss expression
\begin{align}
    &L = A N^{-\alpha} u(P) + B N^{ \beta} P^\beta C^{-\beta}
    \\
    &\frac{\partial L}{\partial N} = - \alpha A N^{-\alpha - 1} u(P) + \beta  B N^{\beta - 1} P^\beta C^{-\beta} = 0 
    \\
    &\frac{\partial L}{\partial P} = - 3  \alpha / \gamma A  N^{-\alpha } u(P)^{\frac{3\alpha+1}{3\alpha}} e^{- P/\gamma} + \beta B N^\beta P^{\beta-1} C^{-\beta} = 0 
\end{align}
Multiplying the first equation by $N$ and dividing the second equation by it reveals that the optimal $P$ satisfies a compute-independent implicit equation
\begin{align}
    \frac{3 }{\bar{\gamma}}\ u(P)^{\frac{1}{3\alpha}} e^{- P/\bar{\gamma}} = P^{-1 } u(P)
\end{align}
This exercise reveals that the compute optimal strategy when allowed to jointly optimize $N,D,P$ is to choose a fixed precision that satisfies the above equation and then to scale up $N,D$ with the prescription in Appendix I.1.1. 

\subsection{Inference-time Cost Model}\label{appdx:inf-cost}

For many, inference is the primary cost of training and serving models. Here, we present a preliminary analysis of an inference-time cost model. The key tension is that inference cost scales as $NP$, so that inference costs at a fixed pretraining loss can be reduce by either reducing model size (and overtraining more) or quantizing post-training

We will assume here that $P=P_\text{post}$ refers to the precision weights will be quantized to. In practice, inference costs may depend on the precision of the KV cache and activations to some extent as well, but we assume this for tractability of the following mathematical model, and to get a sense of how overtraining and post-train quantization concerns play out at inference-time. We can phrase this minimization problem in the following way. 

%
\begin{equation}\label{inf-cost}
    \min_{N, D, P} L(N, D, P) = AN^{-\alpha} + BD^{-\beta} + C_T\frac{D^{\gamma_D}}{N^{\gamma_N}}e^{-P/\gamma} \text { subject to } C = NP 
\end{equation}

The system of first-order conditions that results from this constrained optimization problem is not in general tractable analytically, so we solve the above constrained optimization problem for $P^*(C), N^*(C), D^*(C)$ numerically via a simple grid search. We find that $N^*, D^*$ grow as a power law in $C$ while $P^* \propto \log C$. The clumping in points is an artifact of the numerics of the grid search; the fitted lines represent the loglinear (left) and loglog (middle, right) trends overall. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{inference_cost_model.pdf}
    \caption{{Numerically minimizing a model of inference-time costs with respect to $N, D, P$ after accounting for post-train-quantization degradation and its relation to overtraining.}}
    \label{fig:enter-label}
\end{figure}

It might be surprising that $D^*$ is not taken to infinity since it does not appear in the cost function. The reason for this is because if it was, post-train degradation (the third term) would become large. It might also be surprising that $D^*$ changes with compute at all. The reason for this is because, once again, of the third term: as we allow more inference-time compute we use more $N$, and at a larger $N$ we can now tolerate a larger data budget for a given post-train quantization degradation, so being compute-optimal means taking advantage of this and training that larger parameter count on more data. 

The intuition for why $P^* \sim \log C$ might be as follows. Consider a situation in which $P^*$ is independent of compute: the third term will come to be a bottleneck in loss as compute gets larger because $N, D$ are both being scaled as power laws in compute, and eventually the effect of $e^{-P/\gamma}$ will become non-negligible in comparison to the first two terms in the loss function. To continue decreasing loss at this point, we must make this term smaller at a rate commensurate with the other terms, which go as a power law in compute. Since precision is inside the exponential, this can be done by taking $P \sim \log C$. An important thing to note is that since we are ignoring pretraining costs here, the absolute values of predicted $D^*$ are much larger than would be realistically possible in any reasonably training regime, where pretraining costs do matter, even if less than inference costs. But the empirical trends in $N^*, P^*$ showcase how overtraining with post-train quantization in mind can outperform vanilla overtraining without accounting for its effects on post-train quantization. 



\section{Replicating PTQ Scaling with other Quantization Methods}
\label{appdx:rtn}

Here we replicate the finding that post-train degradation due to post-train quantization increases with token/parameter ratio as $D^{\gamma_D}/N^{\gamma_N}$. We fit the same functional form as in the main text, but get slightly different values of fitted constants, as expect. We replicate on AWQ \citep{lin2023awq} and round-to-nearest quantization. The former is a modern and sophisticated technique, and the latter a simple and naÃ¯ve approach to quantization. The fact they, as well as GPTQ in the main text, share the same failure modes suggests that poor post-training quantization data scaling should be the default expectation for any newly proposed PTQ technique. 

% For AWQ, our fitted constants are $C_T = 2.1068, \quad \delta_D = 0.3586, \quad \delta_N = 0.4922, \quad \gamma = 0.6511, \quad b = 3.0353$, and for RTN they are $C_T = 0.0391, \quad \delta_D = 0.5174, \quad \delta_N = 0.3299, \quad \gamma = 0.6086, \quad b = 0.9558$. $R^2$ values for the two methods are $0.99, 0.96$, respectively, over all fitted points. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{tpr_awq_new_evals.pdf}
    \caption{{Replicating Section \ref{section:ptq} results with AWQ. }}
    \label{fig:awq-replication}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{tpr_rtn_new_evals.pdf}
    \caption{{Replicating Section \ref{section:ptq} results with RTN. }}
    \label{fig:rtn-replication}
\end{figure}

\section{PTQ: Learning Rate Schedule Ablation}

Here, we ablate our learning rate and schedule to use warmup with linear decay, as opposed to a cosine schedule, to check it is not an artifact of our choice of learning rate schedule. We do so on our 30M model due to compute constraints, finding the degradation with token/parameter ratio persists, as expected. See Figure \ref{fig:lr-schedule}. 


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{tpr_gptq_linear_schedule.pdf}
    \caption{{Linear LR Schedule Ablation}}
    \label{fig:lr-schedule}
\end{figure}



\section{Why do language models get more sensitive with overtraining?}
\label{appdx:sharpness}

This section is speculative. 


\textbf{Sharpness.} A canonical line of work in optimization demonstrates that model sharpness increases during learning until it hovers at a maximal value (the ``edge of stability") \citep{cohen2021gradient, gilmer2021loss}, so that movement along the top Hessian eigenvector degrades loss by more throughout training. Though sharpness is formally a worst-case sensitivity, we conjecture similar results hold for average case, such as loss degradation induced by isotropic noise. It may be possible that sharpness during language model pretraining does not reach its maximal value for a long time, which is why sensitivity to noise monotonically seems to increase as $D/N \to \infty$ on realistic data budgets. Closely related is the largest eigenvalue of the neural tangent kernel (NTK) which captures the magnitude of the variance of the predictor under parameter noise. This quantity is known to empirically increase during training in a variety of settings, and is closely related to generalization guarantees \citep{nguyen2021tight, atanasov2022onset}. 

\textbf{Hierarchical learning strategies become more sensitive throughout training.} Our expectation that overtrained language models may degrade more when quantized at inference-time is motivated in part by the following results. The hierarchical nature of learning is by now well understood in some toy settings: in \citep{abbe2021staircase}, it is shown that ``staircase" polynomials of increasing degree are learned faster than high-degree monomials since neural networks combine existing features to learn new ones. In \citep{abbe2022merged} this result was strengthened to show that such hierarchical structure is both necessary and sufficient to learn sparse functions with SGD in two layer neural networks. In this setting, damage to features encoding lower-order polynomials affects all higher-order ones, so that such networks are increasingly sensitive to fixed feature noise throughout learning. Another result of a similar flavor is that of \citep{barak2022hidden}, who explicitly require high-precision gradients for sparse parity to be learned, since sparse parity is learned by the amplification of a small initial signal. If language models learn hierarchically, it is possible that the features that are learned late into overtraining as $D/N \to \infty$ are reliant on base features, so that noise harms the base features and therefore significantly damages higher-order features. 



\section{Granularity Ablations}

Here, we ablate our choice of quantization granularity (per-tensor vs per-channel) compared to the main text, where we do weights per-channel and activations per-tensor. Per-tensor quantization involves keeping one scalar to rescale all values in a tensor into the quantization codebook range, and per-channel means keeping a scalar per channel dimension; therefore, the latter is strictly more expressive and thus incurs lower quantization loss, than the former, at the cost of slightly more memory usage. Here, we ask: is the increased sensitivity of activations a result of them being inherently more sensitive, or due to the per-tensor design choice. 

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{quantization_granularity.pdf}
    \caption{{Quantization granularity ablation: all combination of (training weight precision, training activation precision) $\times$ (per-tensor, per-channel). Dashed lines are per-channel and solid are per-tensor.}}
    \label{fig:enter-label}
\end{figure}

These results show that activations are generally more sensitive than weights, since their loss penalty at lower precision goes up faster even when granularity is kept fixed across the two. In fact, quantizing activations per-channel is almost as hard as quantizing weights per-tensor. This is consistent with a broad line of work in quantization finding that activations comprise the central difficulty in quantization \citep{dettmers2023case, ma2024era}. 


\section{Main Figure Details}
\label{appdx:main-fig}

The model on the left is $N=30$M parameters, chosen because we could train it to the highest token/parameter ratio given our compute budget. On the right we train a suite of models with $NP$ kept constants on 16B tokens (so that $C = \frac{6}{16}NDP$ is matched throughout under our cost model). We plot val loss on Dolma, as throughout the main text, and use floating-point (rather than integer) to make the pretraining claims as realistic as possible. 

\section{Numerical Fits}
\label{appdx: fits}

Following \citep{muennighoff2024scaling}, we tie $\alpha = \beta$ so they do not become very different, though this is not required. Distinct $\alpha, \beta$ only add expressivity to the model and we have verified the plots look similar without tying. We also only use the full scaling law when specified in the text, since the law is developed piecewise through the text. For instance, Figures 3 and 4 solely fit Chinchilla with a substitution $N \mapsto N_\text{eff}(P_\text{w})$ because at that point $P_\text{a}, P_\text{kv}$ have not been introduced. Figures 5, 6, and 7 use our full scaling law, for instance to make predictions. We emphasize our numerical constants are unlikely to be useful because as \citep{hoffmann2022training, sardana2023beyond} show, fitted constant depend heavily on the architecture and dataset used, which differs from setup to setup. Rather, the trends we identify are the key findings. With that said, our fitted constants are as follows. 

% Our final fitted values for the full training scaling law are $$A, \alpha, B, E = 20.0157, 0.1288, 21.2149, 0.7088$$ and $$\gamma_\text{w}, n_\text{w}, \gamma_\text{i}, n_\text{i}, \gamma_\text{kv}, n_\text{kv} = 3.7122, 0.2286, 4.3556, 0.6683, 0.9030, 3.1662$$ We note we do not use this to make predictions on all plots, because the paper goes in an order building up the form from scratch. 

% C_T = 0.0391, delta_D = 0.5174, delta_N = 0.3299, gamma = 0.6086, b = 0.9558
% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% \textbf{Constant} & \textbf{Value} \\
% \hline
% $A$ & 20.0157 \\
% $\alpha$ & 0.1288 \\
% $B$ & 21.2149 \\
% $E$ & 0.7088 \\
% $\gamma_\text{w}$ & 3.7122 \\
% $n_\text{w}$ & 0.2286 \\
% $\gamma_\text{i}$ & 4.3556 \\
% $n_\text{i}$ & 0.6683 \\
% $\gamma_\text{kv}$ & 0.9030 \\
% $n_\text{kv}$ & 3.1662 \\
% $C_T$ & 0.0391 \\
% $\gamma_D$ & 0.5174 \\
% $\gamma_N$ & 0.3299 \\
% $\gamma$ & 0.6086 \\
% $b$ & 0.9558 \\
% \hline
% \end{tabular}
% \caption{Fitted constants and their values}
% \end{table}

% C_T = 0.0598, delta_D = 0.5068, delta_N = 0.3439, gamma = 0.5907, b = 1.1277

%     A = 4299.1207
%     alpha = 0.4965  
%     B = 18060.3203
%     E = 2.7648
%     gamma_w = 2.6745
% n_w = 0.3037
% gamma_i = 2.2102
% n_i = 1.4072
% gamma_kv = 0.9578
% n_kv = 2.4185
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Constant} & \textbf{Value} \\
\hline
$A$ & 4.299e3 \\
$\alpha$ & 0.4965 \\
$B$ & 1.806e4 \\
$E$ & 2.7648 \\
$\gamma_\text{w}$ & 2.6745 \\
$n_\text{w}$ & 0.3037 \\
$\gamma_\text{i}$ & 2.2102 \\
$n_\text{i}$ & 1.4072 \\
$\gamma_\text{kv}$ & 0.9578 \\
$n_\text{kv}$ & 2.4185 \\
$C_T$ & 0.0598 \\
$\gamma_D$ & 0.5068 \\
$\gamma_N$ & 0.3439 \\
$\gamma$ & 0.5907 \\
$b$ & 1.1277 \\
\hline
\end{tabular}
\caption{Fitted constants and their values}
\end{table}

Note that we include biases in our exponent fits, for instance when modelling $N_\text{eff}$ as a saturating exponential, we find that the different parts of a model cause numerical instability at different values of low precisions, so even if they are the same functional form, they may be translated (left/right shifted versions) of eah other. For instance a fit of the form $e^{x/\gamma_x}$ in the main text is really computed with offset $e^{x/\gamma_x + n}$, but including biases everywhere clutters notation and obscures mathematical insight. 


\section{Are Weights, Activations, and KV Cache Equally Sensitive?}
\label{appdx: diff-param-types}

We find that training runs with $P_a \le 3$ or $P_\text{kv} \le 3$ are not numerically stable, and often diverge, while $P_\text{w} = 3$ is still well behaved. In particular, we find activations are more sensitive, though this could be because we quantize activations per-tensor and weights-per channel, rather than activations being inherently more sensitive. Consequently, we do not fit or validate on runs with activations or attention bits equal to 3. We leave a more detailed analysis of fine-grained sensitivity across layers and types of parameters to future work. The Figure below illustrates the empirical sensitivity by plotting $L(P)$ for the three quantities for various runs $(N, D)$. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{loss_grid_plot_new_evals.pdf} % precision_sweeps
    \caption{Sweeping $L(P)$ for the three model parts at various $N, D$.}
    \label{fig:model-parts}
\end{figure}


\newpage 
\section{Empirical \texorpdfstring{$N_\text{eff}$}{}}
\label{appdx: empirical-neff}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{N_eff_plot_empirical_Dolma_new_evals.pdf}% N_eff_plot_empirical_Dolma
    \caption{Plotting what $N_\text{eff}$ looks like empirically. Each black point is a pretraining run, mathematical details of what is plotted here in Appendix \ref{appdx: derivations}. Blue lines are parametric fits of a saturating exponential.} 
    \label{fig:neff-empirical}
\end{figure}

Consider a model trained with some arbitrary $(N, D, P_\text{w})$. Assuming a Chinchilla function form with $N \mapsto N_\text{eff}(P_\text{w})$, we can write the difference between its loss and the loss of a full precision model as 
$$L(N, D, P_\text{w}) - L(N, D, \infty) = A[N_\text{eff}^{-\alpha}-N^{-\alpha}]$$

as the terms involving $B, D, E$ cancel. Note that $N_\text{eff}(P_\text{w}=\infty) = N$ by construction. In practice, we use a BF16 model as the ``infinite-precision" model, finding no real difference if we use an FP32 model or even a functional fit estimating $P_\text{w} \to \infty$ based on our integer quantization loss results. Our goal is to plot what $f(P)$ looks like where $N_\text{eff} = N \cdot f(P)$. Therefore, we can rearrange the above equation as follows 

\begin{equation}
    f(P) := \frac{N_\text{eff}}{N} = \frac{1}{N}\left[\frac{L(N, D, P_\text{w})-L(N,D,P_\text{w}=\infty)}{A}+N^{-\alpha}\right]^{-1/\alpha}
\end{equation}

Then plotting this quantity using our fitted numerical values (See Appendix \ref{appdx: fits}) gives us the empirical tradeoff between precision and parameters. We can see that the tradeoff is quickly saturating in $P$ to a value near 1. While the functional form is the same for the three model parts, the fitted constants are different. For instance, runs with $P_\text{a} \le 3$ or $P_\text{kv} \le 3$ often diverged, and this was not the case with weight precision. Further, we can see that the KV cache is not sensitive to quantization at higher bit value, but very quickly becomes sensitive around 4-5 bit precision. 


Then as far as the joint functional form for $N_\text{eff}(P_\text{w}, P_\text{a}, P_\text{kv})$ is concerned, we acknowledge that alternative factorizations that do not decompose the model into weights, activations, and KV cache, may have an equally good fit. For instance, decomposing the weights term into a product of layer-wise effects has a reasonable fit though introduces more parameters, and a more coarse-grained version may not decompose the model into parts at all, but only consider tied precisions. We choose this factorized form because QAT considers weights only, and activations and attentions are the two other things that must then be kept in low precision to see compute gains. Since practitioners often care about KV cache on its own, we chose to decompose ``activations and attention" as ``activations and KV cache." We emphasize that our main point is not that \textit{this} factorization is objectively correct, but in observing that such a \textit{factorization} that assumes approximate independence is \textit{possible in the first place}. 


\section{Floating-Point Experiments}
\label{appdx:fp}


The key difference between floating point and integer type is that the former allocates some bits to the \textit{exponent} representation and some to the \textit{mantissa}, and these bits play different roles, unlike in integer type where every bit plays the same role in making the quantization lattice uniformly more fine-grained. We hypothesize that if exponent and mantissa bits are scaled jointly (ie. increase together as total bit count does), the overall trend will still be predictable with a functional form like ours. To test this, we fit a parametric form like Equation \ref{eqn:our-chinchilla} with the constants $A, B, E, \alpha = \beta$ listed in the table. The overall fit results in values of $\gamma_w = 2.8111$ and an exponent bias of $b = 0.1240$, showing the functional form is still a good fit to the data, even for floating point, under reasonably standard bit allocation schemes between mantissa and exponent. On the middle and right, we again re-fit the same parametric form on now specific values of $(N, D)$ and visualize the quality of the resulting predictions. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fp_sweep_fit.pdf}
    \caption{{Fitting an effective parameter form to floating-point precision for weight training. (Left) involves checking quality of fit on 140 training runs in floating point precision for weights during training.}}
    \label{fig:fp-fit}
\end{figure}

We use bit allocations of E2M0, E3M0, E4M1, E3M2, E4M2, E5M2, and E5M6 for 3, 4, 5, 6, 7, 8, 12 bits, respectively, with one sign bit throughout. Since exponent and mantissa bits play in general different roles (ie. the effect of a bit on loss and dynamics depends a lot on whether it comes from the mantissa or exponent in floating point), we expect our functional form does well here because mantissa and exponent allocations both increase jointly as precision rises, so overall the trends are predictable in a similar way. We check directly the role of the two by sweeping ExM3 and E3Mx directly, confirming this intuition. This suggests one route for making fine-grained fits for general arbitrary ExMy combinations is to decompose the effects of mantissa and weights, for instance a form like $N_\text{eff}(P_\text{w, m}, P_\text{w, e}, N)$. Since this is not needed for standard bit allocation choices as we can see in Figure \ref{fig:fp-fit}, we do not delve into this complexity. 

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{em_sweep.pdf}
    \caption{{Exponent-mantissa bit allocation sweep. We can see the two types of bits have different scaling behavior, but both fit the saturating form where the first few bits reduce loss a lot, with diminishing returns after that.}}
    \label{fig:enter-label}
\end{figure}



\section{Additional Plots}
\label{appdx: plots}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{schematic_Dolma.pdf}
    \caption{Illustration of what finite-precision effects during training and inference look like on learning curves. }
    \label{fig:schematic-dolma}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{tax_heatmaps_multiple_N_D_new_evals.pdf}
    \caption{Predicted vs actual $\delta_\text{PTQ}$ for several $N, D$.}
    \label{fig:tax-heatmaps-new-evals}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{appdx_marginals_new_evals.pdf} % predictions_vs_actual_Dolma_i_kv
    \caption{Marginal sweeps for precision of activations and KV cache, along with predictions from an $N_\text{eff}$ functional form analogous to Equation 3 fitted from scratch.}
    \label{fig:more-marginals}
\end{figure}

\begin{figure}
    \centering
    % First row: combined_plots_top
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{combined_plots_top_new_evals.pdf} % combined_plots_top
    \end{subfigure}
    
    % Second row: combined_plots_bottom
    \vspace{1em} % Optional: add some vertical spacing between the two rows
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.66\linewidth]{combined_plots_bottom_new_evals.pdf} % combined_plots_bottom
        % \caption{(b) Equation 6, replacing $N \mapsto N_\text{eff}$ and accounting for competing effects results in a substantially stronger fit and higher $R^2$ that can predict degradation up to $10^{-4}$ fidelity. (c) Example predictions vs actual degradation for various pre/post-training precision combinations, $N = 110$M parameters, $D=13$B tokens.}
    \end{subfigure}

    \caption{Combined plots for predicting degradation. (a) and (b) illustrate different fitting approaches to model degradation, demonstrating a stronger fit when $N \mapsto N_\text{eff}$ is used. (c), (d) (e) illustrate our unified degradation form can predict degradation when training and serving in any precision. Plots (c-e) made for varied $P_\text{w}$, but fits in (a) and (b) include runs where $P_\text{a}, P_\text{kv}$ are also jointly varied. 
    }
    \label{fig:ptq-tax-unifying}
\end{figure}


\end{document}
