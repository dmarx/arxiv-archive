\begin{thebibliography}{98}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020languagemodelsfewshotlearners}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Micikevicius et~al.(2022)Micikevicius, Stosic, Burgess, Cornea, Dubey, Grisenthwaite, Ha, Heinecke, Judd, Kamalu, et~al.]{micikevicius2022fp8}
Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et~al.
\newblock Fp8 formats for deep learning.
\newblock \emph{arXiv preprint arXiv:2209.05433}, 2022.

\bibitem[Ma et~al.(2024)Ma, Wang, Ma, Wang, Wang, Huang, Dong, Wang, Xue, and Wei]{ma2024era}
Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li~Dong, Ruiping Wang, Jilong Xue, and Furu Wei.
\newblock The era of 1-bit llms: All large language models are in 1.58 bits.
\newblock \emph{arXiv preprint arXiv:2402.17764}, 2024.

\bibitem[Wang et~al.(2023)Wang, Ma, Dong, Huang, Wang, Ma, Yang, Wang, Wu, and Wei]{wang2023bitnet}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi~Wu, and Furu Wei.
\newblock Bitnet: Scaling 1-bit transformers for large language models.
\newblock \emph{arXiv preprint arXiv:2310.11453}, 2023.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022gpt3}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30318--30332, 2022.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, and Han]{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activationaware weight quantization for llm compression and acceleration. arxiv.
\newblock \emph{MLSys 2024}, 2023.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem[Bahri et~al.(2024)Bahri, Dyer, Kaplan, Lee, and Sharma]{bahri2024explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{Proceedings of the National Academy of Sciences}, 121\penalty0 (27):\penalty0 e2311878121, 2024.

\bibitem[Bordelon et~al.(2024)Bordelon, Atanasov, and Pehlevan]{bordelon2024dynamical}
Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan.
\newblock A dynamical model of neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2402.01092}, 2024.

\bibitem[Lin et~al.(2024{\natexlab{a}})Lin, Wu, Kakade, Bartlett, and Lee]{lin2024scaling}
Licong Lin, Jingfeng Wu, Sham~M Kakade, Peter~L Bartlett, and Jason~D Lee.
\newblock Scaling laws in linear regression: Compute, parameters, and data.
\newblock \emph{arXiv preprint arXiv:2406.08466}, 2024{\natexlab{a}}.

\bibitem[Dettmers and Zettlemoyer(2023)]{dettmers2023case}
Tim Dettmers and Luke Zettlemoyer.
\newblock The case for 4-bit precision: k-bit inference scaling laws.
\newblock In \emph{International Conference on Machine Learning}, pages 7750--7774. PMLR, 2023.

\bibitem[Allen-Zhu and Li(2024)]{allen2024physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 3.3, knowledge capacity scaling laws.
\newblock \emph{arXiv preprint arXiv:2404.05405}, 2024.

\bibitem[Besiroglu et~al.(2024)Besiroglu, Erdil, Barnett, and You]{besiroglu2024chinchilla}
Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You.
\newblock Chinchilla scaling: A replication attempt.
\newblock \emph{arXiv preprint arXiv:2404.10102}, 2024.

\bibitem[Sardana and Frankle(2023)]{sardana2023beyond}
Nikhil Sardana and Jonathan Frankle.
\newblock Beyond chinchilla-optimal: Accounting for inference in language model scaling laws.
\newblock \emph{arXiv preprint arXiv:2401.00448}, 2023.

\bibitem[Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao, Mercat, Fang, Li, Keh, et~al.]{gadre2024language}
Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et~al.
\newblock Language models scale reliably with over-training and on downstream tasks.
\newblock \emph{arXiv preprint arXiv:2403.08540}, 2024.

\bibitem[Team et~al.(2024)Team, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e}, et~al.]{team2024gemma}
Gemma Team, Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al.
\newblock Gemma 2: Improving open language models at a practical size.
\newblock \emph{arXiv preprint arXiv:2408.00118}, 2024.

\bibitem[Snell et~al.(2024)Snell, Lee, Xu, and Kumar]{snell2024scaling}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than scaling model parameters.
\newblock \emph{arXiv preprint arXiv:2408.03314}, 2024.

\bibitem[Brown et~al.(2024)Brown, Juravsky, Ehrlich, Clark, Le, R{\'e}, and Mirhoseini]{brown2024large}
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc~V Le, Christopher R{\'e}, and Azalia Mirhoseini.
\newblock Large language monkeys: Scaling inference compute with repeated sampling.
\newblock \emph{arXiv preprint arXiv:2407.21787}, 2024.

\bibitem[Yang et~al.(2024)Yang, Band, Li, Cand{\`e}s, and Hashimoto]{yang2024synthetic}
Zitong Yang, Neil Band, Shuangping Li, Emmanuel Cand{\`e}s, and Tatsunori Hashimoto.
\newblock Synthetic continued pretraining.
\newblock \emph{arXiv preprint arXiv:2409.07431}, 2024.

\bibitem[Fan et~al.(2024)Fan, Chen, Krishnan, Katabi, Isola, and Tian]{fan2024scaling}
Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, and Yonglong Tian.
\newblock Scaling laws of synthetic images for model training... for now.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7382--7392, 2024.

\bibitem[Bauer et~al.(2024)Bauer, Trapp, Stenger, Leppich, Kounev, Leznik, Chard, and Foster]{bauer2024comprehensive}
Andr{\'e} Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, and Ian Foster.
\newblock Comprehensive exploration of synthetic data generation: A survey.
\newblock \emph{arXiv preprint arXiv:2401.02524}, 2024.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, et~al.]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv preprint arXiv:2402.00838}, 2024.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Chee et~al.(2024)Chee, Cai, Kuleshov, and De~Sa]{chee2024quip}
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher~M De~Sa.
\newblock Quip: 2-bit quantization of large language models with guarantees.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Huang et~al.(2024)Huang, Liu, Qin, Li, Zhang, Liu, Magno, and Qi]{huang2024billm}
Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi.
\newblock Billm: Pushing the limit of post-training quantization for llms.
\newblock \emph{arXiv preprint arXiv:2402.04291}, 2024.

\bibitem[Abdelkhalik et~al.(2022)Abdelkhalik, Arafa, Santhi, and Badawy]{abdelkhalik2022demystifying}
Hamdy Abdelkhalik, Yehia Arafa, Nandakishore Santhi, and Abdel-Hameed~A Badawy.
\newblock Demystifying the nvidia ampere architecture through microbenchmarking and instruction-level analysis.
\newblock In \emph{2022 IEEE High Performance Extreme Computing Conference (HPEC)}, pages 1--8. IEEE, 2022.

\bibitem[Deitke et~al.(2024)Deitke, Clark, Lee, Tripathi, Yang, Park, Salehi, Muennighoff, Lo, Soldaini, et~al.]{deitke2024molmo}
Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae~Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et~al.
\newblock Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models.
\newblock \emph{arXiv preprint arXiv:2409.17146}, 2024.

\bibitem[Zhu et~al.(2024)Zhu, Zhang, Sifferman, Sheaves, Wang, Richmond, Zhou, and Eshraghian]{zhu2024scalable}
Rui-Jie Zhu, Yu~Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, and Jason~K Eshraghian.
\newblock Scalable matmul-free language modeling.
\newblock \emph{arXiv preprint arXiv:2406.02528}, 2024.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: enhanced transformer with rotary position embedding. corr abs/2104.09864 (2021).
\newblock \emph{arXiv preprint arXiv:2104.09864}, 2021.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Greg Yang, Edward~J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Bordelon et~al.(2023)Bordelon, Noci, Li, Hanin, and Pehlevan]{bordelon2023depthwise}
Blake Bordelon, Lorenzo Noci, Mufan~Bill Li, Boris Hanin, and Cengiz Pehlevan.
\newblock Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit.
\newblock \emph{arXiv preprint arXiv:2309.16620}, 2023.

\bibitem[Wortsman et~al.(2023{\natexlab{a}})Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et~al.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock \emph{arXiv preprint arXiv:2309.14322}, 2023{\natexlab{a}}.

\bibitem[Ahmadian et~al.(2023)Ahmadian, Dash, Chen, Venkitesh, Gou, Blunsom, {\"U}st{\"u}n, and Hooker]{ahmadian2023intriguing}
Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen~Stephen Gou, Phil Blunsom, Ahmet {\"U}st{\"u}n, and Sara Hooker.
\newblock Intriguing properties of quantization at scale.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 34278--34294, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Dodge et~al.(2021)Dodge, Sap, Marasovi{\'c}, Agnew, Ilharco, Groeneveld, Mitchell, and Gardner]{dodge2021documenting}
Jesse Dodge, Maarten Sap, Ana Marasovi{\'c}, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
\newblock Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
\newblock \emph{arXiv preprint arXiv:2104.08758}, 2021.

\bibitem[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, et~al.]{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et~al.
\newblock Mixed precision training.
\newblock \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Wortsman et~al.(2023{\natexlab{b}})Wortsman, Dettmers, Zettlemoyer, Morcos, Farhadi, and Schmidt]{wortsman2023stable}
Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt.
\newblock Stable and low-precision training for large-scale vision-language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 10271--10298, 2023{\natexlab{b}}.

\bibitem[Zhu et~al.(2023)Zhu, Li, Liu, Ma, and Wang]{zhu2023survey}
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang.
\newblock A survey on model compression for large language models.
\newblock \emph{arXiv preprint arXiv:2308.07633}, 2023.

\bibitem[Courbariaux et~al.(2014)Courbariaux, Bengio, and David]{courbariaux2014training}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Training deep neural networks with low precision multiplications.
\newblock \emph{arXiv preprint arXiv:1412.7024}, 2014.

\bibitem[Dettmers et~al.(2024)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2024qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Dettmers et~al.(2021)Dettmers, Lewis, Shleifer, and Zettlemoyer]{dettmers20218}
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization.
\newblock \emph{arXiv preprint arXiv:2110.02861}, 2021.

\bibitem[Sun et~al.(2020)Sun, Wang, Chen, Ni, Agrawal, Cui, Venkataramani, El~Maghraoui, Srinivasan, and Gopalakrishnan]{sun2020ultra}
Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El~Maghraoui, Vijayalakshmi~Viji Srinivasan, and Kailash Gopalakrishnan.
\newblock Ultra-low precision 4-bit training of deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1796--1807, 2020.

\bibitem[Liu et~al.(2023)Liu, Oguz, Zhao, Chang, Stock, Mehdad, Shi, Krishnamoorthi, and Chandra]{liu2023llm}
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
\newblock Llm-qat: Data-free quantization aware training for large language models.
\newblock \emph{arXiv preprint arXiv:2305.17888}, 2023.

\bibitem[Lin et~al.(2024{\natexlab{b}})Lin, Tang, Tang, Yang, Chen, Wang, Xiao, Dang, Gan, and Han]{lin2024awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.
\newblock Awq: Activation-aware weight quantization for on-device llm compression and acceleration.
\newblock \emph{Proceedings of Machine Learning and Systems}, 6:\penalty0 87--100, 2024{\natexlab{b}}.

\bibitem[Sheng et~al.(2023)Sheng, Zheng, Yuan, Li, Ryabinin, Chen, Liang, R{\'e}, Stoica, and Zhang]{sheng2023flexgen}
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher R{\'e}, Ion Stoica, and Ce~Zhang.
\newblock Flexgen: High-throughput generative inference of large language models with a single gpu.
\newblock In \emph{International Conference on Machine Learning}, pages 31094--31116. PMLR, 2023.

\bibitem[Dettmers et~al.(2023)Dettmers, Svirschevski, Egiazarian, Kuznedelev, Frantar, Ashkboos, Borzunov, Hoefler, and Alistarh]{dettmers2023spqr}
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
\newblock Spqr: A sparse-quantized representation for near-lossless llm weight compression.
\newblock \emph{arXiv preprint arXiv:2306.03078}, 2023.

\bibitem[Keller(2018)]{Keller2018}
Jordan Keller.
\newblock Tweet.
\newblock \url{https://twitter.com/kellerjordan0/status/1837874116533407990}, 2018.
\newblock Accessed: 2024-11-29.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.(nips), 2017.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 10:\penalty0 S0140525X16001837, 2017.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Le~Scao et~al.(2023)Le~Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{le2023bloom}
Teven Le~Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock 2023.

\bibitem[Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts, Biderman, Scao, Bari, Shen, Yong, Schoelkopf, et~al.]{muennighoff2022crosslingual}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et~al.
\newblock Crosslingual generalization through multitask finetuning.
\newblock \emph{arXiv preprint arXiv:2211.01786}, 2022.

\bibitem[Muennighoff et~al.(2024{\natexlab{a}})Muennighoff, Soldaini, Groeneveld, Lo, Morrison, Min, Shi, Walsh, Tafjord, Lambert, et~al.]{muennighoff2024olmoe}
Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et~al.
\newblock Olmoe: Open mixture-of-experts language models.
\newblock \emph{arXiv preprint arXiv:2409.02060}, 2024{\natexlab{a}}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis, Muennighoff, Mishra, Gu, Dey, et~al.]{allal2023santacoder}
Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et~al.
\newblock Santacoder: don't reach for the stars!
\newblock \emph{arXiv preprint arXiv:2301.03988}, 2023.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.

\bibitem[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, et~al.]{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al.
\newblock Starcoder 2 and the stack v2: The next generation.
\newblock \emph{arXiv preprint arXiv:2402.19173}, 2024.

\bibitem[Luukkonen et~al.(2023)Luukkonen, Komulainen, Luoma, Eskelinen, Kanerva, Kupari, Ginter, Laippala, Muennighoff, Piktus, et~al.]{luukkonen2023fingpt}
Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et~al.
\newblock Fingpt: Large generative models for a small language.
\newblock \emph{arXiv preprint arXiv:2311.05640}, 2023.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[{\"U}st{\"u}n et~al.(2024){\"U}st{\"u}n, Aryabumi, Yong, Ko, D'souza, Onilude, Bhandari, Singh, Ooi, Kayid, et~al.]{ustun2024aya}
Ahmet {\"U}st{\"u}n, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et~al.
\newblock Aya model: An instruction finetuned open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2402.07827}, 2024.

\bibitem[Ruan et~al.(2024)Ruan, Maddison, and Hashimoto]{ruan2024observational}
Yangjun Ruan, Chris~J Maddison, and Tatsunori Hashimoto.
\newblock Observational scaling laws and the predictability of language model performance.
\newblock \emph{arXiv preprint arXiv:2405.10938}, 2024.

\bibitem[H{\"a}gele et~al.(2024)H{\"a}gele, Bakouch, Kosson, Allal, Von~Werra, and Jaggi]{hagele2024scaling}
Alexander H{\"a}gele, Elie Bakouch, Atli Kosson, Loubna~Ben Allal, Leandro Von~Werra, and Martin Jaggi.
\newblock Scaling laws and compute-optimal training beyond fixed training durations.
\newblock \emph{arXiv preprint arXiv:2405.18392}, 2024.

\bibitem[Tay et~al.(2022{\natexlab{a}})Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran, Yogatama, and Metzler]{tay2022scaling}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung~Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh~Q Tran, Dani Yogatama, and Donald Metzler.
\newblock Scaling laws vs model architectures: How does inductive bias influence scaling?
\newblock \emph{arXiv preprint arXiv:2207.10551}, 2022{\natexlab{a}}.

\bibitem[Krajewski et~al.(2024)Krajewski, Ludziejewski, Adamczewski, Pi{\'o}ro, Krutul, Antoniak, Ciebiera, Kr{\'o}l, Odrzyg{\'o}{\'z}d{\'z}, Sankowski, et~al.]{krajewski2024scaling}
Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pi{\'o}ro, Micha{\l} Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Kr{\'o}l, Tomasz Odrzyg{\'o}{\'z}d{\'z}, Piotr Sankowski, et~al.
\newblock Scaling laws for fine-grained mixture of experts.
\newblock \emph{arXiv preprint arXiv:2402.07871}, 2024.

\bibitem[Tao et~al.(2024)Tao, Liu, Dou, Muennighoff, Wan, Luo, Lin, and Wong]{tao2024scaling}
Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong.
\newblock Scaling laws with vocabulary: Larger models deserve larger vocabularies.
\newblock \emph{arXiv preprint arXiv:2407.13623}, 2024.

\bibitem[Clark et~al.(2022)Clark, de~Las~Casas, Guy, Mensch, Paganini, Hoffmann, Damoc, Hechtman, Cai, Borgeaud, et~al.]{clark2022unified}
Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et~al.
\newblock Unified scaling laws for routed language models.
\newblock In \emph{International conference on machine learning}, pages 4057--4086. PMLR, 2022.

\bibitem[Tay et~al.(2022{\natexlab{b}})Tay, Wei, Chung, Tran, So, Shakeri, Garcia, Zheng, Rao, Chowdhery, et~al.]{tay2022transcending}
Yi~Tay, Jason Wei, Hyung~Won Chung, Vinh~Q Tran, David~R So, Siamak Shakeri, Xavier Garcia, Huaixiu~Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et~al.
\newblock Transcending scaling laws with 0.1\% extra compute.
\newblock \emph{arXiv preprint arXiv:2210.11399}, 2022{\natexlab{b}}.

\bibitem[Scao et~al.(2022)Scao, Wang, Hesslow, Saulnier, Bekman, Bari, Biderman, Elsahar, Muennighoff, Phang, et~al.]{scao2022language}
Teven~Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M~Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et~al.
\newblock What language model to train if you have one million gpu hours?
\newblock \emph{arXiv preprint arXiv:2210.15424}, 2022.

\bibitem[Peng et~al.(2024)Peng, Goldstein, Anthony, Albalak, Alcaide, Biderman, Cheah, Ferdinan, Hou, Kazienko, et~al.]{peng2024eagle}
Bo~Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys{\l}aw Kazienko, et~al.
\newblock Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.
\newblock \emph{arXiv preprint arXiv:2404.05892}, 2024.

\bibitem[Aghajanyan et~al.(2023)Aghajanyan, Yu, Conneau, Hsu, Hambardzumyan, Zhang, Roller, Goyal, Levy, and Zettlemoyer]{aghajanyan2023scaling}
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer.
\newblock Scaling laws for generative mixed-modal language models.
\newblock In \emph{International Conference on Machine Learning}, pages 265--279. PMLR, 2023.

\bibitem[Alabdulmohsin et~al.(2022)Alabdulmohsin, Neyshabur, and Zhai]{alabdulmohsin2022revisiting}
Ibrahim~M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 22300--22312, 2022.

\bibitem[Cherti et~al.(2023)Cherti, Beaumont, Wightman, Wortsman, Ilharco, Gordon, Schuhmann, Schmidt, and Jitsev]{cherti2023reproducible}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 2818--2829, 2023.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Isik et~al.(2024)Isik, Ponomareva, Hazimeh, Paparas, Vassilvitskii, and Koyejo]{isik2024scaling}
Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo.
\newblock Scaling laws for downstream task performance of large language models.
\newblock \emph{arXiv preprint arXiv:2402.04177}, 2024.

\bibitem[Li et~al.(2024)Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora, et~al.]{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock \emph{arXiv preprint arXiv:2406.11794}, 2024.

\bibitem[Liu et~al.(2024)Liu, Zheng, Muennighoff, Zeng, Dou, Pang, Jiang, and Lin]{liu2024regmix}
Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin.
\newblock Regmix: Data mixture as regression for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2407.01492}, 2024.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang, Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock \emph{arXiv preprint arXiv:2402.16827}, 2024.

\bibitem[Muennighoff et~al.(2024{\natexlab{b}})Muennighoff, Rush, Barak, Le~Scao, Tazi, Piktus, Pyysalo, Wolf, and Raffel]{muennighoff2024scaling}
Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le~Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin~A Raffel.
\newblock Scaling data-constrained language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Wu et~al.(2020)Wu, Judd, Zhang, Isaev, and Micikevicius]{wu2020integer}
Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.
\newblock Integer quantization for deep learning inference: Principles and empirical evaluation.
\newblock \emph{arXiv preprint arXiv:2004.09602}, 2020.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and Kalenichenko]{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient integer-arithmetic-only inference.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2704--2713, 2018.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{cohen2021gradient}
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of stability.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gilmer et~al.(2021)Gilmer, Ghorbani, Garg, Kudugunta, Neyshabur, Cardoze, Dahl, Nado, and Firat]{gilmer2021loss}
Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David Cardoze, George Dahl, Zachary Nado, and Orhan Firat.
\newblock A loss curvature perspective on training instability in deep learning.
\newblock \emph{arXiv preprint arXiv:2110.04369}, 2021.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{nguyen2021tight}
Quynh Nguyen, Marco Mondelli, and Guido~F Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks.
\newblock In \emph{International Conference on Machine Learning}, pages 8119--8129. PMLR, 2021.

\bibitem[Atanasov et~al.(2022)Atanasov, Bordelon, Sainathan, and Pehlevan]{atanasov2022onset}
Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, and Cengiz Pehlevan.
\newblock The onset of variance-limited behavior for networks in the lazy and rich regimes.
\newblock \emph{arXiv preprint arXiv:2212.12147}, 2022.

\bibitem[Abbe et~al.(2021)Abbe, Boix-Adsera, Brennan, Bresler, and Nagaraj]{abbe2021staircase}
Emmanuel Abbe, Enric Boix-Adsera, Matthew~S Brennan, Guy Bresler, and Dheeraj Nagaraj.
\newblock The staircase property: How hierarchical structure can guide deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 26989--27002, 2021.

\bibitem[Abbe et~al.(2022)Abbe, Adsera, and Misiakiewicz]{abbe2022merged}
Emmanuel Abbe, Enric~Boix Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 4782--4887. PMLR, 2022.

\bibitem[Barak et~al.(2022)Barak, Edelman, Goel, Kakade, Malach, and Zhang]{barak2022hidden}
Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21750--21764, 2022.

\end{thebibliography}
