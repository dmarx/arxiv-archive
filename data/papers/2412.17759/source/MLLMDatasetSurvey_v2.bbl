\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{ref7}
Alwajih, F., Nagoudi, E., Bhatia, G., Mohamed, A., Abdul-Mageed, M.: Peacock: A family of arabic multimodal large language models and benchmarks. arXiv preprint arXiv:2403.01031  (2024)

\bibitem{ref82}
Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F., Weber, G.: Common voice: A massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670  (2019)

\bibitem{ref12}
Aytar, Y., Vondrick, C., Torralba, A.: See, hear, and read: Deep aligned representations. arXiv preprint arXiv:1706.00932  (2017)

\bibitem{ref139}
Bai, T., Liang, H., Wan, B., Xu, Y., Li, X., Li, S., Yang, L., Li, B., Wang, Y., Cui, B., Huang, P., Shan, J., He, C., Yuan, B., Zhang, W.: A survey of multimodal large language model from a data-centric perspective (2024), \url{https://arxiv.org/abs/2405.16640}

\bibitem{ref16}
Baltrušaitis, T., Ahuja, C., Morency, L.: Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence  \textbf{41}(2),  423--443 (2018)

\bibitem{ref101}
Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving (2020), \url{https://arxiv.org/abs/1903.11027}

\bibitem{ref85}
Chang, A., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J.: Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012  (2015)

\bibitem{ref113}
Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts (2021), \url{https://arxiv.org/abs/2102.08981}

\bibitem{ref110}
Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server (2015), \url{https://arxiv.org/abs/1504.00325}

\bibitem{ref95}
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Li, F.: Imagenet: A large-scale hierarchical image database. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 248--255 (2009)

\bibitem{ref137}
Du, J., Na, X., Liu, X., Bu, H.: Aishell-2: Transforming mandarin asr research into industrial scale (2018), \url{https://arxiv.org/abs/1808.10583}

\bibitem{ref102}
Etten, A.V., Lindenbaum, D., Bacastow, T.M.: Spacenet: A remote sensing dataset and challenge series (2019), \url{https://arxiv.org/abs/1807.01232}

\bibitem{ref88}
Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D., Zhu, Y., Anandkumar, A.: Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems  \textbf{35},  18343--18362 (2022)

\bibitem{ref100}
Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark suite. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2012)

\bibitem{ref89}
Gemmeke, J., Ellis, D., Freedman, D., Jansen, A., Lawrence, W., Moore, R., Plakal, M., Ritter, M.: Audio set: An ontology and human-labeled dataset for audio events. In: 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). pp. 776--780 (2017)

\bibitem{ref98}
Girdhar, R., Ramanan, D.: Cater: A diagnostic dataset for compositional actions and temporal reasoning. CoRR  (2019), \url{https://arxiv.org/abs/1910.04744}

\bibitem{ref86}
Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., Martin, M.: Ego4d: Around the world in 3,000 hours of egocentric video. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition pp. 18995--19012 (2022)

\bibitem{ref2}
Hakimov, S., Schlangen, D.: Images in language space: Exploring the suitability of large language models for vision \& language tasks. arXiv preprint arXiv:2305.13782  (2023)

\bibitem{ref103}
Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification (2019)

\bibitem{ref99}
Johnson, A.E.W., Pollard, T.J., Greenbaum, N.R., Lungren, M.P., ying Deng, C., Peng, Y., Lu, Z., Mark, R.G., Berkowitz, S.J., Horng, S.: Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs (2019)

\bibitem{ref93}
Kapoor, R., Butala, Y., Russak, M., Koh, J., Kamble, K., Alshikh, W., Salakhutdinov, R.: Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553  (2024)

\bibitem{ref81}
Kim, C., Kim, B., Lee, H., Kim, G.: Audiocaps: Generating captions for audios in the wild. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. pp. 119--132 (2019)

\bibitem{ref41}
Lei, J., Yu, L., Bansal, M., Berg, T.: Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696  (2018)

\bibitem{ref134}
Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., Liu, Z.: Mimic-it: Multi-modal in-context instruction tuning (2023), \url{https://arxiv.org/abs/2306.05425}

\bibitem{ref77}
Li, Z., Yang, X., Choi, K., Zhu, W., Hsieh, R., Kim, H., Lim, J., Ji, S., Lee, B., Yan, X., Petzold, L.: Mmsci: A multimodal multi-discipline dataset for phd-level scientific comprehension. arXiv preprint arXiv:2407.04903  (2024)

\bibitem{ref59}
Liang, P., Cheng, Y., Fan, X., Ling, C., Nie, S., Chen, R., Deng, Z., Allen, N., Auerbach, R., Mahmood, F., Salakhutdinov, R.: Quantifying $\&$ modeling multimodal interactions: An information decomposition framework. Advances in Neural Information Processing Systems  \textbf{36} (2024)

\bibitem{ref63}
Liang, P., Lyu, Y., Fan, X., Agarwal, A., Cheng, Y., Morency, L., Salakhutdinov, R.: Multizoo $\&$ multibench: a standardized toolkit for multimodal deep learning. The Journal of Machine Learning Research  \textbf{24}(1),  11056--11062 (2023)

\bibitem{ref11}
Liang, P., Zadeh, A., Morency, L.: Foundations and trends in multimodal machine learning: Principles, challenges, and open questions. arXiv preprint arXiv:2209.03430  (2022)

\bibitem{ref136}
Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023), \url{https://arxiv.org/abs/2304.08485}

\bibitem{ref97}
Ma, X., Yong, S., Zheng, Z., Li, Q., Liang, Y., Zhu, S.C., Huang, S.: Sqa3d: Situated question answering in 3d scenes (2023), \url{https://arxiv.org/abs/2210.07474}

\bibitem{ref83}
Manivannan, M., Nethrapalli, V., Cartwright, M.: Emotioncaps: Enhancing audio captioning through emotion-augmented data generation. arXiv preprint arXiv:2410.12028  (2024)

\bibitem{ref19}
Manzoor, M.A., Albarri, S., Xian, Z., Meng, Z., Nakov, P., Liang, S.: Multimodality representation learning: A survey on evolution, pretraining and its applications (2024), \url{https://arxiv.org/abs/2302.00389}

\bibitem{ref80}
Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.: Howto100m: Learning a text-video embedding by watching hundred million narrated video clips (2019), \url{https://arxiv.org/abs/1906.03327}

\bibitem{ref79}
Nagrani, A., Chung, J., Zisserman, A.: Voxceleb: a large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612  (2017)

\bibitem{ref126}
Nan, K., Xie, R., Zhou, P., Fan, T., Yang, Z., Chen, Z., Li, X., Yang, J., Tai, Y.: Openvid-1m: A large-scale high-quality dataset for text-to-video generation (2024), \url{https://arxiv.org/abs/2407.02371}

\bibitem{ref1}
Naveed, H., Khan, A., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., Mian, A.: A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435  (2023)

\bibitem{ref78}
Panayotov, V., Chen, G., Povey, D., Khudanpur, S.: Librispeech: an asr corpus based on public domain audio books. In: 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). pp. 5206--5210 (2015)

\bibitem{ref14}
Pawłowski, M., Wróblewska, A., Sysko-Romańczuk, S.: Does a technique for building multimodal representation matter?--comparative analysis. arXiv preprint arXiv:2206.06367  (2022)

\bibitem{ref39}
Poria, S., Hazarika, D., Majumder, N., Naik, G., Cambria, E., Mihalcea, R.: Meld: A multimodal multi-party dataset for emotion recognition in conversations. arXiv preprint arXiv:1810.02508  (2018)

\bibitem{ref76}
Qiu, J., Zhu, J., Han, W., Kumar, A., Mittal, K., Jin, C., Yang, Z., Li, L., Wang, J., Zhao, D., Li, B., Wang, L.: Mmsum: A dataset for multimodal summarization and thumbnail generation of videos (2023), \url{https://arxiv.org/abs/2306.04216}

\bibitem{ref13}
Rahate, A., Walambe, R., Ramanna, S., Kotecha, K.: Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions. Information Fusion  \textbf{81},  203--239 (2022)

\bibitem{ref58}
Roberts, J., Lüddecke, T., Sheikh, R., Han, K., Albanie, S.: Charting new territories: Exploring the geographic and geospatial capabilities of multimodal llms (2024), \url{https://arxiv.org/abs/2311.14656}

\bibitem{ref115}
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open large-scale dataset for training next generation image-text models (2022), \url{https://arxiv.org/abs/2210.08402}

\bibitem{ref87}
Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., Fox, D.: Alfred: A benchmark for interpreting grounded instructions for everyday tasks (2020), \url{https://arxiv.org/abs/1912.01734}

\bibitem{ref32}
Stan, G., Rohekar, R., Gurwicz, Y., Olson, M., Bhiwandiwalla, A., Aflalo, E., Wu, C., Duan, N., Tseng, S., Lal, V.: Lvlm-intrepret: An interpretability tool for large vision-language models. arXiv preprint arXiv:2404.03118  (2024)

\bibitem{ref92}
Tanaka, R., Nishida, K., Nishida, K., Hasegawa, T., Saito, I., Saito, K.: Slidevqa: A dataset for document visual question answering on multiple images. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol.~37, pp. 13636--13645 (2023)

\bibitem{ref75}
Tarsi, T., Adel, H., Metzen, J., Zhang, D., Finco, M., Friedrich, A.: Sciol and mulms-img: Introducing a large-scale multimodal scientific dataset and models for image-text tasks in the scientific domain. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 4560--4571 (2024)

\bibitem{ref90}
Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X., Chen, G., Chen, X., Wang, Y., He, C.: Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942  (2023)

\bibitem{ref30}
Wu, J., Gan, W., Chen, Z., Wan, S., Philip, S.: Multimodal large language models: A survey. In: 2023 IEEE International Conference on Big Data (BigData). pp. 2247--2256. IEEE (December 2023)

\bibitem{ref138}
Yan, A., Yang, Z., Wu, J., Zhu, W., Yang, J., Li, L., Lin, K., Wang, J., McAuley, J., Gao, J., Wang, L.: List items one by one: A new data source and learning paradigm for multimodal llms (2024), \url{https://arxiv.org/abs/2404.16375}

\bibitem{ref125}
Yang, D., Huang, S., Lu, C., Han, X., Zhang, H., Gao, Y., Hu, Y., Zhao, H.: Vript: A video is worth thousands of words (2024), \url{https://arxiv.org/abs/2406.06040}

\bibitem{ref132}
Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., Chen, E.: A survey on multimodal large language models. National Science Review  \textbf{11}(12) (Nov 2024). \doi{10.1093/nsr/nwae403}, \url{http://dx.doi.org/10.1093/nsr/nwae403}

\bibitem{ref37}
Yin, Z., Wang, J., Cao, J., Shi, Z., Liu, D., Li, M., Sheng, L., Bai, L., Huang, X., Wang, Z., Shao, J., Ouyang, W.: Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark (2023), \url{https://arxiv.org/abs/2306.06687}

\bibitem{ref105}
Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics  \textbf{2},  67--78 (2014)

\bibitem{ref44}
Yuxuan, S., Yunlong, Z., Yixuan, S., Chenglu, Z., Zhongyi, S., Kai, Z., Jingxiong, L., Xingheng, L., Tao, L., Lin, Y.: Pathgen-1.6m: 1.6 million pathology image-text pairs generation through multi-agent collaboration. arXiv preprint arXiv:2407.00203  (2024)

\bibitem{ref71}
Zadeh, A., Liang, P., Poria, S., Cambria, E., Morency, L.P.: Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. pp. 2236--2246 (01 2018). \doi{10.18653/v1/P18-1208}

\bibitem{ref17}
Zhang, C., Yang, Z., He, X., Deng, L.: Multimodal intelligence: Representation learning, information fusion, and applications. IEEE Journal of Selected Topics in Signal Processing  \textbf{14}(3),  478--493 (2020)

\bibitem{ref91}
Zhang, C., Zhang, Y., Shao, Q., Feng, J., Li, B., Lv, Y., Piao, X., Yin, B.: Bjtt: A large-scale multimodal dataset for traffic prediction. IEEE Transactions on Intelligent Transportation Systems  (2024)

\bibitem{ref3}
Zhang, D., Yu, Y., Li, C., Dong, J., Su, D., Chu, C., Yu, D.: Mm-llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601  (2024)

\bibitem{ref133}
Zhang, J., Huang, J., Jin, S., Lu, S.: Vision-language models for vision tasks: A survey (2024), \url{https://arxiv.org/abs/2304.00685}

\bibitem{ref135}
Zhao, B., Wu, B., He, M., Huang, T.: Svit: Scaling up visual instruction tuning (2023), \url{https://arxiv.org/abs/2307.04087}

\bibitem{ref84}
Zhao, J., Zhang, T., Hu, J., Liu, Y., Jin, Q., Wang, X., Li, H.: M3ed: Multi-modal multi-scene multi-label emotional dialogue database. arXiv preprint arXiv:2205.10237  (2022)

\bibitem{ref5}
Zhao, R., Chen, H., Wang, W., Jiao, F., Do, X., Qin, C., Ding, B., Guo, X., Li, M., Li, X., Joty, S.: Retrieving multimodal information for augmented generation: A survey. arXiv preprint arXiv:2303.10868  (2023)

\end{thebibliography}
