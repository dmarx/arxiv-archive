@article{ref1,
  author = {Naveed, H. and Khan, A.U. and Qiu, S. and Saqib, M. and Anwar, S. and Usman, M. and Akhtar, N. and Barnes, N. and Mian, A.},
  title = {A comprehensive overview of large language models},
  journal = {arXiv preprint arXiv:2307.06435},
  year = {2023}
}

@article{ref2,
  author = {Hakimov, S. and Schlangen, D.},
  title = {Images in language space: Exploring the suitability of large language models for vision \& language tasks},
  journal = {arXiv preprint arXiv:2305.13782},
  year = {2023}
}

@article{ref3,
  author = {Zhang, D. and Yu, Y. and Li, C. and Dong, J. and Su, D. and Chu, C. and Yu, D.},
  title = {Mm-llms: Recent advances in multimodal large language models},
  journal = {arXiv preprint arXiv:2401.13601},
  year = {2024}
}

@article{ref4,
  author = {Shukor, M. and Dancette, C. and Rame, A. and Cord, M.},
  title = {Unival: Unified model for image, video, audio and language tasks},
  journal = {Transactions on Machine Learning Research Journal},
  year = {2023}
}

@article{ref5,
  author = {Zhao, R. and Chen, H. and Wang, W. and Jiao, F. and Do, X.L. and Qin, C. and Ding, B. and Guo, X. and Li, M. and Li, X. and Joty, S.},
  title = {Retrieving multimodal information for augmented generation: A survey},
  journal = {arXiv preprint arXiv:2303.10868},
  year = {2023}
}

@inproceedings{ref6,
  author = {Cui, C. and Ma, Y. and Cao, X. and Ye, W. and Zhou, Y. and Liang, K. and Chen, J. and Lu, J. and Yang, Z. and Liao, K.D. and Gao, T.},
  title = {A survey on multimodal large language models for autonomous driving},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages = {958-979},
  year = {2024}
}

@article{ref7,
  author = {Alwajih, F. and Nagoudi, E.M.B. and Bhatia, G. and Mohamed, A. and Abdul-Mageed, M.},
  title = {Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks},
  journal = {arXiv preprint arXiv:2403.01031},
  year = {2024}
}

@article{ref8,
  author = {Xu, M. and Yin, W. and Cai, D. and Yi, R. and Xu, D. and Wang, Q. and Wu, B. and Zhao, Y. and Yang, C. and Wang, S. and Zhang, Q.},
  title = {A survey of resource-efficient llm and multimodal foundation models},
  journal = {arXiv preprint arXiv:2401.08092},
  year = {2024}
}

@inproceedings{ref9,
  author = {Baldassini, F.B. and Shukor, M. and Cord, M. and Soulier, L. and Piwowarski, B.},
  title = {What Makes Multimodal In-Context Learning Work?},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {1539-1550},
  year = {2024}
}

@inproceedings{ref10,
  author = {Hou, I. and Man, O. and Mettille, S. and Gutierrez, S. and Angelikas, K. and MacNeil, S.},
  title = {More robots are coming: large multimodal models (ChatGPT) can solve visually diverse images of Parsons problems},
  booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
  pages = {29-38},
  year = {2024},
  month = {January}
}

@article{ref11,
  author = {Liang, P.P. and Zadeh, A. and Morency, L.P.},
  title = {Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
  journal = {arXiv preprint arXiv:2209.03430},
  year = {2022}
}

@article{ref12,
  author = {Aytar, Y. and Vondrick, C. and Torralba, A.},
  title = {See, hear, and read: Deep aligned representations},
  journal = {arXiv preprint arXiv:1706.00932},
  year = {2017}
}

@article{ref13,
  author = {Rahate, A. and Walambe, R. and Ramanna, S. and Kotecha, K.},
  title = {Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions},
  journal = {Information Fusion},
  volume = {81},
  pages = {203-239},
  year = {2022}
}

@article{ref14,
  author = {Pawłowski, M. and Wróblewska, A. and Sysko-Romańczuk, S.},
  title = {Does a Technique for Building Multimodal Representation Matter?--Comparative Analysis},
  journal = {arXiv preprint arXiv:2206.06367},
  year = {2022}
}

@article{ref15,
  author = {Jabeen, S. and Li, X. and Amin, M.S. and Bourahla, O. and Li, S. and Jabbar, A.},
  title = {A review on methods and applications in multimodal deep learning},
  journal = {ACM Transactions on Multimedia Computing, Communications and Applications},
  volume = {19},
  number = {2s},
  pages = {1-41},
  year = {2023}
}

@article{ref16,
  author = {Baltrušaitis, T. and Ahuja, C. and Morency, L.P.},
  title = {Multimodal machine learning: A survey and taxonomy},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {41},
  number = {2},
  pages = {423-443},
  year = {2018}
}

@article{ref17,
  author = {Zhang, C. and Yang, Z. and He, X. and Deng, L.},
  title = {Multimodal intelligence: Representation learning, information fusion, and applications},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {14},
  number = {3},
  pages = {478-493},
  year = {2020}
}

@article{ref18,
  author = {Chen, Z. and Deng, Y. and Li, Y. and Gu, Q.},
  title = {Understanding transferable representation learning and zero-shot transfer in clip},
  journal = {arXiv preprint arXiv:2310.00927},
  year = {2023}
}

@misc{ref19,
      title={Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications}, 
      author={Muhammad Arslan Manzoor and Sarah Albarri and Ziting Xian and Zaiqiao Meng and Preslav Nakov and Shangsong Liang},
      year={2024},
      eprint={2302.00389},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.00389}, 
}

@article{ref20,
  author = {Summaira, J. and Li, X. and Shoib, A.M. and Li, S. and Abdul, J.},
  title = {Recent advances and trends in multimodal deep learning: A review},
  journal = {arXiv preprint arXiv:2105.11087},
  year = {2021}
}

@article{ref21,
  title = {High-Modality Multimodal Transformer Quantifying Modality and Interaction Heterogeneity for High-Modality Representation Learning},
  journal = {Unknown},
  year = {Unknown}
}

@article{ref22,
  author = {Yoo, J. and Perlin, K. and Kamalakara, S.R. and Araújo, J.G.},
  title = {Scalable training of language models using JAX pjit and TPUv4},
  journal = {arXiv preprint arXiv:2204.06514},
  year = {2022}
}

@article{ref23,
  author = {Alipour, H. and Pendar, N. and Roy, K.},
  title = {ChatGPT Alternative Solutions: Large Language Models Survey},
  journal = {arXiv preprint arXiv:2310.02922},
  year = {2023}
}
@article{ref24,
  author = {Zhang, L. and Liu, P. and Deldjoo, Y. and Zheng, Y. and Gulla, J.A.},
  title = {Understanding Language Modeling Paradigm Adaptations in Recommender Systems: Lessons Learned and Open Challenges},
  journal = {arXiv preprint arXiv:2404.03788},
  year = {2024}
}

@article{ref25,
  author = {Buscemi, A.},
  title = {A comparative study of code generation using chatgpt 3.5 across 10 programming languages},
  journal = {arXiv preprint arXiv:2308.04477},
  year = {2023}
}

@article{ref26,
  author = {Ding, T. and Chen, T. and Zhu, H. and Jiang, J. and Zhong, Y. and Zhou, J. and Wang, G. and Zhu, Z. and Zharkov, I. and Liang, L.},
  title = {The efficiency spectrum of large language models: An algorithmic survey},
  journal = {arXiv preprint arXiv:2312.00678},
  year = {2023}
}

@inproceedings{ref27,
  author = {Xiang, J. and Tao, T. and Gu, Y. and Shu, T. and Wang, Z. and Yang, Z. and Hu, Z.},
  title = {Language models meet world models: Embodied experiences enhance language models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024}
}

@article{ref28,
  author = {Kumar, P.},
  title = {Large language models humanize technology},
  journal = {arXiv preprint arXiv:2305.05576},
  year = {2023}
}

@article{ref29,
  author = {Zhang, L. and Chen, Z.},
  title = {Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview},
  journal = {arXiv preprint arXiv:2312.11701},
  year = {2023}
}

@inproceedings{ref30,
  author = {Wu, J. and Gan, W. and Chen, Z. and Wan, S. and Philip, S.Y.},
  title = {Multimodal large language models: A survey},
  booktitle = {2023 IEEE International Conference on Big Data (BigData)},
  pages = {2247--2256},
  year = {2023},
  month = {December},
  publisher = {IEEE}
}

@article{ref31,
  author = {Kim, Y.J. and Awan, A.A. and Muzio, A. and Salinas, A.F.C. and Lu, L. and Hendy, A. and Rajbhandari, S. and He, Y. and Awadalla, H.H.},
  title = {Scalable and efficient moe training for multitask multilingual models},
  journal = {arXiv preprint arXiv:2109.10465},
  year = {2021}
}

@article{ref32,
  author = {Stan, G.B.M. and Rohekar, R.Y. and Gurwicz, Y. and Olson, M.L. and Bhiwandiwalla, A. and Aflalo, E. and Wu, C. and Duan, N. and Tseng, S.Y. and Lal, V.},
  title = {LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models},
  journal = {arXiv preprint arXiv:2404.03118},
  year = {2024}
}

@article{ref33,
  author = {Mousavi, J. and Termehchy, A.},
  title = {Towards Consistent Language Models Using Declarative Constraints},
  journal = {arXiv preprint arXiv:2312.15472},
  year = {2023}
}

@article{ref34,
  author = {Srivastava, A. and Rastogi, A. and Rao, A. and Shoeb, A.A.M. and Abid, A. and Fisch, A. and Brown, A.R. and Santoro, A. and Gupta, A. and Garriga-Alonso, A. and Kluska, A.},
  title = {Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  journal = {arXiv preprint arXiv:2206.04615},
  year = {2022}
}

@article{ref35,
  author = {Namvarpour, M.O.H.A.M.M.A.D. and Razi, A.F.S.A.N.E.H.},
  title = {Apprentices to Research Assistants: Advancing Research with Large Language Models},
  journal = {arXiv preprint arXiv:2404.06404},
  year = {2024}
}

@article{ref36,
  author = {Boyko, J. and Cohen, J. and Fox, N. and Veiga, M.H. and Li, J.I. and Liu, J. and Modenesi, B. and Rauch, A.H. and Reid, K.N. and Tribedi, S. and Visheratina, A.},
  title = {An interdisciplinary outlook on large language models for scientific research},
  journal = {arXiv preprint arXiv:2311.04929},
  year = {2023}
}

@misc{ref37,
      title={LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark}, 
      author={Zhenfei Yin and Jiong Wang and Jianjian Cao and Zhelun Shi and Dingning Liu and Mukai Li and Lu Sheng and Lei Bai and Xiaoshui Huang and Zhiyong Wang and Jing Shao and Wanli Ouyang},
      year={2023},
      eprint={2306.06687},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.06687}, 
}

@article{ref38,
  author = {Yang, X. and Wu, W. and Feng, S. and Wang, M. and Wang, D. and Li, Y. and Sun, Q. and Zhang, Y. and Fu, X. and Poria, S.},
  title = {MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks},
  journal = {arXiv preprint arXiv:2310.09036},
  year = {2023}
}

@article{ref39,
  author = {Poria, S. and Hazarika, D. and Majumder, N. and Naik, G. and Cambria, E. and Mihalcea, R.},
  title = {Meld: A multimodal multi-party dataset for emotion recognition in conversations},
  journal = {arXiv preprint arXiv:1810.02508},
  year = {2018}
}

@article{ref40,
  author = {Wu, Q. and Teney, D. and Wang, P. and Shen, C. and Dick, A. and Van Den Hengel, A.},
  title = {Visual question answering: A survey of methods and datasets},
  journal = {Computer Vision and Image Understanding},
  volume = {163},
  pages = {21--40},
  year = {2017}
}

@article{ref41,
  author = {Lei, J. and Yu, L. and Bansal, M. and Berg, T.L.},
  title = {Tvqa: Localized, compositional video question answering},
  journal = {arXiv preprint arXiv:1809.01696},
  year = {2018}
}

@article{ref42,
  author = {Kosti, R. and Alvarez, J.M. and Recasens, A. and Lapedriza, A.},
  title = {Context based emotion recognition using emotic dataset},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {42},
  number = {11},
  pages = {2755--2766},
  year = {2019}
}

@article{ref43,
  author = {Zhu, Y. and Wu, Y. and Sebe, N. and Yan, Y.},
  title = {Vision+ x: A survey on multimodal learning in the light of data},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2024}
}

@article{ref44,
  author = {Yuxuan, S. and Yunlong, Z. and Yixuan, S. and Chenglu, Z. and Zhongyi, S. and Kai, Z. and Jingxiong, L. and Xingheng, L. and Tao, L. and Lin, Y.},
  title = {PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration},
  journal = {arXiv preprint arXiv:2407.00203},
  year = {2024}
}

@article{ref45,
  author = {Minaee, S. and Mikolov, T. and Nikzad, N. and Chenaghlu, M. and Socher, R. and Amatriain, X. and Gao, J.},
  title = {Large language models: A survey},
  journal = {arXiv preprint arXiv:2402.06196},
  year = {2024}
}

@article{ref46,
  author = {Kaji, A. and Shah, M.},
  title = {Contextual Code Switching for Machine Translation using Language Models},
  journal = {arXiv preprint arXiv:2312.13179},
  year = {2023}
}

@article{ref47,
  author = {Sarkar, S. and Babar, M.F. and Hasan, M. and Karmaker, S.K.},
  title = {LLMs as On-demand Customizable Service},
  journal = {arXiv preprint arXiv:2401.16577},
  year = {2024}
}

@article{ref48,
  author = {Liu, Y. and He, H. and Han, T. and Zhang, X. and Liu, M. and Tian, J. and Zhang, Y. and Wang, J. and Gao, X. and Zhong, T. and Pan, Y.},
  title = {Understanding llms: A comprehensive overview from training to inference},
  journal = {arXiv preprint arXiv:2401.02038},
  year = {2024}
}

@article{ref49,
  author = {Ahmed, T. and Piovesan, N. and De Domenico, A. and Choudhury, S.},
  title = {Linguistic Intelligence in Large Language Models for Telecommunications},
  journal = {arXiv preprint arXiv:2402.15818},
  year = {2024}
}

@article{ref50,
  author = {Sathish, V. and Lin, H. and Kamath, A.K. and Nyayachavadi, A.},
  title = {LLeMpower: Understanding Disparities in the Control and Access of Large Language Models},
  journal = {arXiv preprint arXiv:2404.09356},
  year = {2024}
}

@inproceedings{ref51,
  author = {Sakib, F.A. and Khan, S.H. and Karim, A.R.},
  title = {Extending the frontier of chatgpt: Code generation and debugging},
  booktitle = {2024 International Conference on Electrical, Computer and Energy Technologies (ICECET)},
  pages = {1--6},
  year = {2024},
  month = {July}
}

@article{ref52,
  author = {Huang, Y. and Chen, Y. and Chen, X. and Chen, J. and Peng, R. and Tang, Z. and Huang, J. and Xu, F. and Zheng, Z.},
  title = {Generative Software Engineering},
  journal = {arXiv preprint arXiv:2403.02583},
  year = {2024}
}

@article{ref53,
  author = {Yang, X. and Yang, X. and Liu, W. and Li, J. and Yu, P. and Ye, Z. and Bian, J.},
  title = {Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R$\&$D Cycle},
  journal = {arXiv preprint arXiv:2310.11249},
  year = {2023}
}

@article{ref54,
  author = {Carolan, K. and Fennelly, L. and Smeaton, A.F.},
  title = {A Review of Multi-Modal Large Language and Vision Models},
  journal = {arXiv preprint arXiv:2404.01322},
  year = {2024}
}

@article{ref55,
  author = {Zhao, S. and Fang, X.},
  title = {Technical Report: Competition Solution For BetterMixture},
  journal = {arXiv preprint arXiv:2403.13233},
  year = {2024}
}

@article{ref56,
  author = {Yang, Z. and Lin, Z. and Guo, L. and Li, Q. and Liu, W.},
  title = {MMED: a multi-domain and multi-modality event dataset},
  journal = {Information Processing \& Management},
  volume = {57},
  number = {6},
  pages = {102315},
  year = {2020}
}

@article{ref57,
  author = {Verma, Y. and Jangra, A. and Kumar, R. and Saha, S.},
  title = {Large scale multi-lingual multi-modal summarization dataset},
  journal = {arXiv preprint arXiv:2302.06560},
  year = {2023}
}

@misc{ref58,
      title={Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs}, 
      author={Jonathan Roberts and Timo Lüddecke and Rehan Sheikh and Kai Han and Samuel Albanie},
      year={2024},
      eprint={2311.14656},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.14656}, 
}

@article{ref59,
  author = {Liang, P.P. and Cheng, Y. and Fan, X. and Ling, C.K. and Nie, S. and Chen, R. and Deng, Z. and Allen, N. and Auerbach, R. and Mahmood, F. and Salakhutdinov, R.R.},
  title = {Quantifying $\&$ modeling multimodal interactions: An information decomposition framework},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024}
}

@article{ref60,
  author = {Zhang, Y. and Xu, Y. and Chen, J. and Xie, F. and Chen, H.},
  title = {Prototypical information bottlenecking and disentangling for multimodal cancer survival prediction},
  journal = {arXiv preprint arXiv:2401.01646},
  year = {2024}
}

@article{ref61,
  author = {Behmanesh, M. and Adibi, P. and Ehsani, S.M.S. and Chanussot, J.},
  title = {Geometric multimodal deep learning with multiscaled graph wavelet convolutional network},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year = {2022}
}

@inproceedings{ref62,
  author = {Pushkarna, M. and Zaldivar, A. and Kjartansson, O.},
  title = {Data cards: Purposeful and transparent dataset documentation for responsible ai},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {1776--1826},
  year = {2022},
  month = {June}
}

@article{ref63,
  author = {Liang, P.P. and Lyu, Y. and Fan, X. and Agarwal, A. and Cheng, Y. and Morency, L.P. and Salakhutdinov, R.},
  title = {MULTIZOO $\&$ MULTIBENCH: a standardized toolkit for multimodal deep learning},
  journal = {The Journal of Machine Learning Research},
  volume = {24},
  number = {1},
  pages = {11056--11062},
  year = {2023}
}

@article{ref64,
  author = {Li, M. and Yumer, E. and Ramanan, D.},
  title = {Budgeted training: Rethinking deep neural network training under resource constraints},
  journal = {arXiv preprint arXiv:1905.04753},
  year = {2019}
}

@article{ref65,
  author = {Anil, R. and Gupta, V. and Koren, T. and Singer, Y.},
  title = {Memory efficient adaptive optimization},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  year = {2019}
}

@article{ref66,
  author = {Meyer, L. and Ribés, A. and Raffin, B.},
  title = {Simulation-Based Parallel Training},
  journal = {arXiv preprint arXiv:2211.04119},
  year = {2022}
}

@article{ref67,
  author = {Huang, S. and Zhang, H. and Gao, Y. and Hu, Y. and Qin, Z.},
  title = {From Image to Video, what do we need in multimodal LLMs?},
  journal = {arXiv preprint arXiv:2404.11865},
  year = {2024}
}

@inproceedings{ref68,
  author = {Piergiovanni, A.J. and Noble, I. and Kim, D. and Ryoo, M.S. and Gomes, V. and Angelova, A.},
  title = {Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {26804--26814},
  year = {2024}
}

@article{ref69,
  author = {Pigozzi, F.},
  title = {Robots: the century past and the century ahead},
  journal = {arXiv preprint arXiv:2204.13331},
  year = {2022}
}

@misc{ref70,
  author = {Tyldum, M. and Desplat, A. and Moore, G. and Grossman, N.},
  title = {The imitation game},
  year = {2015},
  note = {Ascot Elite Home Entertainment}
}



@inproceedings{ref71,
author = {Zadeh, AmirAli and Liang, Paul and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
year = {2018},
month = {01},
pages = {2236-2246},
title = {Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph},
doi = {10.18653/v1/P18-1208}
}

@inproceedings{ref72,
  author = {Gadre, S.Y. and Ilharco, G. and Fang, A. and Hayase, J. and Smyrnis, G. and Nguyen, T. and Marten, R. and Wortsman, M. and Ghosh, D. and Zhang, J. and Orgad, E.},
  title = {Datacomp: In search of the next generation of multimodal datasets},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024}
}

@article{ref73,
  author = {Zhang, K. and Li, B. and Zhang, P. and Pu, F. and Cahyono, J.A. and Hu, K. and Liu, S. and Zhang, Y. and Yang, J. and Li, C. and Liu, Z.},
  title = {Lmms-eval: Reality check on the evaluation of large multimodal models},
  journal = {arXiv preprint arXiv:2407.12772},
  year = {2024}
}

@article{ref74,
  author = {Liang, P.P. and Goindani, A. and Chafekar, T. and Mathur, L. and Yu, H. and Salakhutdinov, R. and Morency, L.P.},
  title = {HEMM: Holistic Evaluation of Multimodal Foundation Models},
  journal = {arXiv preprint arXiv:2407.03418},
  year = {2024}
}

@inproceedings{ref75,
  author = {Tarsi, T. and Adel, H. and Metzen, J.H. and Zhang, D. and Finco, M. and Friedrich, A.},
  title = {SciOL and MuLMS-Img: Introducing A Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages = {4560--4571},
  year = {2024}
}

@misc{ref76,
      title={MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos}, 
      author={Jielin Qiu and Jiacheng Zhu and William Han and Aditesh Kumar and Karthik Mittal and Claire Jin and Zhengyuan Yang and Linjie Li and Jianfeng Wang and Ding Zhao and Bo Li and Lijuan Wang},
      year={2023},
      eprint={2306.04216},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.04216}, 
}

@article{ref77,
  author = {Li, Z. and Yang, X. and Choi, K. and Zhu, W. and Hsieh, R. and Kim, H. and Lim, J.H. and Ji, S. and Lee, B. and Yan, X. and Petzold, L.R.},
  title = {Mmsci: A multimodal multi-discipline dataset for phd-level scientific comprehension},
  journal = {arXiv preprint arXiv:2407.04903},
  year = {2024}
}

@inproceedings{ref78,
  author = {Panayotov, V. and Chen, G. and Povey, D. and Khudanpur, S.},
  title = {Librispeech: an asr corpus based on public domain audio books},
  booktitle = {2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages = {5206--5210},
  year = {2015}
}

@article{ref79,
  author = {Nagrani, A. and Chung, J.S. and Zisserman, A.},
  title = {Voxceleb: a large-scale speaker identification dataset},
  journal = {arXiv preprint arXiv:1706.08612},
  year = {2017}
}

@misc{ref80,
      title={HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips}, 
      author={Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and Ivan Laptev and Josef Sivic},
      year={2019},
      eprint={1906.03327},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1906.03327}, 
}


@inproceedings{ref81,
  author = {Kim, C.D. and Kim, B. and Lee, H. and Kim, G.},
  title = {Audiocaps: Generating captions for audios in the wild},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics},
  pages = {119--132},
  year = {2019}
}

@article{ref82,
  author = {Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F.M. and Weber, G.},
  title = {Common voice: A massively-multilingual speech corpus},
  journal = {arXiv preprint arXiv:1912.06670},
  year = {2019}
}

@article{ref83,
  author = {Manivannan, M. and Nethrapalli, V. and Cartwright, M.},
  title = {EmotionCaps: Enhancing Audio Captioning Through Emotion-Augmented Data Generation},
  journal = {arXiv preprint arXiv:2410.12028},
  year = {2024}
}

@article{ref84,
  author = {Zhao, J. and Zhang, T. and Hu, J. and Liu, Y. and Jin, Q. and Wang, X. and Li, H.},
  title = {M3ED: Multi-modal multi-scene multi-label emotional dialogue database},
  journal = {arXiv preprint arXiv:2205.10237},
  year = {2022}
}

@article{ref85,
  author = {Chang, A.X. and Funkhouser, T. and Guibas, L. and Hanrahan, P. and Huang, Q. and Li, Z. and Savarese, S. and Savva, M. and Song, S. and Su, H. and Xiao, J.},
  title = {Shapenet: An information-rich 3d model repository},
  journal = {arXiv preprint arXiv:1512.03012},
  year = {2015}
}

@article{ref86,
  author = {Grauman, K. and Westbury, A. and Byrne, E. and Chavis, Z. and Furnari, A. and Girdhar, R. and Hamburger, J. and Jiang, H. and Liu, M. and Liu, X. and Martin, M.},
  title = {Ego4d: Around the world in 3,000 hours of egocentric video},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {18995--19012},
  year = {2022}
}

@misc{ref87,
      title={ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks},
      author={Mohit Shridhar and Jesse Thomason and Daniel Gordon and Yonatan Bisk and Winson Han and Roozbeh Mottaghi and Luke Zettlemoyer and Dieter Fox},
      year={2020},
      eprint={1912.01734},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1912.01734}
}

@article{ref88,
  author = {Fan, L. and Wang, G. and Jiang, Y. and Mandlekar, A. and Yang, Y. and Zhu, H. and Tang, A. and Huang, D.A. and Zhu, Y. and Anandkumar, A.},
  title = {Minedojo: Building open-ended embodied agents with internet-scale knowledge},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {18343--18362},
  year = {2022}
}

@inproceedings{ref89,
  author = {Gemmeke, J.F. and Ellis, D.P. and Freedman, D. and Jansen, A. and Lawrence, W. and Moore, R.C. and Plakal, M. and Ritter, M.},
  title = {Audio set: An ontology and human-labeled dataset for audio events},
  booktitle = {2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages = {776--780},
  year = {2017}
}

@article{ref90,
  author = {Wang, Y. and He, Y. and Li, Y. and Li, K. and Yu, J. and Ma, X. and Li, X. and Chen, G. and Chen, X. and Wang, Y. and He, C.},
  title = {Internvid: A large-scale video-text dataset for multimodal understanding and generation},
  journal = {arXiv preprint arXiv:2307.06942},
  year = {2023}
}

@article{ref91,
  author = {Zhang, C. and Zhang, Y. and Shao, Q. and Feng, J. and Li, B. and Lv, Y. and Piao, X. and Yin, B.},
  title = {Bjtt: A large-scale multimodal dataset for traffic prediction},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  year = {2024}
}

@inproceedings{ref92,
  author = {Tanaka, R. and Nishida, K. and Nishida, K. and Hasegawa, T. and Saito, I. and Saito, K.},
  title = {Slidevqa: A dataset for document visual question answering on multiple images},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {11},
  pages = {13636--13645},
  year = {2023}
}

@article{ref93,
  author = {Kapoor, R. and Butala, Y.P. and Russak, M. and Koh, J.Y. and Kamble, K. and Alshikh, W. and Salakhutdinov, R.},
  title = {OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web},
  journal = {arXiv preprint arXiv:2402.17553},
  year = {2024}
}

@article{ref94,
  author = {Zhang, K. and Li, B. and Zhang, P. and Pu, F. and Cahyono, J.A. and Hu, K. and Liu, S. and Zhang, Y. and Yang, J. and Li, C. and Liu, Z.},
  title = {Lmms-eval: Reality check on the evaluation of large multimodal models},
  journal = {arXiv preprint arXiv:2407.12772},
  year = {2024}
}

@inproceedings{ref95,
  author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Li, F.},
  title = {ImageNet: A Large-Scale Hierarchical Image Database},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {248--255},
  year = {2009}
}

@inproceedings{ref96,
  author = {Marino, K. and Rastegari, M. and Farhadi, A. and Mottaghi, R.},
  title = {OK-VQA: A visual question answering benchmark requiring external knowledge},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {3195--3204},
  year = {2019}
}

@misc{ref97,
      title={SQA3D: Situated Question Answering in 3D Scenes}, 
      author={Xiaojian Ma and Silong Yong and Zilong Zheng and Qing Li and Yitao Liang and Song-Chun Zhu and Siyuan Huang},
      year={2023},
      eprint={2210.07474},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.07474}, 
}

@article{ref98,
  title={CATER: A diagnostic dataset for Compositional Actions and Temporal Reasoning},
  author={Rohit Girdhar and Deva Ramanan},
  journal={CoRR},
  year={2019},
  url={https://arxiv.org/abs/1910.04744},
  eprint={1910.04744},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{ref99,
      title={MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs}, 
      author={Alistair E. W. Johnson and Tom J. Pollard and Nathaniel R. Greenbaum and Matthew P. Lungren and Chih-ying Deng and Yifan Peng and Zhiyong Lu and Roger G. Mark and Seth J. Berkowitz and Steven Horng},
      year={2019},
      eprint={1901.07042},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@INPROCEEDINGS{ref100,
author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2012}
}

@misc{ref101,
      title={nuScenes: A multimodal dataset for autonomous driving}, 
      author={Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and Giancarlo Baldan and Oscar Beijbom},
      year={2020},
      eprint={1903.11027},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1903.11027}, 
}

@misc{ref102,
      title={SpaceNet: A Remote Sensing Dataset and Challenge Series}, 
      author={Adam Van Etten and Dave Lindenbaum and Todd M. Bacastow},
      year={2019},
      eprint={1807.01232},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1807.01232}, 
}

@misc{ref103,
      title={EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification}, 
      author={Patrick Helber and Benjamin Bischke and Andreas Dengel and Damian Borth},
      year={2019},
      eprint={1709.00029},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@misc{ref104,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1405.0312}, 
}

@article{ref105,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={67--78},
  year={2014}
}

@misc{ref106,
      title={Multimodal Explanations: Justifying Decisions and Pointing to the Evidence}, 
      author={Dong Huk Park and Lisa Anne Hendricks and Zeynep Akata and Anna Rohrbach and Bernt Schiele and Trevor Darrell and Marcus Rohrbach},
      year={2018},
      eprint={1802.08129},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.08129}, 
}

@article{ref107,
  title={Multimodal machine learning: A survey and taxonomy},
  author={Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={41},
  number={2},
  pages={423--443},
  year={2019},
  publisher={IEEE}
}

@misc{ref108,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, 
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.05918}, 
}

@misc{ref109,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.14198}, 
}

@misc{ref110,
      title={Microsoft COCO Captions: Data Collection and Evaluation Server}, 
      author={Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
      year={2015},
      eprint={1504.00325},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1504.00325}, 
}

@misc{ref111,
      title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations}, 
      author={Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li-Jia Li and David A. Shamma and Michael S. Bernstein and Fei-Fei Li},
      year={2016},
      eprint={1602.07332},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1602.07332}, 
}

@inproceedings{ref112,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@misc{ref113,
      title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts}, 
      author={Soravit Changpinyo and Piyush Sharma and Nan Ding and Radu Soricut},
      year={2021},
      eprint={2102.08981},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.08981}, 
}

@inproceedings{ref114,
 author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 volume = {24},
 year = {2011}
}

@misc{ref115,
      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, 
      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
      year={2022},
      eprint={2210.08402},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.08402}, 
}

@misc{ref116,
      title={LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs}, 
      author={Christoph Schuhmann and Richard Vencu and Romain Beaumont and Robert Kaczmarczyk and Clayton Mullis and Aarush Katta and Theo Coombes and Jenia Jitsev and Aran Komatsuzaki},
      year={2021},
      eprint={2111.02114},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2111.02114}, 
}

@article{ref117,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    editor = "Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q14-1006",
    doi = "10.1162/tacl_a_00166",
    pages = "67--78",
    abstract = "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
}

@article{ref118,
author = {Thomee, Bart and Shamma, David A. and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
title = {YFCC100M: the new data in multimedia research},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2812802},
doi = {10.1145/2812802},
abstract = {This publicly available curated dataset of almost 100 million photos and videos is free and legal for all.},
journal = {Commun. ACM},
month = jan,
pages = {64–73},
numpages = {10}
}

@inproceedings{ref119,
author = {Gu, Jiaxi and Meng, Xiaojun and Lu, Guansong and Hou, Lu and Niu, Minzhe and Liang, Xiaodan and Yao, Lewei and Huang, Runhui and Zhang, Wei and Jiang, Xin and Xu, Chunjing and Xu, Hang},
title = {Wukong: a 100 million large-scale chinese cross-modal pre-training benchmark},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, WukongViT-L achieves an average accuracy of 73.03\%. For the image-text retrieval task, it achieves a mean recall of 71.6\% on AIC-ICC which is 12.9\% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et al.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1916},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@misc{ref120,
      title={PaLI: A Jointly-Scaled Multilingual Language-Image Model}, 
      author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme and Andreas Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
      year={2023},
      eprint={2209.06794},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.06794}, 
}

@misc{ref121,
      title={FILIP: Fine-grained Interactive Language-Image Pre-Training}, 
      author={Lewei Yao and Runhui Huang and Lu Hou and Guansong Lu and Minzhe Niu and Hang Xu and Xiaodan Liang and Zhenguo Li and Xin Jiang and Chunjing Xu},
      year={2021},
      eprint={2111.07783},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2111.07783}, 
}



@misc{ref122,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@misc{ref123,
      title={Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks}, 
      author={Haiyang Xu and Qinghao Ye and Xuan Wu and Ming Yan and Yuan Miao and Jiabo Ye and Guohai Xu and Anwen Hu and Yaya Shi and Guangwei Xu and Chenliang Li and Qi Qian and Maofei Que and Ji Zhang and Xiao Zeng and Fei Huang},
      year={2023},
      eprint={2306.04362},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.04362}, 
}

@misc{ref124,
      title={Learning Audio-Video Modalities from Image Captions}, 
      author={Arsha Nagrani and Paul Hongsuck Seo and Bryan Seybold and Anja Hauth and Santiago Manen and Chen Sun and Cordelia Schmid},
      year={2022},
      eprint={2204.00679},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.00679}, 
}

@misc{ref125,
      title={Vript: A Video Is Worth Thousands of Words}, 
      author={Dongjie Yang and Suyuan Huang and Chengqiang Lu and Xiaodong Han and Haoxin Zhang and Yan Gao and Yao Hu and Hai Zhao},
      year={2024},
      eprint={2406.06040},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.06040}, 
}

@misc{ref126,
      title={OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation}, 
      author={Kepan Nan and Rui Xie and Penghao Zhou and Tiehan Fan and Zhenheng Yang and Zhijie Chen and Xiang Li and Jian Yang and Ying Tai},
      year={2024},
      eprint={2407.02371},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.02371}, 
}

@INPROCEEDINGS{ref127,
  author={Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Audio Set: An ontology and human-labeled dataset for audio events}, 
  year={2017},
  volume={},
  number={},
  pages={776-780},
  keywords={Ontologies;Birds;Music;Taxonomy;Labeling;Audio event detection;sound ontology;audio databases;data collection},
  doi={10.1109/ICASSP.2017.7952261}}

@misc{ref128,
      title={YODAS: Youtube-Oriented Dataset for Audio and Speech}, 
      author={Xinjian Li and Shinnosuke Takamichi and Takaaki Saeki and William Chen and Sayaka Shiota and Shinji Watanabe},
      year={2024},
      eprint={2406.00899},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.00899}, 
}

@misc{ref129,
      title={Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation}, 
      author={Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},
      year={2024},
      eprint={2211.06687},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2211.06687}, 
}

@misc{ref130,
      title={Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning}, 
      author={Luoyi Sun and Xuenan Xu and Mengyue Wu and Weidi Xie},
      year={2024},
      eprint={2309.11500},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2309.11500}, 
}

@misc{ref131,
      title={VidGen-1M: A Large-Scale Dataset for Text-to-video Generation}, 
      author={Zhiyu Tan and Xiaomeng Yang and Luozheng Qin and Hao Li},
      year={2024},
      eprint={2408.02629},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.02629}, 
}

@misc{ref1322,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.11416}, 
}

@article{ref132,
   title={A survey on multimodal large language models},
   volume={11},
   ISSN={2053-714X},
   url={http://dx.doi.org/10.1093/nsr/nwae403},
   DOI={10.1093/nsr/nwae403},
   number={12},
   journal={National Science Review},
   publisher={Oxford University Press (OUP)},
   author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
   year={2024},
   month=nov }

@misc{ref133,
      title={Vision-Language Models for Vision Tasks: A Survey}, 
      author={Jingyi Zhang and Jiaxing Huang and Sheng Jin and Shijian Lu},
      year={2024},
      eprint={2304.00685},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.00685}, 
}

@misc{ref134,
      title={MIMIC-IT: Multi-Modal In-Context Instruction Tuning}, 
      author={Bo Li and Yuanhan Zhang and Liangyu Chen and Jinghao Wang and Fanyi Pu and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2023},
      eprint={2306.05425},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.05425}, 
}

@misc{ref135,
      title={SVIT: Scaling up Visual Instruction Tuning}, 
      author={Bo Zhao and Boya Wu and Muyang He and Tiejun Huang},
      year={2023},
      eprint={2307.04087},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.04087}, 
}

@misc{ref136,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}

@misc{ref137,
      title={AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale}, 
      author={Jiayu Du and Xingyu Na and Xuechen Liu and Hui Bu},
      year={2018},
      eprint={1808.10583},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1808.10583}, 
}

@misc{ref138,
      title={List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs}, 
      author={An Yan and Zhengyuan Yang and Junda Wu and Wanrong Zhu and Jianwei Yang and Linjie Li and Kevin Lin and Jianfeng Wang and Julian McAuley and Jianfeng Gao and Lijuan Wang},
      year={2024},
      eprint={2404.16375},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.16375}, 
}

@misc{ref139,
      title={A Survey of Multimodal Large Language Model from A Data-centric Perspective}, 
      author={Tianyi Bai and Hao Liang and Binwang Wan and Yanran Xu and Xi Li and Shiyu Li and Ling Yang and Bozhou Li and Yifan Wang and Bin Cui and Ping Huang and Jiulong Shan and Conghui He and Binhang Yuan and Wentao Zhang},
      year={2024},
      eprint={2405.16640},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.16640}, 
}
%\bibitem{ref97} Bommasani, R., Hudson, D.A., Wallace, E., and et al., 2021. On the Opportunities and Risks of Foundation Models. arXiv preprint arXiv:2108.07258.
