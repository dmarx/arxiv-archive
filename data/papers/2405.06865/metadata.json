{
  "arxivId": "2405.06865",
  "title": "Disrupting Style Mimicry Attacks on Video Imagery",
  "authors": "Josephine Passananti, Stanley Wu, Shawn Shan, Haitao Zheng, Ben Y. Zhao",
  "abstract": "Generative AI models are often used to perform mimicry attacks, where a pretrained model is fine-tuned on a small sample of images to learn to mimic a specific artist of interest. While researchers have introduced multiple anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence points to a growing trend of mimicry models using videos as sources of training data. This paper presents our experiences exploring techniques to disrupt style mimicry on video imagery. We first validate that mimicry attacks can succeed by training on individual frames extracted from videos. We show that while anti-mimicry tools can offer protection when applied to individual frames, this approach is vulnerable to an adaptive countermeasure that removes protection by exploiting randomness in optimization results of consecutive (nearly-identical) frames. We develop a new, tool-agnostic framework that segments videos into short scenes based on frame-level similarity, and use a per-scene optimization baseline to remove inter-frame randomization while reducing computational cost. We show via both image level metrics and an end-to-end user study that the resulting protection restores protection against mimicry (including the countermeasure). Finally, we develop another adaptive countermeasure and find that it falls short against our framework.",
  "url": "https://arxiv.org/abs/2405.06865",
  "issue_number": 153,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/153",
  "created_at": "2024-12-22T16:37:13Z",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_minutes": 0,
  "last_read": null
}