# Appendices

### Related Work

**Language model reasoning.** Language model performance on challenging mathematical reasoning tasks has rapidly improved in recent years [@lewkowycz2022solving; @geminiteam2024gemini; @openai2024gpt4; @shao2024deepseekmath; @lightman2023lets]. These improvements can be attributed to four primary factors: **1)** running continued pretraining on large corpora of math focused data [@lewkowycz2022solving; @geminiteam2024gemini; @shao2024deepseekmath; @lightman2023lets]; **2)** improving the LLM proposal distribution by either applying targeted optimization on specific reasoning tasks by finetuning with RL [@singh2024human; @zelikman2022star; @shao2024deepseekmath; @yuan2023scaling] enabling models to critique and revise their answers iteratively [@bai2022constitutional; @madaan2023selfrefine; @du2023improving; @saunders2022selfcritiquing]; **3)** enabling LLMs to benefit from additional test-time computation by finetuning verifiers [@lightman2023lets; @cobbe2021training; @uesato2022solving; @wang2023mathshepherd; @yao2023tree; @feng2024alphazerolike; @chen2024alphamath; @tian2024selfimprovement]. Our work builds on these second and third lines of research by analyzing the extent to which test-time compute scaling can be improved by 1) refining an LLM's proposal distribution and 2) conducting search against verifiers.

**Analyzing test-time compute scaling.** The tradeoff between train-time and test-time compute using Monte-Carlo tree search applied to the board game Hex was previously studied by @jones2021scalingscalinglawsboard. We instead focus our analysis on full-scale language model math reasoning problems. A survey work by @epoch2023tradingoffcomputeintrainingandinference analyzed the tradeoff between training and inference across a number of domains. However, much of their language-model analysis focused on test-time compute scaling in settings where the ground-truth answer is known. In contrast, our analysis focuses on the setting when the ground-truth answer is not known. Additionally, a number of works in the RL literature have proposed methods, such as MCTS [@kocsis2006bandit], which aim to navigate the tradeoff between test-time and training-time compute so as to enable a form of iterative self-play. The findings in our work can be used to help develop similar algorithms that can operate on open-ended natural language.

**Augmenting LLMs with test-time compute.** Beyond verifiers and revisions, a number of additional works have proposed alternative methods for enabling LMs to use test-time compute for reasoning. Namely, @wang2024hypothesissearchinductivereasoning conducts a hierarchical hypothesis search to enable inductive reasoning capabilities. A number of related works have proposed augmenting language models with tools at test-time, which can greatly improve their performance on downstream tasks [@gao2023palprogramaidedlanguagemodels; @qin2023toolllmfacilitatinglargelanguage; @qu2024toollearninglargelanguage]. Finally, several works have proposed methods for learning thought tokens in an unsupervised manner [@zelikman2024quietstarlanguagemodelsteach; @goyal2024thinkspeaktraininglanguage], enabling models to more effectively utilize the additional test-time compute that comes with sampling longer sequences. While we focus our analysis on two primary mechanisms by which test-time compute can be scaled in this work (e.g. verifiers and revisions), many of the methods by which we conduct our analysis (e.g. compute optimal scaling according to question difficulty) could, in principle, also be applied to any of these other methods of scaling test-time compute, and we believe that this is an interesting direction for future research.

### Additional Revision Results {#app:additional_revision}

We plot additional results for majority selection using out PaLM 2-S\* revision model in Figure [1](#fig:revision_maj_additional){reference-type="ref" reference="fig:revision_maj_additional"}. With majority selection, we see largely similar trends to those found in Figure [\[fig:iso\_revisions\]](#fig:iso_revisions){reference-type="ref" reference="fig:iso_revisions"} for verifier selection.

![Varying the ratio of generation budget allocated to sequential verses parallel samples, using majority to select the answer, rather than the verifier. **Left:** Each line represents a fixed generation budget as the ratio is changed. We see that similar to the verifier case, in the majority case, there exists an ideal ratio of sequential to parallel test-time compute at a given budget. **Right:** Analyzing performance across difficulty bins, we see that the easier questions are mostly invariant the ratio of sequential to parallel, whereas on the harder questions there is an ideal ratio of sequential to parallel test-time compute.](figures/iso_gens_majority_difficulty.pdf){#fig:revision_maj_additional width="99%"}

### Unsupervised Difficulty Bins {#app:unsupervised_difficulty}

We compute difficulty bins without oracle ground-truth correctness information by averaging the PRM final-answer score over 2048 samples on each question, so as to obtain a value estimate corresponding to the question. We then bin the value for each question in the test-set into five quintiles (using the same procedure as the oracle difficulty bins). We refer to this as "predicted difficulty" rather than "oracle difficulty". Technically this procedure is extremely costly because it requires generating many samples. While we do not account for this cost in our analysis, in a practical production setting, this cost would be problematic. A more efficient approach would be to finetune a model to predict correctness directly, given the question. We do not explore this in our work, but leave such exploration of cheaper methods of estimating difficulty to future work.

In Figure [2](#fig:search_unsupervised_difficulty){reference-type="ref" reference="fig:search_unsupervised_difficulty"} we plot PRM-search results using our difficulty bins, and in Figure [\[fig:revisions\_unsupervised\_difficulty\]](#fig:revisions_unsupervised_difficulty){reference-type="ref" reference="fig:revisions_unsupervised_difficulty"} we plot the corresponding revision results. We see that in both settings these predicted bins demonstrate similar trends to the oracle bins.

![Using our PaLM 2-S\* PRM to compute difficulty bins without ground truth correctness information for PRM search. We see largely similar performance trends with these bins as we do with the ground truth ones in Figure [\[fig:comparing\_search\_and\_beam\_difficulty\]](#fig:comparing_search_and_beam_difficulty){reference-type="ref" reference="fig:comparing_search_and_beam_difficulty"}.](figures/search_unsup_difficulty_output.pdf){#fig:search_unsupervised_difficulty width="60%"}

### PRM Training Details {#app:prm_training}

We finetune our PRM as a binary classifier, where the model predicts a value between 0 and 1 at each step in the solution. We train the model with soft values obtained from the monte-carlo rollouts, using a binary cross entropy loss function (e.g. $-(ylog(\hat{y})+(1-y)log(1-\hat{y}))$ where $y$ corresponds to the soft ground-truth value and $\hat{y}$ the model's predicted value). We finetune the model base model using the AdamW optimizer, with lr 3e-5, batch size 128, dropout 0.05, and Adam betas $(0.9, 0.95)$. We conduct early stopping, selecting the checkpoint with the lowest validation loss on a random held-out validation set, consisting of 10% of the questions in the original PRM800k training split.

We finetune the PRM on 16 samples per question from the corresponding few-shot prompted base model. At each step, we use 16 monte-carlo rollouts, using the same base model and prompt, to estimate the step-level value. We filter out all samples which fail to output a valid, parsable final answer from the training data, as we found these to hurt PRM performance in initial experiments.

When generating the samples, the base model is prompted to output answers in newline separated a step-by-step format, as done in @lightman2023lets. We then separate each of the answers into steps using a simple newline splitting procedure. We include details about our prompt in Appendix [7](#app:prompting){reference-type="ref" reference="app:prompting"}.

### Comparing PRM Aggregation Strategies {#app:comparing_prm_agg}

We compare different methods of aggregating per-step PRM scores to produce a final score for the full solution. Specifically we compare: 1) taking the minimum score accross all steps as done in @lightman2023lets (e.g. "min"); 2) taking the product of all step correctness probabilities (e.g. "prod"); and 3) taking just the last step prediction (e.g. "last"). We see in Figure [3](#fig:comparing_prm_aggregations){reference-type="ref" reference="fig:comparing_prm_aggregations"} that taking the last step outperforms the other two approaches. Prior works [@lightman2023lets; @wang2023mathshepherd] found min to be the best aggregator. We believe that the discrepancy is due to the fact that our verifier was trained with soft MC return labels, which surface very differently from binary correctness labels, and therefore other aggregation strategies may not have the same effect.

Interestingly, when using the last step aggregation, we are effectively using the PRM like an ORM. However, we see that the PRM outperforms the ORM, suggesting that in our case the per-step PRM training may be largely useful as a form of representation learning, rather than purely as a tool at inference time. Future work should further explore this line of reasoning.

![We compare different methods of aggregating per-step PRM scores to produce a final score for the full solution: "min" refers to taking the minimum score accross all steps, "prod" takes the product of all step correctness probabilities, and "last" just uses the last step score. We see that last performs the best across all aggregation strategies.](figures/prm_agg_comparing_output.pdf){#fig:comparing_prm_aggregations width="60%"}

### Comparing PRM and ORM {#app:comparing_prm_orm}

We trained a PRM and ORM model using the PaLM 2-S\* base LM. We see in Figure [4](#fig:prm_v_orm){reference-type="ref" reference="fig:prm_v_orm"}, that the PRM outperforms the ORM, and the gap between the gap between the PRM and ORM grows with the number of samples used. We use the last step prediction from the PRM to score the answers as described in Appendix [5](#app:comparing_prm_agg){reference-type="ref" reference="app:comparing_prm_agg"}.

![We compare PRM and ORM models finetuned from PaLM 2-S\* in a best-of-N evaluation. We use the PaLM 2-S\* base LM to sample outputs, using a few-shot prompt. We see that the PRM greatly outperforms the ORM at a larg number of samples.](figures/prm_comparing_output3.pdf){#fig:prm_v_orm width="60%"}

### Prompting Details {#app:prompting}

In order to enable the base model to output answers in a step-by-step format to which a PRM can be applied, we use a 4-shot prompt consisting of randomly selected correct answer examples from the PRM800k data released by @lightman2023lets. Specifically we use answers from the phase 1 training split. These answers correspond to GPT-4 generated correct answer examples, which include the correct step-by-step format. In initial experiments, we found that this prompting procedure produces similar results to the prompt used in @lewkowycz2022solving. We use this prompt for generating training data for the PRM and the revision model. We also use this prompt when conducting search against the PRM on the test-set. To grade the final answer predicted by this prompt, we use the grading function released by @lightman2023lets.

### Revision Model Finetuning Details {#app:revision_finetune}

For fine-tuning the revision model, we follow the procedure outlined in Section [\[sec:revision\_setup\]](#sec:revision_setup){reference-type="ref" reference="sec:revision_setup"}. We first sample 64 outputs per question. We then filter out all answers which end in an invalid solution. For each correct answer, we then sample a number uniformly between 0 and 4 indicating how many incorrect answers to include in context for training. The correct answer is used as the last answer in the trajectory (which we train the model to produce) and the incorrect answers are included in context. If the sampled number is greater than 0, we then find the closest incorrect answer according to a character-level edit distance metric to include as the last incorrect answer in the trajectory. The goal here is to select an incorrect answer which is somewhat correlated with the correct answer, to improve learning. The remaining incorrect answers, we sample randomly from the set of available answers. In the case where there are fewer than 4 incorrect answers sampled, we truncate the uniform distribution's max to match the number of incorrect samples. We use this procedure to generate trajectories for all questions in the training data.

We then finetune the base language model on the correct answer solutions in these generated trajectories. We use the AdamW optimizer with lr 1e-5, batch size 128, dropout 0.0, and Adam betas $(0.9, 0.95)$.

We find that generally evaluating loss on an evaluation set consisting of trajectories generated as described above, does not provide a good signal for early stopping. Rather, we find that checkpoints much after the evaluation loss begins increasing are much more capable of revisions. This is likely because after finetuning the revision model, the evaluation set represents off-policy data, which will naturally be out-of-distribution compared to the trajectires that the model itself would generate on-policy. We therefore select our revision model checkpoint slightly after the point where we observe overfitting on the validation set.

### Revision Model Selection Criteria {#app:revision_selection}

As described in Section [\[sec:revision\_setup\]](#sec:revision_setup){reference-type="ref" reference="sec:revision_setup"}, in order to effective use our revision model we need to deploy a criteria for selecting the best answer both within a revision trajectory and between multiple parallel trajectories. We use two approaches: 1) ORM verifier; and 2) majority voting.

For the ORM verifier, we train an ORM on the revision model's outputs according to the procedure in Appendix [10](#app:revision_verifier){reference-type="ref" reference="app:revision_verifier"}. At inference, time we then use this verifier to select the best answer. Since we have two axes across which to aggregate (within each revision trajectories and between multiple trajectories), we deploy a hierarchical strategy, first selecting the best answer within each revision trajectory and then aggregating these selected answers across trajectories. To select the best answer within each trajectory, we perform best-of-N weighted aggregation and then choose the highest scoring solution with the maximum best-of-N weighted answer. Then, to select the final answer across all revision chains, we perform another round of best-of-N weighted selection using the best answer from each revision chain. The answer after this second round of best-of-N weighted represents our final answer prediction.

For majority voting we found hierarchical aggregation to create problems when the length of the trajectory or the number of trajectories was too small. The problem being that without enough samples, majority voting is unable to effectively select the best option. Therefore, for majority voting, we simply take all answers, across all trajectories, at once and take their majority as the final-answer. We found this to produce much smoother scaling behavior than the hierarchical approach.

### Revision Model Verifier Training {#app:revision_verifier}

We found that the PRM we finetuned on the PaLM 2-S\* base model outputs was not as effective when applied to the PaLM 2-S\* revision model's outputs (see Figure [\[fig:revision\_verifier\_v\_prm\]](#fig:revision_verifier_v_prm){reference-type="ref" reference="fig:revision_verifier_v_prm"}), likely due to distribution shift with the revision model. We therefore, trained a separate ORM verifier to use with our PaLM 2-S\* revision model. We could have trained a PRM as well, but opted for an ORM due to the high cost of generating per-step PRM labels.

We modified the standard ORM slightly for the revision setting, by finetuning the ORM with previous revision in context, such that the verifier has access to the same context as the revision model, allowing the verifier see the revision model's previous answer attempts when scoring the current answer. All other experiment details are identical to those used for training the PRM.

Empirically, we find that including the revision history in context improves performance slightly (see Figure [\[fig:revision\_verifier\_w\_v\_wo\_history\]](#fig:revision_verifier_w_v_wo_history){reference-type="ref" reference="fig:revision_verifier_w_v_wo_history"}). Additionally, even without the revisions in context, we see that sequential revisions still slightly outperforms parallel, demonstrating improvements from sequential sampling are not just due to the verifier's context.

### $\text{ReST}^{\text{EM}}$ Revision Model Experiments {#app:rest_revision_model}

We experimented with further optimizing our PaLM 2-S\* revision model by training the model with a simplified RL algorithm: $\text{ReST}^{\text{EM}}$ [@singh2024human]. Specifically, we generated 64 revision trajectories of maximum length 5 for each question on the MATH training set. We stopped the revision model at the first correct answer in each trajectory. Using this generated data, we then finetuned the base LM on the correct answer data. To help the model learn the task, we explicitly balanced the distribution of trajectory lengths.

In Figure [5](#fig:rest_em_revisions){reference-type="ref" reference="fig:rest_em_revisions"}, we plot the performance of this new revision model as we vary the sequential to parallel ratio. We see that additional sequential revisions substantially hurts performance with this new model. We hypothesize that this degradation is due to the fact that the online data obtained from running $\text{ReST}^{\text{EM}}$ exacerbates spurious correlations in revision data, causing the optimized model to fail to learn the revision task. We believe that using a more offline data collection strategy, as done in @qu2024recursive, may be more effective, and leave further exploration to future work.

![Performance of our $\text{ReST}^{\text{EM}}$ optimized revision model as the sequential to parallel ratio is varied. We use majority voting to select the answer. We see that this optimized revision model demonstrates substantial performance degradations with additional sequential revisions.](figures/rest_em_revision_model_iso_gens_output.pdf){#fig:rest_em_revisions width="60%"}

### Revision Model Example Outputs {#app:revision_example_outputs}

In Figures [6](#fig:revisions_ex1){reference-type="ref" reference="fig:revisions_ex1"}, [7](#fig:revisions_ex2){reference-type="ref" reference="fig:revisions_ex2"}, [8](#fig:revisions_ex3){reference-type="ref" reference="fig:revisions_ex3"}, [9](#fig:revisions_ex4){reference-type="ref" reference="fig:revisions_ex4"}, [10](#fig:revisions_ex5){reference-type="ref" reference="fig:revisions_ex5"}, [11](#fig:revisions_ex6){reference-type="ref" reference="fig:revisions_ex6"}, and [12](#fig:revisions_ex7){reference-type="ref" reference="fig:revisions_ex7"}, we include select examples of our revision model's outputs.

![Revision model example 1. The model calculates the sum at the end incorrectly on the first two attempts, but on the third attempt it succeeds and gets the answer correct.](revisions_ex1.png){#fig:revisions_ex1 width="75%"}

![Revision model example 2. On the first attempt the model takes the incorrect approach, on the second attempt it gets closer but then makes a mistake towards the end. On the final attempt it gets to the correct answer.](revisions_ex2.png){#fig:revisions_ex2 width="75%"}

![Revision model example 3. On the first attempt the model makes a mistake with the formatting of the final answer; it corrects this on the second attempt.](revisions_ex3.png){#fig:revisions_ex3 width="75%"}

![Revision model example 4. On the first few attempts the model fails the base 10 to base 8 conversion. On the final attempt it makes the correct calculation.](revisions_ex4.png){#fig:revisions_ex4 width="75%"}

![Revision model example 5. On the first two attempts the model makes an error when converting euclidean to polar coordinates. On the final attempt it does not make these mistakes.](revisions_ex5.png){#fig:revisions_ex5 width="75%"}

![Revision model example 6. On the first two attempts the model makes a mistake when summing the proper divisors of 284. On the third attempt, it evaluates this sum correctly.](revisions_ex6.png){#fig:revisions_ex6 width="75%"}

![Revision model example 7. On the first attempt the model evaluates $\frac{1}{3}+2$ incorrectly. On the second attempt it corrects this error.](revisions_ex7.png){#fig:revisions_ex7 width="75%"}

### PRM Beam Search Example Outputs {#app:prm_example_outputs}

In Figures [13](#fig:prm_ex1){reference-type="ref" reference="fig:prm_ex1"}, [14](#fig:prm_ex2){reference-type="ref" reference="fig:prm_ex2"}, [15](#fig:prm_ex3){reference-type="ref" reference="fig:prm_ex3"}, [16](#fig:prm_ex4){reference-type="ref" reference="fig:prm_ex4"}, [17](#fig:prm_ex5){reference-type="ref" reference="fig:prm_ex5"}, and [18](#fig:prm_ex6){reference-type="ref" reference="fig:prm_ex6"}, we include select examples of PRM beam search. We include the PRM score, between 0 and 1, for each step in the examples.

![PRM beam search example 1.](PRM_ex1.png){#fig:prm_ex1 width="100%"}

![PRM beam search example 2.](PRM_ex2.png){#fig:prm_ex2 width="100%"}

![PRM beam search example 3.](PRM_ex3.png){#fig:prm_ex3 width="33%"}

![PRM beam search example 4.](PRM_ex4.png){#fig:prm_ex4 width="100%"}

![PRM beam search example 5.](PRM_ex5.png){#fig:prm_ex5 width="100%"}

![PRM beam search example 6.](PRM_ex6.png){#fig:prm_ex6 width="60%"}
