---
author:
- 
bibliography:
- paper.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data"
---





# Conclusion

Recent advances in LLM mathematical reasoning have mostly been *closed-source* since instruction tuning data is often not shared or has restrictive license. In this paper we contribute towards *open-source* progress by sharing the OpenMathInstruct-2 dataset and all the code necessary to reproduce our work. Besides releasing high-performing models and data, we also conduct detailed ablations that advance our understanding of how to best construct such datasets. In summary, we show that:

1.  Not all chain-of-thought formats are equally effective, and longer solutions are not necessarily better.

2.  Performance on data generated by a strong teacher model surpasses that of equally-sized data produced by a weaker student model.

3.  Data filtering has limited utility for math reasoning datasets as models are quite robust to the presence of incorrect solutions during SFT.

4.  Training on a diverse set of questions is crucial, but proper decontamination has to be performed to ensure the benchmark evaluations accurately represent model strengths.
