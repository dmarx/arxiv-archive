\pdfoutput=1
\documentclass[10pt, logo, twocolumn, copyright, nonumbering]{nvidiatechreport}

\usepackage{hyperref}
\usepackage{url}
\usepackage{colortbl}
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage[dvipsnames]{xcolor}         %
\usepackage{multirow}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{subcaption}    %
\usepackage{float}
\usepackage{stfloats}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{mdframed}
\usepackage{tcolorbox}
\usepackage{arydshln}
\usepackage{booktabs} 
\usepackage{color,soul}
\usepackage[numbers]{natbib}

\tcbuselibrary{listings,breakable}

\definecolor{pastelblue}{RGB}{173,216,230}
\definecolor{pastelyellow}{RGB}{255,253,208}
\definecolor{pastelpink}{RGB}{255,209,220}
\definecolor{pastelgreen}{RGB}{176,226,172}
\definecolor{pastellavender}{RGB}{230,230,250}


\definecolor{NvidiaGreen}{RGB}{118, 185, 0}
\sethlcolor{red!15}


\newcommand{\dataset}[0]{OpenMathInstruct-2\xspace}
\newcommand{\datasize}[0]{14M\xspace}
\newcommand{\uniqquesns}[0]{600K\xspace}

\newcommand{\IG}[1]{{\color{green} [{\bf IG:} [comment] #1]}}
\newcommand{\WD}[1]{{\color{blue} [{\bf WD:} #1]}}
\newcommand{\IM}[1]{{\color{red} [{\bf IM:} #1]}}
\newcommand{\BK}[1]{{\color{brown} [{\bf BK:} #1]}}
\newcommand{\ST}[1]{{\color{purple} [{\bf ST:} #1]}}





\title{\dataset: Accelerating AI for Math with Massive Open-Source Instruction Data} 

\author{
\centering {Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin \hspace{1.8in}Alexan Ayrapetyan, Igor Gitman
}
}






\begin{abstract}
\textbf{Abstract:} 

Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest.  
However, most of the cutting-edge progress in mathematical reasoning with LLMs has become \emph{closed-source} due to lack of access to training data. 
This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. 
With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released \texttt{Llama3.1} family of models. 
Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms equally-sized data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains.   
Based on these insights, we create the \dataset dataset which consists of 14M question-solution pairs ($\approx$ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. 
Finetuning the \texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms \texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\% (51.9\% $\rightarrow$ 67.8\%). 
Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license.
\end{abstract}

\begin{document}

\maketitle


\input{tex/1_introduction}
\input{tex/2_solution_aug}
\input{tex/3_question_augmentation}
\input{tex/4_results}
\input{tex/5_related_work}

\section{Conclusion}
Recent advances in LLM mathematical reasoning have mostly been \emph{closed-source} since instruction tuning data is often not shared or has restrictive license. In this paper we contribute towards \emph{open-source} progress by sharing the \dataset dataset and all the code necessary to reproduce our work. Besides releasing high-performing models and data, we also conduct detailed ablations that advance our understanding of how to best construct such datasets. In summary, we show that:  
\begin{enumerate}[label=\alph*)]
    \setlength{\itemsep}{0pt} %
    \item Not all chain-of-thought formats are equally effective, and longer solutions are not necessarily better.
    \item Performance on data generated by a strong teacher model surpasses that of equally-sized data produced by a weaker student model.
    \item Data filtering has limited utility for math reasoning datasets as models are quite robust to the presence of incorrect solutions during SFT.
    \item Training on a diverse set of questions is crucial, but proper decontamination has to be performed to ensure the benchmark evaluations accurately represent model strengths.
\end{enumerate}






\bibliographystyle{plainnat}
\bibliography{paper}


\clearpage
\appendix

\input{tex/6_appendix}
\input{tex/7_appendix_few_shot}



\end{document}
