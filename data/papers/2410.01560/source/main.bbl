\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{AIME 2024}(2024)]{aime24}
{AIME 2024}.
\newblock \url{https://artofproblemsolving.com/wiki/index.php/2024_AIME_I}, 2024.

\bibitem[Aiyappa et~al.(2023)Aiyappa, An, Kwak, and Ahn]{aiyappa-etal-2023-trust}
Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-yeol Ahn.
\newblock {Can we trust the evaluation on {C}hat{GPT}?}
\newblock In \emph{3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)}, 2023.

\bibitem[{AMC 2023}(2023)]{AMC23}
{AMC 2023}.
\newblock \url{https://github.com/QwenLM/Qwen2.5-Math/blob/main/evaluation/data/amc23/test.jsonl}, 2023.

\bibitem[Azerbayev et~al.(2024)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2024llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen~Marcus McAleer, Albert~Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
\newblock {Llemma: An Open Language Model for Mathematics}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Bansal et~al.(2024)Bansal, Hosseini, Agarwal, Tran, and Kazemi]{bansal2024smallerweakerbettertraining}
Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh~Q. Tran, and Mehran Kazemi.
\newblock {Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling}, 2024.

\bibitem[Brown et~al.(2024)Brown, Juravsky, Ehrlich, Clark, Le, Ré, and Mirhoseini]{brown2024largelanguagemonkeysscaling}
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc~V. Le, Christopher Ré, and Azalia Mirhoseini.
\newblock {Large Language Monkeys: Scaling Inference Compute with Repeated Sampling}, 2024.

\bibitem[Chan et~al.(2024)Chan, Wang, Yu, Mi, and Yu]{chan2024scalingsyntheticdatacreation}
Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu.
\newblock {Scaling Synthetic Data Creation with 1,000,000,000 Personas}, 2024.

\bibitem[Chen et~al.(2023)Chen, Ma, Wang, and Cohen]{DBLP:journals/tmlr/ChenM0C23}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W. Cohen.
\newblock {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}.
\newblock \emph{TMLR}, 2023.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock {Training Verifiers to Solve Math Word Problems}.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[DeepSeek-AI(2024{\natexlab{a}})]{deepseekai2024deepseekcoderv2breakingbarrierclosedsource}
DeepSeek-AI.
\newblock {DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence}, 2024{\natexlab{a}}.

\bibitem[DeepSeek-AI(2024{\natexlab{b}})]{deepseekai2024deepseekv2strongeconomicalefficient}
DeepSeek-AI.
\newblock {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 2024{\natexlab{b}}.

\bibitem[Gou et~al.(2024)Gou, Shao, Gong, Shen, Yang, Huang, Duan, and Chen]{gou2024toratoolintegratedreasoningagent}
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen.
\newblock {ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021measuringmathematicalproblemsolving}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock {Measuring Mathematical Problem Solving With the MATH Dataset}.
\newblock In \emph{NeurIPS Datasets and Benchmarks}, 2021.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and Choi]{Holtzman2020The}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock {The Curious Case of Neural Text Degeneration}.
\newblock In \emph{ICLR}, 2020.

\bibitem[Jain et~al.(2024)Jain, Zhang, Chiang, Gonzalez, Sen, and Stoica]{jain2024llmassisted}
Naman Jain, Tianjun Zhang, Wei-Lin Chiang, Joseph~E. Gonzalez, Koushik Sen, and Ion Stoica.
\newblock {LLM-Assisted Code Cleaning For Training Accurate Code Generators}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Wang, Hu, Wei, Zheng, Hu, Zhang, and Peng]{li2024common}
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng.
\newblock {Common 7B Language Models Already Possess Strong Math Capabilities}.
\newblock \emph{arXiv preprint arXiv:2403.04706}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Wang, Hu, Wei, Zheng, Hu, Zhang, and Peng]{li2024common7blanguagemodels}
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng.
\newblock {Common 7B Language Models Already Possess Strong Math Capabilities}, 2024{\natexlab{b}}.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Yuan, Yuan, Dong, Lu, Wu, Tan, Wang, and Zhou]{li2024mugglemath}
Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou.
\newblock {MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning}.
\newblock In \emph{ACL}, 2024{\natexlab{c}}.

\bibitem[Li et~al.(2024{\natexlab{d}})Li, Beeching, Tunstall, Lipkin, Soletskyi, Huang, Rasul, Yu, Jiang, Shen, et~al.]{li2024numinamath}
Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert~Q Jiang, Ziju Shen, et~al.
\newblock {NuminaMath: The largest public dataset in AI4Maths with 860k pairs of competition math problems and solutions}, 2024{\natexlab{d}}.

\bibitem[Loshchilov and Hutter(2019)]{Loshchilov2019DecoupledWD}
Ilya Loshchilov and Frank Hutter.
\newblock {Decoupled Weight Decay Regularization}.
\newblock In \emph{ICLR}, 2019.

\bibitem[Meta-AI(2024)]{dubey2024llama3herdmodels}
Meta-AI.
\newblock {The Llama 3 Herd of Models}, 2024.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min-etal-2022-rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}
\newblock In \emph{EMNLP}, 2022.

\bibitem[{Mistral AI}(2024)]{mathstral}
{Mistral AI}.
\newblock \url{https://mistral.ai/news/mathstral/}, 2024.

\bibitem[{NVIDIA}(2024)]{nvidia2024nemotron4340btechnicalreport}
{NVIDIA}.
\newblock {Nemotron-4 340B Technical Report}, 2024.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{nye2021workscratchpadsintermediatecomputation}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena.
\newblock {Show Your Work: Scratchpads for Intermediate Computation with Language Models}, 2021.

\bibitem[{Omni-Math}(2024)]{omni_math}
{Omni-Math}.
\newblock \url{https://omni-math.github.io/}, 2024.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.
\newblock URL \url{https://openai.com/research/gpt-4}.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, Hofmann, Jha, Kumar, Lucy, Lyu, Lambert, Magnusson, Morrison, Muennighoff, Naik, Nam, Peters, Ravichander, Richardson, Shen, Strubell, Subramani, Tafjord, Walsh, Zettlemoyer, Smith, Hajishirzi, Beltagy, Groeneveld, Dodge, and Lo]{soldaini-etal-2024-dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li~Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz~Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.
\newblock {Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}.
\newblock In \emph{ACL}, 2024.

\bibitem[Sprague et~al.(2024)Sprague, Yin, Rodriguez, Jiang, Wadhwa, Singhal, Zhao, Ye, Mahowald, and Durrett]{sprague2024cotcotchainofthoughthelps}
Zayne Sprague, Fangcong Yin, Juan~Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi~Ye, Kyle Mahowald, and Greg Durrett.
\newblock {To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning}, 2024.

\bibitem[Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia, and Gitman]{toshniwal2024openmathinstruct}
Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman.
\newblock {OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset}.
\newblock In \emph{NeurIPS Datasets and Benchmarks}, 2024.

\bibitem[Villalobos and Atkinson(2023)]{epoch2023tradingoffcomputeintrainingandinference}
Pablo Villalobos and David Atkinson.
\newblock {Trading Off Compute in Training and Inference}, 2023.
\newblock URL \url{https://epochai.org/blog/trading-off-compute-in-training-and-inference}.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{DBLP:conf/iclr/0002WSLCNCZ23}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V. Le, Ed~H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock {Self-Consistency Improves Chain of Thought Reasoning in Language Models}.
\newblock In \emph{ICLR}, 2023.

\bibitem[Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang, Sreedhar, and Kuchaiev]{wang2024helpsteer2}
Zhilin Wang, Yi~Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy~J. Zhang, Makesh~Narsimhan Sreedhar, and Oleksii Kuchaiev.
\newblock {HelpSteer2: Open-source dataset for training top-performing reward models}, 2024.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Yang et~al.(2024)Yang, Zhang, Hui, Gao, Yu, Li, Liu, Tu, Zhou, Lin, Lu, Xue, Lin, Liu, Ren, and Zhang]{yang2024qwen25mathtechnicalreportmathematical}
An~Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang.
\newblock {Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement}, 2024.

\bibitem[Yang et~al.(2023)Yang, Chiang, Zheng, Gonzalez, and Stoica]{yang2023rethinkingbenchmarkcontaminationlanguage}
Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph~E. Gonzalez, and Ion Stoica.
\newblock {Rethinking Benchmark and Contamination for Language Models with Rephrased Samples}, 2023.

\bibitem[Yu et~al.(2024)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu]{yu2024metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock {MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Yue et~al.(2024{\natexlab{a}})Yue, Qu, Zhang, Fu, Huang, Sun, Su, and Chen]{yue2024mammoth}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock {MA}mmo{TH}: Building math generalist models through hybrid instruction tuning.
\newblock In \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Yue et~al.(2024{\natexlab{b}})Yue, Zheng, Zhang, and Chen]{yue2024mammoth2}
Xiang Yue, Tuney Zheng, Ge~Zhang, and Wenhu Chen.
\newblock {MAmmoTH2: Scaling Instructions from the Web}.
\newblock \emph{arXiv preprint arXiv:2405.03548}, 2024{\natexlab{b}}.

\bibitem[Zeng et~al.(2024)Zeng, Zhong, Zhao, Wei, Yang, He, Cheng, Hu, Liu, Yan, Fang, and Zhou]{zeng2024skyworkmathdatascalinglaws}
Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, Han Fang, and Yahui Zhou.
\newblock {Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On}, 2024.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Yan, Li, and Liu]{zhang2024infinitymath}
Bo-Wen Zhang, Yan Yan, Lin Li, and Guang Liu.
\newblock {InfinityMATH: A Scalable Instruction Tuning Dataset in Programmatic Mathematical Reasoning}.
\newblock \emph{arXiv preprint arXiv:2408.07089}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Da, Lee, Robinson, Wu, Song, Zhao, Raja, Slack, Lyu, Hendryx, Kaplan, Lunati, and Yue]{zhang2024carefulexaminationlargelanguage}
Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue.
\newblock {A Careful Examination of Large Language Model Performance on Grade School Arithmetic}, 2024{\natexlab{b}}.

\end{thebibliography}
