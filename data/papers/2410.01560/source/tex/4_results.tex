\section{Results}

\input{table/result_table}





\paragraph{Training Details.}
All the models are trained with a batch size of 512, using the AdamW optimizer~\citep{Loshchilov2019DecoupledWD} with a constant learning rate of 2e-5 and a weight decay of 1e-2. 
For the 8B model, we train the model on 1M, 2M, and 5M fair downsampled versions of \dataset to understand the impact of the data scaling.  
Due to computational constraints, we train the 70B model only on the 5M subset with a learning rate of 1e-5. 
The models are trained for 2 epochs, and we save 6 equally spaced checkpoints during the training runs, which are averaged to create the final model (See Appendix~\ref{sec:app_ckpt_avging} for performance gains with checkpoint averaging). 


\paragraph{Evaluation Details.} 
We evaluate our models on a set of common benchmarks that consists of GSM8K (1.3K examples), MATH (5K examples), AMC 2023 (40 examples), AIME 2024 (30 examples), and Omni-MATH (4.4K examples) \citep{omni_math}. These datasets cover a broad spectrum of difficulty levels, ranging from grade school mathematics to advanced competition problems. Unless noted otherwise, all fine-tuned models are assessed in a zero-shot setting with both greedy decoding and majority voting out of 256 sampled solutions with temperature of 0.7 \citep{wang2022self}.

We use GPT-4o \citep{openai2023gpt4} as a judge to compare the ground truth answers with those predicted by our models (the detailed prompt is provided in Appendix \ref{sec:llm-evaluation-judge}). 


\footnotetext{Omni-MATH dataset was released after we finished training our models, so we didn't use it during decontamination. After checking for contamination, we found that about 1.4\% of the test set questions are part of our training data.}





\paragraph{Impact of Data Scaling.} 
Figure~\ref{fig:sft_scale} plots the performance on the MATH test set with the increase in SFT data size. With even the 1M fair-downsampled version of OpenMathInstruct-2, the final model easily outperforms \texttt{Llama3.1-8B-Instruct} and \texttt{NuminaMath-7B-CoT}.  
We observe a consistent gain with an increase in data size, and even at 14M dataset size, we see no signs of saturation in performance gains.

\paragraph{Final Results.} Table~\ref{tab:main_results} presents the results for top-performing, open-weight and open-source models (without tool use). 
The \texttt{OpenMath2-Llama3.1-8B} model, which is finetuned on the full OpenMathInstruct-2 dataset, outperforms or matches \texttt{Llama3.1-8B-Instruct} on all the math reasoning benchmarks.  
Among the open-source models, we outperform the recently released \texttt{NuminaMath-7B-CoT} on all benchmarks as well. 
Finally, among all the presented models, the \texttt{OpenMath2-Llama3.1-8B} is second only to the \texttt{Qwen2.5-Math-7B-Instruct}, which has been trained on more than a trillion synthetically generated math reasoning tokens, and starts with a base model, \texttt{Qwen2.5-Math}, which is about 35\% better than \texttt{Llama3.1-8B-Base}.
\footnote{We are unsure of the $n$-gram based data contamination protocol followed by \texttt{Qwen2.5-Math} given its obvious weakness in detecting paraphrases.}



The \texttt{OpenMath2-Llama3.1-70B} is our strongest performing model which is the \texttt{Llama3.1-70B-Base} model finetuned on the 5M fair downsampled subset of OpenMathInstruct-2. While our 8B model demonstrates strong accuracy gains compared to other LLMs of similar size, the 70B model only shows improvements on a subset of benchmarks. We hypothesize that our data blend or solution format might be more suited for weaker models, since we made all of the design decisions based on the 8B model accuracy on validation subsets.

