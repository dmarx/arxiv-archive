\subsubsection{Solution Format}


Finetuning with synthetic chain-of-thought (CoT) solutions~\citep{nye2021workscratchpadsintermediatecomputation, wei2022chain, sprague2024cotcotchainofthoughthelps} has been the key to strong performances of small models on math reasoning tasks~\citep{yu2024metamath, toshniwal2024openmathinstruct, dubey2024llama3herdmodels}. 
We find the Llama's CoT format to be quite verbose,\footnote{\url{https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals/viewer/Meta-Llama-3.1-8B-Instruct-evals__math__details}} and propose an alternate CoT format, \emph{OpenMath CoT}, which is detailed as well but less verbose. 
Figure~\ref{fig:soln_format} shows a sample solution in the two CoT formats. 

\input{table/soln_format_table}


To compare the two CoT formats, we generate SFT data using the \texttt{Llama3.1-405B-Instruct} model. 
For generating solutions in the Llama CoT format we simply use the zero-shot prompt setup as the model was trained on those kinds of solutions. 
However, even when prompting the model with few-shot OpenMath CoT solutions, a substantial number of generations -- 57\% in our experiment -- still follow the Llama CoT format. 
This tendency of \emph{aligned} models reverting to their trained behavior when encountering inputs seen during training has also been observed in prior work~\citep{min-etal-2022-rethinking}. 
We find an interesting workaround to this issue by dropping the special tokens used by Llama-Instruct models. 
Prompting the model with the ``base'' template leads to a dramatic increase in adherence to the OpenMath CoT format and reduces the Llama CoT format generations to only 0.1\%. See Appendix~\ref{sec:app_soln_format} for the prompt and more details. 







With 64 solutions sampled per question, the zero-shot setup results in about 30\% more solutions than the few-shot prompt setup (350K vs 268K). 
To control for the confounding factor of SFT data size, we perform the Matching Coverage operation over the two datasets which reduces the final SFT dataset to 260K question-solution pairs. 
Table~\ref{tab:soln_format} shows that the OpenMath CoT format is 40\% less verbose than the Llama CoT format and also results in a better SFT performance. 
All experiments presented henceforth use the OpenMath CoT format. 

    
    
    
    







