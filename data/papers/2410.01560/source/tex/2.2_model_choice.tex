\subsubsection{Choice of Teacher Model}
\label{sec:teacher_model}


Prior work has shown that with repeated sampling, even weak models can match or outperform much stronger/bigger models~\citep{li2024common7blanguagemodels, brown2024largelanguagemonkeysscaling}. 
In fact, for a fixed compute budget, a weaker model can be a better choice for a teacher model~\citep{bansal2024smallerweakerbettertraining}. 
But data synthesis is a one-time expense and a small portion of the overall compute budget of training LLMs~\citep{epoch2023tradingoffcomputeintrainingandinference}. 
We instead ask the following question: 
\emph{Can a student model learn better from its own generated solutions vs solutions generated by a strong teacher model when matching the SFT data coverage?}
\input{table/teacher_model}

In this ablation, we compare \texttt{Llama3.1-8B-Base} and \texttt{Llama3.1-405B-Instruct} as teacher models.  
We sample solutions using the two models and perform the Matching Coverage operation to match the final SFT datasets precisely. 
The SFT results presented in Table~\ref{tab:teacher_model}  show that even when controlling for the SFT data size, \texttt{Llama3.1-405B-Instruct} is a far superior data generation model. 
Our preliminary analysis suggests that the reason is weaker models generate more  \emph{noisy solutions} that use incorrect reasoning yet end up with the right answer and, ultimately, part of the SFT dataset (Appendix~\ref{sec:app_noisy_solutions}).  
We leave a more detailed analysis regarding this for future work.  
Next, we investigate the impact of these \emph{noisy solutions} among solutions generated by \texttt{Llama3.1-405B-Instruct}. 



