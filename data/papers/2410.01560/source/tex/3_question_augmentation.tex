\section{Data: Question-Solution Augmentation}

In this section, we describe the Question-Solution Augmentation component of the OpenMathInstruct-2 construction pipeline, illustrated in Figure~\ref{fig:data_overview}. This process consists of two stages: (i) question augmentation, and (ii) solution augmentation.


For question augmentation, we utilize the training splits of MATH and GSM8K as seed datasets to generate new questions. We use simple few-shot prompting showing 5 examples of original questions and the new questions written by us that are similar in some aspect. We do not add explicit instructions to increase difficulty or add new conditions, instead relying on the inherent variance of the nucleus sampling that we use to generate new problems.
After filtering out syntactically ill-formed questions, we check the generated questions for potential contamination with test sets of evaluation benchmarks, described in detail in the next section. 
To generate solutions for the new synthesized questions, we use the solution augmentation pipeline from Section~\ref{sec:soln_augmentation}, generating 32 solutions for each question with a temperature of 0.7. Since the newly synthesized questions don't have ground-truth answers to filter solutions, we instead use majority voting among the 32 generations as a proxy for the ground-truth answer.
For more details on question-solution augmentation, see Appendix \ref{sec:app_ques_soln_aug}. 






\subsection{LLM Decontamination}
\label{sec:llm_decontamination}

It has been noted that many widely used benchmarks and datasets suffer from data contamination, where information from the test set unintentionally leaks into the training data~\citep{yang2023rethinkingbenchmarkcontaminationlanguage}. 
This can result in an overly optimistic assessment of the model's performance. 
The most commonly used methods, such as $n$-gram overlap and embedding similarity search, are susceptible to simple variations in test data (e.g., paraphrasing, translation), allowing rephrased samples to bypass these basic detection techniques easily. 

We adopt the approach suggested by~\citet{yang2023rethinkingbenchmarkcontaminationlanguage} to 
remove all potential paraphrases of evaluation benchmark questions from the synthesized questions. 
In our setup, we use the test sets of four evaluation benchmarks, namely GSM8K~\citep{cobbe2021gsm8k}, MATH~\citep{hendrycks2021measuringmathematicalproblemsolving}, AMC 2023~\citep{AMC23}, and AIME 2024~\citep{aime24}. 

The LLM-based decontamination process consists of two main steps. 
First, for each synthesized question, use embedding similarity search to identify the top-$k$ most similar test examples from all benchmark datasets. 
Second, create question pairs by matching the synthesized question with each of these top-$k$ test examples. 
An advanced LLM then evaluates whether any of these pairs are paraphrases via zero-shot prompting.  
To mitigate any positional bias, we generate two pairs for each match: one in which the synthesized question appears first and another in which the test set question is presented first. 
If any of the $2k$ pair is determined to be a paraphrase, the synthesized question is removed.

We use a popular \emph{Sentence Transformer} model for embedding,\footnote{https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1} and \texttt{Llama3.1-405B-Instruct} for paraphrase detection (details on the prompt are provided in Appendix  \ref{sec:LLM-decontamination}). 
In our experiment, we use $k = 5$, which results in $10$ LLM inference calls for each generated question. 
To emphasize the importance of using an LLM in the decontamination pipeline, we provide multiple examples of questions flagged as contaminated that cannot be found via $n$-gram matching (see Table~\ref{tab:ngram_misses} in the Appendix). 
Overall, our decontamination pipeline removes about 50K questions out of the 569K new questions synthesized (569K $\longrightarrow$ 519K). 





