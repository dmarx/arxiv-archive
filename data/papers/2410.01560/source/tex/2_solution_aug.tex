
% \vspace{-0.14in}
\section{Data: Solution Augmentation}
\label{sec:data-solution-augmentation}



In this section, we focus on the \emph{Solution Augmentation} part of the \dataset construction pipeline, shown in Figure~\ref{fig:data_overview}. 
We first give a brief overview of how solutions are synthesized for existing questions, and then present ablation studies designed to understand the impact of the different dataset design choices.  

\subsection{Solution Augmentation Preliminaries}
\label{sec:soln_augmentation}
Let $\mathcal{X} = \{\left(q_i, a_i\right)\}_{i=1}^N$ represent a 
typical mathematical reasoning dataset, where $q_i$ and $a_i$ denote the $i^\text{th}$ question and answer respectively. 
To synthesize solutions for this dataset, a \emph{teacher} LLM $\mathcal{M}$ is prompted as follows: 
$$\mathcal{I}\ \left(q_1, s_1\right), \dots, \left(q_K, s_K\right), q'$$ where $\mathcal{I}$ represents the instruction to answer the given math question, $\{q_1, \dots, q_K\}$ represent $K$ questions  representative of the dataset, $\{s_1, \dots, s_K\}$ represent their respective solutions, and $q'$ represents a question from the training set.
Given this prompt, multiple candidate solutions 
are sampled using $\mathcal{M}$. 
The high-quality solutions, usually those that lead to the correct answer, along with the prompt question $q'$, are added to the SFT dataset.



\input{figures/solution_format}

\subsection{Ablation Studies}
In the previous section, we gave an abstract overview of the solution augmentation pipeline. 
In practice, several design decisions impact the final SFT dataset, such as the solution format of the few-shot examples $\{s_1, \dots, s_K\}$, the choice of the teacher model $\mathcal{M}$, and the solution filtering mechanism. 
In this section, we study the impact of these different design choices on the SFT performance to guide the dataset construction. 

For these ablation experiments, we use the 1K validation split created from MATH~\citep{hendrycks2021measuringmathematicalproblemsolving} training set by \citet{toshniwal2024openmathinstruct}. The remaining 6.5K MATH training set problems are used to create the SFT dataset. 
The solutions are generated using nucleus sampling~\citep{Holtzman2020The} with a temperature of 1.0 and top-$p$ of 0.95.  
The \texttt{Llama3.1-8B-Base} model is used as the \emph{student} model in all the ablation experiments. 
For SFT, the model is trained for 4 epochs, with a batch size of 256, using the AdamW optimizer~\citep{Loshchilov2019DecoupledWD} with a constant learning rate of 5e-6 and a weight decay of 1e-2. 
To account for the variance in performance across runs, we report the performance averaged across 4 runs.   



\paragraph{Data Downsampling}
For efficiency or experiment design reasons, we sometimes need to downsize an SFT dataset to a specific size or to match another SFT dataset in ablation experiments. We introduce the concept of \emph{coverage} and the two downsampling operations used in the paper.

\emph{Coverage} of a SFT dataset $\mathcal{D}=\{\left(q_i, s_i\right)\}_{i=1}^{T}$ synthesized using dataset $\mathcal{X} = \{\left(q_i, a_i\right)\}_{i=1}^N$ is the fraction of questions in $\mathcal{X}$ with at least one solution in $\mathcal{D}$:
\[ \text{Coverage(}\mathcal{D}, \mathcal{X}\text{)} = \frac{|\{q : \left(q, s\right) \in \mathcal{D}\}|}{|\{q : \left(q, a\right) \in \mathcal{X}\}|} \]


\emph{Fair Downsampling} is a question-dependent downsampling method introduced by~\citet{toshniwal2024openmathinstruct}. 
Due to the varying difficulty of questions, the representation of ``easier'' ones can often dominate an SFT dataset, as generating high-quality solutions for them is ``easier''.  
The goal of \emph{fair} downsampling is to sample question-solution pairs from the original SFT dataset in a way that ensures all questions are as equally represented in the downsampled dataset as possible. 

\emph{Matching Coverage}:
The different design choices explored in the ablation studies result in SFT datasets of varying sizes. 
However, to compare the quality of the datasets, we want to control for the dataset size. 
To this end, we introduce the \emph{Matching Coverage} operation, where SFT datasets are matched at the level of questions. Put simply, after matching coverage, the number of unique questions as well as the number of solutions for each individual question in two dataset is the same.

Formally, suppose we're given two SFT datasets $\mathcal{D}_1$ and $\mathcal{D}_2$. 
Let $Q\left(\mathcal{D}_1\right)$ represent the set of unique questions in $\mathcal{D}_1$:
\[Q\left(\mathcal{D}_1\right) = \{q \mid \left(q, s_1\right) \in \mathcal{D}_1\}\]
The set of common questions in $\mathcal{D}_1$ and $\mathcal{D}_2$ is given by:
\[Q_{\text{match}} = Q\left(\mathcal{D}_1\right) \cap  Q\left(\mathcal{D}_2\right)\]
Let $N\left(\mathcal{D}, q\right)$ represent the number of solutions of question $q$ in dataset $\mathcal{D}$. 
In the matching coverage version of the datasets:
\[
N_\text{match}\left(q\right) = \min\left(N\left(\mathcal{D}_1, q\right), N\left(\mathcal{D}_2, q\right)\right)
\]
for each question $q \in Q_\text{match}$, $N_\text{match}\left(q\right)$ solutions are sampled from the respective datasets.  




This covers the two downsampling methods used in this paper: \emph{Fair Downsampling} and \emph{Matching Coverage}. 
Next, we will describe the ablation experiments.






\input{tex/2.1_soln_format}
\input{tex/2.2_model_choice}
\input{tex/2.3_noise}
\input{tex/2.4_question_diversity}


 



