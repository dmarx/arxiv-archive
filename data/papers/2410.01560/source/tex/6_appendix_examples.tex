




\section{Performance Comparison between Different Teacher Models}
\input{table/soln_filtering}

\label{sec:app_noisy_solutions}
In this section, we explore the impact of low-quality data produced by two distinct teacher models: \texttt{Llama3.1-8B-Base} and \texttt{Llama3.1-405B-Instruct}. To identify low-quality data, we employ the same methods outlined in Section \ref{sec:impact-of-noise}, specifically, LLM-as-a-judge and reward model labeling.


\input{table/low_quality_solns_8b}
\input{table/low_quality_solns_405b}


For the teacher model \texttt{Llama3.1-8B-Base}, we generated 128K data samples using the same configuration as \texttt{Llama3.1-405B-Instruct}, with the MATH dataset serving as the seed. We ensured that all solutions produced led to the correct final answer, and restricted the maximum token length of generated solutions to 1024. Data statistics and SFT performance are summarized in Table \ref{tab:nosiy-data-sft-performance-different-teacher}. 

The percentage of low-quality data generated by the \texttt{Llama3.1-8B-Base} teacher model, when applying different filtering strategies, ranged from 45\% to 67\%. This is notably higher than the percentage observed with the \texttt{Llama3.1-405B-Instruct} model, as expected. More advanced teacher models, like \texttt{Llama3.1-405B-Instruct}, generally produce higher-quality data.

The SFT performance of the student model \texttt{Llama3.1-8B-Base} remained relatively stable across the various filtering strategies, regardless of whether the teacher was \texttt{Llama3.1-8B-Base} or \texttt{Llama3.1-405B-Instruct}. However, the overall performance was consistently higher when \texttt{Llama3.1-405B-Instruct} was used as the teacher. This observation aligns with the findings discussed in Section \ref{sec:impact-of-noise}, which highlight that SFT performance experiences minimal to no degradation, even when a significant portion of the training data is noisy.

Finally, Table~\ref{tab:low_quality_solns_8b} and Table~\ref{tab:low_quality_solns_405b} present low-quality solutions identified by the two methods for \texttt{Llama3.1-8B-Base} and \texttt{Llama3.1-405B-Instruct} respectively. 















% \newpage


\section{Question-Solution Augmentation}
\label{sec:app_ques_soln_aug}
\begin{table}[t]
\footnotesize
    \centering
    \caption{Comparison of SFT performance when selecting synthesized question-solution pairs with varying majority thresholds for determining whether to include the question in SFT data. }
    \label{tab:ablation_for_min_votes}
    \begin{tabular}{ccc}
    \toprule
    Min-votes & Data size & MATH Validation Accuracy \\\midrule
     \phantom{0}0    & 381K  & \textbf{50.1} \\
     \phantom{0}8    & 339K  & 49.2 \\
     16              & 254K  & 44.4 \\
     24              & 160K  & 42.0 \\\bottomrule
    \end{tabular}
\end{table}


\input{table/ngram_misses}

\input{table/similar_examples}

\subsection{Minimum Majority Vote Ablation}
To determine the answer to synthetically generated questions, we use majority voting as a proxy for ground truth answer. 
We conduct an ablation study to determine the threshold for a minimum number of majority votes. 
The questions for which the number of majority vote solutions is less than the threshold are removed. 
We generate 32 solutions per question for a small set of initial synthesized questions (after performing decontamination with MATH validation subset) and perform a comparison of varying the majority vote threshold from \{0, 8, 16, 24\}. 
Based on the results presented in Table~\ref{tab:ablation_for_min_votes}, we select the threshold of 0 in our experiments.   









\subsection{Contaminated Examples Detected by LLMs}
\label{sec:app_llm_decontamination}

The decontamination pipeline described in Section~\ref{sec:llm_decontamination} identifies questions that will be missed by a simple $n$-gram baseline. Using it we have effectively filtered out approximately 50K questions from the 569K newly synthesized questions, reducing the total from 569K to 519K.

We show two such examples in Table~\ref{tab:ngram_misses}. 
Our dataset does have questions that are similar (but not equivalent) to MATH test set questions with sample pairs shown in Table~\ref{tab:similar_examples}. 






