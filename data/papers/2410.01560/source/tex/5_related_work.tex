\section{Related Work}



In recent years, significant progress has been made in developing datasets to enhance mathematical reasoning abilities of LLMs. 
NuminaMath \citep{li2024numinamath} contains a collection of 860K pairs of competition-level math problems and solutions, annotated with chain-of-thought traces~\citep{DBLP:conf/iclr/0002WSLCNCZ23}. 
Skywork-MathQA \citep{zeng2024skyworkmathdatascalinglaws} collects 2.5M question-solution pairs, incorporating three different augmentation techniques and a diverse seed problem set. MuggleMath \citep{li2024mugglemath} is created by complicating and diversifying queries, as well as sampling multiple reasoning paths from existing datasets.  MetaMathQA \citep{yu2024metamath} introduced a dataset with 395K entries created by bootstrapping questions from MATH and GSM8K, employing techniques such as semantic rephrasing, self-verification, and backward reasoning. MAmmoTH2 \citep{yue2024mammoth2} introduced a paradigm for efficiently extracting 10 million naturally occurring instruction data points from pre-training web corpora, enhancing LLM reasoning and improving benchmark performance without the need for in-domain training. \citet{li2024common} expanded the MATH dataset to 480K and the GSM8K dataset to 960K by generating both questions and CoT-based solutions, resulting in significant accuracy improvements for fine-tuned models.

Tool-integrated methods for math problem-solving have also become prevalent. \citet{DBLP:journals/tmlr/ChenM0C23} pioneered the Program of Thoughts (PoT) approach, combining text and programming language statements to arrive at solutions. 
Building on similar concepts, other datasets have been developed. For instance, OpenMathInstruct-1 \citep{toshniwal2024openmathinstruct} introduced a math instruction tuning dataset of 1.8 million examples, synthesizing code-interpreter solutions for GSM8K and MATH benchmarks. InfinityMATH  \citep{zhang2024infinitymath} developed a scalable instruction tuning dataset for programmatic mathematical reasoning, consisting of 100K data points.

Similar to prior work, we also leverage CoT-based solutions and question augmentation to construct a novel dataset. Yet our approach distinguishes itself in several important ways: (a) we leverage open-weight models instead of proprietary closed-source LLMs allowing us to release the dataset under a permissive license; (b) we offer novel insights into the impact of low-quality data, and the design of solution format; (c) we ensure our results are accurate by performing a comprehensive decontamination process using an LLM-based pipeline that can detect rephrased variations of test set questions.
























 









