@misc{shen2024nemoalignerscalabletoolkitefficient,
      title={NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment}, 
      author={Gerald Shen and Zhilin Wang and Olivier Delalleau and Jiaqi Zeng and Yi Dong and Daniel Egert and Shengyang Sun and Jimmy Zhang and Sahil Jain and Ali Taghibakhshi and Markel Sanz Ausin and Ashwath Aithal and Oleksii Kuchaiev},
      year={2024},
      eprint={2405.01481},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{dubey2024llama3herdmodels,
      title={{The Llama 3 Herd of Models}}, 
      author={Meta-AI},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}


@inproceedings{aiyappa-etal-2023-trust,
    title = "{Can we trust the evaluation on {C}hat{GPT}?}",
    author = "Aiyappa, Rachith  and
      An, Jisun  and
      Kwak, Haewoon  and
      Ahn, Yong-yeol",
    booktitle = "3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)",
    year = "2023",
}


@misc{yang2023rethinkingbenchmarkcontaminationlanguage,
      title={{Rethinking Benchmark and Contamination for Language Models with Rephrased Samples}}, 
      author={Shuo Yang and Wei-Lin Chiang and Lianmin Zheng and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2311.04850},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{nvidia2024nemotron4340btechnicalreport,
      title={{Nemotron-4 340B Technical Report}}, 
      author={{NVIDIA}},
      year={2024},
      eprint={2406.11704},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{zeng2024skyworkmathdatascalinglaws,
      title={{Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On}}, 
      author={Liang Zeng and Liangjun Zhong and Liang Zhao and Tianwen Wei and Liu Yang and Jujie He and Cheng Cheng and Rui Hu and Yang Liu and Shuicheng Yan and Han Fang and Yahui Zhou},
      year={2024},
      eprint={2407.08348},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{chan2024scalingsyntheticdatacreation,
      title={{Scaling Synthetic Data Creation with 1,000,000,000 Personas}}, 
      author={Xin Chan and Xiaoyang Wang and Dian Yu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2406.20094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{yang2024qwen25mathtechnicalreportmathematical,
      title={{Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement}}, 
      author={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},
      year={2024},
      eprint={2409.12122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{soldaini-etal-2024-dolma,
    title = "{Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}",
    author = "Soldaini, Luca  and
      Kinney, Rodney  and
      Bhagia, Akshita  and
      Schwenk, Dustin  and
      Atkinson, David  and
      Authur, Russell  and
      Bogin, Ben  and
      Chandu, Khyathi  and
      Dumas, Jennifer  and
      Elazar, Yanai  and
      Hofmann, Valentin  and
      Jha, Ananya  and
      Kumar, Sachin  and
      Lucy, Li  and
      Lyu, Xinxi  and
      Lambert, Nathan  and
      Magnusson, Ian  and
      Morrison, Jacob  and
      Muennighoff, Niklas  and
      Naik, Aakanksha  and
      Nam, Crystal  and
      Peters, Matthew  and
      Ravichander, Abhilasha  and
      Richardson, Kyle  and
      Shen, Zejiang  and
      Strubell, Emma  and
      Subramani, Nishant  and
      Tafjord, Oyvind  and
      Walsh, Evan  and
      Zettlemoyer, Luke  and
      Smith, Noah  and
      Hajishirzi, Hannaneh  and
      Beltagy, Iz  and
      Groeneveld, Dirk  and
      Dodge, Jesse  and
      Lo, Kyle",
    booktitle = "ACL",
    year = "2024",
}

@inproceedings{yu2024metamath,
  title={{MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  booktitle="ICLR",
  year={2024}
}

@inproceedings{
    yue2024mammoth,
    title={{MA}mmo{TH}: Building Math Generalist Models through Hybrid Instruction Tuning},
    author={Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
    booktitle={ICLR},
    year={2024},
}

@misc{chen2023chatgptsbehaviorchangingtime,
      title={{How is ChatGPT's behavior changing over time?}}, 
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2024},
      booktitle={Harvard Data Science Review},
}

@misc{nye2021workscratchpadsintermediatecomputation,
      title={{Show Your Work: Scratchpads for Intermediate Computation with Language Models}}, 
      author={Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
      year={2021},
      eprint={2112.00114},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{li2024common7blanguagemodels,
      title={{Common 7B Language Models Already Possess Strong Math Capabilities}}, 
      author={Chen Li and Weiqi Wang and Jingcheng Hu and Yixuan Wei and Nanning Zheng and Han Hu and Zheng Zhang and Houwen Peng},
      year={2024},
      eprint={2403.04706},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{bansal2024smallerweakerbettertraining,
      title={{Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling}}, 
      author={Hritik Bansal and Arian Hosseini and Rishabh Agarwal and Vinh Q. Tran and Mehran Kazemi},
      year={2024},
      eprint={2408.16737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{brown2024largelanguagemonkeysscaling,
      title={{Large Language Monkeys: Scaling Inference Compute with Repeated Sampling}}, 
      author={Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher RÃ© and Azalia Mirhoseini},
      year={2024},
      eprint={2407.21787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}


@inproceedings{min-etal-2022-rethinking,
    title = "{Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}",
    author = "Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    booktitle = "EMNLP",
    year = "2022",
}

@misc{zhang2024carefulexaminationlargelanguage,
      title={{A Careful Examination of Large Language Model Performance on Grade School Arithmetic}}, 
      author={Hugh Zhang and Jeff Da and Dean Lee and Vaughn Robinson and Catherine Wu and Will Song and Tiffany Zhao and Pranav Raja and Dylan Slack and Qin Lyu and Sean Hendryx and Russell Kaplan and Michele Lunati and Summer Yue},
      year={2024},
      eprint={2405.00332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{gou2024toratoolintegratedreasoningagent,
      title={{ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving}}, 
      author={Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
      year={2024},
      booktitle = "ICLR", 
}

@inproceedings{
Holtzman2020The,
title={{The Curious Case of Neural Text Degeneration}},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={ICLR},
year={2020},
}

@inproceedings{Loshchilov2019DecoupledWD,
  title={{Decoupled Weight Decay Regularization}},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={ICLR},
  year={2019},
}

@misc{deepseekai2024deepseekv2strongeconomicalefficient,
      title={{DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{shoeybi2020megatronlm,
      title={{Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hinton2015distilling,
      title={{Distilling the Knowledge in a Neural Network}}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{
agarwal2024onpolicy,
title={{On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes}},
author={Rishabh Agarwal and Nino Vieillard and Yongchao Zhou and Piotr Stanczyk and Sabela Ramos Garea and Matthieu Geist and Olivier Bachem},
booktitle={ICLR},
year={2024},
}

@misc{lu2022knowledge,
      title={{Knowledge Distillation of Transformer-based Language Models Revisited}}, 
      author={Chengqiang Lu and Jianwei Zhang and Yunfei Chu and Zhengyu Chen and Jingren Zhou and Fei Wu and Haiqing Chen and Hongxia Yang},
      year={2022},
      eprint={2206.14366},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mukherjee2023orca,
      title={{Orca: Progressive Learning from Complex Explanation Traces of GPT-4}}, 
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mitra2023orca,
      title={{Orca 2: Teaching Small Language Models How to Reason}}, 
      author={Arindam Mitra and Luciano Del Corro and Shweti Mahajan and Andres Codas and Clarisse Simoes and Sahaj Agarwal and Xuxi Chen and Anastasia Razdaibiedina and Erik Jones and Kriti Aggarwal and Hamid Palangi and Guoqing Zheng and Corby Rosset and Hamed Khanpour and Ahmed Awadallah},
      year={2023},
      eprint={2311.11045},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{gu2024minillmknowledgedistillationlarge,
      title={{MiniLLM: Knowledge Distillation of Large Language Models}}, 
      author={Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
      year={2024},
      eprint={2306.08543},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{hendrycks2021measuringmathematicalproblemsolving,
      title={{Measuring Mathematical Problem Solving With the MATH Dataset}}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
   booktitle   = {NeurIPS Datasets and Benchmarks},
}


@article{cobbe2021gsm8k,
  title={{Training Verifiers to Solve Math Word Problems}},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={NeurIPS},
  year={2022}
}


@misc{wang2024helpsteer2,
      title={{HelpSteer2: Open-source dataset for training top-performing reward models}}, 
      author={Zhilin Wang and Yi Dong and Olivier Delalleau and Jiaqi Zeng and Gerald Shen and Daniel Egert and Jimmy J. Zhang and Makesh Narsimhan Sreedhar and Oleksii Kuchaiev},
      year={2024},
      eprint={2406.08673},
      archivePrefix={arXiv},
}


@inproceedings{DBLP:conf/iclr/SaxtonGHK19,
  author       = {David Saxton and
                  Edward Grefenstette and
                  Felix Hill and
                  Pushmeet Kohli},
  title        = {{Analysing Mathematical Reasoning Abilities of Neural Models}},
  booktitle    = {ICLR},
  year         = {2019},
}


@inproceedings{lan2022mwptoolkit,
  title={{MWPToolkit: An Open-source Framework for Deep Learning-based
Math Word Problem Solvers}},
  author={Lan, Yihuai and Wang, Lei and Zhang, Qiyuan and Lan, Yunshi and Dai, Bing Tian and Wang, Yan and Zhang, Dongxiang and Lim, Ee-Peng},
  booktitle={AAAI},
  year={2022}
}

@misc{li2024numinamath,
  title={{NuminaMath: The largest public dataset in AI4Maths with 860k pairs of competition math problems and solutions}},
  author={Li, Jia and Beeching, Edward and Tunstall, Lewis and Lipkin, Ben and Soletskyi, Roman and Huang, Shengyi and Rasul, Kashif and Yu, Longhui and Jiang, Albert Q and Shen, Ziju and others},
  year={2024}
}


@inproceedings{DBLP:conf/iclr/0002WSLCNCZ23,
  author       = {Xuezhi Wang and
                  Jason Wei and
                  Dale Schuurmans and
                  Quoc V. Le and
                  Ed H. Chi and
                  Sharan Narang and
                  Aakanksha Chowdhery and
                  Denny Zhou},
  title        = {{Self-Consistency Improves Chain of Thought Reasoning in Language Models}},
  booktitle    = {ICLR},
  year         = {2023},
}


@article{shao2024deepseekmath,
  title={{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Yu and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@misc{sprague2024cotcotchainofthoughthelps,
      title={{To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning}}, 
      author={Zayne Sprague and Fangcong Yin and Juan Diego Rodriguez and Dongwei Jiang and Manya Wadhwa and Prasann Singhal and Xinyu Zhao and Xi Ye and Kyle Mahowald and Greg Durrett},
      year={2024},
      eprint={2409.12183},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{toshniwal2024openmathinstruct,
  title={{OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset}},
  author={Toshniwal, Shubham and Moshkov, Ivan and Narenthiran, Sean and Gitman, Daria and Jia, Fei and Gitman, Igor},
  booktitle={NeurIPS Datasets and Benchmarks},
  year={2024}
}

@inproceedings{
azerbayev2024llemma,
title={{Llemma: An Open Language Model for Mathematics}},
author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen Marcus McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
booktitle={ICLR},
year={2024},
}


@misc{deepseekai2024deepseekcoderv2breakingbarrierclosedsource,
      title={{DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence}}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2406.11931},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
}

@article{DBLP:journals/tmlr/ChenM0C23,
  author       = {Wenhu Chen and
                  Xueguang Ma and
                  Xinyi Wang and
                  William W. Cohen},
  title        = {{Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}},
  journal      = {TMLR},
  year         = {2023},
}


@inproceedings{DBLP:conf/acl/HoSY23,
  author       = {Namgyu Ho and
                  Laura Schmid and
                  Se{-}Young Yun},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {{Large Language Models Are Reasoning Teachers}},
  booktitle    = {ACL},
  year         = {2023},
}


@inproceedings{huang2022large,
  title={{Large Language Models Can Self-Improve}},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  booktitle    = {EMNLP},
  year         = {2023},
}

@article{li2022explanations,
  title={{Explanations from Large Language Models Make Small Reasoners Better}},
  author={Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and others},
  journal={arXiv preprint arXiv:2210.06726},
  year={2022}
}

@inproceedings{DBLP:conf/acl/ShridharSS23,
  author       = {Kumar Shridhar and
                  Alessandro Stolfo and
                  Mrinmaya Sachan},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {{Distilling Reasoning Capabilities into Smaller Language Models}},
  booktitle    = {Findings of ACL},
  year         = {2023},
}


@inproceedings{magister2022teaching,
    title = "{Teaching Small Language Models to Reason}",
    author = "Magister, Lucie Charlotte  and
      Mallinson, Jonathan  and
      Adamek, Jakub  and
      Malmi, Eric  and
      Severyn, Aliaksei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "ACL",
    year = "2023",
}

@article{liu2023tinygsm,
  title={{TinyGSM: achieving >80\% on GSM8k with small language models}},
  author={Liu, Bingbin and Bubeck, Sebastien and Eldan, Ronen and Kulkarni, Janardhan and Li, Yuanzhi and Nguyen, Anh and Ward, Rachel and Zhang, Yi},
  journal={arXiv preprint arXiv:2312.09241},
  year={2023}
}


@article{zhang2024infinitymath,
  title={{InfinityMATH: A Scalable Instruction Tuning Dataset in Programmatic Mathematical Reasoning}},
  author={Zhang, Bo-Wen and Yan, Yan and Li, Lin and Liu, Guang},
  journal={arXiv preprint arXiv:2408.07089},
  year={2024}
}


@article{li2024common,
  title={{Common 7B Language Models Already Possess Strong Math Capabilities}},
  author={Li, Chen and Wang, Weiqi and Hu, Jingcheng and Wei, Yixuan and Zheng, Nanning and Hu, Han and Zhang, Zheng and Peng, Houwen},
  journal={arXiv preprint arXiv:2403.04706},
  year={2024}
}



@inproceedings{li2024mugglemath,
  title={{MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning}},
  author={Li, Chengpeng and Yuan, Zheng and Yuan, Hongyi and Dong, Guanting and Lu, Keming and Wu, Jiancan and Tan, Chuanqi and Wang, Xiang and Zhou, Chang},
  booktitle={ACL},
  year={2024}
}


@article{zhu2024key,
  title={{Key-Point-Driven Mathematical Reasoning Distillation of Large Language Model}},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2407.10167},
  year={2024}
}

@article{zhu2024distilling,
  title={{Distilling mathematical reasoning capabilities into Small Language Models}},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={Neural Networks},
  volume={179},
  pages={106594},
  year={2024},
}


@inproceedings{
jain2024llmassisted,
title={{LLM-Assisted Code Cleaning For Training Accurate Code Generators}},
author={Naman Jain and Tianjun Zhang and Wei-Lin Chiang and Joseph E. Gonzalez and Koushik Sen and Ion Stoica},
booktitle={ICLR},
year={2024},
}

@article{yue2024mammoth2,
  title={{MAmmoTH2: Scaling Instructions from the Web}},
  author={Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.03548},
  year={2024}
}


@misc{omni_math,
  author    = {{Omni-Math}},
  howpublished = {\url{https://omni-math.github.io/}}, 
  year = {2024},
}

@misc{aime24,
  author    = {{AIME 2024}},
  howpublished       = {\url{https://artofproblemsolving.com/wiki/index.php/2024_AIME_I}},
  year = {2024},
}



@misc{AMC23,
  author    = {{AMC 2023}},
  howpublished       = {\url{https://github.com/QwenLM/Qwen2.5-Math/blob/main/evaluation/data/amc23/test.jsonl}},
  year = {2023},
}

@misc{mathstral,
 author = {{Mistral AI}},
 howpublished = {\url{https://mistral.ai/news/mathstral/}},
year = {2024},
}


@misc{epoch2023tradingoffcomputeintrainingandinference,
  title={{Trading Off Compute in Training and Inference}},
  author={Pablo Villalobos and David Atkinson},
  year={2023},
  url={https://epochai.org/blog/trading-off-compute-in-training-and-inference},
}



@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}


@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  url={https://openai.com/research/gpt-4},
}
