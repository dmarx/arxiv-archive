---
author:
- Michael M. Bronstein[^1], Joan Bruna[^2], Taco Cohen[^3], Petar Veličković[^4]
bibliography:
- references.bib
citation-style: ieee
date: 2024-12-29
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  **Geometric Deep Learning  
  Grids, Groups, Graphs,  
  Geodesics, and Gauges**
---




# Preface

For nearly two millenia since Euclid’s *Elements*, the word ‘geometry’ has been synonymous with *Euclidean geometry*, as no other types of geometry existed. Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann. Towards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.

A way out of this pickle was shown by a young mathematician Felix Klein, appointed in 1872 as professor in the small Bavarian University of Erlangen. In a research prospectus, which entered the annals of mathematics as the *Erlangen Programme*, Klein proposed approaching geometry as the study of *invariants*, i.e. properties unchanged under some class of transformations, called the *symmetries* of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory. For instance, Euclidean geometry is concerned with lengths and angles, because these properties are preserved by the group of Euclidean transformations (rotations and translations), while affine geometry studies parallelism, which is preserved by the group of affine transformations. The relation between these geometries is immediately apparent when considering the respective groups, because the Euclidean group is a subgroup of the affine group, which in turn is a subgroup of the group of projective transformations.

The impact of the Erlangen Programme on geometry was very profound. Furthermore, it spilled to other fields, especially physics, where symmetry principles allowed to derive conservation laws from first principles of symmetry (an astonishing result known as Noether’s Theorem), and even enabled the classification of elementary particles as irreducible representations of the symmetry group. *Category theory*, now pervasive in pure mathematics, can be “regarded as a continuation of the Klein Erlangen Programme, in the sense that a geometrical space with its group of transformations is generalized to a category with its algebra of mappings”, in the words of its creators Samuel Eilenber and Saunders Mac Lane.

At the time of writing, the state of the field of deep learning is somewhat reminiscent of the field of geometry in the nineteenth century. There is a veritable zoo of neural network architectures for various kinds of data, but few unifying principles. As in times past, this makes it difficult to understand the relations between various methods, inevitably resulting in the reinvention and re-branding of the same concepts in different application domains. For a novice trying to learn the field, absorbing the sheer volume of redundant ideas is a true nightmare.

In this text, we make a modest attempt to apply the Erlangen Programme mindset to the domain of deep learning, with the ultimate goal of obtaining a systematisation of this field and ‘connecting the dots’. We call this geometrisation attempt ‘Geometric Deep Learning’, and true to the spirit of Felix Klein, propose to derive different inductive biases and network architectures implementing them from first principles of symmetry and invariance. In particular, we focus on a large class of neural networks designed for analysing unstructured sets, grids, graphs, and manifolds, and show that they can be understood in a unified manner as methods that respect the structure and symmetries of these domains.

We believe this text would appeal to a broad audience of deep learning researchers, practitioners, and enthusiasts. A novice may use it as an overview and introduction to Geometric Deep Learning. A seasoned deep learning expert may discover new ways of deriving familiar architectures from basic principles and perhaps some surprising connections. Practitioners may get new insights on how to solve problems in their respective fields.

With such a fast-paced field as modern machine learning, the risk of writing a text like this is that it becomes obsolete and irrelevant before it sees the light of day. Having focused on foundations, our hope is that the key concepts we discuss will transcend their specific realisations — or, as Claude Adrien Helvétius put it, *“la connaissance de certains principes supplée facilement à la connoissance de certains faits.”*

# Notation

|                                                     |                                                                                                          |
|:----------------------------------------------------|:---------------------------------------------------------------------------------------------------------|
| $\Omega,u$                                          | Domain, point on domain                                                                                  |
| $x(u) \in \mathcal{X}(\Omega,\mathcal{C})$          | Signal on the domain of the form $x:\Omega\rightarrow \mathcal{C}$                                       |
| $f(x) \in \mathcal{F}(\mathcal{X}(\Omega))$         | Functions on signals on the domain of the form $f:\mathcal{X}(\Omega) \rightarrow \mathcal{Y}$           |
| $\fG,\fg$                                           | Group, element of the group                                                                              |
| $\fg.u, \rho(\fg)$                                  | Group action, group representation                                                                       |
| $\mathbf{X}\in\mathcal{C}^{|\Omega|\times s}$       | Matrix representing a signal on a discrete domain                                                        |
| $\mathbf{x}_u\in\mathcal{C}^{s}$                    | Vector representing a discrete domain signal $\mathbf{X}$ on element $u\in\Omega$                        |
| $x_{uj}\in\mathcal{C}$                              | Scalar representing the $j$th component of a discrete domain signal $\mathbf{X}$ on element $u\in\Omega$ |
| $\mathbf{F}(\mathbf{X})$                            | Function on discrete domain signals that returns another discrete domain signal, as a matrix             |
| $\tau:\Omega\rightarrow\Omega$                      | Automorphism of the domain                                                                               |
| $\eta:\Omega\rightarrow\Omega'$                     | Isomorphism between two different domains                                                                |
| $\sigma: \mathcal{C}\rightarrow\mathcal{C}'$        | Activation function (point-wise non-linearity)                                                           |
| $G=(\mathcal{V},\mathcal{E})$                       | Graph with nodes $\mathcal{V}$ and edges $\mathcal{E}$                                                   |
| $\mathcal{T}=(\mathcal{V},\mathcal{E},\mathcal{F})$ | Mesh with nodes $\mathcal{V}$, edges $\mathcal{E}$, and faces $\mathcal{F}$                              |
| $x\star \theta$                                     | Convolution with filter $\theta$                                                                         |
| $S_v$                                               | Shift operator                                                                                           |
| $\varphi_i$                                         | Basis function                                                                                           |
| $T_u\Omega, T\Omega$                                | Tangent space at $u$, tangent bundle                                                                     |
| $X \in T_u\Omega$                                   | Tangent vector                                                                                           |
| $g_u(X,Y) = \langle X, Y\rangle_u$                  | Riemannian metric                                                                                        |
| $\ell(\gamma), \ell_{uv}$                           | Length of a curve $\gamma$, discrete metric on edge $(u,v)$                                              |

[^1]: Imperial College London / USI IDSIA / Twitter

[^2]: New York University

[^3]: Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

[^4]: DeepMind
