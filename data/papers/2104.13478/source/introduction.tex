
\section{Introduction}                % Print a "chapter" heading

The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale.  
%
 Remarkably, the essence of deep learning is built from two simple algorithmic principles: 
 first, the notion of representation or \emph{feature learning}, whereby adapted, often hierarchical, features  capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as {\em backpropagation}. %, such as gradient descent. 

While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. 

Exploiting the known symmetries of a large system is a powerful and classical remedy against the curse of dimensionality, and forms the basis of most physical theories. Deep learning systems are no exception, and since the early days researchers have adapted neural networks to exploit the low-dimensional geometry arising from physical measurements, e.g. grids in images, sequences in time-series, or position and momentum in molecules, and their associated symmetries, such as translation or rotation. 
Throughout our exposition, we will describe these models, as well as many others, as natural instances of the same underlying principle of geometric regularity.%stability. 

Such a `geometric unification' endeavour 
in the spirit of the Erlangen Program 
%--our own Erlangen Program
%\marginnote{We are mindful that the risk of flashy buzzwords such as {\em The Hilbert Problems of X} or {\em Erlangen Program of Y} inevitably implies the irreverent analogy between ourselves and the aforementioned giants. Our only intent here is to draw inspiration from Klein's approach -- {\em ergo, parce nobis, benevole lector!}
%} -- 
serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, 
and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures %, an essential ingredient to improve the sample efficiency of data-driven methods into computational sciences 
and provide principled way to build future architectures yet to be invented.  

Before proceeding, it is worth noting that our work concerns \emph{representation learning architectures}  and exploiting the symmetries of data therein. The many exciting \emph{pipelines} where such representations may be used (such as self-supervised learning, generative modelling, or reinforcement learning) are \emph{not} our central focus\marginnote{The same applies for techniques used for \emph{optimising} or \emph{regularising} our architectures, such as Adam \citep{kingma2014adam}, dropout \citep{srivastava2014dropout} or batch normalisation \citep{ioffe2015batch}.}. Hence, we will not review in depth influential neural pipelines such as variational autoencoders \citep{kingma2013auto}, generative adversarial networks \citep{goodfellow2014generative}, normalising flows  \citep{rezende2015variational}, deep Q-networks \citep{mnih2015human}, proximal policy optimisation \citep{schulman2017proximal}, or deep mutual information maximisation \citep{hjelm2018learning}. That being said, we believe that the principles we will focus on are of significant importance in all of these areas.
%having good foundations on exploiting the geometry of data will lead to successes when the resulting neural architectures are plugged into any of the above pipelines.

Further, while we have attempted to cast a reasonably wide net in order to illustrate the power of our geometric blueprint, our work does not attempt to accurately summarise the \emph{entire} existing wealth of research on Geometric Deep Learning. Rather, we study several well-known architectures in-depth in order to demonstrate the principles and ground them in existing research, with the hope that we have left sufficient references for the reader to meaningfully apply these principles to any future geometric deep architecture they encounter or devise.

%\raggedbottom
\section{Learning in High Dimensions}

%\textbf{[Summarise this section into the main elements here, with less formality]}


% Main points to illustrate:
% \begin{itemize}
%     \item Modern High-dimensional learning as interpolation. Choosing regularity prior for interpolant in high-dimensions? 
%     \item In low-dimensions, we have good notions of regularity (Sobolev, Holder, etc. ). 
%     \item These can be extended to high-dimensions using reproducing kernel hilbert spaces, or Lipschitz classes. They all suffer from a curse of dimensionality, consequence of the fact that no structural assumptions are made on the data domain. 
%     \item Functions parametrised as neural networks? 
    
% \end{itemize}

Supervised machine learning, in its simplest formalisation, considers a set of $N$ observations $\gD=\{(x_i, y_i)\}_{i=1}^{ N}$ drawn \emph{i.i.d.} 
from an underlying data distribution $P$ defined over $\gX \times \gY$, where $\gX$ and $\gY$ are respectively the data and the label domains. The defining feature in this setup is that $\gX$ is a {\em high-dimensional space}: one typically assumes $\gX= \R^d$ to be a  Euclidean space  of large dimension $d$.

%In our context, however, $\gX$ will often take a more structured form, such as the space of functions $\gX(\Omega) = \{ x: \Omega \to \R \}$ defined over a low-dimensional domain $\Omega$, which can be a geometric object such as a grid, manifold, or a graph. 

Let us further assume that the labels $y$ are generated by an unknown function $f$, such that $y_i = f(x_i)$, 
and the learning problem reduces to estimating the function $f$ using a parametrised function class $\gF=\{ f_{\thetab \in \Theta}\}$. Neural networks are a common realisation of such parametric function classes, in which case $\thetab \in \Theta$ corresponds to the network weights. 
%e.g. to the weights of a neural network. 
In this idealised setup, there is no noise in the labels, and modern deep learning systems typically operate in the so-called \emph{interpolating regime}, where the estimated $\tilde{f} \in \gF$ satisfies $\tilde{f}(x_i) = f(x_i)$ for all $i=1,\hdots, N$.  
The performance of a learning algorithm is measured in terms of the \emph{expected performance} \marginnote{Statistical learning theory is concerned with more refined notions of generalisation based on {\em concentration inequalities}; we will review some of these in future work.} on new samples drawn from ${P}$, using some {\em loss} $L(\cdot,\cdot)$ 
$$\gR(\tilde{f}):= \mathbb{E}_{P}\,\, L(\tilde{f}(x), f(x)),$$
%
with the squared-loss $L(y,y')=\frac{1}{2}|y-y'|^2$ being among the most commonly used ones.  


A successful learning scheme thus needs to encode the appropriate notion of regularity or  \emph{inductive bias} for $f$, imposed through the construction of the function class $\mathcal{F}$ and the use of {\em regularisation}. We briefly introduce this concept in the following section. %, and will explore it in detail in follow-up work. 


\subsection{Inductive Bias via Function Regularity}
\label{sec:inductive}


Modern machine learning operates with large, high-quality datasets, which, together with appropriate computational resources, motivate the design of rich function classes $\gF$ with the capacity to interpolate such large data. 
%
This 
%non-parametric % Taco: can we really call this "non-parametric"? It's just a very large parametric function space, no?
mindset plays well with neural networks, since even the simplest choices of architecture yields a {\em dense} class of functions.\marginnote{A set $\mathcal{A}\subset \mathcal{X}$ is said to be {\em dense} in $\mathcal{X}$ if its closure
$$
\mathcal{A}\cup \{ \displaystyle \lim_{i\rightarrow \infty} a_i : a_i \in \mathcal{A}\} = \mathcal{X}.
$$ 
This implies that any point in $\mathcal{X}$ is arbitrarily close to a point in $\mathcal{A}$. A typical Universal Approximation result shows that the class of functions represented e.g. by a two-layer perceptron, $f(\mathbf{x}) = \mathbf{c}^\top\mathrm{sign}(\mathbf{A}\mathbf{x}+\mathbf{b})$ is dense in the space of continuous functions on $\mathbb{R}^d$. 
} 
%
The capacity to approximate almost arbitrary functions
%\marginnote{More precisely, Borel-measurable. } 
is the subject of various \emph{Universal Approximation Theorems}; several such results were proved and popularised in the 1990s by applied mathematicians and computer scientists (see e.g. \cite{cybenko1989approximation,hornik1991approximation,barron1993universal,leshno1993multilayer,maiorov1999best,pinkus1999approximation}). 
%(Barron, Meir, Maiorov, Pinkus, Cybenko, Hornik, etc.) 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/ua_mlp.png}
    \caption{Multilayer Perceptrons \citep{rosenblatt1958perceptron}, the simplest feed-forward neural networks, are universal approximators: with just one hidden layer, they can represent combinations of step functions, allowing to approximate any continuous function with arbitrary precision.}
    \label{fig:ua_mlp}
\end{figure}%


%\michael{Show an intuitive example of MLP approximating step functions}


Universal Approximation, however, does not imply an {\em absence} of inductive bias. Given a hypothesis space $\gF$ with universal approximation, we can define a complexity measure $c: \gF \to \R_{+}$ and redefine our interpolation problem as 
$$
\tilde{f} \in \arg\min_{g \in \gF} c(g) \quad \mathrm{s.t.} \quad g(x_i) = f(x_i)  \quad \mathrm{for} \,\,\, i=1, \hdots, N,
$$
i.e., we are looking for the most regular functions within our hypothesis class. 
%
For standard function spaces, this complexity measure can be defined as a {\em norm},\marginnote{
%A {\em norm} is a non-negative function $\| \cdot \|$ on a vector space that is {\em homogeneous} $\| \alpha x \| = |\alpha| \| x\|$, {\em subadditive} $\|x+y \| \leq \|x\| + \|y\|$ and {\em positive-definite} $\|x\|=0 \Rightarrow x=0$. 
Informally, a norm $\|x\|$ can be regarded as a ``length'' of vector $x$. 
A {\em Banach space} is a complete vector space equipped with a norm. 
} making $\gF$ a {\em Banach space} and allowing to leverage a plethora of theoretical results in functional analysis. 
%
In low dimensions, splines are a workhorse for function approximation. They can be formulated as above, with a norm capturing 
the classical notion of smoothness, such as the squared-norm of second-derivatives $\int_{-\infty}^{+\infty} |f''(x)|^2 \mathrm{d}x$ for cubic splines. 

%\michael{Notation: change complexity from $\gamma$ to $c$ (which we used for Dirichlet)?}


In the case of neural networks,  the complexity measure $c$ can be expressed in terms of the network weights, i.e. $c(f_{\boldsymbol{\theta}}) = {c}(\boldsymbol{\theta})$.
%
The $L_2$-norm of the network weights, known as \emph{weight decay}, or the so-called \emph{path-norm} \citep{neyshabur2015norm} are popular choices in deep learning literature.  
%\michael{I suggest we give concrete examples here, e.g. sparsity-inducing regularization and low-dimensional projections}
%
%
From a Bayesian perspective, such complexity measures can also be interpreted as the negative log of the prior for the function of interest. More generally, this complexity can be enforced \emph{explicitly} by incorporating it into the empirical loss (resulting in the so-called Structural Risk Minimisation), or \emph{implicitly}, as a result of a certain optimisation scheme. For example, it is well-known that gradient-descent on an under-determined least-squares objective will choose interpolating solutions with minimal $L_2$ norm. The extension of such implicit regularisation results to modern neural networks is the subject of current studies (see e.g. \cite{blanc2020implicit, shamir2020implicit, razin2020implicit, gunasekar2017implicit}).   
All in all, a natural question arises: how to define effective priors that capture the expected regularities and complexities of real-world prediction tasks? 
%We will address this question in Chapter ??




\subsection{The Curse of Dimensionality}

While interpolation in low-dimensions %$\gX = \R^d$ 
(with $d=1,2$ or $3$) is a classic signal processing task with very precise mathematical control of estimation errors using increasingly sophisticated regularity classes (such as spline interpolants, wavelets, curvelets, or ridgelets), the situation for high-dimensional problems is entirely different. 
%

% \begin{figure}[h!]
%     \centering
%     %\includegraphics[width=0.3\linewidth]{figures/curseofdim1.pdf}
%     \caption{An $\epsilon$-net of a  $d$-dimensional Euclidean domain of diameter $1$ grows exponentially as $\epsilon^{-d}$. }
%     \label{fig:curseofdim}
% \end{figure}%

%\michael{We need to clean up notation: in ML problems our main object is $f(x)$ where $x$ is the input, e.g. an image. It has its own structure, being in own turn a signal on the domain, $x(u)$.}

In order to convey the essence of the idea, let us consider a classical notion of regularity that can be easily extended to high dimensions: 1-Lipschitz- functions $f:\mathcal{X} \to \R$, i.e. functions satisfying  $|f(x) - f(x')| \leq \|x - x'\|$ for all $x, x' \in \mathcal{X}$. This hypothesis only asks the target function to be \emph{locally} smooth, i.e., if we perturb the input $x$ slightly (as measured by the norm $\|x - x'\|$), the output $f(x)$ is not allowed to change much. %\michael{Lets avoid confusion and use $\mathcal{X}$ rather than $\Omega$}
%\marginnote{The number of protons in the observable universe, known as the {\em Eddington number}, is currently estimated at $\sim 10^{80}$.}
If our only knowledge of the target function $f$ is that it is $1$-Lipschitz, how many observations do we expect to require to ensure that our estimate $\tilde{f}$ will be close to $f$? 
Figure \ref{fig:curseofdim} reveals that the general answer is necessarily exponential in the dimension $d$, signaling that the Lipschitz class grows `too quickly' as the input dimension increases: in many applications with even modest dimension $d$, the number of samples would be bigger than the number of atoms in the universe.
 The situation is not better if one replaces the Lipschitz class by a global smoothness hypothesis, such as the Sobolev Class $\gH^{s}(\Omega_d)$\marginnote{A function $f$ is in the {\em Sobolev class} $\gH^{s}(\Omega_d)$ if $f \in L^2(\Omega_d)$ and the generalised $s$-th order derivative is square-integrable: $\int |\omega|^{2s+1} |\hat{f}(\omega)|^2 d\omega < \infty$, where 
$\hat{f}$ is the Fourier transform of $f$; see Section~\ref{sec:grids_euclidean}. 
}. Indeed, classic results \citep{tsybakov2008introduction} establish a minimax rate of approximation and learning for the Sobolev class of the order $\epsilon^{-d/s}$, showing that the extra smoothness assumptions on $f$ only improve the statistical picture when $s \propto d$, an unrealistic assumption in practice. 

% on the one hand, `globally' smooth functions, with $s$-order derivatives \marginnote{The \emph{Sobolev} class defines  }

% In fact, even though we can extend the above `classic' functional spaces to arbitrary dimensions, they suffer from the so-called \emph{curse of dimensionality}. In essence, in order to obtain an expected generalization error $\mathbb{E}_{x\sim \mathbb{P}} | \tilde{f}(x) - f(x)| \leq \epsilon$, such regularity hypotheses require an order of $\epsilon^{-d}$ observations, signalling an impossibility of efficient learning : in many applications with even modest dimension $d$, the number of samples would be bigger than the number of atoms in the universe.
% \marginnote{The number of protons in the observable universe, known as the Eddington number, is currently estimated at $\sim 10^{80}$.}

\begin{figure}[h!]
    \centering
%    \includegraphics[width=0.45\linewidth]{figures/curseofdim4.pdf} 
 %       \includegraphics[width=0.45\linewidth]{figures/curseofdim2.pdf}
       \includegraphics[width=\linewidth]{figures/blobs.png} 
    \caption{We consider a Lipschitz function $f(x) = \sum_{j=1}^{2^d} z_j \phi(x-x_j)$ where $z_j=\pm 1$, $x_j \in \R^d$ is placed in each quadrant, and $\phi$ a locally supported Lipschitz `bump'. Unless we observe the function in most of the $2^d$ quadrants, we will incur in a constant error in predicting it. This simple geometric argument can be formalised through the notion of \emph{Maximum Discrepancy} \citep{von2004distance}, defined for the Lipschitz class as $\kappa(d)=\mathbb{E}_{x,x'} \sup_{f \in \mathrm{Lip}(1)} \left| \frac{1}{N}\sum_{l} f(x_l) - \frac{1}{N}\sum_{l} f(x'_l) \right| \simeq N^{-1/d}$, which measures the largest expected discrepancy between two independent $N$-sample expectations. Ensuring that $\kappa(d) \simeq \epsilon$ requires $N = \Theta(\epsilon^{-d})$; the corresponding sample $\{x_l\}_l$ defines an $\epsilon$-net of the domain. For a $d$-dimensional Euclidean domain of diameter $1$, its size grows exponentially as $\epsilon^{-d}$.}
    \label{fig:curseofdim}
\end{figure}


%\michael{\bf [MB: INSERT FIGURE and explain the curse of dimensionality as a geometric phenomenon using the volume of inscribed unit ball]}


%\michael{\bf [MB: General comment: in order not to interrupt the flow, we can have self-contained figures with long captions or `inserts' explaining important concepts. We had it in our Signal Processing Magazine paper and it was a very good idea
%}

%\joan{Agreed. The Curse of Dimensonality could be one such insert, with a panel with (i) a figure that displays the exponential dependence of epsilon-grids wrt dimension, and caption. TODO JOAN}

Fully-connected neural networks define function spaces that enable more flexible notions of regularity, obtained by considering complexity functions $c$ on their weights. In particular, by choosing a sparsity-promoting regularisation, they have the ability to break this curse of dimensionality \citep{bach2017breaking}. However, this comes at the expense of making strong assumptions on the nature of the target function $f$, such as that $f$ depends on a collection of low-dimensional projections of the input (see Figure \ref{fig:curseofdim2}).
%\michael{\bf [MB: not sure I understand this part. Isn't there a universal approximation result?]} \joan{I further clarified}
In most real-world applications (such as computer vision, speech analysis, physics, or  chemistry), functions of interest %$f$ 
tend to exhibits complex long-range correlations that cannot be expressed with low-dimensional projections (Figure \ref{fig:curseofdim2}), making this hypothesis unrealistic.  
It is thus necessary to define an alternative source of regularity, by exploiting the spatial structure of the physical domain and the geometric priors of $f$, as we describe in the next Section~\ref{sec:geom_priors}. 

\begin{figure}[htbp]
    \centering
%    \includegraphics[width=0.45\linewidth]{figures/curseofdim5.pdf} 
%    \includegraphics[width=0.45\linewidth]{figures/mnistm.pdf} %
\includegraphics[width=0.6\linewidth]{figures/mnist.png}
    \caption{%\michael{Lets not use $\Omega$ here but $\mathcal{X}$ as the domain of $f$} 
    If the unknown function $f$ %:\gX \to \R$ 
    is presumed to be well approximated as $f(\mathbf{x}) \approx g(\mathbf{A}\mathbf{x})$ for some unknown $\mathbf{A} \in \mathbb{R}^{k \times d}$ with $k \ll d$, then shallow neural networks can capture this inductive bias, see e.g.  \cite{bach2017breaking}. In typical applications, such dependency on low-dimensional projections is unrealistic, as illustrated in this example: a low-pass filter projects the input images to a low-dimensional subspace; while it conveys most of the semantics, substantial information is lost.%\michael{Joan: why not to show the Swiss-roll surface?} \joan{Yes, we can use it to replace the sketch in the left. }
    % Taco: is the blurred MNIST really a good example? I guess that if we used a PCA projection we could keep >98% of the variance with only a few components for this dataset. But perhaps for natural images this is no longer true..
    }
    \label{fig:curseofdim2}
\end{figure}


\input{geometricpriors}

\input{geometricdomains}

\input{geometricmodels}

\input{geometricapplications}

\input{historic}

\section*{Acknowledgements}

%Our book 
This text
represents a humble attempt to summarise and synthesise decades of existing knowledge in %geometric 
deep learning architectures, through the geometric lens of invariance and symmetry. 
% 
%
We hope that our perspective 
%---largely propelled by inspiration from the Erlangen Program---
will make it easier both for newcomers and practitioners to navigate the field, and for researchers to synthesise novel architectures, as instances of our blueprint. In a way, we hope to have presented \emph{``all you need to build the architectures that are all you need''}---a play on words inspired by \citet{vaswani2017attention}.



%In many ways, the seeds for this text %book 
%have been planted many years before a single paragraph had been written. The four of us had been, perhaps unknowingly, exploring research directions we present here under the title of Geometric Deep Learning for many years.  
%Michael and Joan did it from the lens of signal processing (alongside Yann LeCun, Arthur Szlam and Pierre Vandergheynst), Taco from the lens of group-equivariant convolutions (alongside Mario Geiger, Jonas K\"{o}hler and Max Welling) and Petar through spatial graph representation learning (alongside Yoshua Bengio, Arantxa Casanova, Guillem Cucurull,  Pietro Li\`{o} and Adriana Romero). We deeply thank all of the co-authors listed above, as with their support we produced several pieces of early evidence that the blueprint is worth studying: the first Geometric Deep Learning review paper \citep{bronstein2017geometric} (with highly-attended companion tutorial at NIPS 2017), Spherical CNNs \citep{cohen2018spherical} (best-paper awardee at ICLR 2018) and Graph Attention Networks \citep{velickovic2018graph} (the top-cited paper at ICLR 2018).
%
%The first occasion\marginnote{\includegraphics[width=\linewidth]{figures/Michael.jpeg}\\ \includegraphics[width=\linewidth]{figures/Taco.jpeg}} when all the authors %of our respective work (literally) 
%came together was the Workshop on Representation Learning on Graphs and Manifolds that took place at ICLR  in New Orleans in 2019. Joan and Petar were both co-organisers of the workshop, while Michael and Taco delivered two of the keynote talks on mesh CNNs for protein design and 
%---covering topics of geometric deep learning (for protein design), and 
%gauge-equivariant convolutions,  respectively. 
%Both of these topics feature prominently in this writeup. We would like to thank the entire co-organising team of this workshop (Pete Battaglia, Albert Gu, Beliz Gunel, Will Hamilton, Thomas Kipf, Yujia Li, Max Nickel, Razvan Pascanu, Chris R\'{e}, Adriana Romero, Fred Sala and Marinka \v{Z}itnik), the ICLR 2019 organisers for giving us the opportunity to host such a workshop, and consequently allow us to discuss these topics in the same space.
%\petar{We thank the IAS...}
%
%Though probably each of us had been dreaming of writing at some point a ``deep learning textbook that would not become obsolete by the time of its publication'', the concrete steps towards this project were taken by Joan. 
%
%The first outline was drafted in February 2020 by Joan and Michael while both were visitors at the Institute for Advanced Studies in Princeton (a kind invitation for which we are grateful to Sanjeev Arora). This also happened to be the last in-person meeting, as the start of the coronavirus pandemic made Zoom calls the new normal and also introduced some corrections into the original schedule. 



The bulk of the text was written during late 2020 and early 2021. As it often happens, we had thousands of doubts whether  the whole picture makes sense, and used opportunities provided by our colleagues to  
%
%Ultimately, we are grateful to all of the venues that 
help us break our ``stage fright'' and present early versions of our work, which saw the  
%to wider \emph{Zoom} audiences. 
%In many ways, our work first saw the 
light of day in Petar's talk at Cambridge (courtesy of Pietro Li{\`o}) and Michael's talks at Oxford (courtesy of Xiaowen Dong) and Imperial College (hosted by Michael Huth and Daniel Rueckert). Petar was also able to present our work at Friedrich-Alexander-Universit\"{a}t Erlangen-N\"{u}rnberg---the birthplace of the Erlangen Program!---owing to a kind invitation from Andreas Maier.
%
%
%through Michael's talks on \emph{``Geometric Deep Learning''}\marginnote{Michael's GDL talks came with various subtitles---\emph{``GDL for 3D human body synthesis''}, \emph{``GDL, from Euclid to drug design''}, and \emph{``GDL: the Erlangen Programme of ML''}.} and Petar's talk on \emph{``Theoretical Foundations of Graph Neural Networks''}, and 
The feedback we received for these talks was enormously invaluable to keeping our spirits high, as well as polishing the work further. 
%Accordingly, we thank the organisers of the \emph{Topological Data Analysis and Beyond} Workshop at NeurIPS 2020, \emph{University of Cambridge}, and \emph{Imperial College London}. 
Last, but certainly not least, we thank the organising committee of ICLR 2021, where our work will be featured in a keynote talk, delivered by Michael.

%Karsten Bogwart review on graph kernels

%}

We should note that 
reconciling such a vast quantity of research is seldom enabled by the expertise of only four people. Accordingly, we would like to give due credit to all of the researchers who have carefully studied aspects of our text as it evolved, and provided us with careful comments and references: Yoshua Bengio, Charles Blundell, Andreea Deac, Fabian Fuchs, Francesco di Giovanni, Marco Gori, Raia Hadsell, Will Hamilton, Maksym Korablyov, Christian Merkwirth, Razvan Pascanu, Bruno Ribeiro, Anna Scaife, J\"{u}rgen Schmidhuber, Marwin Segler, Corentin Tallec, Ng\^{a}n V\~{u}, Peter Wirnsberger and David Wong. Their expert feedback was invaluable to solidifying our unification efforts and making them more useful to various niches. Though, of course, any irregularities within this text are our responsibility alone. It is currently very much a work-in-progress, and we are very happy to receive comments at any stage. Please contact us if you spot any errors or omissions.
