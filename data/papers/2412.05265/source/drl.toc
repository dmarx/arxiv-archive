\contentsline {chapter}{\numberline {1}Introduction}{9}{chapter.1}%
\contentsline {section}{\numberline {1.1}Sequential decision making}{9}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Problem definition}{9}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Universal model}{9}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Episodic vs continuing tasks}{11}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Regret}{12}{subsection.1.1.4}%
\contentsline {subsection}{\numberline {1.1.5}Further reading}{12}{subsection.1.1.5}%
\contentsline {section}{\numberline {1.2}Canonical examples}{12}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Partially observed MDPs}{12}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Markov decision process (MDPs)}{13}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Contextual MDPs}{14}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Contextual bandits}{14}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Belief state MDPs}{15}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Optimization problems}{16}{subsection.1.2.6}%
\contentsline {subsubsection}{\numberline {1.2.6.1}Best-arm identification}{16}{subsubsection.1.2.6.1}%
\contentsline {subsubsection}{\numberline {1.2.6.2}Bayesian optimization}{16}{subsubsection.1.2.6.2}%
\contentsline {subsubsection}{\numberline {1.2.6.3}Active learning}{16}{subsubsection.1.2.6.3}%
\contentsline {subsubsection}{\numberline {1.2.6.4}Stochastic Gradient Descent (SGD)}{17}{subsubsection.1.2.6.4}%
\contentsline {section}{\numberline {1.3}Reinforcement Learning}{17}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Value-based RL (Approximate Dynamic Programming)}{17}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Policy-based RL}{18}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Model-based RL}{19}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Dealing with partial observability}{19}{subsection.1.3.4}%
\contentsline {subsubsection}{\numberline {1.3.4.1}Optimal solution}{19}{subsubsection.1.3.4.1}%
\contentsline {subsubsection}{\numberline {1.3.4.2}Finite observation history}{19}{subsubsection.1.3.4.2}%
\contentsline {subsubsection}{\numberline {1.3.4.3}Stateful (recurrent) policies}{19}{subsubsection.1.3.4.3}%
\contentsline {subsection}{\numberline {1.3.5}Software}{20}{subsection.1.3.5}%
\contentsline {section}{\numberline {1.4}Exploration-exploitation tradeoff}{20}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Simple heuristics}{20}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Methods based on the belief state MDP}{21}{subsection.1.4.2}%
\contentsline {subsubsection}{\numberline {1.4.2.1}Bandit case (Gittins indices)}{21}{subsubsection.1.4.2.1}%
\contentsline {subsubsection}{\numberline {1.4.2.2}MDP case (Bayes Adaptive MDPs)}{21}{subsubsection.1.4.2.2}%
\contentsline {subsection}{\numberline {1.4.3}Upper confidence bounds (UCBs)}{22}{subsection.1.4.3}%
\contentsline {subsubsection}{\numberline {1.4.3.1}Basic idea}{22}{subsubsection.1.4.3.1}%
\contentsline {subsubsection}{\numberline {1.4.3.2}Bandit case: Frequentist approach}{22}{subsubsection.1.4.3.2}%
\contentsline {subsubsection}{\numberline {1.4.3.3}Bandit case: Bayesian approach}{22}{subsubsection.1.4.3.3}%
\contentsline {subsubsection}{\numberline {1.4.3.4}MDP case}{23}{subsubsection.1.4.3.4}%
\contentsline {subsection}{\numberline {1.4.4}Thompson sampling}{23}{subsection.1.4.4}%
\contentsline {subsubsection}{\numberline {1.4.4.1}Bandit case}{23}{subsubsection.1.4.4.1}%
\contentsline {subsubsection}{\numberline {1.4.4.2}MDP case (posterior sampling RL)}{24}{subsubsection.1.4.4.2}%
\contentsline {section}{\numberline {1.5}RL as a posterior inference problem}{24}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Modeling assumptions}{24}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Soft value functions}{26}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Maximum entropy RL}{26}{subsection.1.5.3}%
\contentsline {subsection}{\numberline {1.5.4}Active inference}{27}{subsection.1.5.4}%
\contentsline {chapter}{\numberline {2}Value-based RL}{29}{chapter.2}%
\contentsline {section}{\numberline {2.1}Basic concepts}{29}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Value functions}{29}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Bellman's equations}{29}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Example: 1d grid world}{30}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}Computing the value function and policy given a known world model}{31}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Value iteration}{31}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Real-time dynamic programming (RTDP)}{31}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Policy iteration}{32}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Computing the value function without knowing the world model}{33}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Monte Carlo estimation}{33}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Temporal difference (TD) learning}{33}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Combining TD and MC learning using TD($\lambda $)}{34}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Eligibility traces}{35}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}SARSA: on-policy TD control}{35}{section.2.4}%
\contentsline {section}{\numberline {2.5}Q-learning: off-policy TD control}{36}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Tabular Q learning}{36}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Q learning with function approximation}{37}{subsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.2.1}Neural fitted Q}{37}{subsubsection.2.5.2.1}%
\contentsline {subsubsection}{\numberline {2.5.2.2}DQN}{37}{subsubsection.2.5.2.2}%
\contentsline {subsubsection}{\numberline {2.5.2.3}Experience replay}{38}{subsubsection.2.5.2.3}%
\contentsline {subsubsection}{\numberline {2.5.2.4}The deadly triad}{38}{subsubsection.2.5.2.4}%
\contentsline {subsubsection}{\numberline {2.5.2.5}Target networks}{39}{subsubsection.2.5.2.5}%
\contentsline {subsubsection}{\numberline {2.5.2.6}Two time-scale methods}{39}{subsubsection.2.5.2.6}%
\contentsline {subsubsection}{\numberline {2.5.2.7}Layer norm}{39}{subsubsection.2.5.2.7}%
\contentsline {subsection}{\numberline {2.5.3}Maximization bias}{40}{subsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.3.1}Double Q-learning}{40}{subsubsection.2.5.3.1}%
\contentsline {subsubsection}{\numberline {2.5.3.2}Double DQN}{40}{subsubsection.2.5.3.2}%
\contentsline {subsubsection}{\numberline {2.5.3.3}Randomized ensemble DQN}{41}{subsubsection.2.5.3.3}%
\contentsline {subsection}{\numberline {2.5.4}DQN extensions}{41}{subsection.2.5.4}%
\contentsline {subsubsection}{\numberline {2.5.4.1}Q learning for continuous actions}{41}{subsubsection.2.5.4.1}%
\contentsline {subsubsection}{\numberline {2.5.4.2}Dueling DQN}{41}{subsubsection.2.5.4.2}%
\contentsline {subsubsection}{\numberline {2.5.4.3}Noisy nets and exploration}{42}{subsubsection.2.5.4.3}%
\contentsline {subsubsection}{\numberline {2.5.4.4}Multi-step DQN}{42}{subsubsection.2.5.4.4}%
\contentsline {subsubsection}{\numberline {2.5.4.5}Rainbow}{42}{subsubsection.2.5.4.5}%
\contentsline {subsubsection}{\numberline {2.5.4.6}Bigger, Better, Faster}{43}{subsubsection.2.5.4.6}%
\contentsline {subsubsection}{\numberline {2.5.4.7}Other methods}{43}{subsubsection.2.5.4.7}%
\contentsline {chapter}{\numberline {3}Policy-based RL}{45}{chapter.3}%
\contentsline {section}{\numberline {3.1}The policy gradient theorem}{45}{section.3.1}%
\contentsline {section}{\numberline {3.2}REINFORCE}{46}{section.3.2}%
\contentsline {section}{\numberline {3.3}Actor-critic methods}{47}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Advantage actor critic (A2C)}{48}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Generalized advantage estimation (GAE)}{49}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Two-time scale actor critic algorithms}{50}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Natural policy gradient methods}{50}{subsection.3.3.4}%
\contentsline {subsubsection}{\numberline {3.3.4.1}Natural gradient descent}{50}{subsubsection.3.3.4.1}%
\contentsline {subsubsection}{\numberline {3.3.4.2}Natural actor critic}{52}{subsubsection.3.3.4.2}%
\contentsline {section}{\numberline {3.4}Policy improvement methods}{52}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Policy improvement lower bound}{52}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Trust region policy optimization (TRPO)}{53}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Proximal Policy Optimization (PPO)}{53}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}VMPO}{54}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Off-policy methods}{55}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Policy evaluation using importance sampling}{55}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Off-policy actor critic methods}{56}{subsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.2.1}Learning the critic using V-trace}{56}{subsubsection.3.5.2.1}%
\contentsline {subsubsection}{\numberline {3.5.2.2}Learning the actor}{57}{subsubsection.3.5.2.2}%
\contentsline {subsubsection}{\numberline {3.5.2.3}IMPALA}{57}{subsubsection.3.5.2.3}%
\contentsline {subsection}{\numberline {3.5.3}Off-policy policy improvement methods}{58}{subsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.3.1}Off-policy PPO}{58}{subsubsection.3.5.3.1}%
\contentsline {subsubsection}{\numberline {3.5.3.2}Off-policy VMPO}{58}{subsubsection.3.5.3.2}%
\contentsline {subsubsection}{\numberline {3.5.3.3}Off-policy TRPO}{58}{subsubsection.3.5.3.3}%
\contentsline {subsection}{\numberline {3.5.4}Soft actor-critic (SAC)}{59}{subsection.3.5.4}%
\contentsline {subsubsection}{\numberline {3.5.4.1}Policy evaluation}{59}{subsubsection.3.5.4.1}%
\contentsline {subsubsection}{\numberline {3.5.4.2}Policy improvement: Gaussian policy}{60}{subsubsection.3.5.4.2}%
\contentsline {subsubsection}{\numberline {3.5.4.3}Policy improvement: softmax policy}{60}{subsubsection.3.5.4.3}%
\contentsline {subsubsection}{\numberline {3.5.4.4}Adjusting the temperature}{60}{subsubsection.3.5.4.4}%
\contentsline {section}{\numberline {3.6}Deterministic policy gradient methods}{62}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}DDPG}{63}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Twin Delayed DDPG (TD3)}{63}{subsection.3.6.2}%
\contentsline {chapter}{\numberline {4}Model-based RL}{65}{chapter.4}%
\contentsline {section}{\numberline {4.1}Decision-time planning}{67}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Model predictive control (MPC)}{67}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Heuristic search}{67}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Monte Carlo tree search}{68}{subsection.4.1.3}%
\contentsline {subsubsection}{\numberline {4.1.3.1}AlphaGo and AlphaZero}{68}{subsubsection.4.1.3.1}%
\contentsline {subsubsection}{\numberline {4.1.3.2}MuZero}{68}{subsubsection.4.1.3.2}%
\contentsline {subsubsection}{\numberline {4.1.3.3}EfficientZero}{69}{subsubsection.4.1.3.3}%
\contentsline {subsection}{\numberline {4.1.4}Trajectory optimization for continuous actions}{69}{subsection.4.1.4}%
\contentsline {subsubsection}{\numberline {4.1.4.1}Random shooting}{69}{subsubsection.4.1.4.1}%
\contentsline {subsubsection}{\numberline {4.1.4.2}LQG}{69}{subsubsection.4.1.4.2}%
\contentsline {subsubsection}{\numberline {4.1.4.3}CEM}{69}{subsubsection.4.1.4.3}%
\contentsline {subsubsection}{\numberline {4.1.4.4}MPPI}{70}{subsubsection.4.1.4.4}%
\contentsline {subsubsection}{\numberline {4.1.4.5}GP-MPC}{70}{subsubsection.4.1.4.5}%
\contentsline {subsection}{\numberline {4.1.5}SMC for MPC}{70}{subsection.4.1.5}%
\contentsline {section}{\numberline {4.2}Background planning}{71}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}A game-theoretic perspective on MBRL}{72}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Dyna}{73}{subsection.4.2.2}%
\contentsline {subsubsection}{\numberline {4.2.2.1}Tabular Dyna}{74}{subsubsection.4.2.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2.2}Dyna with function approximation}{74}{subsubsection.4.2.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Dealing with model errors and uncertainty}{75}{subsection.4.2.3}%
\contentsline {subsubsection}{\numberline {4.2.3.1}Avoiding compounding errors in rollouts}{75}{subsubsection.4.2.3.1}%
\contentsline {subsubsection}{\numberline {4.2.3.2}End-to-end differentiable learning of model and planner}{75}{subsubsection.4.2.3.2}%
\contentsline {subsubsection}{\numberline {4.2.3.3}Unified model and planning variational lower bound}{75}{subsubsection.4.2.3.3}%
\contentsline {subsubsection}{\numberline {4.2.3.4}Dynamically switching between MFRL and MBRL}{76}{subsubsection.4.2.3.4}%
\contentsline {section}{\numberline {4.3}World models}{76}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Generative world models}{76}{subsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.1.1}Observation-space world models}{76}{subsubsection.4.3.1.1}%
\contentsline {subsubsection}{\numberline {4.3.1.2}Factored models}{76}{subsubsection.4.3.1.2}%
\contentsline {subsubsection}{\numberline {4.3.1.3}Latent-space world models}{76}{subsubsection.4.3.1.3}%
\contentsline {subsubsection}{\numberline {4.3.1.4}Dreamer}{77}{subsubsection.4.3.1.4}%
\contentsline {subsubsection}{\numberline {4.3.1.5}Iris}{79}{subsubsection.4.3.1.5}%
\contentsline {subsection}{\numberline {4.3.2}Non-generative world models}{79}{subsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.2.1}Value prediction}{80}{subsubsection.4.3.2.1}%
\contentsline {subsubsection}{\numberline {4.3.2.2}Self prediction}{80}{subsubsection.4.3.2.2}%
\contentsline {subsubsection}{\numberline {4.3.2.3}Policy prediction}{81}{subsubsection.4.3.2.3}%
\contentsline {subsubsection}{\numberline {4.3.2.4}Observation prediction}{81}{subsubsection.4.3.2.4}%
\contentsline {subsubsection}{\numberline {4.3.2.5}Partial observation prediction}{81}{subsubsection.4.3.2.5}%
\contentsline {subsubsection}{\numberline {4.3.2.6}BYOL-Explore}{82}{subsubsection.4.3.2.6}%
\contentsline {section}{\numberline {4.4}Beyond one-step models: predictive representations}{82}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}General value functions}{82}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Successor representations}{83}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Successor models}{84}{subsection.4.4.3}%
\contentsline {subsubsection}{\numberline {4.4.3.1}Learning SMs}{85}{subsubsection.4.4.3.1}%
\contentsline {subsubsection}{\numberline {4.4.3.2}Jumpy models using geometric policy composition}{86}{subsubsection.4.4.3.2}%
\contentsline {subsection}{\numberline {4.4.4}Successor features}{86}{subsection.4.4.4}%
\contentsline {subsubsection}{\numberline {4.4.4.1}Generalized policy improvement}{87}{subsubsection.4.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.4.2}Option keyboard}{87}{subsubsection.4.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.4.3}Learning SFs}{87}{subsubsection.4.4.4.3}%
\contentsline {subsubsection}{\numberline {4.4.4.4}Choosing the tasks}{88}{subsubsection.4.4.4.4}%
\contentsline {chapter}{\numberline {5}Other topics in RL}{89}{chapter.5}%
\contentsline {section}{\numberline {5.1}Distributional RL}{89}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Quantile regression methods}{89}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Replacing regression with classification}{89}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}Reward functions}{90}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Reward hacking}{90}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Sparse reward}{90}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Reward shaping}{90}{subsection.5.2.3}%
\contentsline {subsection}{\numberline {5.2.4}Intrinsic reward}{91}{subsection.5.2.4}%
\contentsline {subsubsection}{\numberline {5.2.4.1}Knowledge-based intrinsic motivation}{91}{subsubsection.5.2.4.1}%
\contentsline {subsubsection}{\numberline {5.2.4.2}Goal-based intrinsic motivation}{92}{subsubsection.5.2.4.2}%
\contentsline {section}{\numberline {5.3}Hierarchical RL}{92}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Feudal (goal-conditioned) HRL}{92}{subsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.1.1}Hindsight Experience Relabeling (HER)}{93}{subsubsection.5.3.1.1}%
\contentsline {subsubsection}{\numberline {5.3.1.2}Hierarchical HER}{93}{subsubsection.5.3.1.2}%
\contentsline {subsubsection}{\numberline {5.3.1.3}Learning the subgoal space}{94}{subsubsection.5.3.1.3}%
\contentsline {subsection}{\numberline {5.3.2}Options}{94}{subsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.2.1}Definitions}{94}{subsubsection.5.3.2.1}%
\contentsline {subsubsection}{\numberline {5.3.2.2}Learning options}{95}{subsubsection.5.3.2.2}%
\contentsline {section}{\numberline {5.4}Imitation learning}{96}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Imitation learning by behavior cloning}{96}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Imitation learning by inverse reinforcement learning}{96}{subsection.5.4.2}%
\contentsline {subsection}{\numberline {5.4.3}Imitation learning by divergence minimization}{97}{subsection.5.4.3}%
\contentsline {section}{\numberline {5.5}Offline RL}{97}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Offline model-free RL}{98}{subsection.5.5.1}%
\contentsline {subsubsection}{\numberline {5.5.1.1}Policy constraint methods}{98}{subsubsection.5.5.1.1}%
\contentsline {subsubsection}{\numberline {5.5.1.2}Behavior-constrained policy gradient methods}{99}{subsubsection.5.5.1.2}%
\contentsline {subsubsection}{\numberline {5.5.1.3}Uncertainty penalties}{99}{subsubsection.5.5.1.3}%
\contentsline {subsubsection}{\numberline {5.5.1.4}Conservative Q-learning and pessimistic value functions}{99}{subsubsection.5.5.1.4}%
\contentsline {subsection}{\numberline {5.5.2}Offline model-based RL}{100}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Offline RL using reward-conditioned sequence modeling}{100}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Hybrid offline/online methods}{100}{subsection.5.5.4}%
\contentsline {section}{\numberline {5.6}LLMs and RL}{101}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}RL for LLMs}{101}{subsection.5.6.1}%
\contentsline {subsubsection}{\numberline {5.6.1.1}RLHF}{101}{subsubsection.5.6.1.1}%
\contentsline {subsubsection}{\numberline {5.6.1.2}Assistance game}{102}{subsubsection.5.6.1.2}%
\contentsline {subsubsection}{\numberline {5.6.1.3}Run-time inference as MPC}{102}{subsubsection.5.6.1.3}%
\contentsline {subsection}{\numberline {5.6.2}LLMs for RL}{102}{subsection.5.6.2}%
\contentsline {subsubsection}{\numberline {5.6.2.1}LLMs for pre-processing the input}{103}{subsubsection.5.6.2.1}%
\contentsline {subsubsection}{\numberline {5.6.2.2}LLMs for rewards}{103}{subsubsection.5.6.2.2}%
\contentsline {subsubsection}{\numberline {5.6.2.3}LLMs for world models}{104}{subsubsection.5.6.2.3}%
\contentsline {subsubsection}{\numberline {5.6.2.4}LLMs for policies}{104}{subsubsection.5.6.2.4}%
\contentsline {section}{\numberline {5.7}General RL, AIXI and universal AGI}{105}{section.5.7}%
