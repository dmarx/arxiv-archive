{
  "arxivId": "2412.04619",
  "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
  "authors": "Tian Qin, Naomi Saphra, David Alvarez-Melis",
  "abstract": "Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.",
  "url": "https://arxiv.org/abs/2412.04619",
  "issue_number": 164,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/164",
  "created_at": "2024-12-22T17:55:19Z",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_minutes": 0,
  "last_read": null
}