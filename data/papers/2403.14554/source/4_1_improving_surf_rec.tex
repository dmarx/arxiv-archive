% \subsubsection{Improving surface reconstruction.} 

We improve the surface reconstruction method from SuGaR by proposing a way to automatically adjust the hyperparameter of the Poisson surface reconstruction~\cite{kazhdan-2006-poissonsurfacereconstruction} stage.

Poisson surface reconstruction first recovers an underlying occupancy field $\chi:\IR^3\mapsto [0,1]$
and applies a marching algorithm on $\chi$, which allows for a much better mesh reconstruction than the density function. This approach allows for high scalability as the marching algorithm is applied only in voxels located close to the point cloud. 

To estimate $\chi$, Poisson surface reconstruction discretizes the scene into $2^D \times 2^D \times 2^D$ cells by adapting an octree with depth $D\in \IN$ to the input samples.  $D$ is a hyperparameter provided by the user: The higher $D$, the higher the resolution of the mesh.

By default, SuGaR~\cite{guedon2023sugar} uses a large depth $D=10$ for any scene, as it guarantees a high level of details. However, if the resolution is too high with respect to the complexity of the geometry and the size of the details in the scene, the shapes of the Gaussians become visible as ellipsoidal bumps on the surface of the mesh, and create incorrect bumps or self-intersections. More importantly, holes can also appear in the geometry when $D$ is too large with regards to the density of the Gaussians and the sampled point cloud.

% However, depending on the spatial extent of the scene, on the complexity of the geometry, or even on the distance between the camera and the objects to reconstruct, a parameter $D$ that is too large can produce artifacts in the scene, as shown in Figure~\ref{fig:mesh-comparison} and mentioned in GitHub issues of SuGaR's implementation. 


We therefore introduce a method to automatically select $D$. A simple strategy would be to adjust the depth of the octree such that the size of a cell is approximately equal or larger than the average size of the Gaussians in the scene, normalized by the spatial extent of the point cloud used for reconstruction. Unfortunately, this does not work well in practice: We found that whatever the scene (real or synthetic) or the number of Gaussians to represent it, Gaussian Splatting optimization systematically converges toward a varied collection of Gaussian sizes, so that there is no noticeable difference or pattern in the distribution of sizes between scenes.

We noticed that the distance between Gaussians is much more representative of the geometrical complexity of the scene and thus a reliable cue to fix $D$. Indeed, a large but very detailed shape can be reconstructed using Gaussians with large size, if these Gaussians are close to each other. On the contrary, whatever their size, if the centers of the Gaussian are too far from each other, then the rendered geometry will look rough. 

Consequently, to first evaluate the geometrical complexity of a scene, we propose to compute, for each Gaussian $g$ in the scene, the distance between $g$ and its nearest neighbor Gaussian. We use these distances to define the following geometrical complexity score $CS$:
%
\begin{equation}
    CS = Q_{0.1} \left( \left\{\min_{g'\neq g} \frac{\|\mu_g - \mu_{g'}\|_2}{L}\right\}_{g\in \calG} \right) \> ,
    \label{eq:complexity_score}
\end{equation}
%
where $\calG$ is the set of all 3D Gaussians in the scene, $L$ is the length of the longest edge of the bounding box of the point cloud to use in Poisson reconstruction, and $Q_{0.1}$ is the function that returns the 0.1-quantile of a list. We use the 0.1-quantile rather than the average because Gaussians that have a neighbor close to them generally encode details in the scene, which provide a much more reliable and less noisy criterion than using the overall average. We also use a quantile rather than a minimum to be robust to extreme values. In short, this complexity score $CS$ is a canonical distance between the closest Gaussians in the scene, i.e., the distance between neighbor Gaussians that reconstruct details in the scene. 

% \input{figures/complexity_score}

Since the normalized length of a cell in the octree is $2^{-\bar{D}}$ and this score represents a canonical normalized distance between Gaussians representing details in the scene, we can compute a natural optimal depth $\bar{D}$ for the Poisson reconstruction algorithm:
%
\begin{equation}
    \bar{D} = \lfloor -\log_2 \left(\gamma \times CS\right) \rfloor \> ,
    \label{eq:optimal-depth}
\end{equation}
%
where $\gamma > 0$ is a hyperparameter that does not depend on the scene and its geometrical complexity. This formula guarantees that the size of the cells is as close as possible but greater than $\gamma \times CS$. Decreasing the value of $\gamma$ increases the resolution of the reconstruction. But for a given $\gamma$, whatever the dataset or the complexity of the scene, this formula enforces the scene to be reconstructed with a similar level of smoothness. 

Choosing $\gamma$ is therefore much easier than having to tune $D$ as it is not dependent on the scene. 
In practice, we use $\gamma=100$ for all the scenes. Our experiments validates that this method to fix $D$ results in greater rendering performance.

