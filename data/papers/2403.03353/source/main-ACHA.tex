\documentclass[11pt]{article}

\usepackage{bm}
\usepackage{amsmath,amsfonts,amssymb,amsthm,enumerate}
%
\usepackage{color}
\usepackage[margin=1in]{geometry}
\usepackage{mathrsfs}
\usepackage{hyperref}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}

\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}
 


\begin{document}


\title{Hypothesis Spaces for Deep Learning}

\author{Rui Wang\thanks{School of Mathematics, Jilin University, Changchun 130012, P. R. China. E-mail address: {\it rwang11@jlu.edu.cn}.}, \quad Yuesheng Xu\thanks{Department of Mathematics and Statistics, Old Dominion University, Norfolk, VA 23529, USA. This author is also a Professor Emeritus of Mathematics, Syracuse University, Syracuse, NY 13244, USA. E-mail address: {\it y1xu@odu.edu.} All correspondence should be sent to this author.} \quad and \quad Mingsong Yan\thanks{
Department of Mathematics and Statistics, Old Dominion University, Norfolk, VA 23529, USA. E-mail address: {\it myan007@odu.edu}.}}
\date{}

\maketitle

\begin{abstract}
This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel.
\end{abstract}


\textbf{Key words}:
    Reproducing kernel Banach space, deep learning, deep neural network, representer theorem for deep learning

\section{Introduction}
Deep learning has been a huge success in applications. Mathematically, its success is due to the use of deep neural networks (DNNs), neural networks of multiple layers, to describe decision functions. 
Various mathematical aspects of DNNs as an approximation tool were investigated recently in a number of studies \cite{daubechies2022nonlinear, huang2022error,li2024two, mhaskar2016deep, shen2022optimal, xu2022convergence, xu2024convergence, zhou2020universality}.
As pointed out in \cite{cucker2002mathematical}, learning processes do not take place in a vacuum. Classical learning methods took place in a reproducing kernel Hilbert space (RKHS) \cite{Aronszajn1950theory}, which leads to representation of learning solutions in terms of a combination of a finite number of kernel sessions \cite{scholkopf2001generalized} of a universal kernel \cite{micchelli2006universal}. Reproducing kernel Hilbert spaces as appropriate hypothesis spaces for classical learning methods provide a foundation for mathematical analysis of the learning methods. A natural and imperative question is what are appropriate hypothesis spaces for deep learning. Although hypothesis spaces for learning with shallow neural networks (networks of one hidden layer) were investigated recently in a number of studies, (e.g. \cite{bartolucci2023understanding, chung2023barron, parhi2021banach, shenouda2023vector}),  appropriate hypothesis spaces for deep learning are still absent.
The goal of the present study is to understand this imperative theoretical issue. %

The road-map of constructing the hypothesis space for deep learning may be described as follows.
We treat a DNN as a function of two variables, one being the physical variable and the other being the parameter variable. We then consider the set of the DNNs as functions of the physical variable for the parameter variable taking all elements of the set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. Upon completing the linear span of the DNN set in a weak* topology, we construct a Banach space of functions of the physical variable. We establish that the resulting Banach space is a reproducing kernel Banach space (RKBS), on which point-evaluation functionals are continuous, and construct an asymmetric reproducing kernel, for the space, which is a function of the two variables, the physical variable and the parameter variable. We regard the constructed RKBS as the hypothesis space for deep learning. We remark that when deep neural networks reduce to shallow network (having only one hidden layer), our hypothesis space coincides the space for shallow learning studied in \cite{bartolucci2023understanding}.

Upon introducing the hypothesis space for deep learning, we investigate two learning models, the regularized learning and minimum interpolation problem in the resulting RKBS. We establish representer theorems for solutions of the learning models by employing theory of the reproducing kernel Banach space developed in \cite{xu2022sparse, xu2019generalized,  zhang2009reproducing} and representer theorems for solutions of learning in a general RKBS established in \cite{ChengWangXu2023, wang2021representer, wang2023sparse}. %
Like the representer theorems for the classical learning in RKHSs, 
the resulting representer theorems for the two deep learning models in the RKBS reveal that although the learning models  are of infinite dimension, their solutions lay in finite dimensional manifolds. More specifically, they can be expressed as a linear combination of a finite number of kernel sessions, the reproducing kernel evaluated the parameter variable at points determined by given data. The representer theorems established in this paper is data-dependent. Even when deep neural networks reduce to a shallow network, the corresponding representer theorem is still new to our best acknowledge. The hypothesis space and the representer theorems for the two deep learning models in it provide us prosperous insights of deep learning and supply deep learning a sound mathematical foundation for further investigation.



We organize this paper in six sections. We describe in Section 2 an innate deep learning model with DNNs. Aiming at formulating reproducing kernel Banach spaces as hypothesis spaces for deep learning, in Section 3 we elucidate the notion of vector-valued reproducing kernel Banach spaces. Section 4 is entirely devoted to the development of the hypothesis space for deep learning. We specifically show that the completion of the linear span of the primitive DNN set, pertaining to the innate learning model, in a weak* topology is an RKBS, which constitutes the hypothesis space for deep learning. In Section 5, we study learning models in the RKBS, establishing representer theorems for solutions of two learning models (regularized learning and minimum norm interpolation) in the hypothesis space. We conclude this paper in Section 6 with remarks on advantages of learning in the proposed hypothesis space.

\section{Learning with Deep Neural Networks}
We describe in this section an innate learning model with DNNs, considered wildly in the machine learning community.  

We first recall the notation of DNNs.
Let $s$ and $t$ be positive integers. A DNN is a vector-valued function from $\mathbb{R}^s$ to $\mathbb{R}^t$ formed by compositions of functions, each of which is defined by an activation function applied to an affine map. Specifically, for a given univariate function $\sigma: \mathbb{R}\to\mathbb{R}$,
we define a vector-valued function  by 
\begin{equation*}\label{activationF}
\sigma({x}):=[\sigma(x_1),\dots,\sigma(x_s)]^\top, \ \ \mbox{for}\ \ {x}:=[x_1, x_2,\dots, x_s]^\top\in\mathbb{R}^s.
\end{equation*}
For each $n\in\mathbb{N}$, let $\mathbb{N}_n:=\{1,2,\ldots,n\}$. For $k$ vector-valued functions $f_j$, $j\in\mathbb{N}_k$, where the range of $f_j$ is contained in the domain of $f_{j+1}$, for $j\in\mathbb{N}_{k-1}$, we denote the consecutive composition of $f_j$, $j\in\mathbb{N}_k$, by
\begin{equation*}\label{consecutive_composition}
    \bigodot_{j=1}^k f_j:=f_k\circ f_{k-1}\circ\cdots\circ f_2\circ f_1,
\end{equation*}
whose domain is that of $f_1$. 
Suppose that $D\in \mathbb{N}$ is prescribed and fixed. Throughout this paper, we always let $m_0:=s$ and $m_D:=t$. We specify  positive integers $m_j$, $j\in \mathbb{N}_{D-1}$.
For $\mathbf{W}_j\in\mathbb{R}^{m_j\times m_{j-1}}$ and $\mathbf{b}_j\in\mathbb{R}^{m_j}$, $j\in\mathbb{N}_D$, 
a DNN is a function defined by
\begin{equation}\label{DNN}
\mathcal{N}^D({x}):=\left(\mathbf{W}_D\bigodot_{j=1}^{D-1} \sigma(\mathbf{W}_j \cdot+\mathbf{b}_j)+\mathbf{b}_D\right)({x}),\ \ {x}\in\mathbb{R}^s.
\end{equation}
Note that $x$ is the input vector and $\mathcal{N}^D$ has $D-1$ hidden layers and an output layer, which is the $D$-th layer.
%

A DNN may be represented in a recursive manner.
From definition \eqref{DNN}, a DNN can be defined recursively by
\begin{equation*}\label{Step1}
    \mathcal{N}^1({x}):=\mathbf{W}_1 {x}+\mathbf{b}_1, \ \ {x}\in \mathbb{R}^s
\end{equation*}
and
\begin{equation*}\label{Recursion}
    \mathcal{N}^{j+1}({x}):=\mathbf{W}_{j+1}\sigma(\mathcal{N}^j({x}))+\mathbf{b}_{j+1}, \ \ {x}\in \mathbb{R}^s, \ \ \mbox{for all} \ \ j\in \mathbb{N}_{D-1}.
\end{equation*}
We write $\mathcal{N}^D$ as $\mathcal{N}^D(\cdot,\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D)$ when it is necessary to indicate the dependence of DNNs on the parameters. In this paper, when we write the set $\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D$ associated with the neural network $\mathcal{N}^D$, we implicitly give it the order inherited from the definition of $\mathcal{N}^D$. Throughout this paper, we assume that the activation function $\sigma$ is continuous.

%
It is advantageous to consider the DNN $\mathcal{N}^D$ defined above as a function of two variables, one being the physical variable $x\in \mathbb{R}^s$ and the other being the parameter variable $\theta:=\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D$.
Given positive integers $m_j$, $j\in \mathbb{N}_{D-1}$, %
we let
\begin{equation}\label{width-set}
    \mathbb{W}:=\{m_j: j\in\mathbb{N}_{D-1}\}
\end{equation} 
denote the width set  and 
define the primitive set of DNNs of $D$ layers by
\begin{equation}\label{Set_A}
    \mathcal{A}_{\mathbb{W}}:=\left\{ \mathcal{N}^D(\cdot,\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D): \mathbf{W}_j\in \mathbb{R}^{m_j\times m_{j-1}},  \mathbf{b}_j\in\mathbb{R}^{m_j}, j\in\mathbb{N}_D\right\}.
\end{equation}
%
%
%
Clearly, the set $\mathcal{A}_{\mathbb{W}}$ defined by \eqref{Set_A} depends not only on $\mathbb{W}$ but also on $D$. For the sake of simplicity, we will not indicate the dependence on $D$ in our notation when ambiguity is not caused. For example, we will use $\mathcal{N}$ for $\mathcal{N}^D$.
Moreover, an element of $\mathcal{A}_{\mathbb{W}}$ is a vector-valued function mapping from $\mathbb{R}^s$ to  $\mathbb{R}^t$. We shall understand the set $\mathcal{A}_{\mathbb{W}}$. To this end,
we define the parameter space $\Theta$ by letting
\begin{equation}\label{Def:Theta}
\Theta=\Theta_{\mathbb{W}}:= \bigotimes_{j\in\mathbb{N}_D}(\mathbb{R}^{m_j\times m_{j-1}}\otimes \mathbb{R}^{m_j}).
\end{equation}
Note that $\Theta$ is measurable. 
For ${x}\in \mathbb{R}^s$ and $\theta\in \Theta$, we define
\begin{equation}\label{Def:kernel}
    \mathcal{N}({x},\theta):=\mathcal{N}^D({x},\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D).
\end{equation}
For ${x}\in \mathbb{R}^s$ and $\theta\in \Theta$, there holds $\mathcal{N}({x},\theta)\in \mathbb{R}^t$. In this notation, set $\mathcal{A}_{\mathbb{W}}$ may be written as
\begin{equation*}\label{Set_A*}
    \mathcal{A}_{\mathbb{W}}=\left\{ \mathcal{N}(\cdot,\theta): \theta\in \Theta_{\mathbb{W}} \right\}.
\end{equation*}
%

We now describe the innate learning model with DNNs.
Suppose that a training dataset 
\begin{equation}\label{Dataset}
\mathbb{D}_m:=  \{(x_j,y_j) \in\mathbb{R}^s\times\mathbb{R}^t:j\in\mathbb{N}_m\}
\end{equation}
is given and we would like to train a neural network from the dataset. We denote by $\mathcal{L}(\mathcal{N},\mathbb{D}_m): \Theta\to \mathbb{R}$ a loss function determined by the dataset $\mathbb{D}_m$. For example, a loss function may take the form
$$
\mathcal{L}(\mathcal{N},\mathbb{D}_m)(\theta):=\sum_{j\in\mathbb{N}_m}\|\mathcal{N}(x_j,\theta)-y_j\|,
$$
where $\|\cdot\|$ is a norm of $\mathbb{R}^t$.
Given a loss function, a typical deep learning model is to train the parameters $\theta\in \Theta_\mathbb{W}$ from the training dataset $\mathbb{D}_m$ by solving the optimization problem 
\begin{equation}\label{BasicLearningMethod}
    \min\{\mathcal{L}(\mathcal{N},\mathbb{D}_m)(\theta): \theta\in\Theta_\mathbb{W}\},
\end{equation}
where $\mathcal{N}$ has the form in equation \eqref{Def:kernel}. Equivalently, optimization problem \eqref{BasicLearningMethod} may be written as
\begin{equation}\label{BasicLearningMethod-equivalent}
    \min\{\mathcal{L}(\mathcal{N},\mathbb{D}_m): \mathcal{N}\in\mathcal{A}_{\mathbb{W}}\}.
\end{equation}
Model \eqref{BasicLearningMethod-equivalent} is an innate learning model considered wildly in the machine learning community.
Note that the set $\mathcal{A}_{\mathbb{W}}$ lacks either algebraic or topological structures.
It is difficult to conduct mathematical analysis for learning model \eqref{BasicLearningMethod-equivalent}. Even the existence of its solution is not guaranteed.  

We introduce a vector space that contains  $\mathcal{A}_{\mathbb{W}}$ and consider learning in the vector space. 
For this purpose, given a set $\mathbb{W}$  of weight widths defined by \eqref{width-set}, we define the set
\begin{equation}\label{space B Delta}
\mathcal{B}_{\mathbb{W}}:=\left\{ \sum_{l=1}^n c_l\mathcal{N}(\cdot,\theta_l): c_l\in\mathbb{R}, \theta_l\in \Theta_{\mathbb{W}}, l\in \mathbb{N}_n, n\in \mathbb{N}\right\}.
\end{equation}
In the next proposition, we present properties of $\mathcal{B}_{\mathbb{W}}$.

\begin{proposition}\label{prop: BM is a linear space}
If $\mathbb{W}$ is the width set defined by \eqref{width-set}, then 

(i) $\mathcal{B}_{\mathbb{W}}$ defined by \eqref{space B Delta} is the smallest vector space on $\mathbb{R}$ that contains the set $\mathcal{A}_{\mathbb{W}}$, 

(ii) $\mathcal{B}_{\mathbb{W}}$ is of infinite dimension,

(iii) $\mathcal{B}_{\mathbb{W}}\subset\bigcup_{n\in\mathbb{N}}\mathcal{A}_{n\mathbb{W}}$. 
\end{proposition}
\begin{proof}
It is clear that $\mathcal{B}_{\mathbb{W}}$ may be identified as the linear span of $\mathcal{A}_{\mathbb{W}}$, that is, 
$$
\mathcal{B}_{\mathbb{W}}=\text{span} \left\{\mathcal{N}(\cdot,\theta): \theta\in \Theta_{\mathbb{W}}\right\}.
$$
Thus, $\mathcal{B}_{\mathbb{W}}$ is the smallest vector space containing $\mathcal{A}_{\mathbb{W}}$. Item (ii) follows directly from the definition \eqref{space B Delta} of $\mathcal{B}_{\mathbb{W}}$.

It remains to prove Item (iii).
To this end, we let $f\in\mathcal{B}_{\mathbb{W}}$. By the definition \eqref{space B Delta} of $\mathcal{B}_{\mathbb{W}}$, there exist $n'\in \mathbb{N}$, $c_l\in\mathbb{R}$, $\theta_l\in \Theta_\mathbb{W}$, for $l\in \mathbb{N}_{n'}$ such that 
$$
f(\cdot)=\sum_{l=1}^{n'} c_l\mathcal{N}(\cdot,\theta_l).
$$
It suffices to show that $f\in\mathcal{A}_{n'\mathbb{W}}$.
Noting that $\theta_l:=\{\mathbf{W}_j^l,\mathbf{b}_j^l\}_{j=1}^D$, for  $l\in\mathbb{N}_{n'}$,
we set 
\begin{equation*}
\widetilde{\mathbf{W}}_{1}:=\left[\begin{array}{c}
             \mathbf{W}_1^1  \\
              \mathbf{W}_1^2\\
              \vdots\\
              \mathbf{W}_1^{n'}
\end{array}\right],\ \
\widetilde{\mathbf{W}}_j:=\left[\begin{array}{cccc}
\mathbf{W}_j^1 & \mathbf{0} & \cdots & \mathbf{0} \\
\mathbf{0} & \mathbf{W}_j^2 & \cdots & \mathbf{0} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{W}_j^{n'}
\end{array}\right],\ j\in\mathbb{N}_{D-1}\backslash\{1\}, 
\ \
\widetilde{\mathbf{b}}_{j}:=\left[\begin{array}{c}
             \mathbf{b}_j^1  \\
              \mathbf{b}_j^2\\
              \vdots\\
              \mathbf{b}_j^{n'}
\end{array}\right],\ j\in\mathbb{N}_{D-1},
\end{equation*}
and 
\begin{equation*}
\widetilde{\mathbf{W}}_D:=\left[\begin{array}{cccc}
c_1\mathbf{W}_D^1 & c_2\mathbf{W}_D^2 & \cdots & c_{n'}\mathbf{W}_D^{n'}
\end{array}\right],\quad \widetilde{\mathbf{b}}_{D}:=\sum\limits_{j=1}^{n'} c_j \mathbf{b}_D^j. 
\end{equation*}
Clearly, we have that
$
\widetilde{\mathbf{W}}_{1}\in\mathbb{R}^{(n'm_1)\times {m_0}}$, $\widetilde{\mathbf{b}}_{j}\in\mathbb{R}^{n'm_j}$, $j\in\mathbb{N}_{D-1}$,
$\widetilde{\mathbf{W}}_j\in\mathbb{R}^{(n'm_j)\times (n'm_{j-1})}$, $j\in\mathbb{N}_{D-1}\backslash\{1\}$, 
$\widetilde{\mathbf{W}}_D\in\mathbb{R}^{m_D\times (n'm_{D-1})}$, and $\widetilde{\mathbf{b}}_{D}\in\mathbb{R}^{m_D}$. 
Direct computation confirms that $f(\cdot)=\mathcal{N}(\cdot,\widetilde{\theta})$
with $\widetilde{\theta}:=\{\widetilde{\mathbf{W}}_j,\widetilde{\mathbf{b}}_j\}_{j=1}^D$. By definition \eqref{Set_A}, $f\in\mathcal{A}_{n'\mathbb{W}}$. 
\end{proof}

Proposition \ref{prop: BM is a linear space} reveals that $\mathcal{B}_{\mathbb{W}}$ is the smallest vector space that contains $\mathcal{A}_{\mathbb{W}}$. Hence, it is a reasonable substitute of  $\mathcal{A}_{\mathbb{W}}$.
Motivated by Proposition \ref{prop: BM is a linear space}, we propose the following alternative learning model
\begin{equation}\label{GeneralLearningMethod}
    \inf\{\mathcal{L}(\mathcal{N},\mathbb{D}_m): \mathcal{N}\in\mathcal{B}_{\mathbb{W}}\}.
\end{equation}
For a given width set $\mathbb{W}$, unlike learning model \eqref{BasicLearningMethod-equivalent} which searches a minimizer in set $\mathcal{A}_\mathbb{W}$, learning model \eqref{GeneralLearningMethod} seeks a minimizer in the vector space $\mathcal{B}_\mathbb{W}$, which contains  $\mathcal{A}_\mathbb{W}$ and is contained in $\mathcal{A}:=\bigcup_{n\in\mathbb{N}}\mathcal{A}_{n\mathbb{W}}$. 
According to Proposition \ref{prop: BM is a linear space}, learning model \eqref{GeneralLearningMethod} is ``semi-equivalent" to learning model \eqref{BasicLearningMethod-equivalent} in the sense that 
\begin{equation}\label{Comparison-of-three-models}
    \mathcal{L}(\mathcal{N}_{\mathcal{A}},\mathbb{D}_m)\leq \mathcal{L}(\mathcal{N}_{\mathcal{B}_{\mathbb{W}}},\mathbb{D}_m)\leq \mathcal{L}(\mathcal{N}_{\mathcal{A}_{\mathbb{W}}},\mathbb{D}_m),
\end{equation}
where $\mathcal{N}_{\mathcal{B}_{\mathbb{W}}}$ is a minimizer of  model \eqref{GeneralLearningMethod}, $\mathcal{N}_{\mathcal{A}_{\mathbb{W}}}$ and $\mathcal{N}_{\mathcal{A}}$ are the minimizers of model \eqref{BasicLearningMethod-equivalent} and model \eqref{BasicLearningMethod-equivalent} with the set $\mathcal{A}_{\mathbb{W}}$ replaced by $\mathcal{A}$, respectively. One might argue that since model \eqref{BasicLearningMethod-equivalent} is a finite dimension optimization problem while model \eqref{GeneralLearningMethod} is an infinite dimensional one, the alternative model  \eqref{GeneralLearningMethod} may add unnecessary complexity to the original model. Although model \eqref{GeneralLearningMethod} is of infinite dimension, the algebraic structure of the vector space $\mathcal{B}_{\mathbb{W}}$ and its topological structure that will be equipped later provide us with great advantages for mathematical analysis of learning on the space.
As a matter of fact, the vector-valued RKBS to be obtained by completing the vector space $\mathcal{B}_{\mathbb{W}}$ in a weak* topology will lead to the miraculous representer theorem, of the learned solution, which reduces the infinite dimensional optimization problem to a finite dimension one. This addresses the challenges caused by the infinite dimension of the space $\mathcal{B}_{\mathbb{W}}$.



%

%



\section{Vector-Valued Reproducing Kernel Banach Space}
It was proved in the last section that for a given width set $\mathbb{W}$, the set $\mathcal{B}_{\mathbb{W}}$ defined by \eqref{space B Delta} is the smallest vector space that contains the primitive set $\mathcal{A}_\mathbb{W}$. One of the aims of this paper is to establish that the vector space $\mathcal{B}_{\mathbb{W}}$ is dense in a weak* topology in a vector-valued RKBS. For this purpose, in this section we describe the notion of vector-valued RKBSs. 

A Banach space $\mathcal{B}$ with the norm $\|\cdot\|_{\mathcal{B}}$ is called a space of vector-valued functions on a prescribed set $X$ if $\mathcal{B}$ is composed of vector-valued functions defined on $X$ and for each $f\in\mathcal{B}$, $\|f\|_{\mathcal{B}}=0$ implies that $f({x}) = \mathbf{0}$ for all ${x}\in X$. 
For each ${x}\in X$, we define the point evaluation operator $\delta_{{x}}:\mathcal{B}\to\mathbb{R}^n$ as 
\begin{equation*}
    \delta_{{x}}(f):=f({x}), \quad f\in\mathcal{B}. 
\end{equation*}

We provide the definition of vector-valued RKBSs below.

\begin{definition}\label{def: vector-valued RKBS} 
A Banach space $\mathcal{B}$ of vector-valued functions from $X$ to $\mathbb{R}^n$ is called a vector-valued RKBS if there exists a norm $\|\cdot\|$ of $\mathbb{R}^n$ such that for each $x\in X$, the point evaluation operator $\delta_x$ is continuous with respect to the norm $\|\cdot\|$ of $\mathbb{R}^n$ on $\mathcal{B}$, that is, for each $x\in X$, there exists a constant $C_x>0$ %
such that 
\[
\|\delta_x(f)\|\leq C_x\|f\|_{\mathcal{B}}, \quad\text{ for all }\ f\in\mathcal{B}.
\]
\end{definition}

Note that since all norms of  $\mathbb{R}^n$ are equivalent, if a Banach space $\mathcal{B}$ of vector-valued functions from $X$ to $\mathbb{R}^n$ is a vector-valued RKBS with respect to a norm  of $\mathbb{R}^n$, then it must be a vector-valued RKBS with respect to any other norm  of  $\mathbb{R}^n$. Thus, the property of point evaluation operators being continuous on space $\mathcal{B}$ is independent of the choice of the norm of the output space $\mathbb{R}^n$. 

The notion of RKBSs was originally introduced in \cite{zhang2009reproducing}, to guarantee the stability of sampling process and to serve as a hypothesis space for sparse machine learning. Vector-valued RKBSs were studied in \cite{lin2021multi, zhang2013vector}, in which the definition of the vector-valued RKBS involves an abstract Banach space, with a specific norm, as the output space of functions. In Definition \ref{def: vector-valued RKBS}, we limit the output space to the Euclidean space $\mathbb{R}^n$ without specifying a norm, due to the special property that norms on $\mathbb{R}^n$ are all equivalent. 


%
We reveal in the next proposition that point evaluation operators are continuous if and only if component-wise point evaluation functionals are continuous. To this end, for a vector-valued function $f:X\to\mathbb{R}^n$, for each $j\in\mathbb{N}_n$, we denote by $f_j:X\to\mathbb{R}$ the $j$-th component of $f$, that is, 
\begin{equation*}
    f({x}):=[f_j({x}):j\in\mathbb{N}_n]^\top,\quad {x}\in X. 
\end{equation*}
Moreover, for each $x\in X$, $k\in\mathbb{N}_n$, we introduce a linear functional $\delta_{{x},k}:\mathcal{B}\to\mathbb{R}$ by 
\begin{equation*}
    \delta_{{x},k}(f):=f_k({x}),\ \mbox{for}\  f:=[f_k:k\in\mathbb{N}_n]^{\top}\in\mathcal{B}. 
\end{equation*}
\begin{proposition}\label{prop: component wise for vector-valued RKBS}
    A Banach space $\mathcal{B}$ of vector-valued functions from $X$ to $\mathbb{R}^n$ is a vector-valued RKBS if and only if for each ${x}\in X$, $k\in \mathbb{N}_n$, there exists a constant $C_{{x},k}>0$ such that 
    \begin{equation}\label{elementwise functionals}
        |\delta_{{x},k}(f)|\leq C_{{x},k}\|f\|_{\mathcal{B}}, \ \mbox{for all} \ f\in \mathcal{B}.
    \end{equation}
\end{proposition}
\begin{proof}
Suppose that $\mathcal{B}$ is a vector-valued RKBS. Definition \ref{def: vector-valued RKBS} together with the norm equivalence of the Euclidean space $\mathbb{R}^n$ ensures that for each  $x\in X$, point evaluation operator $\delta_x$ is continuous with respect to the maximum norm $\|\cdot\|_{\infty}$ of $\mathbb{R}^n$. That is, for each ${x}\in X$, there exists a positive constant $C_x$ such that $\|f(x)\|_{\infty}\leq C_{{x}}\|f\|_{\mathcal{B}}$ for all $f\in\mathcal{B}$. Hence, for each $x\in X$, $k\in\mathbb{N}_n$, there holds that $\left|\delta_{x,k}(f)\right|=|f_k(x)|\leq \|f(x)\|_{\infty}\leq C_x\|f\|_{\mathcal{B}}$ for all $f\in\mathcal{B}$. 
That is, inequality \eqref{elementwise functionals} holds true with $C_{{x},k}:=C_x$.

Conversely, we assume that for each ${x}\in X$, $k\in \mathbb{N}_n$, there exists $C_{{x},k}>0$ such that  \eqref{elementwise functionals} holds. Note that for each $x\in X$, there holds $\|\delta_x(f)\|_{\infty}=\max_{k\in\mathbb{N}_n}|f_k(x)|$ for all $f\in\mathcal{B}$. By setting $C_x:=\max_{k\in\mathbb{N}_n}C_{x,k}$, we get from inequality \eqref{elementwise functionals} that $\|\delta_x(f)\|_{\infty}\leq C_{x}\|f\|_{\mathcal{B}}$ for all $f\in\mathcal{B}$. That is, for each $x\in X$, the point evaluation operator $\delta_x$ is continuous with respect to the maximum norm $\|\cdot\|_{\infty}$ of $\mathbb{R}^n$, and thus, by Definition \ref{def: vector-valued RKBS}, $\mathcal{B}$ is a vector-valued RKBS.
\end{proof}

We next identify a reproducing kernel for a vector-valued RKBS. We need the notion of the $\delta$-dual space of a vector-valued RKBS.  For a Banach space $B$ with a norm $\|\cdot\|_{B}$, we denote by $B^*$ the dual space of $B$, which is composed of all continuous linear functionals on $B$ endowed with the norm 
$$
\|\nu\|_{B^*}:=\sup_{\|f\|_{B}\leq1}|\nu(f)|,\ \mbox{for all}\ \nu\in B^*.
$$
The dual bilinear form $\langle\cdot,\cdot\rangle_{B}$ on $B^*\times B$ is defined by 
$$
\langle\nu,f\rangle_{B}:=\nu(f),\ \mbox{for all}\ \nu\in B^* \  \mbox{and}\ f\in B.
$$
Suppose that $\mathcal{B}$ is a vector-valued RKBS of functions from $X$ to $\mathbb{R}^n$, with the dual space $\mathcal{B}^*$. We set 
\begin{equation}\label{Def:Delta}
    \Delta:=\mathrm{span}\{\delta_{x,j}:x\in X, j\in\mathbb{N}_n\}.
\end{equation}
Proposition \ref{prop: component wise for vector-valued RKBS} reveals that for each $x\in X$ and $j\in\mathbb{N}_n$, $\delta_{x,j}$ is a continuous linear functional on $\mathcal{B}$. As a result, $\Delta$ is a subset of $\mathcal{B}^*$. We denote by $\mathcal{B}'$ the closure of $\Delta$ in the norm topology on $\mathcal{B}^*$ and  call it the $\delta$-dual space of $\mathcal{B}$. Clearly, we have that $\mathcal{B}'\subseteq\mathcal{B}^*$ and $\mathcal{B}'$ is the smallest Banach space that contains all point-evaluation functionals on $\mathcal{B}$. We remark that the $\delta$-dual space of a scalar-valued RKBS was originally introduced in \cite{xu2022sparse}.

We identify in the next proposition a reproducing kernel for the vector-valued RKBS $\mathcal{B}$ under the assumption that the $\delta$-dual space $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions from a set $X'$ to $\mathbb{R}$. 
%
%
%
%

\begin{proposition}\label{existence of reproducing kernel}
Suppose that $\mathcal{B}$ is a vector-valued RKBS of functions from $X$ to $\mathbb{R}^n$ and its $\delta$-dual space $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions from $X'$ to $\mathbb{R}$, then there exists a unique vector-valued function $K:X\times X'\to\mathbb{R}^n$ such that $K_j(x,\cdot)\in \mathcal{B}'$ for all $x\in X$, $j\in\mathbb{N}_n$, and
\begin{equation}\label{def: reproducing property}
    f_j(x)=\langle K_j(x,\cdot),f\rangle_{\mathcal{B}},\ \mbox{for all}\ f=[f_j:j\in\mathbb{N}_n]\in\mathcal{B} \ \mbox{and all}\  x\in X, \ j\in\mathbb{N}_n.
\end{equation}
\end{proposition}

\begin{proof}
Since $\mathcal{B}$ is a vector-valued RKBS, Proposition \ref{prop: component wise for vector-valued RKBS} ensures that for each $x\in X$, $j\in\mathbb{N}_n$, $\delta_{x,j}$ is a continuous linear functional on $\mathcal{B}$. By noting that $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions from $X'$ to $\mathbb{R}$, there exists $k_{x,j}\in\mathcal{B}'$ such that \begin{equation}\label{K_xj}
    \delta_{x,j}(f)=\langle k_{x,j},f\rangle_{\mathcal{B}}, \ \mbox{for all}\ f\in\mathcal{B}.
    \end{equation}
By defining the $j$-th component of the vector-valued function $K:X\times X'\to\mathbb{R}^n$ by
\begin{equation}\label{def-K}
   K_j(x,x'):=k_{x,j}(x'),\ x\in X, \ x'\in X',\ j\in\mathbb{N}_n, 
\end{equation}
we have that $K_j(x,\cdot)\in\mathcal{B}'$ for all $x\in X$ and $j\in\mathbb{N}_n$. Substituting equation \eqref{def-K} into the right-hand side of equation \eqref{K_xj} with noting that $\delta_{x,j}(f)=f_j(x)$, we obtain equation \eqref{def: reproducing property}. 

It remains to verify that the function $K$ on $X\times X'$ satisfying the above properties is unique. Assume that there exists another $\widetilde{K}:X\times X'\rightarrow \mathbb{R}^n$ such that $\widetilde{K}_j(x,\cdot)\in \mathcal{B}'$ for all $x\in X$, $j\in\mathbb{N}_n$, and $f_j(x)=\langle \widetilde{K}_j(x,\cdot),f\rangle_{\mathcal{B}}$, for all $f\in\mathcal{B}$, $x\in X$, and $j\in\mathbb{N}_n$. It follows from the above equation and equation \eqref{def: reproducing property} that 
$\langle K_j(x,\cdot)-\widetilde{K}_j(x,\cdot),f\rangle_{\mathcal{B}}=0$, for all $f\in\mathcal{B}$ and all $x\in X$.
That is, for all $x\in X$, $j\in\mathbb{N}_n$, $K_j(x,\cdot)-\widetilde{K}_j(x,\cdot)=0$. Noting that $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions on $X'$, we conclude that $K_j(x,x')-\widetilde{K}_j(x,x')=0$ for all $x\in X$, $x'\in X'$, and $j\in\mathbb{N}_n$, which is equivalent to $K=\widetilde{K}$.
%
\end{proof}
  
We call the vector-valued function $K:X\times X'\rightarrow\mathbb{R}^n$ that satisfies $K_j(x,\cdot)\in\mathcal{B}'$ for all $x\in X$, $j\in\mathbb{N}_n$ and equation \eqref{def: reproducing property} the reproducing kernel for the vector-valued RKBS $\mathcal{B}$. Moreover, equation \eqref{def: reproducing property} is called the reproducing property. Clearly, we have that $K(x,\cdot)\in(\mathcal{B}')^n$ for all $x\in X$. The notion of the vector-valued RKBS and its reproducing kernel will serve as a basis for us to understand the hypothesis space for deep learning in the next section.

It is worth of pointing out that although $\mathcal{B}$ is a space of vector-valued functions, the $\delta$-dual space  $\mathcal{B}'$ defined here is a space of scalar-valued functions.  This is determined by the form of the point evaluation functionals in set $\Delta$ defined by \eqref{Def:Delta}. The way of defining the $\delta$-dual space of the vector-valued RKBS $\mathcal{B}$ is not unique. One can also define a $\delta$-dual space of the vector-valued RKBS $\mathcal{B}$ as a space of vector-valued functions. In this paper, we adopt the current form of $\mathcal{B}'$ since it is simple and sufficient to serve our purposes. Other forms of the $\delta$-dual space will be investigated in a different occasion.

%
\section{Hypothesis Space}
%

In this section, we return to understanding the vector space $\mathcal{B}_\mathbb{W}$ introduced in section 2 from the RKBS viewpoint. Specifically, our goal is to introduce a vector-valued RKBS in which the vector space $\mathcal{B}_\mathbb{W}$ is weakly$^*$ dense. The resulting vector-valued RKBS will serve as the hypothesis space for deep learning.

We first construct the vector-valued RKBS. Recalling the parameter space $\Theta$ defined by equation \eqref{Def:Theta}, we use $C_0(\Theta)$ to denote the space of the continuous {\it scalar-valued} functions vanishing at infinity on $\Theta$. We equip the maximum norm on $C_0(\Theta)$, namely, $\|f\|_{\infty}:=\sup_{\theta\in\Theta}|f(\theta)|$, for all $f\in C_0(\Theta)$. For the function $\mathcal{N}(x,\theta)$, $x\in\mathbb{R}^s$, $\theta\in\Theta$, defined by equation \eqref{Def:kernel}, we denote by $\mathcal{N}_k({x},\theta)$ the $k$-th component of $\mathcal{N}({x},\theta)$, for $k\in\mathbb{N}_t$.  
We require that all components $\mathcal{N}_k({x},\cdot)$ with a weight belong to $C_0(\Theta)$ for all $x\in\mathbb{R}^s$. Specifically, we assume that there exists
a continuous weight function $\rho:\Theta\to\mathbb{R}$ such that the functions 
$$
\mathcal{N}_k({x},\cdot)\rho(\cdot)\in C_0(\Theta), \ \ \mbox{for all} \ \ x\in \mathbb{R}^s, \ k\in\mathbb{N}_t.
$$
An example of such a weight function is given by the rapidly decreasing function
\begin{equation}\label{gaussian weight function}
    \rho(\theta):=\exp(-\|\theta\|_2^2), \ \ \theta\in\Theta.
\end{equation}

%
We need a measure on the set $\Theta$.
A Radon measure \cite{folland1999real} on $\Theta$ is a Borel measure on $\Theta$ that is finite on all compact sets of $\Theta$, outer regular on all Borel sets of $\Theta$, and inner regular on all open sets of $\Theta$. Let $\mathcal{M}(\Theta)$ denote the space of finite Radon measures $\mu: \Theta\to \mathbb{R}$, equipped with the total variation norm 
\begin{equation}\label{TVnorm}
    \|\mu\|_{\mathrm{TV}}:=\sup\left\{\sum_{k=1}^\infty\left|\mu(E_k)\right|:\Theta=\bigcup_{k=1}^\infty E_k,\ E_i\cap E_j=\emptyset\text{ whenever }i\neq j \right\},
\end{equation}
where $E_k$ are required to be measurable.
%
Note that  $\mathcal{M}(\Theta)$ is the dual space of  $C_0(\Theta)$ (see, for example, \cite{conway2019course}). Moreover, the dual bilinear form on $\mathcal{M}(\Theta)\times C_0(\Theta)$ is given by 
\begin{equation}\label{DualBilinearForm}
    \langle \mu,g\rangle_{C_0(\Theta)}:=\int_{\Theta} g(\theta)d\mu(\theta),\ \text{for }\mu\in\mathcal{M}(\Theta),\ g\in C_0(\Theta). 
\end{equation}
%
%
For $\mu \in \mathcal{M}(\Theta)$, we let 
\begin{equation}\label{Def:f_mu^k}
    f_{\mu}^k(\cdot):=\int_\Theta \mathcal{N}_k(\cdot,\theta)\rho(\theta)d\mu(\theta), \ \  k\in \mathbb{N}_t, 
\end{equation}
and 
\begin{equation*}\label{vector-valued fmu}
f_\mu(\cdot):=\left[f_{\mu}^k(\cdot): k\in\mathbb{N}_t\right]^\top.
\end{equation*}
We introduce the vector space
\begin{equation}\label{banach space DNN}
    \mathcal{B}_{\mathcal{N}}:=\left\{f_\mu:\mu \in \mathcal{M}(\Theta)\right\},
\end{equation}
with norm 
\begin{equation}\label{banach space norm DNN}
    \|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}:=\inf\left\{\|\nu\|_{\mathrm{TV}}:f_\nu=f_\mu,\ \nu\in\mathcal{M}(\Theta)\right\},
\end{equation}
where $f_\mu^k$, $k\in\mathbb{N}_t$, are defined by equation \eqref{Def:f_mu^k} and $\|\cdot\|_\mathrm{TV}$ is defined as \eqref{TVnorm}. Note that in definition \eqref{banach space norm DNN} of the norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$, the infimum is taken over all the measures $\nu\in\mathcal{M}(\Theta)$ that satisfy $t$ equality constraints
\begin{equation*}
    \int_\Theta \mathcal{N}_k(\cdot,\theta)\rho(\theta)d\mu(\theta)=\int_\Theta \mathcal{N}_k(\cdot,\theta)\rho(\theta)d\nu(\theta),\quad k\in\mathbb{N}_t. 
\end{equation*}
In particular, in the case $t=1$, where $f_\mu$ reduces to a neural network of a scalar-valued output, the norm $ \|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$ is taken over the measures $\nu\in\mathcal{M}(\Theta)$ that satisfies only a single equality constraint. The bigger $t$ is, the larger the norm $ \|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$ will be. We remark that the special case of $\mathcal{B}_{\mathcal{N}}$ with $\mathcal{N}$ being a scalar-valued neural network of a {\it single} hidden layer was recently studied in \cite{bartolucci2023understanding}.

%


We next show that the space $\mathcal{B}_{\mathcal{N}}$ defined by \eqref{banach space DNN} with norm \eqref{banach space norm DNN} is a Banach space having a pre-dual space. This is done by showing that   $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to a quotient space. We recall the concept of the quotient space. Let $B$ be a Banach space with its dual space $B^*$ and $M$ be a closed subspace of $B$. For each $f$ in $B$, the translate $f+M$ which contains $f$ is called the coset of $M$.  The quotient space $B/M$ is defined by  $B/M:=\{f+M: f\in B\}$, with the quotient norm 
\begin{equation*}
    \|f+M\|_{B/M}:=\inf\left\{\|f+g\|:g\in M\right\},\quad f\in B.
\end{equation*}
It is known \cite{megginson2012introduction} that the quotient space $B/M$ is a Banach space. We say that a Banach space $B$ has a pre-dual space if there exists a Banach space $B_*$ such that $(B_*)^*=B$ and we call the space  $B_*$ a pre-dual space of $B$. 
%
%
%
%
%
We also need the notion of annihilators. Let $M$ and
$M'$ be subsets of $B$ and $B^*$, respectively. The annihilator of $M$ in $B^*$ is defined by 
$$
M^\perp:=\{\nu\in B^*:\langle\nu,f\rangle_{B}=0, \ \text{for all }f\in M\}.
$$
The annihilator of $M'$ in $B$ is defined by
$$
^{\perp}M':=\{f\in B: \langle\nu,f\rangle_{B}=0,
\ \mbox{for all}\  \nu\in M'\}.
$$  
We review a result about the dual space of a closed subspace of a Banach space. Specifically, let $M$ be a closed subspace of a Banach space $B$. For each $\nu\in B^*$, we denote by $\nu|_{M}$ the restriction of $\nu$ to $M$. It is clear that $\nu|_{M}\in M^*$ and $\|\nu|_{M}\|_{M^*}\leq\|\nu\|_{B^*}$. The dual space $M^*$ may be identified as $B^*/M^\perp$. In fact, by Theorem 10.1 in Chapter III of \cite{conway2019course}, the map $\tau:B^*/M^\perp\to M^*$ defined by 
$$
\tau(\nu+M^\perp):=\nu|_{M}, \ \mbox{for}\ \nu\in B^*,
$$ 
is an isometric isomorphism between $B^*/M^\perp$ and $M^*$.

For the purpose of proving that  $\mathcal{B}_{\mathcal{N}}$ is a Banach space, we identify the quotient space which is isometrically isomorphic to $\mathcal{B}_{\mathcal{N}}$. To this end, we introduce a closed subspace of $C_0(\Theta)$ as 
\begin{equation}\label{subspace of C0}
\mathcal{S}:=\overline{\mathrm{span}}\{\mathcal{N}_k({x},\cdot)\rho(\cdot): {x}\in\mathbb{R}^s,k\in\mathbb{N}_t\},
\end{equation}
where the closure is taken in the maximum norm. From definition \eqref{subspace of C0}, it is clear that $\mathcal{S}$ is a Banach space of functions defined on the parameter space $\Theta$.

\begin{proposition}\label{prop: BN is isometic isomorphic to quotient space}
Let $\Theta$ be the parameter space defined by \eqref{Def:Theta}. If 
for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)\in C_0(\Theta)$, then the space $\mathcal{B}_{\mathcal{N}}$ defined by \eqref{banach space DNN} endowed with the norm \eqref{banach space norm DNN} is a Banach space with a pre-dual space $\mathcal{S}$ defined by \eqref{subspace of C0}.
\end{proposition}
\begin{proof}
It is clear that $\mathcal{B}_{\mathcal{N}}$ is a normed space endowed with the norm $\|\cdot\|_{\mathcal{B}_{\mathcal{N}}}$. It suffices to verify that $\mathcal{B}_{\mathcal{N}}$ is complete. This is done by showing that $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to the  quotient space $\mathcal{M}(\Theta)/\mathcal{S}^\perp$.
We first characterize the annihilator of $\mathcal{S}$ in $\mathcal{M}(\Theta)$. It follows from definition \eqref{subspace of C0} of $\mathcal{S}$ that $\mu\in \mathcal{S}^\perp$ if and only if $\langle\mu,\mathcal{N}_k({x},\cdot)\rho(\cdot)\rangle_{C_0(\Theta)}=0$ for all $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$. The latter is equivalent to $f_{\mu}^k(x)=0$ for all $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, due to definition \eqref{Def:f_mu^k} of $f_{\mu}^k$ and definition \eqref{DualBilinearForm} of the dual bilinear form on $\mathcal{M}(\Theta)\times C_0(\Theta)$. That is, $f_{\mu}=0$. Consequently, we conclude that
\begin{equation}\label{Sperp}
    \mathcal{S}^\perp=\{\mu\in\mathcal{M}(\Theta): f_{\mu}=0\}.
\end{equation}
We next let $\varphi$ be the map from $\mathcal{B}_{\mathcal{N}}$ to $\mathcal{M}(\Theta)/\mathcal{S}^\perp$ defined for $f_\mu\in \mathcal{B}_{\mathcal{N}}$ by 
\begin{equation}\label{def: mapping phi}\varphi(f_\mu):=\mu+\mathcal{S}^\perp, \quad\mu\in\mathcal{M}(\Theta),
\end{equation}
and show that $\varphi$ is an isometric isomorphism. 

%

We first show that $\varphi$ is an isometry. For any $f_{\mu}\in\mathcal{B}_\mathcal{N}$ with $\mu\in\mathcal{M}(\Theta)$, we have that $\nu\in\mathcal{M}(\Theta)$ satisfies $f_{\nu}=f_{\mu}$ if and only if $f_{\nu-\mu}=0$. Due to representation \eqref{Sperp} of $\mathcal{S}^\perp$, the latter is equivalent to $\nu-\mu\in \mathcal{S}^\perp$. That is, $\nu=\mu+\mu'$ for some $\mu'\in \mathcal{S}^\perp$. Hence, we get by definition \eqref{banach space norm DNN} that 
$$
\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}=\inf\left\{\|\mu+\mu'\|_{\mathrm{TV}}:\mu'\in \mathcal{S}^\perp\right\}.
$$
This together with the definition of the quotient norm yields that 
$$
\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}=\|\mu+\mathcal{S}^\perp\|_{\mathcal{M}(\Theta)/\mathcal{S}^\perp},
$$
which with \eqref{def: mapping phi} leads to $\|\varphi(f_{\mu})\|_{\mathcal{M}(\Theta)/\mathcal{S}^\perp}=\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}$. In other words, $\varphi$ is an isometry. Due to its isometry property,  $\varphi$ is injective. Clearly,  $\varphi$ is surjective. Hence, it is bijective.
Consequently, $\varphi$ is an isometric isomorphism from $\mathcal{B}_{\mathcal{N}}$ to the Banach space $\mathcal{M}(\Theta)/\mathcal{S}^\perp$, and thus, $\mathcal{B}_{\mathcal{N}}$ is complete.  

We now show that $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to the dual space of $\mathcal{S}$. Note that $\mathcal{S}$ is a closed subspace of $C_0(\Theta)$ and $(C_0(\Theta))^*=\mathcal{M}(\Theta)$. By Theorem 10.1 in \cite{conway2019course} with $B:=C_0(\Theta)$ and $M:=\mathcal{S}$, we have that the map $\tau:\mathcal{M}(\Theta)/\mathcal{S}^\perp\to \mathcal{S}^*$ defined by 
    \begin{equation*}\label{pho}
        \tau(\mu+\mathcal{S}^\perp):=\mu|_{\mathcal{S}},\quad\mu\in\mathcal{M}(\Theta),
    \end{equation*}
is an isometric isomorphism. 
As has been shown earlier, %
the map $\varphi$ defined by
\eqref{def: mapping phi} is an isometric isomorphism from $\mathcal{B}_{\mathcal{N}}$ to $\mathcal{M}(\Theta)/\mathcal{S}^\perp$. As a result, $\tau\circ\varphi$ provides an isometric isomorphism from  $\mathcal{B}_{\mathcal{N}}$ to $\mathcal{S}^*$. 
\end{proof}

Proposition \ref{prop: BN is isometic isomorphic to quotient space} and the theorems that follow require that for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$.
This requirement in fact imposes a hypothesis to the activation function $\sigma$: (i) $\sigma$ is continuous and (ii) when the weight function $\rho$ is chosen as \eqref{gaussian weight function}, we need to select the activation function $\sigma$ having a growth rate no greater than polynomials. We remark that many commonly used activation functions satisfy this requirement. They include the ReLU function 
$$
\sigma(x):=\max\{0,x\},\ \ x\in\mathbb{R},
$$ 
and the sigmoid function 
$$
\sigma(x):=\frac{1}{1+e^{-x}},\ \ x\in\mathbb{R}.
$$


Now that the space $\mathcal{B}_{\mathcal{N}}$ with the norm $\|\cdot\|_{\mathcal{B}_{\mathcal{N}}}$, guaranteed by Proposition \ref{prop: BN is isometic isomorphic to quotient space}, is a Banach space, we denote by $\mathcal{B}_{\mathcal{N}}^*$ the dual space of $\mathcal{B}_{\mathcal{N}}$ endowed with the norm  
\begin{equation}\label{norm of BN*}
    \|\ell\|_{\mathcal{B}_{\mathcal{N}}^*}=\sup\{|\langle\ell,f_\mu\rangle_{\mathcal{B}_{\mathcal{N}}}|:\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\}, \ \ \mbox{for}\ \ \ell\in \mathcal{B}_{\mathcal{N}}^*.
\end{equation} 
The dual space  $\mathcal{B}_{\mathcal{N}}^*$ is again a Banach space.
Moreover, it follows from Proposition \ref{prop: BN is isometic isomorphic to quotient space} that the space $\mathcal{S}$ is a pre-dual space of $\mathcal{B}_{\mathcal{N}}$, that is, 
$(\mathcal{B}_{\mathcal{N}})_*=\mathcal{S}.$
We remark that the dual bilinear form on $\mathcal{B}_{\mathcal{N}}\times\mathcal{S}$ is given by
\begin{equation}\label{dual bilinear on BNS}
\langle f_{\mu}, g\rangle_{\mathcal{S}}=\langle\mu, g\rangle_{C_0(\Theta)}, \ \mbox{for}\ f_{\mu}\in\mathcal{B}_{\mathcal{N}},\ g\in \mathcal{S}.
\end{equation}
According to Proposition \ref{prop: BN is isometic isomorphic to quotient space}, the space $\mathcal{S}$ is the pre-dual space of $\mathcal{B}_{\mathcal{N}}$, that is,
$\mathcal{S}^*=\mathcal{B}_{\mathcal{N}}$.
Thus, we obtain that $\mathcal{S}^{**}=\mathcal{B}_{\mathcal{N}}^*$.
It is well-known (for example, see \cite{conway2019course}) that $\mathcal{S}\subseteq \mathcal{S}^{**}$ in the sense of isometric embedding.
%
Hence,   $\mathcal{S}\subseteq \mathcal{B}_{\mathcal{N}}^*$ and there holds 
\begin{equation}\label{natural-map-predual}
\langle g,f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle f_{\mu},g\rangle_{\mathcal{S}},
\ \mbox{for all} \ f_{\mu}\in \mathcal{B}_{\mathcal{N}} \ \mbox{and all} \ g\in \mathcal{S}.
\end{equation} 


We now turn to establishing that $\mathcal{B}_{\mathcal{N}}$ is a vector-valued RKBS on $\mathbb{R}^s$.

\begin{theorem}\label{theorem: BN vector valued RKBS}
Let $\Theta$ be the parameter space defined by \eqref{Def:Theta}. If  
for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$, then the Banach space $\mathcal{B}_{\mathcal{N}}$ defined by \eqref{banach space DNN} endowed with the norm \eqref{banach space norm DNN} is a vector-valued RKBS on $\mathbb{R}^s$.
\end{theorem}
\begin{proof}
According to Proposition \ref{prop: component wise for vector-valued RKBS} with $X:=\mathbb{R}^s$, it suffices to prove that for each $x\in\mathbb{R}^s$, $k\in\mathbb{N}_t$, there exists a positive constant $C_{x,k}$ such that
\begin{equation}\label{proof fmuk leq Cxk f norm}
    |f_\mu^k(x)|\leq C_{x,k}\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}, \ \mbox{for all}\ f_{\mu}\in\mathcal{B}_\mathcal{N}.
\end{equation}
To this end, for any $f_{\mu}\in\mathcal{B}_\mathcal{N}$, we obtain from definition \eqref{Def:f_mu^k} of $f_\mu^k$ that 
\begin{equation}\label{f_mu^k}
|f_\mu^k({x})|\leq \|\mathcal{N}_k({x},\cdot)\rho(\cdot)\|_\infty\|\nu\|_{\mathrm{TV}},
\end{equation}
for any $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\nu=f_\mu$.  
By taking infimum of both sides of inequality \eqref{f_mu^k} over $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\nu=f_\mu$ and employing definition \eqref{banach space norm DNN}, we obtain that 
$$
|f_\mu^k({x})|\leq\|\mathcal{N}_k({x},\cdot)\rho(\cdot)\|_\infty\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}.
$$
Letting $C_{x,k}:=\|\mathcal{N}_k({x},\cdot)\rho(\cdot)\|_\infty$, we get inequality \eqref{proof fmuk leq Cxk f norm}.
\end{proof}

Next, we identify %
the reproducing kernel of the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$. According to Proposition \ref{existence of reproducing kernel}, the existence of the reproducing kernel requires to characterize the $\delta$-dual space of $\mathcal{B}_{\mathcal{N}}$.  
We note that the $\delta$-dual space  $\mathcal{B}'_{\mathcal{N}}$ is the closure of  
\begin{equation*}\label{delta dual for BN}
\Delta:=\mathrm{span}\{\delta_{x,k}:x\in\mathbb{R}^s,\ k\in\mathbb{N}_t\},
\end{equation*}
in the norm topology \eqref{norm of BN*} of $\mathcal{B}_{\mathcal{N}}^*$. 
We will show that $\Delta$ is isometrically isomorphic to  
$$
\mathbb{S}:=\mathrm{span}\{\mathcal{N}_k({x},\cdot)\rho(\cdot):{x}\in\mathbb{R}^s,\ k\in\mathbb{N}_t\},
$$
a subspace of $\mathcal{S}$.   To this end, we introduce a mapping $\Psi:\Delta\to \mathbb{S}$ by 
\begin{equation}\label{isometric isomorphism_delta_KX}   \Psi\left(\sum_{j\in\mathbb{N}_m}\alpha_{j}\delta_{{x}_j,k_j}\right):= \sum_{j\in\mathbb{N}_m}\alpha_{j} \mathcal{N}_{k_j}({x}_j,\cdot)\rho(\cdot),
\end{equation}
for all $m\in\mathbb{N}$, $\alpha_{j}\in\mathbb{R}$, ${x}_j\in\mathbb{R}^s$, $k_j\in\mathbb{N}_t$, and $j\in\mathbb{N}_m$. 

\begin{lemma}\label{Lemma:isometric isomorphism_delta_KX}
The map $\Psi$ defined by \eqref{isometric isomorphism_delta_KX} is an isometric isomorphism between $\Delta$ and $\mathbb{S}$. 
\end{lemma}
\begin{proof}
We first prove that $\Psi$ is an isometry, that is, $\left\|\ell\right\|_{\mathcal{B}_{\mathcal{N}}^*}=\left\|\Psi(\ell)\right\|_{\infty}$, for all $\ell\in\Delta$. Let  $\ell$ be an arbitrary element of $\Delta$. Then there exist $m\in\mathbb{N}$, $\alpha_{j}\in\mathbb{R}$, ${x}_j\in\mathbb{R}^s$, $k_j\in\mathbb{N}_t$, and $j\in\mathbb{N}_m$ such that $\ell=\sum_{j\in\mathbb{N}_m}\alpha_{j}\delta_{{x}_j,k_j}$. By definition \eqref{norm of BN*} and the definition of the functionals $\delta_{x_j,k_j}$, $j\in\mathbb{N}_m$, we have that 
    %
    %
    %
       \begin{equation}\label{proof norm of ell}
        \|\ell\|_{\mathcal{B}_{\mathcal{N}}^*}=\sup\left\{\left|\sum_{j\in\mathbb{N}_m}\alpha_{j} f_\mu^{k_j}(x_j)\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}.
    \end{equation}
    We next compute $\|\Psi(\ell)\|_{\infty}$.
    By noting that $\Psi(\ell)\in \mathcal{S}$ and $\mathcal{S}^*=\mathcal{B}_{\mathcal{N}}$, we have that 
    $$
    \|\Psi(\ell)\|_\infty=\sup\left\{\left|\langle f_\mu, \Psi(\ell)\rangle_{\mathcal{S}}\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}.
    $$
    Substituting equation \eqref{dual bilinear on BNS} with $g:=\Psi(\ell)$ into the right hand side of the above equation, we get that 
    \begin{equation}\label{norm of psi ell}
        \|\Psi(\ell)\|_\infty=\sup\left\{\left|\langle \mu, \Psi(\ell)\rangle_{C_0(\Theta)}\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}.
    \end{equation}
    According to definition \eqref{isometric isomorphism_delta_KX} of $\Psi$, there holds for any $f_{\mu}\in\mathcal{B}_{\mathcal{N}}$ that
    $$
    \langle \mu, \Psi(\ell)\rangle_{C_0(\Theta)}=\sum_{j\in\mathbb{N}_m}\alpha_{j} \langle \mu,\mathcal{N}_{k_j}({x}_j,\cdot)\rho(\cdot)\rangle_{C_0(\Theta)}.
    $$
    This together with definition \eqref{Def:f_mu^k} yields that 
    $
    \langle \mu, \Psi(\ell)\rangle_{C_0(\Theta)}=\sum_{j\in\mathbb{N}_m}\alpha_{j} f_{\mu}^{k_j}(x_j).
    $
    Involving this equation in the right-hand side of \eqref{norm of psi ell} leads to
    \begin{equation}\label{proof: infinity normn of psi(ell) final form}
       \|\Psi(\ell)\|_\infty=\sup\left\{\left|\sum_{j\in\mathbb{N}_m}\alpha_{j} f_{\mu}^{k_j}(x_j)\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}. 
    \end{equation}
    Comparing \eqref{proof norm of ell} and \eqref{proof: infinity normn of psi(ell) final form}, we obtain that 
    $   \|\ell\|_{\mathcal{B}_{\mathcal{N}}^*}=\|\Psi(\ell)\|_\infty$ and hence, $\Psi$ is an isometry between $\Delta$ and $\mathbb{S}$. The isometry of $\Psi$ further implies its injectivity.  Moreover, $\Psi$ is linear and surjective. Thus, $\Psi$ is  bijective. Therefore, $\Psi$ is an isometric isomorphism between $\Delta$ and $\mathbb{S}$. 
\end{proof}

The isometrically isomorphic relation between $\Delta$ and $\mathbb{S}$ is preserved after completing them. We state this result in the following lemma without proof.

\begin{lemma}\label{isometric-isomorphism-after-completion}
    Suppose that $A$ and $B$ are Banach spaces with norms $\|\cdot\|_A$ and $\|\cdot\|_B$, respectively. Let $A_0$ and $B_0$ be dense subsets of $A$ and $B$, respectively. If $A_0$ is isometrically isomorphic to $B_0$, then $A$ is isometrically isomorphic to $B$.
\end{lemma}

Lemma \ref{isometric-isomorphism-after-completion} may be obtained by applying Theorem 1.6-2 in \cite{kreyszig1991introductory}.
With the help of Lemmas \ref{Lemma:isometric isomorphism_delta_KX} and \ref{isometric-isomorphism-after-completion}, we identify in the following theorem the reproducing kernel for the RKBS $\mathcal{B}_{\mathcal{N}}$.

\begin{theorem}\label{theorem: Kernel of BN}
Let $\Theta$ be the parameter space defined by \eqref{Def:Theta}. Suppose that  
for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$. If the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$ is defined by \eqref{banach space DNN} with the norm \eqref{banach space norm DNN}, then the vector-valued function 
\begin{equation}\label{Kernel}
\mathcal{K}(x,\theta):=\mathcal{N}(x,\theta)\rho(\theta), \ \ \mbox{for}\ \ (x,\theta)\in \mathbb{R}^s\times\Theta,
\end{equation}
is the reproducing kernel for space $\mathcal{B}_{\mathcal{N}}$.
\end{theorem}

\begin{proof}
We employ Proposition \ref{existence of reproducing kernel} with $X:=\mathbb{R}^s$ and $X':=\Theta$ to establish that the function $\mathcal{K}$ defined by \eqref{Kernel} is the reproducing kernel of space $\mathcal{B}_{\mathcal{N}}$. According to Lemma \ref{Lemma:isometric isomorphism_delta_KX}, $\Delta$ is isometrically isomorphic to $\mathbb{S}$. Since $\mathcal{B}_{\mathcal{N}}'$ and $\mathcal{S}$ are the completion of $\Delta$ and $\mathbb{S}$, respectively,  by Lemma \ref{isometric-isomorphism-after-completion}, we conclude that 
the $\delta$-dual space $\mathcal{B}'_{\mathcal{N}}$ of $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to $\mathcal{S}$, which is a Banach space of functions from $\Theta$ to $\mathbb{R}$. 
Hence, Proposition \ref{existence of reproducing kernel} ensures that there exists a unique reproducing kernel for $\mathcal{B}_{\mathcal{N}}$. 

We next verify that the vector-valued function $\mathcal{K}$ defined by \eqref{Kernel} is the reproducing kernel for $\mathcal{B}_{\mathcal{N}}$. By noting that the $\delta$-dual space $\mathcal{B}'_{\mathcal{N}}$ is isometrically isomorphic to $\mathcal{S}$, we have for each $x\in\mathbb{R}^s$ and each $k\in\mathbb{N}_t$ that $\mathcal{K}_k(x,\cdot):=\mathcal{N}_k(x,\cdot)\rho(\cdot)\in \mathcal{B}_{\mathcal{N}}'$. The space $\mathcal{S}$, guaranteed by Proposition \ref{prop: BN is isometic isomorphic to quotient space}, is a pre-dual space of $\mathcal{B}_{\mathcal{N}}$. Hence, by equation \eqref{natural-map-predual} with $g:=\mathcal{K}_k(x,\cdot)$, we obtain for each $x\in\mathbb{R}^s$,  $k\in\mathbb{N}_t$ that 
$$
\langle\mathcal{K}_k(x,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle f_{\mu},\mathcal{K}_k(x,\cdot)\rangle_{\mathcal{S}},\ \mbox{for all}\  f_{\mu}\in\mathcal{B}_{\mathcal{N}}.
$$
Substituting equation \eqref{dual bilinear on BNS} with $g:=\mathcal{K}_k(x,\cdot)$ into the right-hand side of the above equation leads to 
$$
\langle\mathcal{K}_k(x,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle\mu, \mathcal{K}_k(x,\cdot)\rangle_{C_0(\Theta)},\ \mbox{for all}\  f_{\mu}\in\mathcal{B}_{\mathcal{N}}.
$$
This together with definitions \eqref{DualBilinearForm}, \eqref{Kernel}  and \eqref{Def:f_mu^k} implies the reproducing property 
\begin{equation}\label{proof reproducing property}
\langle\mathcal{K}_k(x,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=f_{\mu}^k(x),\ \mbox{for all}\  f_{\mu}\in\mathcal{B}_{\mathcal{N}}.
\end{equation}
Consequently, $\mathcal{K}$ is the reproducing kernel of $\mathcal{B}_{\mathcal{N}}$. 
\end{proof}

The reproducing kernel defined by \eqref{Kernel} in Theorem \ref{theorem: Kernel of BN} is an asymmetric kernel, unlike a reproducing kernel in a reproducing kernel Hilbert space, which is always symmetric. It is the asymmetry of the ``kernel" that allows us to encode one variable of the kernel function as the physical variable and one as the parameter variable. Theorem \ref{theorem: Kernel of BN} restricted to the shallow network is still new to our best acknowledge. We will show in the next section a solution of a deep learning model may be expressed as a combination of a finite number of kernel sessions, a kernel with the parameter variable evaluated at a point of the parameter space determined by given data.

We are ready to prove that the vector space $\mathcal{B}_{\mathbb{W}}$, defined by equation \eqref{space B Delta}, is weakly${}^*$ dense in the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$. For this purpose, we recall the concept of the weak${}^*$ topology. Let $B$ be a Banach space. The weak${}^*$ topology of the dual space $B^{*}$ is the smallest topology for $B^{*}$ such that, for each $f\in B$, the linear functional $\nu \rightarrow\langle\nu, f\rangle_{B}$ on $B^*$ is continuous with respect to the topology. For a subset $M'$ of $B^*$, we denote by $\overline{M'}^{w^*}$ the closure of $M'$ in the weak$^*$ topology of $B^*$. We remark that the fact that the Banach space $\mathcal{B}_{\mathcal{N}}$ has a pre-dual space $\mathcal{S}$ makes it valid for $\mathcal{B}_{\mathcal{N}}$ to be equipped with the weak$^*$ topology, the topology of $\mathcal{S}^*$.


\begin{theorem}\label{theorem: H is linear subspace}
Let $\Theta$ be the parameter space defined by \eqref{Def:Theta} and  $\mathbb{W}$ the width set defined by \eqref{width-set}. If for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$, then $\mathcal{B}_{\mathbb{W}}$ is a subspace of $\mathcal{B}_{\mathcal{N}}$ and  
\begin{equation}\label{weak*Dense}
\overline{\mathcal{B}_{\mathbb{W}}}^{w^*}=\mathcal{B}_{\mathcal{N}}.
\end{equation}
\end{theorem}
\begin{proof}
It has been shown in Proposition \ref{prop: BM is a linear space} that $\mathcal{B}_{\mathbb{W}}$ is a vector space. We now show that $\mathcal{B}_{\mathbb{W}}$ is a subspace of $\mathcal{B}_{\mathcal{N}}$. For any $f\in{{{\mathcal{B}_{\mathbb{W}}}}}$, there exist $n\in\mathbb{N}$, $c_l\in\mathbb{R}$, $\theta_l\in\Theta$, $l\in\mathbb{N}_n$ such that $f=\sum_{l=1}^n c_l\mathcal{N}(\cdot,\theta_l)\rho(\theta_l)$. By choosing $\mu:=\sum_{l=1}^n c_l\delta_{\theta_l}$, we have that 
$\mu\in\mathcal{M}(\Theta)$. We then obtain from definition \eqref{Def:f_mu^k} that  
$$
f_{\mu}^k(x)=\sum_{l=1}^n c_l\mathcal{N}_k(x,\theta_l)\rho(\theta_l),\ \mbox{for all}\ x\in\mathbb{R}^s,\  k\in\mathbb{N}_t.
$$
This together with the representation of $f$ yields that $f=f_{\mu}$ and thus, $f\in\mathcal{B}_{\mathcal{N}}$. Consequently, we have that $\mathcal{B}_{\mathbb{W}}\subseteq\mathcal{B}_{\mathcal{N}}$.

It remains to prove equation \eqref{weak*Dense}. Proposition \ref{prop: BN is isometic isomorphic to quotient space} ensures that $\mathcal{S}^*=\mathcal{B}_{\mathcal{N}}$, in the sense of being isometrically isomorphic. Hence, $\mathcal{B}_{\mathbb{W}}$ is a subspace of the dual space of $\mathcal{S}$. It follows from Proposition 2.6.6 of \cite{megginson2012introduction} that $(^{\perp}\mathcal{B}_{\mathbb{W}})^{\perp}=\overline{\mathcal{B}_{\mathbb{W}}}^{w^*}$. It suffices to verify that $(^{\perp}\mathcal{B}_{\mathbb{W}})^{\perp}=\mathcal{B}_{\mathcal{N}}$. Due to definition \eqref{space B Delta} of $\mathcal{B}_{\mathbb{W}}$, $g\in  ^{\perp}\mathcal{B}_{\mathbb{W}}$ if and only if \begin{equation}\label{suff-ness-1}
    \langle \mathcal{N}(\cdot,\theta)\rho(\theta), g\rangle_{\mathcal{S}}=0,\ \mbox{for all}\ \theta\in\Theta.
\end{equation}
By equation \eqref{dual bilinear on BNS} with $f_{\mu}:=\mathcal{N}(\cdot,\theta)\rho(\theta)$ with $\mu=\delta_{\theta}$, equation \eqref{suff-ness-1} is equivalent to 
$\left<\delta_\theta, g\right>_{C_0(\Theta)}=0$, for all $\theta\in\Theta$,
which leads to $g(\theta)=0$ for all $\theta\in\Theta$. That is, $g=0$. Therefore, $^{\perp}\mathcal{B}_{\mathbb{W}}=\{0\}$. This together with the definition of annihilators leads to $(^{\perp}\mathcal{B}_{\mathbb{W}})^\perp=\mathcal{B}_{\mathcal{N}}$, which completes the proof of this theorem.
\end{proof}

To close this section, we summarize the properties of the space $\mathcal{B}_{\mathcal{N}}$ established in 
Theorems \ref{theorem: BN vector valued RKBS}, \ref{theorem: Kernel of BN} and \ref{theorem: H is linear subspace} as follows:

(i) The space $\mathcal{B}_{\mathcal{N}}$ is a vector-valued RKBS.

(ii) The vector-valued function $\mathcal{K}$ defined by \eqref{Kernel}
is the reproducing kernel for space $\mathcal{B}_{\mathcal{N}}$.

(iii) The space $\mathcal{B}_{\mathcal{N}}$ is the weak* completion of the vector space $\mathcal{B}_{\mathbb{W}}$.

\noindent
These favorable properties of the space $\mathcal{B}_{\mathcal{N}}$ motivate us to take it as the hypothesis space for deep learning. 
Thus, we consider the following learning model  
\begin{equation}\label{LearningMethodinRKBS}
    \inf\{\mathcal{L}(f_{\mu},\mathbb{D}_m): f_{\mu}\in\mathcal{B}_{\mathcal{N}}\}.
\end{equation}
If we denote by $\mathcal{N}_{\mathcal{B}_\mathcal{N}}$ the neural network learned from the model \eqref{LearningMethodinRKBS}, then, according to \eqref{Comparison-of-three-models}, we have that
\begin{equation*}
    \mathcal{L}(\mathcal{N}_{\mathcal{B}_\mathcal{N}},\mathbb{D}_m)\leq \mathcal{L}(\mathcal{N}_{\mathcal{B}_\mathbb{W}},\mathbb{D}_m)
    \leq \mathcal{L}(\mathcal{N}_{\mathcal{A}_\mathbb{W}},\mathbb{D}_m).
\end{equation*}
Even though learning model \eqref{LearningMethodinRKBS} is like model \eqref{GeneralLearningMethod}, which is of infinite dimension (unlike model \eqref{BasicLearningMethod-equivalent}, which is of finite dimension), we will show in the next section that a solution of learning model \eqref{LearningMethodinRKBS} lays in a finite dimensional manifold determined by the kernel $\mathcal{K}$ and a given data set.


%
%
%
%
 
 

\section{Representer Theorems for Learning Solutions}

In this section, we consider learning a target function in $\mathcal{B}_{\mathcal{N}}$ from the sampled dataset $\mathbb{D}_m$ defined by \eqref{Dataset}. Learning such a function is an ill-posed problem, whose solutions often suffer from overfitting. For this reason, instead of solving the learning model \eqref{LearningMethodinRKBS} directly, we consider a related regularization problem and MNI problem in the RKBS $\mathcal{B}_{\mathcal{N}}$. The goal of this section is to 
establish %
representer theorems for solutions of these two learning models. 
%


We start with describing the regularized learning problem in the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$.  For the dataset $\mathbb{D}_m$ defined by \eqref{Dataset}, we define the set $\mathcal{X}:=\{x_j:j\in\mathbb{N}_m\}$ and the matrix $\mathbf{Y}:=[y_j^k:k\in\mathbb{N}_t,j\in\mathbb{N}_{m}]\in\mathbb{R}^{t\times m}$, where for each $j\in\mathbb{N}_m$, $y_j^k$, $k\in\mathbb{N}_t$, are the components of vector $y_j$. 
We introduce an operator $\mathbf{I}_\mathcal{X}:{\mathcal{B}_{\mathcal{N}}} \rightarrow \mathbb{R}^{t\times m}$ by 
\begin{equation}\label{L YES DC}
\mathbf{I}_\mathcal{X}(f_\mu):=\left[f^k_{\mu}(x_j): k\in\mathbb{N}_t,j\in\mathbb{N}_{m}\right],\ \mbox{for}\ f_\mu\in\mathcal{B}_{\mathcal{N}}.
\end{equation} 
%
We choose a loss function $\mathcal{Q}: \mathbb{R}^{t\times m} \rightarrow \mathbb{R}_{+}:=[0,+\infty)$ and define 
\begin{equation}\label{loss:An example}
    \mathcal{L}(f_{\mu},\mathbb{D}_m):=\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_\mu)-\mathbf{Y}),\ \mbox{for}\ f_{\mu}\in\mathcal{B}_{\mathcal{N}}.
\end{equation}
Examples of loss functions $\mathcal{Q}({\mathbf{M}})$ may be chosen as a norm of the matrix $\mathbf{M}$.
The proposed regularization problem is formed by adding a regularization
term $\lambda\| f_\mu\|_{{\mathcal{B}_{\mathcal{N}}}}$ to the data fidelity term $\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\mu})-\mathbf{Y})$. That is, %
%
\begin{equation}\label{eq: regularization problem RKBS B measure M(X)}
    \inf \left\{\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_\mu)-\mathbf{Y})+\lambda\| f_\mu\|_{{\mathcal{B}_{\mathcal{N}}}}:  f_\mu\in {\mathcal{B}_{\mathcal{N}}}\right\},
\end{equation}
where $\lambda$ is a positive regularization parameter. The learning model \eqref{eq: regularization problem RKBS B measure M(X)} allows us to learn a function $f_\mu$ in the space $\mathcal{B}_\mathcal{N}$ by solving the optimization problem  \eqref{eq: regularization problem RKBS B measure M(X)}.
%
%


We first comment on the existence of a solution to the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)}. The next proposition follows directly from Proposition 40 of  \cite{wang2021representer}.

%
%


\begin{proposition}\label{Existence-of-Solution}
Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}$ are given.  %
If $\lambda>0$ and the loss function $\mathcal{Q}$ is lower semi-continuous on $\mathbb{R}^{t\times m}$, then the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} has at least one solution. 
\end{proposition}
%
\begin{proof}
We have shown in Proposition \ref{prop: BN is isometic isomorphic to quotient space} that the Banach space $\mathcal{B}_{\mathcal{N}}$ has the pre-dual space $\mathcal{S}$. We specify the function $\varphi:\mathbb{R}_+\to\mathbb{R}_+$ appearing in Proposition 40 of  \cite{wang2021representer} as the identity function $\varphi(x)=x$, $x\in\mathbb{R}_+$, which is lower semi-continuous, increasing and coercive. Since the loss function $\mathcal{Q}$ is lower semi-continuous on $\mathbb{R}^{t\times m}$, the assumptions in Proposition 40 of  \cite{wang2021representer} are all satisfied. Thus, we conclude from Proposition 40 of  \cite{wang2021representer} that the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} has at least one solution.
\end{proof}



It is known that regularization problems are closely related to MNI problems (see, for example, \cite{wang2021representer}). The MNI problem aims at finding a vector-valued function $f_{\mu}$ in $\mathcal{B}_{\mathcal{N}}$, having the smallest norm and
satisfying the interpolation condition $f_{\mu}(x_j)=y_j$, $j\in\mathbb{N}_m$. In other words, the MNI problem has the form  
\begin{equation}\label{MNI-original}
\inf
\{\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}: f_{\mu}(x_j)=y_j, f_{\mu}\in\mathcal{B}_{\mathcal{N}}, j\in\mathbb{N}_m\}.
\end{equation}
Associated with the set $\mathcal{X}$ and matrix $\mathbf{Y}$, we introduce a subset $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ of $\mathcal{B}_{\mathcal{N}}$ as
\begin{equation}\label{hyperplane YES DC}
    \mathcal{M}_{\mathcal{X},\mathbf{Y}}:=\{f_\mu \in {\mathcal{B}_{\mathcal{N}}}: \mathbf{I}_{\mathcal{X}}(f_\mu)=\mathbf{Y}\}.
\end{equation}
We then reformulate the MNI problem \eqref{MNI-original} in an equivalent form as 
\begin{equation}\label{MNI in RKBS B measure M(X)}
    \inf \left\{\left\|f_\mu\right\|_{{\mathcal{B}_{\mathcal{N}}}}: f_\mu \in  \mathcal{M}_{\mathcal{X},\mathbf{Y}}\right\}.
\end{equation}
The MNI problem \eqref{MNI in RKBS B measure M(X)} has a solution if and only if the set $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is nonempty. 
The non-emptiness of the set $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ pertains to the existence of interpolation of a function in $\mathcal{B}_{\mathcal{N}}$ to any given data $\mathbb{D}_m$. For the sake of keeping focus on the main issues of this paper, this issue will be postponed to a different occasion. In this paper, we will always assume that the set $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is nonempty.

Recall that the vector-valued function $\mathcal{K}$ defined by \eqref{Kernel} is the reproducing kernel for $\mathcal{B}_{\mathcal{N}}$. By using the reproducing property \eqref{proof reproducing property}, we represent the operator $\mathbf{I}_{\mathcal{X}}$ defined by \eqref{L YES DC} as 
$$
\mathbf{I}_{\mathcal{X}}(f_\mu)=\left[\langle \mathcal{K}_k(x_j,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}: k\in\mathbb{N}_t,j\in\mathbb{N}_{m}\right],\ \mbox{for}\ f_\mu\in\mathcal{B}_{\mathcal{N}}.
$$
Clearly, the MNI problem \eqref{MNI in RKBS B measure M(X)} includes $tm$ interpolation conditions which are produced by the linear functionals in the set
$$
\mathbb{K}_\mathcal{X}:=\{\mathcal{K}_k(x_j,\cdot):  \ \ j\in\mathbb{N}_m,\ k\in\mathbb{N}_t\}.
$$
It follows from Proposition 1 of \cite{wang2021representer} that the existence of a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)} is guaranteed if the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent elements in $\mathcal{S}$. In fact, the linear independence of the functionals in $\mathbb{K}_\mathcal{X}$ is a sufficient condition that ensures the non-emptiness of the subset $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ defined by \eqref{hyperplane YES DC} for any given
$\mathbf{Y}\in\mathbb{R}^{t\times m}$. If the subset $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is nonempty but the functionals in $\mathbb{K}_\mathcal{X}$ are linearly dependent, one may replace $\mathbb{K}_\mathcal{X}$ by its maximal linearly independent subset. Hence, without loss of generality, we will assume that the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent throughout the rest of this paper. %


A solution of the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} may be identified as a solution of an MNI problem in the form of \eqref{MNI in RKBS B measure M(X)} with different data. In fact, according to \cite{wang2021representer}, every solution $\hat{f}_{\mu}\in\mathcal{B}_{\mathcal{N}}$ of the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} is also a
solution of the MNI problem \eqref{MNI in RKBS B measure M(X)} with $\mathbf{Y}:=\mathbf{I}_{\mathcal{X}}(\hat{f}_{\mu})$. In addition, if $\hat{f}_{\mu}\in\mathcal{B}_{\mathcal{N}}$ is a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)}, then $\hat{f}_{\mu}$ is a solution of the regularization problem 
$$
\inf \left\{\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_\mu)-\mathbf{Y})+\lambda\| f_\mu\|_{{\mathcal{B}_{\mathcal{N}}}}:  f_\mu\in \mathcal{M}_{\mathcal{X},\mathbf{Y}}\right\},
$$
and there holds the relation 
$$
{C^*}\leq\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(\hat{f}_{\mu})-\mathbf{Y})+\lambda\| \hat{f}_{\mu}\|_{{\mathcal{B}_{\mathcal{N}}}},
$$ 
where ${C^*}$ is the infimum of the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)}, since $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is a subset of $\mathcal{B}_{\mathcal{N}}$. 



We now establish a representer theorem for a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)}. This can be accomplished by applying the explicit and data-dependent  representer theorem for the MNI problem in a general Banach space setting, established in our recent paper \cite{wang2023sparse} to the current setting. To this end, we review some necessary notions of Convex Analysis and recall a result of  \cite{wang2023sparse}. %
Let $\mathbb{X}$ be a Hausdorff locally convex topological vector space. A subset $A$ of $\mathbb{X}$ is called a convex set if $t x+(1-t)y\in A$ for all $x,y\in A$ and all $t\in[0,1]$. The convex hull of a subset $A$ of  $\mathbb{X}$, denoted by $\mathrm{co}(A)$, is the smallest convex set that contains $A$. The closed convex hull of $A$, denoted by $\overline{\mathrm{co}}(A)$, is the smallest closed convex set that contains $A$, where the closure is taken under the topology of $\mathbb{X}$. Suppose that $A$ is a nonempty closed convex subset of $\mathbb{X}$. An element $z\in A$ is said to be an extreme point of $A$ if $x,y\in A$ and $tx+(1-t)y=z$ for some $t\in(0,1)$ implies that $x=y=z$. By $\mathrm{ext}(A)$ we denote the set of extreme points of $A$. The celebrated Krein-Milman theorem \cite{megginson2012introduction} states that if $A$ is a nonempty compact convex subset of $\mathbb{X}$, then $A$ is the closed convex hull of its set of extreme points, that is, 
$A=\overline{\mathrm{co}}\left(\mathrm{ext}(A)\right)$.
Let $B$ be a Banach space endowed with norm $\|\cdot\|_B$. Clearly, the norm $\|\cdot\|_B$ is a convex function on $B$. The subdifferential of the norm function $\|\cdot\|_B$ at each $f\in B\backslash\{0\}$ is defined by  \begin{equation*}\label{subdifferential = norming functional}
  \partial \|\cdot\|_B(f):=\left\{\nu\in B^*:\|\nu\|_{B^*}=1,\langle \nu,f\rangle_{B}=\|f\|_{B}\right\}.
\end{equation*}
Notice that the dual space $B^*$ of  $B$ equipped with the weak${}^*$ topology is a Hausdorff locally convex topological vector space. Moreover, for any $f\in B\backslash\{0\}$, the subdifferential set $\partial \|\cdot\|_B(f)$ is a convex and weakly${}^*$ compact subset of $B^*$. Hence, the Krein-Milman theorem ensures that for any $f\in B\backslash\{0\}$, 
\begin{equation*}\label{representation subdifferential set}
    \partial \|\cdot\|_B(f)=\overline{\mathrm{co}}(\mathrm{ext}(\partial \|\cdot\|_{B}(f))),
\end{equation*}
where the closed convex hull is taken under the weak$^*$ topology of $B^*$.

We now review the  representer theorem for the MNI problem in a general Banach space having a pre-dual space established in \cite{wang2023sparse}.  Suppose that $B$ is a Banach space having a pre-dual space $B_*$. Let $\nu_j$, $j\in\mathbb{N}_n$, be linearly independent elements in $B_*$ and $\mathbf{z}:=[z_j:j\in\mathbb{N}_n]\in\mathbb{R}^n$ be a given vector. Set $\mathcal{V}:=\mathrm{span}\{\nu_j:j\in\mathbb{N}_n\}$.
%
%
%
We define an operator $\mathcal{L}: B \rightarrow \mathbb{R}^{n}$ by  
%
    $\mathcal{L}(f):=\left[\left\langle\nu_{j}, f\right\rangle_{B}: j \in \mathbb{N}_{n}\right], \text { for all } f \in B,$
%
and introduce a subset of $B$ as
%
    ${M_{\mathbf{z}}}:=\{f \in B: \mathcal{L}(f)=\mathbf{z}\}$.
    %
The MNI problem with the given data $\{(\nu_j,y_j):j\in\mathbb{N}_n\}$ considered in \cite{wang2023sparse} has the form   
\begin{equation}\label{general MNI in lemma}
    \inf \left\{\|f\|_{B}: f \in {M_{\mathbf{z}}}\right\}.
\end{equation}
 
The representer theorem established in Proposition 7 of \cite{wang2023sparse} provides a representation of any extreme point of the solution set of the MNI problem \eqref{general MNI in lemma} with $\mathbf{z}\in\mathbb{R}^n$. We describe this result  in the next lemma. 

\begin{lemma}\label{lemma: representer for MNI}
Suppose that $B$ is a Banach space having a pre-dual space $B_{*}$. Let $\nu_{j} \in B_{*}$, $j \in \mathbb{N}_{n}$, be linearly independent and $\mathbf{z} \in \mathbb{R}^{n}\backslash\{\mathbf{0}\}$. If  $\mathcal{V}$ and   ${M_{\mathbf{z}}}$ are defined as above and $\hat\nu\in\mathcal{V}$ satisfies 
\begin{equation}\label{Non-empty-set in lemma}
(\|\hat\nu\|_{B_*}\partial\|\cdot\|_{B_*}(\hat\nu))\cap{M_{\mathbf{z}}}\neq\emptyset,
\end{equation}
then for any extreme point $\hat f$ of the solution set of the MNI problem \eqref{general MNI in lemma}, there exist $\gamma_j\in\mathbb{R}$, $j\in\mathbb{N}_{n}$, with  $\sum_{j\in\mathbb{N}_{n}}\gamma_j=\|\hat\nu\|_{B_*}$ and $u_j\in\mathrm{ext}\left(\partial\|\cdot\|_{B_*}(\hat\nu)\right)$, $j\in\mathbb{N}_{n}$, such that
\begin{equation*}\label{eq: expansion of p in lemma}
\hat f=\sum\limits_{j\in\mathbb{N}_{n}} \gamma_j u_j.
\end{equation*}
\end{lemma}

It was pointed out in \cite{wang2023sparse} that the element $\hat\nu$ satisfying \eqref{Non-empty-set in lemma} can be obtained through solving a dual problem of \eqref{general MNI in lemma}. Moreover, we remark that the solution set
is a nonempty, convex and weakly$^*$ compact subset of $B$. Hence, by the Krein-Milman theorem, the set of extreme points of the solution set is nonempty and moreover, any solution of problem \eqref{general MNI in lemma} can be expressed as the weak$^*$ limit of a sequence in the convex hull of the set of extreme points. 

We now present a representer theorem for a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)}, which is a direct consequence of Lemma \ref{lemma: representer for MNI}. We introduce a subspace of $\mathcal{S}$, which is defined by \eqref{subspace of C0} and has been proved to be a pre-dual space of $\mathcal{B}_{\mathcal{N}}$, by
\begin{equation}\label{V_span_kernel}
\mathcal{V}_{\mathcal{N}}:=\mathrm{span}\ \mathbb{K}_\mathcal{X},
\end{equation}
and denote by $\mathbb{S}_{\mathcal{X},\mathbf{Y}}$ the solution set of the MNI problem \eqref{MNI in RKBS B measure M(X)}. 


We prepare applying Lemma \ref{lemma: representer for MNI} to the MNI problem \eqref{MNI in RKBS B measure M(X)}. %
To this end, we introduce the dual problem of problem \eqref{MNI in RKBS B measure M(X)} as 
\begin{equation}\label{dual problem}
    \sup\left\{ \sum_{k\in\mathbb{N}_t}\sum_{j\in\mathbb{N}_m}c_{kj}y_j^k:\left\|\sum_{k\in\mathbb{N}_t}\sum_{j\in\mathbb{N}_m} c_{kj}\mathcal{K}_k(x_j,\cdot)\right\|_{\infty}=1\right\}.
\end{equation}
Note that the dual problem is a finite dimensional optimization problem which has the same optimal value, denoted by ${C^*}$, as the MNI problem \eqref{MNI in RKBS B measure M(X)}. It has been proved in \cite{cheng2021minimum} that there exists at least one solution for the dual problem of the MNI problem in $\ell_1(\mathbb{N})$. By a similar argument, we can show the existence of a solution of the dual problem \eqref{dual problem}. Suppose that $\hat{\mathbf{c}}:=[\hat{c}_{kj}:k\in\mathbb{N}_t,j\in\mathbb{N}_m]\in\mathbb{R}^{t\times m}$ is a solution of the dual problem \eqref{dual problem}. We let
\begin{equation}\label{stage 1 hat g}
    \hat{g}(\cdot):={C^*}\sum_{k\in\mathbb{N}_t}\sum_{j\in\mathbb{N}_m}\hat{c}_{kj}\mathcal{K}_k(x_j,\cdot). 
\end{equation}



\begin{theorem}\label{theorem: direct representer theorem for MNI in BN}
Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. %
%
Let $\hat g$ be  the function defined by \eqref{stage 1 hat g}.
Then for any $\hat f\in\mathrm{ext}(\mathbb{S}_{\mathcal{X},\mathbf{Y}})$, there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$, such that 
    \begin{equation}\label{representer solution of finite linear combination of MNI in BN}
        \hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell h_\ell(x),\ x\in\mathbb{R}^s.
    \end{equation}    
\end{theorem}
\begin{proof}
Proposition \ref{prop: BN is isometic isomorphic to quotient space} ensures that the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$ has the pre-dual space $\mathcal{S}$. Note that the functionals in $\mathbb{K}_\mathcal{X}$ belong to the pre-dual space $\mathcal{S}$ and are linearly independent. Moreover, since $\hat g$ is the function  defined by  \eqref{stage 1 hat g}, we have that $\hat g\in\mathcal{V}_{\mathcal{N}}$, and   according to Proposition 37 of \cite{wang2023sparse}, $\hat g$ satisfies the condition
\begin{equation}\label{non empty set: BN case}
(\|\hat g\|_{\infty}\partial\|\cdot\|_\infty(\hat g))\cap{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}\neq\emptyset.
\end{equation} 
Hence, the hypothesis of Lemma \ref{lemma: representer for MNI} is satisfied. Then by Lemma \ref{lemma: representer for MNI}, we can represent any extreme point $\hat f$ of the solution set $\mathbb{S}_{\mathcal{X},\mathbf{Y}}$ of the MNI problem \eqref{MNI in RKBS B measure M(X)} as in equation \eqref{representer solution of finite linear combination of MNI in BN} for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.  
\end{proof}

Theorem \ref{theorem: direct representer theorem for MNI in BN} provides for each extreme point of the solution set of problem \eqref{MNI in RKBS B measure M(X)} an explicit and data-dependent representation by using the elements in $\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$. Even more significantly, the essence of Theorem \ref{theorem: direct representer theorem for MNI in BN} is that although the MNI problem \eqref{MNI in RKBS B measure M(X)} is of infinite dimension, every extreme point of its solution set lays in a {\it finite} dimensional manifold spanned by $tm$ elements  $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$.

As we have demonstrated earlier, the element $\hat g$ satisfying \eqref{non empty set: BN case} can be obtained by solving the dual problem \eqref{dual problem} of \eqref{MNI in RKBS B measure M(X)}.
Since $\hat{g}$ is an element in $\mathcal{S}$, the subdifferential $\partial\|\cdot\|_\infty(\hat g)$ is a subset of the space $\mathcal{B}_{\mathcal{N}}$, which is the dual space of $\mathcal{S}$. Notice that the subdifferential set $\partial\|\cdot\|_\infty(\hat g)$ may not be included in space $\mathcal{B}_{\mathbb{W}}$  defined by \eqref{space B Delta} which is spanned by the kernel sessions $\mathcal{K}(\cdot,\theta)$, $\theta\in\Theta$.
However, a learning solution in the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$ is expected to be represented by the kernel sessions $\mathcal{K}(\cdot,\theta)$, $\theta\in\Theta$. For the purpose of obtaining a kernel representation for a solution of problem \eqref{MNI in RKBS B measure M(X)}, alternatively to problem \eqref{MNI in RKBS B measure M(X)}, we consider a closely related MNI problem in the measure space $\mathcal{M}(\Theta)$ and apply the representer theorem established in \cite{wang2023sparse} to it. We then translate the resulting representer theorem for the MNI problem in $\mathcal{M}(\Theta)$ to that for problem \eqref{MNI in RKBS B measure M(X)}, by using the relation between the solutions of these two problems.

We now introduce the MNI problem in the measure space $\mathcal{M}(\Theta)$ with respect to the the sampled dataset $\mathbb{D}_m$ and show the relation between its solution and a solution of problem \eqref{MNI in RKBS B measure M(X)}. By defining an operator $\widetilde{\mathbf{I}}_{\mathcal{X}}:\mathcal{M}(\Theta) \rightarrow \mathbb{R}^{t\times m}$ by
\begin{equation*}\label{tilde L on measure space}
\widetilde{\mathbf{I}}_{\mathcal{X}}(\mu):=\left[\langle \mathcal{K}_k(x_j,\cdot),\mu\rangle_{\mathcal{M}(\Theta)}: k\in\mathbb{N}_t, j \in \mathbb{N}_{m}\right],\ \mbox{for all}\ \mu\in\mathcal{M}(\Theta), 
\end{equation*} 
and introducing a subset $\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ of $\mathcal{M}(\Theta)$ as 
\begin{equation}\label{hyperplane in measure space}
    \widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}:=\left\{\mu\in\mathcal{M}(\Theta):\widetilde{\mathbf{I}}_{\mathcal{X}}(\mu)=\mathbf{Y}\right\},
\end{equation}
we formulate the MNI problem in $\mathcal{M}(\Theta)$ as
\begin{equation}\label{MNI in measure space}
    \inf \left\{\left\|\mu\right\|_{\mathrm{TV}}: \mu \in \widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}\right\}.
\end{equation}

The next proposition reveals the relation between the solutions of the MNI problems \eqref{MNI in RKBS B measure M(X)} and \eqref{MNI in measure space}. 

\begin{proposition}\label{prop: MNI solution for measure space is solution for RKBS}
Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. 
If ${\hat{\mu}}$ is a solution of the MNI problem \eqref{MNI in measure space}, then  
$f_{\hat\mu}(x):=\left[f_{\hat\mu}^k(x): k\in\mathbb{N}_t\right]^\top$, $x\in\mathbb{R}^s$,
with $f_{\hat\mu}^k$, $k\in\mathbb{N}_t$, defined as in  \eqref{Def:f_mu^k} with $\mu$ replaced by $\hat{\mu}$, is a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)} 
%
and $\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}=\|{\hat{\mu}}\|_{\mathrm{TV}}$. 
\end{proposition}
\begin{proof}
Note that the measure space $\mathcal{M}(\Theta)$ has the pre-dual space $C_0(\Theta)$. Hence, it follows from Proposition 1 of \cite{wang2021representer} that the linear independence of  the functionals in $\mathbb{K}_\mathcal{X}$ ensures the existence of a solution of problem \eqref{MNI in measure space}. Assume that ${\hat{\mu}}$ is a solution of problem \eqref{MNI in measure space}. We then obtain that ${\hat{\mu}}\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ and 
\begin{equation}\label{proof mu* is a solution}
        \|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|\mu\|_{\mathrm{TV}},\ \text{for all }\mu\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}.
\end{equation} 
By equations \eqref{dual bilinear on BNS} and \eqref{natural-map-predual} with $g:=\mathcal{K}_k(x_j,\cdot)$, we have for each $k\in\mathbb{N}_t$ and each $j\in\mathbb{N}_{m}$ that
    \begin{equation}\label{verify interpolation condition}
    \langle \mathcal{K}_k(x_j,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle \mu,\mathcal{K}_k(x_j,\cdot)\rangle_{C_0(\Theta)}, \ \mbox{for all}\ \mu\in\mathcal{M}(\Theta).
    \end{equation}
    Note that $\mathcal{K}_k(x_j,\cdot)\in C_0(\Theta)$ can be viewed as a bounded linear functional on $\mathcal{M}(\Theta)$ and 
    $$\langle \mu,\mathcal{K}_k(x_j,\cdot)\rangle_{C_0(\Theta)}=\langle \mathcal{K}_k(x_j,\cdot),\mu\rangle_{\mathcal{M}(\Theta)}.
    $$ 
    Substituting the above equation into the right hand of equation \eqref{verify interpolation condition} leads to 
    $$
    \langle \mathcal{K}_k(x_j,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle \mathcal{K}_k(x_j,\cdot),\mu\rangle_{\mathcal{M}(\Theta)}, \ \mbox{for all}\ \mu\in\mathcal{M}(\Theta).
    $$
    This implies that $        \mathbf{I}_{\mathcal{X}}(f_{\mu})=\widetilde{\mathbf{I}}_{\mathcal{X}}(\mu)$ for all $\mu\in\mathcal{M}(\Theta)$. As a result,  
    $\mu\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ if and only if 
 $f_{\mu}\in\mathcal{M}_{\mathcal{X},\mathbf{Y}}$. Since ${\hat{\mu}}\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$, we get that $f_{\hat\mu}\in\mathcal{M}_{\mathcal{X},\mathbf{Y}}$. It suffices to verify that 
  \begin{equation*}
\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}\leq\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}},\quad\text{for all }f_{\mu}\in{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}.
    \end{equation*}
    Let $f_{\mu}$ be an arbitrary element in ${\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$. For any $\nu\in\mathcal{M}(\Theta)$ satisfying $f_{\mu}=f_{\nu}$, there holds
    $f_{\nu}\in{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$. Thus,  $\nu\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$. It follows from inequality \eqref{proof mu* is a solution} that
\begin{equation}\label{Relation:TV-Norm}
     \|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|\nu\|_{\mathrm{TV}}. 
\end{equation}
By taking infimum of both sides of the inequality \eqref{Relation:TV-Norm} over $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\mu=f_\nu$ and noting the definition \eqref{banach space norm DNN} of the  norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$, we get that  
    %
    %
    %
\begin{equation}\label{proof mu* leq fnu}
        \|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}. 
\end{equation}
Again by the definition \eqref{banach space norm DNN} of the  norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$, we obtain that
\begin{equation}\label{proof fmu* leq mu*}    \|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}\leq\|{\hat{\mu}}\|_{\mathrm{TV}}.
\end{equation}
   Combining inequalities \eqref{proof mu* leq fnu} with \eqref{proof fmu* leq mu*}, we conclude that  $\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}\leq\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}$. Therefore, $f_{{\hat{\mu}}}$ is a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)}. %
   Moreover, by taking $\mu={\hat{\mu}}$ in \eqref{proof mu* leq fnu}, we get that $\|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}$. This together with inequality \eqref{proof fmu* leq mu*} leads to $\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}=\|{\hat{\mu}}\|_{\mathrm{TV}}$. 
\end{proof}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

We next derive a representer theorem for a solution of problem \eqref{MNI in measure space} by employing Lemma \ref{lemma: representer for MNI}. 
Applying Lemma \ref{lemma: representer for MNI} to problem \eqref{MNI in measure space} requires the representation of the extreme points of the subdifferential set $\partial\|\cdot\|_\infty(g)$ for any nonzero $g\in C_0(\Theta)$. Here, the subdifferential set $\partial\|\cdot\|_\infty(g)$ is a subset of the measure space $\mathcal{M}(\Theta)$. 
For each $g\in C_0(\Theta)$, let $\Theta(g)$ denote the subset of $\Theta$ where the function $g$ attains its maximum norm $\|g\|_\infty$, that is, 
\begin{equation*}\label{def: infinity set for function}
    \Theta(g):=\left\{\theta\in \Theta:|g(\theta)|=\|g\|_\infty\right\}.
\end{equation*}
For each $g\in C_0(\Theta)$, we introduce a subset of $\mathcal{M}(\Theta)$ by 
\begin{equation}\label{def: Omage f}
    \Omega(g):=\left\{\mathrm{sign}(g(\theta))\delta_\theta:\theta\in\Theta(g)\right\}.
\end{equation}
Lemma $26$ in \cite{wang2023sparse} essentially states that if $g\in C_0(\Theta)\backslash\{0\}$, then
\begin{equation}\label{extreme points of partial infinity norm}
\mathrm{ext}\left(\partial\|\cdot\|_{\infty}(g)\right)=\Omega(g).
\end{equation}
We denote by $\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}$ the solution set of the MNI problem \eqref{MNI in measure space}. We note that the MNI problem \eqref{MNI in measure space} shares the same dual problem \eqref{dual problem} with the MNI problem \eqref{MNI in RKBS B measure M(X)}. 

\begin{proposition}\label{prop: representer theorem for MNI  in measure space}
Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. 
%
Let $\hat g$ be the function defined by \eqref{stage 1 hat g}. Then for any $\hat \mu\in\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$, there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$, such that 
    \begin{equation}\label{eq: representer for measure space}
        \hat\mu=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell\mathrm{sign}(\hat g(\theta_\ell))\delta_{\theta_\ell}.
    \end{equation}
\end{proposition}
\begin{proof}
Note that the measure space $\mathcal{M}(\Theta)$ has the pre-dual space $C_0(\Theta)$  and the functionals $\mathcal{K}_k(x_j,\cdot)$, $k\in\mathbb{N}_t$, $j\in\mathbb{N}_m$, which belong to the pre-dual space $C_0(\Theta)$, are linearly independent. By Proposition 37 in \cite{wang2023sparse},  the function $\hat g$ defined by \eqref{stage 1 hat g} satisfies $\hat g\in\mathcal{V}_{\mathcal{N}}$ and
\begin{equation*}\label{non empty set: BN case tilde}
(\|\hat g\|_{\infty}\partial\|\cdot\|_\infty(\hat g))\cap{\widetilde{\mathcal{M}}}_{\mathcal{X},\mathbf{Y}}\neq\emptyset,
\end{equation*} 
in which the subdifferential set $\partial\|\cdot\|_\infty(\hat g)$ is a subset of the measure space $\mathcal{M}(\Theta)$. As a result, the hypothesis of Lemma \ref{lemma: representer for MNI} is satisfied. According to Lemma \ref{lemma: representer for MNI}, for any $\hat \mu\in\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$, there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $u_\ell\in\mathrm{ext}(\partial \|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$, such that 
\begin{equation}\label{proof: apply general MNI theorem to C0}
\hat\mu=\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell u_\ell.  
\end{equation}
It follows from equation \eqref{extreme points of partial infinity norm} that for each $\ell\in\mathbb{N}_{tm}$, we have that $u_\ell\in \Omega(\hat{g})$. By definition \eqref{def: Omage f} of the set $\Omega(\hat{g})$, for each $\ell\in\mathbb{N}_{tm}$, there exists $\theta_\ell\in\Theta(\hat g)$ such that $u_\ell=\mathrm{sign}(\hat g(\theta_\ell))\delta_{\theta_\ell}$. Therefore, we may rewrite the representation \eqref{proof: apply general MNI theorem to C0} of $\hat{\mu}$ as \eqref{eq: representer for measure space}. 
%
%
%
%
%
\end{proof}

Proposition \ref{prop: representer theorem for MNI  in measure space} provides a representation for an extreme point of the solution set of the MNI problem \eqref{MNI in measure space}. This solution can be converted via Proposition \ref{prop: MNI solution for measure space is solution for RKBS} to a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)}. We present this result in the next theorem.

\begin{theorem}\label{theorem: kernel representer theorem for MNI in BN}
Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. Let $\mathcal{V}_{\mathcal{N}}$ and $\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ be defined by \eqref{V_span_kernel} and \eqref{hyperplane in measure space}, respectively, and let $\hat g$ be the function defined by \eqref{stage 1 hat g}. Then the MNI problem \eqref{MNI in RKBS B measure M(X)} has a solution $\hat f$ in the form
\begin{equation}\label{representer solution of finite linear combination of MNI}
\hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell\mathrm{sign}(\hat g(\theta_\ell))\mathcal{K}(x,\theta_\ell),\ x\in\mathbb{R}^s,
\end{equation}  
for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$.
\end{theorem}

%
%
%
%
%
%

\begin{proof}
%
By Proposition 1 of \cite{wang2021representer}, the MNI problem \eqref{MNI in measure space} has at least one solution. That is, $\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}$ is nonempty and moreover, $\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$ is nonempty. We choose $\hat \mu\in\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$. Proposition \ref{prop: representer theorem for MNI  in measure space} ensures that there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$, such that $\hat\mu$ may be expressed as in equation \eqref{eq: representer for measure space}.
Since $\hat{\mu}$ is a solution of problem \eqref{MNI in measure space}, we get by Proposition \ref{prop: MNI solution for measure space is solution for RKBS} that $f_{\hat\mu}=[f_{\hat\mu}^k:k\in\mathbb{N}_t]$ is a solution of the MNI problem \eqref{MNI in RKBS B measure M(X)}. By definition \eqref{Def:f_mu^k} of $f_{\hat\mu}^k$, $k\in\mathbb{N}_t$, we have that 
\begin{equation}\label{Solution_Form}
f_{\hat\mu}^k(x)=\int_\Theta \mathcal{K}_k(x,\theta)d\hat{\mu}(\theta), \quad\text{for }x\in\mathbb{R}^s,  k\in\mathbb{N}_t. 
\end{equation}
Substituting representation \eqref{eq: representer for measure space} of $\hat\mu$ into the right-hand side of equation \eqref{Solution_Form} yields that 
\begin{equation*}
f_{\hat\mu}^k(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell\mathrm{sign}(\hat g(\theta_\ell))\mathcal{K}_k(x,\theta_\ell), \quad\text{for }x\in\mathbb{R}^s,  k\in\mathbb{N}_t. 
\end{equation*}
    By letting $\hat f:=f_{\hat\mu}$, we conclude that the MNI problem \eqref{MNI in RKBS B measure M(X)} has a solution $\hat f$ in the form of \eqref{representer solution of finite linear combination of MNI}. 
\end{proof}



We now return to considering the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)}. Our goal is to  establish representer theorems for a solution of the regularization problem. To ensure the existence of a solution of the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)}, we always assume that the loss function $\mathcal{Q}$ is lower
semi-continuous on $\mathbb{R}^{t\times m}$. 
Before establishing the representer theorems for a solution of problem \eqref{eq: regularization problem RKBS B measure M(X)}, we point out the relation between the solutions of this problem and the MNI problem \eqref{MNI in RKBS B measure M(X)}. We denote by $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ the solution set of problem \eqref{eq: regularization problem RKBS B measure M(X)}. By Proposition \ref{Existence-of-Solution}, $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ is nonempty. We then introduce a subset $\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ of $\mathbb{R}^{t\times m}$ by  
\begin{equation}\label{general Dy}
\mathcal{D}_{\mathcal{X},\mathbf{Y}}:=\mathbf{I}_{\mathcal{X}}(\mathbb{R}_{\mathcal{X},\mathbf{Y}}). 
\end{equation}
Now, recalling that $\mathbb{S}_{\mathcal{X},\mathbf{Y}}$ denotes the solution set of the MNI problem \eqref{MNI in RKBS B measure M(X)}, 
it follows from  Proposition 41 of \cite{wang2021representer} that
%
%
%
\begin{equation}\label{Proposition41-in- WangXu2}
\bigcup_{{\mathbf{Z}}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}}\mathbb{S}_{\mathcal{X},\mathbf{Z}}=\mathbb{R}_{\mathcal{X},\mathbf{Y}}. 
\end{equation}
Moreover, by Lemma 11 of \cite{wang2023sparse},  if the loss function $\mathcal{Q}$ is convex, then  
    \begin{equation}\label{relation_extreme_sets}
        \mathrm{ext}\left(\mathbb{R}_{\mathcal{X},\mathbf{Y}}\right)\subset\bigcup_{\mathbf{Z}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}}\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\mathbf{Z}}\right).
    \end{equation}

Below, we convert the representer theorem for a solution of problem \eqref{MNI in RKBS B measure M(X)} stated in Theorem \ref{theorem: direct representer theorem for MNI in BN} to that for the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} by making use of the relation between the solutions of problems \eqref{Proposition41-in- WangXu2} and \eqref{relation_extreme_sets}.


\begin{theorem}\label{theorem: representer for regularization}
    Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}$ are given, $\lambda>0$. Let $\mathcal{V}_{\mathcal{N}}$ be defined by \eqref{V_span_kernel} and $\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ be defined by \eqref{general Dy}.  
    \begin{enumerate}
    \item If $\mathcal{D}_{\mathcal{X},\mathbf{Y}}\neq\{\mathbf{0}\}$, then there exists $\hat f\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that 
    \begin{equation}\label{eq: regularization problem increasing case in BN}
        \hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}} \alpha_\ell h_\ell(x),\ x\in\mathbb{R}^s,
    \end{equation}
   for some $\hat g\in\mathcal{V}_{\mathcal{N}}$, $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.
    %
    \item If the loss function $\mathcal{Q}$ is convex, then every nonzero extreme point $\hat f$ of $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ has the form of
    \eqref{eq: regularization problem increasing case in BN} for some $\hat g\in\mathcal{V}_{\mathcal{N}}$, $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.
    \end{enumerate}
\end{theorem}

\begin{proof}
We first prove Item 1. Note that $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ is an nonempty set. It follows from the hypothesis $\mathcal{D}_{\mathcal{X},\mathbf{Y}}\neq\{\mathbf{0}\}$ that there exists $\hat{h}\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that $\hat{\mathbf{Z}}:=\mathbf{I}_{\mathcal{X}}(\hat{h})\neq 0$. According to equation \eqref{Proposition41-in- WangXu2} with noting that $\hat{\mathbf{Z}}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}$, we have that $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\subset\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ and thus, $\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\right)\subset\mathbb{R}_{\mathcal{X},\mathbf{Y}}$. We choose $\hat f\in\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\right)$ and verify that $\hat f$ can be represented as in  \eqref{eq: regularization problem increasing case in BN}. To this end, we choose $\hat{\mathbf{c}}:=[\hat{c}_{kj}:k\in\mathbb{N}_t,j\in\mathbb{N}_m]\in\mathbb{R}^{t\times m}$ to be a solution of the dual problem \eqref{dual problem} with $\mathbf{Y}$ replaced by $\hat{\mathbf{Z}}$. Let $\hat g$ be the function defined by \eqref{stage 1 hat g} with $\hat{\mathbf{c}}$. Theorem \ref{theorem: direct representer theorem for MNI in BN} ensures that $\hat f$, as an extreme point of the solution set $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}$, can be represented as in  \eqref{eq: regularization problem increasing case in BN} for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.

We next show Item 2. Assume that $\hat f$ is an arbitrary nonzero extreme point of $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$, that is $\hat f\in\mathrm{ext}(\mathbb{R}_{\mathcal{X},\mathbf{Y}})\backslash\{0\}$. Because the loss function $\mathcal{Q}$ is convex,  the inclusion relation  \eqref{relation_extreme_sets} is satisfied. By \eqref{relation_extreme_sets}, there exists $\hat{\mathbf{Z}}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ such that $\hat f\in\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\right)$. Clearly, $\hat{\mathbf{Z}}\neq 0.$ Assume to the contrary that $\hat{\mathbf{Z}}= 0.$ We then must have that $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}=\{0\}.$ As a result, $\hat f=0,$ which is a contradiction.
Again, let $\hat g$ be defined by \eqref{stage 1 hat g} with $\hat{\mathbf{c}}$ being a solution of problem \eqref{dual problem} with $\mathbf{Y}$ being replaced by $\hat{\mathbf{Z}}$. By Theorem \ref{theorem: direct representer theorem for MNI in BN}, we can represent $\hat f$ as in  \eqref{eq: regularization problem increasing case in BN} for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.
\end{proof}

Similarly to Theorem \ref{theorem: direct representer theorem for MNI in BN} for a solution of the MNI problem,
Theorem \ref{theorem: representer for regularization} ensures that 
each extreme point of the solution set of the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} lays in a {\it finite} dimensional manifold spanned by $tm$ elements  $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$ and it has an explicit and data-dependent representation.


We further show that there exists a solution of the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} that can be represented as a linear combination of a finite number of kernel sessions.
 

\begin{theorem}
Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}$ are given, $\lambda>0$. Let $\mathcal{V}_{\mathcal{N}}$ be defined by \eqref{V_span_kernel} and $\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ be defined by \eqref{general Dy}.  Then there exists $\hat f\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that 
\begin{equation}\label{eq: kernel representation regularization problem increasing case in BN}
\hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}} \gamma_\ell\mathrm{sign}(\hat g(\theta_\ell)) \mathcal{K}(x,\theta_\ell),\ x\in\mathbb{R}^s,
\end{equation}
for some $\hat g\in\mathcal{V}_{\mathcal{N}}$,  $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$.
\end{theorem}

\begin{proof}
Since $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ is nonempty and $\mathcal{D}_{\lambda,\mathbf{Y}}\neq\{\mathbf{0}\}$, there exists $\hat h \in \mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that $\hat{\mathbf{Z}}:=\mathbf{I}_{\mathcal{X}}(\hat h)\neq \mathbf{0}$. We choose $\hat g$ in the form of \eqref{stage 1 hat g}, where $\hat{\mathbf{c}}$ is a solution of problem \eqref{dual problem} with $\mathbf{Y}$ being replaced by $\hat{\mathbf{Z}}$. According to Theorem \ref{theorem: kernel representer theorem for MNI in BN}, the MNI problem \eqref{MNI in RKBS B measure M(X)} with $\mathbf{Y}:=\hat{\mathbf{Z}}$ has a solution  $\hat f$ in the form of \eqref{eq: kernel representation regularization problem increasing case in BN}, for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with  $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$. In other words, $\hat f\in \mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}$ and it has the form of \eqref{eq: kernel representation regularization problem increasing case in BN}. It follows from relation \eqref{Proposition41-in- WangXu2} that $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\subset\mathbb{R}_{\mathcal{X},\mathbf{Y}}$, which implies that $\hat f\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$.
\end{proof}



%
%

To close this section, we remark on relevant existing work on representer theorems for deep learning solution.
Representer theorems for deep learning have been investigated in \cite{bohn2019representer, unser2019representer}. 
Bohn et al. studied in \cite{bohn2019representer} the composition of reproducing kernel Hilbert spaces as the hypothesis space, and derived a representer theorem for deep kernel learning, which led to a solution in the form of composition of a finite number of kernel sessions. In \cite{unser2019representer}, Unser proposed to optimize the activation functions in the deep neural network by a second-order total-variation regularization, and the resulting representer theorem ensures that the activation function for each node in the optimal network is a linear spline with adaptive knots. 


%

%



 



 








\section{Concluding Remarks}
We have introduced the hypothesis
space $\mathcal{B}_{\mathcal{N}}$  for deep learning. The hypothesis space that we have come up with is a vector-valued RKBS which has a unique vector-valued reproducing kernel $\mathcal{K}$ and the weak* completion of the vector space $\mathcal{B}_{\mathbb{W}}$ which is the linear span of the primitive set $\mathcal{A}_{\mathbb{W}}$ for deep learning. The hypothesis space allows us to understand mathematical insights of deep learning. Specifically, by exploiting the hypothesis space, we have developed representer theorems for solutions of two deep learning models the minimum norm interpolation and regularization problem with deep neural networks.


In the remaining part of this section, we discuss  relations among several learning models considered in this paper and comment on advantages of learning in the proposed hypothesis space $\mathcal{B}_{\mathcal{N}}$.

First, we remark on relations among several learning models studied in this paper.
Suppose that the loss function $\mathcal{L}(f_\mu,\mathbb{D}_m)$ takes the form of \eqref{loss:An example} with a function $\mathcal{Q}:\mathbb{R}^{t\times m}\to \mathbb{R}_+$ satisfying the condition $\mathcal{Q}(0)=0$. If $\hat{f}_\mu\in \mathcal{B}_\mathcal{N}$ is a solution of \eqref{MNI-original}, then we have that $\mathcal{L}(\hat{f}_\mu, \mathbb{D}_m)=0$ and thus, $\hat{f}_\mu$ is a solution
of the learning model \eqref{LearningMethodinRKBS}. Furthermore, let $f_{\mu,\lambda}$ be a solution of the regularized learning problem 
\eqref{eq: regularization problem RKBS B measure M(X)}, and  denote by $f_{\mu,0}$ the limit of $f_{\mu,\lambda}$ as $\lambda\to 0$ if the limit exists. Then, we clearly have that $f_{\mu,0}$ is a solution of the learning model \eqref{LearningMethodinRKBS}. Therefore, learning models \eqref{eq: regularization problem RKBS B measure M(X)} and 
\eqref{MNI-original} are favorable stable substitutes of learning model  \eqref{LearningMethodinRKBS}, which may suffer from instability.

Learning in the RKBS space $\mathcal{B}_{\mathcal{N}}$ has several advantages.
First of all, the primitive learning model \eqref{BasicLearningMethod-equivalent}, a model commonly considered in the machine learning community, is not guaranteed to have a solution since the primitive set $\mathcal{A}_\mathbb{W}$ has neither algebraic nor topological structures. Since the space $\mathcal{B}_\mathcal{N}$, which has the pre-dual space $\mathcal{S}$, is the completion of the linear span $\mathcal{B}_\mathbb{W}$ (of $\mathcal{A}_\mathbb{W}$) in the weak* topology, it is the smallest RKBS that contains $\mathcal{A}_\mathbb{W}$. On one hand, it embraces the intrinsic features of the set $\mathcal{A}_\mathbb{W}$ and on the other hand, it has desired algebraic and topological structures which allow us to conduct mathematical analysis of learning on it. Hence, it is natural to consider the RKBS $\mathcal{B}_\mathcal{N}$ as a hypothesis space for deep learning problems. Due to the algebraic and topological structures of  $\mathcal{B}_\mathcal{N}$, unlike model \eqref{BasicLearningMethod-equivalent}, the learning model \eqref{eq: regularization problem RKBS B measure M(X)} is guaranteed to have a solution under a mild condition.
%
Last but not least, the reproducing kernel of the RKBS $\mathcal{B}_{\mathcal{N}}$ furnishes a learning solution in the RKBS $\mathcal{B}_{\mathcal{N}}$ a representation in terms of the reproducing kernel, which leads to a representer theorem of deep learning. The resulting representer theorems reveal that although the learning models on the proposed hypothesis space are of infinite dimension, their solutions lay in finite dimensional manifolds and can be expressed as a linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel. 
In summary, introducing the hypothesis space $\mathcal{B}_\mathcal{N}$ to deep learning enables us to understand the insights of deep learning and provide a foundation for its mathematical analysis. 


\section*{Acknowledgement}

R. Wang is supported in part by the Natural Science Foundation of China under grant 12171202 and by the National Key Research and Development Program of China under grants 2020YFA0714101; Y. Xu is supported in part by the US National Science Foundation under grant DMS-2208386, and by the US National Institutes of Health under grant R21CA263876. M. Yan is supported in part by Cheng Fund from College of Science at Old Dominion University. All correspondence should be sent to Y. Xu.





 

\bibliographystyle{siam}
\bibliography{ref.bib}

\end{document}





Suppose that $h, \widetilde{h}\in \mathcal{B}_{\mathbb{W}}$. According to the definition \eqref{space B Delta} of  $\mathcal{B}_{\mathbb{W}}$, there exist $n, \widetilde{n}\in \mathbb{N}$ such that
$
h=\sum_{l=1}^n c_l\mathcal{N}(\cdot,\theta_l)
$
and
$
\widetilde{h}=\sum_{l=1}^n\widetilde{c}_l\mathcal{N}(\cdot,\widetilde{\theta_l})$ where $\theta_l:=\{\mathbf{W}_j^l,\mathbf{b}_j^l\}_{j=1}^D$, $\tilde{\theta}_l:=\{\widetilde{\mathbf{W}}_j^l,\widetilde{\mathbf{b}}_j^l\}_{j=1}^D$, $l\in\mathbb{N}_n$. For any $\alpha, \widetilde{\alpha}\in \mathbb{R}$, we have that
$
\alpha h+\widetilde{\alpha}\widetilde{h}=\sum_{l=1}^{\widehat{n}}\widehat{c}_l\mathcal{N}(\cdot,\widehat{\theta_l}),
$
where $\widehat{n}:=n+\widetilde{n}$, $\widehat{c}_l:=\alpha c_l$, $\widehat{\theta_l}:=\theta_l$, for $l\in\mathbb{N}_n$, and $\widehat{c}_l:=\alpha c_l$, $\widehat{\theta_l}:=\widetilde{\theta_l}$, for $l\in\mathbb{N}_{n+\widetilde{n}}\backslash\mathbb{N}_n$. That is, 
$
\alpha h+\widetilde{\alpha}\widetilde{h}\in \mathcal{B}_{\mathbb{W}}
$,
which ensures that $\mathcal{B}_{\mathbb{W}}$ is a vector space.









Let $\mathcal{M}(\Box)$ denote the space of finite Radon measures on a measurable set $\Box$. We equip the space $\mathcal{M}(\mathbb{R}^{m_j\times m_{j-1}})$ with the total variation norm 
$\|\mu\|_{\mathrm{TV}}:=|\mu|(\mathbb{R}^{m_j\times m_{j-1}})$ and likewise, the space $\mathcal{M}(\mathbb{R}^{m_j})$ with 
$\|\mu\|_{\mathrm{TV}}:=|\mu|(\mathbb{R}^{m_j})$. It can be shown that $\mathcal{M}(\mathbb{R}^{m_j\times m_{j-1}})$ and $\mathcal{M}(\mathbb{R}^{m_j})$ for $j\in\mathbb{N}_k$ are all Banach spaces.
Because of \eqref{Def:Theta}, following \cite{} {\bf Mingsong: Please find an appropriate reference for this result.} we obtain that
\begin{equation}\label{Def:M(Theta)}
    \mathcal{M}(\Theta)=\bigotimes_{j\in\mathbb{N}_k}\left[\mathcal{M}(\mathbb{R}^{m_j\times m_{j-1}})\otimes \mathcal{M}(\mathbb{R}^{m_j})\right]
\end{equation}
and $\mathcal{M}(\Theta)$ is a Banach space.
It follows from \eqref{Def:M(Theta)} for $\mu\in \mathcal{M}(\Theta)$ that
\begin{equation}\label{measure-mu}
    \mu=\mathop{\times}_{j \in \mathbb{N}_k}\left(\mu_{\mathbf{W}_j}\times\mu_{\mathbf{b}_j}\right)\ \  \mbox{where}\ \ \mu_{\mathbf{W}_j}\in\mathcal{M}(\mathbb{R}^{m_j\times m_{j-1}}), \ \mu_{\mathbf{b}_j}\in\mathcal{M}(\mathbb{R}^{m_j}), \ j\in\mathbb{N}_k.
\end{equation}
In the remaining part of this paper, we always identify $\theta\in \Theta$ with $\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^k$, where $\mathbf{W}_j\in \mathbb{R}^{m_j\times m_{j-1}},  \mathbf{b}_j\in\mathbb{R}^{m_j}$, unless stated otherwise.
Thus, for $\theta:=\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^k\in \Theta$, there holds
$$
d\mu(\theta)=\prod_{j \in \mathbb{N}_k}\left(d\mu_{\mathbf{W}_j}d\mu_{\mathbf{b}_j}\right).
$$
For ${x}\in \mathbb{R}^s$ and $\theta\in \Theta$, we define
\begin{equation}\label{Def:kernel}
    \mathcal{N}({x},\theta):=\mathcal{N}_k({x},\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^k).
\end{equation}
Note that the function $\mathcal{N}$ defined by equation \eqref{Def:kernel} may be viewed as an asymmetric kernel.
We consider a class of functions
\begin{equation}\label{Def:Space-B_N}
    B_{\mathcal{N}} := \left\{\int_{\Theta}\mathcal{N}(\cdot,\theta) d\mu(\theta): \mu\in 
    \mathcal{M}(\Theta)\right\}. 
\end{equation}
    

Let $d$ be the total number of unknow n parameters in the neural network $\mathcal{N}$, that is 
\begin{equation*}\label{dimension d}
    d:=\sum_{j\in\mathbb{N}_k} m_j(m_{j-1}+1). 
\end{equation*} 
It is clear that the product space  $\Theta$ is isometrically isomorphic to $\mathbb{R}^d$. The following lemma indicates that the set of product Radon measures is the subset of the space of Radon measures on product spaces. 
     
\begin{lemma}\label{lemma: product measure space is subset of M(Rd)}
    Suppose that $d=\sum_{j\in\mathbb{N}_k} d_j$. Then 
    \begin{equation}\label{product measures are subset}
        \left\{\mathop{\times}_{j \in \mathbb{N}_k}\mu_j:\mu_j\in\mathcal{M}(\mathbb{R}^{d_j}),j\in\mathbb{N}_k\right\}\subset\mathcal{M}(\mathbb{R}^d). 
    \end{equation}
\end{lemma}
\begin{proof}
    We notice that for any $l\in\mathbb{N}$, the space $\mathcal{M}(\mathbb{R}^l)$ is composed of finite Radon measures, which are $\sigma$-finite. It follows from Theorem 7.20 in \cite{folland1999real} that the product of Radon measures is again a Radon measure on the product space, and hence for any $\mu_j\in\mathcal{M}(\mathbb{R}^{d_j}),j\in\mathbb{N}_k$, the product measure $\mathop{\times}_{j \in \mathbb{N}_k}\mu_j$ is a Radon measure on the product space $\otimes_{j\in\mathbb{N}_k}\mathbb{R}^{d_j}=\mathbb{R}^d$. Moreover, according to Fubini's theorem, we find that
    \begin{equation}
        \left\|\mathop{\times}_{j \in \mathbb{N}_k}\mu_j\right\|_{\mathrm{TV}}=\left|\mathop{\times}_{j \in \mathbb{N}_k}\mu_j\right|(\mathbb{R}^d)\leq\prod_{j\in\mathbb{N}_k}|\mu_j|(\mathbb{R}^{d_j})=\prod_{j\in\mathbb{N}_k}\|\mu_{j}\|_{\mathrm{TV}}<\infty,
    \end{equation}
    which implies that the product measure $\mathop{\times}_{j \in \mathbb{N}_k}\mu_j$ is of finite total variation norm, and hence 
    \begin{equation*}
        \mathop{\times}_{j \in \mathbb{N}_k}\mu_j\in\mathcal{M}(\mathbb{R}^d).
    \end{equation*}
    Therefore, the relation \eqref{product measures are subset} holds. 
\end{proof}
Lemma \ref{lemma: product measure space is subset of M(Rd)} motivates us to extend the set $B_{\mathcal{N}}$ to a larger Banach space. 


In a manner similar to the MNI problem \eqref{MNI in RKBS B measure M(X)}, we also introduce a related regularization problem in the measure space $\mathcal{M}(\Theta)$ as  
\begin{equation}\label{eq: regularization problem in measure space}
    \inf \left\{\mathcal{Q}(\widetilde{\mathbf{I}}_{\mathcal{X}}(\mu)-\mathbf{Y})+\lambda\|\mu\|_{\mathrm{TV}}:  \mu\in {\mathcal{M}(\Theta)}\right\}
\end{equation}
and provide a solution representation for this optimization. The resulting representer theorem for problem
\eqref{eq: regularization problem in measure space} will be translated to problem \eqref{eq: regularization problem RKBS B measure M(X)} by using the relation between the solutions of these two problems.
The next proposition concers with the relation between the solutions of problems \eqref{eq: regularization problem RKBS B measure M(X)} and \eqref{eq: regularization problem in measure space}. 
\begin{proposition}\label{prop: regularization solution for measure space is solution for RKBS}
    If ${\hat{\mu}}$ is a solution of the regularization problem \eqref{eq: regularization problem in measure space} with $\mathbf{Y}$, then the vector-valued function $f_{{\hat{\mu}}}$, defined by \eqref{vector-valued fmu}, is a solution of the regularization problem \eqref{eq: regularization problem RKBS B measure M(X)} with $\mathbf{Y}$. Moreover, the infimums of problems \eqref{eq: regularization problem RKBS B measure M(X)} and \eqref{eq: regularization problem in measure space} are equal.
\end{proposition}
\begin{proof}
Suppose that ${\hat{\mu}}$ is a solution of problem \eqref{eq: regularization problem in measure space} with $\mathbf{Y}$. It follows that  
\begin{equation}\label{proof regularization leq Qy}
\mathcal{Q}(\widetilde{\mathbf{I}}_{\mathcal{X}}(\hat\mu)-\mathbf{Y})+\lambda\|\hat\mu\|_{\mathrm{TV}}\leq \mathcal{Q}(\widetilde{\mathbf{I}}_{\mathcal{X}}(\nu)-\mathbf{Y})+\lambda\|\nu\|_{\mathrm{TV}},\ \mbox{for all}\ \nu\in\mathcal{M}(\Theta).   
\end{equation}
Let $f_{\mu}$ be an arbitrary element in $\mathcal{B}_{\mathcal{N}}$. It suffices to verify that \begin{equation}\label{proof regularization f hat mu leq f nu}
    \mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\hat\mu})-\mathbf{Y})+\lambda\|f_{\hat\mu}\|_{\mathcal{B}_{\mathcal{X}}}\leq\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\mu})-\mathbf{Y})+\lambda\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}.
\end{equation}
As pointed out in the proof of Proposition \ref{prop: MNI solution for measure space is solution for RKBS}, there holds that $        \mathbf{I}_{\mathcal{X}}(f_{w})=\widetilde{\mathbf{I}}_{\mathcal{X}}(w)$ for all $w\in\mathcal{M}(\Theta)$. Specially, $        \mathbf{I}_{\mathcal{X}}(f_{\hat\mu})=\widetilde{\mathbf{I}}_{\mathcal{X}}(\hat\mu)$. It
follows from definition \eqref{banach space norm DNN} of $\|\cdot\|_{\mathcal{B}_{\mathcal{N}}}$ that $\|f_{\hat\mu}\|_{\mathcal{B}_{\mathcal{N}}}\leq\|\hat\mu\|_{\mathrm{TV}}$. As a result, we get that  
\begin{equation}\label{proof regularization f hat mu leq hat mu}
\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\hat\mu})-\mathbf{Y})+\lambda\|f_{\hat\mu}\|_{\mathcal{B}_{\mathcal{N}}}\leq \mathcal{Q}(\widetilde{\mathbf{I}}_{\mathcal{X}}(\hat\mu))-\mathbf{Y})+\lambda\|\hat\mu\|_{\mathrm{TV}}.
\end{equation}
For any $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\nu=f_\mu$, there holds $        \widetilde{\mathbf{I}}_{\mathcal{X}}(\nu)=\mathbf{I}_{\mathcal{X}}(f_{\mu})$. 
Through taking infimum of both sides of the inequality \eqref{proof regularization leq Qy} over $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\nu=f_\mu$ with noting the above equation, we obtain that 
\begin{equation}\label{proof regularization leq inf last one}
    \mathcal{Q}(\widetilde{\mathbf{I}}_{\mathcal{X}}(\hat\mu)-\mathbf{Y})+\lambda\|\hat\mu\|_{\mathrm{TV}}\leq  \mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\mu})-\mathbf{Y})+\lambda\inf\left\{\|\nu\|_{\mathrm{TV}}: f_{\nu}=f_{\mu},\nu\in\mathcal{M}(\Theta)\right\}. 
\end{equation}
This together with definition \eqref{banach space norm DNN} of $\|\cdot\|_{\mathcal{B}_{\mathcal{N}}}$ yields that 

\begin{equation}\label{proof regularization leq inf last one 1}
\mathcal{Q}(\widetilde{\mathbf{I}}_{\mathcal{X}}(\hat\mu)-\mathbf{Y})+\lambda\|\hat\mu\|_{\mathrm{TV}}\leq  \mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\mu})-\mathbf{Y})+\lambda \|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}. 
\end{equation}
Substituting inequality \eqref{proof regularization leq inf last one 1} into the right hand side of inequality \eqref{proof regularization f hat mu leq hat mu} leads to inequality \eqref{proof regularization f hat mu leq f nu}. Hence, we conclude that $f_{\hat \mu}$ is a solution of problem \eqref{eq: regularization problem RKBS B measure M(X)}.
By taking $\mu=\hat \mu$ in inequality \eqref{proof regularization leq inf last one 1} and combining with inequality \eqref{proof regularization f hat mu leq hat mu}, we get that
$$
\mathcal{Q}(\widetilde{\mathbf{I}}_{\mathcal{X}}(\hat\mu)-\mathbf{Y})+\lambda\|\hat\mu\|_{\mathrm{TV}}=  \mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\hat\mu})-\mathbf{Y})+\lambda \|f_{\hat\mu}\|_{\mathcal{B}_{\mathcal{N}}},
$$
which shows that problems \eqref{eq: regularization problem RKBS B measure M(X)} and \eqref{eq: regularization problem in measure space} attain at the same infimum. 
\end{proof}




