---
abstract: |
  This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak\* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel.
author:
- Rui Wang[^1], Yuesheng Xu[^2] and Mingsong Yan[^3]
bibliography:
- ref.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Hypothesis Spaces for Deep Learning
---





**Key words**: Reproducing kernel Banach space, deep learning, deep neural network, representer theorem for deep learning

# Introduction

Deep learning has been a huge success in applications. Mathematically, its success is due to the use of deep neural networks (DNNs), neural networks of multiple layers, to describe decision functions. Various mathematical aspects of DNNs as an approximation tool were investigated recently in a number of studies . As pointed out in , learning processes do not take place in a vacuum. Classical learning methods took place in a reproducing kernel Hilbert space (RKHS) , which leads to representation of learning solutions in terms of a combination of a finite number of kernel sessions of a universal kernel . Reproducing kernel Hilbert spaces as appropriate hypothesis spaces for classical learning methods provide a foundation for mathematical analysis of the learning methods. A natural and imperative question is what are appropriate hypothesis spaces for deep learning. Although hypothesis spaces for learning with shallow neural networks (networks of one hidden layer) were investigated recently in a number of studies, (e.g. ), appropriate hypothesis spaces for deep learning are still absent. The goal of the present study is to understand this imperative theoretical issue.

The road-map of constructing the hypothesis space for deep learning may be described as follows. We treat a DNN as a function of two variables, one being the physical variable and the other being the parameter variable. We then consider the set of the DNNs as functions of the physical variable for the parameter variable taking all elements of the set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. Upon completing the linear span of the DNN set in a weak\* topology, we construct a Banach space of functions of the physical variable. We establish that the resulting Banach space is a reproducing kernel Banach space (RKBS), on which point-evaluation functionals are continuous, and construct an asymmetric reproducing kernel, for the space, which is a function of the two variables, the physical variable and the parameter variable. We regard the constructed RKBS as the hypothesis space for deep learning. We remark that when deep neural networks reduce to shallow network (having only one hidden layer), our hypothesis space coincides the space for shallow learning studied in .

Upon introducing the hypothesis space for deep learning, we investigate two learning models, the regularized learning and minimum interpolation problem in the resulting RKBS. We establish representer theorems for solutions of the learning models by employing theory of the reproducing kernel Banach space developed in and representer theorems for solutions of learning in a general RKBS established in . Like the representer theorems for the classical learning in RKHSs, the resulting representer theorems for the two deep learning models in the RKBS reveal that although the learning models are of infinite dimension, their solutions lay in finite dimensional manifolds. More specifically, they can be expressed as a linear combination of a finite number of kernel sessions, the reproducing kernel evaluated the parameter variable at points determined by given data. The representer theorems established in this paper is data-dependent. Even when deep neural networks reduce to a shallow network, the corresponding representer theorem is still new to our best acknowledge. The hypothesis space and the representer theorems for the two deep learning models in it provide us prosperous insights of deep learning and supply deep learning a sound mathematical foundation for further investigation.

We organize this paper in six sections. We describe in Section 2 an innate deep learning model with DNNs. Aiming at formulating reproducing kernel Banach spaces as hypothesis spaces for deep learning, in Section 3 we elucidate the notion of vector-valued reproducing kernel Banach spaces. Section 4 is entirely devoted to the development of the hypothesis space for deep learning. We specifically show that the completion of the linear span of the primitive DNN set, pertaining to the innate learning model, in a weak\* topology is an RKBS, which constitutes the hypothesis space for deep learning. In Section 5, we study learning models in the RKBS, establishing representer theorems for solutions of two learning models (regularized learning and minimum norm interpolation) in the hypothesis space. We conclude this paper in Section 6 with remarks on advantages of learning in the proposed hypothesis space.

# Learning with Deep Neural Networks

We describe in this section an innate learning model with DNNs, considered wildly in the machine learning community.

We first recall the notation of DNNs. Let $s$ and $t$ be positive integers. A DNN is a vector-valued function from $\mathbb{R}^s$ to $\mathbb{R}^t$ formed by compositions of functions, each of which is defined by an activation function applied to an affine map. Specifically, for a given univariate function $\sigma: \mathbb{R}\to\mathbb{R}$, we define a vector-valued function by $$\label{activationF}
\sigma({x}):=[\sigma(x_1),\dots,\sigma(x_s)]^\top, \ \ \mbox{for}\ \ {x}:=[x_1, x_2,\dots, x_s]^\top\in\mathbb{R}^s.$$ For each $n\in\mathbb{N}$, let $\mathbb{N}_n:=\{1,2,\ldots,n\}$. For $k$ vector-valued functions $f_j$, $j\in\mathbb{N}_k$, where the range of $f_j$ is contained in the domain of $f_{j+1}$, for $j\in\mathbb{N}_{k-1}$, we denote the consecutive composition of $f_j$, $j\in\mathbb{N}_k$, by $$\label{consecutive_composition}
    \bigodot_{j=1}^k f_j:=f_k\circ f_{k-1}\circ\cdots\circ f_2\circ f_1,$$ whose domain is that of $f_1$. Suppose that $D\in \mathbb{N}$ is prescribed and fixed. Throughout this paper, we always let $m_0:=s$ and $m_D:=t$. We specify positive integers $m_j$, $j\in \mathbb{N}_{D-1}$. For $\mathbf{W}_j\in\mathbb{R}^{m_j\times m_{j-1}}$ and $\mathbf{b}_j\in\mathbb{R}^{m_j}$, $j\in\mathbb{N}_D$, a DNN is a function defined by $$\label{DNN}
\mathcal{N}^D({x}):=\left(\mathbf{W}_D\bigodot_{j=1}^{D-1} \sigma(\mathbf{W}_j \cdot+\mathbf{b}_j)+\mathbf{b}_D\right)({x}),\ \ {x}\in\mathbb{R}^s.$$ Note that $x$ is the input vector and $\mathcal{N}^D$ has $D-1$ hidden layers and an output layer, which is the $D$-th layer.

A DNN may be represented in a recursive manner. From definition , a DNN can be defined recursively by $$\label{Step1}
    \mathcal{N}^1({x}):=\mathbf{W}_1 {x}+\mathbf{b}_1, \ \ {x}\in \mathbb{R}^s$$ and $$\label{Recursion}
    \mathcal{N}^{j+1}({x}):=\mathbf{W}_{j+1}\sigma(\mathcal{N}^j({x}))+\mathbf{b}_{j+1}, \ \ {x}\in \mathbb{R}^s, \ \ \mbox{for all} \ \ j\in \mathbb{N}_{D-1}.$$ We write $\mathcal{N}^D$ as $\mathcal{N}^D(\cdot,\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D)$ when it is necessary to indicate the dependence of DNNs on the parameters. In this paper, when we write the set $\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D$ associated with the neural network $\mathcal{N}^D$, we implicitly give it the order inherited from the definition of $\mathcal{N}^D$. Throughout this paper, we assume that the activation function $\sigma$ is continuous.

It is advantageous to consider the DNN $\mathcal{N}^D$ defined above as a function of two variables, one being the physical variable $x\in \mathbb{R}^s$ and the other being the parameter variable $\theta:=\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D$. Given positive integers $m_j$, $j\in \mathbb{N}_{D-1}$, we let $$\label{width-set}
    \mathbb{W}:=\{m_j: j\in\mathbb{N}_{D-1}\}$$ denote the width set and define the primitive set of DNNs of $D$ layers by $$\label{Set_A}
    \mathcal{A}_{\mathbb{W}}:=\left\{ \mathcal{N}^D(\cdot,\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D): \mathbf{W}_j\in \mathbb{R}^{m_j\times m_{j-1}},  \mathbf{b}_j\in\mathbb{R}^{m_j}, j\in\mathbb{N}_D\right\}.$$ Clearly, the set $\mathcal{A}_{\mathbb{W}}$ defined by depends not only on $\mathbb{W}$ but also on $D$. For the sake of simplicity, we will not indicate the dependence on $D$ in our notation when ambiguity is not caused. For example, we will use $\mathcal{N}$ for $\mathcal{N}^D$. Moreover, an element of $\mathcal{A}_{\mathbb{W}}$ is a vector-valued function mapping from $\mathbb{R}^s$ to $\mathbb{R}^t$. We shall understand the set $\mathcal{A}_{\mathbb{W}}$. To this end, we define the parameter space $\Theta$ by letting $$\label{Def:Theta}
\Theta=\Theta_{\mathbb{W}}:= \bigotimes_{j\in\mathbb{N}_D}(\mathbb{R}^{m_j\times m_{j-1}}\otimes \mathbb{R}^{m_j}).$$ Note that $\Theta$ is measurable. For ${x}\in \mathbb{R}^s$ and $\theta\in \Theta$, we define $$\label{Def:kernel}
    \mathcal{N}({x},\theta):=\mathcal{N}^D({x},\{\mathbf{W}_j,\mathbf{b}_j\}_{j=1}^D).$$ For ${x}\in \mathbb{R}^s$ and $\theta\in \Theta$, there holds $\mathcal{N}({x},\theta)\in \mathbb{R}^t$. In this notation, set $\mathcal{A}_{\mathbb{W}}$ may be written as $$\label{Set_A*}
    \mathcal{A}_{\mathbb{W}}=\left\{ \mathcal{N}(\cdot,\theta): \theta\in \Theta_{\mathbb{W}} \right\}.$$

We now describe the innate learning model with DNNs. Suppose that a training dataset $$\label{Dataset}
\mathbb{D}_m:=  \{(x_j,y_j) \in\mathbb{R}^s\times\mathbb{R}^t:j\in\mathbb{N}_m\}$$ is given and we would like to train a neural network from the dataset. We denote by $\mathcal{L}(\mathcal{N},\mathbb{D}_m): \Theta\to \mathbb{R}$ a loss function determined by the dataset $\mathbb{D}_m$. For example, a loss function may take the form $$\mathcal{L}(\mathcal{N},\mathbb{D}_m)(\theta):=\sum_{j\in\mathbb{N}_m}\|\mathcal{N}(x_j,\theta)-y_j\|,$$ where $\|\cdot\|$ is a norm of $\mathbb{R}^t$. Given a loss function, a typical deep learning model is to train the parameters $\theta\in \Theta_\mathbb{W}$ from the training dataset $\mathbb{D}_m$ by solving the optimization problem $$\label{BasicLearningMethod}
    \min\{\mathcal{L}(\mathcal{N},\mathbb{D}_m)(\theta): \theta\in\Theta_\mathbb{W}\},$$ where $\mathcal{N}$ has the form in equation . Equivalently, optimization problem may be written as $$\label{BasicLearningMethod-equivalent}
    \min\{\mathcal{L}(\mathcal{N},\mathbb{D}_m): \mathcal{N}\in\mathcal{A}_{\mathbb{W}}\}.$$ Model is an innate learning model considered wildly in the machine learning community. Note that the set $\mathcal{A}_{\mathbb{W}}$ lacks either algebraic or topological structures. It is difficult to conduct mathematical analysis for learning model . Even the existence of its solution is not guaranteed.

We introduce a vector space that contains $\mathcal{A}_{\mathbb{W}}$ and consider learning in the vector space. For this purpose, given a set $\mathbb{W}$ of weight widths defined by , we define the set $$\label{space B Delta}
\mathcal{B}_{\mathbb{W}}:=\left\{ \sum_{l=1}^n c_l\mathcal{N}(\cdot,\theta_l): c_l\in\mathbb{R}, \theta_l\in \Theta_{\mathbb{W}}, l\in \mathbb{N}_n, n\in \mathbb{N}\right\}.$$ In the next proposition, we present properties of $\mathcal{B}_{\mathbb{W}}$.

<div class="proposition">

**Proposition 1**.  * If $\mathbb{W}$ is the width set defined by , then*

*(i) $\mathcal{B}_{\mathbb{W}}$ defined by is the smallest vector space on $\mathbb{R}$ that contains the set $\mathcal{A}_{\mathbb{W}}$,*

*(ii) $\mathcal{B}_{\mathbb{W}}$ is of infinite dimension,*

*(iii) $\mathcal{B}_{\mathbb{W}}\subset\bigcup_{n\in\mathbb{N}}\mathcal{A}_{n\mathbb{W}}$.*

</div>

<div class="proof">

*Proof.* It is clear that $\mathcal{B}_{\mathbb{W}}$ may be identified as the linear span of $\mathcal{A}_{\mathbb{W}}$, that is, $$\mathcal{B}_{\mathbb{W}}=\text{span} \left\{\mathcal{N}(\cdot,\theta): \theta\in \Theta_{\mathbb{W}}\right\}.$$ Thus, $\mathcal{B}_{\mathbb{W}}$ is the smallest vector space containing $\mathcal{A}_{\mathbb{W}}$. Item (ii) follows directly from the definition of $\mathcal{B}_{\mathbb{W}}$.

It remains to prove Item (iii). To this end, we let $f\in\mathcal{B}_{\mathbb{W}}$. By the definition of $\mathcal{B}_{\mathbb{W}}$, there exist $n'\in \mathbb{N}$, $c_l\in\mathbb{R}$, $\theta_l\in \Theta_\mathbb{W}$, for $l\in \mathbb{N}_{n'}$ such that $$f(\cdot)=\sum_{l=1}^{n'} c_l\mathcal{N}(\cdot,\theta_l).$$ It suffices to show that $f\in\mathcal{A}_{n'\mathbb{W}}$. Noting that $\theta_l:=\{\mathbf{W}_j^l,\mathbf{b}_j^l\}_{j=1}^D$, for $l\in\mathbb{N}_{n'}$, we set $$\widetilde{\mathbf{W}}_{1}:=\left[\begin{array}{c}
             \mathbf{W}_1^1  \\
              \mathbf{W}_1^2\\
              \vdots\\
              \mathbf{W}_1^{n'}
\end{array}\right],\ \
\widetilde{\mathbf{W}}_j:=\left[\begin{array}{cccc}
\mathbf{W}_j^1 & \mathbf{0} & \cdots & \mathbf{0} \\
\mathbf{0} & \mathbf{W}_j^2 & \cdots & \mathbf{0} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{W}_j^{n'}
\end{array}\right],\ j\in\mathbb{N}_{D-1}\backslash\{1\}, 
\ \
\widetilde{\mathbf{b}}_{j}:=\left[\begin{array}{c}
             \mathbf{b}_j^1  \\
              \mathbf{b}_j^2\\
              \vdots\\
              \mathbf{b}_j^{n'}
\end{array}\right],\ j\in\mathbb{N}_{D-1},$$ and $$\widetilde{\mathbf{W}}_D:=\left[\begin{array}{cccc}
c_1\mathbf{W}_D^1 & c_2\mathbf{W}_D^2 & \cdots & c_{n'}\mathbf{W}_D^{n'}
\end{array}\right],\quad \widetilde{\mathbf{b}}_{D}:=\sum\limits_{j=1}^{n'} c_j \mathbf{b}_D^j.$$ Clearly, we have that $\widetilde{\mathbf{W}}_{1}\in\mathbb{R}^{(n'm_1)\times {m_0}}$, $\widetilde{\mathbf{b}}_{j}\in\mathbb{R}^{n'm_j}$, $j\in\mathbb{N}_{D-1}$, $\widetilde{\mathbf{W}}_j\in\mathbb{R}^{(n'm_j)\times (n'm_{j-1})}$, $j\in\mathbb{N}_{D-1}\backslash\{1\}$, $\widetilde{\mathbf{W}}_D\in\mathbb{R}^{m_D\times (n'm_{D-1})}$, and $\widetilde{\mathbf{b}}_{D}\in\mathbb{R}^{m_D}$. Direct computation confirms that $f(\cdot)=\mathcal{N}(\cdot,\widetilde{\theta})$ with $\widetilde{\theta}:=\{\widetilde{\mathbf{W}}_j,\widetilde{\mathbf{b}}_j\}_{j=1}^D$. By definition , $f\in\mathcal{A}_{n'\mathbb{W}}$. ◻

</div>

Proposition reveals that $\mathcal{B}_{\mathbb{W}}$ is the smallest vector space that contains $\mathcal{A}_{\mathbb{W}}$. Hence, it is a reasonable substitute of $\mathcal{A}_{\mathbb{W}}$. Motivated by Proposition , we propose the following alternative learning model $$\label{GeneralLearningMethod}
    \inf\{\mathcal{L}(\mathcal{N},\mathbb{D}_m): \mathcal{N}\in\mathcal{B}_{\mathbb{W}}\}.$$ For a given width set $\mathbb{W}$, unlike learning model which searches a minimizer in set $\mathcal{A}_\mathbb{W}$, learning model seeks a minimizer in the vector space $\mathcal{B}_\mathbb{W}$, which contains $\mathcal{A}_\mathbb{W}$ and is contained in $\mathcal{A}:=\bigcup_{n\in\mathbb{N}}\mathcal{A}_{n\mathbb{W}}$. According to Proposition , learning model is “semi-equivalent" to learning model in the sense that $$\label{Comparison-of-three-models}
    \mathcal{L}(\mathcal{N}_{\mathcal{A}},\mathbb{D}_m)\leq \mathcal{L}(\mathcal{N}_{\mathcal{B}_{\mathbb{W}}},\mathbb{D}_m)\leq \mathcal{L}(\mathcal{N}_{\mathcal{A}_{\mathbb{W}}},\mathbb{D}_m),$$ where $\mathcal{N}_{\mathcal{B}_{\mathbb{W}}}$ is a minimizer of model , $\mathcal{N}_{\mathcal{A}_{\mathbb{W}}}$ and $\mathcal{N}_{\mathcal{A}}$ are the minimizers of model and model with the set $\mathcal{A}_{\mathbb{W}}$ replaced by $\mathcal{A}$, respectively. One might argue that since model is a finite dimension optimization problem while model is an infinite dimensional one, the alternative model may add unnecessary complexity to the original model. Although model is of infinite dimension, the algebraic structure of the vector space $\mathcal{B}_{\mathbb{W}}$ and its topological structure that will be equipped later provide us with great advantages for mathematical analysis of learning on the space. As a matter of fact, the vector-valued RKBS to be obtained by completing the vector space $\mathcal{B}_{\mathbb{W}}$ in a weak\* topology will lead to the miraculous representer theorem, of the learned solution, which reduces the infinite dimensional optimization problem to a finite dimension one. This addresses the challenges caused by the infinite dimension of the space $\mathcal{B}_{\mathbb{W}}$.

# Vector-Valued Reproducing Kernel Banach Space

It was proved in the last section that for a given width set $\mathbb{W}$, the set $\mathcal{B}_{\mathbb{W}}$ defined by is the smallest vector space that contains the primitive set $\mathcal{A}_\mathbb{W}$. One of the aims of this paper is to establish that the vector space $\mathcal{B}_{\mathbb{W}}$ is dense in a weak\* topology in a vector-valued RKBS. For this purpose, in this section we describe the notion of vector-valued RKBSs.

A Banach space $\mathcal{B}$ with the norm $\|\cdot\|_{\mathcal{B}}$ is called a space of vector-valued functions on a prescribed set $X$ if $\mathcal{B}$ is composed of vector-valued functions defined on $X$ and for each $f\in\mathcal{B}$, $\|f\|_{\mathcal{B}}=0$ implies that $f({x}) = \mathbf{0}$ for all ${x}\in X$. For each ${x}\in X$, we define the point evaluation operator $\delta_{{x}}:\mathcal{B}\to\mathbb{R}^n$ as $$\delta_{{x}}(f):=f({x}), \quad f\in\mathcal{B}.$$

We provide the definition of vector-valued RKBSs below.

<div class="definition">

**Definition 2**.  * A Banach space $\mathcal{B}$ of vector-valued functions from $X$ to $\mathbb{R}^n$ is called a vector-valued RKBS if there exists a norm $\|\cdot\|$ of $\mathbb{R}^n$ such that for each $x\in X$, the point evaluation operator $\delta_x$ is continuous with respect to the norm $\|\cdot\|$ of $\mathbb{R}^n$ on $\mathcal{B}$, that is, for each $x\in X$, there exists a constant $C_x>0$ such that $$\|\delta_x(f)\|\leq C_x\|f\|_{\mathcal{B}}, \quad\text{ for all }\ f\in\mathcal{B}.$$*

</div>

Note that since all norms of $\mathbb{R}^n$ are equivalent, if a Banach space $\mathcal{B}$ of vector-valued functions from $X$ to $\mathbb{R}^n$ is a vector-valued RKBS with respect to a norm of $\mathbb{R}^n$, then it must be a vector-valued RKBS with respect to any other norm of $\mathbb{R}^n$. Thus, the property of point evaluation operators being continuous on space $\mathcal{B}$ is independent of the choice of the norm of the output space $\mathbb{R}^n$.

The notion of RKBSs was originally introduced in , to guarantee the stability of sampling process and to serve as a hypothesis space for sparse machine learning. Vector-valued RKBSs were studied in , in which the definition of the vector-valued RKBS involves an abstract Banach space, with a specific norm, as the output space of functions. In Definition , we limit the output space to the Euclidean space $\mathbb{R}^n$ without specifying a norm, due to the special property that norms on $\mathbb{R}^n$ are all equivalent.

We reveal in the next proposition that point evaluation operators are continuous if and only if component-wise point evaluation functionals are continuous. To this end, for a vector-valued function $f:X\to\mathbb{R}^n$, for each $j\in\mathbb{N}_n$, we denote by $f_j:X\to\mathbb{R}$ the $j$-th component of $f$, that is, $$f({x}):=[f_j({x}):j\in\mathbb{N}_n]^\top,\quad {x}\in X.$$ Moreover, for each $x\in X$, $k\in\mathbb{N}_n$, we introduce a linear functional $\delta_{{x},k}:\mathcal{B}\to\mathbb{R}$ by $$\delta_{{x},k}(f):=f_k({x}),\ \mbox{for}\  f:=[f_k:k\in\mathbb{N}_n]^{\top}\in\mathcal{B}.$$

<div class="proposition">

**Proposition 3**.  * A Banach space $\mathcal{B}$ of vector-valued functions from $X$ to $\mathbb{R}^n$ is a vector-valued RKBS if and only if for each ${x}\in X$, $k\in \mathbb{N}_n$, there exists a constant $C_{{x},k}>0$ such that $$\label{elementwise functionals}
        |\delta_{{x},k}(f)|\leq C_{{x},k}\|f\|_{\mathcal{B}}, \ \mbox{for all} \ f\in \mathcal{B}.$$*

</div>

<div class="proof">

*Proof.* Suppose that $\mathcal{B}$ is a vector-valued RKBS. Definition together with the norm equivalence of the Euclidean space $\mathbb{R}^n$ ensures that for each $x\in X$, point evaluation operator $\delta_x$ is continuous with respect to the maximum norm $\|\cdot\|_{\infty}$ of $\mathbb{R}^n$. That is, for each ${x}\in X$, there exists a positive constant $C_x$ such that $\|f(x)\|_{\infty}\leq C_{{x}}\|f\|_{\mathcal{B}}$ for all $f\in\mathcal{B}$. Hence, for each $x\in X$, $k\in\mathbb{N}_n$, there holds that $\left|\delta_{x,k}(f)\right|=|f_k(x)|\leq \|f(x)\|_{\infty}\leq C_x\|f\|_{\mathcal{B}}$ for all $f\in\mathcal{B}$. That is, inequality holds true with $C_{{x},k}:=C_x$.

Conversely, we assume that for each ${x}\in X$, $k\in \mathbb{N}_n$, there exists $C_{{x},k}>0$ such that holds. Note that for each $x\in X$, there holds $\|\delta_x(f)\|_{\infty}=\max_{k\in\mathbb{N}_n}|f_k(x)|$ for all $f\in\mathcal{B}$. By setting $C_x:=\max_{k\in\mathbb{N}_n}C_{x,k}$, we get from inequality that $\|\delta_x(f)\|_{\infty}\leq C_{x}\|f\|_{\mathcal{B}}$ for all $f\in\mathcal{B}$. That is, for each $x\in X$, the point evaluation operator $\delta_x$ is continuous with respect to the maximum norm $\|\cdot\|_{\infty}$ of $\mathbb{R}^n$, and thus, by Definition , $\mathcal{B}$ is a vector-valued RKBS. ◻

</div>

We next identify a reproducing kernel for a vector-valued RKBS. We need the notion of the $\delta$-dual space of a vector-valued RKBS. For a Banach space $B$ with a norm $\|\cdot\|_{B}$, we denote by $B^*$ the dual space of $B$, which is composed of all continuous linear functionals on $B$ endowed with the norm $$\|\nu\|_{B^*}:=\sup_{\|f\|_{B}\leq1}|\nu(f)|,\ \mbox{for all}\ \nu\in B^*.$$ The dual bilinear form $\langle\cdot,\cdot\rangle_{B}$ on $B^*\times B$ is defined by $$\langle\nu,f\rangle_{B}:=\nu(f),\ \mbox{for all}\ \nu\in B^* \  \mbox{and}\ f\in B.$$ Suppose that $\mathcal{B}$ is a vector-valued RKBS of functions from $X$ to $\mathbb{R}^n$, with the dual space $\mathcal{B}^*$. We set $$\label{Def:Delta}
    \Delta:=\mathrm{span}\{\delta_{x,j}:x\in X, j\in\mathbb{N}_n\}.$$ Proposition reveals that for each $x\in X$ and $j\in\mathbb{N}_n$, $\delta_{x,j}$ is a continuous linear functional on $\mathcal{B}$. As a result, $\Delta$ is a subset of $\mathcal{B}^*$. We denote by $\mathcal{B}'$ the closure of $\Delta$ in the norm topology on $\mathcal{B}^*$ and call it the $\delta$-dual space of $\mathcal{B}$. Clearly, we have that $\mathcal{B}'\subseteq\mathcal{B}^*$ and $\mathcal{B}'$ is the smallest Banach space that contains all point-evaluation functionals on $\mathcal{B}$. We remark that the $\delta$-dual space of a scalar-valued RKBS was originally introduced in .

We identify in the next proposition a reproducing kernel for the vector-valued RKBS $\mathcal{B}$ under the assumption that the $\delta$-dual space $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions from a set $X'$ to $\mathbb{R}$.

<div class="proposition">

**Proposition 4**.  * Suppose that $\mathcal{B}$ is a vector-valued RKBS of functions from $X$ to $\mathbb{R}^n$ and its $\delta$-dual space $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions from $X'$ to $\mathbb{R}$, then there exists a unique vector-valued function $K:X\times X'\to\mathbb{R}^n$ such that $K_j(x,\cdot)\in \mathcal{B}'$ for all $x\in X$, $j\in\mathbb{N}_n$, and $$\label{def: reproducing property}
    f_j(x)=\langle K_j(x,\cdot),f\rangle_{\mathcal{B}},\ \mbox{for all}\ f=[f_j:j\in\mathbb{N}_n]\in\mathcal{B} \ \mbox{and all}\  x\in X, \ j\in\mathbb{N}_n.$$*

</div>

<div class="proof">

*Proof.* Since $\mathcal{B}$ is a vector-valued RKBS, Proposition ensures that for each $x\in X$, $j\in\mathbb{N}_n$, $\delta_{x,j}$ is a continuous linear functional on $\mathcal{B}$. By noting that $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions from $X'$ to $\mathbb{R}$, there exists $k_{x,j}\in\mathcal{B}'$ such that $$\label{K_xj}
    \delta_{x,j}(f)=\langle k_{x,j},f\rangle_{\mathcal{B}}, \ \mbox{for all}\ f\in\mathcal{B}.$$ By defining the $j$-th component of the vector-valued function $K:X\times X'\to\mathbb{R}^n$ by $$\label{def-K}
   K_j(x,x'):=k_{x,j}(x'),\ x\in X, \ x'\in X',\ j\in\mathbb{N}_n,$$ we have that $K_j(x,\cdot)\in\mathcal{B}'$ for all $x\in X$ and $j\in\mathbb{N}_n$. Substituting equation into the right-hand side of equation with noting that $\delta_{x,j}(f)=f_j(x)$, we obtain equation .

It remains to verify that the function $K$ on $X\times X'$ satisfying the above properties is unique. Assume that there exists another $\widetilde{K}:X\times X'\rightarrow \mathbb{R}^n$ such that $\widetilde{K}_j(x,\cdot)\in \mathcal{B}'$ for all $x\in X$, $j\in\mathbb{N}_n$, and $f_j(x)=\langle \widetilde{K}_j(x,\cdot),f\rangle_{\mathcal{B}}$, for all $f\in\mathcal{B}$, $x\in X$, and $j\in\mathbb{N}_n$. It follows from the above equation and equation that $\langle K_j(x,\cdot)-\widetilde{K}_j(x,\cdot),f\rangle_{\mathcal{B}}=0$, for all $f\in\mathcal{B}$ and all $x\in X$. That is, for all $x\in X$, $j\in\mathbb{N}_n$, $K_j(x,\cdot)-\widetilde{K}_j(x,\cdot)=0$. Noting that $\mathcal{B}'$ is isometrically isomorphic to a Banach space of functions on $X'$, we conclude that $K_j(x,x')-\widetilde{K}_j(x,x')=0$ for all $x\in X$, $x'\in X'$, and $j\in\mathbb{N}_n$, which is equivalent to $K=\widetilde{K}$. ◻

</div>

We call the vector-valued function $K:X\times X'\rightarrow\mathbb{R}^n$ that satisfies $K_j(x,\cdot)\in\mathcal{B}'$ for all $x\in X$, $j\in\mathbb{N}_n$ and equation the reproducing kernel for the vector-valued RKBS $\mathcal{B}$. Moreover, equation is called the reproducing property. Clearly, we have that $K(x,\cdot)\in(\mathcal{B}')^n$ for all $x\in X$. The notion of the vector-valued RKBS and its reproducing kernel will serve as a basis for us to understand the hypothesis space for deep learning in the next section.

It is worth of pointing out that although $\mathcal{B}$ is a space of vector-valued functions, the $\delta$-dual space $\mathcal{B}'$ defined here is a space of scalar-valued functions. This is determined by the form of the point evaluation functionals in set $\Delta$ defined by . The way of defining the $\delta$-dual space of the vector-valued RKBS $\mathcal{B}$ is not unique. One can also define a $\delta$-dual space of the vector-valued RKBS $\mathcal{B}$ as a space of vector-valued functions. In this paper, we adopt the current form of $\mathcal{B}'$ since it is simple and sufficient to serve our purposes. Other forms of the $\delta$-dual space will be investigated in a different occasion.

# Hypothesis Space

In this section, we return to understanding the vector space $\mathcal{B}_\mathbb{W}$ introduced in section 2 from the RKBS viewpoint. Specifically, our goal is to introduce a vector-valued RKBS in which the vector space $\mathcal{B}_\mathbb{W}$ is weakly$^*$ dense. The resulting vector-valued RKBS will serve as the hypothesis space for deep learning.

We first construct the vector-valued RKBS. Recalling the parameter space $\Theta$ defined by equation , we use $C_0(\Theta)$ to denote the space of the continuous *scalar-valued* functions vanishing at infinity on $\Theta$. We equip the maximum norm on $C_0(\Theta)$, namely, $\|f\|_{\infty}:=\sup_{\theta\in\Theta}|f(\theta)|$, for all $f\in C_0(\Theta)$. For the function $\mathcal{N}(x,\theta)$, $x\in\mathbb{R}^s$, $\theta\in\Theta$, defined by equation , we denote by $\mathcal{N}_k({x},\theta)$ the $k$-th component of $\mathcal{N}({x},\theta)$, for $k\in\mathbb{N}_t$. We require that all components $\mathcal{N}_k({x},\cdot)$ with a weight belong to $C_0(\Theta)$ for all $x\in\mathbb{R}^s$. Specifically, we assume that there exists a continuous weight function $\rho:\Theta\to\mathbb{R}$ such that the functions $$\mathcal{N}_k({x},\cdot)\rho(\cdot)\in C_0(\Theta), \ \ \mbox{for all} \ \ x\in \mathbb{R}^s, \ k\in\mathbb{N}_t.$$ An example of such a weight function is given by the rapidly decreasing function $$\label{gaussian weight function}
    \rho(\theta):=\exp(-\|\theta\|_2^2), \ \ \theta\in\Theta.$$

We need a measure on the set $\Theta$. A Radon measure on $\Theta$ is a Borel measure on $\Theta$ that is finite on all compact sets of $\Theta$, outer regular on all Borel sets of $\Theta$, and inner regular on all open sets of $\Theta$. Let $\mathcal{M}(\Theta)$ denote the space of finite Radon measures $\mu: \Theta\to \mathbb{R}$, equipped with the total variation norm $$\label{TVnorm}
    \|\mu\|_{\mathrm{TV}}:=\sup\left\{\sum_{k=1}^\infty\left|\mu(E_k)\right|:\Theta=\bigcup_{k=1}^\infty E_k,\ E_i\cap E_j=\emptyset\text{ whenever }i\neq j \right\},$$ where $E_k$ are required to be measurable. Note that $\mathcal{M}(\Theta)$ is the dual space of $C_0(\Theta)$ (see, for example, ). Moreover, the dual bilinear form on $\mathcal{M}(\Theta)\times C_0(\Theta)$ is given by $$\label{DualBilinearForm}
    \langle \mu,g\rangle_{C_0(\Theta)}:=\int_{\Theta} g(\theta)d\mu(\theta),\ \text{for }\mu\in\mathcal{M}(\Theta),\ g\in C_0(\Theta).$$ For $\mu \in \mathcal{M}(\Theta)$, we let $$\label{Def:f_mu^k}
    f_{\mu}^k(\cdot):=\int_\Theta \mathcal{N}_k(\cdot,\theta)\rho(\theta)d\mu(\theta), \ \  k\in \mathbb{N}_t,$$ and $$\label{vector-valued fmu}
f_\mu(\cdot):=\left[f_{\mu}^k(\cdot): k\in\mathbb{N}_t\right]^\top.$$ We introduce the vector space $$\label{banach space DNN}
    \mathcal{B}_{\mathcal{N}}:=\left\{f_\mu:\mu \in \mathcal{M}(\Theta)\right\},$$ with norm $$\label{banach space norm DNN}
    \|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}:=\inf\left\{\|\nu\|_{\mathrm{TV}}:f_\nu=f_\mu,\ \nu\in\mathcal{M}(\Theta)\right\},$$ where $f_\mu^k$, $k\in\mathbb{N}_t$, are defined by equation and $\|\cdot\|_\mathrm{TV}$ is defined as . Note that in definition of the norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$, the infimum is taken over all the measures $\nu\in\mathcal{M}(\Theta)$ that satisfy $t$ equality constraints $$\int_\Theta \mathcal{N}_k(\cdot,\theta)\rho(\theta)d\mu(\theta)=\int_\Theta \mathcal{N}_k(\cdot,\theta)\rho(\theta)d\nu(\theta),\quad k\in\mathbb{N}_t.$$ In particular, in the case $t=1$, where $f_\mu$ reduces to a neural network of a scalar-valued output, the norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$ is taken over the measures $\nu\in\mathcal{M}(\Theta)$ that satisfies only a single equality constraint. The bigger $t$ is, the larger the norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$ will be. We remark that the special case of $\mathcal{B}_{\mathcal{N}}$ with $\mathcal{N}$ being a scalar-valued neural network of a *single* hidden layer was recently studied in .

We next show that the space $\mathcal{B}_{\mathcal{N}}$ defined by with norm is a Banach space having a pre-dual space. This is done by showing that $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to a quotient space. We recall the concept of the quotient space. Let $B$ be a Banach space with its dual space $B^*$ and $M$ be a closed subspace of $B$. For each $f$ in $B$, the translate $f+M$ which contains $f$ is called the coset of $M$. The quotient space $B/M$ is defined by $B/M:=\{f+M: f\in B\}$, with the quotient norm $$\|f+M\|_{B/M}:=\inf\left\{\|f+g\|:g\in M\right\},\quad f\in B.$$ It is known that the quotient space $B/M$ is a Banach space. We say that a Banach space $B$ has a pre-dual space if there exists a Banach space $B_*$ such that $(B_*)^*=B$ and we call the space $B_*$ a pre-dual space of $B$. We also need the notion of annihilators. Let $M$ and $M'$ be subsets of $B$ and $B^*$, respectively. The annihilator of $M$ in $B^*$ is defined by $$M^\perp:=\{\nu\in B^*:\langle\nu,f\rangle_{B}=0, \ \text{for all }f\in M\}.$$ The annihilator of $M'$ in $B$ is defined by $$^{\perp}M':=\{f\in B: \langle\nu,f\rangle_{B}=0,
\ \mbox{for all}\  \nu\in M'\}.$$ We review a result about the dual space of a closed subspace of a Banach space. Specifically, let $M$ be a closed subspace of a Banach space $B$. For each $\nu\in B^*$, we denote by $\nu|_{M}$ the restriction of $\nu$ to $M$. It is clear that $\nu|_{M}\in M^*$ and $\|\nu|_{M}\|_{M^*}\leq\|\nu\|_{B^*}$. The dual space $M^*$ may be identified as $B^*/M^\perp$. In fact, by Theorem 10.1 in Chapter III of , the map $\tau:B^*/M^\perp\to M^*$ defined by $$\tau(\nu+M^\perp):=\nu|_{M}, \ \mbox{for}\ \nu\in B^*,$$ is an isometric isomorphism between $B^*/M^\perp$ and $M^*$.

For the purpose of proving that $\mathcal{B}_{\mathcal{N}}$ is a Banach space, we identify the quotient space which is isometrically isomorphic to $\mathcal{B}_{\mathcal{N}}$. To this end, we introduce a closed subspace of $C_0(\Theta)$ as $$\label{subspace of C0}
\mathcal{S}:=\overline{\mathrm{span}}\{\mathcal{N}_k({x},\cdot)\rho(\cdot): {x}\in\mathbb{R}^s,k\in\mathbb{N}_t\},$$ where the closure is taken in the maximum norm. From definition , it is clear that $\mathcal{S}$ is a Banach space of functions defined on the parameter space $\Theta$.

<div class="proposition">

**Proposition 5**.  * Let $\Theta$ be the parameter space defined by . If for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)\in C_0(\Theta)$, then the space $\mathcal{B}_{\mathcal{N}}$ defined by endowed with the norm is a Banach space with a pre-dual space $\mathcal{S}$ defined by .*

</div>

<div class="proof">

*Proof.* It is clear that $\mathcal{B}_{\mathcal{N}}$ is a normed space endowed with the norm $\|\cdot\|_{\mathcal{B}_{\mathcal{N}}}$. It suffices to verify that $\mathcal{B}_{\mathcal{N}}$ is complete. This is done by showing that $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to the quotient space $\mathcal{M}(\Theta)/\mathcal{S}^\perp$. We first characterize the annihilator of $\mathcal{S}$ in $\mathcal{M}(\Theta)$. It follows from definition of $\mathcal{S}$ that $\mu\in \mathcal{S}^\perp$ if and only if $\langle\mu,\mathcal{N}_k({x},\cdot)\rho(\cdot)\rangle_{C_0(\Theta)}=0$ for all $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$. The latter is equivalent to $f_{\mu}^k(x)=0$ for all $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, due to definition of $f_{\mu}^k$ and definition of the dual bilinear form on $\mathcal{M}(\Theta)\times C_0(\Theta)$. That is, $f_{\mu}=0$. Consequently, we conclude that $$\label{Sperp}
    \mathcal{S}^\perp=\{\mu\in\mathcal{M}(\Theta): f_{\mu}=0\}.$$ We next let $\varphi$ be the map from $\mathcal{B}_{\mathcal{N}}$ to $\mathcal{M}(\Theta)/\mathcal{S}^\perp$ defined for $f_\mu\in \mathcal{B}_{\mathcal{N}}$ by $$\label{def: mapping phi}\varphi(f_\mu):=\mu+\mathcal{S}^\perp, \quad\mu\in\mathcal{M}(\Theta),$$ and show that $\varphi$ is an isometric isomorphism.

We first show that $\varphi$ is an isometry. For any $f_{\mu}\in\mathcal{B}_\mathcal{N}$ with $\mu\in\mathcal{M}(\Theta)$, we have that $\nu\in\mathcal{M}(\Theta)$ satisfies $f_{\nu}=f_{\mu}$ if and only if $f_{\nu-\mu}=0$. Due to representation of $\mathcal{S}^\perp$, the latter is equivalent to $\nu-\mu\in \mathcal{S}^\perp$. That is, $\nu=\mu+\mu'$ for some $\mu'\in \mathcal{S}^\perp$. Hence, we get by definition that $$\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}=\inf\left\{\|\mu+\mu'\|_{\mathrm{TV}}:\mu'\in \mathcal{S}^\perp\right\}.$$ This together with the definition of the quotient norm yields that $$\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}=\|\mu+\mathcal{S}^\perp\|_{\mathcal{M}(\Theta)/\mathcal{S}^\perp},$$ which with leads to $\|\varphi(f_{\mu})\|_{\mathcal{M}(\Theta)/\mathcal{S}^\perp}=\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}$. In other words, $\varphi$ is an isometry. Due to its isometry property, $\varphi$ is injective. Clearly, $\varphi$ is surjective. Hence, it is bijective. Consequently, $\varphi$ is an isometric isomorphism from $\mathcal{B}_{\mathcal{N}}$ to the Banach space $\mathcal{M}(\Theta)/\mathcal{S}^\perp$, and thus, $\mathcal{B}_{\mathcal{N}}$ is complete.

We now show that $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to the dual space of $\mathcal{S}$. Note that $\mathcal{S}$ is a closed subspace of $C_0(\Theta)$ and $(C_0(\Theta))^*=\mathcal{M}(\Theta)$. By Theorem 10.1 in with $B:=C_0(\Theta)$ and $M:=\mathcal{S}$, we have that the map $\tau:\mathcal{M}(\Theta)/\mathcal{S}^\perp\to \mathcal{S}^*$ defined by $$\label{pho}
        \tau(\mu+\mathcal{S}^\perp):=\mu|_{\mathcal{S}},\quad\mu\in\mathcal{M}(\Theta),$$ is an isometric isomorphism. As has been shown earlier, the map $\varphi$ defined by is an isometric isomorphism from $\mathcal{B}_{\mathcal{N}}$ to $\mathcal{M}(\Theta)/\mathcal{S}^\perp$. As a result, $\tau\circ\varphi$ provides an isometric isomorphism from $\mathcal{B}_{\mathcal{N}}$ to $\mathcal{S}^*$. ◻

</div>

Proposition and the theorems that follow require that for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$. This requirement in fact imposes a hypothesis to the activation function $\sigma$: (i) $\sigma$ is continuous and (ii) when the weight function $\rho$ is chosen as , we need to select the activation function $\sigma$ having a growth rate no greater than polynomials. We remark that many commonly used activation functions satisfy this requirement. They include the ReLU function $$\sigma(x):=\max\{0,x\},\ \ x\in\mathbb{R},$$ and the sigmoid function $$\sigma(x):=\frac{1}{1+e^{-x}},\ \ x\in\mathbb{R}.$$

Now that the space $\mathcal{B}_{\mathcal{N}}$ with the norm $\|\cdot\|_{\mathcal{B}_{\mathcal{N}}}$, guaranteed by Proposition , is a Banach space, we denote by $\mathcal{B}_{\mathcal{N}}^*$ the dual space of $\mathcal{B}_{\mathcal{N}}$ endowed with the norm $$\label{norm of BN*}
    \|\ell\|_{\mathcal{B}_{\mathcal{N}}^*}=\sup\{|\langle\ell,f_\mu\rangle_{\mathcal{B}_{\mathcal{N}}}|:\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\}, \ \ \mbox{for}\ \ \ell\in \mathcal{B}_{\mathcal{N}}^*.$$ The dual space $\mathcal{B}_{\mathcal{N}}^*$ is again a Banach space. Moreover, it follows from Proposition that the space $\mathcal{S}$ is a pre-dual space of $\mathcal{B}_{\mathcal{N}}$, that is, $(\mathcal{B}_{\mathcal{N}})_*=\mathcal{S}.$ We remark that the dual bilinear form on $\mathcal{B}_{\mathcal{N}}\times\mathcal{S}$ is given by $$\label{dual bilinear on BNS}
\langle f_{\mu}, g\rangle_{\mathcal{S}}=\langle\mu, g\rangle_{C_0(\Theta)}, \ \mbox{for}\ f_{\mu}\in\mathcal{B}_{\mathcal{N}},\ g\in \mathcal{S}.$$ According to Proposition , the space $\mathcal{S}$ is the pre-dual space of $\mathcal{B}_{\mathcal{N}}$, that is, $\mathcal{S}^*=\mathcal{B}_{\mathcal{N}}$. Thus, we obtain that $\mathcal{S}^{**}=\mathcal{B}_{\mathcal{N}}^*$. It is well-known (for example, see ) that $\mathcal{S}\subseteq \mathcal{S}^{**}$ in the sense of isometric embedding. Hence, $\mathcal{S}\subseteq \mathcal{B}_{\mathcal{N}}^*$ and there holds $$\label{natural-map-predual}
\langle g,f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle f_{\mu},g\rangle_{\mathcal{S}},
\ \mbox{for all} \ f_{\mu}\in \mathcal{B}_{\mathcal{N}} \ \mbox{and all} \ g\in \mathcal{S}.$$

We now turn to establishing that $\mathcal{B}_{\mathcal{N}}$ is a vector-valued RKBS on $\mathbb{R}^s$.

<div class="theorem">

**Theorem 6**.  * Let $\Theta$ be the parameter space defined by . If for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$, then the Banach space $\mathcal{B}_{\mathcal{N}}$ defined by endowed with the norm is a vector-valued RKBS on $\mathbb{R}^s$.*

</div>

<div class="proof">

*Proof.* According to Proposition with $X:=\mathbb{R}^s$, it suffices to prove that for each $x\in\mathbb{R}^s$, $k\in\mathbb{N}_t$, there exists a positive constant $C_{x,k}$ such that $$\label{proof fmuk leq Cxk f norm}
    |f_\mu^k(x)|\leq C_{x,k}\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}, \ \mbox{for all}\ f_{\mu}\in\mathcal{B}_\mathcal{N}.$$ To this end, for any $f_{\mu}\in\mathcal{B}_\mathcal{N}$, we obtain from definition of $f_\mu^k$ that $$\label{f_mu^k}
|f_\mu^k({x})|\leq \|\mathcal{N}_k({x},\cdot)\rho(\cdot)\|_\infty\|\nu\|_{\mathrm{TV}},$$ for any $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\nu=f_\mu$. By taking infimum of both sides of inequality over $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\nu=f_\mu$ and employing definition , we obtain that $$|f_\mu^k({x})|\leq\|\mathcal{N}_k({x},\cdot)\rho(\cdot)\|_\infty\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}.$$ Letting $C_{x,k}:=\|\mathcal{N}_k({x},\cdot)\rho(\cdot)\|_\infty$, we get inequality . ◻

</div>

Next, we identify the reproducing kernel of the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$. According to Proposition , the existence of the reproducing kernel requires to characterize the $\delta$-dual space of $\mathcal{B}_{\mathcal{N}}$. We note that the $\delta$-dual space $\mathcal{B}'_{\mathcal{N}}$ is the closure of $$\label{delta dual for BN}
\Delta:=\mathrm{span}\{\delta_{x,k}:x\in\mathbb{R}^s,\ k\in\mathbb{N}_t\},$$ in the norm topology of $\mathcal{B}_{\mathcal{N}}^*$. We will show that $\Delta$ is isometrically isomorphic to $$\mathbb{S}:=\mathrm{span}\{\mathcal{N}_k({x},\cdot)\rho(\cdot):{x}\in\mathbb{R}^s,\ k\in\mathbb{N}_t\},$$ a subspace of $\mathcal{S}$. To this end, we introduce a mapping $\Psi:\Delta\to \mathbb{S}$ by $$\label{isometric isomorphism_delta_KX}   \Psi\left(\sum_{j\in\mathbb{N}_m}\alpha_{j}\delta_{{x}_j,k_j}\right):= \sum_{j\in\mathbb{N}_m}\alpha_{j} \mathcal{N}_{k_j}({x}_j,\cdot)\rho(\cdot),$$ for all $m\in\mathbb{N}$, $\alpha_{j}\in\mathbb{R}$, ${x}_j\in\mathbb{R}^s$, $k_j\in\mathbb{N}_t$, and $j\in\mathbb{N}_m$.

<div class="lemma">

**Lemma 7**.  * The map $\Psi$ defined by is an isometric isomorphism between $\Delta$ and $\mathbb{S}$.*

</div>

<div class="proof">

*Proof.* We first prove that $\Psi$ is an isometry, that is, $\left\|\ell\right\|_{\mathcal{B}_{\mathcal{N}}^*}=\left\|\Psi(\ell)\right\|_{\infty}$, for all $\ell\in\Delta$. Let $\ell$ be an arbitrary element of $\Delta$. Then there exist $m\in\mathbb{N}$, $\alpha_{j}\in\mathbb{R}$, ${x}_j\in\mathbb{R}^s$, $k_j\in\mathbb{N}_t$, and $j\in\mathbb{N}_m$ such that $\ell=\sum_{j\in\mathbb{N}_m}\alpha_{j}\delta_{{x}_j,k_j}$. By definition and the definition of the functionals $\delta_{x_j,k_j}$, $j\in\mathbb{N}_m$, we have that $$\label{proof norm of ell}
        \|\ell\|_{\mathcal{B}_{\mathcal{N}}^*}=\sup\left\{\left|\sum_{j\in\mathbb{N}_m}\alpha_{j} f_\mu^{k_j}(x_j)\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}.$$ We next compute $\|\Psi(\ell)\|_{\infty}$. By noting that $\Psi(\ell)\in \mathcal{S}$ and $\mathcal{S}^*=\mathcal{B}_{\mathcal{N}}$, we have that $$\|\Psi(\ell)\|_\infty=\sup\left\{\left|\langle f_\mu, \Psi(\ell)\rangle_{\mathcal{S}}\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}.$$ Substituting equation with $g:=\Psi(\ell)$ into the right hand side of the above equation, we get that $$\label{norm of psi ell}
        \|\Psi(\ell)\|_\infty=\sup\left\{\left|\langle \mu, \Psi(\ell)\rangle_{C_0(\Theta)}\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}.$$ According to definition of $\Psi$, there holds for any $f_{\mu}\in\mathcal{B}_{\mathcal{N}}$ that $$\langle \mu, \Psi(\ell)\rangle_{C_0(\Theta)}=\sum_{j\in\mathbb{N}_m}\alpha_{j} \langle \mu,\mathcal{N}_{k_j}({x}_j,\cdot)\rho(\cdot)\rangle_{C_0(\Theta)}.$$ This together with definition yields that $\langle \mu, \Psi(\ell)\rangle_{C_0(\Theta)}=\sum_{j\in\mathbb{N}_m}\alpha_{j} f_{\mu}^{k_j}(x_j).$ Involving this equation in the right-hand side of leads to $$\label{proof: infinity normn of psi(ell) final form}
       \|\Psi(\ell)\|_\infty=\sup\left\{\left|\sum_{j\in\mathbb{N}_m}\alpha_{j} f_{\mu}^{k_j}(x_j)\right|:\left\|f_\mu\right\|_{\mathcal{B}_{\mathcal{N}}}=1, f_\mu\in\mathcal{B}_{\mathcal{N}}\right\}.$$ Comparing and , we obtain that $\|\ell\|_{\mathcal{B}_{\mathcal{N}}^*}=\|\Psi(\ell)\|_\infty$ and hence, $\Psi$ is an isometry between $\Delta$ and $\mathbb{S}$. The isometry of $\Psi$ further implies its injectivity. Moreover, $\Psi$ is linear and surjective. Thus, $\Psi$ is bijective. Therefore, $\Psi$ is an isometric isomorphism between $\Delta$ and $\mathbb{S}$. ◻

</div>

The isometrically isomorphic relation between $\Delta$ and $\mathbb{S}$ is preserved after completing them. We state this result in the following lemma without proof.

<div class="lemma">

**Lemma 8**.  * Suppose that $A$ and $B$ are Banach spaces with norms $\|\cdot\|_A$ and $\|\cdot\|_B$, respectively. Let $A_0$ and $B_0$ be dense subsets of $A$ and $B$, respectively. If $A_0$ is isometrically isomorphic to $B_0$, then $A$ is isometrically isomorphic to $B$.*

</div>

Lemma may be obtained by applying Theorem 1.6-2 in . With the help of Lemmas and , we identify in the following theorem the reproducing kernel for the RKBS $\mathcal{B}_{\mathcal{N}}$.

<div class="theorem">

**Theorem 9**.  * Let $\Theta$ be the parameter space defined by . Suppose that for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$. If the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$ is defined by with the norm , then the vector-valued function $$\label{Kernel}
\mathcal{K}(x,\theta):=\mathcal{N}(x,\theta)\rho(\theta), \ \ \mbox{for}\ \ (x,\theta)\in \mathbb{R}^s\times\Theta,$$ is the reproducing kernel for space $\mathcal{B}_{\mathcal{N}}$.*

</div>

<div class="proof">

*Proof.* We employ Proposition with $X:=\mathbb{R}^s$ and $X':=\Theta$ to establish that the function $\mathcal{K}$ defined by is the reproducing kernel of space $\mathcal{B}_{\mathcal{N}}$. According to Lemma , $\Delta$ is isometrically isomorphic to $\mathbb{S}$. Since $\mathcal{B}_{\mathcal{N}}'$ and $\mathcal{S}$ are the completion of $\Delta$ and $\mathbb{S}$, respectively, by Lemma , we conclude that the $\delta$-dual space $\mathcal{B}'_{\mathcal{N}}$ of $\mathcal{B}_{\mathcal{N}}$ is isometrically isomorphic to $\mathcal{S}$, which is a Banach space of functions from $\Theta$ to $\mathbb{R}$. Hence, Proposition ensures that there exists a unique reproducing kernel for $\mathcal{B}_{\mathcal{N}}$.

We next verify that the vector-valued function $\mathcal{K}$ defined by is the reproducing kernel for $\mathcal{B}_{\mathcal{N}}$. By noting that the $\delta$-dual space $\mathcal{B}'_{\mathcal{N}}$ is isometrically isomorphic to $\mathcal{S}$, we have for each $x\in\mathbb{R}^s$ and each $k\in\mathbb{N}_t$ that $\mathcal{K}_k(x,\cdot):=\mathcal{N}_k(x,\cdot)\rho(\cdot)\in \mathcal{B}_{\mathcal{N}}'$. The space $\mathcal{S}$, guaranteed by Proposition , is a pre-dual space of $\mathcal{B}_{\mathcal{N}}$. Hence, by equation with $g:=\mathcal{K}_k(x,\cdot)$, we obtain for each $x\in\mathbb{R}^s$, $k\in\mathbb{N}_t$ that $$\langle\mathcal{K}_k(x,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle f_{\mu},\mathcal{K}_k(x,\cdot)\rangle_{\mathcal{S}},\ \mbox{for all}\  f_{\mu}\in\mathcal{B}_{\mathcal{N}}.$$ Substituting equation with $g:=\mathcal{K}_k(x,\cdot)$ into the right-hand side of the above equation leads to $$\langle\mathcal{K}_k(x,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle\mu, \mathcal{K}_k(x,\cdot)\rangle_{C_0(\Theta)},\ \mbox{for all}\  f_{\mu}\in\mathcal{B}_{\mathcal{N}}.$$ This together with definitions , and implies the reproducing property $$\label{proof reproducing property}
\langle\mathcal{K}_k(x,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=f_{\mu}^k(x),\ \mbox{for all}\  f_{\mu}\in\mathcal{B}_{\mathcal{N}}.$$ Consequently, $\mathcal{K}$ is the reproducing kernel of $\mathcal{B}_{\mathcal{N}}$. ◻

</div>

The reproducing kernel defined by in Theorem is an asymmetric kernel, unlike a reproducing kernel in a reproducing kernel Hilbert space, which is always symmetric. It is the asymmetry of the “kernel" that allows us to encode one variable of the kernel function as the physical variable and one as the parameter variable. Theorem restricted to the shallow network is still new to our best acknowledge. We will show in the next section a solution of a deep learning model may be expressed as a combination of a finite number of kernel sessions, a kernel with the parameter variable evaluated at a point of the parameter space determined by given data.

We are ready to prove that the vector space $\mathcal{B}_{\mathbb{W}}$, defined by equation , is weakly${}^*$ dense in the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$. For this purpose, we recall the concept of the weak${}^*$ topology. Let $B$ be a Banach space. The weak${}^*$ topology of the dual space $B^{*}$ is the smallest topology for $B^{*}$ such that, for each $f\in B$, the linear functional $\nu \rightarrow\langle\nu, f\rangle_{B}$ on $B^*$ is continuous with respect to the topology. For a subset $M'$ of $B^*$, we denote by $\overline{M'}^{w^*}$ the closure of $M'$ in the weak$^*$ topology of $B^*$. We remark that the fact that the Banach space $\mathcal{B}_{\mathcal{N}}$ has a pre-dual space $\mathcal{S}$ makes it valid for $\mathcal{B}_{\mathcal{N}}$ to be equipped with the weak$^*$ topology, the topology of $\mathcal{S}^*$.

<div class="theorem">

**Theorem 10**.  * Let $\Theta$ be the parameter space defined by and $\mathbb{W}$ the width set defined by . If for each $x\in \mathbb{R}^s$ and $k\in\mathbb{N}_t$, the function $\mathcal{N}_k({x},\cdot)\rho(\cdot)$ belongs to $C_0(\Theta)$, then $\mathcal{B}_{\mathbb{W}}$ is a subspace of $\mathcal{B}_{\mathcal{N}}$ and $$\label{weak*Dense}
\overline{\mathcal{B}_{\mathbb{W}}}^{w^*}=\mathcal{B}_{\mathcal{N}}.$$*

</div>

<div class="proof">

*Proof.* It has been shown in Proposition that $\mathcal{B}_{\mathbb{W}}$ is a vector space. We now show that $\mathcal{B}_{\mathbb{W}}$ is a subspace of $\mathcal{B}_{\mathcal{N}}$. For any $f\in{{{\mathcal{B}_{\mathbb{W}}}}}$, there exist $n\in\mathbb{N}$, $c_l\in\mathbb{R}$, $\theta_l\in\Theta$, $l\in\mathbb{N}_n$ such that $f=\sum_{l=1}^n c_l\mathcal{N}(\cdot,\theta_l)\rho(\theta_l)$. By choosing $\mu:=\sum_{l=1}^n c_l\delta_{\theta_l}$, we have that $\mu\in\mathcal{M}(\Theta)$. We then obtain from definition that $$f_{\mu}^k(x)=\sum_{l=1}^n c_l\mathcal{N}_k(x,\theta_l)\rho(\theta_l),\ \mbox{for all}\ x\in\mathbb{R}^s,\  k\in\mathbb{N}_t.$$ This together with the representation of $f$ yields that $f=f_{\mu}$ and thus, $f\in\mathcal{B}_{\mathcal{N}}$. Consequently, we have that $\mathcal{B}_{\mathbb{W}}\subseteq\mathcal{B}_{\mathcal{N}}$.

It remains to prove equation . Proposition ensures that $\mathcal{S}^*=\mathcal{B}_{\mathcal{N}}$, in the sense of being isometrically isomorphic. Hence, $\mathcal{B}_{\mathbb{W}}$ is a subspace of the dual space of $\mathcal{S}$. It follows from Proposition 2.6.6 of that $(^{\perp}\mathcal{B}_{\mathbb{W}})^{\perp}=\overline{\mathcal{B}_{\mathbb{W}}}^{w^*}$. It suffices to verify that $(^{\perp}\mathcal{B}_{\mathbb{W}})^{\perp}=\mathcal{B}_{\mathcal{N}}$. Due to definition of $\mathcal{B}_{\mathbb{W}}$, $g\in  ^{\perp}\mathcal{B}_{\mathbb{W}}$ if and only if $$\label{suff-ness-1}
    \langle \mathcal{N}(\cdot,\theta)\rho(\theta), g\rangle_{\mathcal{S}}=0,\ \mbox{for all}\ \theta\in\Theta.$$ By equation with $f_{\mu}:=\mathcal{N}(\cdot,\theta)\rho(\theta)$ with $\mu=\delta_{\theta}$, equation is equivalent to $\left<\delta_\theta, g\right>_{C_0(\Theta)}=0$, for all $\theta\in\Theta$, which leads to $g(\theta)=0$ for all $\theta\in\Theta$. That is, $g=0$. Therefore, $^{\perp}\mathcal{B}_{\mathbb{W}}=\{0\}$. This together with the definition of annihilators leads to $(^{\perp}\mathcal{B}_{\mathbb{W}})^\perp=\mathcal{B}_{\mathcal{N}}$, which completes the proof of this theorem. ◻

</div>

To close this section, we summarize the properties of the space $\mathcal{B}_{\mathcal{N}}$ established in Theorems , and as follows:

\(i\) The space $\mathcal{B}_{\mathcal{N}}$ is a vector-valued RKBS.

\(ii\) The vector-valued function $\mathcal{K}$ defined by is the reproducing kernel for space $\mathcal{B}_{\mathcal{N}}$.

\(iii\) The space $\mathcal{B}_{\mathcal{N}}$ is the weak\* completion of the vector space $\mathcal{B}_{\mathbb{W}}$.

These favorable properties of the space $\mathcal{B}_{\mathcal{N}}$ motivate us to take it as the hypothesis space for deep learning. Thus, we consider the following learning model $$\label{LearningMethodinRKBS}
    \inf\{\mathcal{L}(f_{\mu},\mathbb{D}_m): f_{\mu}\in\mathcal{B}_{\mathcal{N}}\}.$$ If we denote by $\mathcal{N}_{\mathcal{B}_\mathcal{N}}$ the neural network learned from the model , then, according to , we have that $$\mathcal{L}(\mathcal{N}_{\mathcal{B}_\mathcal{N}},\mathbb{D}_m)\leq \mathcal{L}(\mathcal{N}_{\mathcal{B}_\mathbb{W}},\mathbb{D}_m)
    \leq \mathcal{L}(\mathcal{N}_{\mathcal{A}_\mathbb{W}},\mathbb{D}_m).$$ Even though learning model is like model , which is of infinite dimension (unlike model , which is of finite dimension), we will show in the next section that a solution of learning model lays in a finite dimensional manifold determined by the kernel $\mathcal{K}$ and a given data set.

# Representer Theorems for Learning Solutions

In this section, we consider learning a target function in $\mathcal{B}_{\mathcal{N}}$ from the sampled dataset $\mathbb{D}_m$ defined by . Learning such a function is an ill-posed problem, whose solutions often suffer from overfitting. For this reason, instead of solving the learning model directly, we consider a related regularization problem and MNI problem in the RKBS $\mathcal{B}_{\mathcal{N}}$. The goal of this section is to establish representer theorems for solutions of these two learning models.

We start with describing the regularized learning problem in the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$. For the dataset $\mathbb{D}_m$ defined by , we define the set $\mathcal{X}:=\{x_j:j\in\mathbb{N}_m\}$ and the matrix $\mathbf{Y}:=[y_j^k:k\in\mathbb{N}_t,j\in\mathbb{N}_{m}]\in\mathbb{R}^{t\times m}$, where for each $j\in\mathbb{N}_m$, $y_j^k$, $k\in\mathbb{N}_t$, are the components of vector $y_j$. We introduce an operator $\mathbf{I}_\mathcal{X}:{\mathcal{B}_{\mathcal{N}}} \rightarrow \mathbb{R}^{t\times m}$ by $$\label{L YES DC}
\mathbf{I}_\mathcal{X}(f_\mu):=\left[f^k_{\mu}(x_j): k\in\mathbb{N}_t,j\in\mathbb{N}_{m}\right],\ \mbox{for}\ f_\mu\in\mathcal{B}_{\mathcal{N}}.$$ We choose a loss function $\mathcal{Q}: \mathbb{R}^{t\times m} \rightarrow \mathbb{R}_{+}:=[0,+\infty)$ and define $$\label{loss:An example}
    \mathcal{L}(f_{\mu},\mathbb{D}_m):=\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_\mu)-\mathbf{Y}),\ \mbox{for}\ f_{\mu}\in\mathcal{B}_{\mathcal{N}}.$$ Examples of loss functions $\mathcal{Q}({\mathbf{M}})$ may be chosen as a norm of the matrix $\mathbf{M}$. The proposed regularization problem is formed by adding a regularization term $\lambda\| f_\mu\|_{{\mathcal{B}_{\mathcal{N}}}}$ to the data fidelity term $\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_{\mu})-\mathbf{Y})$. That is, $$\label{eq: regularization problem RKBS B measure M(X)}
    \inf \left\{\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_\mu)-\mathbf{Y})+\lambda\| f_\mu\|_{{\mathcal{B}_{\mathcal{N}}}}:  f_\mu\in {\mathcal{B}_{\mathcal{N}}}\right\},$$ where $\lambda$ is a positive regularization parameter. The learning model allows us to learn a function $f_\mu$ in the space $\mathcal{B}_\mathcal{N}$ by solving the optimization problem .

We first comment on the existence of a solution to the regularization problem . The next proposition follows directly from Proposition 40 of .

<div class="proposition">

**Proposition 11**.  * Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}$ are given. If $\lambda>0$ and the loss function $\mathcal{Q}$ is lower semi-continuous on $\mathbb{R}^{t\times m}$, then the regularization problem has at least one solution.*

</div>

<div class="proof">

*Proof.* We have shown in Proposition that the Banach space $\mathcal{B}_{\mathcal{N}}$ has the pre-dual space $\mathcal{S}$. We specify the function $\varphi:\mathbb{R}_+\to\mathbb{R}_+$ appearing in Proposition 40 of as the identity function $\varphi(x)=x$, $x\in\mathbb{R}_+$, which is lower semi-continuous, increasing and coercive. Since the loss function $\mathcal{Q}$ is lower semi-continuous on $\mathbb{R}^{t\times m}$, the assumptions in Proposition 40 of are all satisfied. Thus, we conclude from Proposition 40 of that the regularization problem has at least one solution. ◻

</div>

It is known that regularization problems are closely related to MNI problems (see, for example, ). The MNI problem aims at finding a vector-valued function $f_{\mu}$ in $\mathcal{B}_{\mathcal{N}}$, having the smallest norm and satisfying the interpolation condition $f_{\mu}(x_j)=y_j$, $j\in\mathbb{N}_m$. In other words, the MNI problem has the form $$\label{MNI-original}
\inf
\{\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}: f_{\mu}(x_j)=y_j, f_{\mu}\in\mathcal{B}_{\mathcal{N}}, j\in\mathbb{N}_m\}.$$ Associated with the set $\mathcal{X}$ and matrix $\mathbf{Y}$, we introduce a subset $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ of $\mathcal{B}_{\mathcal{N}}$ as $$\label{hyperplane YES DC}
    \mathcal{M}_{\mathcal{X},\mathbf{Y}}:=\{f_\mu \in {\mathcal{B}_{\mathcal{N}}}: \mathbf{I}_{\mathcal{X}}(f_\mu)=\mathbf{Y}\}.$$ We then reformulate the MNI problem in an equivalent form as $$\label{MNI in RKBS B measure M(X)}
    \inf \left\{\left\|f_\mu\right\|_{{\mathcal{B}_{\mathcal{N}}}}: f_\mu \in  \mathcal{M}_{\mathcal{X},\mathbf{Y}}\right\}.$$ The MNI problem has a solution if and only if the set $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is nonempty. The non-emptiness of the set $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ pertains to the existence of interpolation of a function in $\mathcal{B}_{\mathcal{N}}$ to any given data $\mathbb{D}_m$. For the sake of keeping focus on the main issues of this paper, this issue will be postponed to a different occasion. In this paper, we will always assume that the set $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is nonempty.

Recall that the vector-valued function $\mathcal{K}$ defined by is the reproducing kernel for $\mathcal{B}_{\mathcal{N}}$. By using the reproducing property , we represent the operator $\mathbf{I}_{\mathcal{X}}$ defined by as $$\mathbf{I}_{\mathcal{X}}(f_\mu)=\left[\langle \mathcal{K}_k(x_j,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}: k\in\mathbb{N}_t,j\in\mathbb{N}_{m}\right],\ \mbox{for}\ f_\mu\in\mathcal{B}_{\mathcal{N}}.$$ Clearly, the MNI problem includes $tm$ interpolation conditions which are produced by the linear functionals in the set $$\mathbb{K}_\mathcal{X}:=\{\mathcal{K}_k(x_j,\cdot):  \ \ j\in\mathbb{N}_m,\ k\in\mathbb{N}_t\}.$$ It follows from Proposition 1 of that the existence of a solution of the MNI problem is guaranteed if the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent elements in $\mathcal{S}$. In fact, the linear independence of the functionals in $\mathbb{K}_\mathcal{X}$ is a sufficient condition that ensures the non-emptiness of the subset $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ defined by for any given $\mathbf{Y}\in\mathbb{R}^{t\times m}$. If the subset $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is nonempty but the functionals in $\mathbb{K}_\mathcal{X}$ are linearly dependent, one may replace $\mathbb{K}_\mathcal{X}$ by its maximal linearly independent subset. Hence, without loss of generality, we will assume that the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent throughout the rest of this paper.

A solution of the regularization problem may be identified as a solution of an MNI problem in the form of with different data. In fact, according to , every solution $\hat{f}_{\mu}\in\mathcal{B}_{\mathcal{N}}$ of the regularization problem is also a solution of the MNI problem with $\mathbf{Y}:=\mathbf{I}_{\mathcal{X}}(\hat{f}_{\mu})$. In addition, if $\hat{f}_{\mu}\in\mathcal{B}_{\mathcal{N}}$ is a solution of the MNI problem , then $\hat{f}_{\mu}$ is a solution of the regularization problem $$\inf \left\{\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(f_\mu)-\mathbf{Y})+\lambda\| f_\mu\|_{{\mathcal{B}_{\mathcal{N}}}}:  f_\mu\in \mathcal{M}_{\mathcal{X},\mathbf{Y}}\right\},$$ and there holds the relation $${C^*}\leq\mathcal{Q}(\mathbf{I}_{\mathcal{X}}(\hat{f}_{\mu})-\mathbf{Y})+\lambda\| \hat{f}_{\mu}\|_{{\mathcal{B}_{\mathcal{N}}}},$$ where ${C^*}$ is the infimum of the regularization problem , since $\mathcal{M}_{\mathcal{X},\mathbf{Y}}$ is a subset of $\mathcal{B}_{\mathcal{N}}$.

We now establish a representer theorem for a solution of the MNI problem . This can be accomplished by applying the explicit and data-dependent representer theorem for the MNI problem in a general Banach space setting, established in our recent paper to the current setting. To this end, we review some necessary notions of Convex Analysis and recall a result of . Let $\mathbb{X}$ be a Hausdorff locally convex topological vector space. A subset $A$ of $\mathbb{X}$ is called a convex set if $t x+(1-t)y\in A$ for all $x,y\in A$ and all $t\in[0,1]$. The convex hull of a subset $A$ of $\mathbb{X}$, denoted by $\mathrm{co}(A)$, is the smallest convex set that contains $A$. The closed convex hull of $A$, denoted by $\overline{\mathrm{co}}(A)$, is the smallest closed convex set that contains $A$, where the closure is taken under the topology of $\mathbb{X}$. Suppose that $A$ is a nonempty closed convex subset of $\mathbb{X}$. An element $z\in A$ is said to be an extreme point of $A$ if $x,y\in A$ and $tx+(1-t)y=z$ for some $t\in(0,1)$ implies that $x=y=z$. By $\mathrm{ext}(A)$ we denote the set of extreme points of $A$. The celebrated Krein-Milman theorem states that if $A$ is a nonempty compact convex subset of $\mathbb{X}$, then $A$ is the closed convex hull of its set of extreme points, that is, $A=\overline{\mathrm{co}}\left(\mathrm{ext}(A)\right)$. Let $B$ be a Banach space endowed with norm $\|\cdot\|_B$. Clearly, the norm $\|\cdot\|_B$ is a convex function on $B$. The subdifferential of the norm function $\|\cdot\|_B$ at each $f\in B\backslash\{0\}$ is defined by $$\label{subdifferential = norming functional}
  \partial \|\cdot\|_B(f):=\left\{\nu\in B^*:\|\nu\|_{B^*}=1,\langle \nu,f\rangle_{B}=\|f\|_{B}\right\}.$$ Notice that the dual space $B^*$ of $B$ equipped with the weak${}^*$ topology is a Hausdorff locally convex topological vector space. Moreover, for any $f\in B\backslash\{0\}$, the subdifferential set $\partial \|\cdot\|_B(f)$ is a convex and weakly${}^*$ compact subset of $B^*$. Hence, the Krein-Milman theorem ensures that for any $f\in B\backslash\{0\}$, $$\label{representation subdifferential set}
    \partial \|\cdot\|_B(f)=\overline{\mathrm{co}}(\mathrm{ext}(\partial \|\cdot\|_{B}(f))),$$ where the closed convex hull is taken under the weak$^*$ topology of $B^*$.

We now review the representer theorem for the MNI problem in a general Banach space having a pre-dual space established in . Suppose that $B$ is a Banach space having a pre-dual space $B_*$. Let $\nu_j$, $j\in\mathbb{N}_n$, be linearly independent elements in $B_*$ and $\mathbf{z}:=[z_j:j\in\mathbb{N}_n]\in\mathbb{R}^n$ be a given vector. Set $\mathcal{V}:=\mathrm{span}\{\nu_j:j\in\mathbb{N}_n\}$. We define an operator $\mathcal{L}: B \rightarrow \mathbb{R}^{n}$ by $\mathcal{L}(f):=\left[\left\langle\nu_{j}, f\right\rangle_{B}: j \in \mathbb{N}_{n}\right], \text { for all } f \in B,$ and introduce a subset of $B$ as ${M_{\mathbf{z}}}:=\{f \in B: \mathcal{L}(f)=\mathbf{z}\}$. The MNI problem with the given data $\{(\nu_j,y_j):j\in\mathbb{N}_n\}$ considered in has the form $$\label{general MNI in lemma}
    \inf \left\{\|f\|_{B}: f \in {M_{\mathbf{z}}}\right\}.$$

The representer theorem established in Proposition 7 of provides a representation of any extreme point of the solution set of the MNI problem with $\mathbf{z}\in\mathbb{R}^n$. We describe this result in the next lemma.

<div class="lemma">

**Lemma 12**.  * Suppose that $B$ is a Banach space having a pre-dual space $B_{*}$. Let $\nu_{j} \in B_{*}$, $j \in \mathbb{N}_{n}$, be linearly independent and $\mathbf{z} \in \mathbb{R}^{n}\backslash\{\mathbf{0}\}$. If $\mathcal{V}$ and ${M_{\mathbf{z}}}$ are defined as above and $\hat\nu\in\mathcal{V}$ satisfies $$\label{Non-empty-set in lemma}
(\|\hat\nu\|_{B_*}\partial\|\cdot\|_{B_*}(\hat\nu))\cap{M_{\mathbf{z}}}\neq\emptyset,$$ then for any extreme point $\hat f$ of the solution set of the MNI problem , there exist $\gamma_j\in\mathbb{R}$, $j\in\mathbb{N}_{n}$, with $\sum_{j\in\mathbb{N}_{n}}\gamma_j=\|\hat\nu\|_{B_*}$ and $u_j\in\mathrm{ext}\left(\partial\|\cdot\|_{B_*}(\hat\nu)\right)$, $j\in\mathbb{N}_{n}$, such that $$\label{eq: expansion of p in lemma}
\hat f=\sum\limits_{j\in\mathbb{N}_{n}} \gamma_j u_j.$$*

</div>

It was pointed out in that the element $\hat\nu$ satisfying can be obtained through solving a dual problem of . Moreover, we remark that the solution set is a nonempty, convex and weakly$^*$ compact subset of $B$. Hence, by the Krein-Milman theorem, the set of extreme points of the solution set is nonempty and moreover, any solution of problem can be expressed as the weak$^*$ limit of a sequence in the convex hull of the set of extreme points.

We now present a representer theorem for a solution of the MNI problem , which is a direct consequence of Lemma . We introduce a subspace of $\mathcal{S}$, which is defined by and has been proved to be a pre-dual space of $\mathcal{B}_{\mathcal{N}}$, by $$\label{V_span_kernel}
\mathcal{V}_{\mathcal{N}}:=\mathrm{span}\ \mathbb{K}_\mathcal{X},$$ and denote by $\mathbb{S}_{\mathcal{X},\mathbf{Y}}$ the solution set of the MNI problem .

We prepare applying Lemma to the MNI problem . To this end, we introduce the dual problem of problem as $$\label{dual problem}
    \sup\left\{ \sum_{k\in\mathbb{N}_t}\sum_{j\in\mathbb{N}_m}c_{kj}y_j^k:\left\|\sum_{k\in\mathbb{N}_t}\sum_{j\in\mathbb{N}_m} c_{kj}\mathcal{K}_k(x_j,\cdot)\right\|_{\infty}=1\right\}.$$ Note that the dual problem is a finite dimensional optimization problem which has the same optimal value, denoted by ${C^*}$, as the MNI problem . It has been proved in that there exists at least one solution for the dual problem of the MNI problem in $\ell_1(\mathbb{N})$. By a similar argument, we can show the existence of a solution of the dual problem . Suppose that $\hat{\mathbf{c}}:=[\hat{c}_{kj}:k\in\mathbb{N}_t,j\in\mathbb{N}_m]\in\mathbb{R}^{t\times m}$ is a solution of the dual problem . We let $$\label{stage 1 hat g}
    \hat{g}(\cdot):={C^*}\sum_{k\in\mathbb{N}_t}\sum_{j\in\mathbb{N}_m}\hat{c}_{kj}\mathcal{K}_k(x_j,\cdot).$$

<div class="theorem">

**Theorem 13**.  * Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. Let $\hat g$ be the function defined by . Then for any $\hat f\in\mathrm{ext}(\mathbb{S}_{\mathcal{X},\mathbf{Y}})$, there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$, such that $$\label{representer solution of finite linear combination of MNI in BN}
        \hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell h_\ell(x),\ x\in\mathbb{R}^s.$$*

</div>

<div class="proof">

*Proof.* Proposition ensures that the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$ has the pre-dual space $\mathcal{S}$. Note that the functionals in $\mathbb{K}_\mathcal{X}$ belong to the pre-dual space $\mathcal{S}$ and are linearly independent. Moreover, since $\hat g$ is the function defined by , we have that $\hat g\in\mathcal{V}_{\mathcal{N}}$, and according to Proposition 37 of , $\hat g$ satisfies the condition $$\label{non empty set: BN case}
(\|\hat g\|_{\infty}\partial\|\cdot\|_\infty(\hat g))\cap{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}\neq\emptyset.$$ Hence, the hypothesis of Lemma is satisfied. Then by Lemma , we can represent any extreme point $\hat f$ of the solution set $\mathbb{S}_{\mathcal{X},\mathbf{Y}}$ of the MNI problem as in equation for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$. ◻

</div>

Theorem provides for each extreme point of the solution set of problem an explicit and data-dependent representation by using the elements in $\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$. Even more significantly, the essence of Theorem is that although the MNI problem is of infinite dimension, every extreme point of its solution set lays in a *finite* dimensional manifold spanned by $tm$ elements $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$.

As we have demonstrated earlier, the element $\hat g$ satisfying can be obtained by solving the dual problem of . Since $\hat{g}$ is an element in $\mathcal{S}$, the subdifferential $\partial\|\cdot\|_\infty(\hat g)$ is a subset of the space $\mathcal{B}_{\mathcal{N}}$, which is the dual space of $\mathcal{S}$. Notice that the subdifferential set $\partial\|\cdot\|_\infty(\hat g)$ may not be included in space $\mathcal{B}_{\mathbb{W}}$ defined by which is spanned by the kernel sessions $\mathcal{K}(\cdot,\theta)$, $\theta\in\Theta$. However, a learning solution in the vector-valued RKBS $\mathcal{B}_{\mathcal{N}}$ is expected to be represented by the kernel sessions $\mathcal{K}(\cdot,\theta)$, $\theta\in\Theta$. For the purpose of obtaining a kernel representation for a solution of problem , alternatively to problem , we consider a closely related MNI problem in the measure space $\mathcal{M}(\Theta)$ and apply the representer theorem established in to it. We then translate the resulting representer theorem for the MNI problem in $\mathcal{M}(\Theta)$ to that for problem , by using the relation between the solutions of these two problems.

We now introduce the MNI problem in the measure space $\mathcal{M}(\Theta)$ with respect to the the sampled dataset $\mathbb{D}_m$ and show the relation between its solution and a solution of problem . By defining an operator $\widetilde{\mathbf{I}}_{\mathcal{X}}:\mathcal{M}(\Theta) \rightarrow \mathbb{R}^{t\times m}$ by $$\label{tilde L on measure space}
\widetilde{\mathbf{I}}_{\mathcal{X}}(\mu):=\left[\langle \mathcal{K}_k(x_j,\cdot),\mu\rangle_{\mathcal{M}(\Theta)}: k\in\mathbb{N}_t, j \in \mathbb{N}_{m}\right],\ \mbox{for all}\ \mu\in\mathcal{M}(\Theta),$$ and introducing a subset $\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ of $\mathcal{M}(\Theta)$ as $$\label{hyperplane in measure space}
    \widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}:=\left\{\mu\in\mathcal{M}(\Theta):\widetilde{\mathbf{I}}_{\mathcal{X}}(\mu)=\mathbf{Y}\right\},$$ we formulate the MNI problem in $\mathcal{M}(\Theta)$ as $$\label{MNI in measure space}
    \inf \left\{\left\|\mu\right\|_{\mathrm{TV}}: \mu \in \widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}\right\}.$$

The next proposition reveals the relation between the solutions of the MNI problems and .

<div class="proposition">

**Proposition 14**.  * Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. If ${\hat{\mu}}$ is a solution of the MNI problem , then $f_{\hat\mu}(x):=\left[f_{\hat\mu}^k(x): k\in\mathbb{N}_t\right]^\top$, $x\in\mathbb{R}^s$, with $f_{\hat\mu}^k$, $k\in\mathbb{N}_t$, defined as in with $\mu$ replaced by $\hat{\mu}$, is a solution of the MNI problem and $\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}=\|{\hat{\mu}}\|_{\mathrm{TV}}$.*

</div>

<div class="proof">

*Proof.* Note that the measure space $\mathcal{M}(\Theta)$ has the pre-dual space $C_0(\Theta)$. Hence, it follows from Proposition 1 of that the linear independence of the functionals in $\mathbb{K}_\mathcal{X}$ ensures the existence of a solution of problem . Assume that ${\hat{\mu}}$ is a solution of problem . We then obtain that ${\hat{\mu}}\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ and $$\label{proof mu* is a solution}
        \|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|\mu\|_{\mathrm{TV}},\ \text{for all }\mu\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}.$$ By equations and with $g:=\mathcal{K}_k(x_j,\cdot)$, we have for each $k\in\mathbb{N}_t$ and each $j\in\mathbb{N}_{m}$ that $$\label{verify interpolation condition}
    \langle \mathcal{K}_k(x_j,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle \mu,\mathcal{K}_k(x_j,\cdot)\rangle_{C_0(\Theta)}, \ \mbox{for all}\ \mu\in\mathcal{M}(\Theta).$$ Note that $\mathcal{K}_k(x_j,\cdot)\in C_0(\Theta)$ can be viewed as a bounded linear functional on $\mathcal{M}(\Theta)$ and $$\langle \mu,\mathcal{K}_k(x_j,\cdot)\rangle_{C_0(\Theta)}=\langle \mathcal{K}_k(x_j,\cdot),\mu\rangle_{\mathcal{M}(\Theta)}.$$ Substituting the above equation into the right hand of equation leads to $$\langle \mathcal{K}_k(x_j,\cdot), f_{\mu}\rangle_{\mathcal{B}_{\mathcal{N}}}=\langle \mathcal{K}_k(x_j,\cdot),\mu\rangle_{\mathcal{M}(\Theta)}, \ \mbox{for all}\ \mu\in\mathcal{M}(\Theta).$$ This implies that $\mathbf{I}_{\mathcal{X}}(f_{\mu})=\widetilde{\mathbf{I}}_{\mathcal{X}}(\mu)$ for all $\mu\in\mathcal{M}(\Theta)$. As a result, $\mu\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ if and only if $f_{\mu}\in\mathcal{M}_{\mathcal{X},\mathbf{Y}}$. Since ${\hat{\mu}}\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$, we get that $f_{\hat\mu}\in\mathcal{M}_{\mathcal{X},\mathbf{Y}}$. It suffices to verify that $$\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}\leq\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}},\quad\text{for all }f_{\mu}\in{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}.$$ Let $f_{\mu}$ be an arbitrary element in ${\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$. For any $\nu\in\mathcal{M}(\Theta)$ satisfying $f_{\mu}=f_{\nu}$, there holds $f_{\nu}\in{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$. Thus, $\nu\in\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$. It follows from inequality that $$\label{Relation:TV-Norm}
     \|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|\nu\|_{\mathrm{TV}}.$$ By taking infimum of both sides of the inequality over $\nu\in\mathcal{M}(\Theta)$ satisfying $f_\mu=f_\nu$ and noting the definition of the norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$, we get that $$\label{proof mu* leq fnu}
        \|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}.$$ Again by the definition of the norm $\|f_\mu\|_{\mathcal{B}_{\mathcal{N}}}$, we obtain that $$\label{proof fmu* leq mu*}    \|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}\leq\|{\hat{\mu}}\|_{\mathrm{TV}}.$$ Combining inequalities with , we conclude that $\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}\leq\|f_{\mu}\|_{\mathcal{B}_{\mathcal{N}}}$. Therefore, $f_{{\hat{\mu}}}$ is a solution of the MNI problem . Moreover, by taking $\mu={\hat{\mu}}$ in , we get that $\|{\hat{\mu}}\|_{\mathrm{TV}}\leq\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}$. This together with inequality leads to $\|f_{{\hat{\mu}}}\|_{\mathcal{B}_{\mathcal{N}}}=\|{\hat{\mu}}\|_{\mathrm{TV}}$. ◻

</div>

We next derive a representer theorem for a solution of problem by employing Lemma . Applying Lemma to problem requires the representation of the extreme points of the subdifferential set $\partial\|\cdot\|_\infty(g)$ for any nonzero $g\in C_0(\Theta)$. Here, the subdifferential set $\partial\|\cdot\|_\infty(g)$ is a subset of the measure space $\mathcal{M}(\Theta)$. For each $g\in C_0(\Theta)$, let $\Theta(g)$ denote the subset of $\Theta$ where the function $g$ attains its maximum norm $\|g\|_\infty$, that is, $$\label{def: infinity set for function}
    \Theta(g):=\left\{\theta\in \Theta:|g(\theta)|=\|g\|_\infty\right\}.$$ For each $g\in C_0(\Theta)$, we introduce a subset of $\mathcal{M}(\Theta)$ by $$\label{def: Omage f}
    \Omega(g):=\left\{\mathrm{sign}(g(\theta))\delta_\theta:\theta\in\Theta(g)\right\}.$$ Lemma $26$ in essentially states that if $g\in C_0(\Theta)\backslash\{0\}$, then $$\label{extreme points of partial infinity norm}
\mathrm{ext}\left(\partial\|\cdot\|_{\infty}(g)\right)=\Omega(g).$$ We denote by $\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}$ the solution set of the MNI problem . We note that the MNI problem shares the same dual problem with the MNI problem .

<div class="proposition">

**Proposition 15**.  * Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. Let $\hat g$ be the function defined by . Then for any $\hat \mu\in\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$, there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$, such that $$\label{eq: representer for measure space}
        \hat\mu=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell\mathrm{sign}(\hat g(\theta_\ell))\delta_{\theta_\ell}.$$*

</div>

<div class="proof">

*Proof.* Note that the measure space $\mathcal{M}(\Theta)$ has the pre-dual space $C_0(\Theta)$ and the functionals $\mathcal{K}_k(x_j,\cdot)$, $k\in\mathbb{N}_t$, $j\in\mathbb{N}_m$, which belong to the pre-dual space $C_0(\Theta)$, are linearly independent. By Proposition 37 in , the function $\hat g$ defined by satisfies $\hat g\in\mathcal{V}_{\mathcal{N}}$ and $$\label{non empty set: BN case tilde}
(\|\hat g\|_{\infty}\partial\|\cdot\|_\infty(\hat g))\cap{\widetilde{\mathcal{M}}}_{\mathcal{X},\mathbf{Y}}\neq\emptyset,$$ in which the subdifferential set $\partial\|\cdot\|_\infty(\hat g)$ is a subset of the measure space $\mathcal{M}(\Theta)$. As a result, the hypothesis of Lemma is satisfied. According to Lemma , for any $\hat \mu\in\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$, there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $u_\ell\in\mathrm{ext}(\partial \|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$, such that $$\label{proof: apply general MNI theorem to C0}
\hat\mu=\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell u_\ell.$$ It follows from equation that for each $\ell\in\mathbb{N}_{tm}$, we have that $u_\ell\in \Omega(\hat{g})$. By definition of the set $\Omega(\hat{g})$, for each $\ell\in\mathbb{N}_{tm}$, there exists $\theta_\ell\in\Theta(\hat g)$ such that $u_\ell=\mathrm{sign}(\hat g(\theta_\ell))\delta_{\theta_\ell}$. Therefore, we may rewrite the representation of $\hat{\mu}$ as . ◻

</div>

Proposition provides a representation for an extreme point of the solution set of the MNI problem . This solution can be converted via Proposition to a solution of the MNI problem . We present this result in the next theorem.

<div class="theorem">

**Theorem 16**.  * Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}\backslash\{\mathbf{0}\}$ are given, and the functionals in $\mathbb{K}_\mathcal{X}$ are linearly independent. Let $\mathcal{V}_{\mathcal{N}}$ and $\widetilde{\mathcal{M}}_{\mathcal{X},\mathbf{Y}}$ be defined by and , respectively, and let $\hat g$ be the function defined by . Then the MNI problem has a solution $\hat f$ in the form $$\label{representer solution of finite linear combination of MNI}
\hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell\mathrm{sign}(\hat g(\theta_\ell))\mathcal{K}(x,\theta_\ell),\ x\in\mathbb{R}^s,$$ for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$.*

</div>

<div class="proof">

*Proof.* By Proposition 1 of , the MNI problem has at least one solution. That is, $\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}$ is nonempty and moreover, $\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$ is nonempty. We choose $\hat \mu\in\mathrm{ext}\left(\widetilde{\mathbb{S}}_{\mathcal{X},\mathbf{Y}}\right)$. Proposition ensures that there exist $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$, such that $\hat\mu$ may be expressed as in equation . Since $\hat{\mu}$ is a solution of problem , we get by Proposition that $f_{\hat\mu}=[f_{\hat\mu}^k:k\in\mathbb{N}_t]$ is a solution of the MNI problem . By definition of $f_{\hat\mu}^k$, $k\in\mathbb{N}_t$, we have that $$\label{Solution_Form}
f_{\hat\mu}^k(x)=\int_\Theta \mathcal{K}_k(x,\theta)d\hat{\mu}(\theta), \quad\text{for }x\in\mathbb{R}^s,  k\in\mathbb{N}_t.$$ Substituting representation of $\hat\mu$ into the right-hand side of equation yields that $$f_{\hat\mu}^k(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}}\gamma_\ell\mathrm{sign}(\hat g(\theta_\ell))\mathcal{K}_k(x,\theta_\ell), \quad\text{for }x\in\mathbb{R}^s,  k\in\mathbb{N}_t.$$ By letting $\hat f:=f_{\hat\mu}$, we conclude that the MNI problem has a solution $\hat f$ in the form of . ◻

</div>

We now return to considering the regularization problem . Our goal is to establish representer theorems for a solution of the regularization problem. To ensure the existence of a solution of the regularization problem , we always assume that the loss function $\mathcal{Q}$ is lower semi-continuous on $\mathbb{R}^{t\times m}$. Before establishing the representer theorems for a solution of problem , we point out the relation between the solutions of this problem and the MNI problem . We denote by $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ the solution set of problem . By Proposition , $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ is nonempty. We then introduce a subset $\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ of $\mathbb{R}^{t\times m}$ by $$\label{general Dy}
\mathcal{D}_{\mathcal{X},\mathbf{Y}}:=\mathbf{I}_{\mathcal{X}}(\mathbb{R}_{\mathcal{X},\mathbf{Y}}).$$ Now, recalling that $\mathbb{S}_{\mathcal{X},\mathbf{Y}}$ denotes the solution set of the MNI problem , it follows from Proposition 41 of that $$\label{Proposition41-in- WangXu2}
\bigcup_{{\mathbf{Z}}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}}\mathbb{S}_{\mathcal{X},\mathbf{Z}}=\mathbb{R}_{\mathcal{X},\mathbf{Y}}.$$ Moreover, by Lemma 11 of , if the loss function $\mathcal{Q}$ is convex, then $$\label{relation_extreme_sets}
        \mathrm{ext}\left(\mathbb{R}_{\mathcal{X},\mathbf{Y}}\right)\subset\bigcup_{\mathbf{Z}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}}\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\mathbf{Z}}\right).$$

Below, we convert the representer theorem for a solution of problem stated in Theorem to that for the regularization problem by making use of the relation between the solutions of problems and .

<div class="theorem">

**Theorem 17**.  * Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}$ are given, $\lambda>0$. Let $\mathcal{V}_{\mathcal{N}}$ be defined by and $\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ be defined by .*

1.  *If $\mathcal{D}_{\mathcal{X},\mathbf{Y}}\neq\{\mathbf{0}\}$, then there exists $\hat f\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that $$\label{eq: regularization problem increasing case in BN}
            \hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}} \alpha_\ell h_\ell(x),\ x\in\mathbb{R}^s,$$ for some $\hat g\in\mathcal{V}_{\mathcal{N}}$, $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.*

2.  *If the loss function $\mathcal{Q}$ is convex, then every nonzero extreme point $\hat f$ of $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ has the form of for some $\hat g\in\mathcal{V}_{\mathcal{N}}$, $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.*

</div>

<div class="proof">

*Proof.* We first prove Item 1. Note that $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ is an nonempty set. It follows from the hypothesis $\mathcal{D}_{\mathcal{X},\mathbf{Y}}\neq\{\mathbf{0}\}$ that there exists $\hat{h}\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that $\hat{\mathbf{Z}}:=\mathbf{I}_{\mathcal{X}}(\hat{h})\neq 0$. According to equation with noting that $\hat{\mathbf{Z}}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}$, we have that $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\subset\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ and thus, $\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\right)\subset\mathbb{R}_{\mathcal{X},\mathbf{Y}}$. We choose $\hat f\in\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\right)$ and verify that $\hat f$ can be represented as in . To this end, we choose $\hat{\mathbf{c}}:=[\hat{c}_{kj}:k\in\mathbb{N}_t,j\in\mathbb{N}_m]\in\mathbb{R}^{t\times m}$ to be a solution of the dual problem with $\mathbf{Y}$ replaced by $\hat{\mathbf{Z}}$. Let $\hat g$ be the function defined by with $\hat{\mathbf{c}}$. Theorem ensures that $\hat f$, as an extreme point of the solution set $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}$, can be represented as in for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$.

We next show Item 2. Assume that $\hat f$ is an arbitrary nonzero extreme point of $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$, that is $\hat f\in\mathrm{ext}(\mathbb{R}_{\mathcal{X},\mathbf{Y}})\backslash\{0\}$. Because the loss function $\mathcal{Q}$ is convex, the inclusion relation is satisfied. By , there exists $\hat{\mathbf{Z}}\in\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ such that $\hat f\in\mathrm{ext}\left(\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\right)$. Clearly, $\hat{\mathbf{Z}}\neq 0.$ Assume to the contrary that $\hat{\mathbf{Z}}= 0.$ We then must have that $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}=\{0\}.$ As a result, $\hat f=0,$ which is a contradiction. Again, let $\hat g$ be defined by with $\hat{\mathbf{c}}$ being a solution of problem with $\mathbf{Y}$ being replaced by $\hat{\mathbf{Z}}$. By Theorem , we can represent $\hat f$ as in for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$, $\ell\in\mathbb{N}_{tm}$. ◻

</div>

Similarly to Theorem for a solution of the MNI problem, Theorem ensures that each extreme point of the solution set of the regularization problem lays in a *finite* dimensional manifold spanned by $tm$ elements $h_\ell\in\mathrm{ext}(\partial\|\cdot\|_\infty(\hat g))$ and it has an explicit and data-dependent representation.

We further show that there exists a solution of the regularization problem that can be represented as a linear combination of a finite number of kernel sessions.

<div class="theorem">

**Theorem 18**. *Suppose that $m$ distinct points $x_j\in\mathbb{R}^s$, $j\in\mathbb{N}_m$, and $\mathbf{Y}\in\mathbb{R}^{t\times m}$ are given, $\lambda>0$. Let $\mathcal{V}_{\mathcal{N}}$ be defined by and $\mathcal{D}_{\mathcal{X},\mathbf{Y}}$ be defined by . Then there exists $\hat f\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that $$\label{eq: kernel representation regularization problem increasing case in BN}
\hat f(x)=\sum\limits_{\ell\in\mathbb{N}_{tm}} \gamma_\ell\mathrm{sign}(\hat g(\theta_\ell)) \mathcal{K}(x,\theta_\ell),\ x\in\mathbb{R}^s,$$ for some $\hat g\in\mathcal{V}_{\mathcal{N}}$, $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$.*

</div>

<div class="proof">

*Proof.* Since $\mathbb{R}_{\mathcal{X},\mathbf{Y}}$ is nonempty and $\mathcal{D}_{\lambda,\mathbf{Y}}\neq\{\mathbf{0}\}$, there exists $\hat h \in \mathbb{R}_{\mathcal{X},\mathbf{Y}}$ such that $\hat{\mathbf{Z}}:=\mathbf{I}_{\mathcal{X}}(\hat h)\neq \mathbf{0}$. We choose $\hat g$ in the form of , where $\hat{\mathbf{c}}$ is a solution of problem with $\mathbf{Y}$ being replaced by $\hat{\mathbf{Z}}$. According to Theorem , the MNI problem with $\mathbf{Y}:=\hat{\mathbf{Z}}$ has a solution $\hat f$ in the form of , for some $\gamma_\ell\in\mathbb{R}$, $\ell\in\mathbb{N}_{tm}$, with $\sum_{\ell\in\mathbb{N}_{tm}}\gamma_\ell=\|\hat g\|_{\infty}$ and $\theta_\ell\in\Theta(\hat g)$, $\ell\in\mathbb{N}_{tm}$. In other words, $\hat f\in \mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}$ and it has the form of . It follows from relation that $\mathbb{S}_{\mathcal{X},\hat{\mathbf{Z}}}\subset\mathbb{R}_{\mathcal{X},\mathbf{Y}}$, which implies that $\hat f\in\mathbb{R}_{\mathcal{X},\mathbf{Y}}$. ◻

</div>

To close this section, we remark on relevant existing work on representer theorems for deep learning solution. Representer theorems for deep learning have been investigated in . Bohn et al. studied in the composition of reproducing kernel Hilbert spaces as the hypothesis space, and derived a representer theorem for deep kernel learning, which led to a solution in the form of composition of a finite number of kernel sessions. In , Unser proposed to optimize the activation functions in the deep neural network by a second-order total-variation regularization, and the resulting representer theorem ensures that the activation function for each node in the optimal network is a linear spline with adaptive knots.

# Concluding Remarks

We have introduced the hypothesis space $\mathcal{B}_{\mathcal{N}}$ for deep learning. The hypothesis space that we have come up with is a vector-valued RKBS which has a unique vector-valued reproducing kernel $\mathcal{K}$ and the weak\* completion of the vector space $\mathcal{B}_{\mathbb{W}}$ which is the linear span of the primitive set $\mathcal{A}_{\mathbb{W}}$ for deep learning. The hypothesis space allows us to understand mathematical insights of deep learning. Specifically, by exploiting the hypothesis space, we have developed representer theorems for solutions of two deep learning models the minimum norm interpolation and regularization problem with deep neural networks.

In the remaining part of this section, we discuss relations among several learning models considered in this paper and comment on advantages of learning in the proposed hypothesis space $\mathcal{B}_{\mathcal{N}}$.

First, we remark on relations among several learning models studied in this paper. Suppose that the loss function $\mathcal{L}(f_\mu,\mathbb{D}_m)$ takes the form of with a function $\mathcal{Q}:\mathbb{R}^{t\times m}\to \mathbb{R}_+$ satisfying the condition $\mathcal{Q}(0)=0$. If $\hat{f}_\mu\in \mathcal{B}_\mathcal{N}$ is a solution of , then we have that $\mathcal{L}(\hat{f}_\mu, \mathbb{D}_m)=0$ and thus, $\hat{f}_\mu$ is a solution of the learning model . Furthermore, let $f_{\mu,\lambda}$ be a solution of the regularized learning problem , and denote by $f_{\mu,0}$ the limit of $f_{\mu,\lambda}$ as $\lambda\to 0$ if the limit exists. Then, we clearly have that $f_{\mu,0}$ is a solution of the learning model . Therefore, learning models and are favorable stable substitutes of learning model , which may suffer from instability.

Learning in the RKBS space $\mathcal{B}_{\mathcal{N}}$ has several advantages. First of all, the primitive learning model , a model commonly considered in the machine learning community, is not guaranteed to have a solution since the primitive set $\mathcal{A}_\mathbb{W}$ has neither algebraic nor topological structures. Since the space $\mathcal{B}_\mathcal{N}$, which has the pre-dual space $\mathcal{S}$, is the completion of the linear span $\mathcal{B}_\mathbb{W}$ (of $\mathcal{A}_\mathbb{W}$) in the weak\* topology, it is the smallest RKBS that contains $\mathcal{A}_\mathbb{W}$. On one hand, it embraces the intrinsic features of the set $\mathcal{A}_\mathbb{W}$ and on the other hand, it has desired algebraic and topological structures which allow us to conduct mathematical analysis of learning on it. Hence, it is natural to consider the RKBS $\mathcal{B}_\mathcal{N}$ as a hypothesis space for deep learning problems. Due to the algebraic and topological structures of $\mathcal{B}_\mathcal{N}$, unlike model , the learning model is guaranteed to have a solution under a mild condition. Last but not least, the reproducing kernel of the RKBS $\mathcal{B}_{\mathcal{N}}$ furnishes a learning solution in the RKBS $\mathcal{B}_{\mathcal{N}}$ a representation in terms of the reproducing kernel, which leads to a representer theorem of deep learning. The resulting representer theorems reveal that although the learning models on the proposed hypothesis space are of infinite dimension, their solutions lay in finite dimensional manifolds and can be expressed as a linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel. In summary, introducing the hypothesis space $\mathcal{B}_\mathcal{N}$ to deep learning enables us to understand the insights of deep learning and provide a foundation for its mathematical analysis.

# Acknowledgement

R. Wang is supported in part by the Natural Science Foundation of China under grant 12171202 and by the National Key Research and Development Program of China under grants 2020YFA0714101; Y. Xu is supported in part by the US National Science Foundation under grant DMS-2208386, and by the US National Institutes of Health under grant R21CA263876. M. Yan is supported in part by Cheng Fund from College of Science at Old Dominion University. All correspondence should be sent to Y. Xu.

[^1]: School of Mathematics, Jilin University, Changchun 130012, P. R. China. E-mail address: *rwang11@jlu.edu.cn*.

[^2]: Department of Mathematics and Statistics, Old Dominion University, Norfolk, VA 23529, USA. This author is also a Professor Emeritus of Mathematics, Syracuse University, Syracuse, NY 13244, USA. E-mail address: *y1xu@odu.edu.* All correspondence should be sent to this author.

[^3]: Department of Mathematics and Statistics, Old Dominion University, Norfolk, VA 23529, USA. E-mail address: *myan007@odu.edu*.
