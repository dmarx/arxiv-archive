\begin{thebibliography}{133}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abnar et~al.(2022)Abnar, Dehghani, Neyshabur, and Sedghi]{abnar2022exploring}
Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi.
\newblock Exploring the limits of large scale pre-training.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.
\newblock \url{https://arxiv.org/abs/2110.02095}.

\bibitem[Alabdulmohsin et~al.(2022)Alabdulmohsin, Neyshabur, and Zhai]{alabdulmohsin2022revisiting}
Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock In \emph{Advances in Neural Information Processing Systems (NeuIPS)}, 2022.
\newblock \url{https://arxiv.org/abs/2209.06640}.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang, Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2402.16827}.

\bibitem[Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis, Muennighoff, Mishra, Gu, Dey, et~al.]{allal2023santacoder}
Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et~al.
\newblock Santacoder: don't reach for the stars!
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2301.03988}.

\bibitem[Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and Hajishirzi]{mathqa}
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
\newblock {M}ath{QA}: Towards interpretable math word problem solving with operation-based formalisms.
\newblock In \emph{Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NACCL)}, 2019.
\newblock \url{https://aclanthology.org/N19-1245}.

\bibitem[Ansel et~al.(2024)Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao, Berard, Chauhan, Chourdia, Constable, Desmaison, DeVito, Ellison, Feng, Gong, Gschwind, Hirsh, Huang, Kirsch, Lazos, Liang, Liang, Lu, Luk, Maher, Pan, Puhrsch, Reso, Saroufim, Suk, Suo, Tillet, Wang, Wang, Wen, Zhang, Zhao, Zhou, Zou, Mathews, Chanan, Wu, and Chintala]{pytorch2}
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, David Berard, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Laurent Kirsch, Michael Lazos, Yanbo Liang, Jason Liang, Yinghai Lu, CK~Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu~Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala.
\newblock Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation.
\newblock In \emph{International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)}, 2024.
\newblock \url{https://pytorch.org/blog/pytorch-2-paper-tutorial}.

\bibitem[Artetxe et~al.(2022)Artetxe, Bhosale, Goyal, Mihaylov, Ott, Shleifer, Lin, Du, Iyer, Pasunuru, Anantharaman, Li, Chen, Akin, Baines, Martin, Zhou, Koura, O{'}Horo, Wang, Zettlemoyer, Diab, Kozareva, and Stoyanov]{artetxe-etal-2022-efficient}
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi~Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit~Singh Koura, Brian O{'}Horo, Jeffrey Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Veselin Stoyanov.
\newblock Efficient large scale language modeling with mixtures of experts.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2022.
\newblock \url{https://aclanthology.org/2022.emnlp-main.804}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint}, 2016.
\newblock \url{https://arxiv.org/abs/1607.06450}.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint}, 2021.
\newblock \url{https://arxiv.org/abs/2102.06701}.

\bibitem[Bansal et~al.(2022)Bansal, Ghorbani, Garg, Zhang, Krikun, Cherry, Neyshabur, and Firat]{bansal2022data}
Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Maxim Krikun, Colin Cherry, Behnam Neyshabur, and Orhan Firat.
\newblock Data scaling laws in nmt: The effect of noise and architecture.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.
\newblock \url{https://proceedings.mlr.press/v162/bansal22b.html}.

\bibitem[bench authors(2023)]{srivastava2023beyond}
BIG bench authors.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock In \emph{Transactions on Machine Learning Research (TMLR)}, 2023.
\newblock \url{https://openreview.net/forum?id=uyTL5Bvosj}.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings ACM conference on fairness, accountability, and transparency (FAccT)}, 2021.
\newblock \url{https://dl.acm.org/doi/10.1145/3442188.3445922}.

\bibitem[Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu, Gao, Gao, Gao, Ge, Guan, Guo, Guo, Hao, Hao, He, Hu, Huang, Li, Li, Li, Li, Li, Liang, Lin, Liu, Liu, Liu, Liu, Liu, Liu, Lu, Lu, Luo, Ma, Nie, Pei, Piao, Qiu, Qu, Ren, Ren, Ruan, Sha, Shao, Song, Su, Sun, Sun, Tang, Wang, Wang, Wang, Wang, Wang, Wu, Wu, Xie, Xie, Xie, Xiong, Xu, Xu, Xu, Yang, mei You, Yu, yuan Yu, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhao, Zhao, Zhou, Zhou, Zhu, and Zou]{Bi2024DeepSeekLS}
DeepSeek-AI~Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.~K. Li, Wenfeng Liang, Fangyun Lin, A.~X. Liu, Bo~Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Yu~Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yi~Xiong, Hanwei Xu, Ronald~X Xu, Yanhong Xu, Dejian Yang, Yu~mei You, Shuiping Yu, Xin yuan Yu, Bo~Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao
  Zhu, and Yuheng Zou.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2401.02954}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{piqa}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Association for the Advancement of Artificial Intelligence (AAAI)}, 2020.
\newblock \url{https://arxiv.org/abs/1911.11641}.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang, and Weinbach]{neox}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN~Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock \emph{BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models}, 2022.
\newblock \url{https://aclanthology.org/2022.bigscience-1.9}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.
\newblock \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Caballero et~al.(2023)Caballero, Gupta, Rish, and Krueger]{caballero2023broken}
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger.
\newblock Broken neural scaling laws.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2023.
\newblock \url{https://openreview.net/forum?id=sckjveqlCZ}.

\bibitem[Cherti et~al.(2023)Cherti, Beaumont, Wightman, Wortsman, Ilharco, Gordon, Schuhmann, Schmidt, and Jitsev]{cherti2023reproducible}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023.
\newblock \url{https://arxiv.org/abs/2212.07143}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, D{\'i}az, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{Chowdhery2022PaLMSL}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D{\'i}az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
  and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock In \emph{Journal of Machine Learning Research (JMLR)}, 2022.
\newblock \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint}, 2022.
\newblock \url{https://arxiv.org/abs/2210.11416}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock In \emph{Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NAACL)}, 2019.
\newblock \url{https://aclanthology.org/N19-1300}.

\bibitem[Clark et~al.(2020)Clark, Luong, Le, and Manning]{clark2020electra}
Kevin Clark, Minh-Thang Luong, Quoc~V. Le, and Christopher~D. Manning.
\newblock {ELECTRA}: Pre-training text encoders as discriminators rather than generators.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2020.
\newblock \url{https://openreview.net/pdf?id=r1xMH1BtvB}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint}, 2018.
\newblock \url{https://arxiv.org/abs/1803.05457}.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.
\newblock \url{https://arxiv.org/abs/2205.14135}.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.
\newblock \url{https://proceedings.mlr.press/v202/dehghani23a.html}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NAACL)}, 2019.
\newblock \url{https://aclanthology.org/N19-1423}.

\bibitem[Dodge et~al.(2021)Dodge, Sap, Marasovi{\'c}, Agnew, Ilharco, Groeneveld, Mitchell, and Gardner]{c4_ai2}
Jesse Dodge, Maarten Sap, Ana Marasovi{\'c}, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
\newblock Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2021.
\newblock \url{https://aclanthology.org/2021.emnlp-main.98}.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du2022glam}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.
\newblock \url{https://arxiv.org/abs/2112.06905}.

\bibitem[Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela]{ethayarajh2024kto}
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela.
\newblock Kto: Model alignment as prospect theoretic optimization.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2402.01306}.

\bibitem[Gadre et~al.(2023)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Ryan~Marten, Ghosh, Zhang, Orgad, Entezari, Daras, Pratt, Ramanujan, Bitton, Marathe, Mussmann, Richard~Vencu, Krishna, Koh, Saukh, Ratner, Song, Hajishirzi, Farhadi, Beaumont, Oh, Dimakis, Jitsev, Carmon, Shankar, and Schmidt]{datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Mitchell~Wortsman Ryan~Marten, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Mehdi~Cherti Richard~Vencu, Ranjay Krishna, Pang~Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.
\newblock \url{https://arxiv.org/abs/2304.14108}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The {P}ile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint}, 2020.
\newblock \url{https://arxiv.org/abs/2101.00027}.

\bibitem[Ghorbani et~al.(2021)Ghorbani, Firat, Freitag, Bapna, Krikun, Garcia, Chelba, and Cherry]{ghorbani2021scaling}
Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry.
\newblock Scaling laws for neural machine translation.
\newblock \emph{arXiv preprint}, 2021.
\newblock \url{https://arxiv.org/abs/2109.07740}.

\bibitem[Gordon et~al.(2021)Gordon, Duh, and Kaplan]{gordon-etal-2021-data}
Mitchell~A Gordon, Kevin Duh, and Jared Kaplan.
\newblock Data and parameter scaling laws for neural machine translation.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2021.
\newblock \url{https://aclanthology.org/2021.emnlp-main.478}.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, et~al.]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2402.00838}.

\bibitem[Gu \& Dao(2023)Gu and Dao]{mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2312.00752}.

\bibitem[Gu et~al.(2021)Gu, Johnson, Goel, Saab, Dao, Rudra, and R{\'e}]{gu2021combining}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with linear state space layers.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.
\newblock \url{https://openreview.net/forum?id=yWd42CWN3c}.

\bibitem[Gu et~al.(2022)Gu, Goel, and R{\'e}]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.
\newblock \url{https://arxiv.org/abs/2111.00396}.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Cesar, Mendes, Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, Salim, Shah, Singh~Behl, Wang, Bubeck, Eldan, Kalai, Lee, and Li]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio Cesar, Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh~Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need.
\newblock \emph{Preprint}, 2023.
\newblock \url{https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need}.

\bibitem[Gururangan et~al.(2023)Gururangan, Wortsman, Gadre, Dave, Kilian, Shi, Mercat, Smyrnis, Ilharco, Jordan, Heckel, Dimakis, Farhadi, Shankar, and Schmidt]{open_lm}
Suchin Gururangan, Mitchell Wortsman, Samir~Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt.
\newblock {OpenLM}: a minimal but performative language modeling (lm) repository, 2023.
\newblock \url{https://github.com/mlfoundations/open_lm}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.
\newblock \url{https://arxiv.org/abs/2009.03300}.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray, Hallacy, Mann, Radford, Ramesh, Ryder, Ziegler, Schulman, Amodei, and McCandlish]{Henighan2020ScalingLF}
T.~J. Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel~M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{arXiv preprint}, 2020.
\newblock \url{https://arxiv.org/abs/2010.14701}.

\bibitem[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish]{Hernandez2021ScalingLF}
Danny Hernandez, Jared Kaplan, T.~J. Henighan, and Sam McCandlish.
\newblock Scaling laws for transfer.
\newblock \emph{arXiv preprint}, 2021.
\newblock \url{https://arxiv.org/abs/2102.01293}.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou]{og_scaling}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory~Frederick Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{arXiv preprint}, 2017.
\newblock \url{https://arxiv.org/abs/1712.00409}.

\bibitem[Hestness et~al.(2019)Hestness, Ardalani, and Diamos]{hestness2019beyond}
Joel Hestness, Newsha Ardalani, and Gregory Diamos.
\newblock Beyond human-level accuracy: Computational challenges in deep learning.
\newblock In \emph{Principles and Practice of Parallel Programming (PPoPP)}, 2019.
\newblock \url{https://arxiv.org/abs/1909.01736}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{chinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.
\newblock \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Inan et~al.(2017)Inan, Khosravi, and Socher]{Inan2016TyingWV}
Hakan Inan, Khashayar Khosravi, and Richard Socher.
\newblock Tying word vectors and word classifiers: A loss framework for language modeling.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2017.
\newblock \url{https://arxiv.org/abs/1611.01462}.

\bibitem[Isik et~al.(2024)Isik, Ponomareva, Hazimeh, Paparas, Vassilvitskii, and Koyejo]{Isik2024ScalingLF}
Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo.
\newblock Scaling laws for downstream task performance of large language models.
\newblock \emph{arXiv}, 2024.
\newblock \url{https://arxiv.org/abs/2402.04177}.

\bibitem[Ivgi et~al.(2022)Ivgi, Carmon, and Berant]{ivgi2022scaling}
Maor Ivgi, Yair Carmon, and Jonathan Berant.
\newblock Scaling laws under the microscope: Predicting transformer performance from small scale experiments.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2022.
\newblock \url{https://aclanthology.org/2022.findings-emnlp.544}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Diego de~las Casas, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Florian~Bressand Diego de~las Casas, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2310.06825}.

\bibitem[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{pubmed}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.
\newblock Pubmedqa: A dataset for biomedical research question answering.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2019.
\newblock \url{https://aclanthology.org/D19-1259}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint}, 2020.
\newblock \url{https://arxiv.org/abs/2001.08361}.

\bibitem[Klug et~al.(2023)Klug, Atik, and Heckel]{klug2023analyzing}
Tobit Klug, Dogukan Atik, and Reinhard Heckel.
\newblock Analyzing the sample complexity of self-supervised image reconstruction methods.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2305.19079}.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and Soricut]{albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
\newblock {ALBERT:} {A} lite {BERT} for self-supervised learning of language representations.
\newblock \emph{arXiv preprint}, 2019.
\newblock \url{http://arxiv.org/abs/1909.11942}.

\bibitem[Lefaudeux et~al.(2022)Lefaudeux, Massa, Liskovich, Xiong, Caggiano, Naren, Xu, Hu, Tintore, Zhang, Labatut, and Haziza]{xFormers2022}
Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.
\newblock xformers: A modular and hackable transformer modelling library, 2022.
\newblock \url{https://github.com/facebookresearch/xformers}.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and Morgenstern]{winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In \emph{International conference on the principles of knowledge representation and reasoning}, 2012.
\newblock \url{https://aaai.org/papers/59-4492-the-winograd-schema-challenge}.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer]{lewis-etal-2020-bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2020.
\newblock \url{https://aclanthology.org/2020.acl-main.703}.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2305.06161}.

\bibitem[Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang]{Liu2020LogiQAAC}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.
\newblock Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.
\newblock In \emph{International Joint Conference on Artificial Intelligence}, 2020.
\newblock \url{https://arxiv.org/abs/2007.08124}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint}, 2019.
\newblock \url{http://arxiv.org/abs/1907.11692}.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and Xie]{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
\newblock A convnet for the 2020s.
\newblock \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.
\newblock \url{https://arxiv.org/abs/2201.03545}.

\bibitem[Longpre et~al.(2023)Longpre, Mahari, Chen, Obeng-Marnu, Sileo, Brannon, Muennighoff, Khazam, Kabbara, Perisetla, et~al.]{longpre2023data}
Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et~al.
\newblock The data provenance initiative: A large scale audit of dataset licensing \& attribution in ai.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2310.16787}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint}, 2017.
\newblock \url{https://arxiv.org/abs/1711.05101}.

\bibitem[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, Liu, Tian, Kocetkov, Zucker, Belkada, Wang, Liu, Abulkhanov, Paul, Li, Li, Risdal, Li, Zhu, Zhuo, Zheltonozhskii, Dade, Yu, Krauß, Jain, Su, He, Dey, Abati, Chai, Muennighoff, Tang, Oblokulov, Akiki, Marone, Mou, Mishra, Gu, Hui, Dao, Zebaze, Dehaene, Patry, Xu, McAuley, Hu, Scholak, Paquet, Robinson, Anderson, Chapados, Patwary, Tajbakhsh, Jernite, Ferrandis, Zhang, Hughes, Wolf, Guha, von Werra, and de~Vries]{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry~Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae~Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn~Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos~Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de~Vries.
\newblock Starcoder 2 and the stack v2: The next generation.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2402.19173}.

\bibitem[Luukkonen et~al.(2023)Luukkonen, Komulainen, Luoma, Eskelinen, Kanerva, Kupari, Ginter, Laippala, Muennighoff, Piktus, et~al.]{luukkonen2023fingpt}
Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et~al.
\newblock Fingpt: Large generative models for a small language.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2023.
\newblock \url{https://aclanthology.org/2023.emnlp-main.164}.

\bibitem[Magnusson et~al.(2023)Magnusson, Bhagia, Hofmann, Soldaini, Harsh~Jha, Tafjord, Schwenk, Walsh, Elazar, Lo, Groenveld, Beltagy, Hajishirz, Smith, Richardson, and Dodge]{paloma}
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh~Jha, Oyvind Tafjord, Dustin Schwenk, Evan~Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groenveld, Iz~Beltagy, Hanneneh Hajishirz, Noah~A. Smith, Kyle Richardson, and Jesse Dodge.
\newblock {Paloma}: A benchmark for evaluating language model fit.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://paloma.allen.ai}.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and Marcinkiewicz]{ptb}
Mitchell~P. Marcus, Beatrice Santorini, and Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of {E}nglish: The {P}enn {T}reebank.
\newblock In \emph{Computational Linguistics}, 1993.
\newblock \url{https://aclanthology.org/J93-2004}.

\bibitem[Merrill et~al.(2021)Merrill, Ramanujan, Goldberg, Schwartz, and Smith]{merrill-etal-2021-effects}
William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah~A. Smith.
\newblock Effects of parameter norm growth during transformer training: Inductive bias from gradient descent.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2021.
\newblock \url{https://aclanthology.org/2021.emnlp-main.133}.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{OpenBookQA2018}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2018.
\newblock \url{https://arxiv.org/abs/1809.02789}.

\bibitem[MosaicML(2023)]{mosaicml}
MosaicML.
\newblock Llm evaluation scores, 2023.
\newblock \url{https://www.mosaicml.com/llm-evaluation}.

\bibitem[Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts, Biderman, Scao, Bari, Shen, Yong, Schoelkopf, et~al.]{muennighoff2022crosslingual}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et~al.
\newblock Crosslingual generalization through multitask finetuning.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2022.
\newblock \url{https://aclanthology.org/2023.acl-long.891}.

\bibitem[Muennighoff et~al.(2023{\natexlab{a}})Muennighoff, Liu, Zebaze, Zheng, Hui, Zhuo, Singh, Tang, Von~Werra, and Longpre]{muennighoff2023octopack}
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry~Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von~Werra, and Shayne Longpre.
\newblock Octopack: Instruction tuning code large language models.
\newblock \emph{arXiv preprint}, 2023{\natexlab{a}}.
\newblock \url{https://arxiv.org/abs/2308.07124}.

\bibitem[Muennighoff et~al.(2023{\natexlab{b}})Muennighoff, Rush, Barak, Scao, Piktus, Tazi, Pyysalo, Wolf, and Raffel]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
\newblock Scaling data-constrained language models.
\newblock In \emph{Advances in Neural Information Processing Systems (NeuIPS)}, 2023{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/2305.16264}.

\bibitem[Muennighoff et~al.(2024)Muennighoff, Su, Wang, Yang, Wei, Yu, Singh, and Kiela]{muennighoff2024generative}
Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela.
\newblock Generative representational instruction tuning.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2402.09906}.

\bibitem[Nijkamp et~al.(2023)Nijkamp, Xie, Hayashi, Pang, Xia, Xing, Vig, Yavuz, Laban, Krause, Purushwalkam, Niu, Kryscinski, Murakhovs'ka, Choubey, Fabbri, Liu, Meng, Tu, Bhat, Wu, Savarese, Zhou, Joty, and Xiong]{XGen}
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo~Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla~Kumar Choubey, Alex Fabbri, Ye~Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq~Rayhan Joty, and Caiming Xiong.
\newblock Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2309.03450}.

\bibitem[OpenAI(2021)]{triton}
OpenAI.
\newblock Triton, 2021.
\newblock \url{https://github.com/openai/triton}.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.
\newblock \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fernandez]{lambada}
Denis Paperno, Germ\'{a}n Kruszewski, Angeliki Lazaridou, Ngoc~Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2016.
\newblock \url{http://www.aclweb.org/anthology/P16-1144}.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni-etal-2002-bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock {B}leu: a method for automatic evaluation of machine translation.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2002.
\newblock \url{https://aclanthology.org/P02-1040}.

\bibitem[Parrish et~al.(2022)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman]{bbq}
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel Bowman.
\newblock {BBQ}: A hand-built bias benchmark for question answering.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2022.
\newblock \url{https://aclanthology.org/2022.findings-acl.165}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2019.
\newblock \url{https://arxiv.org/abs/1912.01703}.

\bibitem[{Patronus AI}(2023)]{pii}
{Patronus AI}.
\newblock Enterprise{PII} dataset, 2023.
\newblock \url{https://tinyurl.com/2r5x9bst}.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2306.01116}.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Biderman, Cao, Cheng, Chung, Derczynski, Du, Grella, Gv, He, Hou, Kazienko, Kocon, Kong, Koptyra, Lau, Lin, Mantri, Mom, Saito, Song, Tang, Wind, Wo{\'z}niak, Zhang, Zhou, Zhu, and Zhu]{peng-etal-2023-rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart{\l}omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri~Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis{\l}aw Wo{\'z}niak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.
\newblock {RWKV}: Reinventing {RNN}s for the transformer era.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2023.
\newblock \url{https://aclanthology.org/2023.findings-emnlp.936}.

\bibitem[Press \& Wolf(2017)Press and Wolf]{press-wolf-2017-using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock In \emph{Proceedings of the Conference of the {E}uropean Chapter of the Association for Computational Linguistics (EACL)}, 2017.
\newblock \url{https://aclanthology.org/E17-2025}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{Radford2019LanguageMA}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{Preprint}, 2019.
\newblock \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving]{gopher}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F.~J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L.~Sifre, Lena Martens, Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N.~K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris
  Jones, James Bradbury, Matthew~G. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason Gabriel, William~S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem~W. Ayoub, Jeff Stanway, L.~L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint}, 2021.
\newblock \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.
\newblock \url{https://arxiv.org/abs/2305.18290}.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{c4}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{arXiv preprint}, 2019.
\newblock \url{https://arxiv.org/abs/1910.10683}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock In \emph{The Journal of Machine Learning Research (JMLR)}, 2020.
\newblock \url{https://arxiv.org/abs/1910.10683}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2016.
\newblock \url{https://aclanthology.org/D16-1264}.

\bibitem[Reddy et~al.(2019)Reddy, Chen, and Manning]{reddy-etal-2019-coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock {C}o{QA}: A conversational question answering challenge.
\newblock In \emph{Transactions of the Association for Computational Linguistics (TACL)}, 2019.
\newblock \url{https://aclanthology.org/Q19-1016}.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, , and Gordon]{copa}
Melissa Roemmele, Cosmin~Adrian Bejan, , and Andrew~S. Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
\newblock In \emph{Association for the Advancement of Artificial Intelligence (AAAI) Spring Symposium}, 2011.
\newblock \url{https://people.ict.usc.edu/~gordon/copa.html}.

\bibitem[Rosenfeld et~al.(2020)Rosenfeld, Rosenfeld, Belinkov, and Shavit]{rosenfeld_ConstructivePredictionGeneralization_2020}
Jonathan~S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2020.
\newblock \url{https://arxiv.org/abs/1909.12673}.

\bibitem[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and Van~Durme]{winogender}
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van~Durme.
\newblock Gender bias in coreference resolution.
\newblock In \emph{Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NAACL)}, 2018.
\newblock \url{https://aclanthology.org/N18-2002}.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{arXiv preprint}, 2019.
\newblock \url{https://arxiv.org/abs/1907.10641}.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{Sanh2019DistilBERTAD}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock \emph{arXiv preprint}, 2019.
\newblock \url{http://arxiv.org/abs/1910.01108}.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi]{siqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le~Bras, and Yejin Choi.
\newblock Social {IQ}a: Commonsense reasoning about social interactions.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)}, 2019.
\newblock \url{https://aclanthology.org/D19-1454}.

\bibitem[Sardana \& Frankle(2023)Sardana and Frankle]{Sardana2023Beyond}
Nikhil Sardana and Jonathan Frankle.
\newblock Beyond chinchilla-optimal: Accounting for inference in language model scaling laws.
\newblock In \emph{NeurIPS Workshop on Efficient Natural Language and Speech Processing (ENLSP)}, 2023.
\newblock \url{https://arxiv.org/abs/2401.00448}.

\bibitem[Scao et~al.(2022)Scao, Wang, Hesslow, Saulnier, Bekman, Bari, Biderman, Elsahar, Muennighoff, Phang, et~al.]{scao2022language}
Teven~Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M~Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et~al.
\newblock What language model to train if you have one million gpu hours?
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2022.
\newblock \url{https://aclanthology.org/2022.findings-emnlp.54}.

\bibitem[Schaeffer et~al.(2023)Schaeffer, Miranda, and Koyejo]{schaeffer2023emergent}
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.
\newblock Are emergent abilities of large language models a mirage?
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.
\newblock \url{https://arxiv.org/abs/2304.15004}.

\bibitem[Sharma \& Kaplan(2022)Sharma and Kaplan]{sharma_ScalingLawsData_2022}
Utkarsh Sharma and Jared Kaplan.
\newblock A neural scaling law from the dimension of the data manifold.
\newblock In \emph{Journal of Machine Learning Research (JMLR)}, 2022.
\newblock \url{https://arxiv.org/abs/2004.10802}.

\bibitem[Shazeer(2020)]{swiglu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint}, 2020.
\newblock \url{https://arxiv.org/abs/2002.05202}.

\bibitem[Singh et~al.(2024)Singh, Vargus, Dsouza, Karlsson, Mahendiran, Ko, Shandilya, Patel, Mataciunas, OMahony, et~al.]{singh2024aya}
Shivalika Singh, Freddie Vargus, Daniel Dsouza, B{\"o}rje~F Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et~al.
\newblock Aya dataset: An open-access collection for multilingual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2402.06619}, 2024.
\newblock \url{https://arxiv.org/abs/2402.06619}.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2402.00159}.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and Morcos]{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari~S. Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data pruning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.
\newblock \url{https://openreview.net/forum?id=UmvSlP-PyV}.

\bibitem[Su et~al.(2021)Su, Ahmed, Lu, Pan, Bo, and Liu]{rope}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv preprint}, 2021.
\newblock \url{https://arxiv.org/abs/2104.09864}.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant]{talmor-etal-2019-commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {C}ommonsense{QA}: A question answering challenge targeting commonsense knowledge.
\newblock In \emph{Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NAACL)}, 2019.
\newblock \url{https://aclanthology.org/N19-1421}.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Rao, Fedus, Abnar, Chung, Narang, Yogatama, Vaswani, and Metzler]{tay2021scale}
Yi~Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung~Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler.
\newblock Scale efficiently: Insights from pre-training and fine-tuning transformers.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.
\newblock \url{https://openreview.net/forum?id=f2OYVDyfIB}.

\bibitem[Tay et~al.(2023)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran, Yogatama, and Metzler]{tay-etal-2023-scaling}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Tran, Dani Yogatama, and Donald Metzler.
\newblock Scaling laws vs model architectures: How does inductive bias influence scaling?
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2023.
\newblock \url{https://aclanthology.org/2023.findings-emnlp.825}.

\bibitem[Team(2023)]{MosaicML2023Introducing}
MosaicML~NLP Team.
\newblock Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.
\newblock \url{www.mosaicml.com/blog/mpt-7b}.

\bibitem[Thoppilan et~al.(2022)Thoppilan, Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang, Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhao, Zhou, Chang, Krivokon, Rusch, Pickett, Srinivasan, Man, Meier-Hellstern, Morris, Doshi, Santos, Duke, Soraker, Zevenbergen, Prabhakaran, Diaz, Hutchinson, Olson, Molina, Hoffman-John, Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton, Cohen, Bernstein, Kurzweil, Aguera-Arcas, Cui, Croak, Chi, and Le]{thoppilan2022lamda}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du, YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint}, 2022.
\newblock \url{https://arxiv.org/abs/2201.08239}.

\bibitem[{Together Computer}(2023)]{rpj}
{Together Computer}.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock {{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}.
\newblock \emph{arXiv preprint}, 2023{\natexlab{a}}.
\newblock \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}.
\newblock \emph{arXiv preprint}, 2023{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/2307.09288}.

\bibitem[{\"U}st{\"u}n et~al.(2024){\"U}st{\"u}n, Aryabumi, Yong, Ko, D'souza, Onilude, Bhandari, Singh, Ooi, Kayid, et~al.]{ustun2024aya}
Ahmet {\"U}st{\"u}n, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et~al.
\newblock Aya model: An instruction finetuned open-access multilingual language model.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2402.07827}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.
\newblock \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy, Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett, Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng, Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris, Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0 Contributors}]{scipy}
Pauli Virtanen, Ralf Gommers, Travis~E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St{\'e}fan~J. {van der Walt}, Matthew Brett, Joshua Wilson, K.~Jarrod Millman, Nikolay Mayorov, Andrew R.~J. Nelson, Eric Jones, Robert Kern, Eric Larson, C~J Carey, {\.I}lhan Polat, Yu~Feng, Eric~W. Moore, Jake {VanderPlas}, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E.~A. Quintero, Charles~R. Harris, Anne~M. Archibald, Ant{\^o}nio~H. Ribeiro, Fabian Pedregosa, Paul {van Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}.
\newblock \emph{Nature Methods}, 2020.
\newblock \url{https://rdcu.be/b08Wh}.

\bibitem[Wang et~al.(2021)Wang, Liu, Zhong, Zhou, Wei, Chen, and Duan]{Wang2021FromLT}
Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and Nan Duan.
\newblock From lsat: The progress and challenges of complex reasoning.
\newblock \emph{Transactions on Audio, Speech, and Language Processing}, 2021.
\newblock \url{https://arxiv.org/abs/2108.00648}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022{\natexlab{a}}.
\newblock \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
\newblock Emergent abilities of large language models.
\newblock In \emph{Transactions on Machine Learning Research (TMLR)}, 2022{\natexlab{b}}.
\newblock \url{https://openreview.net/forum?id=yzkSU5zdwD}.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang, Cheng, Glaese, Balle, Kasirzadeh, et~al.]{weidinger2021ethical}
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et~al.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{arXiv preprint}, 2021.
\newblock \url{https://arxiv.org/abs/2112.04359}.

\bibitem[Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, et~al.]{workshop2022bloom}
BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint}, 2022.
\newblock \url{https://arxiv.org/abs/2211.05100}.

\bibitem[Wortsman et~al.(2023)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et~al.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2309.14322}.

\bibitem[Yang et~al.(2021)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Greg Yang, Edward~J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs {V}: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock In \emph{Advances in Neural Information Processing Systems (NeuIPS)}, 2021.
\newblock \url{https://arxiv.org/abs/2203.03466}.

\bibitem[Yang et~al.(2024)Yang, Yu, Zhu, and Hayou]{yang2023tensor}
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.
\newblock Feature learning in infinite depth neural networks.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2024.
\newblock \url{https://openreview.net/forum?id=17pVDnpwwl}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2019.
\newblock \url{https://aclanthology.org/P19-1472}.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer]{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.
\newblock \url{https://arxiv.org/abs/2106.04560}.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{Zhang2019RootMS}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock In \emph{Advances in Neural Information Processing Systems (NeuIPS)}, 2019.
\newblock \url{https://arxiv.org/abs/1910.07467}.

\bibitem[Zhang et~al.(2019)Zhang, Titov, and Sennrich]{zhang-etal-2019-improving}
Biao Zhang, Ivan Titov, and Rico Sennrich.
\newblock Improving deep transformer with depth-scaled initialization and merged attention.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)}, 2019.
\newblock \url{https://aclanthology.org/D19-1083}.

\bibitem[Zhao et~al.(2023)Zhao, Gu, Varma, Luo, chin Huang, Xu, Wright, Shojanazeri, Ott, Shleifer, Desmaison, Balioglu, Nguyen, Chauhan, Hao, and Li]{Zhao2023PyTorchFE}
Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li.
\newblock Pytorch fsdp: Experiences on scaling fully sharded data parallel.
\newblock In \emph{Very Large Data Bases Conference (VLDB)}, 2023.
\newblock \url{https://dl.acm.org/doi/10.14778/3611540.3611569}.

\bibitem[Zhong et~al.(2020)Zhong, Xiao, Tu, Zhang, Liu, and Sun]{zhong2019jec}
Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun.
\newblock Jec-qa: A legal-domain question answering dataset.
\newblock In \emph{Association for the Advancement of Artificial Intelligence (AAAI)}, 2020.
\newblock \url{https://arxiv.org/abs/1911.12011}.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: A human-centric benchmark for evaluating foundation models.
\newblock \emph{arXiv preprint}, 2023.
\newblock \url{https://arxiv.org/abs/2304.06364}.

\bibitem[Zhuo et~al.(2024)Zhuo, Zebaze, Suppattarachai, von Werra, de~Vries, Liu, and Muennighoff]{zhuo2024astraios}
Terry~Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de~Vries, Qian Liu, and Niklas Muennighoff.
\newblock Astraios: Parameter-efficient instruction tuning code large language models.
\newblock \emph{arXiv preprint}, 2024.
\newblock \url{https://arxiv.org/abs/2401.00788}.

\end{thebibliography}
