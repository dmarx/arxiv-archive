\begin{table}[t!]
    \centering
    \small
    \caption{\textbf{Token multipliers of existing models.} In our work, we run experiments with token multipliers between 5 and 640 for \{GPT-2~\cite{Radford2019LanguageMA}, LLaMA~\cite{llama}\}-style decoder-only architectures.}
    % \vspace*{2mm}
    \begin{tabular}{lp{1em}rlp{1.5em}rlp{2em}rl}
        \toprule
         Model family & \multicolumn{3}{c}{Parameters $N$} & \multicolumn{3}{c}{Training tokens $D$} & \multicolumn{3}{c}{Token multiplier $M$} \\
         \midrule
         T5~\cite{raffel2020exploring} & & 11B  & & & 34B  & & & 3.1\\
         GPT-3~\cite{gpt3}             & & 175B & & & 300B & & & 1.7\\
         Gopher~\cite{gopher}          & & 280B & & & 300B & & & 1.1\\
         Chinchilla~\cite{chinchilla}  & & 70B  & & & 1.4T & & & 20{.0}\\
         LLaMA~\cite{llama}            & & 7B   & & & 1T   & & & 140{.0}\\
         LLaMA~\cite{llama}            & & 70B  & & & 1.4T & & & 20{.0} \\
         LLaMA-2~\cite{llama2}         & & 7B   & & & 2T   & & & 290{.0} \\
         LLaMA-2~\cite{llama2}         & & 70B  & & & 2T   & & & 30{.0} \\
         XGen~\cite{XGen}              & & 7B   & & & 1.5T & & & 210{.0}\\
         MPT~\cite{MosaicML2023Introducing} & & 7B & & & 1T & & & 140{.0} \\\bottomrule
    \end{tabular}
    \label{tab:other_mults}
\end{table}