\begin{table}[t!]
\centering
\scriptsize
\caption{\textbf{46 downstream tasks.} All downstream tasks considered in this work, evaluated via LLM-foundry~\cite{mosaicml}.
For more information on each dataset and specifics about the LLM-foundry category and evaluation type, please see: \url{https://www.mosaicml.com/llm-evaluation}.
}
\begin{tabular}{lllcrc}
\toprule
Downstream task & LLM-foundry category & Evaluation type & Shots & Samples & Baseline\\\midrule
AGIEval LSAT AR~\cite{zhong2023agieval,zhong2019jec,Wang2021FromLT} & symbolic problem solving & multiple choice & 3 & 230 & 0.25\\
AGIEval LSAT LR~\cite{zhong2023agieval,zhong2019jec,Wang2021FromLT} & reading comprehension & multiple choice & 3 & 510 & 0.25\\
AGIEval LSAT RC~\cite{zhong2023agieval,zhong2019jec,Wang2021FromLT} & reading comprehension & multiple choice & 3 & 268 & 0.25\\
AGIEval SAT English~\cite{zhong2023agieval} & reading comprehension & multiple choice & 3 & 206 & 0.25\\
ARC-Challenge~\cite{arc} & world knowledge & multiple choice & 10 & 2376 & 0.25\\
ARC-Easy~\cite{arc} & world knowledge & multiple choice & 10 & 2376 & 0.25\\
BBQ~\cite{bbq} & safety & multiple choice & 3 & 58492 & 0.50\\
BIG-bench: CS algorithms~\cite{srivastava2023beyond} & symbolic problem solving & language modeling & 10 & 1320 & 0.00\\
BIG-bench: Conceptual combinations~\cite{srivastava2023beyond} & language understanding & multiple choice & 10 & 103 & 0.25\\
BIG-bench: Conlang translation~\cite{srivastava2023beyond} & language understanding & language modeling & 0 & 164 & 0.00\\
BIG-bench: Dyck languages~\cite{srivastava2023beyond} & symbolic problem solving & language modeling & 10 & 1000 & 0.00\\
BIG-bench: Elementary math QA~\cite{srivastava2023beyond} & symbolic problem solving & multiple choice & 10 & 38160 & 0.25\\
BIG-bench: Language identification~\cite{srivastava2023beyond} & language understanding & multiple choice & 10 & 10000 & 0.25\\
BIG-bench: Logical deduction~\cite{srivastava2023beyond} & symbolic problem solving & multiple choice & 10 & 1500 & 0.25\\
BIG-bench: Misconceptions~\cite{srivastava2023beyond} & world knowledge & multiple choice & 10 & 219 & 0.50\\
BIG-bench: Novel Concepts~\cite{srivastava2023beyond} & commonsense reasoning & multiple choice & 10 & 32 & 0.25\\
BIG-bench: Operators~\cite{srivastava2023beyond} & symbolic problem solving & language modeling & 10 & 210 & 0.00\\
BIG-bench: QA WikiData~\cite{srivastava2023beyond} & world knowledge & language modeling & 10 & 20321 & 0.00\\
BIG-bench: Repeat copy logic~\cite{srivastava2023beyond} & symbolic problem solving & language modeling & 10 & 32 & 0.00\\
BIG-bench: Strange stories~\cite{srivastava2023beyond} & commonsense reasoning & multiple choice & 10 & 174 & 0.50\\
BIG-bench: Strategy QA~\cite{srivastava2023beyond} & commonsense reasoning & multiple choice & 10 & 2289 & 0.50\\
BIG-bench: Understanding fables~\cite{srivastava2023beyond} & reading comprehension & multiple choice & 10 & 189 & 0.25\\
BoolQ~\cite{boolq} & reading comprehension & multiple choice & 10 & 3270 & 0.50\\
COPA~\cite{copa} & commonsense reasoning & multiple choice & 0 & 100 & 0.50\\
CoQA~\cite{reddy-etal-2019-coqa} & reading comprehension & language modeling & 0 & 7983 & 0.00\\
Commonsense QA~\cite{talmor-etal-2019-commonsenseqa} & commonsense reasoning & multiple choice & 10 & 1221 & 0.25\\
Enterprise PII classification~\cite{pii} & safety & multiple choice & 10 & 3395 & 0.50\\
HellaSwag (10-shot)~\cite{hellaswag} & language understanding & multiple choice & 10 & 10042 & 0.25\\
HellaSwag (zero-shot)~\cite{hellaswag} & language understanding & multiple choice & 0 & 10042 & 0.25\\
Jeopardy~\cite{mosaicml} & world knowledge & language modeling & 10 & 2117 & 0.00\\
LAMBADA~\cite{lambada} & language understanding & language modeling & 0 & 5153 & 0.00\\
LogiQA~\cite{Liu2020LogiQAAC} & symbolic problem solving & multiple choice & 10 & 651 & 0.25\\
MMLU (5-shot)~\cite{mmlu} & world knowledge & multiple choice & 5 & 14042 & 0.25\\
MMLU (zero-shot)~\cite{mmlu} & world knowledge & multiple choice & 0 & 14042 & 0.25\\
MathQA~\cite{mathqa} & symbolic problem solving & multiple choice & 10 & 2983 & 0.25\\
OpenBook QA~\cite{OpenBookQA2018} & commonsense reasoning & multiple choice & 0 & 500 & 0.25\\
PIQA~\cite{piqa} & commonsense reasoning & multiple choice & 10 & 1838 & 0.50\\
PubMed QA Labeled~\cite{pubmed} & reading comprehension & language modeling & 10 & 1000 & 0.00\\
SIQA~\cite{siqa} & commonsense reasoning & multiple choice & 10 & 1954 & 0.50\\
SQuAD~\cite{squad} & reading comprehension & language modeling & 10 & 10570 & 0.00\\
Simple Arithmetic: NoSpaces~\cite{mosaicml} & symbolic problem solving & language modeling & 10 & 1000 & 0.00\\
Simple Arithmetic: WithSpaces~\cite{mosaicml} & symbolic problem solving & language modeling & 10 & 1000 & 0.00\\
WinoGender MC: Female~\cite{winogender} & safety & multiple choice & 10 & 60 & 0.50\\
WinoGender MC: Male~\cite{winogender} & safety & multiple choice & 10 & 60 & 0.50\\
WinoGrande~\cite{sakaguchi2019winogrande} & language understanding & schema & 0 & 1267 & 0.50\\
WinoGrand~\cite{winograd} & language understanding & schema & 0 & 273 & 0.50\\
\bottomrule
\end{tabular}
\label{tab:eval_info}
\end{table}