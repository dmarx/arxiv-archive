\begin{table*}[tp]
    \centering
    \small
    \caption{\textbf{Main models and hyperparameters used in our investigation.} Models have number of parameters $N$, with number of layers $n_{\text{layers}}$, number of attention heads $n_{\text{heads}}$, model width $d_{\text{model}}$, and width per attention head $d_{\text{head}}$. Batch sizes are global and in units of sequences. Each sequence has 2,048 tokens.
    A100 GPU hours are at $M=20$, which are near compute-optimal runs.
    For the 1.4B scale, a batch size of 256 performs slightly better than 512.
    }    
    \begin{tabular}{rcccccccc}
        \toprule
        $N$ & $n_{\text{layers}}$ & $n_{\text{heads}}$ & $d_{\text{model}}$ & $d_{\text{head}}$ & Warmup & Learning rate & Batch size & $M=20$ A100 hours\\\midrule
        0.011B & 8 & 4 & 96 & 24 & 100 & 3$e$-3 & 64 & 0.3 \\
        0.079B & 8 & 4 & 512 & 128 & 400 & 3$e$-3 & 512 & 5 \\    
        0.154B & 24 & 8 & 576 & 72 & 400 & 3$e$-3 & 512 & 12 \\     
        0.411B & 24 & 8 & 1,024 & 128 & 2,000 & 3$e$-3 & 512 & 75\\    
        1.4B & 24 & 16 & 2,048 & 128 & 5,000 & 3$e$-3 & 256 & 690\\
        6.9B & 32 & 32 & 4,096 & 128 & 5,000 & 3$e$-4 & 2,048 & 17,000 \\\bottomrule
    \end{tabular}    
    \label{tab:hparams}
\end{table*}