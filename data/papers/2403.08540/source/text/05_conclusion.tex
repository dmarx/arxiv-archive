\vspace*{-2mm}
\section{Limitations, future work, and conclusion}
\label{sec:conclusion}

\paragraph{Limitations and future work.}
We identify limitations, which provide motivation for future work.

\begin{itemize}[leftmargin=5mm]
    \vspace*{-1mm}
    \item \textbf{Hyperparameters.}
    While our configurations are surprisingly amenable to reliable scaling across many training and testing distributions without further tuning, there is a need to develop scaling laws that do not require extensive hyperparameter sweeps.
    \vspace*{-1mm}
    \item \textbf{Scaling up.} Validating the trends in this paper for even larger runs is a valuable direction. Additionally, repeating our setup for models that achieve non-trivial performance on harder evaluations like MMLU is left to future work.
    \vspace*{-1mm}
    \item \textbf{Scaling down.} Actualizing predictable scaling with even cheaper runs is important to make this area of research more accessible, especially for downstream error prediction.
    \vspace*{-1mm}
    \item \textbf{Failure cases.} While we present a preliminary analysis of when scaling is unreliable, future work should investigate conditions under which scaling breaks down.
    \vspace*{-1mm}
    \item \textbf{Post-training.} It is common to employ fine-tuning interventions after pre-training, which we do not consider.
    Quantifying to what degree over-training the base model provides benefits \emph{after} post-training is an open area of research.
    \vspace*{-1mm}
    \item \textbf{Individual downstream task prediction.}
    While we find that averaging over many task error metrics can make for a predictable metric, per-task predictions are left to future work.
    \vspace*{-1mm}
    \item  \textbf{In-the-wild performance.}
    Downstream task performance is a proxy for the in-the-wild user experience.
    Analyzing scaling trends in the context of this experience is timely.
    \vspace*{-1mm}
    \item \textbf{Dataset curation.} Our work only deals with existing training datasets.
    Exploring dataset curation for improved model scaling is another promising direction.
    \vspace*{-1mm}
\end{itemize}

\paragraph{Conclusion.}
We show that the loss of over-trained models, trained past compute-optimality, is predictable. Furthermore, we propose and validate a scaling law relating loss to average downstream task performance.
We hope our work will inspire others to further examine the relationship between model training and downstream generalization.
Our testbed will be made publicly available, and we hope it will make scaling research more accessible to researchers and practitioners alike.

\section*{Acknowledgements}
SYG is supported by an NSF Graduate Research Fellowship,
GS by the Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023, and MN by the Federal Ministry of Education and Research of Germany under grant no. 01IS22094B WEST-AI.
We thank Stability AI and Toyota Research Institute (TRI) for access to compute resources.
This research has been supported by NSF Grants AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, and research gifts by Western Digital, Amazon, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, and the Stanly P. Finch Centennial Professorship in Engineering.
We also thank Kushal Arora, Alper Canberk, Mia Chiquier, Sachit Menon, Chuer Pan, Purva Tendulkar, and Mandi Zhao for valuable feedback.