% \vspace*{-2mm}
\section{Related work}
\label{sec:related}
% \vspace*{-2mm}
We review the most closely related work in this section.
For additional related work, see Appendix~\ref{appx:added_related}.
% \vspace*{-2mm}

\paragraph{Scaling laws.}
Early works on scaling artificial neural networks observe predictable power-law scaling in the training set size and number of model parameters~\cite{og_scaling,hestness2019beyond,rosenfeld_ConstructivePredictionGeneralization_2020}. 
\citet{alabdulmohsin2022revisiting} stress the importance of looking at the extrapolation regime of a scaling law.
\citet{yang2022tensor} prescribe architectural and hyperparameter changes when scaling model width to realize performant models; \citet{yang2023tensor} make analogous recommendations when scaling model depth.
\citet{Bi2024DeepSeekLS} propose hyperparameter aware scaling laws.
Unlike the aforementioned work, our investigation focuses on over-training and predicting downstream accuracy.

\citet{chinchilla} investigate how the number of model parameters $N$ and training tokens $D$ should be chosen to minimize loss $L$ given a compute budget $C$.
\citet{chinchilla} find that when scaling up $C$, both $N$ and $D$ should be scaled equally up to a multiplicative constant (i.e., $N \propto C^{\sim 0.5}$ and $D \propto C^{\sim 0.5}$) to realize compute-optimality.
Appendix C of the Chinchilla paper additionally suggests that these findings hold across three datasets.
However, \citet{chinchilla} do not verify their scaling laws for training beyond compute-optimality, or for downstream error prediction---both of which are central to our work. 

\citet{Sardana2023Beyond} propose modifications to the Chinchilla formulation to incorporate inference costs into the definition of compute-optimality and solve for various fixed inference budgets.
Their key finding, which is critical for our work, is that when taking into account a large enough inference budget, it is optimal to train smaller models for longer than the original Chinchilla recommendations.
Our work presupposes that over-training can be beneficial.
Instead of solving for inference-optimal schemes, we support empirically a predictive theory of scaling in the over-trained regime.
Additionally, we provide experiments across many validation and training sets.

For predicting downstream scaling beyond loss,~\citet{Isik2024ScalingLF} relate the number of pre-training tokens to downstream cross-entropy and machine translation BLEU score~\cite{papineni-etal-2002-bleu} after fine-tuning. 
In contrast, we take a holistic approach to evaluation by looking at top-1 error over many natural language tasks. 
\citet{schaeffer2023emergent} argue that emergent abilities~\cite{wei2022emergent} are a product of non-linear metrics and propose smoother alternatives.
As a warmup for why non-linear metrics may be hard to predict, \citet{schaeffer2023emergent} consider predicting an $\ell$ length sequence exactly: $\textsf{Err}(N, \ell) \approx 1 - \textsf{PP}(N)^{-\ell}$, where $N$ is the number of parameters in a model and $\textsf{PP}$ is its perplexity.
This is a special case of our Equations~\eqref{eq:errL} and~\eqref{eq:errPP}, where the number of training tokens does not appear, $\epsilon = 1, k = 1$, and $\gamma = \ell$.
In contrast, we treat $\epsilon, k, \gamma$ as free parameters for a scaling law fit, finding that average error over downstream tasks can make for a predictable metric.

\paragraph{Over-training in popular models.} There has been a rise in over-trained models~\cite{llama,llama2} and accompanying massive datasets~\cite{rpj,refinedweb,soldaini2024dolma,albalak2024survey}. For example, Chinchilla 70B~\cite{chinchilla} is trained with a token multiplier of 20, while LLaMA-2 7B~\cite{llama2} uses a token multiplier of 290.
In our investigation, we look at token multipliers from 5 to 640 to ensure coverage of popular models and relevance for future models that may be trained on even more tokens.