\begin{thebibliography}{84}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.
\newblock \href {https://arxiv.org/pdf/2303.08774} {Gpt-4 technical report}.
\newblock \emph{arXiv preprint arXiv:2303.08774}.

\bibitem[{Alchourrón et~al.(1985)Alchourrón, Gärdenfors, and Makinson}]{alchourron1985logic}
Carlos~E. Alchourrón, Peter Gärdenfors, and David Makinson. 1985.
\newblock \href {https://fitelson.org/piksi/agm.pdf} {On the logic of theory change: Partial meet contraction and revision functions}.
\newblock \emph{The Journal of Symbolic Logic}, 50(2):510--530.

\bibitem[{AlKhamissi et~al.(2022)AlKhamissi, Li, Celikyilmaz, Diab, and Ghazvininejad}]{alkhamissi2022review}
Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. 2022.
\newblock \href {https://arxiv.org/pdf/2204.06031} {A review on language models as knowledge bases}.
\newblock \emph{arXiv preprint arXiv:2204.06031}.

\bibitem[{Andreas(2022)}]{andreas2022language}
Jacob Andreas. 2022.
\newblock \href {https://arxiv.org/pdf/2212.01681.pdf} {Language models as agent models}.
\newblock \emph{arXiv preprint arXiv:2212.01681}.

\bibitem[{Betz and Richardson(2023)}]{betz2023probabilistic}
Gregor Betz and Kyle Richardson. 2023.
\newblock \href {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281372} {Probabilistic coherence, logical consistency, and bayesian learning: Neural language models as epistemic agents}.
\newblock \emph{Plos one}, 18(2):e0281372.

\bibitem[{Brown et~al.(2023)Brown, Godfrey, Nizinski, Tu, and Kvinge}]{brown2023edit}
Davis Brown, Charles Godfrey, Cody Nizinski, Jonathan Tu, and Henry Kvinge. 2023.
\newblock \href {https://arxiv.org/pdf/2303.00046} {Edit at your own risk: evaluating the robustness of edited models to distribution shifts}.
\newblock \emph{arXiv preprint arXiv:2303.00046}.

\bibitem[{Casper et~al.(2023{\natexlab{a}})Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire et~al.}]{casper2023open}
Stephen Casper, Xander Davies, Claudia Shi, Thomas~Krendl Gilbert, J{\'e}r{\'e}my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et~al. 2023{\natexlab{a}}.
\newblock \href {https://arxiv.org/pdf/2307.15217.pdf} {Open problems and fundamental limitations of reinforcement learning from human feedback}.
\newblock \emph{arXiv preprint arXiv:2307.15217}.

\bibitem[{Casper et~al.(2023{\natexlab{b}})Casper, Lin, Kwon, Culp, and Hadfield-Menell}]{casper2023explore}
Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2306.09442} {Explore, establish, exploit: Red teaming language models from scratch}.

\bibitem[{Chen et~al.(2023)Chen, Zhong, Ri, Zhao, He, Steinhardt, Yu, and McKeown}]{chen2023models}
Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He~He, Jacob Steinhardt, Zhou Yu, and Kathleen McKeown. 2023.
\newblock \href {https://arxiv.org/pdf/2307.08678.pdf} {Do models explain themselves? counterfactual simulatability of natural language explanations}.
\newblock \emph{arXiv preprint arXiv:2307.08678}.

\bibitem[{Cohen et~al.(2024)Cohen, Biran, Yoran, Globerson, and Geva}]{cohen2024evaluating}
Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. 2024.
\newblock \href {https://arxiv.org/pdf/2307.12976} {Evaluating the ripple effects of knowledge editing in language models}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:283--298.

\bibitem[{Dai et~al.(2021)Dai, Dong, Hao, Sui, and Wei}]{dai2021knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2021.
\newblock \href {https://arxiv.org/pdf/2104.08696.pdf} {Knowledge neurons in pretrained transformers}.

\bibitem[{De~Cao et~al.(2021)De~Cao, Aziz, and Titov}]{de2021editing}
Nicola De~Cao, Wilker Aziz, and Ivan Titov. 2021.
\newblock \href {https://aclanthology.org/2021.emnlp-main.522} {Editing factual knowledge in language models}.
\newblock In \emph{EMNLP}, pages 6491--6506. Association for Computational Linguistics.

\bibitem[{Debenedetti et~al.(2023)Debenedetti, Severi, Carlini, Choquette-Choo, Jagielski, Nasr, Wallace, and Tramèr}]{debenedetti2023privacy}
Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher~A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, and Florian Tramèr. 2023.
\newblock \href {http://arxiv.org/abs/2309.05610} {Privacy side channels in machine learning systems}.

\bibitem[{Dhingra et~al.(2022)Dhingra, Cole, Eisenschlos, Gillick, Eisenstein, and Cohen}]{dhingra2022time}
Bhuwan Dhingra, Jeremy~R. Cole, Julian~Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William~W. Cohen. 2022.
\newblock \href {https://doi.org/10.1162/tacl_a_00459} {{Time-Aware Language Models as Temporal Knowledge Bases}}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:257--273.

\bibitem[{Du et~al.(2024)Du, Sn{\ae}bjarnarson, Stoehr, White, Schein, and Cotterell}]{du2024context}
Kevin Du, V{\'e}steinn Sn{\ae}bjarnarson, Niklas Stoehr, Jennifer~C White, Aaron Schein, and Ryan Cotterell. 2024.
\newblock \href {https://arxiv.org/pdf/2404.04633} {Context versus prior knowledge in language models}.
\newblock \emph{arXiv preprint arXiv:2404.04633}.

\bibitem[{Farquhar et~al.(2023)Farquhar, Varma, Kenton, Gasteiger, Mikulik, and Shah}]{farquhar2023challenges}
Sebastian Farquhar, Vikrant Varma, Zachary Kenton, Johannes Gasteiger, Vladimir Mikulik, and Rohin Shah. 2023.
\newblock \href {https://arxiv.org/pdf/2312.10029} {Challenges with unsupervised llm knowledge discovery}.
\newblock \emph{arXiv preprint arXiv:2312.10029}.

\bibitem[{Fierro et~al.(2024)Fierro, Garneau, Bugliarello, Kementchedjhieva, and S{\o}gaard}]{fierro2024mulan}
Constanza Fierro, Nicolas Garneau, Emanuele Bugliarello, Yova Kementchedjhieva, and Anders S{\o}gaard. 2024.
\newblock \href {https://arxiv.org/pdf/2404.03036} {Mulan: A study of fact mutability in language models}.
\newblock \emph{arXiv preprint arXiv:2404.03036}.

\bibitem[{Fitelson and Hawthorne(2010)}]{fitelson2010bayesian}
Branden Fitelson and James Hawthorne. 2010.
\newblock \href {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=10a4b8ec84506cb70b13cd6e8cb500e14048ceea} {\emph{How Bayesian confirmation theory handles the paradox of the ravens}}.
\newblock Springer.

\bibitem[{Gangadhar and Stratos(2024)}]{gangadhar2024model}
Govind Gangadhar and Karl Stratos. 2024.
\newblock \href {https://arxiv.org/pdf/2402.11078v3} {Model editing by pure fine-tuning}.
\newblock \emph{arXiv preprint arXiv:2402.11078}.

\bibitem[{Green(2021)}]{sep-speech-acts}
Mitchell Green. 2021.
\newblock \href {https://plato.stanford.edu/archives/win2020/entries/speech-acts/#SpeActSco} {{Speech Acts}}.
\newblock In Edward~N. Zalta, editor, \emph{The {Stanford} Encyclopedia of Philosophy}, {F}all 2021 edition. Metaphysics Research Lab, Stanford University.

\bibitem[{Gude(2023)}]{gude2023factors}
Vinayaka Gude. 2023.
\newblock \href {https://www.tandfonline.com/doi/abs/10.1080/08874417.2023.2280918} {Factors influencing chatgpt adoption for product research and information retrieval}.
\newblock \emph{Journal of Computer Information Systems}, pages 1--10.

\bibitem[{Gupta et~al.(2023)Gupta, Mondal, Sheshadri, Zhao, Li, Wiegreffe, and Tandon}]{gupta2023editing}
Anshita Gupta, Debanjan Mondal, Akshay Sheshadri, Wenlong Zhao, Xiang Li, Sarah Wiegreffe, and Niket Tandon. 2023.
\newblock \href {https://arxiv.org/pdf/2305.14956} {Editing common sense in transformers}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 8214--8232.

\bibitem[{Han et~al.(2023)Han, Li, Li, and Pan}]{han2023divide}
Xiaoqi Han, Ru~Li, Xiaoli Li, and Jeff~Z Pan. 2023.
\newblock \href {https://www.sciencedirect.com/science/article/abs/pii/S0950705123005762} {A divide and conquer framework for knowledge editing}.
\newblock \emph{Knowledge-Based Systems}, 279:110826.

\bibitem[{Hansson(2022)}]{sep-logic-belief-revision}
Sven~Ove Hansson. 2022.
\newblock \href {https://plato.stanford.edu/archives/spr2022/entries/logic-belief-revision/} {{Logic of Belief Revision}}.
\newblock In Edward~N. Zalta, editor, \emph{The {Stanford} Encyclopedia of Philosophy}, {S}pring 2022 edition. Metaphysics Research Lab, Stanford University.

\bibitem[{Hartvigsen et~al.(2023)Hartvigsen, Sankaranarayanan, Palangi, Kim, and Ghassemi}]{hartvigsen2023aging}
Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. 2023.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2023/file/95b6e2ff961580e03c0a662a63a71812-Paper-Conference.pdf} {Aging with grace: Lifelong model editing with discrete key-value adaptors}.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Hase et~al.(2021)Hase, Diab, Celikyilmaz, Li, Kozareva, Stoyanov, Bansal, and Iyer}]{hase2021language}
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021.
\newblock \href {https://arxiv.org/pdf/2111.13654.pdf} {Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs}.
\newblock \emph{arXiv preprint arXiv:2111.13654}.

\bibitem[{Hempel(1945)}]{hempel1945studies}
Carl~G Hempel. 1945.
\newblock Studies in the logic of confirmation (i.).
\newblock \emph{Mind}, 54(213):1--26.

\bibitem[{Henderson et~al.(2023)Henderson, Mitchell, Manning, Jurafsky, and Finn}]{henderson2023self}
Peter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn. 2023.
\newblock \href {https://dl.acm.org/doi/pdf/10.1145/3600211.3604690?casa_token=1auWpvvbXlQAAAAA:rAle3U5NfZs8apFeILketoIn8Q4M0SQ23P44AqGAM01lNdq30m2GkRUoNT-0NpKWL_3mjtmTnCM_UwU} {Self-destructing models: Increasing the costs of harmful dual uses of foundation models}.
\newblock In \emph{Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, pages 287--296.

\bibitem[{Hernandez et~al.(2023)Hernandez, Li, and Andreas}]{hernandez2023inspecting}
Evan Hernandez, Belinda~Z Li, and Jacob Andreas. 2023.
\newblock \href {https://arxiv.org/pdf/2304.00740} {Inspecting and editing knowledge representations in language models}.
\newblock \emph{arXiv preprint arXiv:2304.00740}.

\bibitem[{Hoelscher-Obermaier et~al.(2023)Hoelscher-Obermaier, Persson, Kran, Konstas, and Barez}]{hoelscher2023detecting}
Jason Hoelscher-Obermaier, Julia Persson, Esben Kran, Ioannis Konstas, and Fazl Barez. 2023.
\newblock \href {https://arxiv.org/pdf/2305.17553} {Detecting edit failures in large language models: An improved specificity benchmark}.
\newblock \emph{arXiv preprint arXiv:2305.17553}.

\bibitem[{Hofweber et~al.(2024)Hofweber, Hase, Stengel-Eskin, and Bansal}]{hofweber2024language}
Thomas Hofweber, Peter Hase, Elias Stengel-Eskin, and Mohit Bansal. 2024.
\newblock \href {http://arxiv.org/abs/2406.03442} {Are language models rational? the case of coherence norms and belief revision}.

\bibitem[{Hua et~al.(2024)Hua, Guo, Dong, Zhu, Ng, and Wang}]{hua2024propagation}
Wenyue Hua, Jiang Guo, Mingwen Dong, Henghui Zhu, Patrick Ng, and Zhiguo Wang. 2024.
\newblock \href {https://arxiv.org/pdf/2401.17585} {Propagation and pitfalls: Reasoning-based assessment of knowledge editing through counterfactual tasks}.
\newblock \emph{arXiv preprint arXiv:2401.17585}.

\bibitem[{Jiang et~al.(2020)Jiang, Xu, Araki, and Neubig}]{jiang2020can}
Zhengbao Jiang, Frank~F Xu, Jun Araki, and Graham Neubig. 2020.
\newblock \href {https://arxiv.org/pdf/2212.14315.pdf} {How can we know what language models know?}
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:423--438.

\bibitem[{Joshi et~al.(2024)Joshi, Rando, Saparov, Kim, and He}]{joshi2024personas}
Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, and He~He. 2024.
\newblock \href {http://arxiv.org/abs/2310.18168} {Personas as a way to model truthfulness in language models}.

\bibitem[{Krasheninnikov et~al.(2024)Krasheninnikov, Krasheninnikov, Mlodozeniec, Maharaj, and Krueger}]{krasheninnikov2024implicit}
Dmitrii Krasheninnikov, Egor Krasheninnikov, Bruno Mlodozeniec, Tegan Maharaj, and David Krueger. 2024.
\newblock \href {https://arxiv.org/pdf/2310.15047} {Implicit meta-learning may lead language models to trust more reliable sources}.

\bibitem[{Lazaridou et~al.(2021)Lazaridou, Kuncoro, Gribovskaya, Agrawal, Liska, Terzi, Gimenez, de~Masson~d'Autume, Kocisky, Ruder et~al.}]{lazaridou2021mind}
Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de~Masson~d'Autume, Tomas Kocisky, Sebastian Ruder, et~al. 2021.
\newblock \href {https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf} {Mind the gap: Assessing temporal generalization in neural language models}.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:29348--29363.

\bibitem[{Levy et~al.(2017)Levy, Seo, Choi, and Zettlemoyer}]{levy-etal-2017-zero}
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017.
\newblock \href {https://doi.org/10.18653/v1/K17-1034} {Zero-shot relation extraction via reading comprehension}.
\newblock In \emph{Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)}, pages 333--342, Vancouver, Canada. Association for Computational Linguistics.

\bibitem[{Lewis(1979)}]{lewis1979counterfactual}
David Lewis. 1979.
\newblock Counterfactual dependence and time's arrow.
\newblock \emph{No{\^u}s}, pages 455--476.

\bibitem[{Li et~al.(2024)Li, Pan, Gopal, Yue, Berrios, Gatti, Li, Dombrowski, Goel, Phan et~al.}]{li2024wmdp}
Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin~D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et~al. 2024.
\newblock \href {https://arxiv.org/pdf/2403.03218} {The wmdp benchmark: Measuring and reducing malicious use with unlearning}.
\newblock \emph{arXiv preprint arXiv:2403.03218}.

\bibitem[{Li et~al.(2023)Li, Zhang, Yao, Wang, Chen, and Chen}]{li2023unveiling}
Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi~Chen, and Huajun Chen. 2023.
\newblock \href {https://arxiv.org/pdf/2310.02129} {Unveiling the pitfalls of knowledge editing for large language models}.
\newblock \emph{arXiv preprint arXiv:2310.02129}.

\bibitem[{Lin(2024)}]{sep-epistemology-bayesian}
Hanti Lin. 2024.
\newblock \href {https://plato.stanford.edu/entries/epistemology-bayesian/} {{Bayesian Epistemology}}.
\newblock In Edward~N. Zalta and Uri Nodelman, editors, \emph{The {Stanford} Encyclopedia of Philosophy}, {S}ummer 2024 edition. Metaphysics Research Lab, Stanford University.

\bibitem[{Liu et~al.(2023)Liu, Wu, Michael, Suhr, West, Koller, Swayamdipta, Smith, and Choi}]{liu2023we}
Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah~A Smith, and Yejin Choi. 2023.
\newblock \href {https://arxiv.org/pdf/2304.14399} {We're afraid language models aren't modeling ambiguity}.
\newblock \emph{arXiv preprint arXiv:2304.14399}.

\bibitem[{Longpre et~al.(2021)Longpre, Perisetla, Chen, Ramesh, DuBois, and Singh}]{longpre2021entity}
Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021.
\newblock \href {https://arxiv.org/pdf/2109.05052} {Entity-based knowledge conflicts in question answering}.
\newblock \emph{arXiv preprint arXiv:2109.05052}.

\bibitem[{Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, Paul, and Bossan}]{peft}
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}.

\bibitem[{Marks and Tegmark(2023)}]{marks2023geometry}
Samuel Marks and Max Tegmark. 2023.
\newblock \href {https://arxiv.org/pdf/2310.06824.pdf} {The geometry of truth: Emergent linear structure in large language model representations of true/false datasets}.
\newblock \emph{arXiv preprint arXiv:2310.06824}.

\bibitem[{Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov}]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022.
\newblock \href {https://arxiv.org/pdf/2202.05262.pdf} {Locating and editing factual knowledge in gpt}.
\newblock In \emph{NeurIPS 2022}.

\bibitem[{{Mistral AI}(2023)}]{mistral}
{Mistral AI}. 2023.
\newblock \href {https://mistral.ai/news/announcing-mistral-7b/} {Announcing {M}istral 7{B}}.
\newblock Blogpost.

\bibitem[{Mitchell et~al.(2021)Mitchell, Lin, Bosselut, Finn, and Manning}]{mitchell2021fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D Manning. 2021.
\newblock \href {https://arxiv.org/pdf/2110.11309.pdf} {Fast model editing at scale}.
\newblock \emph{arXiv preprint arXiv:2110.11309}.

\bibitem[{Mitchell et~al.(2022)Mitchell, Lin, Bosselut, Manning, and Finn}]{mitchell2022memory}
Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher~D Manning, and Chelsea Finn. 2022.
\newblock \href {https://proceedings.mlr.press/v162/mitchell22a/mitchell22a.pdf} {Memory-based model editing at scale}.
\newblock In \emph{International Conference on Machine Learning}, pages 15817--15831. PMLR.

\bibitem[{Nie et~al.(2020)Nie, Zhou, and Bansal}]{nie2020can}
Yixin Nie, Xiang Zhou, and Mohit Bansal. 2020.
\newblock \href {https://arxiv.org/pdf/2010.03532} {What can we learn from collective human opinions on natural language inference data?}
\newblock \emph{arXiv preprint arXiv:2010.03532}.

\bibitem[{Onoe et~al.(2023)Onoe, Zhang, Padmanabhan, Durrett, and Choi}]{onoe2023can}
Yasumasa Onoe, Michael~JQ Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. 2023.
\newblock \href {https://arxiv.org/pdf/2305.01651} {Can lms learn new entities from descriptions? challenges in propagating injected knowledge}.
\newblock \emph{arXiv preprint arXiv:2305.01651}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf} {Training language models to follow instructions with human feedback}.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:27730--27744.

\bibitem[{Patil et~al.(2023)Patil, Hase, and Bansal}]{patil2023can}
Vaidehi Patil, Peter Hase, and Mohit Bansal. 2023.
\newblock \href {https://arxiv.org/pdf/2309.17410.pdf} {Can sensitive information be deleted from llms? objectives for defending against extraction attacks}.
\newblock \emph{arXiv preprint arXiv:2309.17410}.

\bibitem[{Pavlick and Kwiatkowski(2019)}]{pavlick2019inherent}
Ellie Pavlick and Tom Kwiatkowski. 2019.
\newblock \href {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00293/43531/Inherent-Disagreements-in-Human-Textual-Inferences} {Inherent disagreements in human textual inferences}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:677--694.

\bibitem[{Petroni et~al.(2019)Petroni, Rockt{\"a}schel, Riedel, Lewis, Bakhtin, Wu, and Miller}]{petroni-etal-2019-language}
Fabio Petroni, Tim Rockt{\"a}schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1250} {Language models as knowledge bases?}
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2463--2473, Hong Kong, China. Association for Computational Linguistics.

\bibitem[{Pinter and Elhadad(2023)}]{pinter-elhadad-2023-emptying}
Yuval Pinter and Michael Elhadad. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.findings-emnlp.1012} {Emptying the ocean with a spoon: Should we edit models?}
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 15164--15172, Singapore. Association for Computational Linguistics.

\bibitem[{Plank(2022)}]{plank2022problem}
Barbara Plank. 2022.
\newblock \href {https://arxiv.org/pdf/2211.02570} {The'problem'of human label variation: On ground truth in data, modeling and evaluation}.
\newblock \emph{arXiv preprint arXiv:2211.02570}.

\bibitem[{Powell et~al.(2024)Powell, Gerych, and Hartvigsen}]{powell2024taxi}
Derek Powell, Walter Gerych, and Thomas Hartvigsen. 2024.
\newblock \href {https://arxiv.org/pdf/2404.15004} {Taxi: Evaluating categorical knowledge editing for language models}.
\newblock \emph{arXiv preprint arXiv:2404.15004}.

\bibitem[{Prystawski et~al.(2023)Prystawski, Li, and Goodman}]{prystawski2024think}
Ben Prystawski, Michael Li, and Noah Goodman. 2023.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2023/file/e0af79ad53a336b4c4b4f7e2a68eb609-Paper-Conference.pdf} {Why think step by step? reasoning emerges from the locality of experience}.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson}]{qi2023fine}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.
\newblock \href {https://arxiv.org/pdf/2310.03693} {Fine-tuning aligned language models compromises safety, even when users do not intend to!}
\newblock \emph{arXiv preprint arXiv:2310.03693}.

\bibitem[{Quine and Ullian(1970)}]{quine1970web}
Willard~V Quine and Joseph~Silbert Ullian. 1970.
\newblock The web of belief.

\bibitem[{Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman}]{rein2023gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman. 2023.
\newblock \href {https://arxiv.org/pdf/2311.12022.pdf} {Gpqa: A graduate-level google-proof q\&a benchmark}.
\newblock \emph{arXiv preprint arXiv:2311.12022}.

\bibitem[{Roberts et~al.(2020)Roberts, Raffel, and Shazeer}]{roberts-etal-2020-much}
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.437} {How much knowledge can you pack into the parameters of a language model?}
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 5418--5426, Online. Association for Computational Linguistics.

\bibitem[{Sharma et~al.(2023)Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston et~al.}]{sharma2023towards}
Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R Johnston, et~al. 2023.
\newblock \href {https://arxiv.org/pdf/2310.13548} {Towards understanding sycophancy in language models}.
\newblock \emph{arXiv preprint arXiv:2310.13548}.

\bibitem[{Simon(1956)}]{simon1956rational}
Herbert~A Simon. 1956.
\newblock \href {https://pages.ucsd.edu/~mckenzie/Simon1956PsychReview.pdf} {Rational choice and the structure of the environment.}
\newblock \emph{Psychological review}, 63(2):129.

\bibitem[{Soares et~al.(2015)Soares, Fallenstein, Armstrong, and Yudkowsky}]{soares2015corrigibility}
Nate Soares, Benja Fallenstein, Stuart Armstrong, and Eliezer Yudkowsky. 2015.
\newblock \href {https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf} {Corrigibility}.
\newblock In \emph{Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence}.

\bibitem[{Starr(2022)}]{sep-counterfactuals}
W.~Starr. 2022.
\newblock \href {https://plato.stanford.edu/entries/counterfactuals/} {{Counterfactuals}}.
\newblock In Edward~N. Zalta and Uri Nodelman, editors, \emph{The {Stanford} Encyclopedia of Philosophy}, {W}inter 2022 edition. Metaphysics Research Lab, Stanford University.

\bibitem[{Tversky and Kahneman(1974)}]{tversky1974judgment}
Amos Tversky and Daniel Kahneman. 1974.
\newblock \href {https://www.science.org/doi/pdf/10.1126/science.185.4157.1124?casa_token=h8Dnb7ql6T4AAAAA:kco51ZIKjs4YcLeyaY_tay6nD0dCfcaTWSaku1VZDrOSDk51mnk3ORnpRWS2N_RQvazFhSc0Ld6d35Q} {Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty.}
\newblock \emph{science}, 185(4157):1124--1131.

\bibitem[{Uma et~al.(2021)Uma, Fornaciari, Hovy, Paun, Plank, and Poesio}]{uma2021learning}
Alexandra~N Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. 2021.
\newblock \href {https://www.jair.org/index.php/jair/article/view/12752} {Learning from disagreement: A survey}.
\newblock \emph{Journal of Artificial Intelligence Research}, 72:1385--1470.

\bibitem[{Vrande{\v{c}}i{\'c} and Kr{\"o}tzsch(2014)}]{vrandevcic2014wikidata}
Denny Vrande{\v{c}}i{\'c} and Markus Kr{\"o}tzsch. 2014.
\newblock \href {https://dl.acm.org/doi/fullHtml/10.1145/2629489} {Wikidata: a free collaborative knowledgebase}.
\newblock \emph{Communications of the ACM}, 57(10):78--85.

\bibitem[{Wan et~al.(2024)Wan, Wallace, and Klein}]{wan2024evidence}
Alexander Wan, Eric Wallace, and Dan Klein. 2024.
\newblock \href {https://arxiv.org/pdf/2402.11782} {What evidence do language models find convincing?}
\newblock \emph{arXiv preprint arXiv:2402.11782}.

\bibitem[{Wang et~al.(2023{\natexlab{a}})Wang, Zhang, Xie, Yao, Tian, Wang, Xi, Cheng, Liu, Zheng et~al.}]{wang2023easyedit}
Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, et~al. 2023{\natexlab{a}}.
\newblock \href {https://arxiv.org/pdf/2308.07269} {Easyedit: An easy-to-use knowledge editing framework for large language models}.
\newblock \emph{arXiv preprint arXiv:2308.07269}.

\bibitem[{Wang et~al.(2023{\natexlab{b}})Wang, Zhu, Liu, Zheng, Chen et~al.}]{wang2023knowledge}
Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et~al. 2023{\natexlab{b}}.
\newblock \href {https://arxiv.org/pdf/2310.16218} {Knowledge editing for large language models: A survey}.
\newblock \emph{arXiv preprint arXiv:2310.16218}.

\bibitem[{Wang et~al.(2021)Wang, Gao, Zhu, Zhang, Liu, Li, and Tang}]{wang2021kepler}
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.
\newblock \href {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089/KEPLER-A-Unified-Model-for-Knowledge-Embedding-and} {Kepler: A unified model for knowledge embedding and pre-trained language representation}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:176--194.

\bibitem[{Wang et~al.(2023{\natexlab{c}})Wang, Feng, Wang, Shi, Balachandran, He, and Tsvetkov}]{wang2023resolving}
Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023{\natexlab{c}}.
\newblock \href {https://arxiv.org/pdf/2310.00935} {Resolving knowledge conflicts in large language models}.
\newblock \emph{arXiv preprint arXiv:2310.00935}.

\bibitem[{Wei et~al.(2023)Wei, Yu, Ma, Lei, Weng, Song, and Liu}]{wei2023assessing}
Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, and Kang Liu. 2023.
\newblock \href {https://arxiv.org/pdf/2311.09053} {Assessing knowledge editing in language models via relation perspective}.
\newblock \emph{arXiv preprint arXiv:2311.09053}.

\bibitem[{Wu et~al.(2023)Wu, Li, Xu, Dong, Wu, Bian, and Xiong}]{wu2023depn}
Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. 2023.
\newblock \href {https://arxiv.org/pdf/2310.20138} {Depn: Detecting and editing privacy neurons in pretrained language models}.
\newblock \emph{arXiv preprint arXiv:2310.20138}.

\bibitem[{Xie et~al.(2023)Xie, Zhang, Chen, Lou, and Su}]{xie2023adaptive}
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu~Su. 2023.
\newblock Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Xu et~al.(2023)Xu, Lin, Yang, Zhang, Shi, Zhang, Fang, Xu, and Qiu}]{xu2023earth}
Rongwu Xu, Brian~S Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu. 2023.
\newblock \href {https://arxiv.org/pdf/2312.09085.pdf} {The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation}.
\newblock \emph{arXiv preprint arXiv:2312.09085}.

\bibitem[{Yao et~al.(2023)Yao, Wang, Tian, Cheng, Li, Deng, Chen, and Zhang}]{yao2023editing}
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023.
\newblock \href {https://arxiv.org/pdf/2305.13172} {Editing large language models: Problems, methods, and opportunities}.
\newblock \emph{arXiv preprint arXiv:2305.13172}.

\bibitem[{Zhong et~al.(2023)Zhong, Wu, Manning, Potts, and Chen}]{zhong2023mquake}
Zexuan Zhong, Zhengxuan Wu, Christopher~D Manning, Christopher Potts, and Danqi Chen. 2023.
\newblock \href {https://arxiv.org/pdf/2305.14795} {Mquake: Assessing knowledge editing in language models via multi-hop questions}.
\newblock \emph{arXiv preprint arXiv:2305.14795}.

\bibitem[{Zhou et~al.(2024)Zhou, Hwang, Ren, and Sap}]{zhou2024relying}
Kaitlyn Zhou, Jena~D Hwang, Xiang Ren, and Maarten Sap. 2024.
\newblock \href {https://arxiv.org/pdf/2401.06730} {Relying on the unreliable: The impact of language models' reluctance to express uncertainty}.
\newblock \emph{arXiv preprint arXiv:2401.06730}.

\bibitem[{Zhou et~al.(2021)Zhou, Nie, and Bansal}]{zhou2021distributed}
Xiang Zhou, Yixin Nie, and Mohit Bansal. 2021.
\newblock \href {https://arxiv.org/pdf/2104.08676} {Distributed nli: Learning to predict human opinion distributions for language reasoning}.
\newblock \emph{arXiv preprint arXiv:2104.08676}.

\bibitem[{Zhu et~al.(2020)Zhu, Rawat, Zaheer, Bhojanapalli, Li, Yu, and Kumar}]{zhu2020modifying}
Chen Zhu, Ankit~Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020.
\newblock \href {https://arxiv.org/pdf/2012.00363.pdf} {Modifying memories in transformer models}.
\newblock \emph{arXiv preprint arXiv:2012.00363}.

\end{thebibliography}
