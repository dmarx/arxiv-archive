**Abstract**

Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3R to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. Project page: <https://stereo4d.github.io>

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Introduction [\[sec:intro\]]{#sec:intro label="sec:intro"}

Simultaneously predicting and understanding geometry and motion---that is, dynamic 3D content---from images is a fundamental building block for computer vision, with applications ranging from robotic interaction and scene reconstruction to novel view synthesis of dynamic scenes. While recent work has made remarkable progress in predicting static 3D structure from images [@wang2024dust3r; @yang2024depth; @bochkovskii2024depth], modeling real-world 3D motion---people gesturing, balls bouncing, leaves rustling in the wind---remains a critical unsolved challenge for building truly general models of the visual world.

Recent breakthroughs in AI, from large language models [@achiam2023gpt; @team2023gemini] to image generation [@polyak2024movie] and static 3D reconstruction [@wang2024dust3r; @bochkovskii2024depth; @yang2024depth], demonstrate a consistent pattern: large amounts of high-quality, realistic training data and scalable architectures enable dramatic performance improvements. In the realm of 3D reasoning, prior works [@li2018megadepth; @depthanything; @ranftl2020towards; @ranftl2021vision; @wang2024dust3r] have shown the value of large-scale training data for strong zero-shot generalization within single-view or two-view static scene settings. But applying this same formula to *dynamic* 3D scenes (i.e. moving 3D structure) requires a corresponding large-scale dataset consisting of diverse visual content paired with corresponding ground-truth 3D motion trajectories. Obtaining such data presents unique challenges. While there are synthetic datasets [@zheng2023point; @butler2012naturalistic; @dosovitskiy2015flownet; @greff2021kubric], these often fail to capture the distribution of real-world content and the nuanced patterns of real-world motion. Traditional approaches to gathering real motion data, such as motion capture systems or multi-view camera arrays [@Joo_2015_ICCV; @Grauman_2024_CVPR; @kirschstein2023nersemble; @isik2023humanrf] are accurate, but difficult to scale and limited in the diversity of scenes they can capture.

We identify online stereoscopic fisheye videos (often referred to as VR180 videos) as an untapped source of such data. These videos, designed to capture immersive VR experiences, provide wide field-of-view stereo imagery with a standardized stereo baseline. We present a pipeline that carefully combines state-of-the-art methods for stereo depth estimation and video tracking along with structure-from-motion methods optimized for dynamic scenes. By combining our system with careful filtering and quality control, we show that we can extract over 100K video sequences, each containing high-quality 3D point clouds with per-point long-term trajectories (see Fig. [\[fig:teaser\]](#fig:teaser){reference-type="ref" reference="fig:teaser"}), as well as all other intermediate quantities: depth maps, camera poses, images, and 2D correspondences. We additionally show the utility of the dataset by training *DynaDUSt3R*, an extension to DUSt3R that can predict high-quality 3D structure *and* motion from challenging image pairs.

Our contributions include: (1) a framework for obtaining real-world, dynamic, and pseudo-metric 4D reconstructions and camera poses at scale from existing online video; (2) DynaDUSt3R, a method that takes a pair of frames from any real-world video, and predicts a pair of 3D point clouds and the corresponding 3D motion trajectories that connect them in time.

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Related work [\[sec:related\]]{#sec:related label="sec:related"}

**2D and 3D motion data.** There has been tremendous progress on the task of motion estimation from images and videos, and in particular for 2D image-space correspondence estimation. Most state-of-the-art methods use neural networks trained on ground truth data to predict these correspondences directly from images. While these approaches require large training datasets, synthetic data from graphics engines [@dosovitskiy2015flownet; @mayer2016large; @harley2022particle; @butler2012naturalistic; @sun2021autoflow; @greff2021kubric; @zheng2023point] has proven surprisingly effective at generalizing to real-world data, likely because the core task, low-level textural correspondence, is similar between the two domains.

However, the same cannot be said for 3D motion estimation, since predicting both 3D structure and motion is usually more ambiguous and can require specific prior knowledge about the real world and how it moves. To help address this domain gap, a number of real-world datasets have been proposed. The KITTI [@geiger2013vision] and Waymo [@sun2020scalability] datasets include real-world autonomous driving sequences with stereo and motion annotations derived from LiDAR and odometry information, but only focus on the relatively closed domain of street scenes, whereas our data depicts more diverse in-the-wild scenarios. A number of annotated smaller-scale datasets, such as TAPVid [@doersch2022tap], TAPVid3D [@koppula2024tapvid3d], and Dycheck [@gao2022monocular], have been proposed, primarily serving as evaluation datasets for benchmarking depth estimation, 3D reconstruction, and 3D motion estimation approaches. WSVD [@wang2019web] and NVDS [@NVDS] are stereo video datasets that include disparity maps derived from optical flow. While their source content is similar, our method provides richer 3D annotations beyond time-independent disparity maps, such as 3D camera parameters and long-term 3D motion tracks.

**Static and dynamic scene reconstruction**

The problem of reconstructing a static 3D scene has been studied for decades. Traditional 3D reconstruction methods tackle this problem by first estimating camera parameters via Structure-from-Motion (SfM) [@snavely2006photo; @pollefeys2008detailed; @pollefeys2004visual; @pollefeys2008detailed; @agarwal2011building; @schonberger2016structure; @sweeney2019structure; @holynski2020reducing] or SLAM [@engel2017direct; @campos2021orb; @mur2015orb; @davison2007monoslam]. Dense scene geometry can then be estimated through Multi-view Stereo (MVS) [@campbell2008using; @jancosek2011multi; @furukawa2010towards; @furukawa2009accurate; @galliani2015massively; @schonberger2016pixelwise; @yao2018mvsnet; @yao2019recurrent] followed by surface reconstruction algorithms [@hoppe1992surface; @curless1996volumetric; @kazhdan2006poisson]. More recently, deep neural network-based approaches have shown promising results in improving camera localization accuracy or scene reconstruction through intermediate representations such as depth maps [@bloesch2018codeslam; @tang2018ba; @teed2024deep; @teed2021droid; @shen2023dytanvo; @li2024megasam], radiance fields [@lin2021barf; @Fu_2024_CVPR; @park2023camp; @gao2024cat3d; @shih2024extranerf; @weber2024nerfiller], or 3D scene coordinates [@brachmann2023ace; @brachmann2024acezero; @leroy2024grounding; @wang2024dust3r; @zhang2024monst3r]. However, these methods assume the input images to be observations of a static environment, and therefore produce inaccurate geometry and camera poses for dynamic scenes.

Reconstructing dynamic scenes is more challenging since scene and object motions violate the multi-view constraints used to reconstruct static scenes. As a result, many prior works require RGBD input [@bozic2020deepdeform; @newcombe2015dynamicfusion] or only recover sparse geometry [@park20103d; @vo2016spatiotemporal; @simon2016kronecker]. Several recent works tackle this problem from monocular input through video depth maps [@zhang2021consistent; @kopf2021rcvd; @zhang2022structure], time-varying radiance fields [@park2021nerfies; @park2021hypernerf; @li2021neural; @liu2023robust; @li2023dynibar; @gao2022monocular; @lei2024mosca; @wang2024shape], or generative priors [@wu2024cat4d].

**Monocular and stereo depth.**

Recent works on single-view depth prediction have shown strong zero-shot generalization to in-the-wild domains by training deep neural networks on diverse RGBD datasets [@li2018megadepth; @li2019learning; @ranftl2021vision; @yin2021learning; @ranftl2020towards; @ke2024repurposing; @depthanything; @yang2024depth; @yin2023metric3d; @piccinelli2024unidepth]. However, producing *temporally consistent* and *metric* depth from video is still challenging. To tackle this, recent works use test-time optimization [@luo2020consistent; @zhang2022structure] or end-to-end learning with temporal attention [@kopf2021rcvd; @hu2024depthcrafter; @shao2024learning; @NVDS]. On the other hand, stereo images or videos are also popular input modalities for obtaining reliable metric depth maps, and various stereo matching algorithms have been proposed [@birchfield1999depth; @hirschmuller2002real; @van2002hierarchical; @klaus2006segment; @sun2003stereo; @pang2017cascade; @chang2018pyramid; @kendall2017end; @zhang2019ga; @li2023temporally; @zhang2023temporalstereo; @karaev2023dynamicstereo; @jing2024matchstereovideos; @wang2025sea]. Building on these advancements, our method bridges ideas from monocular video depth estimation and stereo video processing. We use a light-weight optimization step and extend them to stereo inputs for more consistent motion estimation in metric space.

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Creating a dataset of 4D scenes [\[sec:data\]]{#sec:data label="sec:data"}

![**Data processing pipeline.** Our method starts with VR180 (wide-angle, stereoscopic) videos, and estimates metric stereo depth, 2D point tracks, and camera poses. These quantities allow the tracks to be lifted to 3D where they are filtered and denoised to produce world-space, metric 3D point trajectories.](fig/data_processing_vertical.pdf){#fig:data_pipeline width="\\linewidth"}

A core contribution of this work is a pipeline for extracting high-quality, pseudo-metric, 3D data from online stereoscopic fisheye videos (known as VR180 videos). High-resolution, wide field of view VR180 videos can be found readily online. We show that this data is ideal for deriving rich dynamic 3D information that can power models for predicting geometry and motion from imagery.

Concretely, each instance of data starts as an $N$ frame stereo video consisting of left-right image pairs $\mathbf{I}_i$ and $\mathbf{I}'_i$ indexed by frame index $i\in[1,N]$. We convert these stereo pairs to a dynamic 3D point cloud with $K$ points in a world-space coordinate frame, where each point, indexed by $j\in[1,K]$, has a time-varying position $\mathbf{p}_i^j$. As part of the process of generating this dynamic point cloud, we also extract a number of auxiliary quantities: (1) per-frame camera extrinsics, (the left camera's position $\mathbf{c}_i$ and orientation $\mathbf{R}_i$), (2) rig calibration for the stereo video giving the position $\mathbf{c}_r$ and orientation $\mathbf{R}_r$ of the right camera relative to the left camera, and (3) a per-frame disparity map $\mathbf{D}_i$.

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. Data Processing Pipeline

At a high level, our pipeline for converting a stereoscopic video into a dynamic point cloud involves estimating camera poses, stereo disparity, and 2D tracks, fusing these quantities into a consistent 3D coordinate frame, and performing several filtering operations to ensure temporal consistent, high-quality reconstructions (Fig. [1](#fig:data_pipeline){reference-type="ref" reference="fig:data_pipeline"}). In this section, we describe in detail the key components of this process.

**SfM.** We start by processing the sequence of stereo frames $\mathbf{I}_i \leftrightarrow \mathbf{I}_i'$ to produce camera pose estimates ($\mathbf{c}_i, \mathbf{R}_i$). We first use a SLAM method to divide the video into shots, as in [@zhou2017scene]. For each shot, we run an incremental SfM algorithm similar to COLMAP [@schonberger2016structure]. We initialize the stereo rig calibration $(\mathbf{c}_r, \mathbf{R}_r)$ to a rectified stereo pair with baseline $6.3$cm, but optimize for the calibration in bundle adjustment. In practice, we found that the exact stereo pair orientation can vary significantly from its nominal configuration and that optimizing the rig was critical for good results.

**Depth Estimation.** We next estimate a per-frame disparity map, operating on each frame independently. In particular, we use the estimated camera rig calibration $\mathbf{c}_r, \mathbf{R}_r$ to create rectified stereo pairs from the stereo fisheye video and estimate the per-frame disparity $\mathbf{D}_i$ with RAFT [@sun2022disentangling; @sun2021autoflow; @teed2020raft].

**3D Track Estimation and Optimization.** We extract long-range 2D point trajectories using BootsTAP [@doersch2024bootstap]. Using the camera poses $\mathbf{c}_i, \mathbf{R}_i$ and disparity maps $\mathbf{D}_i$, we unproject these tracked points into 3D space, turning each 2D track $j$ into a 3D motion trajectory $\mathbf{p}^j_1, \ldots, \mathbf{p}^j_N$ across all frames. In general, each point will usually only be tracked in a subset of frames, but for simplicity, we describe the formulation as if all points are always visible. Moreover, since subsequent steps are done independently per-track, we drop the superscript $j$ in future references.

Since stereo depth estimation is performed per-frame, the initial disparity estimates (and therefore, the 3D track positions) are likely to exhibit high-frequency temporal jitter. To compensate for potentially inconsistent disparity estimates, we formulate an optimization strategy that solves for a per-frame scalar offset $\delta_i \in \mathbb{R}$ that moves each point $\mathbf{p}_i$ along the ray from camera location $\mathbf{c}_i$ to $\mathbf{p}_i$ at frame $i$. This ray is denoted $\mathbf{r}_i = (\mathbf{p}_i - \mathbf{c}_i) / ||\mathbf{p}_i - \mathbf{c}_i||$, and we refer to the updated location as $\mathbf{p}_i' = \mathbf{p}_i + \delta_i \mathbf{r}_i$.

To ensure static points remain stationary while moving tracks maintain realistic, smooth motion, avoiding abrupt depth changes frame by frame, we design an optimization objective comprising three terms: a static loss $\mathcal{L}_{\mathsf{static}}$, a dynamic loss $\mathcal{L}_{\mathsf{dynamic}}$, and a regularization loss $\mathcal{L}_{\mathsf{reg}}$. The static loss $\mathcal{L}_{\mathsf{static}}$ minimizes jitter by encouraging points to remain close to each other in world space: $$\mathcal{L}_{\mathsf{static}} = \sum_{i=1}^{N} \sum_{j=1}^{N} \frac{\| \mathbf{p}_i' - \mathbf{p}_j' \|^2}{N_p'^2}
\label{eq:objective_function}$$ where $N_p' = \sum_{i=1}^N{\|\mathbf{p}'_i\|} / N$ is a normalizing factor. The dynamic loss term reduces jitter by minimizing acceleration along the camera ray through a discrete Laplacian operator: $$\mathcal{L}_{\mathsf{dynamic}} = \sum_{i=1}^{N} \sum_{\Delta\in\mathcal{W}} \left[ \left( \mathbf{p}_{i+\Delta}' - 2\mathbf{p}_i' + \mathbf{p}_{i-\Delta}' \right)^\top \mathbf{r}_i \right]^2
\label{eq:dynamic_objective}$$ where the acceleration along the ray is calculated over multiple window sizes $\mathcal{W}=\{1,3,5\}$.

The two loss terms are weighted by a track-dependent function, $\sigma(m)$, where $m$ is a measure of the motion magnitude of the track. Motion is measured in 2D rather than 3D because distant points can appear to have a larger 3D motion due to noise amplification at low disparities. Specifically, we project the 3D motion trajectory between time $i - w_o$ and the current time $i$ into 2D image-space at time $i$, and calculate the track's motion magnitude $m$ as the 90$^{th}$ percentile of the track's trail length across all frames. The track trail length for a frame is measured by projected 3D points along the track to the current frame as if the camera is *static* in a window of $w_o=16$ frames, $$\label{eqn:trail_length_def}
    m = \mathsf{Percentile}_{i=1:N}^{90}\left[\max_{w=1:w_o}\|\pi_i(\mathbf{p}_i) - \pi_i(\mathbf{p}_{i-w})\|\right]$$ where $\pi_i(\cdot)\in \mathbb{R}^2$ gives the projected pixel location of a 3D point on camera $\mathbf{c}_i$'s image plane. The weighting function $\sigma(m)$ is defined as $\sigma(m) = \frac{1}{1 + \exp(m - m_0)}$ where $m_0 = 20$. Finally, to encourage faithfulness to the originally estimated disparities, we regularize the displacements in disparity space: $$\mathcal{L}_{\mathsf{reg}} = \lambda_{\mathsf{reg}} \sum_{i=1}^{T} \left( \frac{1}{\delta_i + \|\mathbf{p}_i-\mathbf{c}_i\|} - \frac{1}{\|\mathbf{p}_i-\mathbf{c}_i\|} \right)^2,$$ where use of disparity space reflects the fact that the measurements themselves originate as disparities. Practically, the impact of the use of disparity is that larger deviations are tolerated at more distant points, where depth is intrinsically more uncertain.

The full objective function is $$\min_{\{\delta_i\}_{i=1}^N} \sigma(m)\mathcal{L}_{\mathsf{static}} + (1-\sigma(m))\mathcal{L}_{\mathsf{dynamic}} + \mathcal{L}_{\mathsf{reg}}.
\label{eqn:objective}$$ We set $\lambda_{\mathsf{reg}}=10^{-4}$ and optimize Eqn. [\[eqn:objective\]](#eqn:objective){reference-type="ref" reference="eqn:objective"} using Adam with a learning rate of 0.05 for 100 steps. The effect of track optimization is shown in Fig. [2](#fig:track_optimization){reference-type="ref" reference="fig:track_optimization"}. The optimized motion is smoother and does not contain high frequency noise.

![**Effect of track optimization.** Comparing motion trajectories before and after track optimization, we see that optimization resolves the high frequency jitter along camera rays, affecting both static and dynamic content. After optimization, static content has static tracks, and dynamic tracks are less noisy.](fig/optimization.pdf){#fig:track_optimization width="\\linewidth"}

![image](fig/diversity-highres.pdf){width="\\textwidth"}

**Implementation details.** *Shot-selection.* Rather than work with the full video, we break the footage into discrete, trackable shots using ORB-SLAM2's stereo estimation mode [@murartal2015orbslam] following [@zhou2018stereo]. *Field of View.* While estimating pose, we use a $140^\circ$ FoV fisheye format, which we found to capture more of the (usually static) background and less of the (often dynamic) foreground, yielding more reliable camera poses. *Stereo Confidence Checks.* We discard pixels where the $y$-component of RAFT flow is more than 1 pixel (since rectified stereo pairs should have perfectly horizontal motion) and where the stereo cycle consistency error is more than 1 pixel (since such pixels are unreliable). *Dense 2D tracks.* To get dense tracks, we run BootsTAP with dense query points: for every 10th frame, we uniformly initialize $128\times128$ query points on frames of resolution 512 $\times$ 512. We then prune redundant tracks that overlap on the same pixel. *Drifting tracks.* Since 2D tracks can drift on textureless regions, we discard moving 3D tracks that correspond to certain semantic categories (*e.g.*, "walls", "building", "road", "earth", "sidewalk"), detected by DeepLabv3 [@chen2017rethinking] on ADE20K classes [@zhou2017scene; @zhou2019semantic].

**Filtering details.** A fraction of the video clips that are processed may be unsuitable because they either (1) are not videos, and are entirely static images, (2) contain cross-fades, or (3) have text or other synthetic graphics. To discard text and title sequences, we avoid creating video clips from the start and ends of the source videos. We identify cross-fades by running SIFT [@lowe2004sift] matching through the video at multiple temporal scales and discarding video clips with static camera but with fewer than 5 SIFT matches between frames that are 5 seconds apart.

![**Diverse scene content:** A word cloud of captioned frames from our dataset shows our data is diverse, including a variety of common objects seen in videos.](fig/wordcloud.png){#fig:wordcloud width="\\linewidth"}

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. Stereo4D Dataset

Fig. [\[fig:motion\_distribution\]](#fig:motion_distribution){reference-type="ref" reference="fig:motion_distribution"} illustrates a subset of videos and reconstructions from a dataset processed with the above pipeline, encompassing more than 100K clips capturing everyday scenes and activities. To visualize the range of content, we used an automatic captioning system to generate captions for the dataset and created a word cloud (Fig. [3](#fig:wordcloud){reference-type="ref" reference="fig:wordcloud"}) highlighting the most frequently observed objects.

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Learning a prior on how things move We now describe our method for predicting dynamic 3D point clouds from pairs of images, and how we train it with our Stereo4D data. Our model is based on DUSt3R [@wang2024dust3r], which predicts a 3D point cloud for a static scene from images. Given two input images, it uses a ViT-based architecture [@dosovitskiy2020image] to extract image features and uses a transformer-based decoder to cross-attend features from two images, and then use a downstream *pointmap* decoder to output pointmaps for the two images, aligned in the first image's coordinate frame.

**DynaDUSt3R model.**

While DUSt3R focuses on static scene structure, our proposed DynaDUSt3R method, illustrated in Fig. [4](#fig:method){reference-type="ref" reference="fig:method"}, works with dynamic scenes by adding a *motion* head that predicts how the points move between two frames. As input, DynaDUSt3R accepts two images: $\mathbf{I}_0$ at time $t_0$, and $\mathbf{I}_1$ at time $t_1$ (where $t_0$ and $t_1$ may be seconds apart). It also accepts an intermediate query time $t_q \in [0,1]$; the motion head is asked to predict 3D scene flow from the two input frames to query time $t_q$, as described below.

Like DUSt3R, DynaDUSt3R begins by encoding the images with a shared ViT and cross-attention decoder, producing global features $G^0$ and $G^1$ for $\mathbf{I}_0$ and $\mathbf{I}_1$, respectively. Each feature embedding can be converted into geometry using DUSt3R's point head: e.g., for image $\mathbf{I}_0$, the point head produces a pointmap $\mathbf{P}^0 \in \mathbb{R}^{H \times W \times 3}$ representing the geometry at time $t_0$, as well as a point confidence map $\mathbf{C}^0_\mathsf{point} \in \mathbb{R}^{H \times W}$. Each point cloud is predicted in the coordinate frame of $\mathbf{I}_0$, but *at the time of its respective image* (so, the two point clouds may differ due to scene motion).

We add a separate *motion* head in parallel to the original point head, to predict a map of 3D displacement vectors (that is, a scene flow map, which we refer to as a *motion map*) for each pointmap. The motion map should displace each input frame to an intermediate time $t_q \in [0,1]$, where $t_q = 0$ corresponds to $t_0$, the time of $\mathbf{I}_0$, and similarly for $t_q = 1$, $t_1$, and $\mathbf{I}_1$. The motivation for predicting motion to an intermediate time (inclusive of the endpoints) is twofold: first, it leads to a more general prediction task where we can predict a full motion trajectory between two frames, and second, it allows us to use partial ground truth 3D trajectories as supervision; not all trajectories may span all the way from $t_0$ to $t_1$, but may span through some intermediate time.

For each image $\mathbf{I}_v$ (with $v \in \{0,1\}$), the network outputs a 3D motion map $\mathbf{M}^{v\to t_q}$ for the corresponding pointmap from $t_v$ to $t_q$ with corresponding motion confidence map $\mathbf{C}_\mathsf{mot}^{v}\in\mathbb{R}^{H\times W}$. This prediction is based on the global feature $G^v$ as well as an embedding of the query time $\texttt{emb}(t_q)$. We use positional embedding [@vaswani2017attention] to encode time $t_q$ to a 128-D vector and inject it to the motion features in the motion head via linear projection layers.

**Training objective.**

We use the same confidence-aware scale-invariant 3D regression loss as in DUSt3R. We first normalize both the predicted and ground truth pointmaps using scale factors $z=\text{norm}(\mathbf{P}^0, \mathbf{P}^1)$ and $\bar{z}=\text{norm}(\bar{\mathbf{P}}^0, \bar{\mathbf{P}}^1)$, respectively (where a bar, e.g., $\bar{\mathbf{P}}^0$, denotes a ground truth quantity, and where '$\text{norm}$' computes the average distance between a set of points and the world origin). We scale the motion maps with the same scales $z$ and $\bar{z}$. Following DUSt3R, we compute a Euclidean distance loss on the pointmap, setting $\mathcal{L}_\mathsf{point}$ to $$\sum_{v\in\{0,1\}}\sum_{i\in\mathcal{D}^v}\mathbf{C}_{\mathsf{point},i}^v\left\|\frac{1}{z}\mathbf{P}_i^v - \frac{1}{\bar{z}}\bar{\mathbf{P}}_i^v\right\| - \alpha_p\log\mathbf{C}_{\mathsf{point},i}^v
    \label{eqn:loss_point}$$ where $\mathcal{D}^v$ corresponds to the valid pixels where ground truth is defined and $\alpha_p$ is a weighting hyperparameter. We additionally compute a Euclidean distance loss on the position *after motion*, which encourages the network to learn correct displacements. This loss $\mathcal{L}_\mathsf{motion}$ is defined as $$\label{eqn:loss_motion}
    \sum_{v\in\{0,1\}}\sum_{i\in\mathcal{D}^v}\mathbf{C}_{\mathsf{mot},i}^v\left\|\frac{1}{z}\mathbf{P}_i^{v\to t_q} - \frac{1}{\bar{z}}\bar{\mathbf{P}}_i^{v\to t_q}\right\| \\
    - \alpha_m\log\mathbf{C}_{\mathsf{mot},i}^v,$$ where $\mathbf{P}_i^{v\to t_q} = \mathbf{P}_i^v + \mathbf{M}_i^{v\to t_q}$.

![**DynaDUSt3R architecture.** Given two images $(\mathbf{I}_0, \mathbf{I}_1)$ of a dynamic scene and a desired target time $t_q$, the images are passed through a ViT encoder and transformer decoder. The resulting features are processed by (1) a pointmap head that predicts 3D points in the coordinate frame of $\mathbf{I}_0$, and (2) a 3D motion head that predicts the motion of all points to the target time $t_q$. A double outline indicates a new component compared to DUSt3R.](fig/method-HgkMYtMSdW4-clip8.pdf){#fig:method width="\\linewidth"}

**Training details.**

We initialize our network with DUSt3R weights and initialize the motion head with the same weights as the point head. We finetune for 49k iterations, with batch size 64, learning rate 2.5e-5, optimized by Adam with weight decay 0.95. During training, we randomly sample pairs of video frames that are at most 60 frames apart. The weight for the confidence loss in Eqn [\[eqn:loss\_point\]](#eqn:loss_point){reference-type="ref" reference="eqn:loss_point"}-[\[eqn:loss\_motion\]](#eqn:loss_motion){reference-type="ref" reference="eqn:loss_motion"} is $\alpha_m = \alpha_p = 0.2$. The model is trained on tracks extracted from both 60$^\circ$ FoV videos for (higher quality) and 120$^\circ$ FoV videos for (larger coverage).

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Experiments

We conduct a series of experiments to validate the effectiveness of our proposed data and techniques. First, we evaluate our proposed real-world Stereo4D data mined from VR180 videos on the DynaDUSt3R task. In particular, we compare models that are individually trained with our real-world data and with synthetic data, and we show that our data enables model learning more accurate 3D motion priors (Sec. [\[sec:motion\_eval\]](#sec:motion_eval){reference-type="ref" reference="sec:motion_eval"}). Second, we show that our trained model that adapts DUSt3R has strong generalization to in-the-wild images of dynamic scenes, and enables accurate predictions of underlying geometry (Sec. [\[sec:structure\_eval\]](#sec:structure_eval){reference-type="ref" reference="sec:structure_eval"}).

![image](fig/ours_qualitative.pdf){width="\\linewidth"}

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. 3D motion prediction [\[sec:motion\_eval\]]{#sec:motion_eval label="sec:motion_eval"}

**Baselines and metrics.** To evaluate the efficacy of our data paradigm on motion prediction, we primarily compare DynaDUSt3R trained on Stereo4D to the same network trained on a synthetic dataset, PointOdyssey [@zheng2023point]. PointOdyssey contains ground truth depth maps and 3D motion tracks rendered from an animation engine; we supervise the model with this data using the same hyperparameter settings as described above. During inference, given two video frames sampled from a video of a dynamic scene, we compare 3D end-point-error (EPE) between ground truth and predicted 3D motion vectors. We also compute the fraction of 3D points that have motion within 5 cm and 10 cm compared to ground truth ($\delta_{3D}^{0.05}, \delta_{3D}^{0.10}$), following [@teed2021raft3d; @wang2024shape]. Since our model outputs point clouds up to an unknown scale, we align each prediction with the ground truth through a median scale estimate. We evaluate models trained on each of these two data sources on a held-out Stereo4D test-set, as well as on Arial Digital Twin (ADT) [@pan2023aria] data containing scene motion, processed by the TapVid3D benchmark [@koppula2024tapvid3d]. As test data, we randomly sample pairs of frames that are at most 30 frames apart from both Stereo4D and ADT.

![**Qualitative comparisons, 3D motion on the Stereo4D.** We compare variants of DynaDUSt3R trained on different data sources. The PointOdyssey-trained model incorrectly predicts significant 3D motion on static elements such as the building wall and the banners near the streetlight, while the Stereo4D-trained model correctly predicts these elements as stationary. The Stereo4D model also makes more precise motion predictions for dynamic objects, such as humans with large movements (bottom row).](fig/qualitative_comparison_on_motion_stereo4d.pdf){#fig:compare-stereo4d width="\\linewidth"}

**Quantitative results.**

We show numerical results for two-frame 3D motion prediction in Tab. [\[tab:motion\_3d\_eval\]](#tab:motion_3d_eval){reference-type="ref" reference="tab:motion_3d_eval"}. DynaDUSt3R trained on real-world data achieves better generalization and outperforms the baseline trained on PointOdyssey significantly across all metrics. This suggests the potential of our data for more effective learning of real-world 3D motion priors.

![**Qualitative comparisons of predicted 3D motion on ADT [@pan2023aria].** DynaDUSt3R trained on Stereo4D produces more accurate 3D motion compared to training on PointOdyssey.](fig/tapvid3d_adt.pdf){#fig:compare-stereo4d-adt width="\\linewidth"}

**Qualitative results.**

Fig. [\[fig:result-wall-stereo4dtest\]](#fig:result-wall-stereo4dtest){reference-type="ref" reference="fig:result-wall-stereo4dtest"} shows example results for three dynamic scenes in our Stereo4D test set, including visualizations of 3D point clouds and motion tracks. DynaDUSt3R produces accurate 3D shape and motion tracks over the timespan defined by the two input images. Despite the inputs being two sparse images, our architecture enables querying intermediate motion states, resulting in continuous and potentially non-linear motion trajectories.

We also qualitatively compare predicted 3D motion tracks between DynaDUSt3R networks trained on Stereo4D and on PointOdyssey, by projecting their predicted 3D motion vectors into 2D image space. Fig. [5](#fig:compare-stereo4d){reference-type="ref" reference="fig:compare-stereo4d"} and Fig. [6](#fig:compare-stereo4d-adt){reference-type="ref" reference="fig:compare-stereo4d-adt"} show comparisons on the Stereo4D and ADT test set respectively. DynaDUSt3R trained on Stereo4D produces more accurate 3D motion estimates for both static and moving objects. For instance, DynaDUSt3R trained on PointOdyssey produces non-zero motion for the stationary street banner and erroneous motions for the walking people in Fig. [5](#fig:compare-stereo4d){reference-type="ref" reference="fig:compare-stereo4d"}.

![image](fig/depth_comparison.pdf){width="\\linewidth"}

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. Structure prediction [\[sec:structure\_eval\]]{#sec:structure_eval label="sec:structure_eval"} **Baseline and metrics.** We evaluate the quality of predicted 3D structure by comparing depth maps predicted by DUSt3R [@wang2024dust3r], MonST3R [@zhang2024monst3r], and DynaDUSt3R trained on Stereo4D or PointOdyssey. DUSt3R is designed to predict aligned point clouds from two input images of a static scene. MonST3R, a concurrent approach, extends DUSt3R to handle dynamic scenes by predicting time-varying point clouds without modeling motion.

We evaluate predicted depth accuracy on the Bonn [@palazzolo2019iros] dataset and our held-out test set, where we sample two views that are 30 frames apart from a video. Since we focus on the two-frame case, we do not apply any post-optimization to the network outputs. In addition, since all methods predict 3D point clouds in the coordinate frame of the first image, we include the two points clouds predicted from both input frames in our evaluation. We use standard depth metrics, including absolute relative error (Abs Rel) and percentage of inlier points $\delta<1.25$, following prior work [@NVDS; @zhang2024monst3r]. We use the same median alignment as before to align the predicted depth map with the ground truth.

**Quantitative comparisons.**

We show quantitative comparisons of depth predicted by different methods in Tab. [\[tab:depth\_eval\]](#tab:depth_eval){reference-type="ref" reference="tab:depth_eval"}. DynaDUSt3R trained on Stereo4D outperforms all other baselines by a large margin. In particular, we demonstrate improved depth prediction on the unseen Bonn dataset.

**Qualitative comparisons.**

We provide additional visual comparisons in Fig. [\[fig:compare-bonn\]](#fig:compare-bonn){reference-type="ref" reference="fig:compare-bonn"}, where we visualize ground truth 3D point clouds and predictions from our approach and the other three baselines at different input time steps. DuST3R predicts inaccurate depth relationships for the two moving people, while MonsT3R and DynaDUSt3R trained on PointOdyssey both predict distorted scene geometry. In contrast, our model trained on Stereo4D produces 3D structure that most closely resembles the ground truth.

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Discussion and Conclusion **Limitations.** Our data curation pipeline and trained model have limitations. The quality of the long-range 3D motion tracks depends on the accuracy of optical flow and 2D point tracking and may degrade for distant background regions or objects occluded for long periods. Additionally, DynaDUSt3R is a non-generative model that only operates on two-frame inputs. Extending our model to video input by adopting an extra global optimization [@zhang2024monst3r] or integrating generative priors for modeling ambiguous motion content is a promising future direction.

**Conclusion.**

We presented a pipeline for mining high-quality 4D data from Internet stereoscopic videos. Our framework automatically annotates each real-world video sequence with camera parameters, 3D point clouds, and long-range 3D motion trajectories by consolidating different noisy structure and motion estimates derived from videos. Furthermore, we show that training a variant of DUSt3R on our real-world 4D data enables more accurate learning of 3D structure and motion in dynamic scenes, outperforming other baselines.

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Stereo4D Statistics We collected around 110k clips from 6,493 Internet VR180 videos. Fig. [\[fig:supp:camera\_stats\]](#fig:supp:camera_stats){reference-type="ref" reference="fig:supp:camera_stats"} shows the camera translation distribution between the first and last frame of each clip. In Fig. [8](#fig:supp:motion_stats){reference-type="ref" reference="fig:supp:motion_stats"}, we measure the motion in terms of pixel displacement projected onto the image frame. Measuring motion in pixel-space emphasizes motion that occurs closer to the camera, since such motion yields larger pixel displacements, while naturally de-emphasizing motion further from the camera.

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. More qualitative comparisons

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. More results on held-out Stereo4D examples Fig. [\[fig:supp:qualitative-stereo4d\]](#fig:supp:qualitative-stereo4d){reference-type="ref" reference="fig:supp:qualitative-stereo4d"} shows additional DynaDUSt3R predictions on the Stereo4D held-out test set, extending Fig. [\[fig:result-wall-stereo4dtest\]](#fig:result-wall-stereo4dtest){reference-type="ref" reference="fig:result-wall-stereo4dtest"} from the main paper. Fig. [\[fig:supp:compare-stereo4d\]](#fig:supp:compare-stereo4d){reference-type="ref" reference="fig:supp:compare-stereo4d"} shows additional qualitative examples of motion comparisons on Stereo4D test set, extending Fig. [5](#fig:compare-stereo4d){reference-type="ref" reference="fig:compare-stereo4d"} from the main paper. Fig. [\[fig:supp:compare-stereo4d\]](#fig:supp:compare-stereo4d){reference-type="ref" reference="fig:supp:compare-stereo4d"} compares variants of DynaDUSt3R trained on different data sources. The model trained on PointOdyssey incorrectly predicts large 3D motions, while the model trained on Stereo4D makes more accurate motion predictions, closer to ground truth.

![Scene motion statistics from Stereo4D. We measure the motion in terms of pixel displacement projected onto the image frame. For each video, we measure the percentage of tracks that have 3D trail length greater than 50 pixels. The 3D trail length is measured by Eqn. [\[eqn:trail\_length\_def\]](#eqn:trail_length_def){reference-type="ref" reference="eqn:trail_length_def"}. ](fig/supp/camera_stats.png){#fig:supp:motion_stats width="\\linewidth"}

![Scene motion statistics from Stereo4D. We measure the motion in terms of pixel displacement projected onto the image frame. For each video, we measure the percentage of tracks that have 3D trail length greater than 50 pixels. The 3D trail length is measured by Eqn. [\[eqn:trail\_length\_def\]](#eqn:trail_length_def){reference-type="ref" reference="eqn:trail_length_def"}. ](fig/supp/track_stats.png){#fig:supp:motion_stats width="\\linewidth"}

![image](fig/supp/qualitative-stereo4d.pdf){width="\\textwidth"}

![image](fig/supp/qualitative_comparison_on_motion_stereo4d.pdf){width="\\textwidth"}

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. More qualitative examples on track optimization In Fig. [\[fig:supp:track\_comparison\]](#fig:supp:track_comparison){reference-type="ref" reference="fig:supp:track_comparison"}, we illustrate estimated tracks for a video sequence featuring a forward-moving camera and vehicles driving towards the camera. Our initial 3D tracks derived directly from RAFT depth, BootsTAP 2D tracks, and SfM camera pose, show significant jitter for both dynamic (vehicle) and static (ground) points. However, after applying our track optimization, the ground points produce stable, static tracks, and vehicle tracks become smooth and coherent.

![image](fig/supp/equirect-sample.pdf){width="\\textwidth"}

![image](fig/supp/track_optimization_car_comparison-2.pdf){width="80%"}

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. Dataset curation details -8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. Equirectangular videos The raw videos that we collect (see examples in Fig. [\[fig:supp:equirect\]](#fig:supp:equirect){reference-type="ref" reference="fig:supp:equirect"}) are natively stored in a cropped equirectangular format, which differs from a full 360$^\circ$ equirectangular projection as the horizontal field of view of the cropped format typically spans 180$^\circ$---half of a full sphere. These videos often contain metadata specifying the horizontal and vertical field of view. For instance, metadata for a typical video might specify $\mathsf{start_{yaw}}=-90.0^\circ$, $\mathsf{end_{yaw}}=90.0^\circ$, $\mathsf{start_{tilt}}=-90.0^\circ$, $\mathsf{end_{tilt}}=90.0^\circ$; Since many VR180 videos are designed for an immersive VR experience, they are typically viewed with headsets. Hence, the baseline between the left and right cameras typically closely matches the average human eye distance of 6.3 cm.

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. SfM For ease of processing with standard 3D computer vision pipelines, and to benefit from the wide FoV of the input videos, we convert the videos from their native format (equirectangular projections) to a fisheye format for camera pose estimation. We use a 140$^\circ$ field of view for these fisheye-projected videos, because many equirectangular videos have a black fade-out/feathering/vignetting effect applied at the boundary, as shown in Fig. [\[fig:supp:equirect\]](#fig:supp:equirect){reference-type="ref" reference="fig:supp:equirect"}. We found that using wider FoV frames significantly improves camera pose estimation in dynamic scenes. When using narrow FoV projections, dynamic objects are more likely to occupy a large fraction of the frame; when these dynamic foreground objects are rich in features, they can confuse camera tracking algorithms, leading to inaccurate camera poses that track the dynamic object rather than producing true camera motion with respect to the environment. In contrast, wide-angle fisheye videos capture more background regions, which tend to have stable features for tracking, yielding more reliable camera poses.

We first use ORB-SLAM2's stereo estimation mode [@murartal2015orbslam] to identify trackable sequences within the videos, utilizing the method devised by Zhou *et al*.. to divide videos into discrete, trackable shots [@zhou2018stereo]. For each given shot, consisting of frames $(I_i, \ldots, I_n)$, we estimate camera poses and rig calibration via an incremental global bundle adjustment algorithm similar to COLMAP [@schonberger2016structure]. We initialize the stereo rig calibration to be that of a rectified stereo pair with baseline 6.3 cm, but optimize for the calibration as part of the bundle adjustment process, as in practice the stereo rig can vary significantly from its nominal configuration. This process yields a camera position $\mathbf{c}_i$ and orientation $\mathbf{R}_i$ for each frame $i$ (defined as the pose of the left camera), and a position $\mathbf{c}_r$ and orientation $\mathbf{R}_r$ for the right camera relative to the left (assumed to be constant throughout the shot).

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. Depth estimation Depth estimation is first performed on a per-frame basis, with disparity maps computed independently for each frame.

We use the estimated camera rig calibration $\mathbf{c}_r, \mathbf{R}_r$ to rectify the original high resolution equirectangular video frames, ensuring that (1) the left and right views have centered principal points, (2) are oriented perpendicular to the baseline, and (3) pointing in a parallel direction. We then convert the equirectangular videos to perspective projections for downstream predictions.

Disparity is estimated from optical flow [@teed2020raft; @sun2022disentangling] between the rectified left and right frames. The $x$-component of the optical flow is used as disparity, which is converted to metric depth using: $$\mathsf{Depth} = \frac{\mathsf{baseline}  \times f}{\mathsf{disparity}}.$$ Here $\mathsf{baseline}=0.063$m, and $f$ is the frame's focal length.

**Outlier Rejection.** Several criteria are applied to filter out unreliable pixels: *Inconsistency between left and right eyes:* Disparity is rejected if the optical flow fails a cycle-consistency check with an error exceeding one pixel. *Depth values exceeding 20 meters* are considered invalid. Estimating accurate depth beyond a certain range requires sub-pixel disparity estimation, and therefore the resulting depths are usually very noisy. *Negative flow values* that shouldn't occur, but can, often due to errors in textureless regions. *Large vertical flow:* pixels with a y-component of flow exceeding one pixel are removed (as in our rectified stereo pairs correspondences should have the same $y$-value, and violating that epipolar constraint indicates uncertain matches). *Occlusion boundaries:* Depth gradients exceeding a threshold ($\mathsf{threshold} = 0.3$) indicate occlusion boundaries and are rejected. For a pixel location $(x, y)$, depth gradients are computed as: $$\mathsf{grad_x}=|{\mathsf{Depth}(x+1, y)-\mathsf{Depth}(x-1,y)} |,$$ $$\mathsf{grad_y}=|{\mathsf{Depth}(x, y+1)-\mathsf{Depth}(x,y-1)} |.$$ Pixels are rejected if $\mathsf{grad_x} > \mathsf{threshold} \times \mathsf{Depth}(x,y)$ or $\mathsf{grad_y} > \mathsf{threshold} \times \mathsf{Depth}(x,y)$.

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. 2D tracks We extract long-range 2D point trajectories using BootsTAP [@doersch2024bootstap]. We run tracking on the left-eye video only. For every 10 frames, we uniformly initialize query points on image with stride 4. We then remove duplicated queries if earlier tracks fall within 1 pixel of a query point.

-8pt plus -2pt minus -2pt6pt \*subsection2 -8pt plus -2pt minus -2pt6pt -1em. Choice of FoV and resolution for perspective projection. When converting the equirectangular videos to perspective projections, we use two FoVs: 60$^\circ$ and 120$^\circ$. Both perspective videos are set to a resolution of $512\times512$, the maximum supported by BootsTAP. The 60$^\circ$ projection offers a higher sampling rate in scene units, which improves the accuracy of depth estimation and 2D tracks when measured in meters. Additionally, it has smaller perspective distortion near the image boundaries. In contrast, the $120^\circ$ projection provides wider coverage, ensuring longer 2D tracks across the videos. This trade-off allows us to balance data quality with spatial coverage for downstream tasks, e.g. DynaDUSt3R. We take the union of the 3D tracks derived from each of these videos for DynaDUSt3R training supervision.

-10pt plus -2pt minus -2pt7pt \*section1 -10pt plus -2pt minus -2pt7pt -1em. DynaDUSt3R training details.

**Dataloader.**

During training, we randomly sample two frames from the training videos that are at most 60 frames apart, at times $t_0$ and $t_1$, ($t_0 < t_1$). Additionally, we also sample one auxiliary frame in between, at time $t_{\mathsf{aux}}, t_0<t_\mathsf{aux}<t_1$, for additional track supervision between the two input frames. During training, we add data augmentation by applying random crops and color jitter to the input images and cropping the ground truth pointmap and motionmap accordingly.

**Training.**

The network takes input the two RGB images as well as query times $t_q = \{0, 1, \frac{t_\mathsf{aux}-t_0}{t_1-t_0}\}$ and predicts the pointmaps for the two input views and motionmaps for each query $t_q$. We supervise the network with losses defined in Eqn. [\[eqn:loss\_point\]](#eqn:loss_point){reference-type="ref" reference="eqn:loss_point"} and [\[eqn:loss\_motion\]](#eqn:loss_motion){reference-type="ref" reference="eqn:loss_motion"}. We initialize our network with the DUSt3R weights and initialize the motion head with the same weights as the point head. We finetune for 49k iterations with batch size 64, learning rate $2.5\times 10^{-5}$, and optimized by Adam with weight decay 0.95.
