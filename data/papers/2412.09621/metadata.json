{
  "arxivId": "2412.09621",
  "title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos",
  "authors": "Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander Holynski",
  "abstract": "Learning to understand dynamic 3D scenes from imagery is crucial for\napplications ranging from robotics to scene reconstruction. Yet, unlike other\nproblems where large-scale supervised training has enabled rapid progress,\ndirectly supervising methods for recovering 3D motion remains challenging due\nto the fundamental difficulty of obtaining ground truth annotations. We present\na system for mining high-quality 4D reconstructions from internet stereoscopic,\nwide-angle videos. Our system fuses and filters the outputs of camera pose\nestimation, stereo depth estimation, and temporal tracking methods into\nhigh-quality dynamic 3D reconstructions. We use this method to generate\nlarge-scale data in the form of world-consistent, pseudo-metric 3D point clouds\nwith long-term motion trajectories. We demonstrate the utility of this data by\ntraining a variant of DUSt3R to predict structure and 3D motion from real-world\nimage pairs, showing that training on our reconstructed data enables\ngeneralization to diverse real-world scenes. Project page:\nhttps://stereo4d.github.io",
  "url": "http://arxiv.org/abs/2412.09621v1",
  "issue_number": 52,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/52",
  "created_at": "2024-12-23T05:04:45.332202",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null
}