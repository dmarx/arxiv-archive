\subsection{Model-pretraining}\label{model-pretrain}
% \subsubsection{Overview}
We use Flow Matching~\cite{lipman2022flow} for model training and split the training process into multiple stages. We first pretrain our model on 256px and 512px images, then conduct joint training on images and videos from 256px to 960px. 
% Our training is composed of multiple stages. We first pretrain our model on 256px $\rightarrow$ 512px images, then conduct joint training on images and videos from 256px to 720px. 


\subsubsection{Training Objective}
% \kathy{better move this  }
In this work, we employ the Flow Matching framework~\cite{lipman2022flow,esser2024scaling,chen2018neural} to train our image and video generation model. Flow Matching transforms a complex probability distribution into a simple probability distribution through a series of variable transformations of the probability density function, and generates new data samples through inverse transformations.

During the training process, given an image or video latent representation ${\rm \bf{x}}_1$ in the training set. We first sample $t\in [0,1]$  from a logit-normal distribution~\cite{esser2024scaling} and initialize a noise ${\rm \bf{x}}_0 \sim \mathcal{N}({\rm \bf{0}}, {\rm \bf{I}})$ following Gaussion distribution. The training sample ${\rm \bf{x}}_t$ is then constructed using a linear interpolation method~\cite{lipman2022flow}. The model is trained to predict the velocity ${\rm \bf{u}}_t=d {\rm \bf{x}}_t/dt$, which guides the sample ${\rm \bf{x}}_t$ towards the sample ${\rm \bf{x}}_1$. The model parameters are optimized by minimizing the mean squared error between the predicted velocity ${\rm \bf{v}}_t$ and the ground truth velocity ${\rm \bf{u}}_t$, expressed as the loss function
\begin{equation}
  \label{eq:fm}
  \mathcal{L}_{{\rm generation}}=\mathbb{E}_{t,{\rm \bf{x}}_0,{\rm \bf{x}}_1}\|{\rm \bf{v}}_t - {\rm \bf{u}}_t  \|^2.
\end{equation}

During the inference process, a noise sample ${\rm \bf{x}}_0 \sim \mathcal{N}({\rm \bf{0}}, {\rm \bf{I}})$ is drawn initially. The first-order Euler ordinary differential equation (ODE) solver is then used to compute ${\rm \bf{x}}_1$ by integrating the model's estimated values for $d{\rm \bf{x}}_t/dt$. This process ultimately generates the final sample ${\rm \bf{x}}_1$.
% which reflects the characteristics of the target distribution. 
% These processes enable the model to learn the transformation from a simple distribution to a complex one during training and to generate new samples from the learned distribution during inference.

\subsubsection{Image Pre-training}
% \myPara{Image Pre-training}
% \dq{@Jian Wei, Zijian, Wuyue, Yanxin}
% We have validated the optimal network structure through detailed experiments including modulation module design, architecture of DiT block, different denosier scheme and text encoders. The overview of our network is shown in Fig.[]. 

At our early experiments, we found that a well pretrained model significantly accelerates the convergence of video training and improves video generation performance. Therefore, we introduce a two-stage progressive image pretraining strategy to serve as a warmup for video training. 

\textbf{Image stage 1 (256px training).} The model is first pretrained with low-resolution 256px images. Specifically, we follow previous work~\cite{podell2023sdxl} to enable multi-aspect training based on 256px, which helps the model learn to generate images with a wide range of aspect ratios while avoiding the text-image misalignments caused by the crop operation in image preprocessing. Meanwhile, pretraining with low resolution samples allows the model to learn more low-frequency concepts from a larger amount of samples. 

\textbf{Image stage 2 (mix-scale training).} We introduce a second image-pretraining stage to further facilitate the model ability on higher resolutions, such as 512px. A trivial solution is to directly funetuning on images based on 512px. However, we found that the model performance finetuned on 512px images will degrade severely on 256px image generation, which may affect the following video pretraining on 256px videos. Therefore, we propose \textit{mix-scale training}, where two or more scales of multi-aspect buckets are included for each training global batch. Each scale have an anchor size, and then the multi-aspect buckets are built based on the anchor size. We train the model on a two-scale dataset with anchor sizes 256px and 512px for learning higher resolution images while maintaining the ability on low resolutions. We also introduce dynamic batch sizes for micro batches with different image scales, maximaizing the GPU memory and computation utilization.


\subsubsection{Video-Image joint training}
% \myPara{Video-Image Joint-training}
% \dq{@Weijie, Tianqi, Daquan}

\textbf{Multiple aspect ratios and durations bucketization.}  After the data filtering process described in Section \ref{data_filtering}, the videos have different aspect ratios and durations. To effectively utilize the data, we categorize the training data into buckets based on duration and aspect ratio. We create $B_T$ duration buckets and $B_{AR}$ aspect ratio buckets, resulting in a total of $B_T \times B_{AR}$ buckets. As the number of tokens varies across buckets, we assign each bucket a maximum batch size that can prevent out-of-memory (OOM) errors, to optimize GPU resource utilization. Before training, all data is allocated to the nearest bucket. During training, each rank randomly pre-fetches batch data from a bucket. This random selection ensures the model is trained on varying data sizes at each step, which helps maintain model generalization by avoiding the limitations of training on a single size.

\textbf{Progressive Video-Image Joint Training.} Generating high-quality, long-duration video sequences directly from text often leads to difficulties in model convergence and suboptimal results. Therefore, progressive curriculum learning has become a widely adopted strategy for training text-to-video models. In \nameofmethod{}, we designed a comprehensive curriculum learning strategy, starting with model initialization using T2I parameters and progressively increasing video duration and resolution.

\begin{itemize}
\item \textbf{Low-resolution, short video stage.} The model establishes the basic mapping between text and visual content, ensuring consistency and coherence in short-term actions.

\item \textbf{Low-resolution, long video stage.} The model learns more complex temporal dynamics and scene changes, ensuring temporal and spatial consistency over a longer duration.

\item \textbf{High-resolution, long video stage.} The model enhances video resolution and detail quality while maintaining temporal coherence and managing complex temporal dynamics.
\end{itemize}

Additionally, at each stage, we incorporate images in varying proportions for video-image joint training. This approach addresses the scarcity of high-quality video data, enabling the model to learn more extensive and diverse world knowledge. It also effectively prevents catastrophic forgetting of image-space semantics due to distributional discrepancies between video and image data.

\subsection{Prompt Rewrite} 
To address the variability in linguistic style and length of user-provided prompts, we employ the Hunyuan-Large model \cite{sun2024hunyuanlargeopensourcemoemodel} as our prompt rewrite model to adapt the original user prompt to the model-preferred prompt. Utilized within a training-free framework, the prompt rewrite model capitalizes on detailed prompt instructions and in-context learning examples to enhance its performance. The key functionalities of this prompt rewrite module are as follows:

\begin{itemize}
    \item \textbf{Multilingual Input Adaptation}: The module is designed to process and comprehend user prompts across various languages, ensuring that meaning and context are preserved.
    \item \textbf{Standardization of Prompt Structure}: The module rephrases prompts to conform to a standardized information architecture, akin to training captions.
    \item \textbf{Simplification of Complex Terminology}: The module simplifies complex user wording into more straightforward expressions, all while maintaining the user's original intent.
\end{itemize}

Furthermore, we implement a self-revision technique \cite{kim2024reex} to refine the final prompt. This involves a comparative analysis between the original prompt and the rewritten version, ensuring that the output is both accurate and aligned with the model's capabilities.

To accelerate and simplify the application process, we also fine-tune a Hunyuan-Large model with LoRA for prompt rewriting. The training data for this LoRA tuning was sourced from the high-quality rewrite pairs collected through the training-free method.


\subsection{High-performance Model Fine-tuning}\label{sft}
In the pre-training stage, we utilized a large dataset for model training. While this dataset is rich in information, it displayed considerable variability in data quality. To create a robust generation model capable of producing high-quality, dynamic videos and improving its proficiency in continuous motion control and character animation, we carefully selected four specific subsets from the full dataset for fine-tuning.
These subsets underwent an initial screening using automated data filtering techniques, followed by manual review. Additionally, we implemented various model optimization strategies to maximize generation performance.

% The fine-tuning strategy adheres to a sequential process of independent training followed by model merging. Specifically, we first identified the optimal checkpoint in the pre-training stage, and then this checkpoint serves as the initialization parameter for subsequent fine-tuning of the model across the four designated subsets.
% Finally, we employ model merging based on the optimal pre-training model and the four fine-tuned models, and determine the final fusion model by performing a grid search on the fusion coefficient.

% \dq{@Tianqi, Weijie, Daquan}