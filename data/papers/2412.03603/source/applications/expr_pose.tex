\subsubsection{Fully-Controlled Whole-Body Avatar Generation} Controlling digital character's motion and expression explicitly has been a long-standing problem in both academia and industry, and recent advancement of diffusion models paved the first step to realistic avatar animation. However, current avatar animation solutions suffer from partial controllability due to limited capability of foundation video generation model. We demonstrate that a stronger T2V model boosts the avatar video generation to fully-controllable stage. We show how {\nameofmethod} serves as strong foundation with limited modifications to extent general T2V model to fully-controllable avatar generation model in Fig. \ref{fig:application-method} (c).

% \subsubsection{Pose-Driven} 
\paragraph{Pose-Driven}
We can control the digital character's body movements explicitly using pose templates. We use Dwpose~\cite{yang2023effective} to detect skeletal video from any source video, and use 3DVAE to transform it to latent space as $z_{\rm pose}$. We argue that this eases the fine-tuning process because both input and driving videos are in image representation, and are encoded with shared VAE, resulting same latent space. We then inject the driving signals to the model by element-wise add as $\hat{z}_t + z_{\rm pose}$. Note that $\hat{z}_t$ contains the appearance information of reference image. We use full-parameters finetune with pretrained T2V weights as initialization. 

% \subsubsection{Expression-Driven} 
\paragraph{Expression-Driven}
We can also control the facial expressions of digital character using implicit expression representations. Although facial landmarks are widely adopted in this area~\cite{ma2024follow, chen2024echomimic}, we argue using landmarks brings ID leak due to cross-ID misalignment. Instead, we use implicit representations as driving signals for their ID and expression disentanglement capabilities. In this work, we use VASA~\cite{xu2024vasa} as expression extractor. As shown in Fig. \ref{fig:application-method} (c), we adopt a light-weight expression encoder to transform the expression representation to token sequence in latent space as $z_{\rm exp} \in \mathbb{R}^{t \times n \times c}$, where $n$ is the number of tokens per frame. Typically, we set $n = 16$. Unlike pose condition, we inject $z_{\rm exp}$ using cross-attention because $\hat{z}_t$ and $z_{\rm exp}$ are not naturally aligned in spatial aspect. We add cross-attention layer ${\rm Attn_{exp}}(q,k,v)$ every $K$ double and single-stream DiT layers to inject expression latent. Denote the hidden states after $i$-th DiT layer as $h_{i}$, the injection of expression $z_{\rm exp}$ to $h_{i}$ could be derived as: $h_{i} + {\rm Attn_{exp}} (h_i, z_{\rm exp}, z_{\rm exp}) \ast \mathcal{M}_{\rm face}$, where $\mathcal{M}_{\rm face}$ is the face region mask that guides where $z_{\rm exp}$ should be applied at, and $\ast$ stands for element-wise multiplication. Also, full-parameters tuning strategy is adopted.


% \subsubsection{Hybrid Condition Driven} 
\paragraph{Hybrid Condition Driven}
Combining both pose and expression driven strategies derives hybrid control approach. In this scenario, the body motion is controlled by explicit skeletal pose sequence, and the facial expression is determined by implicit expression representation. We jointly fine-tune T2V modules and added modules in an end-to-end fasion. During inference, the body motion and facial motion could be controlled by separate driving signals, empowering richer editability. 