@article{li2023clipa,
  title={CLIPA-v2: Scaling CLIP Training with 81.1\% Zero-shot ImageNet Accuracy within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8\% Accuracy},
  author={Li, Xianhang and Wang, Zeyu and Xie, Cihang},
  journal={arXiv preprint arXiv:2306.15658},
  year={2023}
}

@article{anagnostidis2023navigating,
  title={Navigating Scaling Laws: Accelerating Vision Transformer's Training via Adaptive Strategies},
  author={Anagnostidis, Sotiris and Bachmann, Gregor and Hofmann, Thomas},
  journal={arXiv preprint arXiv:2311.03233},
  year={2023}
}

@article{izsak2021train,
  title={How to train BERT with an academic budget},
  author={Izsak, Peter and Berchansky, Moshe and Levy, Omer},
  journal={arXiv preprint arXiv:2104.07705},
  year={2021}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in {NLP}},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}

@article{dettmers2023qlora,
  title={{QLoRA}: Efficient finetuning of quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@misc{mccandlish2018empirical,
      title={An Empirical Model of Large-Batch Training},
      author={Sam McCandlish and Jared Kaplan and Dario Amodei and OpenAI Dota Team},
      year={2018},
      eprint={1812.06162},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/1812.06162},
      primaryClass={cs.LG}
}

@misc{molga2005test,
    title = {Test functions for optimization needs},
    author = {Marcin Molga and Czesław Smutnicki},
    year = {2005},
    howpublished = {\url{https://web.archive.org/web/20140204004638/http://www.zsd.ict.pwr.wroc.pl/files/docs/functions.pdf}},
    note = {Accessed: 28th August 2023},
}

@misc{peterson2012matrix,
    author       = "K. B. Petersen and M. S. Pedersen",
    title        = "The Matrix Cookbook",
    year         = "2012",
    month        = "nov",
    keywords     = "Matrix identity, matrix relations, inverse, matrix derivative",
    publisher    = "Technical University of Denmark",
    address      = "",
    note         = "Version 20121115",
    url          = "http://www2.compute.dtu.dk/pubdb/pubs/3274-full.html",
    abstract     = "Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices."
}

@book{goodfellow2016deep,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{stephan2017stochastic,
  author  = {Stephan Mandt and Matthew D. Hoffman and David M. Blei},
  title   = {Stochastic Gradient Descent as Approximate Bayesian Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {134},
  pages   = {1--35},
  url     = {http://jmlr.org/papers/v18/17-214.html}
}

@InProceedings{schaul13no,
  title = 	 {No more pesky learning rates},
  author = 	 {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {343--351},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/schaul13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/schaul13.html},
  abstract = 	 {The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.}
}

@book{provost1992quadratic,
  title={Quadratic Forms in Random Variables: Theory and Applications/ A.M. Mathai, Serge B. Provost},
  author={Provost, S.B. and Mathai, A.M.},
  series={Statistics : textbooks and monographs},
  url={https://books.google.ca/books?id=-SafoAEACAAJ},
  year={1992},
  publisher={Marcel Dek ker}
}


@misc{goodfellow2015efficient,
      title={Efficient Per-Example Gradient Computations},
      author={Ian Goodfellow},
      year={2015},
      eprint={1510.01799},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/1510.01799},
      primaryClass={stat.ML}
}

@misc{li2022large,
      title={Large Language Models Can Be Strong Differentially Private Learners},
      author={Xuechen Li and Florian Tramèr and Percy Liang and Tatsunori Hashimoto},
      year={2022},
      eprint={2110.05679},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2110.05679},
      primaryClass={cs.LG}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners},
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2005.14165},
      primaryClass={cs.CL}
}

@misc{ditzhaus2023inference,
      title={Inference for all variants of the multivariate coefficient of variation in factorial designs},
      author={Marc Ditzhaus and Łukasz Smaga},
      year={2023},
      eprint={2301.12009},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{vanvalen1974multivariate,
  title = {Multivariate structural statistics in natural history},
  journal = {Journal of Theoretical Biology},
  volume = {45},
  number = {1},
  pages = {235-247},
  year = {1974},
  issn = {0022-5193},
  doi = {https://doi.org/10.1016/0022-5193(74)90053-8},
  url = {https://www.sciencedirect.com/science/article/pii/0022519374900538},
  author = {Leigh {Van Valen}},
  abstract = {Some multivariate statistics usually used for structural purposes are inappropriate for this. The total variance and derived parameters, some new, measure multivariate variation. The two major properties of correlation must be generalized differently: the total correlation for joint dependence and the tightness. The term coregression refers to the trend of two mutually dependent variables. A generalization of information theory permits estimation of the redundance produced by correlations among many continuous variables. A new parameter, the modality, expresses the non-uniformity of a set of metric points. A suitable distance among populations can be determined in the general bivariate case and, approximately, in most realistic multivariate cases.}
}

 @misc{ratioestimator,
   author = "{Wikipedia contributors}",
   title = "Ratio estimator --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2023",
   url = "https://en.wikipedia.org/wiki/Ratio_estimator",
   note = "[Online; accessed 15-September-2023]"
 }

@article{choquet1999bootstrap,
    author = {Choquet, Denis and L'Ecuyer, Pierre and L\'{e}ger, Christian},
    title = {Bootstrap Confidence Intervals for Ratios of Expectations},
    year = {1999},
    issue_date = {Oct. 1999},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {9},
    number = {4},
    issn = {1049-3301},
    url = {https://doi.org/10.1145/352222.352224},
    doi = {10.1145/352222.352224},
    journal = {ACM Trans. Model. Comput. Simul.},
    month = {oct},
    pages = {326–348},
    numpages = {23},
    keywords = {confidence intervals, bootstrap, ratio estimation problem, regenerative simulation}
}

@misc{rochette2019efficient,
      title={Efficient Per-Example Gradient Computations in Convolutional Neural Networks},
      author={Gaspar Rochette and Andre Manoel and Eric W. Tramel},
      year={2019},
      eprint={1912.06015},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/1912.06015},
      primaryClass={cs.LG}
}

@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}

@misc{dey2023cerebrasgpt,
      title={Cerebras-{GPT}: Open Compute-Optimal Language Models Trained on the {C}erebras Wafer-Scale Cluster},
      author={Nolan Dey and Gurpreet Gosal and Zhiming and Chen and Hemant Khachane and William Marshall and Ribhu Pathria and Marvin Tom and Joel Hestness},
      year={2023},
      eprint={2304.03208},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2304.03208},
      primaryClass={cs.LG}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{graunt1676natural,
    title={Natural and Political Observations Mentioned in a following index made upon the Bills of Mortality},
    author={Graunt, John},
    year={1676},
    publisher={London: Printed by John Martyn, Printer to the Royal Society, at the Bell in St. Paul's Church-yard},
    edition={5th}
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models},
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rock2018einsum,
    title = {{EINSUM IS ALL YOU NEED - EINSTEIN SUMMATION IN DEEP LEARNING}},
    author = {Rocktäschel, Tim},
    year = {2018},
    howpublished = {\url{https://rockt.github.io/2018/04/30/einsum}},
    note = {Accessed: 10th November 2023},
}

@article{einstein1916foundation,
  title = {The Foundation of the General Theory of Relativity},
  author={Albert Einstein},
  journal={Annalen der Physik},
  volume={354},
  number={7},
  pages={769},
  year={1916},
  doi={10.1002/andp.19163540702}
}

@misc{rolnick2018deep,
      title={Deep Learning is Robust to Massive Label Noise},
      author={David Rolnick and Andreas Veit and Serge Belongie and Nir Shavit},
      year={2018},
      eprint={1705.10694},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wen2020batchensemble,
title={BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning},
author={Yeming Wen and Dustin Tran and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Sklf1yrYDr}
}

@misc{jiang2023accelerating,
      title={Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)},
      author={{Guo-qing} Jiang and Jinlong Liu and Zixiang Ding and Lin Guo and Wei Lin},
      year={2023},
      eprint={2309.13681},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{gray2023efficient,
    title={Efficient and Approximate Per-Example Gradient Norms for Gradient Noise Scale},
    author={Gavia Gray and Anshul Samar and Joel Hestness},
    booktitle={Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@NeurIPS 2023)},
    year={2023},
    url={https://openreview.net/forum?id=xINTMAvPQA}
}

@inproceedings{dangel2020backpack,
    title     = {Back{PACK}: Packing more into Backprop},
    author    = {Felix Dangel and Frederik Kunstner and Philipp Hennig},
    booktitle = {International Conference on Learning Representations},
    year      = {2020},
    url       = {https://openreview.net/forum?id=BJlrF24twB}
}

@misc{backpackpr,
    title = {Update from development into rnn \#188},
    author = {Felix Dangel and Tim Schäfer},
    year = {2021},
    howpublished = {\url{https://github.com/f-dangel/backpack/pull/188}},
    note = {Accessed: 24th March 2024},
}

@misc{zhang2019algorithmic,
    title={Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model},
    author={Guodong Zhang and Lala Li and Zachary Nado and James Martens and Sushant Sachdeva and George E. Dahl and Christopher J. Shallue and Roger Grosse},
    year={2019},
    eprint={1907.04164},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{hilton2022batch,
      title={Batch size-invariance for policy optimization},
      author={Jacob Hilton and Karl Cobbe and John Schulman},
      year={2022},
      eprint={2110.00641},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{opacus,
      title={Opacus: User-Friendly Differential Privacy Library in PyTorch},
      author={Ashkan Yousefpour and Igor Shilov and Alexandre Sablayrolles and Davide Testuggine and Karthik Prasad and Mani Malek and John Nguyen and Sayan Ghosh and Akash Bharadwaj and Jessica Zhao and Graham Cormode and Ilya Mironov},
      year={2022},
      eprint={2109.12298},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jouppi2023tpu,
      title={TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
      author={Norman P. Jouppi and George Kurian and Sheng Li and Peter Ma and Rahul Nagarajan and Lifeng Nai and Nishant Patil and Suvinay Subramanian and Andy Swing and Brian Towles and Cliff Young and Xiang Zhou and Zongwei Zhou and David Patterson},
      year={2023},
      eprint={2304.01433},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@misc{jouppi2017datacenter,
      title={In-Datacenter Performance Analysis of a Tensor Processing Unit},
      author={Norman P. Jouppi and Cliff Young and Nishant Patil and David Patterson and Gaurav Agrawal and Raminder Bajwa and Sarah Bates and Suresh Bhatia and Nan Boden and Al Borchers and Rick Boyle and Pierre-luc Cantin and Clifford Chao and Chris Clark and Jeremy Coriell and Mike Daley and Matt Dau and Jeffrey Dean and Ben Gelb and Tara Vazir Ghaemmaghami and Rajendra Gottipati and William Gulland and Robert Hagmann and C. Richard Ho and Doug Hogberg and John Hu and Robert Hundt and Dan Hurt and Julian Ibarz and Aaron Jaffey and Alek Jaworski and Alexander Kaplan and Harshit Khaitan and Andy Koch and Naveen Kumar and Steve Lacy and James Laudon and James Law and Diemthu Le and Chris Leary and Zhuyuan Liu and Kyle Lucke and Alan Lundin and Gordon MacKean and Adriana Maggiore and Maire Mahony and Kieran Miller and Rahul Nagarajan and Ravi Narayanaswami and Ray Ni and Kathy Nix and Thomas Norrie and Mark Omernick and Narayana Penukonda and Andy Phelps and Jonathan Ross and Matt Ross and Amir Salek and Emad Samadiani and Chris Severn and Gregory Sizikov and Matthew Snelham and Jed Souter and Dan Steinberg and Andy Swing and Mercedes Tan and Gregory Thorson and Bo Tian and Horia Toma and Erick Tuttle and Vijay Vasudevan and Richard Walter and Walter Wang and Eric Wilcox and Doe Hyun Yoon},
      year={2017},
      eprint={1704.04760},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@misc{zhao2023pytorch,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
      year={2023},
      eprint={2304.11277},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{einops,
title={Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},
author={Alex Rogozhnikov},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=oapKSVM2bcj}
}

@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{gokaslan2019owt,
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}},
	year={2019}
}

@misc{golden2024flash,
      title={Is Flash Attention Stable?},
      author={Alicia Golden and Samuel Hsia and Fei Sun and Bilge Acun and Basil Hosmer and Yejin Lee and Zachary DeVito and Jeff Johnson and Gu-Yeon Wei and David Brooks and Carole-Jean Wu},
      year={2024},
      eprint={2405.02803},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{byrd2012sample,
  title={Sample size selection in optimization methods for machine learning},
  author={Byrd, Richard H and Chin, Gillian M and Nocedal, Jorge and Wu, Yuchen},
  journal={Mathematical programming},
  volume={134},
  number={1},
  pages={127--155},
  year={2012},
  publisher={Springer}
}

@article{de2016big,
  title={Big batch {SGD}: Automated inference using adaptive batch sizes},
  author={De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  journal={arXiv preprint arXiv:1610.05792},
  year={2016}
}

@article{balles2016coupling,
  title={Coupling adaptive batch sizes with learning rates},
  author={Balles, Lukas and Romero, Javier and Hennig, Philipp},
  journal={arXiv preprint arXiv:1612.05086},
  year={2016}
}

@inproceedings{yin2018gradient,
  title={Gradient diversity: a key ingredient for scalable distributed learning},
  author={Yin, Dong and Pananjady, Ashwin and Lam, Max and Papailiopoulos, Dimitris and Ramchandran, Kannan and Bartlett, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1998--2007},
  year={2018},
  organization={PMLR}
}

@article{chen2018effect,
  title={The effect of network width on the performance of large-batch training},
  author={Chen, Lingjiao and Wang, Hongyi and Zhao, Jinman and Papailiopoulos, Dimitris and Koutris, Paraschos},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{yang2020feature,
  title={Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

@inproceedings{schaul2013no,
  title={No more pesky learning rates},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle={International conference on machine learning},
  pages={343--351},
  year={2013},
  organization={PMLR}
}

@article{kunstner2019limitations,
  title={Limitations of the empirical fisher approximation for natural gradient descent},
  author={Kunstner, Frederik and Hennig, Philipp and Balles, Lukas},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{faghri2020study,
  title={A study of gradient variance in deep learning},
  author={Faghri, Fartash and Duvenaud, David and Fleet, David J and Ba, Jimmy},
  journal={arXiv preprint arXiv:2007.04532},
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@article{nguyen2019first,
  title={First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise},
  author={Nguyen, Thanh Huy and Simsekli, Umut and Gurbuzbalaban, Mert and Richard, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  pages={5827--5837},
  year={2019},
  organization={PMLR}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{reddi2019convergence,
  title={On the convergence of {Adam} and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@inproceedings{agarwal2022estimating,
  title={Estimating example difficulty using variance of gradients},
  author={Agarwal, Chirag and D'souza, Daniel and Hooker, Sara},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10368--10378},
  year={2022}
}

@article{smith2017bayesian,
  title={A {Bayesian} perspective on generalization and stochastic gradient descent},
  author={Smith, Samuel L and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.06451},
  year={2017}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

@inproceedings{shazeer2018adafactor,
  title={{Adafactor}: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@misc{crowson2024kdiffusion,
  author = {Katherine Crowson},
  title = {k-diffusion},
  year = {2024},
  note = {GitHub repository},
  howpublished = {\url{https://github.com/crowsonkb/k-diffusion}},
  commit = {21d12c9}
}

@misc{eleuther2024gptneox,
  author = {EleutherAI},
  title = {GPT-NeoX},
  year = {2024},
  note = {GitHub repository},
  howpublished = {\url{https://github.com/EleutherAI/gpt-neox}},
  commit = {d037756}
}

@inproceedings{black2022gptneox,
  title = {{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
  author = {Black, Sidney and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, Usvsn Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  year = 2022,
  booktitle = {Proceedings of the ACL Workshop on Challenges {\&} Perspectives in Creating Large Language Models}
}

@article{crowson2024scalable,
  title={Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers},
  author={Crowson, Katherine and Baumann, Stefan Andreas and Birch, Alex and Abraham, Tanishq Mathew and Kaplan, Daniel Z and Shippole, Enrico},
  journal={arXiv preprint arXiv:2401.11605},
  year={2024}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{anil2019memory,
  title={Memory efficient adaptive optimization},
  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{yang2022mup,
  title = {{Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}},
  author = {Yang, Greg and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  year = 2021,
  booktitle = {Advances in Neural Information Processing Systems}
}

@article{shallue2019measuring,
  title={Measuring the effects of data parallelism on neural network training},
  author={Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={112},
  pages={1--49},
  year={2019}
}

@article{smith2018opt_einsum,
    doi = {10.21105/joss.00753},
    url = {https://doi.org/10.21105/joss.00753},
    year = {2018}, publisher = {The Open Journal},
    volume = {3},
    number = {26},
    pages = {753},
    author = {Daniel G. A. Smith and Johnnie Gray},
    title = {opt\_einsum - A Python package for optimizing contraction order for einsum-like expressions},
    journal = {Journal of Open Source Software}
}

@misc{zhang2019root,
      title={Root Mean Square Layer Normalization},
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{wortsman2023small,
      title={Small-scale proxies for large-scale Transformer training instabilities},
      author={Mitchell Wortsman and Peter J. Liu and Lechao Xiao and Katie Everett and Alex Alemi and Ben Adlam and John D. Co-Reyes and Izzeddin Gur and Abhishek Kumar and Roman Novak and Jeffrey Pennington and Jascha Sohl-dickstein and Kelvin Xu and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
      year={2023},
      eprint={2309.14322},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.14322},
}

@misc{dao2022flash,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135},
}

@misc{ba2022high,
      title={High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation},
      author={Jimmy Ba and Murat A. Erdogdu and Taiji Suzuki and Zhichao Wang and Denny Wu and Greg Yang},
      year={2022},
      eprint={2205.01445},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.01445},
}

@misc{karras2024edm2,
      title={Analyzing and Improving the Training Dynamics of Diffusion Models},
      author={Tero Karras and Miika Aittala and Jaakko Lehtinen and Janne Hellsten and Timo Aila and Samuli Laine},
      year={2024},
      eprint={2312.02696},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.02696},
}

@misc{miyato2018spectralnorm,
      title={Spectral Normalization for Generative Adversarial Networks},
      author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
      year={2018},
      eprint={1802.05957},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.05957},
}

@misc{zhai2023stabilizing,
      title={Stabilizing Transformer Training by Preventing Attention Entropy Collapse},
      author={Shuangfei Zhai and Tatiana Likhomanenko and Etai Littwin and Dan Busbridge and Jason Ramapuram and Yizhe Zhang and Jiatao Gu and Josh Susskind},
      year={2023},
      eprint={2303.06296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.06296},
}

@article{zhang2022adam,
  title={Adam can converge without any modification on update rules},
  author={Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={28386--28399},
  year={2022}
}

@article{kunstner2024heavy,
  title={Heavy-tailed class imbalance and why {Adam} outperforms gradient descent on language models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@article{kunstner2023noise,
  title={Noise is not the main factor behind the gap between {SGD} and {Adam} on transformers, but sign descent might be},
  author={Kunstner, Frederik and Chen, Jacques and Lavington, Jonathan Wilder and Schmidt, Mark},
  journal={arXiv preprint arXiv:2304.13960},
  year={2023}
}

