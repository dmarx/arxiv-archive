\begin{algorithm}
\caption{Layernorm Simultaneous Per-Example Gradient Norm Computation}
\label{alg:ln-layer}
\begin{algorithmic}[1]
\Require gradient tensor $\gB$ of shape $(B, ..., K)$, input activation tensor $\xB$ of shape $(B, ..., K)$
\Ensure gamma gradient tensor $\gammaB'$ of shape $(K,)$, mean of per-example squared norms $\sqn{\gammaB_b'}$, gradient tensor $\betaB'$ of shape $(K,)$, mean of per-example squared norms $\sqn{\betaB_b'}$
\State $\gammaB_b' \gets \text{einsum}(\mlq b...k,b...k \rightarrow bk \mrq, \xB, \gB)$
\State $\sB_{\gamma} \gets \text{einsum}(\mlq b k \rightarrow b \mrq, \gamma_b'^2)$
\State $\gammaB' \gets \text{einsum}(\mlq b k \rightarrow k \mrq, \gammaB_b')$
\State $\sqn{\gammaB_b'} \gets 1/B \times \text{einsum}(\sB_\gamma, \mlq b \rightarrow \mrq) \times B^2$ \# reduce by mean then apply correction
\State $\betaB_b' \gets \text{einsum}(\mlq b...k \rightarrow bk \mrq, \gB)$
\State $\sB_{\beta} \gets \text{einsum}(\mlq b k \rightarrow b \mrq, \betaB_b'^2)$
\State $\betaB' \gets \text{einsum}(\mlq b k \rightarrow k \mrq, \betaB_b')$
\State $\sqn{\betaB_b'} \gets 1/B \times \text{einsum}(\sB_\beta, \mlq b \rightarrow \mrq) \times B^2$ \# reduce by mean then apply correction
\State \Return $\gammaB', \sqn{\gammaB_b'}$, $\betaB', \sqn{\betaB_b'}$
\end{algorithmic}
\end{algorithm}
