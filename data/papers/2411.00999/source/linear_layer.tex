\begin{algorithm}
\caption{Linear Layer Simultaneous Per-Example Gradient Norm Computation}
\label{alg:linear-layer}
\begin{algorithmic}[1]
\Require gradient tensor $\gB$ of shape $(B, ..., L)$, input activation tensor $\xB$ of shape $(B, ..., K)$
\Ensure weight gradient tensor $\wB'$ of shape $(K, L)$, mean of per-example squared norms $\sqn{\wB_b'}$
\State $\wB_b' \gets \text{einsum}(\mlq b...k,b...l \rightarrow bkl \mrq, \xB, \gB)$
\State $\mathbf{s}_{w} \gets \text{einsum}(\mlq b k l \rightarrow b \mrq, \wB_b'^2)$
\State $\wB' \gets \text{einsum}(\mlq b k l \rightarrow k l \mrq, \wB_b')$
\State $\sqn{\wB_b'} \gets 1/B \times \text{einsum}(\mathbf{s}_w, \mlq b \rightarrow \mrq) \times B^2$ \# reduce by mean then apply correction
\State \Return $\wB', \sqn{\wB_b'}$
\end{algorithmic}
\end{algorithm}
