\section{Limitations}\label{limitations}

In this paper, we only studied Transformers, which include Normalization
sub-layers natively.  While Transformers are ubiquitous in machine
learning, there are many models, including variations of RNNs, CNNs,
and state-space models, that do not use such layers conventionally.
However, we note LayerNorm could be added to these networks with very
little overhead (in fact, the desire to normalize activations in RNNs
was one of the original motivations for developing LayerNorm;
application of batch normalization~\cite{ioffe2015batch} to RNNs was
``not obvious''~\cite{ba2016layer}).
Nevertheless, investigating LayerNorm-based \ac{gns} in these other
models requires further work.


Our work is also part of efforts to improve efficiency and address the
increasing costs of training and tuning large neural
networks~\cite{bender2021dangers}.
We provide both a more-efficient technique for computing the GNS, and
also, by enabling use of GNS statistics, we support compute-efficient
training recipes, such as use of dynamic batch sizes.
While some have argued that hyperscalers may re-invest any efficiency
savings into ever-larger models~\cite{patterson2021carbon}, for
academic researchers, such savings could allow pushing the
state-of-the-art, while still getting results in a reasonable
timeframe.
Recent efforts to enable frontier-model-performance within academic
budgets are encouraging, both to reduce
memory~\cite{malladi2023fine,dettmers2023qlora} and save compute
\cite{li2023clipa,anagnostidis2023navigating}.
Of course, even for such economical approaches, ``extensive
hyperparameter search'' may still be required~\cite{izsak2021train}.
There is a growing awareness that hyperparameter tuning has a negative
impact on equity in AI research, as tuning success depends directly on
researcher finances~\cite{strubell2019energy}.
A correlated trend is to use better training measurements (such as
gradient noise in batch and step size optimizers
(Section~\ref{related-work})) to reduce dependence on hyperparameters,
and in this way we hope our work can also ultimately improve research
equity.
