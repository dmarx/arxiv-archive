{
  "arxivId": "2411.00999",
  "title": "Normalization Layer Per-Example Gradients are Sufficient to Predict\n  Gradient Noise Scale in Transformers",
  "authors": "Gavia Gray, Aman Tiwari, Shane Bergsma, Joel Hestness",
  "abstract": "Per-example gradient norms are a vital ingredient for estimating gradient\nnoise scale (GNS) with minimal variance. Observing the tensor contractions\nrequired to compute them, we propose a method with minimal FLOPs in 3D or\ngreater tensor regimes by simultaneously computing the norms while computing\nthe parameter gradients. Using this method we are able to observe the GNS of\ndifferent layers at higher accuracy than previously possible. We find that the\ntotal GNS of contemporary transformer models is predicted well by the GNS of\nonly the normalization layers. As a result, focusing only on the normalization\nlayer, we develop a custom kernel to compute the per-example gradient norms\nwhile performing the LayerNorm backward pass with zero throughput overhead.\nTracking GNS on only those layers, we are able to guide a practical batch size\nschedule that reduces training time by 18% on a Chinchilla-optimal language\nmodel.",
  "url": "https://arxiv.org/abs/2411.00999",
  "issue_number": 427,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/427",
  "created_at": "2024-12-28T18:17:08.156757",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-28T18:08:42.178Z",
  "main_tex_file": null,
  "published_date": "2024-11-01T19:50:00Z",
  "arxiv_tags": [
    "cs.LG",
    "stat.ML",
    "I.2.6"
  ]
}