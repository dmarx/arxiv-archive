---
abstract: |
  Per-example gradient norms are a vital ingredient for estimating gradient noise scale (GNS) with minimal variance. Observing the tensor contractions required to compute them, we propose a method with minimal FLOPs in 3D or greater tensor regimes by simultaneously computing the norms while computing the parameter gradients. Using this method we are able to observe the GNS of different layers at higher accuracy than previously possible. We find that the total GNS of contemporary transformer models is predicted well by the GNS of only the normalization layers. As a result, focusing only on the normalization layer, we develop a custom kernel to compute the per-example gradient norms while performing the LayerNorm backward pass with zero throughput overhead. Tracking GNS on only those layers, we are able to guide a practical batch size schedule that reduces training time by 18% on a Chinchilla-optimal language model.
author:
- |
  Gavia Gray  
  Cerebras Systems  
  Toronto, Canada  
  `gavia.gray@cerebras.net`  
  Aman Tiwari  
  subjective.dev  
  London, UK  
  Shane Bergsma  
  Cerebras Systems  
  Toronto, Canada  
  Joel Hestness  
  Cerebras Systems  
  Sunnyvale, CA  
bibliography:
- references.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers
---





# Introduction

The gradients gathered during the backward pass while training a neural network are typically inspected via their Frobenius norm, the magnitude of the vector. This gradient vector may be viewed as the sum of gradients computed over each individual example in the minibatch. Each of these has its own norm. In this work, we develop a method to access these norms that works at any scale, for three common layer types in deep learning models: linear, normalization and embedding layers.

<figure id="fig:diagram">

<figcaption> Gradient noise scale (GNS) is typically computed by comparing per-minibatch (aggregated-across-layers) gradients to gradients “Aggregated” across minibatches. We estimate GNS with lower variance by making each minibatch a single example, and maintain per-layer GNS estimates. We find the magnitude of gradients (visualized by the length of red arrows) to be consistent across layers, enabling overall GNS to be computed very cheaply using only gradient stats from LayerNorm layers.</figcaption>
</figure>

One primary application of a per-example gradient norm is in estimating the <span acronym-label="gns" acronym-form="singular+short">gns</span> , a metric that has been shown to be useful in training large scale models . The uncertainty of the <span acronym-label="gns" acronym-form="singular+short">gns</span> estimator depends directly on the size of the batch used to compute the small batch gradient norm as shown in Section . So, the most precise estimate of the <span acronym-label="gns" acronym-form="singular+short">gns</span> is obtained by computing the gradient norms for *each* example in the minibatch: the per-example gradient norm.

To demonstrate <span acronym-label="gns" acronym-form="singular+short">gns</span> measurement in practice we perform experiments on contemporary language model architectures, providing a detailed visualisation of the movement of the <span acronym-label="gns" acronym-form="singular+short">gns</span> components throughout training, presented in Section . By inspecting these components it was found that the <span acronym-label="gns" acronym-form="singular+short">gns</span> of the model is highly correlated between layer types, which we give an intuition for in Figure .

However, the practical utility of measuring <span acronym-label="gns" acronym-form="singular+short">gns</span> with per-example gradient norms is only present if it can be gathered without affecting training time. Focusing on LayerNorm  layers, we note the main speed bottleneck is the memory I/O when not implemented as a fused kernel. To demonstrate this, we develop a custom kernel to compute both the backward pass and the per-example gradient norms at the same time. Using this kernel the throughput overhead of gathering the per-example gradient is zero, even outperforming PyTorch’s LayerNorm at larger dimensions. We apply this to a practical batch size schedule case study in Section .

To reiterate, the contributions of this work are:

- A minimal FLOP algorithm and implementation for computing gradients and per-example gradient norms of linear layers simultaneously.[^1]

- Observations that the measured <span acronym-label="gns" acronym-form="singular+short">gns</span> for LayerNorm layers is highly correlated with the GNS of the remaining layers.

- Development of an example kernel to implement tracking the <span acronym-label="gns" acronym-form="singular+short">gns</span> of LayerNorm layers that does not affect network throughput (tokens/sec).

- Demonstration of a real application of <span acronym-label="gns" acronym-form="singular+short">gns</span> tracking in a batch size schedule experiment that obtains an 18% wall-time speedup in training a Chinchilla-optimal  LLM.

# Background

## Gradient Noise Scale

<span acronym-label="gns" acronym-form="singular+short">gns</span> is a metric derived from observing a second order Taylor expansion of the change in a loss function under the following assumption on the noise in the gradient estimate , $$\begin{aligned}
    G_{\text{est}(\theta)} \sim \mathcal{N} \left(G (\theta), \frac{1}{B} \Sigma (\theta) \right),
\end{aligned}$$ where $G_{\text{est}}$ is the observed gradient, $B$ is the batch size, and $\theta$ the parameters of the model. Here, $G$ is the unobserved “true” gradient and $\Sigma$ is the covariance of the gradient estimate. The Taylor expansion mentioned is, $$\begin{aligned}
    \mathbb{E}[L(\theta - \epsilon G_{est})] = L(\theta) - \epsilon |G|^2 + \frac{1}{2} \epsilon^2 \left( G^T HG + \frac{tr(H\Sigma)}{B} \right).
\end{aligned}$$ Where $\epsilon$ is the learning rate and $H$ is the Hessian of the loss. On the right hand side is a factor that depends on $B$. It may be shown  that the optimal step size and optimal change in the loss is achieved when $B = \mathcal{B}_{\textrm{noise}}:= tr(H \Sigma) / G^T H G$. Averaging this optimal step over an entire run, and measuring this value by a grid search, yields $\mathcal{B}_{\textrm{crit}}$ which describes a batch size that meets an optimal tradeoff between cost and training speed. It is shown by analysis and experiment that $\mathcal{B}_{\textrm{noise}}\approx \mathcal{B}_{\textrm{crit}}$.

As this depends on the Hessian, which is typically unavailable, suggest making the assumption that the Hessian is diagonal, which yields $$\begin{aligned}
    \mathcal{B}_{\textrm{simple}}= \frac{tr(\Sigma)}{G^T G}.
\end{aligned}$$

To compute $\mathcal{B}_{\textrm{simple}}$ define the unbiased estimators $\mathcal{S}$ and $\left\lVert\mathcal{G}\right\rVert_2^2$ as: $$\begin{aligned}
    \left\lVert\mathcal{G}\right\rVert_2^2 := \frac{1}{B_{\textrm{big}}- B_{\textrm{small}}} \left( B_{\textrm{big}}\left\lVert G_{B_{\textrm{big}}}\right\rVert_2^2 - B_{\textrm{small}}\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2 \right) \approx G^T G \label{eq:g-est}\\
    \mathcal{S} := \frac{1}{1 / B_{\textrm{small}}- 1 / B_{\textrm{big}}} \left( \left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2 - \left\lVert G_{B_{\textrm{big}}}\right\rVert_2^2 \right) \approx tr(\Sigma),
    \label{eq:s-est}
\end{aligned}$$ where $B_{\textrm{big}}$ and $B_{\textrm{small}}$ are the batch sizes used to compute the gradients $G_{B_{\textrm{big}}}$ and $G_{B_{\textrm{small}}}$, respectively (potentially corresponding to *Aggregated* and *Minibatch* gradients as depicted in Figure ).

$\left\lVert G_{B_{\textrm{big}}}\right\rVert_2$ is trivially computed using the gradients accumulated for the optimizer but $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2$ is not. One option is to use the gradients communicated between <span acronym-label="ddp" acronym-form="singular+short">ddp</span> nodes, but this has two downsides: (1) the variance of the estimate is tied to the <span acronym-label="ddp" acronym-form="singular+short">ddp</span> configuration and (2) the estimate is not available in all training configurations. For example, experiments on a single GPU cannot use this method. One can also access the gradients during gradient accumulation, but this similarly depends on the training configuration. A full taxonomy of the options for computing $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2$ is provided in Appendix .

For each observation of $\left\lVert G_{B_{\textrm{big}}}\right\rVert_2$ we may observe multiple $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2$, typically $B_{\textrm{big}}/ B_{\textrm{small}}$ of them. On each step the estimate of $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ is therefore a mean over $B_{\textrm{big}}/ B_{\textrm{small}}$ samples, whose variance is reduced according to the law of large numbers. However, the GNS is a ratio of the unbiased estimators in Equations and , so it may not be clear how this affects uncertainty in the <span acronym-label="gns" acronym-form="singular+short">gns</span> estimate. Figure  explores this relationship by simulation of a setting where the <span acronym-label="gns" acronym-form="singular+short">gns</span> is set to 1 while varying $B_{\textrm{big}}$ and $B_{\textrm{small}}$. We find it is always better (less uncertainty) to use the smallest possible $B_{\textrm{small}}$ to estimate the <span acronym-label="gns" acronym-form="singular+short">gns</span>, while the choice of $B_{\textrm{big}}$ is irrelevant.

<figure id="fig:variance">
<figure>
<p><span class="image placeholder" data-original-image-src="output_figures/jackknife_Bbig.pdf" data-original-image-title="" width="\textwidth">image</span> </p>
</figure>
<figure>
<p><span class="image placeholder" data-original-image-src="output_figures/jackknife_Bsmall.pdf" data-original-image-title="" width="\textwidth">image</span> </p>
</figure>
<figcaption> The variance of the <span data-acronym-label="gns" data-acronym-form="singular+short">gns</span> estimator for different <span class="math inline">\(B_{\textrm{big}}\)</span> (left) and <span class="math inline">\(B_{\textrm{small}}\)</span> (right) sizes. <span class="math inline">\(B_{\textrm{big}}= l\)</span> and <span class="math inline">\(B_{\textrm{small}}= s\)</span> in legends. Stderr is estimated using a jackknife resampling method for ratio estimators <span class="citation" data-cites="choquet1999bootstrap"></span>. For the same number of samples processed, a smaller <span class="math inline">\(B_{\textrm{small}}\)</span> always has a lower standard error, while the size of the large batch, <span class="math inline">\(B_{\textrm{big}}\)</span> does not affect the standard error.</figcaption>
</figure>

## Efficient Per-example Gradient Norms

proposes a trick to compute gradient norms for individual examples in a minibatch, which would provide the minimum variance estimate of the <span acronym-label="gns" acronym-form="singular+short">gns</span> as described in Section . Neglecting the original derivation, by writing the desired squared norm as a tensor contraction the trick may be reproduced automatically via `einsum` path optimization . The tensor contraction for per-example gradient norms, $n_{b}^2$, of a linear layer in the 2D setting is, $$n_{b}^2 = \sum_{i,k} (w')^2_{bik} = \sum_{i, k} x_{bi} x_{bi} y'_{bk} y'_{bk},$$ where $x$ are the activations prior to a linear layer, $y'$ are the gradients of the loss with respect to the outputs of the linear layer and $w'$ are the gradients of the loss with respect to the weights of the linear layer.

extend this trick to the three dimensional case. For inputs $\XB \in \mathbb{R}^{B \times T \times I}$ and outputs $\YB \in \mathbb{R}^{B
\times T \times K}$, the per-example gradient norm $n_{b}$ is, $$n_{b}^2 = (w')^2_{bik} = (\sum_t x_{bti} y'_{btk})^2 = x_{bti} y'_{btk} x_{bui} y'_{buk} = \langle \XB \XB^T, \YB' \YB'^T \rangle_F^2,$$ which has $O(T^2)$ memory complexity in the sequence length $T$.[^2]. Index sets are $b \in [1,B], \; i \in [1,I], \; k \in [1,K], \; t,u \in [1,T]$. At some point, the I/O cost of computing the per-example gradient norms by computing the full $w'_b$ explicitly will be cheaper. Noting this fact motivated the work in Section  and the practical relationship between these resource costs is explored in Section .

## Related Work

#### Gradient norms

One common motivation for computing per-example gradient norms is for differential privacy. By bounding the gradient for any single example, we can ensure each example has a limited impact on the final parameters . Per-example gradient clipping has been performed with convolutional networks  and sequential models, e.g., LLMs . These methods allow control over per-example gradient norms even when training with large batch sizes. Approaches like these are implemented in the differential-privacy library Opacus , and have support natively in PyTorch, but are less efficient than the methods proposed in this paper. An alternative mechanism to manifest per-example gradient norms is to simply use a batch size of one. While not efficient enough for training large-scale networks, such sequential training may arise in situations such as reinforcement learning, where per-example gradient clipping has also been performed (to improve stability ).

#### Gradient noise scale

The <span acronym-label="gns" acronym-form="singular+long">gns</span>  has been widely used for training large-scale neural networks. For example, note the <span acronym-label="gns" acronym-form="singular+short">gns</span> was measured during training and used to guide batch sizing when training GPT-3. mention that operating near the critical batch size, as dictated by the <span acronym-label="gns" acronym-form="singular+short">gns</span>, is important for hyperparameter transfer under the maximal update parameterization . Even when not explicitly mentioned in publications, open source code often implements the <span acronym-label="gns" acronym-form="singular+short">gns</span> (e.g., see codebases  for GPT-NeoX  and Hourglass Diffusion Transformer ).

Measurements similar to the <span acronym-label="gns" acronym-form="singular+short">gns</span> have also been used in a range of prior work to guide batch sizing for minibatch SGD . show experimentally that wider networks can be trained using larger batches; they also establish a theoretical connection between wider networks and gradient variance, albeit for simple two-layer networks. In contrast, found empirically that *narrower* Transformers scale better to larger batch sizes. propose a noise scale based not on gradient variance, but on the learning rate, dataset size, and batch size (similar to the notion of temperature in Section ). find the critical batch size depends on the choice of optimizer. introduce a gradient clustering and stratified sampling approach to minimize minibatch gradient variance, and use this approach as a tool to help understand optimization.

#### Gradient variance

Beyond computing the <span acronym-label="gns" acronym-form="singular+short">gns</span>, our method can support other applications where measuring the distribution of per-example gradients is useful or informative. Gradient variance has been used to classify the *difficulty* of examples , which can be used, for example, to surface problematic examples for human auditing. The question of whether gradient distributions tend toward Gaussian in the (central) limit is of theoretical significance , with implications toward the ability of SGD to escape sharp minima and land in wide basins . Bounded gradient variance is also assumed in some convergence analysis , as noted in .

Perhaps the most familiar use of gradient variance is of course in adaptive optimizers like Adagrad, Adam, and others that reduce step sizes in high-gradient-noise directions . directly relate Adam second moment statistics to a *component-wise* version of the <span acronym-label="gns" acronym-form="singular+short">gns</span>. Optimizers typically estimate gradients jointly across training steps and minibatches, however vSGD  leverages separate components for gradient momentum and for gradient variation across samples. find the variance of gradient norms across examples predictive of whether vanilla SGD outperforms adaptive optimizers, however recent work has shown Adam to outperform SGD even in the (noise-free) full gradient descent setting .

# Simultaneous Per-example Gradient Norms

As described in Section , computing <span acronym-label="gns" acronym-form="singular+short">gns</span> requires small batch gradient norms. Typically, these may be gathered during gradient accumulation or <span acronym-label="ddp" acronym-form="singular+short">ddp</span> communication.[^3] However, these methods are not universally applicable and may not be available in all training configurations. In this section we describe a method for baking the computation of the per-example gradient norms into the computation graph, making it universally applicable. The typical tensor contraction used to compute the backward gradient in a linear layer using the input activations, $\xB$, and gradients, $\gB$, is, $$w_{k, l}' = \sum x_{\ldots k} g_{\ldots l},$$ in other words, a sum over vector outer products for every vector in the trailing dimension. In principle, it is possible to access the intermediate tensor containing the batch dimension $w_{b k l}' = \sum
x_{b \ldots k} g_{b \ldots l}$. This allows us to compute the per-example gradient norms with FLOPs scaling at the same rate as the normal, non-per-example backward pass (Figure ), albeit at increased I/O cost due to having to materialize the intermediate tensor.

A generic algorithm to compute the per-example gradient norms simultaneously with the weight gradient in a standard linear layer is provided in Algorithm  using `einsum` for readability and portability.[^4] The reason for the correction in step 4 can be seen by considering the gradient of loss function $L$ with respect to the weights on a single example $b$, $w_b$, $$\nabla_{w_b} \frac{1}{B} \sum_b L(x_b) = \frac{1}{B} \nabla_{w_b} L(x_b),$$ computing the squared norm of this will therefore contain a factor of $1/B^2$, which must be corrected for.

## FLOPs and I/O Costs

The computational cost of computing per-example gradient norms can be broken down into FLOPs, in Figure , and I/O, in Figure , with matrix multiplication on current devices being potentially bottlenecked by both. We estimate ideal FLOP and DRAM I/O costs, assuming optimal reuse of data loaded from DRAM into SRAM with no recomputation. In practice, duplicate computation may be used to improve wall-clock time and to fit within hardware limitations of the amount of shared memory available. We compare here against the efficient per-example gradient norm method described by , which the authors note is only efficient (in terms of I/O cost) when $2 T^2 < P D$, where $T$ is the sequence length, $P$ is input and $D$ is output dimension of the linear layer. This bound is discussed further in Appendix .

In terms of FLOPS, Figure  shows the simultaneous per-example gradient norms are almost always preferable, only being more expensive for very short sequence lengths in small models. The reason for this is shown on the right hand side; the number of FLOPs required to compute the simultaneous per-example gradient norms is independent of the sequence length.

<figure id="fig:gns-flops">
<p><span class="image placeholder" data-original-image-src="figures/total-flops-grads-and-gns.pdf" data-original-image-title="" width="0.45\linewidth">image</span>  <span class="image placeholder" data-original-image-src="figures/prop-flops-grads-and-gns.pdf" data-original-image-title="" width="45%">image</span></p>
<figcaption> FLOP cost of computing per-example gradient norms. (Left) Total FLOP cost. (Right) Proportional cost versus one model forward and backward pass. The FLOP cost of Simultaneous per-example gradient norms is strictly dominant to alternative methods (left) and the ratio of this additional cost to the FLOP cost of processing the entire model does not depend on context length (right).</figcaption>
</figure>

The I/O cost shown in illustrates a tradeoff in computing the per-example gradient norm. The simultaneous method is more expensive at large model sizes with short sequence length because it must act on a large intermediate tensor.

<figure id="fig:total-gns-io">
<span class="image placeholder" data-original-image-src="figures/total-io-grads-and-gns.pdf" data-original-image-title="" width="0.6\linewidth"></span>
<figcaption> Total I/O cost of computing per-example gradient norms, assuming gradients and parameters are stored with 4 bytes of precision. The relative IO cost of Simultaneous per-example gradient norms is less than <span class="citation" data-cites="li2022large"></span> for very long contexts for all model scales, approximately equivalent for models of 10B parameters and 4096 context length, and higher for shorter contexts with larger models. The IO cost of LN (LayerNorm) per-example gradient norms alone is much lower than either method.</figcaption>
</figure>

To estimate model flops, we use PyTorch’s FLOPCounterMode, which only measures the FLOPs in matrix multiplications and attention computation, however these make up the vast majority of the FLOPs in a Transformer model.

# Gradient Noise Scale in Transformer Language Models

Using the methods described in previous sections to measure per-example gradient norms and estimate the <span acronym-label="gns" acronym-form="singular+short">gns</span>, we perform experiments on a 111M parameter Chinchilla-optimal language model  using the OpenWebText dataset .[^5] As the prior work was performed on Pile , Appendix  describes an experiment to check the optimality of the Chinchilla model on this dataset. We also found Flash attention led to numerical instability, which we were able to mitigate with an architectural modification described in Appendix .

All experiments computed per-example gradient norms for all layers in the model with the exception of the performance results of Sections  and , which only computed per-example gradient norms for the normalization layers. Each experiment was run on Nvidia A10 GPUs, in either 12 or 24 hours depending on the precision used, Bfloat16 or Float32 respectively. We used the nanoGPT[^6] codebase with the layers described in Section  added.

Having an accurate estimate of the <span acronym-label="gns" acronym-form="singular+short">gns</span> statistics $\left\lVert\mathcal{G}\right\rVert_2^2$ and $\mathcal{S}$ allows us to visualize the movement of both in a phase space during training as shown in Figure . LayerNorm layers are separate from the rest of the network because their statistics are much smaller and to illustrate how the resulting GNS estimates on the right track each other. To observe these trends in another training regime, see Figure  in Appendix .

<figure id="fig:gns_by_index">
<span class="image placeholder" data-original-image-src="output_figures/gns_vs_trace.layer_idx.pdf" data-original-image-title="" width="80%"></span>
<figcaption> GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row. Component estimators of Equations <span class="math inline">\(\ref{eq:g-est}\)</span> and <span class="math inline">\(\ref{eq:s-est}\)</span> are shown (left) with the GNS over the course of training on the (right).</figcaption>
</figure>

## The Temperature of Training

observed that the <span acronym-label="gns" acronym-form="singular+short">gns</span> measurement depends on the batch size and learning rate used in training. In fact, from the derivation outlined in Section , the gradient noise scale is only well-defined at the optimal learning rate. Using a toy model of a quadratic loss function, they observed that the <span acronym-label="gns" acronym-form="singular+short">gns</span> should be inversely proportional to the temperature, $T$, a ratio of batch size $B$ to learning rate $\epsilon$: $$\mathcal{B}_{\textrm{noise}}\propto \mathcal{B}_{\textrm{simple}}\propto \frac{1}{T} = \frac{B}{\epsilon}.$$ This enables a testable prediction that the <span acronym-label="gns" acronym-form="singular+short">gns</span> will increase with increasing batch size or with descending learning rate. This prediction was found to accurately describe experiments on a small convolutional model on the SVHN dataset. We repeat it here in the setting described above in Figure . To match the results of , all interventions tested should yield the same result. We find the <span acronym-label="gns" acronym-form="singular+short">gns</span> does indeed react predictably to changes in the learning rate, but the reactions to changes in the batch size are not predicted by the theory.

<figure id="fig:temp">
<span class="image placeholder" data-original-image-src="figures/temp.pdf" data-original-image-title="" width="60%"></span>
<figcaption> During the middle of training a 111M parameter language model on OpenWebText, the learning rate, <span class="math inline">\(\epsilon\)</span> or batch size, <span class="math inline">\(B\)</span> were varied, restarting the run from the same point. This Figure replicates an experiment from <span class="citation" data-cites="mccandlish2018empirical"></span> showing how varying the ratio causes changes in the measured GNS, but here only due to changes in the learning rate. Changes in the batch size do not have the predicted effect.</figcaption>
</figure>

## GNS Correlates Between Layer Types

Inspection of Figure  suggests the LayerNorm layers produce a similar GNS, when combined, as the total GNS of the model. Before describing how to quantify this relationship we must first note that the unbiased estimators $\left\lVert\mathcal{G}\right\rVert_2^2$ and $\mathcal{S}$ are noisy. All <span acronym-label="gns" acronym-form="singular+short">gns</span> figures presented in this paper and other work smooth both of these estimators, typically with an <span acronym-label="ema" acronym-form="singular+short">ema</span> filter, before computing the <span acronym-label="gns" acronym-form="singular+short">gns</span> ratio.[^7]

So, when quantifying the relationship between the GNS of different layers, it must be compared for different smoothing factors. Here, we show the regression coefficients with respect to the alpha of the <span acronym-label="ema" acronym-form="singular+short">ema</span> filter in Figure . The results show that the GNS of the LayerNorm and Attention layers are highly predictive of the total GNS of the model. In both cases, the slope is approximately 1.4, meaning the total GNS is approximately 1.4 times the GNS of the LayerNorm or Attention layers.

Comparing the quality of this fit versus the quality of prior work’s overall fit of the <span acronym-label="gns" acronym-form="singular+short">gns</span> to the critical batch size (measured empirically) , the quality seems acceptable and we do not need to apply this 1.4x correction factor, rather we just note that the true $\mathcal{B}_{\textrm{crit}}$ may be greater than the measured $\mathcal{B}_{\textrm{simple}}$.

<figure id="fig:regress_gns">
<span class="image placeholder" data-original-image-src="owt_figures/regress_gns.pdf" data-original-image-title="" width="80%"></span>
<figcaption> Regression of total GNS using the GNS of each layer type. (Left) GNS of each layer type and the total GNS are plotted against the number of tokens processed for varying <span data-acronym-label="ema" data-acronym-form="singular+short">ema</span> alpha settings. (Center &amp; Right) The slope and Pearson’s correlation coefficient of the regression of the total GNS against the GNS of each layer type, respectively, as a function of the same <span data-acronym-label="ema" data-acronym-form="singular+short">ema</span> alpha values. The total GNS (black) on the left is predicted well by individual layer types as indicated by the correlation coefficients (right), however the type with slope closest to 1 is LayerNorm (center), only overestimating the GNS by less than 40% across EMA alpha values.</figcaption>
</figure>

# Batch Size Scheduling

We focus on two concerns that affect the practicality of batch size scheduling. First, measuring the appropriate batch size without incurring any additional training time. We find this is possible with the method described in Section . Second, whether batch size scheduling is effective in practice. We find it can offer significant savings in the required number of tokens processed in Section .

## Universal GNS with Zero Overhead

Capturing a GNS estimate for a linear layer is powerful, but efficiently doing so presents a challenge. Such an estimate requires accumulating per-example gradients of $\text{hidden\_size}^2$ across the sequence dimension, compared to just $\text{hidden\_size}$ with LayerNorm. This increased size requires using more complex reductions in the kernel, rather than a simple warp reduction followed by shared-memory atomic reduction with a final atomic global reduction (as we can implement for LayerNorm per-example gradients within shared memory). In addition, linear layer kernels are already highly optimized and require using advanced techniques to keep GPU tensor cores fed with data, so combining such a kernel with per-example gradient computation - with its own memory overheads and corresponding available bandwidth reduction - would be a difficult undertaking.

We thus implemented a LayerNorm-specific CUDA kernel that also captures GNS. In experiments with language models at different scales, illustrated in Figure , we find this kernel has practically zero overhead compared to PyTorch’s LayerNorm implementation. The complete source code for this kernel is provided with the accompanying code for this paper[^8].

<figure id="fig:pt-vs-kernel">
<span class="image placeholder" data-original-image-src="figures/ln-vs-kernel-time.pdf" data-original-image-title="" width="60%"></span>
<figcaption> Comparison of average time taken for a LayerNorm forward and backward pass with gradient accumulation when using PyTorch’s native implementation versus our custom kernel computing per-example gradient norms in tandem. Measured on an Nvidia H100 GPU.</figcaption>
</figure>

## Case Study: Batch Size Schedule

As a case study we continue with the 111M parameter language model on OpenWebText described above. Over three seeds, we run both a fixed batch size and a batch size schedule that increases linearly with the number of tokens processed to the original batch size. We vary the batch size during training by varying the number of gradient accumulation steps.

The results of this experiment are shown in Figure . The left plot shows the progression of the loss for both models, with the range of values captured over different seeds. The mean loss for the linear batch size schedule leads the fixed batch size throughout training. On the right, this lead is quantified by interpolating the number of tokens saved to achieve the same loss. The precise schedule used is shown in Figure  in Appendix .

<figure id="fig:111M_bss">
<span class="image placeholder" data-original-image-src="figures/bs_sched.pdf" data-original-image-title="" width="\textwidth"></span>
<figcaption> (Left) Linear batch size schedule tracking the GNS over 2.2 billion tokens processed. Loss is plotted over a smoothed range from 3 runs using different Seeds. (Right) The number of tokens saved over the fixed batch size run to achieve the same loss.</figcaption>
</figure>

# Conclusion

This work set out to provide a practical method for computing the per-example gradient norms necessary to compute the <span acronym-label="gns" acronym-form="singular+short">gns</span> independent of the training configuration. In the process we discovered that not all the layers are necessary for a practical estimate of the <span acronym-label="gns" acronym-form="singular+short">gns</span> and that the per-example gradient norms can be computed for the normalization layers with zero overhead. This enabled practical experiments, such as a batch size schedule and replicating prior <span acronym-label="gns" acronym-form="singular+short">gns</span> observations. We are hopeful that democratising access to <span acronym-label="gns" acronym-form="singular+short">gns</span> statistics, on any device, will enable subsequent discoveries.

# Taxonomy

included an prior version of this taxonomy in their work.

The following taxonomy describes the different methods available to compute the $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ necessary to compute the <span acronym-label="gns" acronym-form="singular+short">gns</span> as described in Section . “Gradient norm cost” below refers to the cost of computing the norm of the gradient for all parameters in the model, which is typically orders of magnitude smaller than the cost of forward or backward passes.

- Microbatch: multiple $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ are computed over a set of microbatches

  - DDP: Each $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ is computed before gradients are communicated between <span acronym-label="ddp" acronym-form="singular+short">ddp</span> nodes .

    <span style="color: teal">Pros:</span> Only gradient norm cost.

    <span style="color: orange">Cons:</span> Variance tied to number of DDP nodes (see Figure ), can’t be used on one node.

  - Sequential: Each $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ are computed sequentially during gradient accumulation.

    <span style="color: teal">Pros:</span> Only gradient norm cost.

    <span style="color: orange">Cons:</span> Variance tied to the number of gradient accumulation steps.

- Subbatch: During gradient accumulation, select $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ partway through.

  <span style="color: teal">Pros:</span> Only gradient norm cost, easy to implement.

  <span style="color: orange">Cons:</span> Higher variance than Microbatch as $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ is not averaged.

- Per-example:

  <span style="color: teal">Pros:</span> Independent of gradient accumulation or DDP configuration, minimal variance.

  - Exact:

    - $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ is computed directly by the per-example gradient trick .

      <span style="color: teal">Pros:</span> Minimal cost in 2D regime.

      <span style="color: orange">Cons:</span> Redundant computation required in 3D regime.

    - $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ is computed in tandem with the parameter gradients using the method described in Section .

      <span style="color: teal">Pros:</span> No redundant computation.

      <span style="color: orange">Cons:</span> Expansion in memory causes slowdowns as described in Section .

  - Approximation: $\left\lVert G_{B_{\textrm{small}}}\right\rVert_2^2$ is approximated by assuming input activations are normally distributed with mean zero .

    <span style="color: teal">Pros:</span> Fewer FLOPs than Exact methods.

    <span style="color: orange">Cons:</span> Not exact.

All of the methods described above can be measured either online or offline. The description above focuses on the online case; i.e. measuring the gradient norms during training. To use these methods offline: run the models without performing weight updates and measure gradient norms the same way. The estimators of Equation  and  can then be aggregated using a mean rather than an EMA or by using a method to estimate measurement uncertainty such as the jackknife mentioned in Figure  (described in the context of <span acronym-label="gns" acronym-form="singular+short">gns</span> by ). This can be useful to estimate how long to run the offline estimate.

# Additional Simultaneous Per-Example Gradient Norm Computations

Algorithms  and  describe the process for computing the per-example gradient norms for the embedding and LayerNorm layers, which are typically the remaining layers in Transformer models. RMSNorm  is practically identical to LayerNorm in this case because the parameters the gradient is computed wrt are in the affine transform, which is the same in both layer types.

# Language Model Experiment Details

As mentioned in the text, the code to run the experiments described in this paper can be found at <https://github.com/CerebrasResearch/nanoGNS/tree/main/exact>.

## Optimality on OpenWebText

We chose to use the Cerebras-GPT  recipes for experiments as they are designed to be Chinchilla optimal. This means that each model size should achieve the lowest possible loss for a given FLOP budget . However, these recipes were tuned on the Pile dataset  and we used the OpenWebText dataset  so that results could be replicated (Pile is no longer publicly available).

To verify that the training protocol is optimal on OpenWebText, we performed a small study to illustrate how the performance would vary as we vary the size and total tokens trained on. Model size was varied by changing the hidden size: the 70M model has a hidden size of 576, the 111M model has a hidden size of 768 and the 161M model has a hidden size of 960. The token budget for each model size was chosen to keep the total FLOPs constant.

The learning rate was varied to observe a minima in the loss at each model scale. The results are shown in Figure . While we found that the learning rate may be increased overall, the 111M model was found to have the lowest loss of the three models. From these results we conclude that the training protocol is optimal within this range of model sizes and we assume 111M is good enough. In other words, a better model might exist between 70M and 161M parameters for this FLOP budget but it isn’t outside of this range.

<figure id="fig:owt_optimal">
<span class="image placeholder" data-original-image-src="figures/owt_optimality.pdf" data-original-image-title="" width="80%"></span>
<figcaption> The loss of models trained on OpenWebText with 70M, 111M and 161M parameters. The learning rate was varied to find the minima in the loss at each model scale. The optimal learning rate for each model size is annotated.</figcaption>
</figure>

## Flash Attention Numerical Instability

The experiments described in Sections  and  involve Chinchilla optimal language models at a 111M scale . These experiments were replicated according to the published information. We encountered diverging runs when executing in bfloat16 <span acronym-label="amp" acronym-form="singular+short">amp</span> consistent with the default settings in [nanoGPT](https://github.com/karpathy/nanoGPT). These experiments were executed on NVIDIA A10 GPUs for accessible replication at small scale. By ablation it was found that these runs would diverge:

- Regardless of batch size schedule

- Regardless of hyperparameters: learning rate, weight decay, LayerNorm epsilon or Adam epsilon 

- When using PyTorch’s AMP in bfloat16 precision

- When using Flash attention 

This was surprising because prior work had trained these models successfully . In that work the model was also trained using bfloat16 AMP precision, but it was trained on a Cerebras CS-2 system. Due to this difference, we suspected the issue was due to a difference between the efficient attention kernel and the Flash attention kernel in PyTorch.

By inspecting the histograms of weights and biases in the Query, Key, Value (QKV) projection during training, we found that range grew the fastest in block 1 (the *second* block in the model). In addition, we observed that the histogram of the query and key projection weights became *bimodal* as the gradient norm diverged. This is illustrated in Figure . Further analysis of a checkpoint taken at this point in training focused on the difference between gradients computed using the flash attention kernel and the nanoGPT pure PyTorch attention implementation using float32 precision. At initialization the gradients were not significantly different but at the point of divergence there was a significant difference coinciding with increased parameter norms in that layer.

<figure id="fig:controls_optional">
<span class="image placeholder" data-original-image-src="figures/sl718-controls-optional-hist.pdf" data-original-image-title="" width="80%"></span>
<figcaption> Histograms of weights and biases for the 111M experiment described in Sections <span class="math inline">\(\ref{experiments}\)</span> and <span class="math inline">\(\ref{bss}\)</span> from the attention block containing the QKV projection, self-attention and output projection layers. The histograms for the query and key projection weights and biases are bimodal while the value projection weights and biases are not.</figcaption>
</figure>

To replicate the issue from scratch, we came up with a simulation from a generic initialization. Inspired by the teacher-student experiment protocol proposed by (although otherwise unrelated) we set up a learning task with a “teacher” and “student” model with the same architecture. Both networks begin with the same weights but we add a small amount of noise to the teacher’s weights. The student is trained to match the teacher’s output. After experimenting with hyperparameters we were able to replicated the divergence seen during training[^9], as illustrated in Figure .

<figure id="fig:flash_drift">
<span class="image placeholder" data-original-image-src="figures/flash_drift.pdf" data-original-image-title="" width="80%"></span>
<figcaption> Two “student” networks, identical to the “teacher” network except for the addition of a small amount of noise to the teacher’s QKV projection bias. As training progresses, the student using Flash attention diverges for the same inputs. Plots are, clockwise from top left: “Bias Norms” shows the norms of the bias layer in each of the networks, “Distances to Teacher” shows the L2 distance from each student to the teacher. “Flash to Non-Flash Distance” shows the L2 distance between the student using Flash attention and not, “Teacher Distance Difference” is the difference between the distances to the teacher for both cases.</figcaption>
</figure>

Using this isolated simulation we were able to test different methods to mitigate the divergence. suggested that cosine attention could address similar divergences attributed to self-attention. In Figure  we replicated the experiment described in Figure  using cosine attention and found that the divergence no longer occurred.

<figure id="fig:flash_cosattn">
<span class="image placeholder" data-original-image-src="figures/flash_cosattn.pdf" data-original-image-title="" width="80%"></span>
<figcaption> Replication of the experiment described in Figure <span class="math inline">\(\ref{fig:flash_drift}\)</span> using cosine attention instead of Flash attention. The divergence observed no longer occurs.</figcaption>
</figure>

Separately, experimenting with precision ablation found that if float32 precision was used only in block 1 (2nd) then the divergence would also not occur. Based on this and the above, we found the following two architectural mitigations for the divergence, in *only block 1 (2nd)*:

- Use cosine attention, i.e. normalize the query and key head vectors before self-attention. *OR*

- Use spectral normalization  on the QKV projection.

Critically, only modifying a single layer does not affect the throughput of the model, the observed <span acronym-label="mfu" acronym-form="singular+short">mfu</span> did not decrease by more than 1% in either case. Both of these bound the norm of the query and key head vectors prior to attention. Spectral normalization achieves this because the QKV projection is preceded by a LayerNorm layer. Using this mitigation on the 111M model allowed the experiment to be replicated on an NVIDIA A10 GPU and we observed the same behaviour as running the model more slowly in float32 precision.

Similar divergences are discussed in prior literature (and [in nanoGPT’s issue tracker](https://github.com/karpathy/nanoGPT/issues/137)) but we are unable to verify that it is the same problem. discuss how to build similar experiments to those described above but do not investigate flash attention specifically. investigate the numerical stability of Flash attention but neglect to demonstrate a failure mode that affects real training runs. focus on the numerical stability of attention in general and propose a similar mitigation (their method, $\sigma$Reparam, is a scaled version of spectral normalization) but do not investigate flash attention specifically.

It is likely that the mitigation proposed will not work in all cases, such as for larger models. However, we only needed to replicate at the scale we were working at. The experiments in Figure  and Figure  are included to illustrate how bounding the norm of the query and key head vectors seems to be important for numerical stability. However, this may change in future versions of the flash attention kernel, these results were obtained with PyTorch 2.4.0.

# Additional GNS Results

## Additional GNS Phase Plot

Figure  shows the GNS phase plot for the same model as described in Section  but with the linear batch size schedule described in Section .

<figure id="fig:gns_by_index_bss">
<span class="image placeholder" data-original-image-src="output_figures/gns_vs_trace_bss.layer_idx.pdf" data-original-image-title="" width="\textwidth"></span>
<figcaption> GNS phase plot as in Figure <span class="math inline">\(\ref{fig:gns_by_index}\)</span> but focusing on the batch size schedule described in Section <span class="math inline">\(\ref{bss}\)</span>. Linear/Embedding layers are separated from LayerNorm layers by row and the component estimators of Equations <span class="math inline">\(\ref{eq:g-est}\)</span> and <span class="math inline">\(\ref{eq:s-est}\)</span> are plotted (left), with the GNS over the course of training (right).</figcaption>
</figure>

## Batch Size Schedule

The batch size schedule used in the experiment described in Section  is shown in Figure .

<figure id="fig:111M_schedule">
<span class="image placeholder" data-original-image-src="neurips2024rebuttals/bs_plot/bs_schedule.pdf" data-original-image-title="" width="80%"></span>
<figcaption> The batch size schedule used and GNS observed in the 111M batch size schedule experiment illustrated in Figure <span class="math inline">\(\ref{fig:111M_schedule}\)</span>. An aliasing issue is noticeable in the interpolated linear batch size schedule that was used. This has since been fixed in the published code.</figcaption>
</figure>

## Larger Scale Training

To demonstrate that the method scales to larger models, we trained a 1.3B parameter GPT model[^10] on OpenWebText using 8 H100 GPUs. The results of this experiment are shown in Figure . The left plot shows the per-example gradient norms for all layers, while the right plot shows the per-example gradient norms for only the LayerNorm layers. The GNS computed using the traditional <span acronym-label="ddp" acronym-form="singular+short">ddp</span> method is also shown for comparison. In Figure  we observe that the LayerNorm remains predictive of the total GNS, as in the 111M model results of Figure . When all non-fused simultaneous per-example gradient norms were collected we observed an <span acronym-label="mfu" acronym-form="singular+short">mfu</span> of 40% and when only the fused LayerNorm layers were collected we observed an <span acronym-label="mfu" acronym-form="singular+short">mfu</span> of 57%.

After completing this experiment a bug was discovered in the code that decreased per-example gradient norms by a constant factor. This caused an underestimation of the <span acronym-label="gns" acronym-form="singular+short">gns</span>. In Figure  this can be seen when we compare the <span acronym-label="gns" acronym-form="singular+short">gns</span> estimated via <span acronym-label="ddp" acronym-form="singular+short">ddp</span> method. Initially, we assumed that this constant factor was due a failure of the LayerNorm <span acronym-label="gns" acronym-form="singular+short">gns</span> approximation to larger models. Unfortunately, we did not have the budget in time or resources to rerun the experiment so we corrected the results by multiplying by the constant factor observed in the comparison to the <span acronym-label="ddp" acronym-form="singular+short">ddp</span> method.

This may be representative of real world scenarios where a large model is pretrained over many <span acronym-label="ddp" acronym-form="singular+short">ddp</span> nodes. As the user has access to two methods to estimate the <span acronym-label="gns" acronym-form="singular+short">gns</span>, they may account for any bias or slope between the estimates. Then, if it is necessary to continue training on a single node, they can use the per-example gradient norms to estimate the <span acronym-label="gns" acronym-form="singular+short">gns</span>. Similar techniques can involve enabling per-example <span acronym-label="gns" acronym-form="singular+short">gns</span> estimation for all layers for a short time, or estimating the <span acronym-label="gns" acronym-form="singular+short">gns</span> offline as described in Appendix .

<figure id="fig:1p3b">
<figure id="fig:1p3b_left_subplot">
<span class="image placeholder" data-original-image-src="neurips2024rebuttals/regress_gns.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>Regression analysis repeating Figure 6.</figcaption>
</figure>
<figure id="fig:1p3b_right_subplot">
<span class="image placeholder" data-original-image-src="neurips2024rebuttals/1p3B_ddp_vs_pe_gns.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>Comparison of GNS computed using traditional DDP methods and per-example gradient norms.</figcaption>
</figure>
<figcaption> 1.3B GPT model train on OpenWebText using 8 H100s, trained twice. (Left) Per-example gradient norms for all layers were gathered to replicate the analysis in Figure <span class="math inline">\(\ref{fig:regress_gns}\)</span>. (Right) Per-example gradient norms were gathered for only LayerNorm layers, then compared to the GNS computed using traditional DDP methods.</figcaption>
</figure>

# FLOP & I/O Formulae

We use the following formulae in our FLOP and I/O cost estimations, where $B = \text{Batch Size}$, $T = \text{Sequence Length}$, $K = \text{Input Dimension}$, $L = \text{Output Dimension}$:

<div class="center">

|  Algorithm   |                    Weight Gradient                    |                    Gradient Norms                    |
|:------------:|:-----------------------------------------------------:|:----------------------------------------------------:|
| Simultaneous | $B K L \left(2 T - 1\right) + K L \left(B - 1\right)$ |           $B K L + B \left(K L - 1\right)$           |
|              |             $K L \left(2 B T - 1\right)$              | $B T^{2} \cdot \left(2 K + 2 L - 2\right) + B T^{2}$ |

|  Algorithm   |     Weight Gradient     | Gradient Norms  |
|:------------:|:-----------------------:|:---------------:|
| Simultaneous | $B K L + B K T + B L T$ |   $B K L + B$   |
|              |  $B K T + B L T + K L$  | $2 B T^{2} + B$ |

</div>

Solving the I/O equations above reproduces ’s analysis with $T = \frac{\sqrt{2} \sqrt{K L}}{2}$ at the cross-over point above which simultaneous calculation is more I/O efficient. Solving for FLOPs gives: $$T = \sqrt{\frac{2 K L - 1}{2 K + 2 L - 1}}.$$

[^1]: Similar algorithms for other layer types described in Appendix 

[^2]: This specific Einstein contraction is not used by but appears in the Backpack library  We provide the vector algebra contraction path chosen by on the right.

[^3]: A complete taxonomy for small batch gradient computation is given in Appendix .

[^4]: Additional algorithms for Embedding and LayerNorm layers are described in Appendix .

[^5]: The code to replicate these experiments may be found at <https://github.com/CerebrasResearch/nanoGNS/tree/main/exact>.

[^6]: <https://github.com/karpathy/nanoGPT>

[^7]: The results described in Figure  are explored at a 1.3B parameter scale in Appendix .

[^8]: <https://github.com/CerebrasResearch/nanoGNS/tree/main/exact/normgnorm>

[^9]: The code for this experiment is available at <https://gist.github.com/gaviag-cerebras/b77aef9de29e859a5e999a582d57f6a2>

[^10]: Again following the GPT2-like prescription from .
