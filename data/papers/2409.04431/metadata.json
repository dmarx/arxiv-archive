{
  "arxivId": "2409.04431",
  "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
  "authors": "Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge, Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, Russ Webb",
  "abstract": "Attention is a key part of the transformer architecture. It is a\nsequence-to-sequence mapping that transforms each sequence element into a\nweighted sum of values. The weights are typically obtained as the softmax of\ndot products between keys and queries. Recent work has explored alternatives to\nsoftmax attention in transformers, such as ReLU and sigmoid activations. In\nthis work, we revisit sigmoid attention and conduct an in-depth theoretical and\nempirical analysis. Theoretically, we prove that transformers with sigmoid\nattention are universal function approximators and benefit from improved\nregularity compared to softmax attention. Through detailed empirical analysis,\nwe identify stabilization of large initial attention norms during the early\nstages of training as a crucial factor for the successful training of models\nwith sigmoid attention, outperforming prior attempts. We also introduce\nFLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid\nattention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100\nGPUs. Experiments across language, vision, and speech show that properly\nnormalized sigmoid attention matches the strong performance of softmax\nattention on a wide range of domains and scales, which previous attempts at\nsigmoid attention were unable to fully achieve. Our work unifies prior art and\nestablishes best practices for sigmoid attention as a drop-in softmax\nreplacement in transformers.",
  "url": "https://arxiv.org/abs/2409.04431",
  "issue_number": 726,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/726",
  "created_at": "2025-01-02T15:30:35.224706",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 8,
  "last_read": "2025-01-02T15:30:35.226973",
  "last_visited": "2025-01-02T15:27:57.300Z",
  "main_tex_file": null,
  "published_date": "2024-09-06T17:53:26Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}