\section{Limitations}
\label{sec:limitations}

While our work demonstrates that $\sigmoidattn$ can serve as a viable drop-in replacement for $\softmaxattn$ in many domains and scales, there are a few key limitations to note:

\begin{enumerate}
    \item In large-scale (1B parameter, 4096 context length) language modeling, we observed some gradient norm spikes and a slight performance gap between $\sigmoidattn$ and $\softmaxattn$ (\Cref{tab:lm_results}). While runs at smaller context lengths (1B parameter, n=2048) were stable and matched $\softmaxattn$ performance, further research is needed to fully close the performance gap and ensure training stability for very large language models using $\sigmoidattn$.
    \item Our theoretical analysis proves that transformers with $\sigmoidattn$ are universal function approximators and have improved regularity compared to $\softmaxattn$. However, the bounds we derive, while tighter than those for $\softmaxattn$, may not be maximally tight. There could be room for further theoretical refinements.
    \item We focused our empirical evaluation on standard benchmarks in language, vision, and speech domains. Performance on more niche or emerging applications remains to be validated. 
    \item In automatic speech recognition experiments, we observed that $\sigmoidattn$ can be sensitive to the choice of positional embeddings and may require careful initialization of the attention bias term to ensure stable training. Specifically, we found that the CAPE positional embedding was the most unstable for $\sigmoidattn$. Further work is needed to develop robust initialization schemes that work well across different positional embeddings. Moreover we found that w/o QK norm or with post-LayerNorm $\sigmoidattn$ is unstable and can underperforms $\softmaxattn$, thus further investigation is needed.
    \item \textsc{FlashSigmoid} demonstrates promising inference and training speed-ups by exploiting $\sigmoidattn$'s simpler kernel structure compared to $\softmaxattn$. However, realizing these gains at scale in distributed training setups may require additional engineering to optimize communication bottlenecks.
\end{enumerate}

Despite these limitations, we believe this work establishes a strong foundation for $\sigmoidattn$, unifying prior art and demonstrating its potential as a drop-in $\softmaxattn$ replacement. We hope our theoretical grounding and empirical results motivate further research into this simple yet effective architectural variation.

\section{Broader Impact}
\label{sec:broader_impact}
The development of efficient and theoretically grounded attention mechanisms has the potential for significant positive impact across a range of applications. By establishing $\sigmoidattn$ as a viable alternative to $\softmaxattn$, our work expands the toolkit of architectural choices available to researchers and practitioners.
Positive impacts of this work may include:
\begin{enumerate}
    \item Improved computational efficiency: \textsc{FlashSigmoid}'s faster kernel implementation could lead to more efficient training and inference for attention-based models, reducing energy consumption and enabling deployment on resource-constrained devices. This could democratize access to powerful models.
    \item Theoretical understanding: Our universal approximation results and tighter bounds on the regularity of $\sigmoidattn$ contribute to a deeper theoretical understanding of this key component. A stronger theoretical foundation can guide principled model design and architectural search.
    \item Application-specific benefits: Across language, vision, and speech domains, $\sigmoidattn$'s performance could translate into improved user experiences, such as more natural language interactions, enhanced image understanding, and robust speech recognition. These advancements could have positive societal impacts, such as improved accessibility tools and more effective educational technologies.
\end{enumerate}
However, as with any foundational machine learning advance, there are also risks of negative impacts that must be considered and mitigated:
\begin{enumerate}
    \item Fairness and bias considerations: As with any machine learning model, it is important to carefully evaluate $\sigmoidattn$ based models for fairness and potential biases when applied to sensitive use cases. The unique properties of $\sigmoidattn$ may have unexpected interactions with data biases. Researchers and practitioners should follow best practices for auditing and mitigating unwanted biases to ensure equitable outcomes.
    \item Environmental impact: While \textsc{FlashSigmoid} is more computationally efficient than \textsc{FlashAttention}, the overall trend of scaling up attention-based models has significant energy costs. Further efficiency improvements and the use of renewable energy sources are important to mitigate environmental harms.
\end{enumerate}
We believe that the benefits of $\sigmoidattn$ outweigh the risks, but it is crucial for the research community to actively consider and address these potential negative impacts. By doing so, we can work towards a future where the efficiency and expressivity of $\sigmoidattn$ are used for societal benefit.
