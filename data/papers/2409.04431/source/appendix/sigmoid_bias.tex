\section{The Bias Term of Sigmoid Attention}
\label{app:sigmoid_bias}

One of the differences between $\sigmoidattn$ and $\softmaxattn$ is the normalization constant. In $\sigmoidattn$, one way to emulate the effect of a normalization constant (which links all the elements of the input together and defines a distribution over them), is to include a bias term in the definition as proposed in $\cref{eq:sigmoid_attn}$.


For an input vector $\vz\in\mathbb{R}^n$, the output of the sigmoid with bias $b$ is $$
\sigma^b(\vz)_i := \frac{\exp(z_i)}{\exp(z_i) + \exp(-b)}
$$
Contrary to the softmax, this output cannot always sum to one because there is no normalization.
We therefore seek a value for $b$ that \emph{approximately} normalizes $\sigma^b(\vz)$, i.e., such that $\sum_{i=1}^n \sigma^b(\vz)_i \simeq 1$.
We have
\begin{proposition}
    Let $\vz \in\mathbb{R}^n$, and take $m, M \in\mathbb{R}$ such that for all $i$, it holds $m\leq z_i\leq M$. Then, the equation $\sum_{i=1}^n \sigma^b(\vz)_i = 1$ with variable $b$ has a single solution $b^*$ with
    $$-\log(n-1) - M\leq b^*\leq -\log(n - 1) - m\enspace.$$
\end{proposition}
\begin{proof}
    The function $\phi: b\to \sum_{i=1}^n \sigma^b(\vz)_i$ is smooth and monotonically increasing, and we have $\phi(-\log(n-1)  - M) \leq 1$ and $\phi(-\log(n-1) - m)\geq1$. This shows the existence of $b^*$ as well as the advertised bound on $b^*$. 
\end{proof}
This suggests using a $b$ of the order of $-\log(n)$; in practice we use $b=-\log(n)$.


We can also look for a bias term $b$, which helps to approximate the softmax function by the sigmoid function.




We assume that softmax provides us with the true distribution $p^\star$, where $p_i^\star = \frac{e^{z_i}}{e^{z_i} + \sum_{j \neq i} e^{z_j}}$. The goal is to find the bias term $b$ such that sigmoid function with weights over all elements denoted by $p$, where $p_i = \sigma^b(\vz)_i$, approximates $p^\star$. Note that, as mentioned before, $p$ is not necessarily a distribution, i.e.\ $\sum_{i = 1}^n p_i$ is not always equal to one.


In technical terms, we aim to estimate the normalizing factor $\mZ = \sum_{i = 1}^n e^{z_i}$. The existing approaches for estimating $\mZ$ is compute-expensive for high dimensions and requires resampling methods.
Also, the optimal value of $b$ would depend on the exact values of $\vz$, which is unknown beforehand. Therefore, we propose a more intuitive way to estimate the order of bias but possibly with larger disparity.
To distribute the independent masses in $\sigmoidattn$, we assume that each element has uniform weight for the model apriori, which means that none of the elements of the input vector $\vz$ has any known importance over the others. In the simplest case when softmax is a uniform distribution, we ideally want to have the same order of values for sigmoid as of softmax, which should be $\frac{1}{n}$. Therefore, we can write down the following:
\begin{align}
    \forall i \qquad p_i = \frac{1}{1 + e^{-(z_i + b)}} \simeq \frac{1}{n} = p_i^\star
\end{align}
Ideally, we would like to have $1 + e^{-(z_i + b)} \simeq n$. Requiring that $p=p^*$ in the case where all the $z_i$ are $0$ gives $\exp(-b) = n - 1$, i.e. $b \simeq -\log(n)$ for large $n$. In the case that all the $z_i$ are bounded, $|z_i| \leq M < \infty$ for some constant $M$, then $b \simeq -(M + \log(n)) \approx -\max\{M, \log(n)\}$. However, in most cases we do not know $M$. When the sequence length $n$ is large enough, the constant $M$ loses its importance while in short sequence length, it impacts distributing the weights over elements more. To resolve this issue, we assume that $z_i$ are sampled from a standard Gaussian distribution, i.e. $z_i \sim \mathcal{N}(0, \sigma^2)$ where $\sigma = 1$. Note that this assumption comes from the fact that $z_i$ in our problem is one of the elements of $\mQ\mK^T / \sqrt{d_{qk}}$, which is the sum of $d_{qk}$ random variables. Using Central Limit Theorem, we can assume that $z_i$ is sampled from a Gaussian distribution. The idea is to estimate $M$, such that with high probability, $|z_i| \leq M$, i.e. $\mathbb{P}\left(|z_i| > M\right) \leq \epsilon$ for a desired $\epsilon$. Therefore, we have
\begin{align}
    \mathbb{P}\left(|z_i| > M\right)  = \mathbb{P}\left(|z_i| > \frac{M}{\sigma}\sigma\right) \leq \frac{1}{(\frac{M}{\sigma})^2} = \frac{\sigma^2}{M^2} \leq \epsilon,
\end{align}
where the inequality is resulted from Chebychev's inequality. Setting $\sigma = 1$, we have $M \simeq \sqrt{1/\epsilon}$. Therefore, the order-optimal value would be $b \simeq -\max\{\sqrt{1/\epsilon}, \log(n)\}$, and for long sequence length, $b \simeq -\log(n)$. For example, if we want $90\%$ accuracy in our estimation, $M \approx 3\sigma = 3$, which means $b \simeq -\max\{3, \log(n)\}$.
Note that this approximation also follows the intuition that as $n$ grows, we expect the $\sigmoidattn$ without bias term overestimate the mass on each point, so we need to normalize the mass according to $n$ at each point as well. 

On another side, one may be more interested in the gradients of $p^\star$ and $p$ with respect to $z_i$ to behave similarly. We show that $b \simeq -\log(n)$ is still a good choice in this scenario. Let us derive the derivative of $\sigmoidattn$ and $\softmaxattn$ with respect to the input. We note that for any $i$, both functions can be written as $\frac{e^{z_i}}{e^{z_i} + \mZ_{-i}}$ where $\mZ_{-i}$ is the share of normalization factor except element $i$ of $\vz$. For $\softmaxattn$, $\mZ_{-i} = \sum_{j \neq i} e^{z_j}$ and for $\sigmoidattn$, $\mZ_{-i} = e^{-b}$. Now, we have
\begin{align}
     \frac{\partial}{\partial z_i} \frac{e^{z_i}}{e^{z_i} + \mZ_{-i}} = \frac{e^{z_i}\mZ_{-i}}{\left(e^{z_i} + \mZ_{-i}\right)^2}.
\end{align}
Therefore, we have the following
\begin{align}
    \frac{\partial p_i^\star}{\partial z_i} &= p_i^\star (1-p_i^\star)\\
    \frac{\partial p_i}{\partial z_i} &= p_i (1-p_i).
\end{align}
We can see that if $p_i \simeq p_i^\star$, then $\frac{\partial p_i}{\partial z_i} \simeq \frac{\partial p_i^\star}{\partial z_i}$. So, the previous choice of bias term $b \simeq -\log(n)$ approximates the order of gradients as well. In fact, this is the only valid choice even though we have a quadratic term.
\begin{align}
    \frac{\partial p_i}{\partial z_i} \simeq \frac{\partial p_i^\star}{\partial z_i} &\quad\Longleftrightarrow\quad p_i^\star (1-p_i^\star) = p_i (1-p_i)\\
    &\quad\Longleftrightarrow\quad \left(p_i - p_i^\star\right)\left(p_i - (1-p_i^\star)\right) = 0.
\end{align}
Which means either $p_i \simeq p_i^\star$ or $p_i \simeq 1- p_i^\star$. The first one provides us with $b \simeq -\log(n)$ while the second one cannot happen since the nominator of $p_i$ is dependent on $z_i$ while the nominator of $1-p_i^\star$ is independent of $z_i$.
