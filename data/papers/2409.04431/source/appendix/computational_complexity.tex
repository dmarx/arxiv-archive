\subsection{Computational Complexity of Sigmoid and Softmax.}
\label{sec:parameter_and_flops}
\vspace{-0.1in}
\begin{table}[h!]
\small
\centering
\caption{Forward floating operations per token per attention head. 
$n_\text{ctx}$ and $d_\text{head}$ are the context length and head dimension respectively. 
$\Delta$ measures the compute difference between sigmoid and softmax as a multiple of the floating operations for computing the attention logits. 
$c$ accounts for causal ($c=(n_\text{ctx}+1)/2n_\text{ctx}\sim1/2$), or standard ($c=1$) attention.
Typical values are taken from the 1B LLM results ($n_\text{ctx}=2048$, $d_\text{head}=64$).
The difference in floating operations between Sigmoid and Softmax attention mechanisms is subleading ($\sim1\%$) compared to other operations in the attention mechanism like computing attention logits $\mL$ (shown below), and the attention matrix $\times$ values operation. This analysis precludes hardware aware improvements (\Cref{sec:FlashSigmoidHardwareAwareImplementation}).
}
\label{tab:flop-counts}
\begin{tabular}{lcccc}
\toprule
& $\mL=\mQ\mK^T$ & $\softmax\left(\mL\right)$ & $\sigmoid\left(\mL+\vb\right)$ & $\Delta$ \\ \midrule 
Expression & $ 2\,c\,n_\text{ctx}\,d_{\text{head}}$ & $ 3\,c\,n_\text{ctx}$ & $ 5\,c\,n_\text{ctx}$ & $1/d_{\text{head}}$\\
Value & $262144\,c$ & $6144\,c$ & $10240\,c$ & $1/64$ \\ \bottomrule
\end{tabular}
\end{table}
