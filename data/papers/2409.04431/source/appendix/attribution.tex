\newpage
\section{Contributions}
\label{sec:attribution}

All authors contributed to writing this paper, designing the experiments, discussing results at each stage of the project.

\paragraph{Preliminary work} 
Preliminary viability of $\sigmoidattn$ done by Jason Ramapuram.

\paragraph{Universal Function Approximation} 
Proof of UFA (\Cref{sec:ufa,app:UAP_proof}) sculpted by Federico Danieli.

\paragraph{Lipschitzness of Sigmoid Attention}
Lipschitzness analysis (\Cref{sec:regularity,app:lipschitz_proof}) molded by Pierre Ablin.

\paragraph{FlashSigmoid}
Implementation and analysis driven by Eeshan Dhekane in collaboration with Jagrit Digani (\Cref{sec:FlashSigmoidHardwareAwareImplementation,sec:DetailsOfFlashSigmoid}).

\paragraph{Bias Analysis}
Theoretical grounding for bias (\Cref{app:sigmoid_bias}) done by Amitis Shidani in discussion with Pierre Ablin.

\paragraph{Language Modeling Results}
All large scale language model pretraining and evaluation (\Cref{sec:llm,sec:llm_appendix}) driven by Floris Weers.

\paragraph{Stability Analysis}
QK norm (\Cref{fig:qk_norm_ablation}), LayerScale (\Cref{fig:layerscale_ablation}) and bias (\Cref{fig:const_attn_bias_ablation}) ablations crafted by Dan Busbridge using Attention Simulator. Attention Simulator written by Jason Ramapuram and used to validate norm growth (\Cref{fig:rope_vs_sincos,fig:rope_vs_rope,fig:rope_vs_alibi,fig:rope_vs_rope_b-10,fig:seq_len_scaling,fig:sqrt_scaling}).

\paragraph{ASR Results}
All ASR experiments (\Cref{sec:asr}) and ablations (\Cref{sec:asr_appendix_ablations_and_results}) are conducted by Tatiana Likhomanenko in discussions with Jason Ramapuram and Zijin Gu. Baseline ASR models code is written by Zijin Gu and Tatiana Likhomanenko. Baseline models are optimized by Zijin Gu to be close to state-of-the-art results.

\paragraph{Vision Results}
All vision experiments (\Cref{sec:supervised_image_classification,sec:ssl}) and ablations (\Cref{fig:imagenet_top_1_ablations,app:top1_results,fig:layerscale_free_sigmoid,fig:attention_relaxations}) conducted and written by Jason Ramapuram.

\paragraph{Simple Experiments}
Simple experiments to compare $\sigmoidattn$ to $\softmaxattn$, including visualizing attention evolution and simple sequence length generalization analysis (\Cref{sec:appendix_simple_experiments}) conducted by Russ Webb.
