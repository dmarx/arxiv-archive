\section{Universal Approximation Property for Sigmoid Attention}
\label{app:UAP_proof}
This section is dedicated to the proof for the Universal Approximation Property for attention equipped with sigmoid nonlinearity. The proof follows closely the one provided in \citet[Sec.~3]{Yun_UAP}, of which we inherit much of the notation, and we encourage the interested reader to refer to the original source for a more comprehensive understanding of its details. Here we first provide context by outlining the main steps in the original proof, before proceeding to adapt its key components to the $\sigmoidattn$ case.

The proof aims at showing that a transformer network can approximate to arbitrary accuracy any continuous, permutation-equivariant function with compact support. The proof is constructive in nature, in that it explicitly defines the architecture (and particularly, the sequence of self-attention and feed-forward layers) that can approximate a given target function. To do so, it proceeds in steps (see \citet[Sec.~3.2]{Yun_UAP}):
\begin{enumerate}
    \item\label{step::piecewise_approx} prove that any continuous function with compact support can be approximated to arbitrary accuracy by a piecewise constant function
    \item prove that an aptly-constructed \emph{modified} transformer network, (where the softmax nonlinearity is substituted with a hardmax nonlinearity), can exactly represent such piecewise constant function. This step is further divided into three sub-steps (see \citet[Sec.~4]{Yun_UAP}):
        \begin{enumerate}
            \item\label{step::quantisation} prove that a series of feed-forward layers can quantize any input to a specific discretization grid in the compact domain
            \item\label{step::context_map} prove that a series of self-attention layers can implement a \emph{contextual mapping} (see \citet[Def.~3.1]{Yun_UAP})
            \item\label{step::output_map} prove that a series of feed-forward layers can map the output of the contextual mapping to the desired output of the target piecewise-constant approximation
        \end{enumerate} 
    \item\label{step::modified_trans} prove that a (classical) transformer network can approximate such modified transformer network to arbitrary accuracy
\end{enumerate}
Fortunately, some of the steps outlined above do not rely on a specific nonlinear function being used within the attention mechanism, and can be directly reused in our proof, virtually unchanged. Notice however that \cref{step::context_map,step::modified_trans} are directly impacted by modifications to the attention layer, and hence require adaptation in our case. This is the focus of the next sections.

\subsection{Proof of \cref{step::modified_trans}: Sigmoid Transformers can Approximate Modified Sigmoid Transformers}
\label{sec::proof_modified_sigmoid}
In \cite{Yun_UAP}, to implement contextual mappings, the authors rely on a \emph{modified} version of transformers, for the sake of simplifying the analysis. In their modified version, the (row-wise) softmax operation is substituted with a (row-wise) hardmax operation. This substitution is valid because a classical transformer can still be made arbitrarily close to such modified transformer, in light of the fact that
\begin{equation}
    \text{softmax}(\lambda\bb{X})\xrightarrow[]{\lambda\to\infty}\text{hardmax}(\bb{X}).
    \label{eqn::classic2modified_softmax}
\end{equation}
In our proof, we follow a similar strategy to define our modified sigmoid transformer (and in particular, its self-attention mechanism). We have that
\begin{equation}
    \sigma(\lambda\bb{X})\xrightarrow{\lambda\to\infty}H(\bb{X}),
    \label{eqn::classic2modified_sigmoid}
\end{equation}
where $\sigma(x)=(1+e^{-x})^{-1}$ is the (elementwise) sigmoid function, while
\begin{equation}
    H(x) = \begin{cases}
        1 & x>0\\
        \frac{1}{2} & x=0\\
        0 & x<0
    \end{cases}
\end{equation}
denotes the (elementwise) Heaviside step function. This allows us to define our modified sigmoid self-attention layer, as follows.
\begin{definition}[Modified sigmoid self-attention layer]
\label{def::modified_sigmoid_attention}
    Given an input $\bb{X}\in\mathbb{R}^{d\times n}$, the action of a modified sigmoid self-attention layer with shifts and a single one-dimensional head is defined as $\bb{X}\mapsto \bb{X} + \psi(\bb{X};\bb{q},\bb{b}_q,\bb{k},\bb{b}_k,\bb{v},\bb{o})$, where
    \begin{equation}
        \psi(\bb{X};\bb{q},\bb{b}_q,\bb{k},\bb{b}_k,\bb{v},\bb{o}) = \bb{o}\left(\bb{v}^T \bb{X}\right) H\left(\left(\bb{q}^T \bb{X}-\bb{b}_q^T\right)^T\left(\bb{k}^T \bb{X}-\bb{b}_k^T\right)\right)
        \label{eqn::modified_sigmoid_attention}
    \end{equation}
    with $\bb{q},\bb{k},\bb{v}\in\mathbb{R}^{d}$ representing the query, key, and value vectors, $\bb{b}_q,\bb{b}_k\in\mathbb{R}^{n}$ the corresponding query and key bias vectors, while $\bb{o}\in\mathbb{R}^{d}$ denotes the output vector.
\end{definition}
Analogously to \cref{eqn::classic2modified_softmax}, \cref{eqn::classic2modified_sigmoid} guarantees that sigmoid attention can approximate modified sigmoid attention by simply increasing the magnitude of its inner parameters.

Here and in the following, the length of the input sequence is denoted as $n$, while $d$ represents the dimensionality of the tokens. Notice that we are considering the input tensor $\bb{X}\in\mathbb{R}^{d\times n}$, (as opposed to $\in\mathbb{R}^{n\times d}$) to better align out notation with the one used in \cite{Yun_UAP}. 

\subsection{Proof of \cref{step::context_map}: Modified Sigmoid Transformers can Implement Contextual Mappings}
\label{sec::proof_contextual_mapping_top}
The core of the proof consists in showing how, by opportunely combining the operations in \cref{eqn::modified_sigmoid_attention}, one can build an architecture capable of implementing a \emph{contextual mapping}. For completeness, we report next the definition of such a map (see also \citet[Def.~3.1]{Yun_UAP}).
\begin{definition}[Contextual mapping]
    A map $\bb{q}:\mathbb{L}\to\mathbb{R}^{n}$ from a finite set $\mathbb{L}\subset\mathbb{R}^{d\times n}$ is said to be a \emph{contextual mapping} if both the following conditions hold:
    \begin{enumerate}[label=(\roman*)]
        \item $q_i(\bb{X})\neq q_j(\bb{X})$, $\forall i\neq j$ and $\forall\bb{X}\in\mathbb{L}$
        \item $q_i(\bb{X})\neq q_j(\bb{X}')$, $\forall i,j$ and $\forall\bb{X},\bb{X}'\in\mathbb{L}$, with $\bb{X}\neq\bb{X}'$
    \end{enumerate}
    where $q_i(\bb{X})$ denotes the $i$-th component of $\bb{q}(\bb{X})$.
    \label{def::contextual_mapping}
\end{definition}
Namely, a contextual mapping is such that it transforms each token in an input sequence to a value depending \emph{uniquely} on the \emph{whole} sequence. By satisfying this property, we can ensure that any element of the quantization of the input domain (achieved by \cref{step::quantisation}) can be mapped to a unique identifying value (depending on the whole input) via a sequence of modified sigmoid self-attention layers. It is then up to the MLP (in \cref{step::output_map}) to correctly map this value to the corresponding output value in the piece-wise constant approximation.

In particular, after defining a uniform discretization (characterized by the parameter $\delta$) of the unitary hypercube $[0,1]^{d}\subset\mathbb{R}^{d}$, namely
\begin{equation}
    \mathbb{G}_\delta \coloneqq \{\bb{g}:g_i\in\{0,\delta,2\delta,\dots,1-\delta\}, \quad\forall i=1\dots d\},
\end{equation}
we consider as input a tensor $\bb{X}$ (composed of columns $\bb{X}=[\bb{x}_{i}]_{i=1}^{n}$) such that
\begin{equation}
    \bb{X}\in\mathbb{L}\coloneqq\{\bb{X}: \bb{x}_{i}\in\mathbb{G}_\delta\;\forall i=1\dots n,\quad\text{and}\quad \bb{x}_{i}\neq\bb{x}_{j}\;\forall i\neq j\}\subset\mathbb{R}^{d\times n},
    \label{eqn::quantised_input}
\end{equation}
that is, a 2D tensor whose columns are element of the discretization $\mathbb{G}_\delta$, and that all differ from each other (at least for one element).
We want to build a contextual mapping acting on $\mathbb{L}$, by stacking layers parameterized according to \cref{def::modified_sigmoid_attention}. In \cref{sec::building_blocks} we define the basic building blocks of our architecture; in \cref{sec::action_of_building_blocks} we describe how to stack them, and the effect the architecture has on a given input; finally, in \cref{sec::proof_contextual_mapping} we prove that this architecture indeed implements a contextual mapping.


\subsubsection{Basic Building Blocks of Contextual Mapping}
\label{sec::building_blocks}
The strategy we follow to assemble a contextual mapping consists in sequentially looking at each column of the input, progressively updating and storing information regarding its content in a uniquely identifiable manner, and finally broadcasting this information back to every element in the sequence. The difficulty lies in the fact that each of these updates must be carried on while relying solely on applications of the modified $\sigmoidattn$ layer in \cref{def::modified_sigmoid_attention}. In the following, we describe how we can tweak its parameters to achieve exactly this.

\paragraph{From $d$-dimensional quantized vectors to scalars}
As a first simplification, we can get rid of the $d$-dimension in the $\bb{X}$ tensor by mapping each of its columns to a corresponding identifying scalar, uniquely defined by the specific column components. This step is also performed in \citet[App.~B.5]{Yun_UAP}, and can be achieved rather straightforwardly, by defining 
\begin{equation}\bb{v}\equiv\bb{q}\equiv\bb{k}\equiv\bb{u}\coloneqq[1,\delta^{-1},\delta^{-2},\dots,\delta^{-d+1}]^T.
    \label{eqn::vector2linear_map}
\end{equation}
Notice in fact that, since each column $\bb{x}_{i}$ belongs to $\mathbb{G}_\delta$, it can equivalently be written in the form $\bb{x}_{i}=\delta\cdot[\text{id}_{0,i}, \text{id}_{1,i},\dots,\text{id}_{d-1,i}]^T$, where $\text{id}_{j,i}\in\{0,1,2,\dots,\delta^{-1}-1\}$ represents the (indexed) coordinate of the discretization along the $j$-th dimension. Scalar-multiplying $\bb{X}$ by $\bb{u}$ in \cref{eqn::vector2linear_map}, then, turns this tuple of indices into a single one, in a bijective fashion\footnote{For example, consider $d=3$ and the column defined as $\bb{x}_{i} = [3\delta, 10\delta, 2\delta]^T$, that is, the column identified by the \emph{triplet} of indices $[3,10,2]$. Multiplying by $\bb{u}$ would then give the scalar $\bb{u}^T\bb{x}_{i} = (3 + 10N + 2N^2) \delta$, where $N=\delta^{-1}$, which is uniquely identified by the \emph{single} index $(3 + 10N + 2N^2)$.}.

This allows us to equivalently consider a single vector $\bb{u}^T\bb{X}\in\mathbb{R}^{n}$, rather than the whole tensor $\bb{X}\in\mathbb{R}^{d\times n}$ in the remainder of our analysis. Analogously, choosing $\bb{o}\equiv\bb{e}_0\coloneqq[1,0,\dots,0]^T$ in \cref{eqn::modified_sigmoid_attention} constraints the effect of the layer application to impact only the first row of the tensor: the goal is then to store in this row the result of the target contextual mapping $\bb{q}$ in \cref{def::contextual_mapping}.
To slim our notation, in the following we often refer to $\bb{u}^T\bb{X}$ as the vector $\bb{l}\in\mathbb{R}^{n}$, with components $l_i$.

In light of the simplification above, we can rewrite \cref{eqn::modified_sigmoid_attention} more compactly, as follows:
\begin{equation}
    \psi(\bb{X};\bb{q}=\bb{k}=\bb{v}\equiv\bb{u},\bb{o}\equiv\bb{e}_0;\bb{b}_q,\bb{b}_k) = \bb{e}_0\bb{l}^T H\left(\left(\bb{l}-\bb{b}_q\right)\otimes\left(\bb{l}-\bb{b}_k\right)\right)
    \label{eqn::modified_attention_1}
\end{equation}

Notice that, since the elements of both $\bb{X}$ and $\bb{u}$ are always non-negative, so are those of $\bb{l}$, too. Moreover, since we are interested in permutation-equivariant functions with respect to the columns of $\bb{X}$, without loss of generality we can consider the elements of $\bb{l} = \bb{u}^T\bb{X}$ to be ordered: $0\leq l_i< l_j$, $\forall i<j$.


\paragraph{Selective shift operation for sigmoid attention} Since we aim to recover a contextual map by sequentially updating the elements of $\bb{l}$, we proceed by designing a modification of \cref{eqn::modified_attention_1} which affects only a certain selected element at a time. This is were our second simplification comes into play, and this time it pertains the roles of the bias vectors $\bb{b}_q$ and $\bb{b}_k$. Since $\bb{l}\geq 0$, these vectors have the effect of tweaking the sign of the inner arguments of the Heaviside function in \cref{eqn::modified_attention_1}, hence directly impacting when its application outputs $0$ or $1$. By aptly selecting the values of $\bb{b}_k$ and $\bb{b}_q$, then, we can explicitly decide when a specific layer triggers an update, which elements are affected by the update, and what elements to consider to compute the update itself.

More in detail, take $\bb{b}_q=\bb{1}b_q$ and $\bb{b}_v=\bb{1}b_v$, for some scalars $b_q,b_v$, and with $\bb{1}$ being the all-one vector. Plugging this into \cref{eqn::modified_attention_1}, we have
\begin{equation}
\begin{split}
    \tilde{\psi}(\bb{X};b_q,b_k) &\coloneqq \psi(\bb{X};\bb{q}=\bb{k}=\bb{v}\equiv\bb{u},\bb{o}\equiv\bb{e}_0,\bb{b}_q=\bb{1}b_q,\bb{b}_k=\bb{1}b_k)\\
    &= \bb{e}_0\bb{l}^T H\left(\left(\bb{l}-\bb{1}b_q\right)\otimes\left(\bb{l}-\bb{1}b_k\right)\right)
    =\bb{e}_0\begin{cases}
        \sum_{i:l_i<b_v} l_i &\text{ if } l_j < b_k \\
        \sum_{i:l_i>b_v} l_i &\text{ if } l_j > b_k
    \end{cases};
    \label{eqn::modified_attention_2}
\end{split}
\end{equation}
notice how $b_q$ determines what elements of $\bb{l}$ compose the update (as it impacts the indices considered in the sum), while $b_k$ defines the elements impacted by the update itself
\footnote{\label{fnt::sigmoid_attention_mat}
This can be better seen by considering independently the effects of the two parameters $b_k$, $b_q$ on the modified sigmoid attention matrix $H\left(\left(\bb{l}-\bb{1}b_q\right)\otimes\left(\bb{l}-\bb{1}b_k\right)\right)$. We have in fact, with $b_q=0$,
\begin{equation}
    \arraycolsep=2.5pt
    \begin{array}{cc}    
    &\begin{array}{rl}
        l_j<b_k & l_j>b_k
    \end{array}\\
    \begin{array}{r}
    H\left(\bb{l}\otimes\left(\bb{l}-\bb{1}b_k\right)\right)
    \end{array}=& \left[\begin{array}{ccc|ccc}
    0      & \cdots & 0      & 1      & \cdots & 1     \\[-1ex]
    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    0      & \cdots & 0      & 1      & \cdots & 1     \\
    \end{array}\right].\\
    \end{array}
\end{equation}
This shows how, by modifying $b_k$, one can decide which columns will receive an update: namely, all those with index $l_j>b_k$. By combining two such operators with $b_k=\left(i-\frac{1}{2}\right)\delta$ and $b_k=\left(i+\frac{1}{2}\right)\delta$, we then recover
\begin{equation}
    \arraycolsep=2.5pt
    \begin{array}{cc}
    &l_j=i\delta\\%[-1ex]
    \begin{array}{r}
    H\left(\bb{l}\otimes\left(\bb{l}-\bb{1}\left(i-\frac{1}{2}\right)\delta\right)\right)\\
    -H\left(\bb{l}\otimes\left(\bb{l}-\bb{1}\left(i+\frac{1}{2}\right)\delta\right)\right) 
    \end{array}=&
    \left[\begin{array}{ccc|c|ccc}
    0      & \cdots & 0      & 1      & 0      & \cdots & 0     \\[-1ex]
    \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
    0      & \cdots & 0      & 1      & 0      & \cdots & 0     \\
    \end{array}\right],
    \end{array}
    \label{eqn::multihead_bq0}
\end{equation}
which allows us to limit the update to only one specific column: the one with index $l_j=i\delta$.

The parameter $b_q$ acts analogously, but varies the output of the Heaviside function as we move down the rows, rather than the columns. The same operator as in \cref{eqn::multihead_bq0}, but with $b_q=\left(i+\frac{1}{2}\right)\delta$ gives us in fact:
\begin{equation}
    \arraycolsep=2.5pt
    \begin{array}{ccc}
    &l_j=i\delta&\\%[-1ex]
    \begin{array}{r}
        H\left(\left(\bb{l}-\bb{1}\left(i+\frac{1}{2}\right)\delta\right)\otimes\left(\bb{l}-\bb{1}\left(i-\frac{1}{2}\right)\delta\right)\right)\\
        -H\left(\left(\bb{l}-\bb{1}\left(i+\frac{1}{2}\right)\delta\right)\otimes\left(\bb{l}-\bb{1}\left(i+\frac{1}{2}\right)\delta\right)\right) \\
    \end{array}=&
    \left[\begin{array}{ccc|c|ccc}
    0      & \cdots & 0      & -1     & 0      & \cdots & 0     \\[-1ex]
    \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
    0      & \cdots & 0      & -1     & 0      & \cdots & 0     \\\hline
    0      & \cdots & 0      & 1      & 0      & \cdots & 0     \\[-1ex]
    \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
    0      & \cdots & 0      & 1      & 0      & \cdots & 0     \\
    \end{array}\right]&
    \begin{array}{c}
    l_j<i\delta\\
    \\
    \\
    l_j>i\delta\\
    \end{array}
    \end{array}.
    \label{eqn::multihead_bq}
\end{equation}
Finally, \cref{eqn::selective_shift} can be recovered by combining \cref{eqn::multihead_bq0,eqn::multihead_bq}: this has the effect of removing the $-1$'s in \cref{,eqn::multihead_bq}.}.
If we opportunely combine \emph{four} modified sigmoid self-attention heads $\tilde{\psi}(\bb{X};b_q,b_k)$, we recover, for a given index $i=0\dots\delta^{-d}-1$,
\begin{equation}
\begin{split}
    \Psi^{(i)}(\bb{X})\coloneqq& \bb{X}
    + \frac{1}{2}c \left(\begin{array}{l}
    \tilde{\psi}\left(\bb{X};b_q=0,b_k=\left(i -\frac{1}{2}\right)\delta\right)\\
    - \tilde{\psi}\left(\bb{X};b_q=0,b_k=\left(i +\frac{1}{2}\right)\delta\right)\\
    - \tilde{\psi}\left(\bb{X};b_q=b_k=\left(i +\frac{1}{2}\right)\delta\right)\\
    + \tilde{\psi}\left(\bb{X};b_q=\left(i +\frac{1}{2}\right),b_k=\left(i -\frac{1}{2}\right)\delta\right)\\
    \end{array}\right)\\
    = & \bb{X}
    + \frac{1}{2}c \bb{e}_0\bb{l}^T\left(\begin{array}{l}
    H\left(\bb{l}\otimes\left(\bb{l}-\left(i -\frac{1}{2}\right)\delta\right)\right)\\
    - H\left(\bb{l}\otimes\left(\bb{l}-\left(i +\frac{1}{2}\right)\delta\right)\right)\\
    - H\left(\left(\bb{l}-\left(i +\frac{1}{2}\right)\delta\right)\otimes\left(\bb{l}-\left(i +\frac{1}{2}\right)\delta\right)\right)\\
    + H\left(\left(\bb{l}-\left(i +\frac{1}{2}\right)\delta\right)\otimes\left(\bb{l}-\left(i -\frac{1}{2}\right)\delta\right)\right)\\
    \end{array}\right)\\
    \Longrightarrow &\Psi^{(i)}_{1,j}(\bb{X})=\bb{X}_{1,j}+c\begin{cases}
        \sum_{k:l_k> i\delta} l_k &\text{ if }  l_j = i\delta \\
        0 &\text{ otherwise}
    \end{cases}\\
    \Longrightarrow &\Psi^{(i)}_{k>1,j}(\bb{X})=\bb{X}_{k,j},
    \end{split}
    \label{eqn::selective_shift}
\end{equation}
where $c\equiv c(\delta,d,n)$ is a multiplicative constant which will be chosen later. 

The operator assembled in \cref{eqn::selective_shift} defines the basic layer of the architecture that we use in our proof. Notice $\Psi^{(i)}(\bb{X})$ has the effect of modifying only the column $\bb{x}_j$ which has index $l_j=\bb{u}^T\bb{x}_j=i\delta$ (if at all present in the input $\bb{X}$). This layer covers a similar role to the \emph{selective shift operation} introduced in \citet[App.~B.5]{Yun_UAP}, but it has been adapted to account for the presence of a sigmoid nonlinearity: notice this required us to use $4$-headed attention, while in \cite{Yun_UAP} a $2$-headed version is sufficient.


\subsubsection{Result of Applying a Sequence of Selective Shifts}
\label{sec::action_of_building_blocks}
Ultimately we want to show how, by stacking a sequence of selective shift layers \cref{eqn::selective_shift} for increasing $i=0\dots\delta^{-d}-1$ and one additional global shift, we can build an architecture capable of representing a contextual mapping. As a preliminary step, in this section we provide an explicit formula for the result of applying such an architecture. Once again, we are proceeding analogously to \citet[App.~B.5.1]{Yun_UAP}.

\paragraph{After the first selective shift application} Consider a quantized input sequence $\bb{X}\in\mathbb{L}$ as defined in \cref{eqn::quantised_input}, with its columns ordered according to their scalar indices $\bb{l}=\bb{u}^T\bb{X}$. The sequence of selective shift layers $\Psi^{(0)},\Psi^{(1)},\dots$ initially has no effect on the input itself, and it leaves it unchanged until we hit the layer corresponding to the index of the first column in the input, $\Psi^{(\hat{i})}$, where $l_1=\bb{u}^T\bb{x}_1=\hat{i}\delta$. At this point, following \cref{eqn::selective_shift}, the first column of the input is modified into
\begin{equation}
    \bb{x}_1\quad\mapsto\quad\Psi^{(\hat{i})}_{|,1}(\bb{X})
    =\bb{x}_1+ c\bb{e}_0\sum_{k:l_k>l_1}l_k
    =\bb{x}_1+ c\bb{e}_0\left(\sum_{k=1}^{n}l_k - l_1\right)
    \label{eqn::first_shift}
\end{equation}
while the other columns are still left untouched. In the following, we compactly refer to the quantities $\sum_{k=1}^{n}l_k - l_i$ as $s_i$:
\begin{equation}
    \bb{s} = [s_1,s_2,\dots,s_n]^T \coloneqq \left[\sum_{k=1}^{n}l_k - l_1, \sum_{k=1}^{n}l_k - l_2,\dots,\sum_{k=1}^{n}l_k - l_n\right]^T.
    \label{eqn::partial_sum}
\end{equation}
According to \cref{eqn::first_shift}, the index $l_1$ of column $\bb{x}_1$ is then analogously mapped to
\begin{equation}
    l_1=\bb{u}^T\bb{x}_1\quad\mapsto\quad\tilde{l}_1\coloneqq\bb{u}^T\Psi^{(\hat{i})}_{|,1}(\bb{X})
    =\bb{u}^T\bb{x}_1+ cs_1 = l_1 + cs_1.
    \label{eqn::tildel1}
\end{equation}
Notice that, by choosing $c>1$, we can ensure
\begin{equation}
    c>1\quad\Longrightarrow\quad\tilde{l}_1>\cancel{l_1} + \sum_{k=1}^{n}l_k-\cancel{l_1} > \sum_{k=1}^{n} > l_i\quad\forall i,
    \label{eqn::tildel1_is_larger}
\end{equation}
and particularly $\tilde{l}_1>l_2$, implying that at the next (effective) application of the selective shift operation, this term, too, will contribute to the update.

\paragraph{Subsequent selective shift applications}
Following similar considerations, the next effective update will be applied by the layer $\Psi^{(\hat{i})}$ with $l_2=\bb{u}^T\bb{x}_2=\hat{i}\delta$. At this point, the second column index is updated as follows:
\begin{equation}
\begin{split}
    l_2=\bb{u}^T\bb{x}_2\qquad\mapsto\qquad\tilde{l}_2\coloneqq&\bb{u}^T\Psi^{(\hat{i})}_{|,2}(\bb{X})
    =\bb{u}^T\bb{x}_2+ c\left(\sum_{k:l_k>l_2}l_k + \tilde{l}_1\right) \\
    =& l_2 + c \left(\sum_{k=1}^{n}l_k - l_2 - \cancel{l_1} + \cancel{l_1} + cs_1\right) = l_2 + c s_2 + c^2 s_1
\end{split}
\end{equation}
where $\tilde{l}_1$ is also included in light of \cref{eqn::tildel1_is_larger}, and we used the definitions \cref{eqn::tildel1,eqn::partial_sum}. Continuing to apply $\Psi^{(i)}(\bb{X})$, for increasing $i$, and unrolling the recursion, we recover
\begin{equation}
\begin{split}
    \tilde{l}_3 &= l_3 + c\left(\sum_{k=1}^{n}l_k - l_1 - l_2 -l_3 + \tilde{l}_1 + \tilde{l}_2\right)
        = l_3 + cs_3 + c^{2}(s_2+s_1) + c^{3}s_1\\
    \tilde{l}_4 &= l_4 + c\left(\sum_{k=1}^{n}l_k - l_1 - l_2 - l_3 - l_4 + \tilde{l}_1 + \tilde{l}_2 + \tilde{l}_3\right)\\
        &= l_4 + cs_4 + c^{2}(s_3+s_2+s_1) + c^{3}(s_2+2s_1) + c^{4}s_1\\
    \tilde{l}_5 &= l_5 + c\left(\sum_{k=1}^{n}l_k - l_1 - l_2 - l_3 - l_4 - l_5 + \tilde{l}_1 + \tilde{l}_2 + \tilde{l}_3 + \tilde{l}_4\right)\\
        &= l_5 + cs_5 + c^{2}(s_4+s_3+s_2+s_1) + c^{3}(s_3+2s_2+3s_1) + c^{4}(s_2+3s_1) + c^{5}s_1\\
        &\vdots
\end{split}
\label{eqn::ltilde_unroll}
\end{equation}
which eventually allows us to write the general formula
\footnote{From \cref{eqn::ltilde_unroll}, we can notice that, for a given $\tilde{l}_k$, the coefficients $a_{i,j}^{(k)}$ appearing in front of the various $s_{k-i}$ for each of the $c^{j}$ terms, are first given by a list of ones, $a_{i,1}^{(k)} = 1$, then a list of increasing numbers $a_{i,2}^{(k)}=i \Longrightarrow a_{-,2}^{(k)} = \text{cumsum}(a_{-,1}^{(k)})$, then a list of triangular numbers $a_{i,3}^{(k)}=i(i+1)/2\Longrightarrow a_{-,3}^{(k)} = \text{cumsum}(a_{-,2}^{(k)})$, and so on: $a_{-,j}^{(k)}= \text{cumsum}(a_{-,j-1}^{(k)})$. The result of iterated applications of cumsum, starting from an all-one vector, can be compactly described via the binomial coefficient: we have in fact
$$a_{i,j} = [\text{cumsum}^j([1,1,\dots])]_i=\binom{i+j-2}{j-1}.$$ The actual formula \cref{eqn::ltilde} can be recovered after a few algebraic steps, by rearranging the summation indices.}
\begin{equation}
\begin{split}
    \tilde{l}_j &\coloneqq l_j + cs_j +\sum_{i=0}^{j-2}c^{i+2}\sum_{k=i}^{j-2}\binom{k}{i}s_{k-i+1}, \qquad\qquad j=1\dots n.
    \label{eqn::ltilde}
\end{split}
\end{equation}


\subsubsection{Result of Applying One Last \emph{Global Shift} Layer}
After the last selective shift layer, the original input $\bb{X}$ has been mapped to a modified one $\tilde{\bb{X}}$ whereby each column $\tilde{\bb{x}}_j$ is characterized by the index $\tilde{l}_j=\bb{u}^T\tilde{\bb{x}}_j$ given in \cref{eqn::ltilde}. Remember our goal is to recover a contextual mapping, but notice that these $\tilde{l}_j$ indices are \emph{not} uniquely defined by the input\footnote{To convince ourselves of this, it suffices to look at the formula for \cref{eqn::tildel1}: two sequences with different elements $\bb{l}\neq\bb{l}'$, but such that $l_1=l_1'$ and $s_1=s_1'$ (that is, with $\sum_{i=1}^n l_i=\sum_{i=1}^n l_i'$) would map to the same $\tilde{l}_1=\tilde{l}_1'$.}; in other words, they do not satisfy property (2) in \cref{def::contextual_mapping}. The only exception to this is the last index $\tilde{l}_n$, as (loosely speaking) it has ``seen'' all the previous updates - and indeed in \cref{sec::proof_contextual_mapping} we prove this rigorously, under some assumption on the yet-undefined coefficient $c(\delta,d,n)$.

A straightforward way to recover a one-to-one mapping for the whole sequence, then, is to update every index $\tilde{l}_j$ via a quantity directly depending on $\tilde{l}_n$. This is precisely what the last \emph{global shift} layer $\bar{\Psi}(\bb{X})$ aims to accomplish. This last layer is also defined starting from the simplified modified sigmoid attention \cref{eqn::modified_attention_2}, by picking $b_k=0$ and $b_q=\left(c(\delta,d,n)^n+\frac{1}{2}\right)\delta$: if, for any input, we can guarantee that
\begin{equation}
    \tilde{l}_j\leq c(\delta,d,n)^n\delta\quad j<n\qquad\text{and}\qquad \tilde{l}_n>c(\delta,d,n)^n\delta,
    \label{eqn::last_layer_condition}
\end{equation}
then the application of the global shift layer would result in\footnote{As in \cref{fnt::sigmoid_attention_mat}, this is also better seen by considering the resulting modified sigmoid attention matrix. With $b_k=0$ and $b_q=\left(c(\delta,d,n)^n+\frac{1}{2}\right)\delta$, in fact, if condition \cref{eqn::last_layer_condition} is verified, this matrix is given by
\begin{equation}
    \arraycolsep=2.5pt
    \begin{array}{rcl}
    \begin{array}{c}
        H\left(\left(\tilde{\bb{l}}-\bb{1}\left(c^n+\frac{1}{2}\right)\delta\right)\otimes\tilde{\bb{l}}\right)
    \end{array}=&
    \left[\begin{array}{ccc}
    0      & \cdots & 0      \\[-1ex]
    \vdots & \ddots & \vdots \\
    0      & \cdots & 0      \\\hline
    1      & \cdots & 1      \\
    \end{array}\right]&
    \begin{array}{l}
    \\
    \\
    \tilde{l}_j,\,j<n\\
    \\
    \tilde{l}_n\\
    \end{array}
    \end{array}.
\end{equation}
}:
\begin{equation}
\begin{split}
    \bar{\Psi}(\tilde{\bb{X}})\coloneqq&\tilde{\bb{X}} + c^{n+1}\tilde{\psi}\left(\tilde{\bb{X}};b_q=\left(c^n+\frac{1}{2}\right)\delta,b_k=0\right)\\
    \Longrightarrow &\bar{\Psi}_{1,j}(\tilde{\bb{X}}) = \tilde{\bb{X}}_{1,j} + c^{n+1}\tilde{l}_n\\
    \Longrightarrow &\bar{\Psi}_{k>1,j}(\tilde{\bb{X}})=\tilde{\bb{X}}_{k,j}.
\end{split}
\label{eqn::global_shift}
\end{equation}

The global shift \cref{eqn::global_shift} is the last layer we need to define our candidate contextual mapping. Collecting the results from this section together, our architecture is defined by sequentially composing the selective shift layers with the global shift one,
\begin{equation}
    \Psi(\bb{X}) \coloneqq \bar{\Psi}\circ\Psi^{(\delta^{-d}-1)}\circ\cdots\circ\Psi^{(2)}\circ\Psi^{(1)}(\bb{X}).
\end{equation}
After being scalar-multiplied by $\bb{u}$, this results in a sequence
\begin{equation}
    \bb{q}(\bb{X}) \coloneqq \bb{u}^T\Psi(\bb{X}) = \tilde{\bb{l}} + c^{n+1} \bb{1}\tilde{l}_n
    \label{eqn::final_q}
\end{equation}
which we aim to prove is a contextual mapping. This is shown in the next section.






\subsubsection{A Sequence of Selective Shifts Followed by a Global Shift Produces a Contextual Mapping}
\label{sec::proof_contextual_mapping}
To complete the proof, it remains to show that the recovered sequence \cref{eqn::final_q} represents a contextual mapping and, in particular, that it is \emph{(i)} one-to-one in $\mathbb{L}$, and that \emph{(ii)} all of its elements are distinct for different inputs. To do so, we need a few preparatory lemmas. The first few are needed to show that each of the basic components of \cref{eqn::final_q} is indeed a one-to-one map.


\begin{lemma}
\label{thm::S_1to1}
The map $\bb{l}\mapsto\bb{s}$ in \cref{eqn::partial_sum} is one-to-one.
\end{lemma}
\begin{proof}
The target map can be compactly represented as a linear operator $S$:
\begin{equation}
    \bb{l}\mapsto \bb{s}\coloneqq \bb{1}\sum_{k=1}^n l_k - \bb{l} = (\bb{1}\otimes\bb{1} - I)\bb{l}\eqqcolon S\bb{l}
    \label{eqn::s_as_linear_op}
\end{equation}
which is invertible\footnote{Indeed its inverse can be explicitly recovered by directly applying Sherman-Morrison formula.}, denoting that $\bb{l}\mapsto\bb{s}$ is bijective.
\end{proof}

\begin{lemma}
\label{thm::ln_1to1}
The map $\bb{l}\mapsto\tilde l_n$ in \cref{eqn::ltilde} is one-to-one, under the condition 
\begin{equation}
    c(\delta,d,n)>(n-1)(\delta^{-d}-1)\binom{n-1}{\left\lceil\frac{n-1}{2}\right\rceil}.
    \label{eqn::condition_on_c_final}
\end{equation}
\end{lemma}
\begin{proof}
Consider two vectors of column indices $\bb{l},\bb{l}'$ differing for at least one element. We have by definition \cref{eqn::ltilde} that
\begin{equation}
    \tilde{l}_n-\tilde{l}_n' = 
        (l_n-l_n') + c(s_n-s_n')
        +\sum_{i=0}^{n-2}c^{i+2}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')
\end{equation}
By absurd, assume $\tilde{l}_n-\tilde{l}_n'=0$ even though $\exists i:l_i\neq l_i'$. We have then that it must hold
\begin{equation}
\begin{split}
    (l_n'-l_n) &= c(s_n-s_n')
    +\sum_{i=0}^{n-2}c^{i+2}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\\
    &=c\left((s_n-s_n')
    +\sum_{i=0}^{n-2}c^{i+1}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right)
\end{split}
\label{eqn::ln'-ln}
\end{equation}
Notice that, for $c(\delta,d,n)$ large enough, the right-hand side does not have enough \emph{granularity} to counter the left-hand side: in fact, since $l_n\in\{0,\delta,2\delta,\dots,\delta^{-d+1}-\delta\}$, the left-hand side can attain values
\begin{equation}
    l_n'-l_n\in\{0,\pm\delta,\pm2\delta,\dots,\pm(\delta^{-d+1}-\delta)\}
\end{equation}
while the former, in light of the presence of the $c(\delta,d,n)$ factor, can only attain values $\in\{0,\pm c\delta,\pm2c\delta,\dots\}$. Picking $c>\delta^{-d}-1$, then, ensures that equality between the two sides of \cref{eqn::ln'-ln} can only be achieved if they are both $0$. In this case, we need to impose
\begin{equation}
\begin{split}
    &c(s_n'-s_n) = 
    \sum_{i=0}^{n-2}c^{i+1}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\\
    \quad\Longleftrightarrow\quad& s_n'-s_n %
    =c\left(\sum_{i=0}^{n-2}c^i\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right).
\end{split}
\label{eqn::condition_on_sn_for_1to1}
\end{equation}
Similarly, notice that\footnote{This is a direct consequence of the definition of operator $S$ in \cref{eqn::s_as_linear_op}: since it has $1$'s everywhere but on its diagonal, its $\infty$-norm is simply $n-1$.}, $\forall i$,
\begin{equation}
    |s_i-s_i'|=\left|\sum_{k=1}^{n} (l_k-l_k') - (l_i-l_i')\right|=\left|\sum_{k=1,k\neq i}^{n} (l_k-l_k')\right|<(n-1)(\delta^{-d+1}-\delta),
\end{equation}
implying that $s_n'-s_n\in\{0,\pm\delta,\pm2\delta,\dots,\pm(n-1)(\delta^{-d}-1)\delta\}$. Again, by picking $c(\delta,d,n)>(n-1)(\delta^{-d}-1)$ we ensure that the right-hand side does not have enough granularity, and hence
\begin{equation}
    c(\delta,d,n)>(n-1)(\delta^{-d}-1) \qquad\Longrightarrow\qquad s_n'-s_n=0,  
    \label{eqn::conditions_on_c_for_1to1_0}
\end{equation}
implying
\begin{equation}
\begin{split}
    &c\left(\sum_{i=0}^{n-2}c^i\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right)=0\\
    \Longleftrightarrow\quad& \sum_{k=0}^{n-2}\binom{k}{0}(s_{k+1}'-s_{k+1}) = c\left(\sum_{i=1}^{n-2}c^{i-1}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right)\\
    \Longleftrightarrow\quad& \sum_{k=0}^{n-2}(s_{k+1}'-s_{k+1}) = c\left(\sum_{i=1}^{n-2}c^{i-1}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right).\\
\end{split}
\end{equation}
Following a similar reasoning as the one applied above shows us that picking
\begin{equation}
    c(\delta,d,n)>(n-1)^2(\delta^{-d}-1) \qquad\Longrightarrow\qquad \sum_{k=0}^{n-2}(s_{k+1}-s_{k+1}')=0,
\end{equation}
and requires us to satisfy
\begin{equation}
\begin{split}
    & c\left(\sum_{i=1}^{n-2}c^{i-1}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right)=0\\
    \Longleftrightarrow\quad& \sum_{k=1}^{n-2}\binom{k}{1}(s_{k}'-s_{k}) = c\left(\sum_{i=2}^{n-2}c^{i-2}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right)\\
    \Longleftrightarrow\quad& \sum_{k=1}^{n-2}k(s_{k}'-s_{k}) = c\left(\sum_{i=2}^{n-2}c^{i-2}\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-i+1}')\right).\\
\end{split}
    \label{}
\end{equation}
Once again, then, by choosing 
\begin{equation}
    c(\delta,d,n)>\frac{(n-2)(n-1)^2}{2}(\delta^{-d}-1)\qquad\Longrightarrow\qquad\sum_{k=1}^{n-2}k(s_{k}-s_{k}')=0.
\end{equation}
This reasoning can be repeated recursively: at each step $i$ of the recursion, by imposing a stricter and stricter bound on $c(\delta,d,n)$ we gain more and more conditions that the quantity $\bb{s}'-\bb{s}$ needs to satisfy:
\begin{equation}
    c(\delta,d,n)>(n-1)(\delta^{-d}-1)\sum_{k=i}^{n-2}\binom{k}{i}\qquad\Longrightarrow\qquad\sum_{k=i}^{n-2}\binom{k}{i}(s_{k-i+1}-s_{k-1+1}') = 0.
    \label{eqn::conditions_on_c_for_1to1_i}
\end{equation}
Notice that, every time we increase $i=0\dots n-2$, these conditions involve one less term $s_{k-i+1}-s_{k-i+1}'$, $k=i\dots n-2$: if we were to collect all these conditions within a single linear system, the system would have an upper-triangular structure, and hence be non-singular. This implies that for the set of $n$ independent conditions on $\bb{s}-\bb{s}'$ to hold (we have $n-1$ in \cref{eqn::conditions_on_c_for_1to1_i}, plus one more in \cref{eqn::conditions_on_c_for_1to1_0}), the only possibility is that $\bb{s}\equiv\bb{s}'$. Because of \cref{thm::S_1to1}, though, this also implies $\bb{l}\equiv\bb{l}'$: we have finally reached a contradiction, and proven that indeed $\bb{l}\mapsto\tilde{l}_n$ is one-to-one, under an opportune condition on $c(\delta,d,n)$.
Such condition can be promptly recovered\footnote{This is a consequence of some useful properties of the binomial coefficient, namely the Hockey stick identity \cite{HockeyStick}, and the symmetry of $\binom{k}{i}$ with respect to $i$.} by \cref{eqn::conditions_on_c_for_1to1_i}:
\begin{equation}
    \max_{i=0 \dots n-2}\sum_{k=i}^{n-2}\binom{k}{i} =\max_{i=0 \dots n-2}\binom{n-1}{i+1}=\binom{n-1}{\left\lceil\frac{n-1}{2}\right\rceil}.
    \label{eqn::max_binomial}
\end{equation}
Substituting this in \cref{eqn::conditions_on_c_for_1to1_i}, we recover that it suffices to impose
\begin{equation}
    c(\delta,d,n)>(n-1)(\delta^{-d}-1)\binom{n-1}{\left\lceil\frac{n-1}{2}\right\rceil}.
\end{equation}
\end{proof}

The next few lemmas are needed to bound the elements in the $\tilde{l}_j$ sequence, which in turn are used to prove property \emph{(ii)} in \cref{def::contextual_mapping}.

\begin{lemma}
\label{thm::ltilde_increasing}
    $\tilde{l}_j$ in \cref{eqn::ltilde} is an increasing sequence.
\end{lemma}
\begin{proof}
This can be proven directly: we have in fact, by definition \cref{eqn::ltilde},
    \begin{equation}
    \begin{split}
        \tilde{l}_j>\tilde{l}_{j-1} \quad\Longleftrightarrow\quad&
            l_j + cs_j +\sum_{i=0}^{j-2}c^{i+2}\sum_{k=i}^{j-2}\binom{k}{i}s_{k-i+1}\\
            &>l_{j-1} + cs_{j-1} +\sum_{i=0}^{j-3}c^{i+2}\sum_{k=i}^{j-3}\binom{k}{i}s_{k-i+1}\\
            \prescript{\text{\tiny combine sums}}{}{\quad\Longleftrightarrow\quad}&
            (l_j-l_{j-1})(1-c) +\sum_{i=0}^{j-2}c^{i+2}\binom{j-2}{i}s_{j-1-i}>0\\
            \prescript{\text{\tiny $\binom{j-2}{i}\geq1,c^{i+2}\geq c^2$}}{}{\quad\Longleftarrow\quad}&
            (l_j-l_{j-1})(1-c) +c^2\sum_{i=0}^{j-2}s_{j-1-i}>0\\
            \prescript{\text{\tiny \cref{eqn::partial_sum}}}{}{\quad\Longleftrightarrow\quad}&
            (l_j-l_{j-1})(1-c) +c^2\sum_{i=0}^{j-2}\left(\sum_{k=1}^n l_k - l_{j-1-i}\right)>0\\
            \quad\Longleftrightarrow\quad&(l_j-l_{j-1})(1-c) +c^2\left((j-1)\sum_{k=1}^n l_k - \sum_{k=1}^{j-1}l_{k}\right)>0\\
            \quad\Longleftrightarrow\quad&(1-c)l_j+(c-1)l_{j-1} +c^2(j-2)\sum_{k=1}^n l_k + c^2\sum_{k=j}^{n}l_{k}>0\\
            \quad\Longleftrightarrow\quad&(c^2-c+1)l_j+(c-1)l_{j-1}
            +c^2(j-2)\sum_{k=1}^n l_k + c^2\sum_{k=j+1}^{n}l_{k}>0\\
    \end{split}
    \end{equation}
    Already with $c>1$, all the coefficients are positive (and at least one is non-zero), implying that the condition above is always satisfied and that indeed $\tilde{l}_j$ is an increasing sequence.
\end{proof}



\begin{lemma}
\label{thm::ltilde_bounds}
    Under constraint \cref{eqn::condition_on_c_final}, each term $\tilde{l}_j$, $j>1$ in \cref{eqn::ltilde} is bounded from below by
    $$\tilde{l}_j>c^j\delta,$$
    and each term $\tilde{l}_j$, $1<j<n$ is bounded from above by
    $$\tilde{l}_j<c^{j+1}\delta.$$
\end{lemma}
\begin{proof}
    We start by proving the lower bound. By definition \cref{eqn::ltilde}, we have
    \begin{equation}
        \tilde{l}_j = l_j+cs_j +\sum_{i=0}^{j-2}c^{i+2}\sum_{k=i}^{j-2}\binom{k}{i}s_{k-i+1} 
                    = l_j+cs_j +c^j s_{1} + \sum_{i=0}^{j-3}c^{i+2}\sum_{k=i}^{j-2}\binom{k}{i}s_{k-i+1}.
    \label{eqn::tildelj_simp}
    \end{equation}
    Since by assumption $l_j$ is an ordered sequence without repetitions, for $j>1$ we necessarily have $l_j>l_1\geq0$, and hence $l_j\geq\delta$. All the other terms in \cref{eqn::tildelj_simp} are non-negative, so we can safely claim that
    \begin{equation}
        \tilde{l}_j \geq \delta + c^j\delta > c^j\delta \qquad\forall j>1,
    \end{equation}
    which confirms the lower bound.
    
    For the upper bound, we start again from the definition of $\tilde{l}_j$:
    \begin{equation}
    \begin{split}
        \tilde{l}_{j} &= l_{j} + cs_{j} +\sum_{i=0}^{j-2}c^{i+2}\sum_{k=i}^{j-2}\binom{k}{i}s_{k-i+1}\\
            &<    (\delta^{-d}-1)\delta + c(n-1)(\delta^{-d}-1)\delta + s_1\sum_{i=0}^{j-2}c^{i+2}\binom{j-1}{i+1}\\
            &\leq (n-1)(\delta^{-d}-1)\binom{j-1}{\left\lceil\frac{j-1}{2}\right\rceil}\delta \sum_{i=0}^{j}c^{i}
            = (n-1)(\delta^{-d}-1)\binom{j-1}{\left\lceil\frac{j-1}{2}\right\rceil}\delta \frac{1-c^{j+1}}{1-c},
    \end{split}
    \label{eqn::tildelj_simp2}
    \end{equation}   
    where we used relationship \cref{eqn::max_binomial} and collected all $c$ terms within the sum. Notice that, for a given $a>1$ we have that
    \begin{equation}
        \frac{1-c^{j+1}}{1-c}\leq ac^{j},
        \label{eqn::ratio_of_cs}
    \end{equation}
    provided that $c\geq\frac{a}{a-1}$. In fact,
    \begin{equation}
    \begin{split}
        \frac{1-c^{j+1}}{1-c} \leq ac^{j} \quad\Longleftrightarrow\quad& \frac{1-c^{j+1}-ac^{j}+ac^{j+1}}{1-c} \leq 0 \\
        \quad\Longleftarrow\quad& \frac{1}{a-1}+\left(c-\frac{a}{a-1}\right)c^{j} \geq 0
        \quad\Longleftarrow\quad \frac{1}{a-1} \geq 0\\
    \end{split}
    \end{equation}
    which is always satisfied. After substituting \cref{eqn::ratio_of_cs} in \cref{eqn::tildelj_simp2}, this allows us to write
    \begin{equation}
        \tilde{l}_{j} < a(n-1)(\delta^{-d}-1)\binom{j-1}{\left\lceil\frac{j-1}{2}\right\rceil}\delta c^{j}.
    \end{equation}   
    To prove that $\tilde{l}_{j}<\delta c^{j+1}$, then, it remains to show that
    \begin{equation}
        c \geq a(n-1)(\delta^{-d}-1)\binom{j-1}{\left\lceil\frac{j-1}{2}\right\rceil}\qquad\forall 1<j<n.
    \end{equation}   
    Substituting condition \cref{eqn::condition_on_c_final} in the inequality above, we are left with proving
    \begin{equation}
        \binom{n-1}{\left\lceil\frac{n-1}{2}\right\rceil} \geq \max_{j=2\dots n-1}a\binom{j-1}{\left\lceil\frac{j-1}{2}\right\rceil}=a\binom{n-2}{\left\lceil\frac{n-2}{2}\right\rceil}.
    \end{equation}   
    The outcome depends on the parity of $n$. For $n$ odd, we have
    \begin{equation}
        \binom{n-1}{\left\lceil\frac{n-1}{2}\right\rceil} \geq a\binom{n-2}{\left\lceil\frac{n-2}{2}\right\rceil}
        \qquad\Longleftrightarrow\qquad 2\frac{n-1}{n-1}\geq a,
    \end{equation}   
    to satisfy which it suffices to pick $a=2$. This requires having $c\geq\frac{a}{a-1}=2$, which is automatically satisfied. For $n$ even, on the other hand, the binomial coefficients simplify to
    \begin{equation}
        \binom{n-1}{\left\lceil\frac{n-1}{2}\right\rceil} \geq a\binom{n-2}{\left\lceil\frac{n-2}{2}\right\rceil}
        \qquad\Longleftrightarrow\qquad 2\frac{n-1}{n}\geq a.
    \end{equation}   
    To satisfy this, we need to pick $a=2\frac{n-1}{n}$, which requires $c\geq\frac{a}{a-1}=2\frac{n-1}{n-2}$; however, this too is automatically satisfied by \cref{eqn::condition_on_c_final} provided $n\geq4$. This completes the proof.
\end{proof}


\begin{lemma}
    Under the constraint \cref{eqn::condition_on_c_final}, condition \cref{eqn::last_layer_condition} holds.
\end{lemma}
\begin{proof}
    We remind that condition \cref{eqn::last_layer_condition} is necessary for the correct ``functioning'' of the global shift layer, and it composes of two parts.
    The first part requires that $\tilde{l}_{j}<c^n\delta$ $\forall j<n$. Thanks to \cref{thm::ltilde_increasing}, it suffices to show that $\tilde{l}_{n-1}<c^n\delta$, but this is already granted by the upper bound in \cref{thm::ltilde_bounds}. Analogously, for the second part, we need to show that $\tilde{l}_n>c^n\delta$: for this too we can use the lower bound in \cref{thm::ltilde_bounds}.    
\end{proof}

We finally have all the ingredients to prove the main theorem of this section:
\begin{theorem}
    The map in \cref{eqn::final_q}, given by
    $$\bb{X}\mapsto\bb{q}(\bb{X}) = \bb{u}^T\Psi(\bb{X})$$
    represents a contextual mapping.
\end{theorem}
\begin{proof}
    As defined in \cref{def::contextual_mapping}, a contextual mapping must satisfy two conditions. The first one is that
    \begin{equation}
        q_i(\bb{X})\neq q_j(\bb{X}), \quad\forall i\neq j\qquad\text{and}\qquad\forall\bb{X}\in\mathbb{L}.
    \end{equation}
    This is directly proven by considering \cref{thm::ltilde_increasing}: since $\tilde{l}_j$ is a (strictly) increasing sequence, all its elements are already distinct. The action of the last global shift layer merely translates all these elements by a same quantity, but they remain distinct nonetheless.
    
    The second condition for a contextual mapping is given by
    \begin{equation}
        q_i(\bb{X})\neq q_j(\bb{X}'),\quad\forall i,j\quad\text{and}\quad\forall\bb{X},\bb{X}'\in\mathbb{L},\quad \text{with}\quad\bb{X}\neq\bb{X}'.
    \end{equation}
    We prove that this holds for \cref{eqn::final_q} by directly considering the difference between two components $i,j$ for different inputs:
    \begin{equation}
        q_i(\bb{X})- q_j(\bb{X}') = \tilde{l}_i-\tilde{l}_j' + c^{n+1}\left(\tilde{l}_n-\tilde{l}_n'\right)=0
        \quad\Longleftrightarrow\quad \tilde{l}_i-\tilde{l}_j' = c^{n+1}\left(\tilde{l}_n'-\tilde{l}_n\right).
    \end{equation}
    Notice that, due to \cref{thm::ln_1to1}, we have $\tilde{l}_n-\tilde{l}_n'\neq0$ and particularly, $|\tilde{l}_n-\tilde{l}_n'|\geq\delta$. On the other hand, in light of the bounds in \cref{thm::ltilde_bounds}, we have that the left-hand side $|\tilde{l}_j-\tilde{l}_i|<c^{n}\delta$. Consequently, the two sides can never cancel each other out, and the proof is complete.
\end{proof}
