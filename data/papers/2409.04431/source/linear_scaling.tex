\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{ bbold }

\usepackage[margin=1.1in]{geometry}
\usepackage[ruled,vlined]{algorithm2e}

\definecolor{linkcolor}{RGB}{83,83,182}
\hypersetup{
    colorlinks=true,
    citecolor=linkcolor,
    linkcolor=linkcolor
}

\def\algorithmautorefname{Algorithm}
\def\propautorefname{Proposition}
\def\assumptionautorefname{Assumptions}
\def\lemmaautorefname{Lemma}
\def\corautorefname{Corollary}
\def\corautorefname{Assumption}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{assumption}{Assumptions}

\usepackage[parfill]{parskip}


\usepackage[textwidth=\marginparwidth, textsize=tiny]{todonotes}

\title{Linear scaling for sigmoid attention?}
\author{}
\date{}

\SetKwInput{KwInit}{Initialization}

\begin{document}

\maketitle



Given a sequence $X = (x_1, \dots, x_n)\in\mathbb{R}^{n\times d}$, sigmoid attention outputs a new sequence $(y_1, \dots, y_n)$ where each output token $y_i$ is given by 
$$
y_i = \frac1{n^\alpha}\sum_{j=1}^n\sigma(x_i^TAx_j)W_vx_j
$$
where $\alpha\geq0$ is a scaling power, $A = \frac{W_q^TW_k}{\sqrt{d}}$ is a $d\times d$ matrix and $W_v$ is the values matrix.

\textbf{What is the best value of $\alpha$?}
A reasonable proxy is that $y_i$ ``converges'' as $n$ goes to infinity: the tokens should neither go to $0$ or to infinity.

We can rewrite $y_i$ as a scaled expectation:
$$
y_i = \frac1{n^{\alpha-1}}\times \frac1n\sum_{j=1}^n\sigma(x_i^TAx_j)W_vx_j = \frac1{n^{\alpha-1}}\mathbb{E}_{z\sim p_n}\left[\sigma(x_i^TAz)W_vz\right]
$$
where $p_n$ is the uniform distribution over the sequence $x_1, \dots, x_n$.

Letting $p^*$ the true distribution of the input tokens $x_i$, we know that the expectation over $p_n$ converges to the expectation over $p^*$ as $n$ goes to infinity. So, 
$$
y_i\simeq \frac1{n^{\alpha-1}} \underbrace{\int\sigma(x_i^TAz)W_vz dp^*(z)}_{\text{Independent from }n}
$$
and we see that
\begin{itemize}
    \item If $\alpha > 1$, the tokens collapse to 0
    \item If $\alpha < 1$ the tokens go to infinity
    \item If $\alpha=1$ the tokens converge to a non-trivial limit
\end{itemize}
So $\alpha=1$ seems to be the correct scaling.

\section{Sequence doubling argument}

Assume that we input a sequence and its copy in sigmoid attention: $\hat{X} = (x_1, x_1, \dots, x_n, x_n) \in \mathbb{R}^{2n \times d}$.
We want the output of sigmoid attention to be $\hat{Y} = (y_1, y_1, \dots, y_n, y_n)$.

We see that 
$$
\hat{y}_1 = \frac{1}{(2n)^\alpha}\sum_{j=1}^{2n}\sigma(x_1^TA\hat{x}_j)W_v\hat{x}_j = \frac{2}{(2n)^\alpha}\sum_{j=1}^{n}\sigma(x_1^TAx_j)W_vx_j = \frac2{2^\alpha}y_1
$$
And similarly for all the other output tokens. Hence, only $\alpha=1$ gives this nice property.
\end{document}
