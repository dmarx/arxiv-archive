\section{Introduction}
\label{sec:intro}

The success of modern machine learning can be largely attributed to the attention mechanism \citep{DBLP:journals/corr/BahdanauCB14,DBLP:conf/nips/VaswaniSPUJGKP17}. Attention uses a sequence-to-sequence (seq-to-seq) map to build context-aware token representations.
Classically, attention relies on the softmax function ($\softmaxattn$) to recover token representations as data-dependent convex combinations of values.

Despite its widespread use and effectiveness, softmax in $\softmaxattn$ is not without limitations. For instance, the softmax function can sometimes lead to a concentration of attention on just a few features \citep{DBLP:conf/iclr/YangDSC18,DBLP:conf/icml/GaneaGBS19}, potentially neglecting other informative aspects of the input data. Moreover, applying $\softmaxattn$ requires performing a \textit{row-wise} reduction along the length of the input sequence, which in the case of efficient attention kernels \citep{DBLP:conf/nips/DaoFERR22,DBLP:journals/corr/abs-2307-08691}, slows down computations.
In this work, we relax this constraint by substituting the \textit{row-wise} softmax operation with an \textit{element-wise} sigmoid nonlinearity. We highlight that the central problem with na\"ive sigmoid attention ($\sigmoidattn$) is that of large initial attention norms and propose solutions to alleviate it. \textbf{Our contributions are as follows:}



\begin{enumerate}[itemsep=0pt,leftmargin=*]
    \item We prove $\sigmoidattn$ is a universal function approximator on seq-to-seq tasks (\cref{sec:ufa}).
    \item We analyze $\sigmoidattn$'s regularity and provide its worst-case Jacobian bound (\cref{sec:regularity}).
    \item We extend \textsc{FlashAttention2} \citep{DBLP:conf/nips/DaoFERR22,DBLP:journals/corr/abs-2307-08691} with the sigmoid kernel, reducing kernel inference wall-clock time by up to 17\% and real world inference by up to 8\% (\cref{sec:FlashSigmoidHardwareAwareImplementation}).
    \item We show that $\sigmoidattn$ matches $\softmaxattn$ in various tasks and domains (\cref{sec:experiments}).
\end{enumerate}
