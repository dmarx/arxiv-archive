\section{Related Work}
\label{sec:related}
Recent studies in supervised image classification \citep{DBLP:journals/corr/abs-2110-00476} and self-supervised learning (SSL), including approaches like SigLIP \citep{DBLP:journals/corr/abs-2303-15343}, 
are shifting large-scale machine learning training from output conditional categorical distributions, traditionally parameterized by softmax functions, to richer pointwise Bernoulli conditionals parameterized 
by sigmoid functions. In this study, our focus shifts to refining the model's internal mechanics, specifically by substituting the softmax component of the attention mechanism with a pointwise sigmoid function.

Previous work has explored the replacing softmax with the ReLU activation in both practical \citep{DBLP:journals/corr/abs-2302-06461,DBLP:conf/icml/HronBSN20} and theoretical settings \citep{DBLP:conf/nips/BaiCWXM23,DBLP:conf/nips/Fu00M23}. Other works explores using the ReLU$^2$ activation \citep{DBLP:conf/icml/HuaDLL22}, exploring purely linear attention \citep{DBLP:conf/icml/KatharopoulosV020,DBLP:conf/nips/LuYZZXGXXZ21,DBLP:conf/wacv/KoohpayeganiP24} or cosine-similarity based attention \citep{DBLP:conf/icann/LuoZXWRY18,DBLP:conf/cvpr/Liu0LYXWN000WG22}. 
Our work builds upon these explorations, particularly \cite{wortsman2023replacing}, which replaces softmax with various activation functions scaled by $n^{-\alpha}$, where $n$ corresponds to the sequence length and $\alpha$, a hyper-parameter. However, we find that their formulation does not match expected performance without proper $b$ initialization and the use of LayerScale (\cref{fig:imagenet_top_1_ablations}-a, \cref{sec:appendix_normalization}). 
