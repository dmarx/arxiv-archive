\section{Sigmoid Attention}
\label{sec:methods}

Let $\mX \in \mathbb{R}^{n \times d}$ be the input sequence of $n$ vectors, where each vector has dimension $d$. We define three learnable weight matrices $\mW_q \in \mathbb{R}^{d \times d_{qk}}$, $\mW_k \in \mathbb{R}^{d \times d_{qk}}$, and $\mW_v \in \mathbb{R}^{d \times d_v}$, which are used to compute the queries $\mQ \in \mathbb{R}^{n \times d_{qk}}$, keys $\mK \in \mathbb{R}^{n \times d_{qk}}$, and values $\mV \in \mathbb{R}^{n \times d_v}$ as follows:
\begin{equation}
\mQ = \mX \mW_q, \quad \mK = \mX \mW_k, \quad \text{and} \quad \mV = \mX \mW_v.
\end{equation}
Self-attention~\citep{DBLP:journals/corr/BahdanauCB14,DBLP:conf/nips/VaswaniSPUJGKP17} can be compactly written as
\begin{equation}
\label{eq:attn_short}
\softmaxattn(\mX) = \softmax(\mQ \mK^T / \sqrt{d_{qk}}) \mV,
\end{equation}
where the $\softmax$ function \textit{normalizes each row }of the input matrix.
We  replace the $\softmax$ with
\begin{tcolorbox}[colback=applelightblue, colframe=black, boxrule=1pt, arc=5mm, boxsep=1mm, left=0mm, top=0mm, right=2mm, valign=center]
\begin{align}
\begin{split}
    \label{eq:sigmoid_attn}
    \sigmoidattn(\mX) = \sigma(\mQ\mK^T / \sqrt{d_{qk}})\mV,\\
    \text{with }\sigma:u\mapsto \mathrm{sigmoid}(u + b)\coloneqq (1+e^{-(u+b)})^{-1}.
\end{split}
\end{align}
\end{tcolorbox}
Here, $\sigma$ is applied \textit{element-wise} to the input matrix in \cref{eq:sigmoid_attn}.
The activation function $\sigma$ has a hyper-parameter $b\in\mathbb{R}$. In \cref{app:sigmoid_bias}, we discuss an intuitive way to choose the order-optimal bias term, resulting in $b = -\log(n)$.
This choice of $b$ allows us to make sense of $\sigmoidattn$ for any sequence length.
Indeed, letting $(\vy_1, \dots, \vy_n) = \sigmoidattn(\mX)$ be the output sequence, we have
\begin{equation}
    \label{eq:sigmoid_attn_sequence}
    \vy_i = \sum_{j=1}^n \frac{\exp(\langle \mW_q\vx_i, \mW_k\vx_j\rangle)}{\exp(\langle \mW_q\vx_i, \mW_k\vx_j\rangle) + n}\mW_v\vx_j
    \xrightarrow[n\to+\infty]{} \int \exp(\langle \mW_q\vx_i, \mW_k\vx \rangle)\mW_v\vx d\mu(\vx),
\end{equation}
where 
$\mu = \frac1n\sum_{j=1}^n\delta_{\vx_j}$ is the empirical measure corresponding to $\mX$.
Notably, \cref{eq:sigmoid_attn_sequence} still makes sense in the infinite length limit, where the measure $\mu$ is not a sum of Diracs. \citet{wortsman2023replacing} do not use a bias, and propose a $n^{-1}$ normalization for various attention activations, such as sigmoid and ReLU, but leave the reason as an open question.
Our variable bias has a similar effect in the large $n$ limit, and we posit that recovering a finite output limit as $n$ increases is the why it works in practice.

A multi-head version of \cref{eq:sigmoid_attn} is obtained by combining the outputs of several $\sigmoidattn$, as follows:
\begin{equation}
    \left[\sigmoidattn_1(\bb{X}),\dots,\sigmoidattn_h(\bb{X})\right]\bb{W}_o,
\end{equation}
for a learnable output weight matrix $\bb{W}_o\in\mathbb{R}^{hd_v\times d}$, where $h$ denotes the number of \emph{heads}.



