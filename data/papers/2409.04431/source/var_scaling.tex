\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{ bbold }

\usepackage[margin=1.1in]{geometry}
\usepackage[ruled,vlined]{algorithm2e}

\definecolor{linkcolor}{RGB}{83,83,182}
\hypersetup{
    colorlinks=true,
    citecolor=linkcolor,
    linkcolor=linkcolor
}

\def\algorithmautorefname{Algorithm}
\def\propautorefname{Proposition}
\def\assumptionautorefname{Assumptions}
\def\lemmaautorefname{Lemma}
\def\corautorefname{Corollary}
\def\corautorefname{Assumption}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{assumption}{Assumptions}

\usepackage[parfill]{parskip}


\usepackage[textwidth=\marginparwidth, textsize=tiny]{todonotes}

\title{Scaling based on variance and potential mistake}
\author{}
\date{}

\SetKwInput{KwInit}{Initialization}

\begin{document}

\maketitle



Given a sequence $X = (x_1, \dots, x_n)\in\mathbb{R}^{n\times d}$, sigmoid attention outputs a new sequence $(y_1, \dots, y_n)$ where each output token $y_i$ is given by 
\begin{equation}
y_i = \frac1{n^\alpha}\sum_{j=1}^n\sigma(x_i^TAx_j)W_vx_j    
=  \frac1{n^\alpha}\sum_{j} a_{i,j}W_vx_j
\end{equation}
where $\alpha\geq0$ is a scaling power, $A = \frac{W_q^TW_k}{\sqrt{d}}$ is a $d\times d$ matrix and $W_v$ is the values matrix.

\textbf{What is the best value of $\alpha$?}

What we want to preserve is the 2-norm of each $y_i$
\begin{equation}
    ||y_i||_2\equiv \sqrt{y_i^\intercal y_i}.
\end{equation}
To simplify things, let's set $d=1$ and $W_v=1$,
so we can write
\begin{equation}
y_i 
= \frac1{n^\alpha}\sum_{j} a_{i,j}\,x_j.
\end{equation}
Let's now treat tilde versions of objects as random variables, and let's write
\begin{equation}
\tilde{y}_i 
= \frac1{n^\alpha}\sum_{j} \tilde{a}_{i,j}\,\tilde{x}_j.
\end{equation}
We wish to preserve the expected 2-norm of $\tilde y_i$, which, assuming $\tilde{y}_i$ is zero-centered, is the variance
\begin{equation}
    \mathbb E_{\tilde y_i\sim p_y} [\tilde y_i^2] = \textrm{Var}(\tilde y_i),
\end{equation}
and we would like it such that
\begin{equation}
    \textrm{Var}(\tilde y_i)=\textrm{Var}(\tilde x_i),
\end{equation}
i.e. the attention mechanism is norm (or equivalently) variance preserving.
Let's compute
\begin{align}
\textrm{Var}(y_i) 
&= \textrm{Var}\left(\frac1{n^\alpha}\sum_{j} \tilde{a}_{i,j}\,\tilde{x}_j\right)\\
&= \frac1{n^{2\alpha}}\textrm{Var}\left(\sum_{j} \tilde{a}_{i,j}\,\tilde{x}_j\right)\\
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(  \tilde{a}_{i,j}\,\tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j,k}\textrm{Cov}\left( \tilde{a}_{i,j}\,\tilde{x}_j, \tilde{a}_{i,k}\,\tilde{x}_k\right).
\end{align}

\textbf{Mistake: what happens if we take the $\tilde {a}_{i,j}$ as data-independent weights?}

If we assume the attention matrix $\tilde {a}_{i,j}=\tilde a$ is independent of the data RV $\tilde x_i$, then for zero-centered RVs $X_i$ we can use
\begin{align}
    \textrm{Var}\left(\prod_i X_i\right)
    &=
    \prod_i \textrm{Var}(X_i) & \quad(\textrm{independent } X_i)\\
    \textrm{Var}(X_1X_2) &=\mathbb E\left(X_1^2X_2^2\right)-\left[\textrm{Cov}(X_1,X_2)\right]^2& (\textrm{dependent } X_i)\\
    \textrm{Cov}(X_1X_2,X_3X_4)
    &=
    \textrm{Cov}(X_1,X_3)\,\textrm{Cov}(X_2,X_4)+
    \textrm{Cov}(X_1,X_4)\,\textrm{Cov}(X_2,X_3) &(\textrm{dependent } X_i)
\end{align}
and we see
\begin{align}
\textrm{Var}(y_i) 
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(  \tilde{a}\tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}\textrm{Cov}\left( \tilde{a}\,\tilde{x}_j, \tilde{a}\tilde{x}_k\right)\\
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(\tilde{a}\right)\textrm{Var}\left(  \tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\left[\textrm{Cov}\left( \tilde{a}\,\tilde{x}_j, \tilde{a}\,\tilde{x}_k\right)\right]\\
&= 
\frac1{n^{2\alpha}}
\textrm{Var}\left(\tilde{a}\right)
\sum_{j}\textrm{Var}\left(  \tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\left[
\textrm{Cov}\left( \tilde{a}, \tilde{a}\right) 
\textrm{Cov}\left( \tilde{x}_j, \tilde{x}_k\right)
+
\textrm{Cov}\left( \tilde{a}, \tilde{x}_k\right) 
\textrm{Cov}\left( \tilde{x}_j,  \tilde{a}\right)
\right]\\
&= 
\frac1{n^{2\alpha - 1}}
\textrm{Var}\left(\tilde{a}\right)
\textrm{Var}\left(  \tilde{x}_j\right) 
\end{align}
and to cancel any $n$-dependent growth, we need to choose $\alpha = 1/2$ (i.e. sqrt scaling).

\textbf{Fix: what happens if keep $\tilde {a}_{i,j}$ data-dependent?}

In this case we have
\begin{align}
\textrm{Var}(y_i) 
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(  \tilde{a}_{i,j}\tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}\textrm{Cov}\left( \tilde{a}_{i,j}\,\tilde{x}_j, \tilde{a}_{i,k}\tilde{x}_k\right)\\
&=
\frac1{n^{2\alpha}}\sum_{j}
\left(
\mathbb E[\tilde a_{i,j}^2\tilde x_j^2]
- \left[\textrm{Cov}(\tilde a_{i,j},\tilde x_j)\right]^2
\right)
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\Big[
\textrm{Cov}\left( \tilde{a}_{i,j}, \tilde{a}_{i,k}\right) 
\textrm{Cov}\left( \tilde{x}_j, \tilde{x}_k\right)\nonumber\\
&+
\textrm{Cov}\left( \tilde{a}_{i,j}, \tilde{x}_k\right) 
\textrm{Cov}\left( \tilde{x}_j,  \tilde{a}_{i,k}\right)
\Big]\\
&=
\frac1{n^{2\alpha}}\sum_{j}
\left(
\mathbb E[\tilde a_{i,j}^2\tilde x_j^2]
- \left[\textrm{Cov}(\tilde a_{i,j},\tilde x_j)\right]^2
\right)
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\textrm{Cov}\left( \tilde{a}_{i,j}, \tilde{x}_k\right) 
\textrm{Cov}\left( \tilde{x}_j,  \tilde{a}_{i,k}\right)
\end{align}
















\end{document}
