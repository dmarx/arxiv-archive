---
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Scaling based on variance and potential mistake
---





Given a sequence $X = (x_1, \dots, x_n)\in\mathbb{R}^{n\times d}$, sigmoid attention outputs a new sequence $(y_1, \dots, y_n)$ where each output token $y_i$ is given by $$y_i = \frac1{n^\alpha}\sum_{j=1}^n\sigma(x_i^TAx_j)W_vx_j    
=  \frac1{n^\alpha}\sum_{j} a_{i,j}W_vx_j$$ where $\alpha\geq0$ is a scaling power, $A = \frac{W_q^TW_k}{\sqrt{d}}$ is a $d\times d$ matrix and $W_v$ is the values matrix.

**What is the best value of $\alpha$?**

What we want to preserve is the 2-norm of each $y_i$ $$||y_i||_2\equiv \sqrt{y_i^\intercal y_i}.$$ To simplify things, let’s set $d=1$ and $W_v=1$, so we can write $$y_i 
= \frac1{n^\alpha}\sum_{j} a_{i,j}\,x_j.$$ Let’s now treat tilde versions of objects as random variables, and let’s write $$\tilde{y}_i 
= \frac1{n^\alpha}\sum_{j} \tilde{a}_{i,j}\,\tilde{x}_j.$$ We wish to preserve the expected 2-norm of $\tilde y_i$, which, assuming $\tilde{y}_i$ is zero-centered, is the variance $$\mathbb E_{\tilde y_i\sim p_y} [\tilde y_i^2] = \textrm{Var}(\tilde y_i),$$ and we would like it such that $$\textrm{Var}(\tilde y_i)=\textrm{Var}(\tilde x_i),$$ i.e. the attention mechanism is norm (or equivalently) variance preserving. Let’s compute $$\begin{aligned}
\textrm{Var}(y_i) 
&= \textrm{Var}\left(\frac1{n^\alpha}\sum_{j} \tilde{a}_{i,j}\,\tilde{x}_j\right)\\
&= \frac1{n^{2\alpha}}\textrm{Var}\left(\sum_{j} \tilde{a}_{i,j}\,\tilde{x}_j\right)\\
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(  \tilde{a}_{i,j}\,\tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j,k}\textrm{Cov}\left( \tilde{a}_{i,j}\,\tilde{x}_j, \tilde{a}_{i,k}\,\tilde{x}_k\right).
\end{aligned}$$

**Mistake: what happens if we take the $\tilde {a}_{i,j}$ as data-independent weights?**

If we assume the attention matrix $\tilde {a}_{i,j}=\tilde a$ is independent of the data RV $\tilde x_i$, then for zero-centered RVs $X_i$ we can use $$\begin{aligned}
    \textrm{Var}\left(\prod_i X_i\right)
    &=
    \prod_i \textrm{Var}(X_i) & \quad(\textrm{independent } X_i)\\
    \textrm{Var}(X_1X_2) &=\mathbb E\left(X_1^2X_2^2\right)-\left[\textrm{Cov}(X_1,X_2)\right]^2& (\textrm{dependent } X_i)\\
    \textrm{Cov}(X_1X_2,X_3X_4)
    &=
    \textrm{Cov}(X_1,X_3)\,\textrm{Cov}(X_2,X_4)+
    \textrm{Cov}(X_1,X_4)\,\textrm{Cov}(X_2,X_3) &(\textrm{dependent } X_i)
\end{aligned}$$ and we see $$\begin{aligned}
\textrm{Var}(y_i) 
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(  \tilde{a}\tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}\textrm{Cov}\left( \tilde{a}\,\tilde{x}_j, \tilde{a}\tilde{x}_k\right)\\
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(\tilde{a}\right)\textrm{Var}\left(  \tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\left[\textrm{Cov}\left( \tilde{a}\,\tilde{x}_j, \tilde{a}\,\tilde{x}_k\right)\right]\\
&= 
\frac1{n^{2\alpha}}
\textrm{Var}\left(\tilde{a}\right)
\sum_{j}\textrm{Var}\left(  \tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\left[
\textrm{Cov}\left( \tilde{a}, \tilde{a}\right) 
\textrm{Cov}\left( \tilde{x}_j, \tilde{x}_k\right)
+
\textrm{Cov}\left( \tilde{a}, \tilde{x}_k\right) 
\textrm{Cov}\left( \tilde{x}_j,  \tilde{a}\right)
\right]\\
&= 
\frac1{n^{2\alpha - 1}}
\textrm{Var}\left(\tilde{a}\right)
\textrm{Var}\left(  \tilde{x}_j\right) 
\end{aligned}$$ and to cancel any $n$-dependent growth, we need to choose $\alpha = 1/2$ (i.e. sqrt scaling).

**Fix: what happens if keep $\tilde {a}_{i,j}$ data-dependent?**

In this case we have $$\begin{aligned}
\textrm{Var}(y_i) 
&= \frac1{n^{2\alpha}}\sum_{j}\textrm{Var}\left(  \tilde{a}_{i,j}\tilde{x}_j\right) 
+ \frac1{n^{2\alpha}}\sum_{j\neq k}\textrm{Cov}\left( \tilde{a}_{i,j}\,\tilde{x}_j, \tilde{a}_{i,k}\tilde{x}_k\right)\\
&=
\frac1{n^{2\alpha}}\sum_{j}
\left(
\mathbb E[\tilde a_{i,j}^2\tilde x_j^2]
- \left[\textrm{Cov}(\tilde a_{i,j},\tilde x_j)\right]^2
\right)
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\Big[
\textrm{Cov}\left( \tilde{a}_{i,j}, \tilde{a}_{i,k}\right) 
\textrm{Cov}\left( \tilde{x}_j, \tilde{x}_k\right)\nonumber\\
&+
\textrm{Cov}\left( \tilde{a}_{i,j}, \tilde{x}_k\right) 
\textrm{Cov}\left( \tilde{x}_j,  \tilde{a}_{i,k}\right)
\Big]\\
&=
\frac1{n^{2\alpha}}\sum_{j}
\left(
\mathbb E[\tilde a_{i,j}^2\tilde x_j^2]
- \left[\textrm{Cov}(\tilde a_{i,j},\tilde x_j)\right]^2
\right)
+ \frac1{n^{2\alpha}}\sum_{j\neq k}
\textrm{Cov}\left( \tilde{a}_{i,j}, \tilde{x}_k\right) 
\textrm{Cov}\left( \tilde{x}_j,  \tilde{a}_{i,k}\right)
\end{aligned}$$
