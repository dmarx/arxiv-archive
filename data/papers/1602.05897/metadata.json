{
  "arxivId": "1602.05897",
  "title": "Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity",
  "authors": "Amit Daniely, Roy Frostig, Yoram Singer",
  "abstract": "We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power.",
  "url": "https://arxiv.org/abs/1602.05897",
  "issue_number": 496,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/496",
  "created_at": "2024-12-30T08:27:37.538781",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-29T11:06:20.722Z",
  "main_tex_file": null,
  "published_date": "2016-02-18T18:14:19Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI",
    "cs.CC",
    "cs.DS",
    "stat.ML"
  ]
}