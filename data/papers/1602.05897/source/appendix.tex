\section{Multivariate Activations}
\begin{itemize}
\item Explain that activations like ReLU+MaxPool correspond to kernels of the form $f(k(x_1,y_1),k(x_2,y_2),k(x_3,y_3),k(x_4,y_4))$.
\item Explain how to calculate the correct initialization
\item Analyse a simple example. Is this operation induces a kernel that implies small norm for functions $f:I\to I$ such that $f\circ F\approx f$ for Liphshitz $F:I\to I$?
Leave ReLU+MaxPool to future work.
\end{itemize}

\section{Dropped discussion parts}

\paragraph{$\ch_\cs$ as a proxy.} One of the motivations for developing a theory of deep learning (and theory in general), is to establish concepts, notions and connections that will enable one to speak, think and reason about the learning process in an intuitive and effective manner.

As demonstrated in this paper, adopting $\ch_\cs$ as a proxy for the network can potentially lead to  such a theory.
Given that, instead of proximity, it might be more relevant to measure the correlation between the performance of optimizing of $\ch_\cs$ and the performance of learning the network.

\paragraph{Modularity, transparency and automation of NN learning.}
As noted above, our formalism and theory result with a modular and more lucid process of NN learning.
In particular, it makes a clear distinction between input-representation, modelling, network-design, initialization and optimization:
In the input representation part one should present the inputs as a sequence of unit vectors.
The modelling part consists of designing a skeleton. As explained earlier, to do so, one has to give a rough description of the route of the computation from an input to an output.
In many cases, this description is intuitive and/or can be guided by a physical instrument (e.g., our brain) that already computes the desired function. Therefore, designing a skeleton might be an effective way to exploit human expertise and prior knowledge.
The next step is the concrete architecture design. In this part the realization factor is decided. On one hand, a large realization factor will result with a more accurate approximation of $\ch_\cs$. On the other hand, it will create a bigger network that is expensive in terms of training time, testing time, data and memory. Of course, at this point, one can decide to use the kernel directly.
Lastly, a random initialization (as prescribed in Definition \ref{def:rand_weights}) followed by optimization step is employed.


In the future, one might hope that the input representation, the architecture design, and the optimization parts will be standardize and automatize. All the developer has to do will be to specify a skeleton.
Such an automation might be useful for improving the state of the art in various tasks. Furthermore, it will allow un-professionals to use machine learning effectively. Hopefully, with little cost of mastering ML, medical doctors, biology researchers, economists and so on will be able to make a powerful use of machine learning, that is currently done only by ML experts.


\paragraph{NN learning and learning theory} Learning theory seems to stand in a very sharp contrast with the current practice of NN-learning. From learning theoretic perspective~\cite{KlivansSh06, daniely2013average, danielySh2014, daniely2015complexity}, it seemed like when one goes to hypothesis classes even slightly more expressive than linear classes, supervised learning problems become extremely hard, and efficiently predicting even just slightly better than random is impossible. On the other hand, in practice, NN-algorithms seems to learn classes that are way more expressive than linear classes. Our results can potentially bridge this gap, as they suggest that what is actually learned by NN is close to (carefully and wisely crafted) linear classes.
Bringing learning theory closer to learning practice is useful, as it can make NN-learning much more transparent. In addition, this might enable to combine the various ideas and algorithms developed in the learning theory community in the context of linear methods. This can also be useful for enriching future interplay between NN-learning and learning theory. We outline next two more concrete consequences to learning theory research.


\paragraph{Statistical Complexity of NN.} As mentioned in section \ref{sec:cur_understand}, understanding NN-learning is traditionally decomposed into three parts -- the statistical part (why/when good performance  on the sample implies good performance), the expressive part (why/when a good hypothesis exists), and the computational part (why/when algorithms can find good weights). Our theory mostly concerns the computational part, where our state of understanding was the worst. Yet, as we describe in this paragraph and the next, it also suggests new angles to investigate the two other parts.

As for the statistical part, classic results \cite{BaumHa89} show that when the number of examples is larger than the number of weights, small empirical loss implies small loss. Other results \cite{Bartlett98, behnam2015norm} show a similar conclusion, when the number of examples is larger than sum of the absolute values of the weights, raised to the depth of the network.
While these results certainly shed light on the statistical part, in many practical cases, it is not true that the number of examples is large enough to make these bound meaningful (see, however, a recent progress~\cite{hardt2015train}). On the other hand, in the realm of kernel learning, we have better~\cite{BoucheronBoLu05, BartlettMe02} statistical bounds. Bringing NN-learning closer to kernel learning might therefore help to understand better the statistical aspects of NN learning. We remark that a similar approach was suggested by Hazan and Jaakkola~\cite{hazan2015steps}.


\paragraph{Expressive Power of NN.} Most theoretical studies in the context of the expressive power of neural networks concerned the expressive power of {\em all} functions expressed by a neural net. Some papers investigated what can be expressed by such networks~\cite{karp1980some, Barron93}. Other papers~\cite{rossman2015average, hastad1986almost, eldan2015power, cohen2015expressive} showed that deeper networks can express strictly more functions than shallower ones. These studies are useful for sketching the limitation of NN learning. Yet, deriving conclusions and developing intuition based on these studies might be misleading. Indeed, the fact that a function can be expressed by the network does not mean that the network can actually learn it. Hardness results show that many functions expressed by NN are hard to learn. For a more concrete example, fully connected nets of depth $2$ are already very expressive, and can express parities on any subset of variables. Yet, this class of function is provably hard to lo learn by NN algorithms~\cite{blum2003noise}.

Hence, investigating the expressive power of compositional kernel spaces might be more useful for gaining intuition about what can actually be learned by NN. We remark that previous studies of the sign-rank and the margin complexity~\cite{razborov2008sign, sherstov2007halfspace, linial2007complexity, klivans2001learning, ben2003limitations} can already provide answers to questions of the form: ``Is the class $\ch$ expressible by a compositional kernel space?" In particular, they show that many classes, including DNF formulas, Parities, Networks of depth $\ge 2$, and more cannot be realized by such spaces.


\paragraph{Human and biological perception.} While non of the authors is an expert in Neuroscience \todo{Yoram?}, we believe that our theory might shed some light on human (and other species) perception. Roughly speaking, suppose we make the (probably oversimplified) assumption that from a coarse perspective, the structure of the nerve-system is rather similar among different human beings, but from a finer perspective, the local connection between neurons is random.
In this case, our theory suggest that the concepts humans can at all perceive tend to be compositional. Namely, each concept is composed from several simpler concepts. Furthermore, the concepts expressible by the nerve-system are rather uniform among different human beings, and moreover, can be learned. We believe that this is fairly consistent with what is observed in reality.

\paragraph{Kernel Learning vs NN Learning.} Our theory provides a common
approach to kernel learning and NN-learning. Any skeleton has an NN variant,
as well as a kernel variant. It is therefore natural to compare these two
approaches. Kernel learning optimizes directly on the compositional kernel
space, is more robust, and has much fewer hyper-parameters. On the other hand,
NN learning has an additional ``elbow room", in the form of the fine tuning.
As for training and testing times, as well as memory usage, it is not clear
a-priory which method is favourable. For kernel methods, these quantities
depends on the number of support vectors, while for NN methods, they depends
of the realization factor.

So, what is the bottom line? Which method is better? The current state of
affairs seems to suggest that NN methods are superior. Is this the definite
answer? And, if yes, then why?  An immediate answer would attribute their
exceptional performance to the fine-tuning step. While this certainly might be
the case, one can still suggest a different explanation. In NN learning, via
the design of the network, the developer has a lot of flexibility to express
her prior knowledge. In the way NN learning is currently carried out, the
process of transforming prior knowledge into a network architecture is
relatively intuitive, and is similar to the process of designing a skeleton.
In contrast, the arsenal of kernels that is used in practice is less rich.
E.g., when kernel methods are used for image recognition, one often uses one
of the standard symmetric kernels (Linear, RBF or polynomial) over
pre-designed features. Our paper and other recent papers~\cite{strobl2013deep,
bo2011object, mairal2014convolutional, cho2009kernel} can potentially change
this state of affairs. Indeed, skeletons provide a formalism in which the
developer has a lot of flexibility in designing a kernel. Furthermore, the
design is intuitive and enable to express prior knowledge in a convenient way,
that is even simpler than designing a network. Checking whether compositional
kernels form an alternative to NN is largely left to future work.


\section{Dropped text}
Let us now illustrate the power of compositional kernels through examples.
In the first example we make dual activations the same and equal to
$\hat\sigma(\rho)=\frac{\rho+\rho^2}{2}$. As we see later, it is simple to
show using Hermite polynomials that the activation function is
$\sigma(x)=\frac{x^2+\sqrt{2}x-1}{2}$. In this setting, each node can be
viewed as a linear functional on its inputs and then applying a second degree
polynomial with bounded coefficients. Concretely,
we can define a capacity measure $\Delta_\cs$ such that functions obtained by
simple compositions according to $\cs$ have small capacity.
\todo{This seem unclear and somewhat redundant --- \\
	It also holds
	that $\|f\|^2_\cs\le \Delta_\cs(f)$, hence such function also have small
	complexity according to\footnote{We note that this is not true that {\em
			all} small norm functions in $\ch_\cs$ are ones with small $\Delta_\cs$, but
		rather, there are much more small norm functions.} $\|\cdot\|_\cs$.}

The complexity of $\cs$ is defined inductively by following its DAG structure.
First, for an input node $v$ corresponding to an input coordinate $\x^i$ and a
function of the form $f(\x)=\inner{\w,\x^i}$ we let $\Delta_v(f)=\|\w\|^2$.
Next, for a node $v$ whose incoming nodes is the set $\IN(v)$ and and a
function of the form $f=\frac{\sum_{u\in\IN(v)} f_u}{|\IN(v)|}$, we define
$\tilde{\Delta}_{v}(f)=\frac{\sum_{u\in\IN(v)} \Delta_{u}(f_u)}{|\IN(v)|}$.
Next, we define the capacity of a function of the form
$f(\cdot)=ag(\cdot)+bg_1(\cdot)g_2(\cdot)$ as
$\Delta_v(f) =
2a^2\tilde{\Delta}_{v}(g) +
2b^2\tilde{\Delta}_{v}(g_1) \tilde{\Delta}_{v}(g_2).$
Finally, if $v$ is the output node of $\cs$ we define the overall complexity
of $\cs$ as $\Delta_\cs=\Delta_v$.

The complexity of $\cs$ is defined inductively by following its DAG structure.
First, for an input node $v$ corresponding to an input coordinate $\x^i$ and a
function of the form $f(\x)=\inner{\w,\x^i}$ we let $\Delta_v(f)=\|\w\|^2$.
Next, for a node $v$ whose incoming nodes is the set $\IN(v)$ and and a
function of the form $f=\frac{\sum_{u\in\IN(v)} f_u}{|\IN(v)|}$, we define
$\tilde{\Delta}_{v}(f)=\frac{\sum_{u\in\IN(v)} \Delta_{u}(f_u)}{|\IN(v)|}$.
Next, we define the capacity of a function of the form
$f(\cdot)=ag(\cdot)+bg_1(\cdot)g_2(\cdot)$ as
$\Delta_v(f) =
2a^2\tilde{\Delta}_{v}(g) +
2b^2\tilde{\Delta}_{v}(g_1) \tilde{\Delta}_{v}(g_2).$
Finally, if $v$ is the output node of $\cs$ we define the overall complexity
of $\cs$ as $\Delta_\cs=\Delta_v$.

\subsection{Examples and Remarks}
We next explain how to construct useful network topologies. We will concentrate on topologies that result with kernel spaces that are expressive for visual and acoustic domains.
These topologies are composed of layers. Each layer is of one of two types: Convolutional or Fully Connected.

\paragraph{Fully connected layers.}
The topology of of a fully connected layer consists of $n$ input nodes $v_1,\ldots,v_n$, and a single output node $v_{\out}$ that is connected to all input nodes. The dual activation the output node can be arbitrary.

We note that the kernel space corresponding to the output node is $\ch_v=\psi\left(\ch_1,\ldots,\ch_n\right)$. This means small-norm functions in $\ch_v$ consist of low degree polynomials applied on functions with small-norm functions in $\ch_1,\ldots,\ch_n$. For examples, these can be functions like $f_1\cdot f_2+g_2\cdot h_2$ where $f_1\in\ch_1$ and $f_2,g_2,h_2\in\ch_2$.


\begin{remark}[Duplicate nodes are redundant] The reader familiar with neural networks might wonder why we have just a single output node. We note that in our framework there is no point to have two nodes with the same input nodes and the dual activation. Indeed two such nodes $v_1$ and $v_2$ will correspond to the same kernel space $\ch$. Hence, if we want a node in the layer to define a kernel space involving $\ch$, we can connect it to either $v_1$ or $v_2$. Hence, there is no point to have both of them.
The fact that there is no need to duplicate nodes, makes the network topologies much more compact than their counterpart neural-nets.
\end{remark}

\begin{remark}[Consecutive fully connected layers are redundant]
Assume that we have two consecutive fully connected layers. That is, we have a DAG with nodes $u_1,u_2$ and $v_1,\ldots,v_n$, where $v_1,\ldots,v_n$ are input nodes connected to $u_1$, $u_1$ is an internal node connected to $u_2$ and labelled by the dual activation $\hat\sigma_1$, and $u_2$ is an output labelled by the dual activation $\hat\sigma_2$. Note that is $\ch_1,\ldots,\ch_n$ are the kernel spaces corresponding to $v_1,\ldots,v_n$ then the kernel space corresponding to $u_2$ is $\hat\sigma_2\left(\hat\sigma_2\left(\frac{\ch_1\oplus\ldots\oplus\ch_n}{n}\right)\right)=\hat\sigma_2\circ\hat\sigma_2\left(\frac{\ch_1\oplus\ldots\oplus\ch_n}{n}\right)$. Hence, the composition of the two layers can be replaced by a single layer with $n$ input nodes and a single output node labelled by $\hat{\sigma}_2\circ\hat\sigma_1$.
\end{remark}


\paragraph{Convolutional layers.}
For simplicity, we will define convolutional layers applied on one dimensional greed. A similar definition can be given for convolutions on the more popular two and three dimensional grids.
A convolutional layer has $n$ input nodes $v_1,\ldots,v_m$ and $k$ output nodes $u_1,\ldots,u_k$. It is defined by a dual activation $\hat\sigma$, a {\em window size} $1\le w\le n$ and a {\em stride} $1\le s \le n$. It is assumed that $n=s\cdot (k-1) + w$. The dual activations of all output nodes will be $\hat\sigma$. For $1\le i \le k$, the output node $u_i$ is connected to the input nodes $v_{s(i-1)+1},\ldots,v_{s(i-1)+w}$.

\begin{remark}[Duplicate channels are redundant]
The reader that is familiar with convolutional nets might wonder why we only have a single channel. As with the fully connected layers, this is because there is no point to have two nodes with the same dual activation and the same input nodes. Again, this make the structure of the topology much more compact than the corresponding convolutional neural networks.
\end{remark}

\subsection{The ReLU activation}
Consider the activation $\sigma(x)=\max(0,x)$. To calculate $\hat\sigma$ we will calculate the Hermite expansion of $\sigma$. Let $H_0,H_1,\ldots$ be the hermite polynomials. For $n\ge 0$, we have
\begin{eqnarray*}
\int_{-\infty}^\infty \sigma(x)H_n(x)e^{-\frac{x^2}{2}}dx &=& \int_{0}^\infty xH_n(x)e^{-\frac{x^2}{2}}dx
\\
&=& \int_{0}^\infty H_{n+1}(x) + nH_{n-1}(x)e^{-\frac{x^2}{2}}dx
\end{eqnarray*}
It is therefore left to calculate the numbers $\int_{0}^\infty H_{n}(x)e^{-\frac{x^2}{2}}dx$ for $n=0,1,\ldots$. For $n=0$ we have $\int_{0}^\infty H_{n}(x)e^{-\frac{x^2}{2}}dx = \int_{0}^\infty e^{-\frac{x^2}{2}}dx = \sqrt{\frac{\pi}{2}}$. For $n\ge 1$ we have
\[
\int_{0}^\infty H_{n}(x)e^{-\frac{x^2}{2}}dx = -\frac{1}{n}\left[ H'_{n}(x)e^{-\frac{x^2}{2}}\mid^\infty_0\right] = H_{n-1}(0) = \begin{cases}0 &\text{if }n\text{ is even}\\
(-1)^{\frac{n-1}{2}}(n-2)!!&\text{if }n\text{ is odd}\end{cases}
\]
To summarize, for $n\ge 2$ we have $\int_{-\infty}^\infty \sigma(x)H_n(x)e^{-\frac{x^2}{2}}dx = 0$ for odd $n$. For even $n\ge 2$ we have
\[
\int_{-\infty}^\infty \sigma(x)H_n(x)e^{-\frac{x^2}{2}}dx = (-1)^{\frac{n}{2}}(n-1)!! - n\cdot (-1)^{\frac{n}{2}}(n-3)!! =  (-1)^{\frac{n+2}{2}}(n-3)!!
\]
Recall that $h_n=\frac{1}{\sqrt{n!}} H_n$. Hence, for $n\ge 2$,
\[
\int_{-\infty}^\infty \sigma(x)h_n(x)\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx = \begin{cases} 0 & \text{if }n\text{ is odd}\\
\frac{(-1)^{\frac{n+2}{2}}(n-3)!!}{\sqrt{2\pi n!}} & \text{if }n\text{ is even}\end{cases}
\]
For $n=0,1$ we can calculate directly (recall that $h_0(x)=1, h_1(x)=x$):
\[
\int_{-\infty}^\infty \sigma(x)h_0(x)\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx = \frac{1}{2}\E_{X\sim\cn(0,1)}|X| = \frac{1}{\sqrt{2\pi}}
\]
\[
\int_{-\infty}^\infty \sigma(x)h_1(x)\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx = \frac{1}{2}\E_{X\sim\cn(0,1)}X^2 = \frac{1}{2}
\]
From the last three equations and lemma \ref{lem:dual_activation} it follows that $\hat\sigma(\rho)=\sum_{n=0}^\infty b_n\rho^n$ where
\[
b_n=\begin{cases}
\frac{((n-3)!!)^2}{2\pi n!} & \text{if }n\text{ is even}
\\
\frac{1}{4} & \text{if }n = 1
\\
0 & \text{if }n\text{ is odd }\ge 3
\end{cases}
\]




\section{Related Work and Our Contribution (Internal)}
\subsection*{Deep Kernels}
\begin{itemize}
\item \cite{scholkopf1998prior, grauman2005pyramid} Introduce a preliminary version of compositional kernel, and in particular the convolutional version. No formal definition, just a few concrete examples.
\item \cite{strobl2013deep} Give a definition of fairly general compositional kernels. Don't restrict to the sphere. Bit messy. Stays in the ``kernel world".
\item \cite{bo2011object} Define a fairly general version of compositional kernels. Not identical, but similar to ours. Do not restrict to the sphere. Concentrate on vision. Stay in the ``kernel world"
\item \cite{cho2009kernel} Introduce compositinal kernels for fully connected multilayered nets. Calculate the dual activation of activations of the form $\sigma(x)=\mathrm{Step}(x)x^n=(x)^n_+$
\item \cite{mairal2014convolutional} Introduce compositinal kernels for networks with fully connected and convolutional layers. Use exponential kernels and activations as local kernels (with RR features). Remark that "leveraging the kernel interpretation of our convolutional neural networks to better understand the theoretical properties of the feature spaces that these networks produce". Also interesting: They show that the convolution patches learned by {\em unsupervised} training of the net is very similar to what is obtained by supervised training.
\item \cite{anselmi2015deep} Suggest to view NN as kernel machines. Don't really do anything. Concentrate of CNN
\item \cite{parviainen2010interpreting, parviainen2013connection, hazan2015steps} Suggest to view NN as compositinal kernels (they call this ``extreme learning" of ``infinite network"). Don't do any analysis. \cite{hazan2015steps} claim that it shed light on the sample complexity.
\item \cite{montavon2011kernel} Show that deeper layers of a trained net provides better representations as we go deeper. Not clear that is related to us.
\end{itemize}

\subsection*{Random Features / Depth-2 nets}
\begin{itemize}
\item \cite{RahimiRe07, rahimi2009weighted} Analyse the case of the RBF kernel
\item \cite{pennington2015spherical} Analyse polynomial kernels on the sphere.
\end{itemize}

\subsection*{Our Contribution}
\begin{itemize}
\item General scheme for connecting NN to Kernels. Previous paper gave fairly general definitions of compositional kernels (and of course NN). They also made connections, but only for fully connected and conv nets. They didn't prove that the connection is valid in any form (except 2 layer nets)
\item First analysis that the random initialization converges to the correct kernel. Previous papers ``guessed" the correct kernel (for conv nets and fully connected nets), but did not provide any formal analysis in what sense the random initialization approximated the kernel (except the case of depth $2$ fully connected nets that is well studied). Some papers make the ``assumption" that the number of hidden neurons is infinite.
\item We are the first to connect the dual activation to the Hermite expansion. This does not result with computation of new dual activations, but does help to see the big picture.
\item Analysis of compositional kernel spaces.
\item We are the first that try to understand the architecture trough the kernel space. This was suggested in previous work as an open direction~\cite{mairal2014convolutional}, but no formal analysis was given. Should emphasize this.
\item We restrict to the sphere. On one hand this somewhat limit the generality. On the other hand, it makes the analysis far more elegant.
It also implies some stuff (like the ``correct normalization" of the activations).
This also seems like a reasonable thing to do in practice.
\item The formalism of computation skeletons.
\end{itemize}


