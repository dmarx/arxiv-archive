\section{Discussion}\label{sec:discuss}

\paragraph{Role of initialization and training.} Our results surface the
question of the extent to which random initialization accounts for the success of
neural networks. While we mostly leave this question for future research, we
would like to point to empirical evidence supporting the important role of
initialization. First, numerous researchers and practitioners demonstrated
that random initialization, similar to the scheme we analyze, is crucial to
the success of neural network learning (see for
instance~\cite{glorot2010understanding}). This suggests that starting from
arbitrary weights is unlikely to lead to a good solution. Second,
several studies show that the contribution of optimizing the representation
layers is relatively small~\cite{saxe2011random, jarrett2009best,
pinto2009high, pinto2012evaluation, cox2011beyond}. For example, competitive
accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by
optimizing merely the last layer~\cite{mairal2014convolutional,
saxe2011random}. Furthermore, Saxe et al.~\cite{saxe2011random} show that
the performance of training the last layer is quite correlated with training
the entire network. The effectiveness of optimizing solely the last layer is
also manifested by the popularity of the random features
paradigm~\cite{rahimi2009weighted}. Finally, other studies show that the
metrics induced by the initial and fully trained representations are not
substantially different. Indeed, Giryes et al.~\cite{giryes2015deep}
demonstrated that for the MNIST and CIFAR-10 datasets the distances'
histogram of different examples barely changes when moving from the
initial to the trained representation. For the ImageNet dataset the
difference is more pronounced yet still moderate.

\paragraph{The role of architecture.} By using skeletons and compositional
kernel spaces, we can reason about functions that the network can actually
learn rather than merely express. This may explain in retrospect past
architectural choices and potentially guide future choices. Let us consider
for example the task of object recognition. It appears intuitive, and is
supported by visual processing mechanisms in mammals, that in order to
perform object recognition, the first processing stages are confined to
local receptive fields. Then, the result of the local computations are
applied to detect more complex shapes which are further combined towards a
prediction. This processing scheme is naturally expressed by convolutional
skeletons.  A two dimensional version of Example~\ref{exam:diff_struct}
demonstrates the usefulness of convolutional networks for vision and speech
applications.

The rationale we described above was pioneered by LeCun and
colleagues~\cite{lecun1998gradient}. Alas, the mere fact that a network can
express desired functions does not guarantee that it can actually learn
them.  Using for example Barron's theorem~\cite{Barron93}, one may claim
that vision-related functions are expressed by fully connected two layer
networks, but such networks are inferior to convolutional networks in
machine vision applications. Our result mitigates this gap. First, it
enables use of the original intuition behind convolutional networks in order
to design function spaces that are provably learnable. Second, as detailed
in Example~\ref{exam:diff_struct}, it also explains why convolutional
networks perform better than fully connected networks.

\paragraph{The role of other architectural choices.} In addition to the
general topology of the network, our theory can be useful for understanding
and guiding other architectural choices. We give two examples. First,
suppose that a skeleton $\cs$ has a fully connected layer with the dual
activation $\hat{\sigma}_1$, followed by an additional fully connected layer
with dual activation $\hat{\sigma}_2$. It is straightforward to verify that if these
two layers are replaced by a single layer with dual activation
$\hat{\sigma}_2\circ \hat{\sigma}_1$, the corresponding compositional kernel
space remains the same. This simple observation can be useful in
potentially saving a whole layer in the corresponding networks.

The second example is concerned with the ReLU activation, which is one of the
most common activations used in practice. Our theory suggests a
somewhat surprising explanation for its usefulness. First, the dual kernel
of the ReLU activation enables expression of non-linear functions. However, this
property holds true for many activations. Second,
Theorem~\ref{thm:main_ker_ReLU} shows that even for quite deep networks with
ReLU activations, random initialization approximates the corresponding
kernel. While we lack a proof at the time of writing, we conjecture that this property holds true for many other activations. 
What is then so special about the ReLU? Well,
an additional property of the ReLU is being {\em
positive homogeneous}, i.e.\ satisfying $\sigma(ax)=a\sigma(x)$ for all
$a\ge 0$.  This fact makes the ReLU activation robust to small perturbations
in the distribution used for initialization. Concretely, if we multiply the
variance of the random weights by a constant, the distribution of the
generated representation and the space $\ch_\w$ remain the same up to a
scaling. Note moreover that training algorithms are sensitive to the
initialization.  Our initialization is very similar to approaches used in
practice, but encompasses a small ``correction'', in the form of a
multiplication by a small constant which depends on the activation.  For
most activations, ignoring this correction, especially in deep networks,
results in a large change in the generated representation. The ReLU
activation is more robust to such changes. We note that
similar reasoning applies to the max-pooling operation.

\paragraph{Future work.} Though our formalism is fairly general, we mostly analyzed
fully connected and convolutional layers. Intriguing
questions remain, such as the analysis of max-pooling and recursive
neural network components from the dual perspective. On the algorithmic side,
it is yet to be seen whether our framework can help in understanding
procedures such as dropout~\cite{srivastava2014dropout} and
batch-normalization~\cite{ioffe2015batch}. Beside studying existing
elements of neural network learning, it would be interesting to devise new
architectural components inspired by duality.
%
More concrete questions are concerned with quantitative improvements of the
main results. In particular, it remains open whether the dependence on
$2^{O\left(\depth (\cs)\right)}$ can be made polynomial and the quartic
dependence on $1/\epsilon$, $R$, and $L$ can be improved.  In addition to
being interesting in their own right, improving the bounds may further
underscore the effectiveness of  random initialization as a way of
generating low dimensional embeddings of compositional kernel spaces. Randomly
generating such embeddings can be also considered on its own, and we are
currently working on design and analysis of random features a la
Rahimi and Recht~\cite{RahimiRe07}.
