\section{Mathematical background}\label{sec:math}
%
\paragraph*{Reproducing kernel Hilbert spaces (RKHS).}
%
The proofs of all the theorems we quote here are well-known and can be found
in Chapter~2~of~\citep{Saitoh88} and similar textbooks. Let $\ch$ be a Hilbert space
of functions from $\cx$ to $\reals$. We say that $\ch$ is a {\em reproducing
kernel Hilbert space}, abbreviated RKHS or kernel space, if for every $\x\in
\cx$ the linear functional $f\mapsto{}f(\x)$ is bounded. The following theorem
provides a one-to-one correspondence between kernels and kernel spaces.
%
\begin{theorem}\label{thm:RKHS_basic}
%
(i) For every kernel $\kappa$ there exists a unique kernel space
$\ch_\kappa$ such that for every $\x\in \cx$,
$\kappa(\cdot,\x) \in \ch_\kappa$ and for all $f\in \ch_\kappa,\;
f(\x) = \langle f(\cdot),\kappa(\cdot,\x)\rangle_{\ch_\kappa}$.
\, (ii) A Hilbert space $\ch\subseteq \reals^\cx$ is a kernel space if
and only if there exists a kernel $\kappa:\cx\times \cx\to \reals$
such that $\ch=\ch_\kappa$.
\end{theorem}
%
The following theorem describes a tight connection between embeddings
of $\cx$ into a Hilbert space and kernel spaces.
%
\begin{theorem}\label{thm:RKHS_embedding}
%
A function $\kappa:\cx\times \cx\to\reals$ is a kernel if and only if there
exists a mapping $\Phi:\cx\to \ch$ to some Hilbert space for which
$\kappa(\x,\x')=\langle \Phi(\x),\Phi(\x')\rangle_{\ch}$. In addition, the
following two properties hold,
\begin{itemize}
\item $\ch_\kappa=\{f_\bv :\bv\in  \ch\}$,
  where $f_\bv(\x)=\langle \bv,\Phi (\x)\rangle_{\ch}$.
\item For every $f\in \ch_\kappa$,
  $\|f\|_{\ch_\kappa} = \inf\{\|\bv\|_{\ch}\mid f=f_\bv\}$.
\end{itemize}
\end{theorem}

\paragraph*{Positive definite functions.} A function
$\mu:[-1,1]\to\reals$ is {\em positive definite} (PSD) if there are
non-negative numbers $b_0,b_1,\ldots$ such that
$$\sum_{i=0}^\infty b_i < \infty ~ \mbox{ and } ~
  \forall x\in [-1,1],\; \mu(x)=\sum_{i=0}^\infty b_ix^i \, .$$
The {\em norm} of $\mu$ is defined as
$\|\mu\|:=\sqrt{\sum_{i} b_i}=\sqrt{\mu(1)}$.
We say that $\mu$ is {\em normalized} if $\|\mu\|= 1$
\begin{theorem}[Schoenberg, \cite{schoenberg1942positive}]\label{thm:psd_func}
%
A continuous function $\mu:[-1,1]\to\reals$ is PSD if and only if for all
$d=1,2,\ldots,\infty$, the function
$\kappa:\mathbb{S}^{d-1}\times\mathbb{S}^{d-1}\to\mathbb{R}$ defined by
$\kappa(\x,\x')=\mu(\inner{\x,\x'})$ is a kernel.
%
\end{theorem}
\noindent The restriction to the unit sphere of many of the kernels used in
machine learning applications corresponds to positive definite functions. An
example is the Gaussian kernel,
$$\kappa(\x,\x') = \exp\left(-\frac{\|\x-\x'\|^2}{2\sigma^2}\right) \,.$$
Indeed, note that for unit vectors $\x,\x'$ we have
$$\kappa(\x,\x')
  = \exp\left(-\frac{\|\x\|^2+\|\x'\|^2-2\inner{\x,\x'}}{2\sigma^2}\right)
  = \exp\left(-\frac{1-\inner{\x,\x'}}{\sigma^2}\right) \,.$$
Another example is the Polynomial kernel
$\kappa(\x,\x')=\inner{\x,\x'}^d$.


\paragraph{Hermite polynomials.} The normalized {\em Hermite polynomials} is
the sequence $h_0,h_1,\ldots$ of orthonormal polynomials obtained by
applying the Gram-Schmidt process to the sequence $1,x,x^2,\ldots$ w.r.t.\ the
inner-product 
$\inner{f,g}=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty
f(x)g(x)e^{-\frac{x^2}{2}}dx$.
Recall that we define activations as square integrable functions w.r.t.\
the Gaussian measure. Thus, Hermite polynomials form an orthonormal
basis to the space of activations. In particular, each activation $\sigma$
can be uniquely described in the basis of Hermite polynomials,
\begin{equation}\label{eq:hermite_expansion}
\sigma(x) = a_0h_0(x)+a_1h_1(x)+a_2h_2(x)+\ldots ~,
\end{equation}
where the convergence holds in $\ell^2$ w.r.t.\ the Gaussian measure. This
decomposition is called the Hermite {\em expansion}. Finally, we use
the following facts (see Chapter~11~in~\cite{o2014analysis} and the relevant 
\href{https://en.wikipedia.org/wiki/Hermite_polynomials}{entry} in Wikipedia):
\begin{eqnarray}
\forall n\ge 1,\;h_{n+1}(x) & =&
  \frac{x}{\sqrt{n+1}}h_n(x) - \sqrt{\frac{n}{n+1}} h_{n-1}(x) ~,
  \label{eq:hermite_recursion} \\
\forall n\ge 1,\;h'_{n}(x) & = & \sqrt{n}h_{n-1}(x)
  \label{eq:hermite_diff} \\
  \E_{(X,Y) \sim \gaussian_\rho} \hspace{-4pt} h_m(X)h_n(Y)
  & = & \begin{cases} \rho^n & n=m\\ 0 & n\ne m\end{cases}
    ~\mbox{ where }~n,m\ge 0, \, \rho\in[-1,1] ~ ,
  \label{eq:hermite_ort} \\
h_n(0) & = &
\begin{cases}
  0,  & \mbox{if }n\mbox{ is odd} \\
  \frac{1}{\sqrt{n!}}(-1)^{\tfrac{n}{2}} (n-1)!! & \mbox{if }n\mbox{ is even}
\end{cases}
~,  \label{eq:hermite_zero_val}
\end{eqnarray}
where
$$
n!! =
\begin{cases}
	1 & n \le  0 \\
	n \cdot (n-2) \cdots 5 \cdot 3 \cdot 1 & n>0 \mbox{ odd }\\
	n \cdot (n-2) \cdots 6 \cdot 4 \cdot 2 & n>0 \mbox{ even }\\
\end{cases}
\,.
$$

\section{Compositional kernel spaces}\label{sec:comp_ker}
We now describe the details of compositional kernel spaces. Let $\cs$
be a skeleton with normalized activations and $n$ input nodes associated
with the input's coordinates. Throughout the rest of the section we study
the functions in $\ch_\cs$ and their norm. In particular, we show that
$\kappa_\cs$ is indeed a normalized kernel. Recall that $\kappa_\cs$ is
defined inductively by the equation,
\begin{equation}\label{eq:recursive_ker}
\kappa_v(\x,\x') = \hat\sigma_v
  \left(
    \frac{\sum_{u\in \IN(v)}\kappa_{u}(\x,\x')}{|\IN(v)|}
  \right)\,.
\end{equation}
The recursion \eqref{eq:recursive_ker} describes a means for generating
a kernel form another kernel. Since kernels correspond to kernel spaces,
it also prescribes an operator that produces a kernel space from other kernel
spaces. If $\ch_v$ is the space corresponding to $v$, we denote this
operator by
\begin{equation}\label{eq:recursive_space}
\ch_v=\hat{\sigma}_v\left(\frac{\oplus_{u\in\IN(v)}\ch_{u}}{|\IN(v)|}\right)\,.
\end{equation}
The reason for using the above notation becomes clear in the sequel. The space
$\ch_\cs$ is obtained by starting with the spaces $\ch_{v}$ corresponding to
the input nodes and propagating them according to the structure of $\cs$,
where at each node $v$ the operation~\eqref{eq:recursive_space} is applied.
Hence, to understand $\ch_\cs$ we need to understand this operation
as well as the spaces corresponding to input nodes. The latter spaces are rather simple: for an input node $v$
corresponding to the variable $\x^i$, we have that
$ \ch_v=\{f_{\w}\mid \forall \x,\;f_\w(\x)=\inner{\w,\x^i}\}$
and
$\|f_\w\|_{\ch_v} = \|\w\|$.
To understand \eqref{eq:recursive_space}, it is
convenient to decompose it into two
operations. The first operation, termed the {\em direct average}, is
defined through the equation $\tilde{\kappa}_v(\x,\x') =
\frac{\sum_{u\in\IN(v)}\kappa_u(\x,\x')}{|\IN(v)|}$, and the resulting kernel
space is denoted $\ch_{\tilde{v}} =
\frac{\oplus_{u\in\IN(v)}\ch_{u}}{|\IN(v)|}$. The second operation, called
the {\em extension} according to $\hat{\sigma}_v$, is defined through
$\kappa_v(\x,\x') = \hat{\sigma}_v\left(\tilde{\kappa}_v(\x,\x')\right)$.
The resulting kernel space is denoted
$\ch_{v} = \hat{\sigma}_v\left(\ch_{\tilde{v}}\right)$. We next analyze these
two operations.

\paragraph{The direct average of kernel spaces.} Let $\ch_1,\ldots,\ch_n$ be
kernel spaces with kernels
$\kappa_1,\ldots,\kappa_n:\cx\times\cx\to\mathbb{R}$. Their {\em direct
average}, denoted $\ch=\frac{\ch_1\oplus\cdots\oplus\ch_n}{n}$, is the
kernel space corresponding to the kernel
$\kappa(\x,\x')=\frac{1}{n}\sum_{i=1}^n\kappa_i(\x,\x')$.
\begin{lemma}\label{lem:direct_avg}
The function $\kappa$ is indeed a kernel. Furthermore, the following
properties hold.
\begin{enumerate}
\item \label{item:dir_avg_1}
  If $\ch_1,\ldots,\ch_n$ are normalized then so is $\ch$.
\item \label{item:dir_avg_2}
  $\ch = \left\{\frac{f_1+\ldots+f_n}{n}\mid f_i\in\ch_{i}\right\}$
\item \label{item:dir_avg_3}
    $\|f\|^2_{\ch} = \inf
     \left\{ \frac{\|f_1\|^2_{\ch_1}+\ldots+\|f_n\|^2_{\ch_n}}{n}
     \mbox{ s.t. } f=\frac{f_1+\ldots+f_n}{n},\;f_i\in\ch_i\right\}$
\end{enumerate}
\end{lemma}
\proof {\bf (outline)}
The fact that $\kappa$ is a kernel follows directly from the definition of a
kernel and the fact that an average of PSD matrices is PSD. Also, it is
straight forward to verify item \ref{item:dir_avg_1}. We now proceed to
items \ref{item:dir_avg_2} and \ref{item:dir_avg_3}. By Theorem
\ref{thm:RKHS_embedding} there are Hilbert spaces $\cg_1,\ldots,\cg_n$ and
mappings $\Phi_i:\cx\to\cg_i$ such that $\kappa_i(\x,\x')=\inner{\Phi_i(\x),
\Phi_i(\x')}_{\cg_i}$. Consider now the mapping
\[
\Psi(\x) = 
  \left(\frac{\Phi_1(\x)}{\sqrt{n}},\ldots,\frac{\Phi_n(\x)}{\sqrt{n}}\right)
  \,.
\]
It holds that $\kappa(\x,\x')=\inner{\Psi(\x),\Psi(\x')}$. Properties
\ref{item:dir_avg_2} and \ref{item:dir_avg_3} now follow directly form
Thm.~\ref{thm:RKHS_embedding} applied to $\Psi$.  \proofbox

\paragraph{The extension of a kernel space.} Let $\ch$ be a normalized kernel
space with a kernel $\kappa$. Let $\mu(x)=\sum_{i} b_i x^i$ be a
PSD function. 
As we will see shortly, a function is PSD if and only if it is a
dual of an activation function.
The {\em extension} of $\ch$ w.r.t.\ $\mu$, denoted
$\mu\left(\ch\right)$, is the kernel space corresponding to the kernel
$\kappa'(\x,\x')=\mu(\kappa(\x,\x'))$.
\begin{lemma}\label{lem:extension}
The function $\kappa'$ is indeed a kernel. Furthermore, the following
properties hold.
\begin{enumerate}
\item \label{item:ext_1}
  $\mu(\ch)$ is normalized if and only if $\mu$ is.
\item \label{item:ext_2}
  $\mu(\ch) = \overline{\mathrm{span}}
    \left\{\displaystyle \prod_{g\in A}g\mid A\subset \ch,\; b_{|A|}>0 \right\}$
		where $\overline{\mathrm{span}}({\cal A})$ is the closure of the
		span of ${\cal A}$.
\item \label{item:ext_3}
  $\|f\|_{\mu(\ch)}\le\inf \left\{\displaystyle
    \sum_{A}\frac{\prod_{g\in A}\|g\|_{\ch}}{\sqrt{b_{|A|}}}
    \mbox{ s.t. } f=\sum_{A}\prod_{g\in A}g,\;A\subset\ch\right\}$
\end{enumerate}
\end{lemma}
\proof {\bf (outline)}
Let $\Phi:\cx \to \cg$ be a mapping from $\cx$ to the unit ball of a Hilbert
space $\cg$ such that $\kappa(\x,\x')=\inner{\Phi(\x),\Phi(\x')}$. Define
\[
\Psi(\x)=\left(\sqrt{b_0},\sqrt{b_1}\Phi(\x),\sqrt{b_2}\Phi(\x)\otimes \Phi(\x), \sqrt{b_3}\Phi(\x)\otimes \Phi(\x)\otimes \Phi(\x),\ldots \right)
\]
It is not difficult to verify that
$\inner{\Psi(\x),\Psi(\x')}=\mu(\kappa(\x,\x'))$. Hence, by
Thm.~\ref{thm:RKHS_embedding}, $\kappa'$ is indeed a kernel. Verifying
property \ref{item:ext_1} is a straightforward task. Properties
\ref{item:ext_2} and \ref{item:ext_3} follow by applying
Thm.~\ref{thm:RKHS_embedding} on the mapping $\Psi$. \proofbox

\section{The dual activation function} \label{dualact:sec}
%
The following lemma describes a few basic properties of the dual activation. These properties follow easily from the definition of the dual activation and equations
\eqref{eq:hermite_expansion}, \eqref{eq:hermite_diff}, and
\eqref{eq:hermite_ort}.
\begin{lemma}\label{lem:dual_activation}
The following properties of the mapping $\sigma\mapsto \hat\sigma$ hold:
\begin{enumerate}[label=(\alph*)]
\item If $\sigma =\sum_{i} a_i h_i$ is the Hermite expansion of
  $\sigma$, then  $\hat\sigma(\rho) = \sum_i a_i^2 \rho^i$.
	\label{lem:da_1}
\item For every $\sigma$, $\hat\sigma$ is positive definite.
	\label{lem:da_2}
\item Every positive definite function is a dual of some activation.
	\label{lem:da_3}
\item The mapping $\sigma\mapsto\hat\sigma$ preserves norms.
	\label{lem:da_4}
\item The mapping $\sigma\mapsto\hat\sigma$ commutes with differentiation.
	\label{lem:da_5}
\item For $a\in\reals$, $\widehat{a\sigma} = a^2\hat\sigma$.
	\label{lem:da_6}
\item For every $\sigma$, $\hat{\sigma}$ is continuous in $[-1,1]$ and smooth in $(-1,1)$.
	\label{lem:da_7}
\item For every $\sigma$, $\hat{\sigma}$ is non-decreasing and convex in $[0,1]$.
	\label{lem:da_8}
\item For every $\sigma$, the range of $\hat{\sigma}$ is $\left[-\|\sigma\|^2,\|\sigma\|^2\right]$.
\item For every $\sigma$, $\hat \sigma(0) = \left(\E_{X\sim N(0,1)}\sigma(X)\right)^2$ and $\hat{\sigma}(1)=\|\sigma\|^2$.
	\label{lem:da_9}
\end{enumerate}
\end{lemma}
\noindent
We next discuss a few examples for activations and calculate their dual
activation and kernel. Note that the dual of the exponential activation
was calculated in~\cite{mairal2014convolutional} and the duals of the step and the ReLU activations were calculated in~\cite{cho2009kernel}.
Here, our derivations are different and
may prove useful for future calculations of duals for other activations.

\paragraph*{The exponential activation.}
Consider the activation function $\sigma(x)=Ce^{ax}$ where $C>0$ is a
normalization constant such that $\|\sigma\|=1$. The actual value of $C$ is
$e^{-2a^2}$ but it will not be needed for the derivation below. From
properties~\ref{lem:da_5}~and~\ref{lem:da_6} of
Lemma~\ref{lem:dual_activation} we have that,
$$
\left(\hat{\sigma}\right)' = \widehat{\sigma'} =
	\widehat{a \sigma} = a^2 \hat{\sigma} \,.
$$
The the solution of ordinary differential equation
$\left(\hat{\sigma}\right)' =  a^2 \hat{\sigma}$ is of the form
$\hat{\sigma}(\rho) = b \exp\left(a^2 \rho\right)$. Since $\hat\sigma(1) = 1$
we have $b=e^{-a^2}$. We therefore obtain that the dual activation
function is
$$
\hat\sigma(\rho) = e^{a^2 \rho - a^2} = e^{a^2 (\rho - 1)} \,.
$$
Note that the kernel induced by $\sigma$ is the RBF kernel, restricted to the
$d$-dimensional sphere,
$$\kappa_\sigma (\x,\x') =
e^{a^2(\inner{\x,\x'}-1)} = e^{-\frac{a^2\|\x-\x'\|^2 }{2}} \,.$$

\iffalse
\paragraph*{The exponential activation.} Consider the activation
$\sigma(x)={e^{ax}}/{e^{2a^2}}$. We have
\[
e^{2a^2}\hat \sigma (\rho) =
  \E_{(X,Y)\sim\gaussian_{\rho}}e^{aX}e^{aY} =
  \E_{(X,Y)\sim\gaussian_{\rho}}e^{a(X+Y)}\,.
\]
Now, $X+Y$ is a normal variable with expectation $0$ and variance $2+2\rho$.
Since the moment generating function on a standard Gaussian is
$e^{\frac{t^2}{2}}$, it follows  that $e^{2a^2}\hat \sigma (\rho) =
e^{a^2(1+\rho)}$. We note that the kernel induced by $\sigma$ is the RBF
kernel, restricted to the sphere,
$$\kappa_\sigma (\x,\x') =
e^{-2a^2}e^{a^2(1+\inner{\x,\x'})} = e^{-\frac{a^2\|\x-\x'\|^2 }{2}} \,.$$
\fi

\paragraph*{The Sine activation and the Sinh kernel.} Consider the activation
$\sigma(x)=\sin(ax)$. We can write
$\sin(ax) = \frac{e^{iax} - e^{-iax}}{2i}$. We have
\begin{eqnarray*}
\hat \sigma (\rho) &=&
  \E_{(X,Y)\sim\gaussian_{\rho}}
    \left(\frac{e^{iaX} - e^{-iaX}}{2i}\right)
    \left(\frac{e^{iaY} - e^{-iaY}}{2i}\right)
\\
&=& -\frac{1}{4}\E_{(X,Y)\sim\gaussian_{\rho}}
  \left(e^{iaX} - e^{-iaX}\right)
  \left(e^{iaY} - e^{-iaY}\right)
\\
&=& -\frac{1}{4}\E_{(X,Y)\sim\gaussian_{\rho}}
  \left[ e^{ia(X+Y)}- e^{ia(X-Y)}-e^{ia(-X+Y)}+e^{ia(-X-Y)} \right]\,.
\end{eqnarray*}
Recall that the
characteristic function, $\E[e^{itX}]$, when $X$ is distributed $N(0,1)$
is $e^{-\frac{1}{2} t^2}$.
Since $X+Y$ and $-X-Y$ are normal variables with expectation $0$ and
variance of $2+2\rho$, it follows that,
$$\E_{(X,Y)\sim\gaussian_{\rho}}e^{ia(X+Y)} =
  \E_{(X,Y)\sim\gaussian_{\rho}}e^{-ia(X+Y)} =
  e^{-\frac{a^2(2+2\rho)}{2}} \,.$$
Similarly, since the variance of $X-Y$ and $Y-X$ is $2-2\rho$, we get
$$\E_{(X,Y)\sim\gaussian_{\rho}}e^{ia(X-Y)} =
  \E_{(X,Y)\sim\gaussian_{\rho}}e^{ia(-X+Y)} =
  e^{-\frac{a^2(2-2\rho)}{2}} \,.$$
We therefore obtain that
\[
\hat\sigma(\rho) =
  \frac{e^{-a^2(1-\rho)} - e^{-a^2(1+\rho)}}{2} = e^{-a^2}\sinh (a^2\rho)\,.
\]

\paragraph*{Hermite activations and polynomial kernels.} From Lemma
\ref{lem:dual_activation} it follows that the dual activation of the Hermite
polynomial $h_n$ is $\hat h_n(\rho)=\rho^n$. Hence, the corresponding kernel
is the polynomial kernel.

\paragraph*{The normalized step activation.}
Consider the activation
$$\sigma(x)=\begin{cases} \sqrt{2} & x>0\\ 0 & x  \le 0\end{cases} \,.$$
To calculate $\hat{\sigma}$ we compute the Hermite expansion of
$\sigma$. For $n\ge 0$ we let
\[
a_n =
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty\sigma(x)h_n(x)e^{-\frac{x^2}{2}}dx
= 
\frac{1}{\sqrt{\pi}}\int_{0}^\infty h_n(x)e^{-\frac{x^2}{2}}dx\,.
\]
Since $h_0(x)=1$, $h_1(x)=x$, and $h_2(x)=\frac{x^2-1}{\sqrt{2}}$,
we get the corresponding coefficients,
\begin{eqnarray*}
a_0 & = &\E_{X\sim\gaussian(0,1)}[\sigma(X)] \,=\,\frac{1}{\sqrt{2}} \\
a_1 & = &\E_{X\sim\gaussian(0,1)}[\sigma(X)X] \,=\,
  \frac{1}{\sqrt{2}}\E_{X\sim\gaussian(0,1)}[|X|] = \frac{1}{\sqrt{\pi}} \\
a_2 & = &\frac{1}{\sqrt{2}}\E_{X\sim\gaussian(0,1)}[\sigma(X)(X^2-1)]
  \,=\, \frac{1}{2}\E_{X\sim\gaussian(0,1)}[X^2-1] \,=\, 0 \,.
\end{eqnarray*}
For $n \ge 3$ we write $g_n(x)=h_n(x)e^{-\frac{x^2}{2}}$ and note that
\begin{eqnarray*}
g'_{n}(x) &=& \left[h'_n(x)-xh_n(x)\right]e^{-\frac{x^2}{2}}
\\
&=& \left[\sqrt{n}h_{n-1}(x)-xh_n(x)\right]e^{-\frac{x^2}{2}}
\\
&=& -\sqrt{n+1}\,h_{n+1}(x)e^{-\frac{x^2}{2}}
\\
&=& -\sqrt{n+1}\,g_{n+1}(x) \,.
\end{eqnarray*}
Here, the second equality follows from \eqref{eq:hermite_diff}
and the third form \eqref{eq:hermite_recursion}.
We therefore get
\begin{eqnarray*}
a_n &=& \frac{1}{\sqrt{\pi}}\int_{0}^\infty g_n(x)dx
\\
&=& -\frac{1}{\sqrt{n\pi}}\int_{0}^\infty g'_{n-1}(x)dx
\\
&=& \frac{1}{\sqrt{n\pi}}\left(g_{n-1}(0) - \overbrace{g_{n-1}(\infty)}^{=0}
\right)
\\
&=& \frac{1}{\sqrt{n\pi}}h_{n-1}(0)
\\
&=&\begin{cases}
\frac{(-1)^{\frac{n-1}{2}}(n-2)!!}{\sqrt{n\pi}\sqrt{(n-1)!}} =
\frac{(-1)^{\frac{n-1}{2}}(n-2)!!}{\sqrt{\pi n!}}& \text{if }n\text{ is odd}
\\
0 & \text{if }n\text{ is even}
\end{cases} \,.
\end{eqnarray*}
The second equality follows from \eqref{eq:hermite_recursion} and
the last equality follows from \eqref{eq:hermite_zero_val}.
Finally, from Lemma~\ref{lem:dual_activation} we have that
$\hat\sigma(\rho)=\sum_{n=0}^\infty b_n\rho^n$ where
\[
b_n=\begin{cases}
\frac{((n-2)!!)^2}{\pi n!} & \text{if }n\text{ is odd}
\\
\frac{1}{2} & \text{if }n = 0
\\
0 & \text{if }n\text{ is even }\ge 2
\end{cases} \,.
\]
In particular, $(b_0,b_1,b_2,b_3,b_4,b_5,b_6) =
\left(\frac{1}{2},\frac{1}{\pi},0,\frac{1}{6\pi},0,\frac{3}{40\pi},0\right)$.
Note that from the Taylor expansion of $\cos^{-1}$ it follows
that $\hat\sigma(\rho)= 1 - \frac{\cos^{-1}(\rho)}{\pi}$.

\paragraph*{The normalized ReLU activation.}
%
Consider the activation $\sigma(x)=\sqrt{2}\max(0,x)$. We now write
$\hat\sigma(\rho)=\sum_{i} b_i\rho^i$. The first coefficient is
$$b_0 = \left(\E_{X\sim\gaussian(0,1)}\sigma(X)\right)^2 =
\frac{1}{2}\left(\E_{X\sim\gaussian(0,1)}|X|\right)^2 = \frac{1}{\pi} \,. $$
To calculate the remaining coefficients we simply note that the derivative
of the ReLU activation is the step activation and the mapping
$\sigma\mapsto\hat\sigma$ commutes with differentiation. Hence, from the
calculation of the step activation we get,
\[
b_n=\begin{cases}
\frac{((n-3)!!)^2}{\pi n!} & \text{if }n\text{ is even}
\\
\frac{1}{2} & \text{if }n = 1
\\
0 & \text{if }n\text{ is odd }\ge 3
\end{cases} \,.
\]
In particular, $(b_0,b_1,b_2,b_3,b_4,b_5,b_6) =
\left(\frac{1}{\pi}, \frac{1}{2}, \frac{1}{2\pi}, 0,
  \frac{1}{24\pi}, 0, \frac{1}{80\pi}\right)$.
We see that the coefficients corresponding to the degrees $0$, $1$, and $2$
sum to $0.9774$. The sums up to degrees $4$ or $6$ are $0.9907$ and
$0.9947$ respectively. That is, we get an excellent approximation of less
than $1\%$ error with a dual activation of degree $4$.

\paragraph*{The collapsing tower of fully connected layers.}
%
To conclude this section we discuss the case of very
deep networks. The setting is taken for illustrative purposes but
it might surface when building networks with numerous fully connected
layers. Indeed, most deep architectures that we are aware of do not employ
more than five {\em consecutive} fully connected layers.

Consider a
skeleton $\cs_m$ consisting of $m$ fully connected layers, each layer
associated with the same (normalized) activation $\sigma$. We would like to
examine the form of the compositional kernel as the number of layers becomes
very large. Due to the repeated structure and activation we have
$$\kappa_{\cs_m}(\x,\y)=\alpha_m\left(\frac{\inner{\x,\y}}{n}\right)
~ \mbox{ where } ~
\alpha_m = \hat{\sigma}^m =
	\overbrace{\hat{\sigma}\circ\ldots\circ\hat{\sigma}}^{m\text{ times}} ~.
$$
Hence, the limiting properties of $\kappa_{\cs_m}$ can be understood from
the limit of $\alpha_m$. In the case that $\sigma(x)=x$ or $\sigma(x)=-x$,
$\hat{\sigma}$ is the identity function. Therefore $\alpha_m(\rho) =
\hat{\sigma}(\rho)=\rho$ for all $m$ and $\kappa_{\cs_m}$ is simply the
linear kernel. Assume now that $\sigma$ is neither the identity nor its
negation.  The following claim shows that $\alpha_m$ has a point-wise limit
corresponding to a degenerate kernel.
\begin{claim}
There exists a constant $0\le  \alpha_{\sigma} \le 1$ such that for all
$-1 < \rho < 1$, 
\[
\lim_{m\to\infty}\alpha_m(\rho)=\alpha_{\sigma}
\]
\end{claim}
\noindent
Before proving the claim, we note that for $\rho=1$, $\alpha_m(1)=1$ for all
$m$, and therefore $\lim_{m\to\infty}\alpha_m(1)=1$. For $\rho = -1$, if
$\sigma$ is anti-symmetric then $\alpha_m(-1)=-1$ for all $m$, and in
particular $\lim_{m\to\infty}\alpha_m(-1)=-1$. In any other case, our
argument can show that $\lim_{m\to\infty}\alpha_m(-1)=\alpha_{\sigma}$.
\proof
Recall that $\hat{\sigma}(\rho) = \sum_{i=0}^\infty b_i\rho^i$ where the
$b_i$'s are non-negative numbers that sum to 1. By the assumption that
$\sigma$ is not the identity or its negation, $b_1 < 1$.  We first claim
that there is a unique $\alpha_\sigma\in [0,1]$ such that
\begin{equation}\label{eq:babel_1}
\forall x \in (-1,\alpha_\sigma)\, ,\;\;
	\hat{\sigma}(\rho) > \rho
\text{ and }~~
\forall x \in (\alpha_\sigma,1)\, ,\;\;
	\alpha_\sigma< \hat{\sigma}(\rho) < \rho
\end{equation}
To prove~\eqref{eq:babel_1} it suffices to prove the following properties.
\begin{enumerate}[label=(\alph*)]
\item $\hat{\sigma}(\rho) > \rho$ for $\rho\in (-1,0)$
\item $\hat{\sigma}$ is non-decreasing and convex in $[0,1]$
\item $\hat{\sigma}(1)=1$
\item the graph of $\hat{\sigma}$ has at most a single intersection
		in $[0,1)$ with the graph of $f(\rho)=\rho$
\end{enumerate}
If the above properties hold we can take $\alpha_\sigma$ to be the
intersection point or $1$ if such a point does not exist.
We first show (a). For $\rho\in (-1,0)$ we have that
\begin{eqnarray*}
\hat{\sigma}(\rho) &=& b_0 + \sum_{i=1}^\infty b_i\rho^i
\;\ge \; b_0 - \sum_{i=1}^\infty b_i |\rho|^i
\; > \;  - \sum_{i=1}^\infty b_i |\rho|
\; \ge \;  - |\rho| \; = \; \rho ~.
\end{eqnarray*}
Here, the third inequality follows form the fact that $b_0 \ge 0$ and for
all $i$, $-b_i|\rho|^i \ge -b_i|\rho|$. Moreover since $b_1<1$, one of these
inequalities must be strict.
%
Properties (b) and (c) follows from Lemma \ref{lem:dual_activation}. Finally, to show (d), we note
that the second derivative of $\hat{\sigma}(\rho) - \rho$ is $
\sum_{i \geq 2} i(i-1)b_i \rho^{i-2}$ which is non-negative in $[0,1)$.
Hence, $\hat{\sigma}(\rho) - \rho$ is convex in $[0,1]$ and in particular
intersects with the $x$-axis at either $0$, $1$, $2$ or infinitely many times
in $[0,1]$. As we assume that $\hat{\sigma}$ is not the identity, we can
rule out the option of infinitely many intersections. Also, since
$\hat{\sigma}(1)=1$, we know that there is at least one intersection in
$[0,1]$. Hence, there are $1$ or $2$ intersections in $[0,1]$ and because one
of them is in $\rho=1$, we conclude that there is at most one intersection
in $[0,1)$.

Lastly, we derive the conclusion of the claim from equation (\ref{eq:babel_1}).
Fix $\rho\in (-1,1)$. Assume first that $\rho \ge \alpha_\sigma$. By
(\ref{eq:babel_1}), $\alpha_m(\rho)$ is a monotonically non-increasing
sequence that is lower bounded by $\alpha_\sigma$. Hence, it has a limit
$\alpha_\sigma \le \tau \le \rho < 1$. Now, by the continuity of
$\hat{\sigma}$ we have
\[
\hat{\sigma}(\tau) = \hat{\sigma}\left(\lim_{m\to\infty}\alpha_m(\rho)\right)
 = \lim_{m\to\infty}\hat{\sigma}(\alpha_m(\rho))
  = \lim_{m\to\infty}\alpha_{m+1}(\rho) = \tau \,.
\]
Since the only solution to $\hat{\sigma}(\rho) = \rho$ in $(-1,1)$ is
$\alpha_\sigma$, we must have $\tau = \alpha_\sigma$. We next deal with the
case that $-1<\rho < \alpha_\sigma$. If for some $m$, $\alpha_m(\rho)\in
[\alpha_\sigma,1)$, the argument for $\alpha_\sigma\le \rho$ shows that
$\alpha_\sigma = \lim_{m\to\infty}\alpha_m(\rho)$. If this is not the case,
we have that for all $m$, $\alpha_m(\rho)\le \alpha_{m+1}(\rho)\le
\alpha_\sigma$. As in the case of $\rho\ge \alpha_\sigma$, this can be used
to show that $\alpha_m(\rho)$ converges to $\alpha_\sigma$.
%
\proofbox
