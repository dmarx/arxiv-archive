\section{Proofs}

\subsection{Well-behaved activations}
%
The proof of our main results applies to activations that are decent,
i.e.\ well-behaved, in a sense defined in the sequel.  We then show that
$C$-bounded activations as well as the ReLU activation are decent. We
first need to extend the definition of the dual activation and kernel to apply
to vectors in $\reals^d$, rather than just $\mathbb{S}^d$. We denote by
$\cm_+$  the collection of $2\times 2$ positive semi-define matrices and by
$\cm_{++}$ the collection of positive definite matrices.
\begin{definition}
Let $\sigma$ be an activation. Define the following,
\[
\bar\sigma:\cm_{+}^2\to\reals ~~ , ~~
\bar\sigma(\Sigma)=\E_{(X,Y)\sim\gaussian(0,\Sigma)}\sigma(X)\sigma(Y) ~~ , ~~
k_\sigma(\x,\y)=\bar\sigma\begin{pmatrix}
\|\x\|^2 & \inner{\x,\y}
\\
\inner{\x,\y} & \|\y\|^2
\end{pmatrix} \,.
\]
\end{definition}
\noindent
We underscore the following properties of the extension of a
dual activation.
\begin{enumerate}[label=(\alph*)]
\item The following equality holds,
	$$\hat\sigma(\rho)=\bar\sigma\begin{pmatrix}
		1 & \rho \\
		\rho & 1
	\end{pmatrix}$$

\item The restriction of the extended $k_\sigma$ to the sphere agrees
with the restricted definition.

\item The extended dual activation and kernel are defined for every
	activation $\sigma$ such that for all $a\ge 0$, $x\mapsto \sigma(ax)$ is
	square integrable with respect to the Gaussian measure.

\item For $\x,\y\in\reals^d$, if $\w\in\reals^d$ is a multivariate normal
	distribution with zero mean vector and identity covariance matrix,
	then
	$$k_\sigma(\x,\y)=\E_{\w}\sigma(\inner{\w,\x})\sigma(\inner{\w,\y}) \,.$$
\end{enumerate}
Denote
$$\cm^\gamma_+:=\left\{\begin{pmatrix}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{12} & \Sigma_{22}
\end{pmatrix}\in \cm_+\mid 1-\gamma\le \Sigma_{11},\Sigma_{22}
	\le 1+\gamma\right\} \,. $$
\begin{definition}
A normalized activation $\sigma$ is {\em
$(\alpha,\beta,\gamma)$-decent} for $\alpha,\beta,\gamma\ge 0$ if the
following conditions hold.
\begin{enumerate}[label=(\roman*)]
\item The dual activation $\bar\sigma$ is $\beta$-Lipschitz in
	$\cm_+^\gamma$ with respect to the $\infty$-norm.

\item If $(X_1,Y_1),\ldots,(X_r,Y_r)$ are independent samples from
	$\gaussian\left(0,\Sigma\right)$ for $\Sigma\in \cm_+^\gamma$ then
\[
\Pr\left(\left|\frac{\sum_{i=1}^r\sigma(X_i)\sigma(Y_i)}{r} -
	\bar\sigma(\Sigma)\right|\ge\epsilon\right)
	\le 2\exp\left(-\frac{r\epsilon^2}{2\alpha^2}\right) \,.
\]
\end{enumerate}
\end{definition}

\begin{lemma}[Bounded activations are decent]
	\label{lem:bounded_are_decent}
Let $\sigma:\reals\to\reals$ be a $C$-bounded normalized activation. Then,
$\sigma$ is $(C^2,2C^2,\gamma)$-decent for all $\gamma \ge 0$.
\end{lemma}
\proof
It is enough to show that the following properties hold.
\begin{enumerate}
\item The (extended) dual activation $\bar\sigma$ is $2C^2$-Lipschitz in
	$\cm_{++}$ w.r.t.\ the $\infty$-norm.
\item If $(X_1,Y_1),\ldots,(X_r,Y_r)$ are
 independent samples from $\gaussian\left(0,\Sigma\right)$ then
\[
\Pr\left(\left|\frac{\sum_{i=1}^r\sigma(X_i)\sigma(Y_i)}{r}-\bar\sigma(\Sigma)\right|\ge\epsilon\right) \le 2\exp\left(-\frac{r\epsilon^2}{2C^4}\right)
\]
\end{enumerate}
\noindent
From the boundedness of $\sigma$ it holds that $|\sigma(X)\sigma(Y)| \leq
C^2$. Hence, the second property follows directly from Hoeffding's bound.
We next prove the first part. Let $\z=(x,y)$ and
$\phi(\z) = \sigma(x)\sigma(y)$. Note that for
$\Sigma\in\cm_{++}$ we have
$$\bar\sigma(\Sigma) =
	\frac{1}{2\pi\sqrt{\det(\Sigma)}}
	\int_{\reals^2}\phi(\z)e^{-\frac{\z^\top\Sigma^{-1}\z}{2}}d\z \,.$$
Thus we get that,
\begin{eqnarray*}
\frac{\partial \bar\sigma}{\partial \Sigma} &=&
	\frac{1}{2\pi}\int_{\reals^2}
		\phi(\z)\left[
			\frac{\frac{1}{2}\sqrt{\det(\Sigma)}\Sigma^{-1} -
				\frac{1}{2}\sqrt{\det(\Sigma)}(\Sigma^{-1}\z\z^\top\Sigma^{-1})}
				{\det(\Sigma)}
						\right]
		e^{-\frac{\z^\top\Sigma^{-1}\z}{2}}d\z
\\
&=& \frac{1}{2\pi\sqrt{\det(\Sigma)}}\int_{\reals^2}\phi(\z)\frac{1}{2}\left[
\Sigma^{-1}-\Sigma^{-1}\z\z^\top\Sigma^{-1}
\right]e^{-\frac{\z^\top\Sigma^{-1}\z}{2}}d\z
\end{eqnarray*}
Let $g(\z)=e^{-\frac{\z^\top\Sigma^{-1}\z}{2}}$. Then, the first and second
order partial derivatives of $g$ are
\begin{eqnarray*}
\frac{\partial g}{\partial \z} & = &
	-\Sigma^{-1}\z e^{-\frac{\z^\top\Sigma^{-1}\z}{2}} \\
\frac{\partial^2 g}{\partial^2 \z} & = &
	\left[-\Sigma^{-1} +
		\Sigma^{-1}\z\z^\top\Sigma^{-1}\right]e^{-\frac{\z^\top\Sigma^{-1}\z}{2}} \,.
\end{eqnarray*}
We therefore obtain that,
\[
\frac{\partial \bar\sigma}{\partial \Sigma} =
	-\frac{1}{4\pi\sqrt{\det(\Sigma)}} \int_{\reals^2}
		\phi\frac{\partial^2 g}{\partial^2 \z} d\z \,.
\]
By the product rule we have
\[
\frac{\partial \bar\sigma}{\partial \Sigma} =
-\frac{1}{2\pi\sqrt{\det(\Sigma)}}\frac{1}{2}\int_{\reals^2}\frac{\partial^2
\phi}{\partial^2 \z} gd\z = -\frac{1}{2}\E_{(X,Y)\sim\gaussian(0,\Sigma)}\left[\frac{\partial^2 \phi}{\partial^2 \z}(X,Y)\right]
\]
We conclude that $\bar\sigma$ is differentiable in $\cm_{++}$ with
partial derivatives that are point-wise bounded by $\frac{C^2}{2}$. Thus,
$\bar\sigma$ is $2C^2$-Lipschitz in $\cm_+$ w.r.t.\ the $\infty$-norm. \qed

\medskip
We next show that the ReLU activation is decent.
\begin{lemma}[ReLU is decent]\label{lem:relu_is_decent}
%
There exists a constant $\alpha_\mathrm{ReLU}\ge 1$ such that for
$0\le \gamma\le 1$, the normalized ReLU activation
$\sigma(x)=\sqrt{2}\max(0,x)$ is
$(\alpha_\mathrm{ReLU},1+o(\gamma),\gamma)$-decent.
\end{lemma}
\proof
The measure concentration property follows from standard concentration
bounds for sub-exponential random variables (e.g.\ ~\cite{shalev2014understanding}).  It
remains to show that $\bar\sigma$ is $(1+o(\gamma))$-Lipschitz in
$\cm^\gamma_+$. We first calculate an exact expression for $\bar\sigma$.
The expression was already calculated in~\cite{cho2009kernel}, yet we give
here a derivation for completeness.
%
\begin{claim}\label{claim:relu_dual_ext}
The following equality holds for all $\Sigma\in\cm_{+}^2$,
$$\bar\sigma(\Sigma) = \sqrt{\Sigma_{11}\Sigma_{22}} \,
	\hat\sigma\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
\,. $$
\end{claim}
\proof Let us denote
$$\tilde{\Sigma} = \begin{pmatrix}
1 & \frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{12}}} \\
\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{12}}} & 1
\end{pmatrix} \,. $$
By the positive homogeneity of the ReLU activation we have
\begin{eqnarray*}
\bar\sigma\left(\Sigma\right) &=&
	\E_{(X,Y)\sim\gaussian(0,\Sigma)}\sigma(X)\sigma(Y) \\
&=& \sqrt{\Sigma_{11}\Sigma_{22}}
		\E_{(X,Y)\sim\gaussian(0,\Sigma)}
			\sigma\!\left(\frac{X}{\sqrt{\Sigma_{11}}}\right)
			\sigma\!\left(\frac{Y}{\sqrt{\Sigma_{22}}}\right) \\
&=& \sqrt{\Sigma_{11}\Sigma_{22}}
			\E_{(\tilde{X},\tilde{Y})\sim\gaussian\left(0,\tilde{\Sigma}\right)}
			\sigma\!\left(\tilde{X}\right)\sigma\!\left(\tilde{Y}\right) \\
&=& \sqrt{\Sigma_{11}\Sigma_{22}}\,
	\hat{\sigma}\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
	\, .
\end{eqnarray*}
which concludes the proof. \qed

\medskip

For brevity, we henceforth drop the argument from $\bar{\sigma}(\Sigma)$ and
use the abbreviation $\bar{\sigma}$. In order to show that $\bar\sigma$ is
$(1+o(\gamma))$-Lipschitz w.r.t.\ the $\infty$-norm it is enough to show that
for every $\Sigma\in\cm_+^\gamma$ we have,
\begin{equation}\label{eq:grad_l1_bound}
\|\nabla \bar \sigma\|_1 =
	\left|\frac{\partial \bar\sigma}{\partial \Sigma_{12}}\right| +
	\left|\frac{\partial \bar\sigma}{\partial \Sigma_{11}}\right| +
	\left|\frac{\partial \bar\sigma}{\partial \Sigma_{22}}\right|\le
		1+o(\gamma) \,.
\end{equation}
First, Note that ${\partial \bar\sigma}/{\partial \Sigma_{11}}$ and
${\partial \bar\sigma}/{\partial \Sigma_{22}}$ have the same sign,
hence,
$$\|\nabla \bar \sigma\|_1 =
	\left|\frac{\partial \bar\sigma} {\partial \Sigma_{12}}\right| +
	\left|\frac{\partial \bar\sigma}{\partial \Sigma_{11}} +
				\frac{\partial \bar\sigma}{\partial \Sigma_{22}}\right| \,.$$
Next we get that,
\begin{eqnarray*}
\frac{\partial \bar\sigma}{\partial \Sigma_{11}} & = &
	\frac{1}{2}\sqrt{\frac{\Sigma_{22}}{\Sigma_{11}}}\,
	\hat\sigma\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right) -
	\frac{1}{2}\sqrt{\frac{\Sigma_{22}}{\Sigma_{11}}}
		\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\,
		\hat\sigma'\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
\\
\frac{\partial \bar\sigma}{\partial \Sigma_{22}} & = &
	\frac{1}{2}\sqrt{\frac{\Sigma_{11}}{\Sigma_{22}}}\,
	\hat\sigma\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right) -
	\frac{1}{2}\sqrt{\frac{\Sigma_{11}}{\Sigma_{22}}}
	\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\,
	\hat\sigma'\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
\\
\frac{\partial \bar\sigma}{\partial \Sigma_{12}} & = &
	\hat\sigma'\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
	\, .
\end{eqnarray*}
We therefore get that the $1$-norm of $\nabla\bar\sigma$ is,
\[
\|\nabla \bar \sigma\|_1 =
\frac{1}{2}\frac{\Sigma_{11}+\Sigma_{22}}{\sqrt{\Sigma_{11}\Sigma_{22}}}
\left|
	\hat\sigma\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right) -
	\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\,
	\hat\sigma'\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
\right| +
	\hat\sigma'\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
	\,.
\]
The gradient of
$\frac{1}{2}\frac{\Sigma_{11}+\Sigma_{22}}{\sqrt{\Sigma_{11}\Sigma_{22}}}$
at $(\Sigma_{11},\Sigma_{22})=(1,1)$ is $(0,0)$. Therefore, from the mean
value theorem we get,
$\frac{1}{2}\frac{\Sigma_{11}+\Sigma_{22}}{\sqrt{\Sigma_{11}\Sigma_{22}}} =
	1+o(\gamma)$.
Furthermore, $\hat\sigma$, $\hat\sigma'$ and
$\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}$ are bounded by $1$ in
absolute value. Hence, we can write,
\[
\|\nabla \bar \sigma\|_1 =
\left|
\hat\sigma\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right) -
\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}
\hat\sigma'\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
\right| +
\hat\sigma'\!\left(\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}\right)
+ o(\gamma) \,.
\]
Finally, if we let $t=\frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}$,
we can further simply the expression for $\nabla\bar\sigma$,
\begin{eqnarray*}
\|\nabla \bar \sigma(\Sigma)\|_1 &=& |\hat\sigma(t)-t\hat\sigma'(t)| + |\hat\sigma'(t)|  + o(\gamma)
\\
&=& \frac{\sqrt{1-t^2}}{\pi} + 1 - \frac{\cos^{-1}(t)}{\pi}  + o(\gamma) \,.
\end{eqnarray*}
Finally, the proof is obtained from the fact that the function $f(t)=\frac{\sqrt{1-t^2}}{\pi} + 1 - \frac{\cos^{-1}(t)}{\pi}$ satisfies $0\le f(t)\le 1$ for every $t\in [-1,1]$.
Indeed, it is simple to verify that $f(-1)=0$ and $f(1)=1$. Hence, it suffices to
show that $f'$ is non-negative in $[-1,1]$ which is indeed the case since,
\[
f'(t) = \frac{1}{\pi}\frac{1-t}{\sqrt{1-t^2}} =
	\frac{1}{\pi}\sqrt{\frac{1-t}{1+t}} \ge 0 \,. \qedhere
\]

\subsection{Proofs of Thms.~\ref{thm:main_ker}~and~\ref{thm:main_ker_ReLU}}
We start by an additional theorem which serves as a simple stepping stone
for proving the aforementioned main theorems.
\begin{theorem}\label{thm:ker_appr}
%
Let $\cs$ be a skeleton with $(\alpha,\beta,\gamma)$-decent
activations, $0<\epsilon \le \gamma$, and
$B_d = \sum_{i=0}^{d-1}\beta^i$. Let $\w$ be a random initialization of the
network $\cn=\cn(\cs,r)$ with
$$r \ge \frac{2\alpha^2B_{\depth(\cs)}^2
	\log\left(\frac{8|\cs|}{\delta}\right)} {\epsilon^2} \,. $$
Then, for every $\x,\y$ with probability of at least $1-\delta$, it holds that
\[
|\kappa_\w(\x,\y)-\kappa_\cs(\x,\y)|\le \epsilon \,.
\]
\end{theorem}
\noindent
Before proving the theorem we show that together with
Lemmas~\ref{lem:bounded_are_decent}~and~\ref{lem:relu_is_decent},
Theorems~\ref{thm:main_ker}~and~\ref{thm:main_ker_ReLU} follow from
Theorem~\ref{thm:ker_appr}. We restate them as corollaries, prove them,
and then proceed to the proof of Theorem \ref{thm:ker_appr}.
\begin{corollary}
Let $\cs$ be a skeleton with $C$-bounded activations. Let $\w$ be a random
initialization of $\cn=\cn(\cs,r)$ with
$$r \ge \frac{(4C^4)^{\depth(\cs)+1}
\log\left(\frac{8|\cs|}{\delta}\right)}{\epsilon^2} \,.$$
Then, for every $\x,\y$, w.p.\ $\ge 1-\delta$,
\[
|\kappa_\w(\x,\y)-\kappa_\cs(\x,\y)|\le \epsilon\,.
\]
\end{corollary}
\proof
From Lemma~\ref{lem:bounded_are_decent}, for all $\gamma>0$, each
activation is $(C^2,2C^2,\gamma)$-decent. By Theorem
\ref{thm:ker_appr}, it suffices to show that
$$2\left(C^2\right)^2\left(\sum_{i=0}^{\depth(\cs)-1}(2C^2)^{i}\right)^2
	\le (4C^4)^{\depth(\cs)+1} \,. $$
The sum of can be bounded above by,
\[
\sum_{i=0}^{\depth(\cs)-1}\!\!\!(2C^2)^{i} =
	\frac{(2C^2)^{\depth(\cs)}-1}{2C^2-1} \le
	\frac{(2C^2)^{\depth(\cs)}}{C^2} \,.
\]
Therefore, we get that,
\[
2\left(C^2\right)^2\left(\sum_{i=0}^{\depth(\cs)-1}\!\!\!(2C^2)^{i}\right)^2
	\le \frac{2C^4(4C^4)^{\depth(\cs)}}{C^4} \le (4C^4)^{\depth(\cs)+1} \,,
\]
which concludes the proof. \qed

\begin{corollary}
Let $\cs$ be a skeleton with ReLU activations, and $\w$ a random
initialization of $\cn(\cs,r)$ with $r \ge c_1 \frac{\depth^2(\cs)
\log\left(\frac{8|\cs|}{\delta}\right)}{\epsilon^2}$. For all $\x,\y$ and
$\epsilon\le \min(c_2,\frac{1}{\depth(\cs)})$, w.p.\ $\ge 1-\delta$,
	\[
	|\kappa_\w(\x,\y)-\kappa_\cs(\x,\y)|\le \epsilon
	\]
	Here, $c_1,c_2>0$ are universal constants.
\end{corollary}
\proof
From Lemma \ref{lem:relu_is_decent}, each activation is
$(\alpha_{\mathrm{ReLU}},1+o(\epsilon),\epsilon)$-decent. By Theorem
\ref{thm:ker_appr}, it is enough to show that
$$\sum_{i=0}^{\depth(\cs)-1}\!\!\!(1+o(\epsilon))^{i}=O(\depth(\cs)) \,.$$
This claim follows from the fact that
$(1+o(\epsilon))^{i}\le e^{o(\epsilon)\depth(\cs)}$ as long as
$i\le \depth(\cs)$. Since we assume that
$\epsilon\le{1}/{\depth(\cs)}$, the expression is bounded by $e$
for sufficiently small $\epsilon$.
\proofbox

\medskip

\noindent We next prove Theorem \ref{thm:ker_appr}.
\proof (Theorem \ref{thm:ker_appr})
For a node $u\in\cs$ we denote by $\Psi_{u,\w}:\cx\to\reals^r$ the normalized
representation of $\cs$'s sub-skeleton rooted at $u$.
Analogously, $\kappa_{u,\w}$ denotes the empirical kernel of that network.
When $u$ is the output node of $\cs$ we still use $\Psi_{\w}$ and $\kappa_\w$
for $\Psi_{u,\w}$ and $\kappa_{u,\w}$. Given two fixed $\x,\y\in\cx$ and a node
$u\in\cs$, we denote
\[
\mathcal{K}_\w^u=
\begin{pmatrix}
\kappa_{u,\w}(\x,\x) &
\kappa_{u,\w}(\x,\y)
\\
\kappa_{u,\w}(\x,\y) &
\kappa_{u,\w}(\y,\y)
\end{pmatrix},\;\; \mathcal{K}^u = \begin{pmatrix}
\kappa_u(\x,\x) & \kappa_u(\x,\y)
\\
\kappa_u(\x,\y) & \kappa_u(\y,\y)
\end{pmatrix}
\]
\[
\mathcal{K}_\w^{\leftarrow u}=\frac{\sum_{v\in\IN(u)}\mathcal{K}^{v}_\w}{|\IN(u)|}
,\quad \mathcal{K}^{\leftarrow u}=\frac{\sum_{v\in\IN(u)}\mathcal{K}^{v}}{|\IN(u)|} \,.
\]
For a matrix $\mathcal{K}\in\cm_+$ and a function $f:\cm_+\to\reals$, we denote
\[
f^p(\mathcal{K})=\begin{pmatrix}
f\!\begin{pmatrix}
\mathcal{K}_{11} & \mathcal{K}_{11}
\\
\mathcal{K}_{11} & \mathcal{K}_{11}
\end{pmatrix} & f(\mathcal{K})
\\
f(\mathcal{K}) & f\!\begin{pmatrix}
\mathcal{K}_{22} & \mathcal{K}_{22}
\\
\mathcal{K}_{22} & \mathcal{K}_{22}
\end{pmatrix}
\end{pmatrix}
\]
Note that $\mathcal{K}^u=\bar\sigma_u^p(\mathcal{K}^{\leftarrow u})$.
We say that a node $u\in \cs$, is {\em well-initialized} if
\begin{equation}\label{eq:1}
\|\mathcal{K}_\w^u- \mathcal{K}^u\|_\infty
	\le \epsilon\frac{B_{\depth(u)}}{B_{\depth(\cs)}} \,.
\end{equation}
Here, we use the convention that $B_{0}=0$. It is enough to show that with
probability of at least $\ge 1-\delta$ all nodes are well-initialized. We first note that input nodes are well-initialized by construction since
$\mathcal{K}^u_\w=\mathcal{K}^u$. Next, we show that given that all incoming
nodes for a certain node are well-initialized, then w.h.p.\ the node is
well-initialized as well.
\begin{claim}\label{claim1}
Assume that all the nodes in $\IN(u)$ are well-initialized. Then, the node
$u$ is well-initialized with probability of at least $1-\frac{\delta}{|\cs|}$.
\end{claim}
\proof
It is easy to verify that $\mathcal{K}_\w^u$ is the empirical
covariance matrix of $r$ independent variables distributed according to
$\left(\sigma(X),\sigma(Y)\right)$ where
$(X,Y)\sim\gaussian\left(0,\mathcal{K}_\w^{\leftarrow u}\right)$.
Given the assumption that all nodes incoming to $u$ are well-initialized,
we have,
\begin{eqnarray}\label{eq:2}
\left\|\mathcal{K}_\w^{\leftarrow u}-\mathcal{K}^{\leftarrow u}\right\|_\infty
&=&
\left\|
	\frac{\sum_{v\in\IN(v)}\mathcal{K}_\w^{v}}{|\IN(v)|} -
	\frac{\sum_{v\in\IN(v)}\mathcal{K}^{v}}{|\IN(v)|}\right\|_\infty\nonumber
\\
&\le& \frac{1}{|\IN(v)|}\sum_{v\in\IN(v)}
	\left\|\mathcal{K}_\w^{v}-\mathcal{K}^{v}\right\|_\infty
\\
&\le&  \epsilon\frac{B_{\depth(u)-1}}{B_{\depth(\cs)}}\nonumber \,.
\end{eqnarray}
Further, since $\epsilon\le \gamma$ then
$\mathcal{K}^{\leftarrow u}_\w\in \cm_+^\gamma$. Using the fact
that $\sigma_u$ is
$(\alpha,\beta,\gamma)$-decent and that
$r\ge \frac{2\alpha^2 B^2_{\depth(\cs)}
	\log\left(\frac{8|\cs|}{\delta}\right)}{\epsilon^2}$,
we get that w.p.\ of at least $1- \frac{\delta}{|\cs|}$,
\begin{equation}\label{eq:3}
\left\|\mathcal{K}^u_\w -
	\bar\sigma^p_u\left(\mathcal{K}_\w^{\leftarrow u}\right)\right\|_\infty
	\le \frac{\epsilon}{B_{\depth(\cs)}} \,.
\end{equation}
Finally, using \eqref{eq:2}~and~\eqref{eq:3} along with the fact that
$\bar\sigma$ is $\beta$-Lipschitz, we have
\begin{eqnarray*}
\|\mathcal{K}_\w^{u}-\mathcal{K}^{u}\|_\infty &=&
\left\|\mathcal{K}_\w^{u} -
	\bar\sigma^p_u\left(\mathcal{K}^{\leftarrow u}\right)\right\|_\infty
\\
&\le & \left\|\mathcal{K}^u_\w -
	\bar\sigma^p_u\left(\mathcal{K}_\w^{\leftarrow u}\right)\right\|_\infty +
	\left\| \bar\sigma^p_u\left(\mathcal{K}_\w^{\leftarrow u}\right) -
	\bar\sigma^p_u\left(\mathcal{K}^{\leftarrow u}\right)\right\|_\infty
\\
&\le &   \frac{\epsilon}{B_{\depth(\cs)}} +
\beta\left\| \mathcal{K}_\w^{\leftarrow u} -
	\mathcal{K}^{\leftarrow u}\right\|_\infty
\\
&\le &  \frac{\epsilon}{B_{\depth(\cs)}} +
\beta \epsilon \frac{B_{\depth(u)-1}}{B_{\depth(\cs)}}
\;=\;  \epsilon \frac{B_{\depth(u)}}{B_{\depth(\cs)}} \,. \hspace{2cm} \qed
\end{eqnarray*}

We are now ready to conclude the proof. Let $u_1,\ldots,u_{|\cs|}$ be an ordered
list of the nodes in $\cs$ in accordance to their depth, starting with the
shallowest nodes, and ending with the output node. Denote by $A_q$ the event
that $u_1,\ldots, u_q$ are well-initialized. We need to show that
$\Pr(A_{|\cs|})\ge 1-\delta$. We do so using an induction on $q$ for the
inequality $\Pr(A_q)\ge 1-\frac{q\delta}{|\cs|}$. Indeed, for $q=1,\ldots,n$,
$u_q$ is an input node and $\Pr(A_q)=1$. Thus, the base of the induction
hypothesis holds. Assume that $q>n$. By Claim (\ref{claim1}) we have that
$\Pr(A_q|A_{q-1})\ge 1-\frac{\delta}{|\cs|}$. Finally, from the induction
hypothesis we have,
\[
\Pr(A_q) \geq \Pr(A_q|A_{q-1})\Pr(A_{q-1}) \ge
	\left(1-\frac{\delta}{|\cs|}\right)
	\left(1-\frac{(q-1)\delta}{|\cs|}\right) \ge
	1-\frac{q\delta}{|\cs|} \,. \qed
\]

\subsection{Proofs of Thms.~\ref{thm:main_dist}~and~\ref{thm:main_dist_ReLU}}
Theorems~\ref{thm:main_dist}~and~\ref{thm:main_dist_ReLU} follow from using
the following lemma combined with
Theorems~\ref{thm:main_ker}~and~\ref{thm:main_ker_ReLU}. When we apply
the lemma, we always focus on the special case where one of the kernels is
constant w.p.\ $1$.
\begin{lemma} 
	\label{lem:app_ker_app_act}
Let $\cd$ be a distribution on $\cx\times\cy$, $\ell:\reals\times\cy\to\reals$
be an $L$-Lipschitz loss, $\delta>0$, and $\kappa_1,\kappa_2:\cx\times\cx\to\reals$ be two
independent random kernels sample from arbitrary distributions.
Assume that the following properties hold.
	\begin{itemize}
		\item For some $C>0$, $\forall \x\in \cx,\;
			\kappa_1(\x,\x),\kappa_2(\x,\x)\le C$.
		\item $\forall \x,\y\in
			\cx,\;\Pr_{\kappa_1,\kappa_2}
				\left(|\kappa_1(\x,\y)-\kappa_2(\x,\y)|\ge \epsilon\right)\le \tilde{\delta}$
				for $\tilde{\delta} < c_2 \frac{\epsilon^2\delta}{C^2\log^2\left(\frac{1}{\delta}\right)}$ where $c_2>0$ is a
				universal constant.
	\end{itemize}
	Then, w.p.\ $\ge 1-\delta$ over the choices of $\kappa_1,\kappa_2$, for every
	$f_1\in \ch^{M}_{\kappa_1}$ there is $f_2\in\ch^{\sqrt{2}M}_{\kappa_2}$ such that $\cl_{\cd}(f_2)\le \cl_{\cd}(f_1) + \sqrt{\epsilon}4LM$.
\end{lemma}
\noindent
To prove the above lemma, we state another lemma below followed by a basic
measure concentration result.
\begin{lemma}\label{lem:small_norm_small_l1}
Let $\x_1,\ldots,\x_m\in \reals^d$, $\w^*\in\reals^d$ and
$\epsilon>0$. There are weights $\alpha_1,\ldots,\alpha_m$
such that for $\w:=\sum_{i=1}^m\alpha_i\x_i$ we have,
\begin{itemize}
	\item $\cl(\w):=\frac{1}{m}\sum_{i=1}^m|\inner{\w,\x_i}-\inner{\w^*,\x_i}|\le\epsilon$
	\item $\sum_i |\alpha_i| \le \frac{\|\w^*\|^2}{\epsilon}$
	\item $\|\w\| \le \|\w^*\|$
\end{itemize}
\end{lemma}
\proof
Denote $M=\|\w^*\|$, $C = \max_i \|\x_i\|$, and
$y_i=\inner{\w^*,\x_i}$. Suppose that we run stochastic gradient decent on
the sample $\{(\x_1,y_1),\ldots,(\x_m,y_m)\}$ w.r.t.\ the loss $\cl(\w)$, with
learning rate $\eta = \frac{\epsilon}{C^2}$, and with projections onto the
ball of radius $M$. Namely, we start with $\w_0=0$ and at each iteration
$t\ge 1$, we choose at random $i_t\in [m]$ and perform the update,
\[
\tilde{\w}_t = \begin{cases}
\w_{t-1}-\eta\x_{i_t} & \inner{\w_{t-1},\x_{i_t}} \ge y_{i_t}
\\
\w_{t-1}+\eta\x_{i_t} & \inner{\w_{t-1},\x_{i_t}} < y_{i_t}
\end{cases}
\]
\[
\w_t = \begin{cases}
\tilde{\w}_{t} & \|\tilde{\w}_{t}\|\le M
\\
\frac{M \tilde{\w}_{t}}{\|\tilde{\w}_{t}\|} & \|\tilde{\w}_{t}\| > M
\end{cases}
\]
After $T=\frac{M^2C^2}{\epsilon^2}$ iterations the loss in expectation would
be at most $\epsilon$ (see for instance Chapter 14 in
\cite{shalev2014understanding}). In particular, there exists a sequence of
at most $\frac{M^2C^2}{\epsilon^2}$ gradient steps that attains a solution
$\w$ with $\cl(\w)\le \epsilon$.  Each update adds or subtracts
$\frac{\epsilon}{C^2}\x_i$ from the current solution. Hence $\w$ can be
written as a weighted sum of $\x_i$'s where the sum of each coefficient
is at most $T\frac{\epsilon}{C^2}=\frac{M^2}{\epsilon}$.
\proofbox

\begin{theorem}[\citet{BartlettMe02}] \label{thm:radamacher}
Let $\cd$ be a distribution over $\cx\times\cy$, $\ell:\reals\times\cy\to\reals$
a $1$-Lipschitz loss, $\kappa:\cx\times\cx\to \reals$ a kernel, and
$\epsilon,\delta>0$. Let $S=\{(\x_1,y_1),\ldots,(\x_m,y_m)\}$ be i.i.d.\
samples from $\cd$ such that
$m \ge c
	\frac{M^2 \max_{\x\in\cx}\kappa(\x,\x)+\log\left(\frac{1}{\delta}\right)}
	{\epsilon^2}$ where $c$ is a constant.
Then, with probability of at least $1-\delta$ we have,
\[
\forall f\in \ch^M_{\kappa},\; |\cl_\cd(f) - \cl_S(f)| \le \epsilon \,.
\]
\end{theorem}
%%
\proof (of Lemma \ref{lem:app_ker_app_act})
By rescaling $\ell$, we can assume w.l.o.g that $L=1$.  Let
$\epsilon_1=\sqrt{\epsilon}M$ and $S=\{(\x_1,y_1),\ldots,(\x_m,y_m)\}\sim\cd$
be i.i.d.\ samples which are independent of the choice of
$\kappa_1,\kappa_2$. By Theorem \ref{thm:radamacher}, for a large enough
constant $c$, if $m=c \frac{C  M^2 \log\left(\frac{1}{\delta}\right)}{\epsilon_1^2}=c \frac{C\log\left(\frac{1}{\delta}\right)}{\epsilon}$,
then w.p.\ $\ge 1-\frac{\delta}{2}$ over the choice of the samples we have,
\begin{equation}\label{eq:4}
\forall f\in \ch_{\kappa_1}^{M}\cup\ch_{\kappa_2}^{\sqrt{2}M} ,\;|\cl_\cd(f) - \cl_{S}(f)|\le \epsilon_1
\end{equation}
Now, if we choose $c_2=\frac{1}{2c^2}$ then w.p.\ $\ge 1-m^2\tilde{\delta} \ge 1-\frac{\delta}{2}$
(over the choice of the examples and the kernel), we have that
\begin{equation}\label{eq:5}
\forall i,j\in [m], |\kappa_1(\x_i,\x_j)-\kappa_2(\x_i,\x_j)|< \epsilon \,.
\end{equation}
In particular, w.p.\ $\ge 1-\delta$ \eqref{eq:4} and \eqref{eq:5} hold and
therefore it suffices to prove the conclusion of the theorem under these
conditions. Indeed, let $\Psi_1,\Psi_2:\cx\to \ch$ be two mapping from $\cx$ to
a Hilbert space $\ch$ so that $\kappa_i(\x,\y)=\inner{\Psi_i(\x),\Psi_i(\y)}$.
Let $f_1\in\ch^M_{\kappa_1}$. By lemma \ref{lem:small_norm_small_l1} there are
$\alpha_1,\ldots,\alpha_m$ so that for the vector
$\w=\sum_{i=1}^m\alpha_1\Psi_1(\x_i)$ we have
\begin{equation}\label{eq:6}
\frac{1}{m}\sum_{i=1}^m|\inner{\w,\Psi_1(\x_i)}-f_1(\x_i)|\le
	\epsilon_1,\;\;\|\w\|\le M \,,
\end{equation}
and
\begin{equation}\label{eq:7}
\sum_{i=1}^m|\alpha_i|\le \frac{M^2}{\epsilon_1} \,.
\end{equation}
Consider the function $f_2\in \ch_2$ defined by $f_2(\x)=\sum_{i=1}^m\alpha_1\inner{\Psi_2(\x_i),\Psi_2(\x)}$. We note that
\begin{eqnarray*}
\|f_2\|^2_{\ch_{k_2}} &\le & \left\|\sum_{i=1}^m\alpha_i\Psi_2(\x_i)\right\|^2
\\
&=& \sum_{i,j=1}^m \alpha_i\alpha_j\kappa_2(\x_i,\x_j)
\\
&\le& \sum_{i,j=1}^m \alpha_i\alpha_j\kappa_1(\x_i,\x_j)+\epsilon\sum_{i,j=1}^m |\alpha_i\alpha_j|
\\
&=& \|\w\|^2+\epsilon \left(\sum_{i=1}^m |\alpha_i|\right)^2
\\
&\le& M^2+\epsilon \frac{M^4}{\epsilon^2_1} = 2M^2 \,.
\end{eqnarray*}
Denote by $\tilde f_1(\x) = \inner{\w,\Psi_1(\x)}$ and note that for every
$i\in [m]$ we have,
\begin{eqnarray*}
|\tilde f_1(\x_i)-f_2(\x_i)| &=& \left|\sum_{j=1}^m\alpha_j\left(\kappa_1(\x_i,\x_j)-\kappa_2(\x_i,\x_j)\right)\right|
\\
&\le &\epsilon\sum_{i=1}^m|\alpha_i|\le
	\epsilon \frac{M^2}{\epsilon_1} = \epsilon_1 \,.
\end{eqnarray*}
Finally, we get that,
\begin{eqnarray*}
\cl_{\cd}(f_2) &\le& \cl_{S}(f_2) + \epsilon_1
\\
&=& \frac{1}{m}\sum_{i=1}^m \ell\left(f_2(\x_i),y_i\right) + \epsilon_1
\\
&\le& \frac{1}{m}\sum_{i=1}^m \ell\left(\tilde{f}_1(\x_i),y_i\right) + \epsilon_1 + \epsilon_1
\\
&\le& \frac{1}{m}\sum_{i=1}^m \ell\left(f_1(\x_i),y_i\right) + |\tilde f_1(\x_i)-f_1(\x_i)| + 2\epsilon_1
\\
&\le& \frac{1}{m}\sum_{i=1}^m \ell\left(f_1(\x_i),y_i\right) + 3\epsilon_1
\\
&\le& \cl_S(f_1) + 3\epsilon_1 \le \cl_{\cd}(f_1) + 4\epsilon_1 \,,
\end{eqnarray*}
which concludes the proof.\proofbox
