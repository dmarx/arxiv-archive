We develop a general duality between neural networks and compositional kernels,
striving towards a better understanding of deep learning. We show that initial
representations generated by common random initializations are sufficiently
rich to express all functions in the dual kernel space. Hence, though the
training objective is hard to optimize in the worst case, the initial
weights form a good starting point for optimization. Our dual view also
reveals a pragmatic and aesthetic perspective of neural networks and
underscores their expressive power.

\iffalse
We develop a general duality between neural networks and composite kernel
Hilbert spaces. To construct the duality we introduce the notion of a
computation skeleton which amounts to a directed acyclic graph with nodes
associated with activation functions. A random network is generated from a
computation skeleton by replicating its internal nodes and assigning random
weights to the edges of the expanded skeleton using a variance-normalized
multivariate Gaussians. To construct the kernel we first define the notion of
dual representation for an activation function. The resulting kernel is an
analytic computation scheme which starts by taking the inner-products between
input pairs, and then alternating between local summations and dual
activation, as prescribed by the skeleton (DAG) topology. We prove that the
inner-product defined by the output of the random network rapidly converge to
the analytic composite kernel. Moreover, the rate of convergence exhibits only
a mild dependence in the depth of the network. Our analysis provides several
insights:
1. Success of ReLU and bounded activation functions in general.
2. Succession of fully connected layers.
3. Stepping stone towards an apparatus for devising random, highly-nonlinear,
   with provable guarantees; enjoying the perils of convexity

We develop a general duality between neural networks and compositional
kernel spaces.
%
To achieve this, we introduce the notion of a computation
skeleton, an acyclic graph that succinctly describes both (i) a
family of neural networks with activations and (ii) a corresponding
kernel space. The kernel space comprises functions that arise by
composition, averaging, and non-linear transformation as determined by
the skeleton's graph topology and activation functions.

Via this framework, we prove that networks with random weights---drawn
as per the common initialization scheme for neural network
training---yield embeddings whose inner products rapidly approximate
the corresponding analytic kernel.
%
Our analysis yields several upshots:
\begin{enumerate}
\item The initial point of training is sufficiently
  rich to express all functions in the kernel space. Qualitatively,
  this may give a good starting point for optimization despite the
  worst-case hardness of the training objective.
\item The effect of architectural choices on the kernel space gives
  grounds for reasoning about network design. As examples: stacked
  fully-connected layers approach a degenerate kernel approximation,
  bounded activations imply faster concentration for the approximation,
  and ReLU activations allow the kernel approximation to hold even if the random
  weights are improperly scaled.
\item Optimizing the last layer of a random network could serve as a
  convex proxy for choosing among architectures prior to full training,
  as advocated and tested empirically in~\citet{saxe2011random}.
\end{enumerate}
\fi
