\section{Setting}

\paragraph{Notation.} We denote
%% scalars by lowercase and uppercase letters and (e.g., $x, X$),
vectors by bold-face letters (e.g.\ $\x$), and
matrices by upper case Greek letters (e.g.\ $\Sigma$). The $2$-norm of $\x
\in \reals^d$ is denoted by $\|\x\|$. For functions $\sigma:\reals\to\reals$
we let
$$
\|\sigma\| \textstyle
	:=\sqrt{\E_{X\sim\cn(0,1)}\sigma^2(X)}
	\; = \sqrt{\frac{1}{\sqrt{2\pi}}
		\int_{-\infty}^\infty \sigma^2(x)e^{-\frac{x^2}{2}}dx} \,.
$$
%
Let $G=(V,E)$ be a directed acyclic graph. The set of neighbors incoming to
a vertex $v$ is denoted $\IN(v):=\{u\in V\mid uv\in E\}$.
%% The $d$ dimensional vector space of the reals is denoted by
%% $\reals^d$.
The $d-1$ dimensional sphere is denoted $\sphere^{d-1} =
\{\x\in\reals^d \mid \|\x\|=1\}$. We provide a brief overview of
reproducing kernel Hilbert spaces in the sequel and merely introduce
notation here. In a Hilbert space $\ch$, we use a slightly
non-standard notation $\ch^B$ for the ball of radius $B$, $\{\x \in
\ch \mid \|\x\|_\ch \leq B\}$. We use $[x]_+$ to denote $\max(x,0)$
and $\ind[b]$ to denote the indicator function of a binary variable
$b$.

\paragraph{Input space.} Throughout the paper we assume that each example is
a sequence of $n$ elements, each of which is represented as a unit vector. 
Namely, we fix $n$ and take the input space to be
	$\cx=\cx_{n,d}=\left(\sphere^{d-1}\right)^n$.
Each input example is denoted,
\begin{align} \label{eq:coordinates}
	\x=(\x^1,\ldots,\x^n), ~\textwhere \x^i\in \sphere^{d-1} \,.
\end{align}
%
We refer to each vector $\x^i$ as the input's $i$th {\em
  coordinate}, and use $x^i_{j}$ to denote it $j$th scalar
entry. Though this notation is slightly non-standard, it unifies 
input types seen in various domains. For example,
binary features can be encoded by taking $d=1$, in which case
$\cx=\{\pm 1\}^n$. Meanwhile, images and audio signals are often
represented as bounded and continuous numerical values---we can assume
in full generality that these values lie in $[-1,1]$. To match the setup
above, we embed $[-1,1]$ into the circle $\sphere^1$, e.g.\ via the map $x
\mapsto \left(\sin\left(\frac{\pi x}{2}\right), \cos\left(\frac{\pi
  x}{2}\right)\right)$. When each coordinate is categorical---taking
one of $d$ values---we can represent category $j\in[d]$ by the unit
vector $\mathbf e_j\in\sphere^{d-1}$. When $d$ may be very large or
the basic units exhibits some structure, such as when the input is a sequence
of words, a more concise encoding may be
useful, e.g.\ as unit vectors in a low dimension space $\sphere^{d'}$
where $d'\ll d$ (see for
instance~\citet{mikolov2013distributed,levy2014neural}).

\paragraph{Supervised learning.} The goal in supervised learning is to
devise a mapping from the input space $\cx$ to an output space $\cy$ based on a sample
$S=\{(\x_1,y_1),\ldots,(\x_m,y_m)\}$, where $(\x_i,y_i)\in\cx\times\cy$, 
drawn i.i.d.\ from a distribution $\cd$ over $\cx\times\cy$.
%
A supervised learning problem is further specified by an output length $k$ and a loss function
$\ell : \reals^k \times \cy \to [0,\infty)$, and the goal is to find a
predictor $h:\cx\to\reals^k$ whose loss,
$\cl_{\cd}(h) := \E_{(\x,y)\sim\cd} \ell(h(\x),y)$, is small.
%
The {\em empirical} loss $\cl_{S}(h):= \frac 1 m \sum_{i=1}^m
\ell(h(\x_i),y_i)$ is commonly used as a proxy for the loss
$\cl_{\cd}$.
%
Regression problems correspond to $\cy=\reals$ and, for
instance, the squared loss $\ell(\hat y,y)=(\hat y -y)^2$.
%
Binary classification is captured by $\cy=\{\pm 1\}$ and, say, the
zero-one loss $\ell(\hat y,y)= \ind[\hat y y \leq 0]$ or the hinge
loss $\ell(\hat y,y)=[1-\hat y y]_+$, with standard extensions to the
multiclass case.
A loss $\ell$ is $L$-Lipschitz if $|\ell(y_1,y) - \ell(y_2,y) | \leq L
|y_1 - y_2|$ for all $y_1,y_2 \in \reals^k$, $y \in \cy$, and it is
convex if $\ell(\cdot,y)$ is convex for every $y\in\cy$.
%% Moreover, $\ell$ is
%% $L$-Lipschitz if $\ell(\cdot,y)$ is $L$-Lipschitz for every $y\in\cy$.

\paragraph{Neural network learning.} We define a {\em neural network} $\cn$
to be a vertices weighted directed acyclic graph (DAG) whose nodes are denoted $V(\cn)$ and edges
$E(\cn)$. The weight function will be denoted by $\delta:V(\cn)\to [0,\infty)$, and its sole role would be to dictate the distribution of the initial weights (see definition \ref{def:rand_weights}).
Each of its internal units, i.e.\ nodes with both incoming and
outgoing edges, is associated with an {\em activation} function
$\sigma_v:\reals\to\reals$. In this paper's context, an activation can
be any function that is square integrable with respect to the Gaussian
measure on $\reals$. We say that $\sigma$ is {\em normalized} if
$\|\sigma\|=1$. The set of nodes having only incoming edges are called the
output nodes.
%
To match the setup of a supervised learning problem, a network $\cn$ has
$nd$ input nodes and $k$ output nodes, denoted $o_1,\ldots,o_k$. A
network $\cn$ together with a weight vector $\w=\{w_{uv} \mid uv\in E\}$ defines a
predictor $h_{\cn,\w}:\cx\to\reals^k$ whose prediction
is given by ``propagating'' $\x$ forward through the network.
Formally, we define $h_{v,\w}(\cdot)$ to be the output of the subgraph
of the node $v$ as follows: for an input node $v$, $h_{v,\w}$ outputs the corresponding coordinate in $\x$, and
for all other nodes, we define $h_{v,\w}$ recursively as
$$h_{v,\w}(\x) = \sigma_v\left(\textstyle
	\sum_{u\in \IN(v)}\, w_{uv}\,h_{u,\w}(\x)\right)\,.$$
Finally, we let $h_{\cn,\w}(\x)=(h_{o_1,\w}(\x),\ldots,h_{o_k,\w}(\x))$.
We also refer to internal nodes as {\em hidden units}. The {\em output
layer} of $\cn$ is the sub-network consisting of all output neurons of $\cn$
along with their incoming edges. The {\em representation} induced by a network
$\cn$ is the network $\netrep(\cn)$ obtained from $\cn$ by removing the output
layer. The {\em representation} function induced by the weights $\w$ is
$\rep_{\cn,\w}:=h_{\netrep(\cn),\w}.$
Given a sample $S$, a learning algorithm searches
for weights $\w$ having small empirical loss
$\cl_S(\w)=\frac{1}{m}\sum_{i=1}^m \ell(h_{\cn,\w}(\x_i),y_i)$. A popular
approach is to randomly initialize the weights and then use a variant
of the stochastic gradient method to improve these weights in the
direction of lower empirical loss.

\paragraph{Kernel learning.} A function $\kappa:\cx\times \cx\to \reals$ is
a {\em reproducing kernel}, or simply a kernel, if for every
$\x_1,\ldots,\x_r\in\cx$, the $r \by r$ matrix $\Gamma_{i,j} = \{\kappa(\x_i,\x_j)\}$
is positive semi-definite. Each kernel induces a Hilbert space
$\ch_{\kappa}$ of functions from $\cx$ to $\reals$ with a corresponding norm
$\|\cdot\|_{\ch_\kappa}$. A kernel and its corresponding space are {\em
normalized} if $\forall \x\in\cx,\;\kappa(\x,\x)=1$. Given a convex loss
function $\ell$, %$:\reals^k\times \cy\to [0,\infty)$,
a sample $S$, and a kernel
$\kappa$, a kernel learning algorithm finds a function
$f=(f_1,\ldots,f_k)\in\ch^k_{\kappa}$ whose empirical loss,
$\cl_S(f)=\frac{1}{m}\sum_i \ell(f(\x_i),y_i)$, is minimal among all
functions with $\sum_{i}\|f_i\|_\kappa^2 \le R^2$ for some $R>0$.
Alternatively, kernel algorithms minimize the {\em regularized loss},
$$\cl^R_S(f)=\frac{1}{m}\sum_{i=1}^m \ell(f(\x_i),y_i) +
\frac{1}{R^2}\sum_{i=1}^k\|f_i\|^2_{\kappa} \,, $$
a convex objective that often can be efficiently minimized.
