\section{Main results} \label{results:sec}
%
We review our main results. Let us fix a compositional kernel $\cs$. There are a
few upshots to underscore upfront. First, our analysis implies that a
representation generated by a random initialization of $\cn=\cn(\cs,r,k)$
approximates the kernel $\kappa_\cs$. The sense in which the result holds is
twofold. First, with the proper rescaling we show that
$\inner{\rep_{\cn,\w}(\x),\rep_{\cn,\w}(\x')}\approx \kappa_{\cs}(\x,\x')$.
Then, we also show that the functions obtained by composing
bounded linear functions with $\rep_{\cn,\w}$ are approximately the bounded-norm functions in $\ch_\cs$. In other words, the functions
expressed by $\cn$ under varying the weights of the last layer are
approximately bounded-norm functions in $\ch_\cs$. For simplicity, we restrict
the analysis to the case $k=1$. We also confine the analysis to either bounded
activations, with bounded first and second derivatives, or the ReLU activation.
Extending the results to a broader family of activations is left for future
work. Through this and remaining sections we use $\gtrsim$ to hide universal constants.
%
\begin{definition}
%
An activation $\sigma:\reals\to\reals$ is {\em $C$-bounded} if it is twice
continuously differentiable and $\|\sigma\|_{\infty},
\|\sigma'\|_{\infty},\|\sigma''\|_\infty\le \|\sigma\|C$.
%
\end{definition}
%
\noindent Note that many activations are $C$-bounded for some constant $C>0$. In
particular, most of the popular sigmoid-like functions such as
${1}/({1+e^{-x}})$, $\erf(x)$, ${x}/{\sqrt{1+x^2}}$, $\tanh(x)$,
and $\tan^{-1}(x)$ satisfy the boundedness requirements. We next introduce
terminology that parallels the representation layer of $\cn$ with a kernel
space. Concretely, let $\cn$ be a network whose representation part has $q$
output neurons. Given weights $\w$, the {\em normalized representation}
$\Psi_\w$ is obtained from the representation $R_{\cn,\w}$ by
dividing each output neuron $v$ by $\|\sigma_v\|\sqrt{q}$. The {\em empirical
kernel} corresponding to $\w$ is defined as
$\kappa_\w(\x,\x')=\inner{\Psi_{\w}(\x),\Psi_{\w}(\x')}$. We also define the
{\em empirical kernel space} corresponding to $\w$ as
$\ch_\w=\ch_{\kappa_\w}$. Concretely,
\[
\ch_\w=\left\{h_{\bv}(\x)=\inner{\bv,\Psi_\w(x)}\mid \bv\in\reals^q\right\}~,
\]
and the norm of $\ch_\w$ is defined as $\|h\|_\w=\inf\{\|\bv\|\mid{}h=h_\bv\}$. Our first result shows that the empirical kernel approximates
the kernel $k_\cs$.
\begin{theorem}\label{thm:main_ker}
Let $\cs$ be a skeleton with $C$-bounded activations. Let $\w$ be a random
initialization of $\cn=\cn(\cs,r)$ with
$$r \ge \frac
	{(4C^4)^{\depth(\cs)+1} \log\left({8|\cs|}/{\delta}\right)}
	{\epsilon^2} \,.$$
Then, for all $\x,\x'$, with probability of at least $1-\delta$,
$$|k_\w(\x,\x')-k_\cs(\x,\x')|\le \epsilon\,.$$
\end{theorem}
%
\noindent We note that if we fix the activation and assume that the depth of $\cs$ is
logarithmic, then the required bound on $r$ is polynomial. For the
ReLU activation we get a stronger bound with only quadratic dependence on
the depth. However, it requires that $\epsilon\le{1}/{\depth(\cs)}$.
%
\begin{theorem}\label{thm:main_ker_ReLU}
Let $\cs$ be a skeleton with ReLU activations. Let $\w$ be a random
initialization of $\cn(\cs,r)$ with
$$
	r \gtrsim \frac{\depth^2(\cs) \,
		\log\left({|\cs|}/{\delta}\right)}
	{\epsilon^2} \,.
$$
Then, for all $\x,\x'$ and $\epsilon\lesssim{}1/{\depth(\cs)}$, with
probability of at least $1-\delta$,
$$ |\kappa_\w(\x,\x')-\kappa_\cs(\x,\x')|\le \epsilon \,.$$
\end{theorem}
%
\noindent For the remaining theorems, we fix a $L$-Lipschitz loss
$\ell:\reals\times\cy\to [0,\infty)$.  For a distribution $\cd$ on
$\cx\times\cy$ we denote by $\|\cd\|_0$ the cardinality of the support of
the distribution. We note that $\log\left(\|\cd\|_0\right)$ is bounded by,
for instance, the number of bits used to represent an element in
$\cx\times\cy$. We use the following notion of approximation.
%
\begin{definition}
Let $\cd$ be a distribution on $\cx\times\cy$. A space
$\ch_1\subset\reals^\cx$ {\em $\epsilon$-approximates} the space
$\ch_2\subset\reals^\cx$ w.r.t.\ $\cd$ if for every $h_2\in\ch_2$ there is
$h_1\in\ch_1$ such that $\cl_\cd(h_1)\le \cl_\cd(h_2)+\epsilon$.
\end{definition}
\begin{theorem}\label{thm:main_dist}
Let $\cs$ be a skeleton with $C$-bounded activations. Let $\w$ be a random
initialization of $\cn(\cs,r)$ with
$$r \gtrsim \frac
	{L^4 \, R^4 \, (4C^4)^{\depth(\cs)+1}
		\log\left(\frac{LRC |\cs|}{\epsilon\delta}\right)}
	{\epsilon^4} \,.$$
Then, with probability of at least $1-\delta$ over the choices
of $\w$ we have that $\ch_\w^{\sqrt{2}R}$ $\epsilon$-approximates
$\ch^R_\cs$ and $\ch_\cs^{\sqrt{2}R}$ $\epsilon$-approximates $\ch^R_\w$.
\end{theorem}
%
\begin{theorem}\label{thm:main_dist_ReLU}
Let $\cs$ be a skeleton with ReLU activations and
$\epsilon\lesssim {1}/{\depth(\cc)}$. Let $\w$ be a random initialization of
$\cn(\cs,r)$ with
$$r \gtrsim \frac{L^4 \,R^4\, \depth^2(\cs)\,
	\log\left(\frac{\|\cd\|_0 |\cs|}{\delta}\right)}{\epsilon^4} \,.
$$ Then, with probability of at least $1-\delta$ over the choices of $\w$ we
have that $\ch_\w^{\sqrt{2}R}$ $\epsilon$-approximates $\ch^R_\cs$ and
$\ch_\cs^{\sqrt{2}R}$ $\epsilon$-approximates $\ch^R_\w$.
\end{theorem}
\noindent As in Theorems \ref{thm:main_ker} and \ref{thm:main_ker_ReLU}, for a fixed
$C$-bounded activation and logarithmically deep $\cs$, the required bounds on $r$
are polynomial. Analogously, for the ReLU activation the bound is polynomial
even without restricting the depth. However, the polynomial growth in
Theorems~\ref{thm:main_dist}~and~\ref{thm:main_dist_ReLU} is rather large.
Improving the bounds, or proving their optimality, is left to future work.

\subsection{Incorporating bias terms}
Our results can be extended to incorporate bias terms. Namely, in addition to the weights we can add a bias vector $\bb = \{b_v\mid v\in V\}$ and let each neuron compute the function
$$h_{v,\w,\bb}(\x) = \sigma_v\left(\textstyle
	\sum_{u\in \IN(v)}\, w_{uv}\,h_{u,\w}(\x) + b_v\right)\,.$$
To do so, we extend the definition of random initialization and compositional kernel as follows:

\begin{definition}[Random weights with bias terms]
%
Let $0\le \beta\le 1$. A {\em $\beta$-biased random initialization} of a neural network $\cn$ is a
multivariate Gaussian $(\bb,\w)=((w_{uv})_{uv\in E(\cn)},(b_v)_{v\in V(\cn)})$ such that each weight
$w_{uv}$ is sampled independently from a normal distribution with mean $0$
and variance ${(1-\beta)d\delta(u)}/{\delta(\IN(v))}$ if $u$ is an input neuron and ${(1-\beta)\delta(u)}/{\left(\|\sigma_{u}\|^2\,\delta(\IN(v))\right)}$ otherwise. Finally, each bias term
$b_{v}$ is sampled independently from a normal distribution with mean $0$
and variance $\beta$.
%
\end{definition}


\begin{definition}[Compositional kernels with bias terms]
%
Let $\cs$ be a computation skeleton with normalized activations and
(a single) output node $o$, and let $0\le \beta\le 1$.
%
For every node $v$, inductively define a kernel
$\kappa^\beta_v:\cx\times\cx\to\reals$ as follows.
%
For an input node $v$ corresponding to the $i$th coordinate,
define $\kappa^\beta_{v}(\x,\y)=\inner{\x^i, \y^i}$.
%
For a non-input node $v$, define
$$
\kappa^\beta_v(\x,\y) =
	\hat\sigma_v\left((1-\beta)
		\frac{\sum_{u\in \IN(v)}\kappa^\beta_{u}(\x,\y)}{|\IN(v)|} + \beta\right) \,.
    $$
The final kernel $\kappa^\beta_\cs$ is $\kappa^\beta_o$, the kernel associated with
the output node $o$.
\end{definition}
Note that our original definitions correspond to $\beta=0$. With the above definitions, Theorems \ref{thm:main_ker}, \ref{thm:main_ker_ReLU}, \ref{thm:main_dist} and \ref{thm:main_dist_ReLU} extend to the case when there exist bias terms. To simplify the notation, we focus on the case when there are no biases.
