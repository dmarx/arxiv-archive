%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
\label{sec:related}

\paragraph{Current theoretical understanding of NN learning.} Understanding
neural network learning, particularly its recent successes, commonly
decomposes into the following research questions.

\begin{enumerate}[label=(\roman*)]
  \item \label{related_q1} What functions can be efficiently expressed by
    neural networks?

  \item \label{related_q2} When does a low empirical loss result in a low
    population loss?

	\item \label{related_q3} Why and when do efficient algorithms, such
	as stochastic gradient, find good weights?

\end{enumerate}

\noindent Though still far from being complete, previous work provides some
understanding of questions~\ref{related_q1}~and~\ref{related_q2}. Standard
results from complexity theory~\cite{karp1980some} imply that essentially all
functions of interest (that is, any efficiently computable function) can be
expressed by a network of moderate size. Biological phenomena show that many
relevant functions can be expressed by even simpler networks, similar to
convolutional neural networks~\cite{lecun1998gradient} that are dominant in ML
tasks today. Barron's theorem~\cite{Barron93} states that even two-layer
networks can express a very rich set of functions. As for
question~\ref{related_q2}, both classical~\cite{BaumHa89, Bartlett98,
AnthonyBa99} and more recent~\cite{behnam2015norm,hardt2015train} results from
statistical learning theory show that as the number of examples grows in
comparison to the size of the network the empirical loss must be close to the
population loss. In contrast to the first two, question~\ref{related_q3} is
rather poorly understood. While learning algorithms succeed in practice,
theoretical analysis is overly pessimistic. Direct interpretation of
theoretical results suggests that when going slightly deeper beyond single
layer networks, e.g.\ to depth two networks with very few hidden units, it is
hard to predict even marginally better than random~\cite{KearnsVa89,
KlivansSh06, danielySh2014, daniely2013average, daniely2015complexity}.
Finally, we note that the recent empirical successes of NNs have prompted a
surge of theoretical work around NN learning \cite{safran2015basin,
andoni2014learning, arora2014provable, bruna2013invariant, neyshabur2015path,
livni2014computational, giryes2015deep, sedghi2014provable,
choromanska2015loss}.

\paragraph{Compositional kernels and connections to networks.} The idea of
composing kernels has repeatedly appeared throughout the machine learning
literature, for instance in early work by~\citet{scholkopf1998prior,
grauman2005pyramid}. Inspired by deep networks' success, researchers
considered deep composition of kernels~\cite{mairal2014convolutional,
cho2009kernel, bo2011object}. For fully connected two-layer networks, the
correspondence between kernels and neural networks with random weights has
been examined in~\cite{rahimi2009weighted, RahimiRe07, neal2012bayesian,
williams1997infinite}. Notably, Rahimi and Recht~\cite{rahimi2009weighted}
proved a formal connection (similar to ours) for the RBF kernel. Their work
was extended to include polynomial kernels~\cite{kar2012random,
pennington2015spherical} as well as other kernels~\cite{bach2015equivalence,
bach2014breaking}. Several authors have further explored ways to extend this
line of research to deeper, either fully-connected
networks~\cite{cho2009kernel} or convolutional networks~\cite{hazan2015steps,
anselmi2015deep, mairal2014convolutional}.  Our work sets a common foundation
for and expands on these ideas. We extend the analysis from fully-connected
and convolutional networks to a rather broad family of architectures. In
addition, we prove approximation guarantees between a network and its
corresponding kernel in our more general setting. We thus extend previous
analyses that only applies to fully connected two-layer networks. Finally, we
use the connection as an analytical tool to reason about architectural design
choices.
