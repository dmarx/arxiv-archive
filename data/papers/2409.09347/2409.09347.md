---
abstract: |
  Mass transport problems arise in many areas of machine learning whereby one wants to compute a map transporting one distribution to another. Generative modeling techniques like Generative Adversarial Networks (GANs) and Denoising Diffusion Models (DDMs) have been successfully adapted to solve such transport problems, resulting in CycleGAN and Bridge Matching respectively. However, these methods do not approximate Optimal Transport (OT) maps, which are known to have desirable properties. Existing techniques approximating OT maps for high-dimensional data-rich problems, such as DDM-based Rectified Flow and Schrödinger Bridge procedures, require fully training a DDM-type model at each iteration, or use mini-batch techniques which can introduce significant errors. We propose a novel algorithm to compute the Schrödinger Bridge, a dynamic entropy-regularised version of OT, that eliminates the need to train multiple DDM-like models. This algorithm corresponds to a discretisation of a flow of path measures, which we call the Schrödinger Bridge Flow, whose only stationary point is the Schrödinger Bridge. We demonstrate the performance of our algorithm on a variety of unpaired data translation tasks.
author:
- |
  Valentin De Bortoli [^1]  
  Google DeepMind Iryna Korshunova   
  Google DeepMind Andriy Mnih  
  Google DeepMind Arnaud Doucet  
  Google DeepMind
bibliography:
- main.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  Schrödinger Bridge Flow  
  for Unpaired Data Translation
---





# Introduction

The problem of finding a map to transport one probability distribution to another one has numerous applications in machine learning. In particular, it is at the core of generative modeling where the idea is to transform a noise distribution into the data distribution, and is also central to transfer learning tasks such as image-to-image translation. For discrete probability distributions, it is possible to compute the Optimal Transport (OT) map but this is computationally expensive . By showing that an entropy-regularised version of OT, the Entropic OT (EOT), could be computed much more efficiently using the Sinkhorn algorithm, has enabled transport ideas to be used in numerous applications . However, the computational complexity of Sinkhorn algorithm is quadratic in the sample size, which makes its application to very large datasets impractical. Mini-batch versions have been proposed, see e.g. , but tend to introduce significant errors in high dimensions .

In the context of generative modeling, Denoising Diffusion Models (DDMs) have shown impressive performance in a variety of domains. DDMs define a forward process progressively noising the data, and sample generation is achieved by approximating the time-reversal of this diffusion. In order to leverage the iterative refinement properties of DDMs in the OT setting, methods exploiting the equivalence between the static versions of (E)OT and their dynamic counterparts have been developed. A procedure to approximate the dynamic OT is considered by , while techniques to approximate the dynamic equivalent to EOT, the Bridge (SB), have been proposed in . These techniques are expensive however, as they require training multiple DDM-type models. Mini-batch versions of OT and Sinkhorn combined with bridge or flow matching have also been proposed to approximate the OT path and SB, but they optimise a minibatch OT objective that can introduce significant errors in high dimensions: the error in Wasserstein-$1$ distance is of order $O(B^{-1/(2d)})$, where $d$ is the dimension of the problem and $B$ the minibatch size, see .

In this paper, we propose a novel approach to computing the SB. Similarly to Iterative Markovian Fitting (IMF) and its practical implementation, Diffusion Schrödinger Bridge Matching (DSBM) , it leverages the fact that the SB is the only Markov process with prescribed marginals at the endpoints which is in the reciprocal class of the Brownian motion, i.e. it has the same bridge as the Brownian motion ; see for more details on Markov processes and the reciprocal class. Compared to DSBM, our approach is easier to implement as it does not require caching samples, alternating between optimising two different losses, and, optionally, uses one neural network instead of two. In , we start by introducing a flow of path measures whose time-discretisation yields a family of algorithms called $\alpha$-IMF and presented in . Notably, we show that $\alpha$-IMF converges to the Schrödinger Bridge for any $\alpha \in (0,1]$. Additionally, for a special value of the discretisation stepsize $\alpha = 1$, we recover the IMF procedure , while $\alpha < 1$ corresponds to online versions of IMF. We implement a parametric version of the $\alpha$-IMF as an online DSBM procedure, called $\alpha$-DSBM. We illustrate the efficiency of our approach in *unpaired* image-to-image translation settings in .

#### Notation.

We denote the space of *path measures* by $\pathmeas$, i.e. $\pathmeas=\mathcal{P}(\rmC(\ccint{0,1}, \rset^d))$, where $\rmC(\ccint{0,1}, \rset^d)$ is the space of continuous functions from $\ccint{0,1}$ to $\rset^d$. The subset of *Markov* path measures associated with a diffusion of the form $\rmd \bfX_t = v_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t$, with $\sigma,v$ locally Lipschitz, is denoted $\calM$. For $\Qbb$ induced by $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$, with $\vareps >0$ and $(\bfB_t)_{t \geq 0}$ a $d$-dimensional Brownian motion, the *reciprocal class* of $\Qbb$ is denoted $\calR(\Qbb)$, see . For any $\Pbb \in \pathmeas$, we denote by $\Pbb_t$ its marginal distribution at time $t$, $\Pbb_{s,t}$ the joint distribution at times $s,t$, $\Pbb_{s|t}$ the conditional distribution at time $s$ given the state at time $t$, and $\Pbb_{|0,1} \in \pathmeas$ the distribution of the path on time interval $(0,1)$ given its endpoints; e.g. $\Qbb_{|0,1}$ is a scaled Brownian bridge. Unless specified otherwise, all gradient operators $\nabla$ are w.r.t. the variable $x_t$ with time index $t$. Given probability spaces $(\msx, \mathcal{X})$ and $(\mathsf{Y}, \mathcal{Y})$, a Markov kernel $\mathrm{K}: \ \msx \times \mathcal{Y} \to [0,1]$, and a probability measure $\mu$ defined on $\mathcal{X}$, we write $\mu \mathrm{K}$ for the probability measure on $\mathcal{Y}$ such that for any $\mathsf{A} \in \mathcal{Y}$ we have $\mu \mathrm{K}(\mathsf{A}) = \int_{\msx} \mathrm{K}(x, \mathsf{A}) \rmd \mu(x)$. In particular, for any joint distribution $\Pi_{0,1}$ over $\rset^d \times \rset^d$, we denote the *mixture of bridges* measure as $\Pi = \Pi_{0,1} \Pbb_{|0,1} \in \pathmeas$, which is short for $\Pi(\cdot) = \int_{\rset^d \times \rset^d} \Pbb_{|0,1}(\cdot|x_0, x_1) \rmd \Pi_{0,1}(x_0, x_1)$. Finally, we define the Kullback–Leibler (KL) divergence between two probability measures $\pi_0, \pi_1 \in \mathcal{P}(\msx)$ as $\KL(\pi_0 | \pi_1) = \int_{\msx} \log ((\rmd \pi_0 / \rmd \pi_1)(x)) \rmd \pi_0(x)$ if $\pi_0$ is absolutely continuous w.r.t. $\pi_1$ and $\KL(\pi_0 | \pi_1) = +\infty$ otherwise.

# Optimal Transport and Schrödinger Bridge

#### Unpaired Transfer and Optimal Transport.

Given unpaired data samples from $\pi_0$ and $\pi_1$, where $\pi_0,\pi_1$ are two distributions on $\rset^d$, we are interested in designing a transport map from $\pi_0$ to $\pi_1$. This corresponds to an *unpaired data transfer task*. We can formulate this problem as finding a distribution $\Pi$ on $\rset^d\times \rset^d$ with marginals $\Pi_0 = \pi_0$ and $\Pi_1 = \pi_1$ so that if $\bfX_0 \sim \pi_0$ then $\bfX_1|\bfX_0 \sim \Pi_{1|0}(\cdot|\bfX_0)$ satisfies $\bfX_1 \sim \pi_1$. Among an infinite number of such so-called coupling distributions $\Pi$, we are here interested in finding the Entropic Optimal Transport (EOT) coupling $\Pi^\star$ defined as $$\label{eq:static_ot}
   \Pi^\star = \argmin_{\Pi \in \calP(\rset^d\times \rset^d)} \left\{ \int_{\rset^d \times \rset^d} \frac{1}{2} \| x - y \|^2 \rmd \Pi(x,y) - \varepsilon \mathrm{H}(\Pi) \ ; \ \Pi_0 = \pi_0, \ \Pi_1 = \pi_1 \right\} ,$$ where $\mathrm{H}(\Pi)$ is the differential entropy of $\Pi$ and $\vareps > 0$ is a regularisation hyperparameter . For $\vareps= 0$, we recover the standard OT.

In order to leverage the recent advances in generative modeling, and in particular the concept of *iterative refinement* central to DDMs, we turn to a *dynamic* formulation of EOT known as the *Schrödinger Bridge* problem . It is defined as follows: find $\Pbb^\star \in \mathcal{P}(\mathcal{C})$ such that $$\label{eq:dynamic_ot}
    \Pbb^\star = \argmin_{\Pbb \in \calP(\calC)} \{ \KL(\Pbb | \Qbb)  \ ; \ \Pbb_0 = \pi_0, \ \Pbb_1 = \pi_1 \} ,$$ with $\Qbb \in \pathmeas$ induced by a scaled $d$-dimensional Brownian motion $(\sqrt{\varepsilon} \bfB_t)_{t \in [0,1]}$. The term *dynamic* here refers to the fact that is defined on path measures, i.e. on (stochastic) processes, in contrast to the *static* problem which is defined on measures on the space $\rset^d \times \rset^d$. In , we show that solving is equivalent to optimising the vector field of a stochastic process using objectives similar to the ones of bridge matching . Under mild assumptions, it can be shown that $\Pbb^\star_{0,1} = \Pi^\star$, see e.g. . Hence solving reduces to solving . Once we have found $\Pbb^\star$ associated with $(\bfX_t^\star)_{t \in [0,1]}$, we can sample from $\Pbb^\star$ by first sampling $\bfX_0^\star \sim \pi_0$ and then sampling the trajectory $(\bfX_t^\star)_{t \in (0,1]}$ which yields $(\bfX_0^\star, \bfX_1^\star)\sim \Pi^\star$.

#### Reciprocal and Markov projections.

To introduce our methodology, it is necessary to recall the notions of reciprocal and Markov projections. We refer to for more details. For practitioners, a more intuitive explanation of these projections is given in .

In other words, $\Pbb$ is in the reciprocal class of $\Qbb$ if the conditional distribution of a path given its endpoints is identical under $\Pbb$ and $\Qbb$, see . Sampling from the reciprocal projection of $\Pbb$ can be achieved by sampling a path $(\bfX_t)_{t\in[0,1]}$ from $\Pbb$, keeping only the values of the endpoints, say $\bfX_0,\bfX_1$, and then sampling a new value for the bridge $(\bfX_t)_{t\in(0,1)}$ from $\Qbb_{|0,1}$.

In practice, implementing a Markovian projection requires solving a regression problem to approximate $\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t]$, similar to the one appearing in bridge matching and flow matching. One key property of the Markovian projection is that $\Mbb_t = \Pbb_t$ for all $t \in [0,1]$, i.e. the Markovian projection preserves the marginals; see for instance.

#### Iterative Markovian Fitting.

Leveraging the reciprocal and Markovian projections, and concurrently introduced IMF. Starting from $\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$, a measure where endpoints are sampled independently from $\pi_0$ and $\pi_1$ and then interpolated using a (scaled) Brownian bridge, they define a sequence of path measures $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ where $\Pbb^{n} = \projM(\hat{\Pbb}^{n})$ and $\hat{\Pbb}^{n+1} = \projsimpleR(\Pbb^{n})$. This ensures that $\Pbb^n_0 = \pi_0$, $\Pbb^n_1 = \pi_1$ for all $n$, and it can be shown that the sequence $(\Pbb^n)_{n \in \nset}$ converges to the SB, see . The practical implementation of this algorithm proposed by is called DSBM. Implementing DSBM poses challenges, as each Markovian projection requires training a neural network to approximate the relevant conditional expectations by minimising a bridge matching loss. Furthermore, in practice, generated model samples are stored in a cache in order to train the next iterations of DSBM. This introduces additional hyperparameters that require tuning. In we propose $\alpha$-IMF, an algorithm which can be interpreted as the discretisation of a *flow of path measures*. This leads to $\alpha$-DSBM, an algorithm that is computationally much more efficient than DSBM as it does not rely on a Markovian projection at each step.

# Schrödinger Bridge flow

We will now introduce a flow of path measures $(\Pbb^s)_{s \geq 0}$, and show that the time-discretisation of this flow with an appropriate stepsize $\alpha \in (0,1]$ yields a family of procedures called $\alpha$-IMF, which all converge to the Schrödinger Bridge. While $\alpha=1$ yields the classical IMF, $\alpha \in (0,1)$ yields an *incremental* version of IMF. In Section we show that $\alpha$-IMF can be implemented as an *online* version of DSBM.

## A flow of path measures

Let $(\Pbb^s, \hat{\Pbb}^s)_{s \geq 0}$ be a *flow of path measures* defined for any $s \geq 0$ by $$\begin{aligned}
    &\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}, \quad \partial_s \hat{\Pbb}^s = \projsimpleR(\projM(\hat{\Pbb}^s)) -\hat{\Pbb}^s , \quad \Pbb^s = \projM(\hat{\Pbb}^s), \label{eq:flow_hat_p}
\end{aligned}$$

which we assume is well-defined. Note that for any $s \geq 0$, $\Pbb^s$ is Markov while $\hat{\Pbb}^s$ is in the reciprocal class of $\Qbb$. Crucially, the only fixed point of is the SB. Indeed, let $\bar{\Pbb}$ be a fixed point of $(\Pbb^s)_{s \geq 0}$ in . Then, we have that $\bar{\Pbb} = \projsimpleR(\projM(\bar{\Pbb}))$. Hence, we get $\bar{\Pbb} = \projsimpleR(\projM(\dots(\projsimpleR(\projM(\bar{\Pbb})))\dots))$. Hence, under mild assumptions, $\bar{\Pbb}$ is a limit point of IMF and therefore $\bar{\Pbb}$ is the SB $\Pbb^\star$ given by , see .

Next, for any $\alpha \in (0,1]$, we define the following discretisation of called $\alpha$-IMF: $$\label{eq:discretisation_flow_path_measure}
        \hat{\Pbb}^{n+1} = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\projM(\hat{\Pbb}^n)) ,$$ and $\Pbb^n = \projM(\hat{\Pbb}^n)$. Note that for any $n \in \nset$, $\hat{\Pbb}^n \in \calR(\Qbb)$. This recovers the IMF procedure when $\alpha = 1$. Using the definition of the sequence $(\hat{\Pbb}^n)_{n \in \nset}$, it is possible to analyse the sequence $(\Pbb^n)_{n \in \nset}$ using the properties of the KL divergence as well as the Pythagorean identities derived in . Using the characterisation of the SB as the only path measure that preserves $\pi_0$, $\pi_1$, and is both Markov and in the reciprocal class of $\Qbb$ (see e.g. ), we get the following result.

## Discretisation and non-parametric loss

We show here that $\alpha$-IMF is associated with an *incremental* version of DSBM for $\alpha \in (0,1)$.

#### Iterative Markovian Fitting.

For any $v: \ [0,1] \times \rset^d \to \rset^d$, we introduce the loss function $$\label{eq:loss_function}
   \Lnonparam(v, \Pbb)  = \int_0^1\Lnonparam_t(v_t, \Pbb) \rmd t = \int_0^1 \int_{(\rset^d)^3} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t | x_0, x_1) \rmd t ,$$ where we recall that $\Qbb$ is induced by $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ for some $\vareps >0$. This loss was already considered in . We also define the path measure $\Pbb_v \in \pathmeas$ associated with $$\label{eq:mixture_bridge_sde}
    \rmd \bfX_t = v_t(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .$$ Consider first the sequence $(v^n)_{n \in \nset}$ defined by $$\label{eq:view_dsbm_non_param}
    v^{n+1} = \argmin_v \Lnonparam(v, \Pbb_{v^n}) .$$ Using , we have that $\Pbb_{v^{n+1}} = \projM(\projsimpleR(\Pbb_{v^n}))$, which corresponds to $\Pbb^{n+1}$ in the IMF sequence. Therefore we have that $\lim_{n \to +\infty} \Pbb_{v^n} = \Pbb^\star$ under mild assumptions .

#### Functional gradient descent.

We now introduce a relaxation of , where, instead of considering the $\argmin$, we update the vector field with one gradient step. To define this relaxation, we recall that for a functional $F: \ \mathcal{F} \to \rset$, where $\mathcal{F}$ is an appropriate function space, its functional derivative with reference measure $\mu$ is denoted $\nabla_\mu F$ and is given for any $\phi \in \mathcal{F}$, when it exists, by $$\label{eq:functional_derivative}
 \textstyle \lim_{\gamma \to 0} (F(f + \gamma \phi) - F(f)) / \gamma = \int \langle \nabla_\mu F(f) (x), \phi(x) \rangle \rmd \mu(x).$$ Initialised with $v_t^0(x) = (\mathbb{E}_{\hat{\Pbb}^0_{1|t}}[\bfX_1 \ | \ \bfX_t = x] - x)/(1-t)$, where $\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$, we now introduce a sequence of vector fields $(v^n)_{n \in \nset}$. This corresponds to training a bridge matching model (see e.g. ), giving $\Pbb_{v^0} = \projM(\hat{\Pbb}^0)$. Then for $n \in \nset$, let $$\label{eq:update_non_parametric}
    v^{n+1}_t(x) = v^n_t(x) - \delta_n \nabla_{\mu^n} \Lnonparam_t(v^n_t, \Pbb_{v^n})(x) ,$$ with $\delta_n > 0$ and $\mu^n \in \calP(\calC)$. The parameters $(\delta_n, \mu^n)_{n \in \nset}$ will be made explicit in . We emphasize that, in contrast to the IMF procedure, in the online update we do not need to solve a Markovian projection problem at every step; instead we simply take a gradient step on the loss .

#### Connection with $\alpha$-IMF. 

The following proposition shows that $(\Pbb_{v^n})_{n \in \nset}$ defined by is associated with $\alpha$-IMF defined in .

Combining to , we get that $\lim_{n \to +\infty} \Pbb_{v^n} = \Pbb^\star$, i.e. the non-parametric procedure converges to the SB.

# $\alpha$-Diffusion Schrödinger Bridge Matching

#### From DSBM to $\alpha$-DSBM.

In , we introduced $\alpha$-IMF, a scheme which defines a sequence of path measures converging to the SB for all $\alpha \in (0,1]$. For $\alpha=1$, this corresponds to the IMF, whose practical DSBM implementation requires repeatedly solving an expensive minimisation problem . In contrast, for $\alpha < 1$ we are only required to take one (non-parametric) gradient step to update the vector field, see . This suggests the following practical implementation of $\alpha$-IMF, called $\alpha$-DSBM: First, pretrain a bridge matching model so that for $t \in [0,1]$ and $x \in \rset^d$, $v_t^\theta(x) = (\mathbb{E}_{\hat{\Pbb}^0_{1|t}}[\bfX_1 \ | \ \bfX_t = x] - x)/(1-t)$, where $\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. Then, perform the parametric version of the update : $$\label{eq:update_alpha_imf}
    \theta \leftarrow \theta - \alpha \nabla_\theta \Lparam(\theta, \Pbb_{\bar{\theta}})  ; \  \Lparam(\theta, \Pbb) =  \int_0^1 \int_{(\rset^d)^3} \Big\| v_t^\theta(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb_{0,1}(x_0, x_1) \rmd \Qbb_{|0,1}(x_t|x_0,x_1) \rmd t ,$$ where $\Pbb_{\bar{\theta}}$ is a stop-gradient version of $\Pbb_{v^\theta}$. In , we give a theoretical justification for this parametric equivalent of and by showing that, as $\alpha \to 0$, the update on the velocity fields $v^\theta$ given by corresponds to a direction of descent for the non-parametric loss on average. Once again, we emphasize that if we replace the gradient step in with the minimisation $\theta \leftarrow \argmin_\theta \Lparam(\theta, \Pbb_{\bar{\theta}})$, we recover DSBM.

#### Bidirectional online procedure.

As with DSBM, directly implementing leads to error quickly accumulating, see for details. One way to circumvent this error accumulation issue is to consider a *bidirectional* procedure, in which we train both a forward and a backward model. This is possible because the Markovian projection coincides for forward and backward path measures, see . This suggests considering the loss $\Lnonparam(v^\vsra, v^\vsla, \Pbb^\vsra, \Pbb^\vsla)=\int^1_0 \Lnonparam_t(v^\vsra_t, v^\vsla_t, \Pbb^\vsra, \Pbb^\vsla)\rmd t$, which is an extension of , where $$\begin{aligned}
\label{eq:loss_function_bidirectional}
    \Lnonparam_t(v^\vsra_t, v^\vsla_t, \Pbb^\vsra, \Pbb^\vsla) &=   \int_{(\rset^d)^3} \Big\| v_t^\vsra(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb^\vsla_{0,1}(x_0, x_1) \rmd \Qbb_{t|0,1}(x_t|x_0,x_1)  \\
    &\qquad + \int_{(\rset^d)^3} \Big\| v_{1-t}^\vsla(x_t) - \frac{x_0 - x_t}{t} \Big\|^2 \rmd \Pbb^\vsra_{0,1}(x_0, x_1) \rmd \Qbb_{t|0,1}(x_t|x_0,x_1). 
\end{aligned}$$ Similarly to , we define $\Pbb_{v^\vsra}, \Pbb_{v^\vsla}$, associated with $(\bfX_t)_{t \in [0,1]}$ and $(\bfY_{1-t})_{t \in [0,1]}$ respectively, which are defined by forward and backward SDEs $$\label{eq:mixture_bridge_sde_forward_backward}
    \textrm{(fwd):} \  \rmd \bfX_t = v_t^\vsra(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \ \bfX_0 \sim \pi_0 , \; \textrm{(bwd):} \  \rmd \bfY_t = v_t^\vsla(\bfY_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \ \bfY_0 \sim \pi_1 .$$ Similarly to , we define non-parametric updates for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$ $$\label{eq:update_non_parametric_fb}
    (v^{n+1,\vsra}_t(x),v^{n+1, \vsla}_t(x))  = \left(v^{n,\vsra}_t(x),v^{n, \vsla}_t(x)\right) - \delta_n \nabla_{\mu^n} \Lnonparam_t\left(v^{n, \vsra}_t(x),v^{n, \vsla}_t(x), \Pbb_{v^{n, \vsra}}, \Pbb_{v^{n, \vsla}}\right)(x).$$ We have the following proposition which ensures our bidirectional procedure is still valid and that the results of still hold.

In , we show that in the Gaussian setting the bidirectional procedure does not accumulate error when the vector field is approximated, while the unidirectional one does.

#### Vector field parameterisation.

Contrary to existing procedures , we do not parameterise $v^\vsra$ and $v^\vsla$ using two separate networks. Instead, we consider an additional input $s \in \{0,1\}$ such that $v_\theta(1, \cdot) \approx v^\vsra$ and $v_\theta(0, \cdot) \approx v^\vsla$. This allows us to substantially reduce the number of parameters in the model. The conditioning on $s$ in the network is detailed in . Before stating our full algorithm in , we introduce a batched parametric version of . For ease of notation, we write $\mathrm{Interp}_t$ for the operation corresponding to sampling from $\Qbb_{t|0,1}$, i.e. $$\label{eq:interpolation_brownian_motion}
    \mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ) = (1-t)\bfX_0 + t \bfX_1 + \sqrt{\vareps(1-t)t} \bfZ .$$ We are now ready to introduce the batched parametric version of . For a given batch of inputs $\bfX_0^{1:B}$ and $\bfX_1^{1:B}$, timesteps $t \sim \mathrm{Unif}([0,1])^{\otimes B}$, and $\bfX_t =\mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ)$ with $\bfZ \sim \gN(0, \Id)^{\otimes B}$, we compute the empirical forward and backward losses as $$\begin{aligned}
\label{eq:empirical_loss}
   &\ell^{\vsra}(\theta; t, \bfX_1, \bfX_t)  = \frac{1}{B}\sum_{i=1}^B \| v_\theta\left(1, t^i, \bfX_t^i\right) - \left(\bfX_1^i - \bfX_t^i\right)/(1-t^i) \|^2,\\
    &\ell^{\vsla}(\theta; t, \bfX_0, \bfX_t)  = \frac{1}{B} \sum_{i=1}^B \Big\| v_\theta\left(0, 1 - t^i, \bfX_t^i\right) - \left(\bfX_0^i - \bfX_t^i\right)/t^i \Big\|^2 . 
\end{aligned}$$

We present the resulting $\alpha$-DSBM in . Note that in this algorithm, we maintain an Exponential Moving Average (EMA) of model parameters, as is common in diffusion models . During the finetuning stage, when we generate samples to use as model’s inputs, we then have a choice of sampling using the EMA or non-EMA parameters. At test time, we always sample using the EMA parameters, as it is known to improve the visual quality . In , we specify $\alpha \in (0,1]$ as a stepsize parameter. In practice, we use Adam  for optimization, thus the choice of $\alpha$ is implicit and adaptive throughout the training. We refer to for more details on our experimental setup.

# Related work

#### Solving Schrödinger Bridge problems.

Schrödinger Bridges have been thoroughly studied through the lens of probability theory and stochastic control . They recently found applications in generative modeling and related fields leveraging recent advances in diffusion models . Extensions of these methods to other machine learning problems and modalities were studied in . concurrently introduced the DSBM algorithm which relies on a new procedure called IMF, while the DSB algorithm introduced in is based on the standard Iterative Proportional Fitting (IPF) scheme. generalise DSBM to arbitrary cost functions, albeit at the expense of having to learn the reciprocal projection which is no longer given by a Brownian bridge. These new methodologies translate to improved numerics when compared to their IPF counterparts, but they remain reliant on alternating between the optimisation of two losses. Finally, we note that the Schrödinger Bridge flow and the $\alpha$-IMF procedure can be linked to the Sinkhorn flow recently introduced by , see for a detailed discussion.

#### Sampling-free methodologies.

Sampling-free methodologies have been proposed to solve OT related objectives. In , the authors perform one step of DSBM, i.e. only consider the pretraining stage of our algorithm. While the obtained bridge might enjoy transport properties, it does not solve an OT problem. In another line of work, have proposed simulation-free methods to minimise OT objectives. However, they target not the OT problem, but a minibatch version of it which coincides with OT only in the limit of infinite batch size, see . Other sampling-free methods to solve the Schrödinger Bridge problem include both of which rely on adversarial losses to solve the OT problem. In the adversarial objective is dropped and instead the procedure requires alternating objectives during training and is not sampling-free. We also highlight the line of work of in which the Schrödinger Bridge potentials are parameterised with mixtures of Gaussians, allowing for fast training in small dimensions. Finally, recently introduced a variation on Schrödinger Bridge for generative modeling, which while still not sampling-free, does not require learning a forward process.

# Experiments

In this section, we illustrate the efficiency of $\alpha$-DSBM on different tasks. In , we compare $\alpha$-DSBM to DSBM in a Gaussian setting where the EOT coupling is tractable and show that $\alpha$-DSBM recovers the solution faster than DSBM. In , we illustrate the scalability of our method through a range of unpaired image translation experiments.

## Gaussian case

We compare $\alpha$-DSBM to DSBM in the Gaussian setting where $\pi_0 = \gN(0, \sigma_0^2 \Id)$, $\pi_1 = \gN(0, \sigma_1^2 \Id)$ and $\Qbb$ is associated with $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ with $\sqrt \vareps = 0.5$. In this case, the EOT coupling is $\gN(0, \Sigma_\star)$, with $\Sigma_\star$ given by $$\Sigma_\star = \left( \begin{matrix} \sigma_0^2 \Id & \sigma_\star^2 \Id \\ \sigma_\star^2 \Id & \sigma_1^2 \Id \end{matrix} \right) ,~\textup{where~~}\sigma_\star^2 = (1/2)((\sigma_0^2 \sigma_1^2 + \vareps^2)^{1/2} - \vareps),$$ with $\Id$ being a $d \times d$ identity matrix. We consider $d=50$, $\sigma_0 = \sigma_1 =1$, resulting in $\sigma_\star^2 \approx 0.88$. To showcase the robustness of $\alpha$-DSBM, we consider the initial coupling $\Pbb_{0,1}$, where $(\bfX_0, \bfX_1) \sim \Pbb_{0,1}$, $\bfX_0 \sim \gN(0, \Id)$, $\bfX_1 = -\bfX_0$, and let $\hat{\Pbb}^0 = \Pbb_{0,1} \Qbb_{|0,1}$. In this setting, the base model, i.e. bridge matching, significantly underestimates the true covariance $\sigma_\star^2$, as shown in . Additionally, the figure illustrates that online finetuning approaches the true solution faster than the original iterative DSBM finetuning. For the latter, we can set how often we alternate between updating the forward and backward networks, and as this frequency increases, the behaviour approaches that of the online finetuning.

<figure id="fig:mnist_sigma_sweep_bars">

<figcaption><strong>Left</strong>: FID and Mean Squared Distance (MSD) on EMNIST to MNIST translation before and after finetuning with different values of <span class="math inline">\(\vareps\)</span>. <strong>Right</strong>: AFHQ-64 samples after the finetuning. For both, we use a bidirectional model with online finetuning. More results are in  and <span class="math inline">\(\ref{sec:experimental_details_afhq}\)</span>.</figcaption>
</figure>

## Image datasets

Similarly to , we apply our method to image translation problems, such as MNIST digits to EMNIST letters , and Wild to Cat domains from the Animal Faces-HQ (AFHQ) dataset , downsampled to 64 $\times$ 64 and 256 $\times$ 256 resolutions.

The whole training procedure can be framed as a two-stage process: first, we train a base model on the true data samples, performing bridge matching , and then we finetune this model. We compare models that combine different vector field parameterisations (two networks vs. one bidirectional net), finetuning methods (iterative vs. online), and sample generation strategies during the finetuning stage.

Following the established practice , we evaluate our models using FID for visual quality, and mean squared distance (MSD) or LPIPS   for alignment. It is important to note that for image translation tasks at hand, FID scores are not ideal, as FID was designed for natural RGB images, which is not the case for MNIST. It is also not well suited for small sample sizes as it is the case with AFHQ, where the test set in each domain has fewer than 500 examples. Thus quantitative results in Table should be interpreted cautiously, and we recommend a visual inspection of samples to complement these quantitative measures, especially for the AFHQ models. Samples from the models along with the training and evaluation protocols are given in Appendix .

Compared to the iterative DSBM, our online finetuning $\alpha$-DSBM reduces the number of tunable hyperparameters, i.e. inner and outer iterations, refresh rate and the size of the cache for storing generated samples. This simplifies implementation and makes the algorithm more practical. The primary remaining hyperparameter, the variance of a Brownian motion $\vareps$, requires careful tuning as it influences the trade-off between the visual quality and alignment, as was also observed in  . An appropriate $\vareps$ needs to balance the two: setting $\vareps$ too low results in poor visual quality, while high values of $\vareps$ cause poorly aligned and oversmoothed samples. illustrates how FID and MSD metrics vary with $\vareps$ for the case of MNIST. Additionally, it demonstrates the impact of $\vareps$ on the generated samples for the AFHQ-64 model.

<div id="table:mnist_afhq">

| Method                              | EMNIST $\rightarrow$ MNIST |       | AFHQ-64 Wild $\rightarrow$ Cat |       |
|:------------------------------------|:---------------------------|:------|:-------------------------------|:------|
| 2-3(lr)4-5                          | FID                        | MSD   | FID                            | LPIPS |
| DSBM\*                              | 10.59                      | 0.375 | –                              | –     |
| Pretrained two-networks model       | 6.02                       | 0.564 | 25.97                          | 0.589 |
| \(a\) iterative finetuning          | 5.25                       | 0.345 | 25.41                          | 0.485 |
| \(b\) online finetuning             | 4.28                       | 0.368 | 28.752                         | 0.487 |
| \(c\) online finetuning without EMA | 4.23                       | 0.361 | 32.665                         | 0.445 |
| Pretrained bidirectional model      | 6.33                       | 0.572 | 29.44                          | 0.584 |
| \(d\) online finetuning             | 4.39                       | 0.387 | 26.579                         | 0.482 |
| \(e\) online finetuning without EMA | 4.57                       | 0.369 | 30.638                         | 0.451 |

Results of image translation between EMNIST and MNIST, and AFHQ 64$\times$<!-- -->64 between Wild and Cat domains. DSBM\* results are from . Our reimplementation of DSBM corresponds to row (a). For MNIST and AFHQ models, we used $\vareps=1$ and $\vareps=0.75^2$, respectively. Each finetuning run was done with 5 random seeds, and we report mean scores ± standard deviation.

</div>

# Discussion

In this paper we have introduced $\alpha$-Diffusion Schrödinger Bridge Matching ($\alpha$-DSBM), a new methodology to solve Entropic Optimal Transport problems. $\alpha$-DSBM is an improved version of DSBM, which does not require training multiple DDM-type models. We have shown that a non-parametric version of this method recovers the Schrödinger Bridge (SB). In addition, $\alpha$-DSBM is easier to implement than existing SB methodologies while exhibiting similar performance. We illustrated the efficiency of our algorithm on a variety of unpaired transfer tasks.

While $\alpha$-DSBM solves one of the most critical limitations of DBSM, namely the alternative optimisation, several issues remain to be addressed in order for the method to scale comparably to generative DDMs. In particular, the method is not sampling-free, as during training it requires sampling from the model from the previous iteration to obtain the training data for the current iteration. While it seems difficult to derive a completely sampling-free method to solve SB problems without resorting to the Minibatch OT approximation, there is still room for improvement.

# Appendix organisation

The supplementary material is organised as follows. First in , we analyze an Euclidean counterpart to the $\alpha$-IMF sequence identified in and the associated flow. In , we show that the Markovian projection can be recovered as the parameterisation of the vector field that minimises the accumulation of errors, extending the results of . Theoretical results are gathered in . In particular in we show that the proposed non-parametric method coincides with the $\alpha$-IMF and prove the convergence of the $\alpha$-IMF. In , we show the connection between the non-parametric and the parametric updates. In , we provide more background on DSBM and propose an extension of the DSBM methodology. Consistency losses similar to are proposed in . Model stitching procedures are described in . We comment on extended related work in . In particular we draw connections with Sinkhorn flows , Reinforcement Learning policies, Expectation-Maximisation schemes following and comment on finetuning of diffusion models. In , we investigate the accumulation of bias in a Gaussian setting and compare forward-forward and forward-backward methods. In , we derive the preconditioning of loss following the principles of in the case of bridge matching. Additional results and experimental details are presented in .

# Euclidean flow and iterative procedure

In this section, we study a simplified counterpart of the Schrödinger flow and of DSBM in a Euclidean setting. The goal of this section is to draw some conclusions in the Euclidean case which also remain true empirically when analyzing the Schrödinger Bridge problem.

We consider the set $\mathsf{A}_1 = \ensembleLigne{(x,y) \in \rset^2}{y \ge x}$ and the set $\mathsf{A}_2 = \ensembleLigne{(x,y) \in \rset^2}{y \leq 0}$. Loosely speaking, one can identify $\mathsf{A}_1$ with the reciprocal class $\mathcal{R}(\mathbb{Q})$ and $\mathsf{A}_2$ with the set of Markov path measures $\projM$. In that case, we have that for any $(x,y) \in \rset^2$, $\mathrm{proj}_{\mathsf{A}_1}((x,y)) = ((x+y)/2,(x+y)/2)$ if $(x,y) \notin \mathsf{A}_1$ and otherwise, $\mathrm{proj}_{\mathsf{A}_1}((x,y)) = (x,y)$. In addition, we have that for any $(x,y) \in \rset^2$, $\mathrm{proj}_{\mathsf{A}_2}((x,y)) = (x,0)$ if $(x,y) \notin \mathsf{A}_2$ and $\mathrm{proj}_{\mathsf{A}_2}((x,y)) = (x,y)$ otherwise. We consider the following flow $(x_t,y_t)_{t \geq 0}$ given by $$\partial_t (x_t,y_t) = \mathrm{proj}_{\mathsf{A}_1}(\mathrm{proj}_{\mathsf{A}_2}((x_t,y_t))) - (x_t,y_t) .$$ Let $(x_0,y_0) \notin \mathsf{A}_1$ and $(x_0,y_0) \notin \mathsf{A}_2$. Denote $T$ the explosion time of $(x_t,y_t)$, i.e. for any $t \geq T$ we have that $(x_t,y_t) = \infty$, where $\rset^2 \cup \{ \infty \}$ is the one-point compactification of $\rset^2$. Finally, denote $\tau \leq T$ such that for any $t \in [0, \tau]$, $(x_t, y_t) \not \in \mathsf{A}_1$ and $(x_t, y_t) \not \in \mathsf{A}_2$. Then, we have $$\partial_t (x_t,y_t) = (-x_t/2, x_t/2 - y_t) .$$ Hence, we have that $x_t = x_0 \exp[-t/2]$ for any $t \in [0, \tau]$ and $y_t = x_0 \exp[-t/2] + (x_0 \exp[-t/2])^2 (y_0 - x_0)/x_0^2$. Therefore, we get that $\tau = T = +\infty$ and we have that for any $t \geq 0$ $$x_t = x_0 \exp[-t/2] , \qquad y_t = x_t +x_t^2 (y_0 - x_0)/x_0^2  .$$ Hence, $((x_t, y_t))_{t \geq 0}$ converges exponentially fast to $(0,0)$ with rate $1/2$.

We now investigate the rate of convergence of the alternate projection scheme, i.e. the Euclidean equivalent of DSBM. We define $((x_n, y_n))_{n \in \nset}$ such that for any $n \in \nset$, $$(x_{n+1}, y_{n+1}) = \mathrm{proj}_{\mathsf{A}_1}(\mathrm{proj}_{\mathsf{A}_2}((x_n,y_n))) = (x_n/2, 0) .$$ Hence, we get that $x_n = x_0 2^{-n}$ and therefore $((x_n, y_n))_{n \in \nset}$ converges exponentially fast to $(0,0)$. Note that this procedure corresponds to a discretisation of the flow $((x_t, y_t))_{n \in \nset}$ with stepsize $\alpha =1$.

More generally, we define for any $\alpha \in (0, 1]$, $((x_n^\alpha, y_n^\alpha))_{n \in \nset}$ such that for any $n \in \nset$, $$(x_{n+1}^\alpha, y_{n+1}^\alpha) = \alpha \mathrm{proj}_{\mathsf{A}_1}(\mathrm{proj}_{\mathsf{A}_2}((x_n^\alpha,y_n^\alpha))) + (1-\alpha)  (x_{n}^\alpha, y_{n}^\alpha) .$$ Hence, we get that $x_n = x_0 2^{-n}$ and therefore $((x_n, y_n))_{n \in \nset}$ converges exponentially fast to $(0,0)$. It can be shown that for any $n \in \nset$, $x_n^\alpha =x_0^\alpha (1 - \alpha /2)^n$ and in addition, we have that $$y_n^\alpha = (1-\alpha)^n y_0^\alpha + \alpha x_0^\alpha  \sum_{k=0}^{n-1} (1-\alpha)^k (1-\alpha/2)^{n-k} .$$ Therefore, we get that $$y_n = (1-\alpha)^{n}y_0^\alpha + 2 (1 - (1 - \alpha/(2-\alpha))^n)(1-\alpha/2)^n x_0^\alpha .$$

We now analyse the complexity of the different discretisations assuming that the cost of discretising the flow with stepsize $\alpha \in (0,1]$ is $C^\alpha$. In that case in order to reach the threshold value $\vareps$, i.e. $\abs{x_n^\alpha} \leq \vareps$, we get a total cost $C^\alpha_n = O(\log(1/\vareps)C^\alpha/\log(1/(1-\alpha/2)))$, where we have neglected the terms that do not depend on $\log(1/\vareps)$. Hence, if $C^\alpha$ is constant then the choice $\alpha = 1$ is the best possible one in the range $\alpha \in (0,1]$. Otherwise, one has to consider the ratio $C^\alpha / \log(1/(1-\alpha/2))$, where the lower is the better. The flow procedure and the iterative based one are presented in .

Based on this simplified Euclidean experiment, we draw some conclusions which also remain true in our setting, see for more experimental details. First, we have that different discretisations of the flow yield different convergence rates. Large stepsizes incur faster convergence. This suggests to choose $\alpha =1$. However, if the cost of choosing $\alpha =1$ is too high then one might turn to alternative schemes with $\alpha \in (0,1)$ assuming that $C^\alpha < C^1$ in that case. To draw a parallel with our setting, in the case of DSBM (case $\alpha =1$), we need to solve the projection subproblem at each step which incurs a great cost. On the other hand, one step of the online algorithm only requires sampling once from the model and performing one gradient step.

# Minimisation of errors and Markovian projection

For a given non-Markovian (stochastic) interpolant process (see definition below), there exist an infinite number of Markov processes admitting the same marginals . In this section, when it is well-defined, we show that the Markovian projection corresponds to the process which minimises an error measure (defined further) in case one has access to the oracle of $x_t \mapsto \mathbb{E}[\bfX_1 \ | \ \bfX_t = x_t]$.

#### Stochastic Interpolant.

We first start by recalling the framework of . Consider a coupling $\Pi$ between $\pi_0$ and $\pi_1$, one builds a (stochastic) flow between $\pi_0$ and $\pi_1$ using the following interpolation procedure $$\label{eq:interpolant}
    \bfX_t = \mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ) = \alpha_t \bfX_0 + \beta_t \bfX_1 + \gamma_t \bfZ , \quad (\bfX_0, \bfX_1) \sim \Pi , \quad \bfZ \sim \gN(0, \Id) ,$$ where $\alpha_1  = \beta_0 = \gamma_0 = \gamma_1 = 0$ and $\alpha_0 = \beta_1 = 1$. This defines a non-Markovian process. We denote by $\pi_t$ the induced unconditional distribution of $\bfX_t$. Let us now consider the Markov process $(\bfX_t^\vareps)$ given by $$\label{eq:stochastic_flow}
    \rmd \bfX_t^\vareps = \mathbb{E}\left[\dot{\alpha}_t \bfX_0 + \dot{\beta}_t \bfX_1 + (\dot{\gamma}_t - \vareps_t^2 /(2\gamma_t)) \bfZ \ | \ \bfX_t = \bfX_t^\vareps\right] + \vareps_t \rmd \bfB_t , \qquad \bfX_0^\vareps \sim \pi_0,$$ where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion and $\vareps_t$ is an additional hyperparameter. It can then be shown that $(\bfX_t^\vareps)_{t \in [0,1]}$ satisfies that $\bfX_t^\vareps \sim \pi_t$ for all $t\in[0,1]$; see e.g. . Hence $(\bfX_t^\vareps)_{t \in [0,1]}$ is a (stochastic) flow mapping $\pi_0$ onto $\pi_1$. Note that $(\bfX_t^\vareps)_{t \in [0,1]}$ in can be rewritten as $$\label{eq:stochastic_flow_rewrite}
    \rmd \bfX_t^\vareps = (\dot{\alpha}_t/\alpha_t) \bfX_t^\vareps + \mathbb{E}\left[(\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \bfX_1 + (\dot{\gamma}_t - \gamma_t \dot{\alpha}_t/\alpha_t -  \vareps_t^2 /(2\gamma_t)) \bfZ \ | \ \bfX_t = \bfX_t^\vareps\right] + \vareps_t \rmd \bfB_t.$$

In the specific case where $\alpha_t = 1-t$, $\beta_t = t$ and $\gamma_t = \sigma_0 \sqrt{t(1-t)}$ then becomes $$\label{eq:interpolation_brownian_motion_appendix}
    \bfX_t = \mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ) = (1-t) \bfX_0 + t \bfX_1 + \sigma_0 \sqrt{t(1-t)} \bfZ.$$ This corresponds to the marginal distribution of the bridge associated with $(\sigma_0 \bfB_t)_{t \in [0,1]}$. In this case, becomes $$\label{eq:markov_projection}
    \rmd \bfX_t^\vareps = \mathbb{E}\left[(\bfX_1 - \bfX_t)/(1-t) | \bfX_t = \bfX_t^\vareps\right] \rmd t + \sqrt{2} \sigma_0 \rmd \bfB_t$$ for $\vareps_t^2 = (2\gamma_t)(\dot{\gamma}_t - \gamma_t \dot{\alpha}_t / \alpha_t)=2\sigma^2_0$. In , we will show that this choice of $(\vareps_t)_{t \in [0,1]}$ is optimal in some sense.

Consider $(\hat{\bfX}_t^\vareps)_{t \in [0,1]}$ given by $$\label{eq:stochastic_flow_rewrite_approximate}
        \rmd \hat{\bfX}_t^\vareps = (\dot{\alpha}_t/\alpha_t) \hat{\bfX}_t^\vareps + (\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \mathbb{E}[\bfX_1 \ | \ \bfX_t = \hat{\bfX}_t^\vareps] + (\dot{\gamma}_t - \gamma_t \dot{\alpha}_t/\alpha_t -  \vareps_t^2 /(2\gamma_t)) \hat{\bfZ}(t, \hat{\bfX}_t^\vareps)  + \vareps_t \rmd \bfB_t ,$$ with $\hat{\bfX}_0^\vareps \sim \pi_0$ and where $\hat{\bfZ}(t, x)$ is an approximation of $\mathbb{E}[ \bfZ | \bfX_t = x]$. We have the following result.

<div class="proof">

*Proof.* We have that for any $$\KL(\Pbb^\vareps | \hat{\Pbb}^\vareps) = \int_0^1 \frac{1}{\vareps_t^2}(\dot{\gamma}_t - \gamma_t \dot{\alpha}_t/\alpha_t -  \vareps_t^2 /(2\gamma_t))^2 \mathbb{E}[\Delta_t] \rmd t,$$ where $\Delta_t = \| \hat{\bfZ}(t, \bfX_t^\vareps) - \mathbb{E}[\bfZ \ | \ \bfX_t = \bfX_t^\vareps]\|^2$ and the expectation is w.r.t. $\Pbb^\vareps$. We have that $\KL(\Pbb^\vareps | \hat{\Pbb}^\vareps) =0$ if $\vareps = \vareps^\star$, which concludes the proof. ◻

</div>

is related to . Therein it is noticed that, in the case of Augmented Bridge matching , the choice of $\vareps_t$ does not affect the joint distribution of $(\bfX_0^\vareps, \bfX_1^\vareps)$. The authors then select $(\vareps_t)$ so as to minimise an approximation error. They show that, in that case, they recover the Föllmer process.

We now show that can be further strengthened to establish that $\vareps^\star$ is also the optimal value if we interpolate between $\pi_s$ and $\pi_1$, or $\pi_0$ and $\pi_s$, for any $s \in [0,1]$ and $\pi_s$ the distribution of $\bfX_s$. Consider in this context for any $s, t \in [0,1]$ with $t \geq s$, $\gamma_t / \gamma_s \geq \alpha_t \geq \alpha_s$ the following interpolation model. $$\label{eq:interpolation_with_s}
    \bfX_t = (\alpha_t/\alpha_s) \bfX_s + (\beta_t - \alpha_t \beta_s /\alpha_s) \bfX_1 + \sqrt{\gamma^2_t - \alpha_t^2 \gamma^2_s / \alpha_s^2} \bfZ ,$$ where $\bfX_s \sim \pi_s$, $\bfX_1 \sim \pi_1$ and $\bfZ \sim \gN(0, \Id)$. Assume that $\alpha_t = 1-t$, $\beta_t = t$ and $\gamma_t = \sigma_0 \sqrt{t (1-t)}$ for any $t \in [0,1]$ then corresponds to the Brownian bridge associated with $(\sigma_0 \bfB_t)_{t \in [0,1]}$ with endpoints $\bfX_s$ at time $s$ and $\bfX_1$ at time $1$. We have the following proposition.

<div class="proof">

*Proof.* We let $s \in [0,1]$ and $\bfX_1, \bfX_s \in \rset^d$. From , we have directly that for any $t \in [s,1]$ $$\rmd \bfX_t = [(\dot{\alpha}_t / \alpha_s) \bfX_s + (\dot{\beta}_t - \dot{\alpha}_t \beta_s / \alpha_s) \bfX_1 + \dot{\gamma}_{t,s} \bfZ] \rmd t ,$$ where $\bfZ \sim \gN(0, \Id)$. In addition, rearranging , we also have that $$\bfX_s = (\alpha_s / \alpha_t) \bfX_t - (\alpha_s \beta_t / \alpha_t - \beta_s) \bfX_1 - \gamma_{t,s} (\alpha_s / \alpha_t) \bfZ .$$ Hence, by combining these two expressions, we get that $$\rmd \bfX_t = [(\dot{\alpha}_t / \alpha_t) \bfX_t + (\dot{\beta}_t - \dot{\alpha}_t \beta_t / \alpha_s) \bfX_1 + (\dot{\gamma}_{t,s} - \gamma_{t,s}(\dot{\alpha}_t / \alpha_t)) \bfZ] \rmd t .$$ It follows that $(\bfX_{t,s})_{t \in [s,1]}$ given by $$\begin{aligned}
\label{eq:stochastic_flow_determistic_rewrite_appendix}
    \rmd \bfX_{t,s} &= (\dot{\alpha}_t/\alpha_t) \bfX_{t,s} + (\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \mathbb{E}[ \bfX_1 \ | \ \bfX_t = \bfX_{t,s}] \\
    & \qquad + (\dot{\gamma}_{t,s} - \gamma_{t,s} \dot{\alpha}_t/\alpha_t) \mathbb{E}[ \bfZ \ | \ \bfX_t = \bfX_{t,s}]  , \quad \bfX_{s,s} \sim \pi_s ,
\end{aligned}$$ is such that for any $t\in [s,1]$ the same distribution as $\bfX_t$ defined by . Then, we conclude similarly to . ◻

</div>

We now consider the following approximate version of $$\begin{aligned}
    \rmd \hat{\bfX}_{t,s}^\vareps &= (\dot{\alpha}_t/\alpha_t) \hat{\bfX}_{t,s}^\vareps + (\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \mathbb{E}\left[\bfX_1 \ | \ \bfX_t = \hat{\bfX}_{t,s}^\vareps\right] \\
    &\qquad + (\dot{\gamma}_{t,s} - \gamma_{t,s} \dot{\alpha}_t/\alpha_t -  \vareps_{t,s}^2 /(2\gamma_{t,s})) \hat{\bfZ}(t, \bfX_{t,s}^\vareps) + \vareps_{t,s} \rmd \bfB_t , \quad \bfX_{s,s}^\vareps \sim \pi_s ,
\label{eq:stochastic_flow_rewrite_appendix_approximate}
\end{aligned}$$ Similarly to we consider the best choice of $\vareps$ to minimise the interpolation cost.

<div class="proof">

*Proof.* Similarly to , we get first that for any $s, t \in [0,1]$ with $s \leq t$ $$\label{eq:uno_interp_appendix}
    \vareps_{t,s}^\star = 2 \gamma_{t,s} \dot{\gamma}_{t,s} - 2 \gamma_{t,s}^2 \dot{\alpha}_t/\alpha_t .$$ Second, we have that for any $s, t \in [0,1]$ with $s \leq t$ $$\label{eq:duo_interp_appendix}
    2 \dot{\gamma}_{t,s} \gamma_{t,s} = \dot{\gamma}^2_{t,s}  = 2 \dot{\gamma}_t \gamma_t - 2 \dot{\alpha}_t \alpha_t \gamma_s^2 / \alpha_s^2 .$$ Third, we have that $$\label{eq:tertio_interp_appendix}
    \gamma_{t,s}^2 \dot{\alpha}_t / \alpha_t = \gamma_t^2 \dot{\alpha}_t / \alpha_t - \dot{\alpha}_t \alpha_t \gamma_s^2 / \alpha_s^2 .$$ Combining , and , we can conclude. ◻

</div>

# Theoretical results

In this section, we prove the main theoretical results of the paper. In , we first prove the convergence of the $\alpha$-IMF sequence, i.e. we prove . Second, we show that the non-parametric updates correspond to the $\alpha$-IMF sequence, i.e. we prove . In , we link the non-parametric updates to the parametric updates.

## Non-parametric sequence and convergence

Let $\Qbb \in \mathcal{P}(\mathcal{C})$ be associated with $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$, where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion and $\vareps > 0$. In this section, we abuse notation and denote $\mathcal{P}(\mathcal{C})$ the set of *path measures* which are not necessarily *probability* path measures. In particular, we will consider $\Qbb \in \mathcal{P}(\mathcal{C})$ associated with $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ with $\Qbb_0 = \Leb$. In that case, the Kullback–Leibler divergence is still well-defined and we refer to for more details. We recall that we have defined $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ for any $n \in \nset$ and $\alpha \in (0,1]$ by $$\label{eq:discretisation_flow_path_measure_repeat}
        \Pbb^n = \projM(\hat{\Pbb}^n) , \qquad \hat{\Pbb}^{n+1} = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\projM(\hat{\Pbb}^n)) .$$ In addition, for any $n \in \nset$, $t \in [0,1)$ and $x \in \rset^d$ we have defined $$\label{eq:update_non_parametric_appendix_2}
    v^{n+1}_t(x) = v^n_t(x) - \delta_n \nabla_{\mu^n} \Lnonparam_t(v^n_t, \Pbb_{v^n})(x) ,$$ where $$\begin{aligned}
\label{eq:locallosst}
    \Lnonparam_t(v_t, \Pbb)&=\frac{1}{2} \int_{(\rset^d)^3} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t | x_0, x_1) \\
    &=\frac{1}{2}  \int_{(\rset^d)^3} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \projsimpleR(\Pbb)_{1,t}.
\end{aligned}$$ We define $(\Pbb_{v^n})_{n \in \nset}$ associated with , where for any suitable vector field $v$, $\Pbb_v$ is associated with $$\rmd \bfX_t = v_t(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t ,$$ where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion.

In order to rigorously prove detailed further, we introduce $\calP_2(\calC)$, such that $\Pbb \in \calP_2(\calC)$ if $\Pbb \in \calP(\calC)$ and for $$\int_{(\rset^d)^2} \{ \| x_0 \|^2 + \|x_1 \|^2 \} \rmd \Pbb_{0,1}(x_0, x_1) < +\infty .$$ Note that if $\Pbb \in \calP_2(\calC)$ then we have that for any $t \in [0,1]$ $$\int_{\rset^d} \| x_t \|^2 \rmd \projsimpleR(\Pbb)_t < +\infty .$$ In addition, we recall that $\phi \in \mathrm{L}^2(\mu)$ for $\mu \in \calP(\rset^d)$ if $\phi: \ \rset^d \to \rset^d$ and $$\int_{\rset^d} \| \phi(x) \|^2 \rmd \mu(x) < +\infty .$$ Finally, we define $$\mathsf{A}_2 = \ensembleLigne{(\phi, \Pbb)}{\Pbb \in \calP_2(\calC), \ \phi \in \mathrm{L}^2(\Pbb)} .$$ Then for any $t \in [0,1)$, we define $\Lnonparam_t : \ \mathsf{A}_2 \to \rset$ given for any $(v, \Pbb) \in \mathsf{A}_2$ by .

<div class="proof">

*Proof.* First, we have that for any $t \in [0,1)$, $v, \Pbb \in \mathsf{A}_2$ and $\phi \in \mathrm{L}^2(\Pbb_t)$ we have $$\begin{aligned}
     \Lnonparam_t(v_t + \vareps \phi,\Pbb) &= \Lnonparam_t(v_t,\Pbb) + \vareps \int_{(\rset^d)^3} \langle \phi(x_t), v_t(x_t) - \tfrac{x_1 - x_1}{1-t}\rangle \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t|x_0, x_1) \\
     &  \qquad + (\vareps^2/2) \int_{(\rset^d)^3} \| \phi(x_t) \|^2 \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t|x_0, x_1) \\ 
     &= \Lnonparam_t(v_t,\Pbb) + \vareps \int_{(\rset^d)^2} \langle \phi(x_t), v_t(x_t) \\
     & \quad - \Bigl(\int_{\rset^d} x_1 \rmd \projsimpleR(\Pbb)_{1|t}(x_1 | x_t) - x_t\Bigr)/(1-t) \rangle \rmd \projsimpleR(\Pbb)_t(x_t) \\
     &  \qquad + (\vareps^2/2) \int_{\rset^d} \| \phi(x_t) \|^2 \projsimpleR(\Pbb)_t(x_t) . 
 
\end{aligned}$$ Hence, we have that $$\label{eq:functional_derivative_loss}
    \nabla_{\mu} \Lnonparam_t(v_t,\Pbb_v)(x_t) = (v_t(x_t) - \left(\mathbb{E}_{\projsimpleR(\Pbb)}[ \bfX_1 \ | \ \bfX_t = x_t] - x_t\right)/(1-t)) (\rmd \projsimpleR(\Pbb_v)_t / \rmd \mu_t)(x_t) .$$ Assume that for some $n \in \nset$ we have that for any $t \in [0,1)$ and $x_t \in \rset^d$, we have $v^k_t(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^k}[\bfX_1 \ | \ \bfX_t = x_t] - x_t\right) / (1-t)$. We are going to show that for any $t \in [0,1)$ and $x_t \in \rset^d$, we have $v^{n+1}_t(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{n+1}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t\right) / (1-t)$. For any $t \in [0,1)$ and $x_t \in \rset^d$, we denote $$\bar{\delta}^n_t(x_t) = \delta_n (\rmd \projsimpleR(\Pbb^n)_t / \rmd \mu_t^n)(x_t) .$$ Since we have that $\delta_n = \alpha$ and $\mu^n = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\Pbb^n)$, we obtain for any $t \in [0,1]$ and $x_t \in \rset^d$ $$\label{eq:delta_bar_simplif}
    \bar{\delta}^n_t(x_t) = \alpha (\rmd \projsimpleR(\Pbb^n)_t / \rmd ( (1-\alpha) \hat{\Pbb}^n_t + \alpha \projsimpleR(\Pbb^n)_t)(x_t),$$ so that $$\label{eq:one_minus_delta_bar_simplif}
    1 - \bar{\delta}^n_t(x_t) = (1-\alpha) (\rmd \hat{\Pbb}^n_t /  \rmd ( (1-\alpha) \hat{\Pbb}^n_t + \alpha \projsimpleR(\Pbb^n)_t)(x_t) .$$ Therefore, combining with , , , we get that for any $t \in [0,1)$ and $x_t \in \rset^d$ $$\begin{aligned}
    v_t^{n+1}(x_t) & =(1 - \bar{\delta}_t^n(x_t))v_t^{n}(x_t) \\
    & \qquad + \bar{\delta}^n_t(x_t) \left(\mathbb{E}_{\projsimpleR(\Pbb^n)}[\bfX_1 \ | \ \bfX_t=x_t] - x_t\right)/(1-t) \\
    &=(1 - \bar{\delta}_t^n(x_t)) \left(\mathbb{E}_{\hat{\Pbb}^n}[\bfX_1 \ | \ \bfX_t=x_t] - x_t\right)/(1-t) \\
    & \qquad + \bar{\delta}^n_t(x_t) \left(\mathbb{E}_{\projsimpleR(\Pbb^n)}[\bfX_1 \ | \ \bfX_t=x_t] - x_t\right)/(1-t) \\
    &=  (1 - \bar{\delta}_t^n(x_t)) \left(\int_{\rset^d} x_1 \rmd \hat{\Pbb}^n_{1|t}(x_1 | x_t) - x_t\right) / (1-t) \\
    & \qquad + \bar{\delta}^n_t(x_t) \left(\int_{\rset^d} x_1 \rmd \projsimpleR(\Pbb^n)_{1|t}(x_1 | x_t) - x_t\right) / (1-t) \\
    &=  \int_{\rset^d} (x_1 - x_t) / (1-t) \rmd [ (1 - \bar{\delta}_t^n(x_t)) \hat{\Pbb}^n_{1|t}+ \bar{\delta}_t^n(x_t) \projsimpleR(\Pbb^n)_{1|t}] (x_1 | x_t) \\
    &=  \int_{\rset^d} x_1 \rmd [ (1-\alpha) \hat{\Pbb}^n+ \alpha \projsimpleR(\Pbb^n)]_{1|t} (x_1 | x_t) . 
\end{aligned}$$ Hence, we have that for any $t \in [0,1)$ and $x_t \in \rset^d$, $v_t^{n+1}(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{n+1}}[\bfX_1  \ | \ \bfX_t = x_t] - x_t\right)/(1-t)$. Since, for any $t \in [0,1)$ and $x_t \in \rset^d$, $v_t^0(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{0}}[\bfX_1  \ | \ \bfX_t = x_t] - x_t\right / (1-t)$ by definition, we get that for any $n \in \nset$, $t \in [0,1)$ and $x_t \in \rset^d$, $v_t^{n}(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{n}}[\bfX_1  \ | \ \bfX_t = x_t] - x_t\right) / (1-t)$. Using, , we get that $\Pbb_{v^n} = \projM(\hat{\Pbb}^n)$, which concludes the proof. ◻

</div>

Before stating our convergence theorem, we show the following result which is a direct consequence of   and . We recall that the differential entropy of a probability measure $\pi$ is given by $$\mathrm{H}(\pi) =- \int_{\rset^d} \log((\rmd \pi / \rmd \mathrm{Leb})(x)) \rmd \pi(x) ,$$ if $\pi$ admits a density with respect to the Lebesgue measure and $+\infty$ otherwise.

<div class="proof">

*Proof.* First, we have that $\Qbb_{0,1}$ is equivalent to $\Leb \otimes \Leb$. Indeed, we have that for any $x_0, x_1 \in \rset^d$ $$(\rmd \Qbb_{0,1} / \rmd (\Leb \otimes \Leb))(x_0,x_1) =(2 \uppi \vareps)^{-d/2} \exp[-\| x_0 - x_1 \|^2 / (2 \vareps) ] .$$ Similarly, we have that for any $t \in (0,1)$ and $x_t \in \rset^d$, $\Qbb_{0,1|t}(\cdot|x_t)$ is equivalent to $\Leb \otimes \Leb$. Indeed, we have that for any $t \in (0,1)$ and $x_0, x_t, x_1 \in \rset^d$ $$\begin{aligned}
    (\rmd \Qbb_{0,1|t}(\cdot|x_t) / \rmd (\Leb \otimes \Leb))(x_0,x_1) &= (2 \uppi \vareps t)^{-d/2}  \exp[-\| x_0 - x_t \|^2 / (2 \vareps t)] \\
    & \qquad \times (2 \uppi \vareps(1-t))^{-d/2} \exp[- \| x_t -  x_1 \|^2 / (2 \vareps(1-t)) ] . 
\end{aligned}$$ Hence, for any $t \in (0,1)$ and $x_t \in \rset^d$, $\Qbb_{0,1|t}(\cdot|x_t)$ is equivalent to $\Qbb_{0,1}$. Since $\Pbb^\star$ is Markov and $\Pbb^\star \in \mathcal{R}(\Qbb)$ we get that there exist $\varphi^\circ_0$ and $\varphi^\star_1$ which are Lebesgue measurable such that for any $\omega \in \calC$ we have that $$\label{eq:existence_potentials}
    (\rmd \Pbb^\star / \rmd \Qbb)(\omega) = \varphi_0^\circ(\omega_0) \varphi_1^\star(\omega_1) .$$ Second we verify that the conditions (i)-(vii) of are satisfied. First, $\Qbb$ is Markov and hence (i) is satisfied. Then, (ii) is satisfied since for any $t \in (0,1)$ and $x_t \in \rset^d$, $\Qbb_{0,1|t}(\cdot|x_t)$ is equivalent to $\Qbb_{0,1}$. We have that $\Qbb_0 = \Qbb_1 = \Leb$ and (iii) is satisfied. We have that for any $x_0, x_1 \in \rset^d$ $$\begin{aligned}
    (\rmd \Qbb_{0,1} / \rmd (\Leb \otimes \Leb))(x_0,x_1) &= (2 \uppi \vareps)^{-d/2} \exp[-\| x_0 - x_1 \|^2 / (2 \vareps) ]\\
    &\geq (2 \uppi \vareps)^{-d/2} \exp[-\| x_0 \|^2/\vareps -\| x_1 \|^2/\vareps]. 
\end{aligned}$$ Hence, (iv) is satisfied and we let $A: \ \rset^d \to \rset_+$ be given for any $x \in \rset^d$ by $A(x) = \| x \|^2 / \vareps$. In addition, we have that for any $x_0,x_1 \in \rset^d$ $$\int_{(\rset^d)^2} \exp[-\| x_0 \|^2 / \vareps - \| x_1 \|^2 / \vareps] \rmd \Qbb_{0,1}(x_0,x_1) < +\infty .$$ Hence, (v) is satisfied and we let $B: \ \rset^d \to \rset_+$ given for any $x \in \rset^d$ by $B(x) = \| x \|^2 / \vareps$. By assumption (vi) and (vii) are satisfied. We conclude the proof upon using and . ◻

</div>

We are now ready to state our main convergence result.

<div class="proof">

*Proof.* Using the convexity of the Kullback–Leibler divergence with respect to its first argument (see e.g. ), the data processing inequality (see e.g. ), the fact that the Schrödinger Bridge is Markov and in the reciprocal class of $\Qbb$ (see e.g.  and ), and the Pythagorean theorem for the Markovian projection , we have that for any $n \in \nset$ $$\begin{aligned}
    \KL(\hat{\Pbb}^{n+1} | \Pbb^\star) &= \KL((1-\alpha) \hat{\Pbb}^{n} + \alpha \projsimpleR(\projM(\hat{\Pbb}^n)) | \Pbb^\star) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \projsimpleR(\projM(\hat{\Pbb}^n)) | \Pbb^\star) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \projM(\hat{\Pbb}^n)_{0,1} | \Pbb^\star_{0,1}) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \projM(\hat{\Pbb}^n) | \Pbb^\star) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \hat{\Pbb}^n | \Pbb^\star) - \alpha \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) .
    \label{eq:kl_inequality_phat}
\end{aligned}$$ Therefore, we get that $$\alpha \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) \leq \KL( \hat{\Pbb}^{n} | \Pbb^\star) - \KL(\hat{\Pbb}^{n+1} | \Pbb^\star) .$$ Hence, it follows that $$\sum_{n \in \nset} \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) \leq 2 \KL(\hat{\Pbb}^0 | \Pbb^\star) < +\infty .$$ So we obtain $\lim_{n \to +\infty} \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) = 0$. In addition, using we have that $\KL(\hat{\Pbb}^n | \Pbb^\star) \leq \KL(\hat{\Pbb}^0 | \Pbb^\star) < +\infty$ for all $n \in \nset$. Using , we also get that $\KL(\projM(\hat{\Pbb}^n) | \Pbb^\star) \leq \KL(\hat{\Pbb}^0 | \Pbb^\star) < +\infty$ for any $n \in \nset$. Hence both the sequences $(\hat{\Pbb}^n)_{n \in \nset}$ and $(\Pbb^n)_{n \in \nset} = (\projM(\hat{\Pbb}^n))_{n \in \nset}$ are relatively compact in $\mathcal{P}(\mathcal{C})$. Let $\bar{\Pbb} \in \calP(\calC)$ be an adherent point to the sequence $(\hat{\Pbb}^n)_{n \in \nset}$ and $\varphi: \ \nset \to \nset$ increasing such that $\lim_{n \to +\infty} \Pbb^{\varphi(n)} = \bar{\Pbb}$. Similarly, let $\phi: \ \nset \to \nset$ increasing such that $(\phi(n))_{n \in \nset}$ is a subsequence of $(\varphi(n))_{n \in \nset}$ such that $\lim_{n \to +\infty} \projM(\hat{\Pbb}^n) = \bar{\Pbb}'$, with $\bar{\Pbb}'$ and adherent point to the sequence $(\projM(\hat{\Pbb}^n))_{n \in \nset}$. Using the lower semi-continuity of the Kullback-Leibler divergence in both arguments , we get that $$\KL(\bar{\Pbb} | \bar{\Pbb}') \leq \liminf_{n \to + \infty} \KL(\hat{\Pbb}^{\phi(n)} | \projM(\hat{\Pbb}^{\phi(n)})) = 0 .$$ Since the set of Markov measures and the set of reciprocal measures w.r.t. $\Qbb$ are both closed, we have that $\bar{\Pbb}$ is Markov and in the reciprocal class of $\Qbb$. Since we also have that $\bar{\Pbb}_0 = \pi_0$ and $\bar{\Pbb}_1 = \pi_1$, we get that $\bar{\Pbb} = \Pbb^\star$ using . Since every adherent point of $(\hat{\Pbb}^n)_{n \in \nset}$ is $\Pbb^\star$, we have that $\lim_{n \to +\infty} \hat{\Pbb}^n = \Pbb^\star$. Similarly, using that $\lim_{n \to +\infty} \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) = 0$ and again the lower semi-continuity of the Kullback–Leibler divergence in both arguments, we get that every adherent point of $(\projM(\hat{\Pbb}^n))_{n \in \nset}$ is $\Pbb^\star$. Hence, we have that $\lim_{n \to +\infty} \Pbb^n = \Pbb^\star$, which concludes the proof. ◻

</div>

## From parametric to non-parametric.

In this section, we show that the parametric updates considered in are a preconditioned version of the non-parametric updates considered in . We first recall the non-parametric loss $$\label{eq:loss_function_appendix}
    \Lnonparam(v, \Pbb) = \int_0^1 \Lnonparam_t(v_t, \Pbb) \rmd t = \frac{1}{2} \int_0^1 \int_{(\rset^d)^2}\Big\|  v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\| ^2 \rmd \projsimpleR(\Pbb)_{t,1}(x_t,x_1) \rmd t$$ and the parametric loss $$\label{eq:update_alpha_imf_appendix}
     \Lparam(\theta, \Pbb) = \frac{1}{2} \int_0^1 \int_{\rset^d \times \rset^d} \Big\|  v_t^\theta(x_t) - \frac{x_1 - x_t}{1 - t} \Big\| ^2 \rmd \projR{\Qbb}(\Pbb)_{t,1}(x_t, x_1) \rmd t.$$ The non-parametric sequence $(v^n)_{n \in \nset}$ is given by , i.e. we have for any $n \in \nset$, $t  \in [0,1]$ and $x \in \rset^d$ $$\label{eq:update_non_parametric_appendix}
    v^{n+1}_t(x) = v^n_t(x) - \delta_n \nabla_{\mu^n} \Lnonparam_t(v^n_t, \Pbb_{v^n})(x).$$ Similarly the sequence of parametric updates is given for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$ by $$\theta_{n+1} = \theta_n - \alpha \nabla_\theta \Lparam(\theta_n, \Pbb^{\bar{\theta}_n}) .$$ We recall that $\Pbb^{\bar{\theta}_n}$ is a stop gradient version of $\Pbb_{v^{\bar{\theta}_n}}$. We are going to show that on average the parametric algorithm yields a direction of descent for the non-parametric loss. We assume that the set of parameters $\Theta$ is an open subset of $\rset^p$ for some $p \in \nset$. For any $t \in [0,1]$ and $x \in \rset^d$ we assume that $\theta \mapsto v^\theta_t(x)$ is twice continuously differentiable and denote $\mathrm{D}_\theta v^\theta_t(x) \in \rset^{d \times p}$ its Jacobian and $\mathrm{D}_\theta^2 v^\theta_t(x)$ its Hessian. For any $\theta \in \Theta$, we denote $$h_\theta = \nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}}) .$$ We show the following result.

<div class="proof">

*Proof.* First, we have that for any $\mu \in \calP(\calC)$ $$\nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}}) = \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top (v_s^\theta(x_s) - (x_1 - x_s)/(1-s)) \rmd \projsimpleR(\Pbb^{\bar{\theta}})_{s, 1}(x_s, x_1) \rmd s.$$ Therefore, using , we get that $$\nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}}) = \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top  \nabla_\mu \Lnonparam_s(v_s^\theta, \Pbb^{\bar{\theta}})(x_s) \rmd \mu_s(x_s) \rmd s .$$ Let $\theta \in \Theta$ and denote $\theta' = \theta - \alpha \nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}})$. Using a Taylor expansion, we get that for any $\theta \in \Theta$, we have that $$\begin{aligned}
    v_t^{\theta'}(x) &= v_t^\theta(x) - \alpha \rmD_\theta v_t^\theta(x) \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top  \nabla_\mu \Lnonparam_s(v_s^\theta, \Pbb^{\bar{\theta}})(x_s) \rmd \mu_s(x_s) \rmd s \\
    & \qquad \qquad + \alpha^2  \int_0^1 (1-s)  \mathrm{D}_\theta^2 v_s^{\theta - \alpha s h_\theta}(x) (h_\theta, h_\theta) \rmd s.  
\end{aligned}$$ Since the functional gradient is not applied on the second coordinate, we can drop the stop gradient operator and therefore we have for any $\theta \in \Theta$ $$\begin{aligned}
    v_t^{\theta'}(x) &= v_t^\theta(x) - \alpha \rmD_\theta v_t^\theta(x) \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top  \nabla_\mu \Lnonparam_s(v_s^\theta, \Pbb_{v^\theta})(x_s) \rmd \mu_s(x_s) \rmd s \\
    & \qquad \qquad + \alpha^2  \int_0^1 (1-s)  \mathrm{D}_\theta^2 v_s^{\theta - \alpha s h_\theta}(x) (h_\theta, h_\theta) \rmd s . 
\end{aligned}$$ Combining this result with , we conclude the proof. ◻

</div>

The corresponding update on the velocity field is given for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$ by $$d^n_t(x) = -\alpha \mathrm{D}_\theta v_t^{\theta_n}(x) \int_0^1 \int_{\rset^d} \mathrm{D}_\theta v_{s}^\theta(\tilde{x})^\top \nabla_{\mu^n} \Lnonparam_s(v^{\theta_n}, \Pbb_{v^{\theta_n}})(\tilde{x})  \rmd \mu^n_s(\tilde{x}) + o(\alpha) .$$ We immediately have the following corollary.

# Background material on DSBM and extensions

In this section we recall some basics on Markovian and reciprocal projections in . We explain the link between the concept of *iterative refinement* and Schrödinger Bridges in . Then, we briefly present Diffusion Schrödinger Bridge Matching (DSBM) in and propose some new extensions in .

## Markov and reciprocal projections in practice

In this section, we recall the definition of the reciprocal and Markov projection. We provide more details on how these different projections can be performed and illustrate them on simple examples.

#### Markov projection.

First, we recall the definition of the Markovian projection.

In and , we illustrate the effect of the Markovian projection, following the example of . We consider two distributions $\pi_0$ and $\pi_1$ such that $$\pi_0 = \frac{1}{2}\mathcal{N}([-2,-2], \Id) + \frac{1}{2} \mathcal{N}([-2,2], \Id) , \quad \pi_1 = \frac{1}{2} \mathcal{N}([2,-2], \Id) +\frac{1}{2}\mathcal{N}([2,2], \Id) .$$ In , we display samples from the distributions $\pi_0$ and $\pi_1$ as well as trajectories from the path measure $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. Practically, this means that we sample $\bfX_0 \sim \pi_0$ and $\bfX_1 \sim \pi_1$ independently and then consider a Brownian bridge between $\bfX_0$ and $\bfX_1$. The SDE associated with the Brownian bridge with scale $\vareps >0$ is given for any $t \in [0,1]$ by $$\label{eq:brownian_bridge_appendix}
    \rmd \bfX_t = (\bfX_1 - \bfX_t)/ (1-t) \rmd t + \sqrt{\vareps} \rmd \bfB_t .$$ Note that the measure $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$ is in the reciprocal class, i.e. $\Pbb \in \mathcal{R}(\Qbb)$.

<figure id="fig:original_coupling">
<span class="image placeholder" data-original-image-src="img/original_coupling.png" data-original-image-title="" width=".4\linewidth"></span>
<figcaption>Samples from the original distributions <span class="math inline">\(\pi_0\)</span> (left) and <span class="math inline">\(\pi_1\)</span> (right) are shown in red, while sample paths from <span class="math inline">\(\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}\)</span> are shown in blue.</figcaption>
</figure>

Next in , we display samples from the distributions $\pi_0$ and $\pi_1$ as well as trajectories from the path measure $\Pbb^\star = \projM(\Pbb)$. Note that in , contrary to , we observe less crossings between the trajectories. Indeed in the limit case where $\vareps \to 0$ the Markov measures $\Pbb^\star$ is an ODE with regular coefficients and therefore admits a unique solution for every starting point in the space so no crossing is possible. In particular, note that most of the trajectories starting from the upper-left Gaussian end at the upper-right Gaussian. Similarly, most of the trajectories starting from the lower-left Gaussian end at the lower-right Gaussian.

<figure id="fig:markov_projection">
<p>o <span class="image placeholder" data-original-image-src="img/markov_projection.png" data-original-image-title="" width=".4\linewidth">image</span></p>
<figcaption>Samples from the original distributions are shown in red, while sample paths from <span class="math inline">\(\Mbb = \projM(\Pbb)\)</span> are shown in blue.</figcaption>
</figure>

In practice, computing the Markov projection involves finding the optimal drift $v_t^\star$. This optimal drift is the minimizer of a regression problem, see for more details. Hence, computing the Markovian projection requires training a neural network to define a vector field.

#### Reciprocal projection.

First, we recall the definition the reciprocal projection.

To sample from $\Pbb^\star = \projsimpleR(\Pbb)$, we only need to sample $(\bfX_0, \bfX_1) \sim \Pbb_{0,1}$ and then to sample from the Brownian bridge conditioned on $(\bfX_0, \bfX_1)$. This means that in order to sample $\bfX_t^\star \sim \Pbb^\star_t$, we only need to sample $(\bfX_0, \bfX_1) \sim \Pbb_{0,1}$ and then compute $$\label{eq:interpolation_appendix_brownian}
    \bfX_t = (1-t) \bfX_0 + t \bfX_1 + \sqrt{\vareps t (1-t)} \bfZ ,$$ with $\bfZ \sim \mathcal{N}(0, \Id)$. In particular, sampling from $\Pbb^\star = \projsimpleR(\Pbb)$ does *not* require training any neural network. However, in practice, in order to obtain samples $(\bfX_0, \bfX_1) \sim \Pbb$, we have that $\Pbb$ is associated with an SDE and therefore obtaining $(\bfX_0, \bfX_1)$ requires unrolling the SDE associated with $\Pbb$. In , the measure $\Pbb$ is associated with an SDE with parametric drift $v^\theta$.

In , we continue our study of the example of that we used to explain the concept of Markovian projection. We consider the path measure $\Mbb$ obtained as the Markov projection of $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. In , we display samples from the distributions $\pi_0$ and $\pi_1$ as well as trajectories from the path measure $\Pbb^\star = \Mbb_{0,1} \Qbb_{|0,1}$. In order to sample from $\Pbb^\star$ we first sample $(\bfX_0, \bfX_1) \sim \Mbb_{0,1}$. This involves unrolling the SDE associated with $\Mbb$. Once we have access to samples $(\bfX_0, \bfX_1)$, we draw trajectories from the Brownian bridge following the SDE . We can also sample from any time $t$ without having to unroll the SDE by simply sampling from . This is what is done in .

<figure id="fig:reciprocal_projection">
<span class="image placeholder" data-original-image-src="img/reciprocal_projection.png" data-original-image-title="" width=".4\linewidth"></span>
<figcaption>Samples from the original distributions are shown in red, while sample paths from <span class="math inline">\(\Pbb^\star = \projsimpleR(\Mbb)\)</span> are shown in blue.</figcaption>
</figure>

## Iterative refinement and Schrödinger Bridge

The Schrödinger Bridge problem can be solved leveraging techniques from diffusion models and bridge matching. consider an alternating projection algorithm, corresponding to a dynamic version of the celebrated Sinkhorn algorithm. introduce the Iterative Markovian Fitting procedure which corresponds to perform an alternating projection algorithm on the class of Markov processes and the reciprocal class of the Brownian motion. It can be shown that the solution of this iterative algorithm converges to the Schrödinger Bridge under mild assumptions, see . We highlight that in the case where $\vareps \to 0$ then DSBM is equivalent to the Rectified Flow algorithm . One of the main limitation of those previously introduced procedures which provably converge to the solution of the Schrödinger Bridge problem is that they rely on these expensive iterative solvers and requires to consider two networks, one parameterising the forward process $\pi_0 \to \pi_1$ and one parameterising the backward $\pi_1 \to \pi_0$.

## Diffusion Schrödinger Bridge Matching

Diffusion Schrödinger Bridge Matching corresponds to the practical implementation of the Iterative Markovian Fitting procedure proposed in . The IMF procedure alternates between projecting on the Markov class $\calM$ and the reciprocal class $\calR_\Qbb$. In what follows, we denote $\Mbb^{n+1} = \Pbb^{2n+1} \in \calM$ and $\Pi^n = \Pbb^{2n} \in \calR(\Qbb)$. We also recall that $\Qbb$ is a (rescaled) Brownian motion associated with $(\sigma_0 \bfB_t)_{t \in [0,1]}$ and that therefore sampling from $\Qbb_{|0,1}(\cdot | x_0, x_1)$ corresponds to sampling from $$\rmd \bfX_t = (x_1 - \bfX_t) / (1 - t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 = x_0 .$$ We recall that the main computational bottleneck of the DSBM lies in the approximation of the Markovian projection. Indeed, using , we have that $\Mbb^\star=\projM(\Pi)$ is associated with the process $$\rmd \bfX_t = (\mathbb{E}_{\Pi}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) \rmd t + \sigma_0 \rmd \bfB_t \qquad \bfX_0 \sim \pi_0 .$$ We also have using that $\Mbb^\star$ can be approximated using $\Mbb^{\theta^\star}$ given by $$\begin{aligned}
&  \label{eq:approximate_markovian_proj_forward}
  \rmd \bfX_t =  v_{\theta^\star}(t, \bfX_t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 , \\
  \label{eq:loss_function_theta}
 & \theta^\star = \argmin_{\theta \in \Theta} \int_0^1 \mathbb{E}_{\Pi_{t,1}}[\|(\bfX_1 - \bfX_t)/(1-t)  - v_\theta(t,\bfX_t)\|^2]\rmd t,
\end{aligned}$$ where $\ensembleLigne{v_\theta}{\theta \in \Theta}$ is a parametric family of functions, usually given by a neural network.

Hence, since we can approximate $\projR{\Qbb}(\Mbb)$ and $\projM(\Pi)$ we can approximate the IMF procedure. This is the DSBM algorithm introduced in . We describe the first few iterations. Let $\Pi^0=   \Pi_{0,1}^0 \Qbb_{|0,1}$ where $\Pi^0_0 = \pi_0$, $\Pi^0_1 = \pi_1$. Learn $\Mbb^1 \approx \projM(\Pi^0)$ given by with $v_{\theta^\star}$ given by . Next, sample from $\Pi^1 = \projR{\Qbb}(\Mbb^1)= \Mbb^1_{0,1} \Qbb_{|0,1}$ by sampling from $\Mbb^1_{0,1}$ and reconstructing the bridge $\Qbb_{|0,1}$. Upon iterating the previous procedure, we obtain a sequence $(\Pi^n,\Mbb^{n+1})_{n \in \nset}$. To mitigate the bias accumulation problem caused by approximating *only* the forward process, we alternate between a *forward* Markovian projection and a *backward* Markovian projection. We give more details on the advantage of using a forward-backward parameterisation instead of a forward-forward in . This procedure is valid using . The optimal backward process is approximated with $$\begin{aligned}
&  \label{eq:approximate_markovian_proj_backward}
  \rmd \bfY_t = v_{\phi^\star}(1-t, \bfY_t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfY_0 \sim \pi_1 , \\ 
  \label{eq:loss_function_backward}
 &  \phi^\star = \argmin_{\phi \in \Phi} \int_0^1 \expeLigne{\Pi_{0,t}}{\normLigne{(\bfX_0 - \bfX_t)/t - v_\phi(t,\bfX_t)}^2}  \rmd t.
\end{aligned}$$ We recall the full DSBM algorithm in .

## A Reflection-projection extension

First, we consider a reflection-projection method similar to the one investigated in . We recall that the DSBM algorithm is associated with a sequence $(\Pbb^n)_{n \in \nset}$ such that for any $n \in \nset$ $$\Pbb^{n+1/2} = \projM(\Pbb^n) , \qquad \Pbb^{n+1} = \projR{\Qbb}(\Pbb^{n+1/2}) .$$ In a reflection-projection scheme, one of the projection is replaced by a reflection. As noted in , this can yield faster convergence rates in practice. We consider the sequence $(\Pbb^n)_{n \in \nset}$ such that for any $n \in \nset$ $$\label{eq:iteration_reflection_projection}
    \Pbb^{n+1/2} = \projM(\Pbb^n) , \qquad \Pbb^{n+1} = \projR{\Qbb}(2\Pbb^{n+1/2} - \Pbb^n) .$$ In what follows, we make the assumption that $2\Pbb^{n+1/2} - \Pbb^n$ is a probability measure, even though it is not clear if this path measure is non-negative. However, even by making this strong assumption, we show that we can recover DSBM in . By considering a relaxation of the reflection-projection scheme.

First, note that for any $n \in \nset$, $\Pbb^n_{|0}$ is Markov, see for instance. Hence, we assume that $\Pbb^n$ is associated with $$\label{eq:markov_reciprocal}
    \rmd \bfX_t^n = v_{t,0}^n(\bfX_t, \bfX_0) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .$$

#### Estimating $\Pbb^{n+1}$.

First, we compute $v_{t,0}^{n+1}$ assuming that we can sample from $\Pbb^n$ and $\Pbb^{n+1/2}$. Since $\Pbb^{n+1}$ is in the reciprocal class, we have that $\Pbb^{n+1}$ is associated with $$\rmd \bfX_t = (\mathbb{E}_{\Pbb^{n+1}_{1|0,t}}[\bfX_1 \ | \ \bfX_t, \bfX_0] - \bfX_t) / (1 - t) \rmd t + \sigma_0 \rmd \bfB_t .$$ We refer to for a proof of this fact. Hence, using , we have that $$\begin{aligned}
     v_{t,0}^{n+1} &=   \argmin_{v} \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t,0}(t, x_t, x_0) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n+1}_{0,t,1}(x_0, x_t, x_1) \\
    &=   \argmin_{v} 2 \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t,0}(t, x_t, x_0) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n+1/2}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) \\
    &\qquad \qquad \qquad    - \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t,0}(t, x_t, x_0) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) . \label{eq:non_markov_dr}
\end{aligned}$$ Next, we turn to the estimation of $\Pbb^{n+3/2}$.

#### Estimating $\Pbb^{n+3/2}$.

Next, we assume that for any $n \in \nset$, $\Pbb^{n+1/2}$ is associated with $$\rmd \bfX_t^n = v_t^n(\bfX_t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .$$ Using , we have that $v_t^{n+1}$ is given by $$v_t^{n+1} = \argmin_{v} \int_0^1 \int_{\rset^d \times \rset^d} \| v_t(t, x_t) - v_{t,0}^{n+1}(t, x_t, x_0) \|^2 \rmd \Pbb^{n+1}_{0,t}(x_0, x_t) .$$ We note also that using and the fact $\Pbb^n$ is in the reciprocal class of $\Qbb$, we also have that $$\begin{aligned}
v_t^{n+1} &=   \argmin_{v} 2 \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t}(t, x_t) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n+1/2}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) \\
    &\qquad \qquad \qquad     - \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t}(t, x_t) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) . \label{eq:markov_dr}
\end{aligned}$$ Hence, assuming that we can sample from $\Pbb^n$ and $\Pbb^{n+1/2}$ then we can estimate $v_{t,0}^{n+1}$ and $v_t^{n+1}$, i.e. sample from $\Pbb^{n+1}$ and $\Pbb^{n+3/2}$. Note that the losses and only differ by the conditioning with respect to the initial condition $x_0$ and therefore the optimisation can be conducted in parallel. We are now able to propose the following projection-reflection algorithm, see .

Note that in we only consider the optimisation of a forward process but similarly to , one can construct a forward backward extension to alleviate some of the bias accumulation problems. Finally, we can interpolate between DSBM and this new reflection algorithm and DSBM by introducing an hyperparameter $\alpha \geq 0$ and consider the following extension given in

Using different values of $\alpha \geq 0$ in , we recover different existing algorithms. If $\alpha =1$, we recover DSBM . Finally, if $\alpha = 1/2$, we recover the reflection algorithm .

# Consistency in Schrödinger Bridge

The idea of training both the forward and the backward jointly was mentioned in . However, it was still assumed that, while being trained jointly, the forward and backward vector fields were obtained using an $\argmin$ operation, see . In addition, in a consistency loss was proposed in order to enforce that the forward and backward processes match, see . In this section, we leverage new results from in order to enforce the internal consistency of the model.

First, note that for any $(\bfX_t)_{t \in [0,1]}$ associated with $\Pbb \in \mathcal{R}(\Qbb)$ we have for any $0 \leq t_0 \leq t \leq t_1 \leq 1$ that $$\bfX_t = \frac{t - t_0}{t_1 - t_0} \bfX_{t_1} + \frac{t_1 - t}{t_1 - t_0} \bfX_{t_0} +  \sigma_{t_0,t,t_1} \bfZ , \qquad \bfZ \sim \gN(0, \Id),$$ where $$\sigma_{t_0,t,t_1} = \sqrt{\frac{(t - t_0)(t_1 - t)}{t_1 - t_0}}.$$ Let $p_t$ be the density of $\bfX_t$ with respect to the Lebesgue measure, we have that for any $0 \leq t_0 \leq t \leq t_1 \leq 1$ and $x_t \in \rset^d$ $$p_t(x_t) = \int_{\rset^d \times \rset^d} (2 \uppi \sigma_{t_0,t,t_1}^2)^{-d/2} \exp \biggl(-\frac{\| x_t - \frac{t - t_0}{t_1 - t_0} x_{t_1} - \frac{t_1 - t}{t_1 - t_0} x_{t_0} \|^2}{2 \sigma_{t_0, t, t_1}^2}\biggr) p_{t_0}(x_{t_0}) p_{t_1}(x_{t_1}) \rmd x_{t_0} \rmd x_{t_1} .$$ Using the change of variable $x_{t_0} \to x_{t_0} + x_t$ and $x_{t_1} \to x_{t_1} + x_t$ we get that for any $0 \leq t_0 \leq t \leq t_1 \leq 1$ and $x_t \in \rset^d$ $$\begin{aligned}
\label{eq:tsi}
  \nabla \log p_t(x_t) = \int_{\rset^d \times \rset^d} \{ \nabla \log p_{t_0}(x_{t_0}) + \nabla \log p_{t_1}(x_{t_1}) \}  p_{t_0, t_1 | t}(x_{t_0}, x_{t_1} | x_t) \rmd x_t . 
\end{aligned}$$ This identity for the score has already been presented in a bridge matching context in . Let $\Pbb \in \mathcal{R}(\Qbb)$ then we have that $\projM(\Pbb)$ is such that for any $t \in [0,1]$, $\projM(\Pbb)_t = \Pbb_t$, see . We have that for any $t \in [0,1]$, $v_t^\vsra(x) + v_{1-t}^\vsla(x) =  \sigma_0^2 \nabla \log p_t(x)$. Combining this result and , this suggests considering the following consistency loss $$\begin{aligned}
\label{eq:consistency_loss}
    \ell_{\mathrm{cons}, (t_0, t, t_1)}(\theta) &= \mathbb{E}[\| v_\theta(t, 1, \bfX_t) + v_\theta(1-t, 0, \bfX_t) \\
    & \qquad - v_\theta(t_0, 1, \bfX_t) - v_\theta(1-t_0, 0, \bfX_{t_0}) - v_\theta(t_1, 1, \bfX_{t_1}) - v_\theta(1-t_1, 0, \bfX_{t_1}) \|^2] . 
\end{aligned}$$ Similarly to , we can consider an empirical version of .

# Model stitching

In , the finetuning stage requires a pretrained bridge matching model interpolating between $\pi_0$ and $\pi_1$ (lines 2-7). However, for large datasets with complex distributions $\pi_0$ and $\pi_1$, e.g. ImageNet, training this bridge model from scratch can be computationally expensive. To improve efficiency, we can leverage existing diffusion models targeting $\pi_0$ and $\pi_1$. Specifically, we assume access to generative models transferring between $\gN(0, \Id)$ and $\pi_0$, and between $\gN(0, \Id)$ and $\pi_1$. In the rest of this section, we show how one can adapt to this setting. We then comment on the link between the proposed algorithm and Dual Diffusion Implicit Bridges .

#### Setting.

For simplicity, assume that we have two pretrained diffusion models for $\pi_0$ and $\pi_1$. We describe our procedure for $\pi_0$. Consider a forward process of the form $\bfX_t = \bfX_0 + \sigma_t \bfZ$, with $\bfZ \sim \gN(0, \Id)$, where $\sigma_t$ is a hyperparameter. Note that we could have considered an interpolant of the form $\bfX_t = \alpha_t \bfX_0 + \sigma_t \bfZ$ instead, see for instance.

We assume that the model $\bfX_t = \bfX_0 + \sigma_t \bfZ$ is associated with the forward diffusion model $$\label{eq:forward_process_appendix}
    \rmd \bfX_t =  g_t \rmd \bfB_t,$$ where we assume that $g_t \geq 0$ for all $t \in [0,1]$. Note that we have that for any $t \in [0,1]$, $\sigma_t^2 = \int_0^t g_s^2 \rmd s$. In particular, we have that for any $s, t \in [0,1]$ with $s \leq t$ $$\bfX_t = \bfX_s + \sqrt{\sigma_t^2 - \sigma_s^2} \bfZ , \qquad \bfZ \sim \gN(0, \Id) .$$ Our goal is to solve the following Entropic Optimal Transport problem $$\label{eq:static_ot_appendix}
  \Pi^\star = \argmin_{\Pi \in \calP(\rset^{2d})} \Bigl\{ \int_{\rset^d \times \rset^d} c(x,y) \rmd \Pi(x,y) - \varepsilon \mathrm{H}(\Pi) \ ; \ \Pi_0 = \pi_0, \ \Pi_1 = \pi_1 \Bigr\} ,$$ where $\vareps > 0$ is some entropic regularisation. We assume that $\vareps >0$ is fixed and assume that there exists $t' \in [0,1]$ such that $\sigma_{t'}^2 = \vareps/2$. We now consider a dynamic version of with $$\label{eq:dynamic_ot_appendix}
    \Pbb^\star = \argmin_{\Pbb \in \calP(\calC)} \{ \KL(\Pbb | \Qbb)  \ ; \ \Pbb_0 = \pi_0, \ \Pbb_{t'} = \pi_1 \} ,$$ where $\Qbb$ is associated with $(\bfX_t)_{t \in [0, t']}$ . Note that contrary to the setting presented in the main paper, here we do not consider the integration between time $0$ and $1$ but between time $0$ and $t'$. It can be shown that for any $t \in [0, t']$, $(\bfX_t)_{t \in [0,t']}$ associated with $\Qbb_{t|0, t'}$ is given by $$\label{eq:interpolation_brownian_motion_appendix}
\bfX_t = \mathrm{Interp}_t(\bfX_0, \bfX_{t'}, \bfZ) = \left(1-\frac{\sigma_t^2}{\sigma_{t'}^2}\right) \bfX_0 + \frac{\sigma_t^2}{\sigma_{t'}^2} \bfX_{t'} + \sigma_t \sqrt{1 - \frac{\sigma_t^2}{\sigma_{t'}^2} } \bfZ , \quad \bfZ \sim \gN(0, \Id) .$$ Solving is equivalent to solving . We now propose an algorithm to solve . It corresponds to the finetuning stage of with a specific initialisation, similar to DSBM-IPF in .

By $v_\phi$, we denote a DDM model associated with $\pi_1$: $$\label{eq:pi_one_gen_model}
    \rmd \bfX_t = v_\phi(t, \bfX_t) \rmd  t + g_t \rmd \bfB_t , \quad \bfX_0 \sim \gN(0, \Id), \quad \bfX_1 \sim \pi_1.$$

Similarly, $v_\theta$ denotes a diffusion model associated with $\pi_0$: $$\label{eq:pi_zero_gen_model}
    \rmd \bfY_t = v_\theta(t, \bfY_t) \rmd  t + g_t \rmd \bfB_t , \quad \bfY_0 \sim \gN(0, \Id), \quad\bfY_1 \sim \pi_0$$

In analogy to , the two equations above correspond to the forward and backward SDEs.

For a given batch of inputs $\bfX_0^{1:B}$ and $\bfX_1^{1:B}$, timesteps $t \sim \mathrm{Unif}([0,t'])^{\otimes B}$, and interpolations $\bfX_t^\vsla$ and $\bfX_t^\vsra$, we compute the empirical forward and backward losses as the following modification of :

$$\begin{aligned}
\label{eq:empirical_loss_stitched}
 &\ell^{\vsra}(\phi; t, \bfX_1, \bfX_t^\vsra)  = \frac{1}{B} \sum_{i=1}^B \Big\| v_\phi\left(t^i, \bfX_t^{\vsra i}\right) - \left(\bfX_1^i - \bfX_t^{\vsra i}\right)/\sigma_t^i \Big\|^2, \\
   &\ell^{\vsla}(\theta; t, \bfX_0, \bfX_t^\vsla)  = \frac{1}{B}\sum_{i=1}^B \| v_\theta\left(t^i, \bfX_t^{\vsla i}\right) - \left(\bfX_0^i - \bfX_t^{\vsla i}\right)/\sqrt{\sigma^2_{t'}-\sigma^2_{t^i})} \|^2.
\end{aligned}$$

corresponds to an online version of DSBM-IPF with the initialisation given by two generative models. In , we finetune the trained vector fields to solve the interpolation task. At inference time, the SDE associated with vector field $v_\theta$ interpolates between $\pi_1 \to \pi_0$, while the SDE associated with the vector field $v_\phi$ interpolates between $\pi_0 \to \pi_1$.

Our model stitching approach is related to Dual Diffusion Implicit Bridges (DDIB) , which uses pretrained diffusion models, but without further finetuning. As highlighted in , DDIB is inferior to DSBM in terms of quality and alignement of the samples.

# Extended related work

We highlight links between our proposed flow and Sinkhorn flows in . We draw connection between our practical approach and Reinforcement Learning in . We discuss how $\alpha$-IMF is related to (incremental) Expectation-Maximisation in . Finally, we discuss how our algorithm can be seen as an instance of continual learning in .

## Links with Sinkhorn flow

In this section, we discuss the links between our approach and the Sinkhorn flow introduced by . We start by recalling how Sinkhorn flows are defined and then discuss how they are related to our approach.

#### $\gamma$-Sinkhorn and Sinkhorn flows.

We first consider the static EOT problem and recall the Sinkhorn procedure, also called Iterative Proportional Fitting. We define a sequence of coupling $(\bar{\Pi}^n, \Pi^n)_{n \in \nset}$, i.e. for any $n \in \nset$, $\Pi^n \in \mathcal{P}(\rset^d \times \rset^d)$. We let $\Pi^0 = \Qbb_{0,1}$ and we consider for any $n \in \nset$, $$\begin{aligned}
\label{eq:sinkhorn}
    &\Pi^n = \argmin \ensembleLigne{ \KL(\Pi \ | \ \bar{\Pi}^n) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_0 = \pi_0}, \\
     &\bar{\Pi}^{n+1} = \argmin \ensembleLigne{ \KL(\Pi \ | \ \Pi^n) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_1 = \pi_1},
\end{aligned}$$ In , the authors generalise by introducing an extra hyperparameter $\gamma \in (0,1]$ and defining $$\begin{aligned}
\label{eq:gamma_sinkhorn}
    &\Pi^n = \argmin \ensembleLigne{ \KL(\Pi \ | \ \bar{\Pi}^n) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_1 = \pi_1}, \\
     &\bar{\Pi}^{n+1} = \argmin \ensembleLigne{ \gamma \KL(\Pi \ | \ \Pi^n) + (1-\gamma) \KL(\Pi \ | \ \bar{\Pi}^{n}) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_0 = \pi_0},
\end{aligned}$$ Using , we have that for any $\gamma \in (0,1]$, any $n \in \nset$ and any $x_0, x_1 \in \rset^d$ $$\label{eq:pi_n_sinkhorn}
    (\rmd \bar{\Pi}^n / \rmd \Qbb_{0,1})(x_0, x_1) = \exp[f_\gamma^n(x_0) + g_\gamma^n(x_1)] ,$$ with $f_\gamma^0 = g_\gamma^0 = 0$ and for any $n \in \nset$, $\gamma \in (0,1]$ and $x_1 \in \rset^d$ $$\label{eq:g_gamma_update}
    g_\gamma^{n+1}(x_1) = g_\gamma^n(x_1) - \gamma \log (\rmd \bar{\Pi}^n_1 / \rmd \pi_1)(x_1) .$$ In addition, using we have that for any $n \in \nset$, $\gamma \in (0,1]$ and $x_0 \in \rset^d$ $$f_\gamma^n(x_0) = - \log \left(\int_{\rset^d} \exp[g_\gamma^n(x_1) - (1/(2\vareps)) \| x_0 - x_1 \|^2] \rmd \pi_1(x_1)\right) .$$ When letting $\gamma \to 0$, and suggest to consider for any $s \geq 0$, $x_0, x_1 \in \rset^d$ $$(\rmd \bar{\Pi}^s / \rmd \Qbb_{0,1})(x_0, x_1) = \exp[f^s(x_0) + g^s(x_1)] , \qquad \Pi^s = \argmin \ensembleLigne{ \KL(\Pi \ | \ \bar{\Pi}^s) }{\Pi, \ \Pi_1 = \pi_1} ,$$ where for any $s \geq 0$, $x_1 \in \rset^d$ $$\label{eq:flow_f_g}
     \partial_s g^s(x_1) = - \log (\rmd \bar{\Pi}^s_1 / \rmd \pi_1)(x_1) , \qquad \partial_s f^s(x_0) = \int_{\rset^d} \log (\rmd \bar{\Pi}^s_1 / \rmd \pi_1)(x_1) \rmd \bar{\Pi}^s(x_1|x_0) .$$

#### Comparison with Schrödinger Bridge flows.

In order to compare our approach with the one of , we start by rewriting the $\gamma$-Sinkhorn algorithm defined by . To do so, we introduce the projection on the measures with fixed marginal.

With these definitions, we have that for any $n \in \nset$, $\bar{\Pi}^{n+1} = \projzero{\pi_0}(\projone{\pi_1}(\bar{\Pi}^n))$, with $(\bar{\Pi}^n)_{n \in \nset}$ the original Sinkhorn sequence defined by . Similarly, we have that the original Iterative Markovian Fitting (IMF) sequence $(\hat{\Pbb}^n)_{n \in \nset}$ as defined in with $\alpha = 1$ satisfies for any $n \in \nset$, $\hat{\Pbb}^{n+1} = \projR{\Qbb}(\projM(\Pbb^n))$. The analogy between the Sinkhorn iterates and the IMF sequence was already highlighted in and further studied in . We know show that similarly, we can draw an analogy between the sequences defined in with $\alpha \in (0,1)$ and the sequences obtained in $\gamma$-Sinkhorn. To do so, we start by introducing for any $\Pi, \tilde{\Pi} \in \mathcal{P}(\rset^d \times \rset^d)$ $$\Lnonparam^{\mathrm{IPF}}(\Pi, \tilde{\Pi}) = \KL(\Pi \ | \projone{\pi_1}(\tilde{\Pi})) , \qquad R^{\mathrm{IPF}}(\Pi, \tilde{\Pi}) =  \KL(\Pi \ | \tilde{\Pi}) .$$

<div id="table:comparison_flow">

|                |              $\gamma$-Sinkhorn              |                                             $\gamma$-IMF                                              |
|:---------------|:-------------------------------------------:|:-----------------------------------------------------------------------------------------------------:|
| Loss function  | $\KL(\Pi \ | \projone{\pi_1}(\tilde{\Pi}))$ | $\int_0^1 \mathbb{E}_{\projR{\Qbb}(\Pbb)} [\| v_t(\bfX_t)-\tfrac{\bfX_1 - \bfX_t}{1-t} \|^2]  \rmd t$ |
| regularisation |         $\KL(\Pi \ | \tilde{\Pi})$          |          $\int_0^1 \int_{\rset^d} \| f_t(x_t) - \tilde{f}_t(x_t)\|^2 \rmd \mu_t(x_t) \rmd t$          |
| Update         |                  Implicit                   |                                               Explicit                                                |

Comparison between $\gamma$-Sinkhorn and $\gamma$-IMF.

</div>

With this notation, we can now rewrite for any $n \in \nset$ as $$\begin{aligned}
 \label{eq:rewriting_gamma_sinkhorn_2}
     &\bar{\Pi}^{n+1} = \argmin \ensembleLigne{\Lnonparam^{\mathrm{IPF}}(\Pi, \bar{\Pi}^n) + ((1-\gamma)/\gamma) R^{\mathrm{IPF}}(\Pi, \bar{\Pi}^n)}{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_0 = \pi_0} . 
 
\end{aligned}$$ Now, we are going to see that is linked with the discretisation of the path measure flow described in . Recall that for any suitable $v$, we define the path measure $\Pbb_v$ associated with $$%\label{eq:mixture_bridge_sde}
    \rmd \bfX_t = v_t(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .$$ We define $$\label{eq:loss_function_appendix}
    \Lnonparam(v, \Pbb) =\int_0^1  \Lnonparam(v_t, \Pbb)\rmd t= \int_0^1 \int_{\rset^d \times \rset^d} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1-t}\Big\|^2 \rmd \projR{\Qbb}(\Pbb)_{t,1}(x_t, x_1) \rmd s . 
    % =  \int_0^1 \mathbb{E}_{\projR{\Qbb}(\Pbb)}[\| \bfX_1 - f_t(\bfX_t) \|^2] \rmd t .$$ Similarly, for any $\mu \in \mathcal{P}(\mathcal{C})$, we define $$R_\mu(\Pbb_v, \Pbb_{\tilde{v}}) = \int_0^1 \int_{\rset^d} \| v_t(x_t) - \tilde{v}_t(x_t)\|^2 \rmd \mu_t(x_t) \rmd t .$$ Next, we define the sequence of path measures $(\bar{\Pbb}^n)_{n \in \nset}$ such that for any $n \in \nset$ $$\begin{aligned}
 \label{eq:rewriting_gamma_sinkhorn}
     &\bar{\Pbb}^{n+1} = \argmin \ensembleLigne{\Lnonparam(\Pbb, \bar{\Pbb}^n) + (1/\alpha) R_{\mu^n}(\Pi, \bar{\Pbb}^n)}{\Pbb = \Pbb_v, \ \text{for some $v$}} .
 
\end{aligned}$$ Now, if we denote $(v^n)_{n \in \nset}$ the sequence such that for any $n \in \nset$, $\bar{\Pbb}^n = \Pbb_{v^n}$ then we have that for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$ $$\label{eq:update_implicit}
     v^{n+1}_t(x) = v^n_t(x) - \delta \nabla_{\mu^n} \Lnonparam_t(v^{n+1}, \bar{\Pbb}^n)(x) .$$ Recall that $(\Pbb^n)_{n \in \nset}$ given by is associated with $(v^n)_{n \in \nset}$ such that for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$ $$\label{eq:update_explicit}
     v^{n+1}_t(x) = v^n_t(x) - \delta \nabla_\mu^n \Lnonparam_t(v^{n}, \Pbb^n)_t(x) ,$$ see . Therefore, the only difference between and is that is an explicit update whereas is an implicit update. We summarise the differences between $\gamma$-Sinkhorn and the discretisation we introduce in .

## Links with Reinforcement Learning

In this section, we draw some connection between and self-play in Reinforcement Learning. In particular, we introduce a generalisation of which uses the concept of replay buffer commonly used in Reinforcement learning, see for instance.

We first present a generalisation of called Replay Buffer Diffusion Schrödinger Bridge Matching . We define a buffer $\mathcal{B}$ as a collection of samples $\{ (\bfX_0^k, \bfX_1^k) \}_{k=1}^N$, where $N \in \nset$ is the size of the buffer equipped with two functions $\mathrm{Add}$ and $\mathrm{Sample}$. We have that $\mathrm{Add}: \ \Omega \times \bigsqcup_{k \in \nset} (\rset^{2d})^k \times (\rset^{2d})^N \to (\rset^{2d})^N$, where $\Omega$ is a probability space. In practice $\mathrm{Add}$ takes a random number (the function can be stochastic), any number of proposed samples as well as the current buffer. As an output $\mathrm{Add}$ returns the updated buffer. We also define $\mathrm{Sample}: \ \Omega \times \nset \times (\rset^{2d})^N \to \bigsqcup_{k \in \nset} (\rset^{2d})^k$. This function takes a random number (the function can be stochastic), a natural number $k$ representing the number of samples to return as well as the current buffer. As an output $\mathrm{Sample}$ returns a batch of $k$ samples from the buffer.

In , we allow for more flexibility than the online procedure by leveraging the concept of replay buffer originally introduced in Reinforcement Learning . The concept of replay buffer has been used previously in Schrödinger Bridge works, with the notion of cache where every $n_{\mathrm{refresh}}$ steps a cache is emptied and filled with new samples. If $n_{\mathrm{refresh}} = 1$, $N=B$ for both $\mathcal{B}^{\mathrm{fwd}}$ and $\mathcal{B}^{\mathrm{fwd}}$ we have that for any $\omega \in \Omega$ and $(\bfX_0^{1:B}, \bfX_1^{1:B}) \in (\rset^{2d})^{B}$ $$\begin{aligned}
    &\mathrm{Add}(\omega, (\bfX_0^{1:B}, \bfX_1^{1:B}), \mathcal{B}) = (\bfX_0^{1:B}, \bfX_1^{1:B}) , \\ &\mathrm{Sample}(\omega, B, (\bfX_0^{1:B}, \bfX_1^{1:B})) = (\bfX_0^{1:B}, \bfX_1^{1:B}) .
\end{aligned}$$ This means that the $\mathrm{Add}$ simply fills the buffer with the new samples while $\mathrm{Sample}$ just return the whole current buffer. In that case we recover . For more general update rules, the replay buffers $\mathcal{B}^{\mathrm{fwd}}$ and $\mathcal{B}^{\mathrm{bwd}}$ allow us to collect previous samples and therefore to keep a memory of the past experiences. In future work, we plan to investigate popular choice in experience replay and their impact on the performance of .

## Links with Expectation Maximisation

In this section, we make a connection between DSBM and the Expectation Maximisation (EM) algorithm, and show that the discretisation of the Schrödinger Flow proposed in corresponds to some incremental version of an idealised algorithm, as discussed in . We would like to emphasize that the link between the EM algorithm and Diffusion Schrödinger Bridge based methodologies was already highlighted by . Below, we follow the framework of and recall the following definitions.

In , the authors choose because this corresponds to the *Maximisation* step in an EM algorithm while the corresponds to the *expectation* step in the EM algorithm. In , the authors highlight that the Iterative Proportional Fitting procedure is a Expectation-Expectation procedure, i.e. the alternating projections are both $\mathrm{E}$-projections. In contrast, the Iterative Markovian Fitting procedure is a Maximisation-Maximisation procedure, i.e. the alternating projections are both $\mathrm{M}$-projections. In particular, we can define the following sequence of path measures $(\Pbb^n)_{n \in \nset}$, where for any $n \in \nset$ we have $$\Pbb^{n+1/2} = \argmin_{\Pbb \in \projM}\KL(\Pbb^n \ | \Pbb) , \qquad \Pbb^{n+1} = \argmin_{\Pbb \in \mathcal{R}(\Qbb)}\KL(\Pbb^{n+1/2} \ | \Pbb) .$$ In addition, we have that $$\Pbb^{n+1/2} = \Pbb_{v_\star^{n+1}}, \qquad v_\star^{n+1} = \argmin_v \Lnonparam(v, \Pbb^{n}) , \qquad \Pbb^{n+1} = \argmin_{\Pbb \in \mathcal{R}(\Qbb)}\KL(\Pbb_{v_\star^{n+1}} \ | \ \Pbb)  ,$$ since we have that $\Pbb^n = \Pbb^{v_\star^n}$. Hence, our online procedure , which corresponds to the discretisation of the flow of path measures can be rewritten as $$\Pbb^{n+1/2} = \Pbb_{v_\star^{n+1}}, \quad v_\star^{n+1} = \mathrm{Gradientstep}(\Lnonparam(v, \Pbb^n) , \quad \Pbb^{n+1} = \argmin_{\Pbb \in \mathcal{R}(\Qbb)}\KL(\Pbb_{v_\star^{n+1}} \ | \ \Pbb) .$$ Therefore, our proposed algorithm can be seen as an incremental version of the Maximisation-Maximisation algorithm associated with DSBM instead of an incremental version of the Expectation-Maximisation algorithm discussed in .

## Links with finetuning of diffusion models

can be seen as a method to finetune bridge matching. Finetuning of diffusion models and flow matching procedures is an active research area. Most of the existing methodologies optimise for an external cost after a pretraining phase. These procedures rely on Reinforcement Learning strategies . Recently Direct Preference optimisation (DPO) has been applied to the finetuning of diffusion models in . Our approach departs from these works as the objective we minimise is given by the EOT cost. However all of these approaches involve some level of self-play, i.e. are not simulation free.

## Links with continual learning

Continual learning develops techniques to train models when the dataset changes during the training, usually to solve different tasks . In the context of diffusion models, continual learning has been investigated in . In , the authors consider a weighted loss between a diffusion model loss and a distillation loss which ensures some consistency between the model being trained and the previous task model. Similarly to our approach this distillation loss is not simulation-free but, contrary to our loss, the clean samples are not obtained by unrolling the diffusion model but by applying a one-step prediction operator. In , consider different replay buffer techniques to train continual diffusion models and observe that experience replay with a small coefficient can bring improvements. Finally, in , the authors consider the continual training of a text-to-image diffusion model with LoRA .

# Forward-Forward, Forward-Backward and accumulation of error

In this section, we investigate how error accumulates in the context of DSBM. In practice, we observe similar conclusions in the case of the online version of DSBM. We compare two methods: one which only trains a forward model and one which trains a forward and a backward model.

In what follows, we assume that $\pi_0 = \pi_1 = \gN(0, \Id)$, we also assume that $\Qbb$ is associated with $(\sqrt{2} \bfB_t)_{t \in [0,1]}$. We recall that for any $t \in [0,1]$, we have that $$\bfX_t = (1-t) \bfX_0 + t \bfX_1 + \sqrt{2t(1-t)} \bfZ , \qquad \bfZ \sim \gN(0, \Id) .$$ We are going to consider to approximate schemes to implement IMF.

#### Forward-forward.

First, we consider the following sequence of path measures $(\Pbb^n)_{n \in \nset}$. We set $\Pbb^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. For any $n \in \nset$, we define $\Pbb^{2n+2} = \Pbb^{2n+1}_{0,1} \Qbb_{|0,1}$, i.e. $\Pbb^{2n+2} = \projsimpleR(\Pbb^{2n+1})$. In addition, we define $\Pbb^{2n+1} = \projM^{\vareps, \vsra}(\Pbb^{2n})$ such that $\Pbb^{2n+1}$ is associated with $(\bfX_t)_{t \in [0,1]}$ where for any $t \in [0,1]$ $$\label{eq:markov_proj_approximate}
    \rmd \bfX_t = \{ (\mathbb{E}_{\Pbb^{2n}_{1|t}}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) + \vareps \bfX_t \} \rmd t + \sqrt{2} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 ,$$ with $\vareps \in \rset$. Recall that if we define $\bar{\Pbb}^{2n+1} = \projM(\Pbb^{2n})$ we have that for any $t \in [0,1]$, $\bar{\Pbb}^{2n+1}$ is associated with $(\bfX_t)_{t \in [0,1]}$ where for any $t \in [0,1]$ $$\rmd \bfX_t = (\mathbb{E}_{\Pbb^{2n}_{1|t}}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) \rmd t + \sqrt{2} \rmd \bfB_t .$$ Hence, $\projM^{\vareps, \vsra}$ corresponds to making an error of order $x \mapsto \vareps x$ on the estimated velocity field. Doing so, we now longer have that for any $n \in \nset$, $\Pbb^n_1 = \pi_1$. In what follows, we are going to show how the error accumulates for the sequence $\Pbb^n_{0,1}$.

Before stating , we introduce $f: \ \rset^4 \to \rset$ such that for any $c_{0,0}, c_{1,1}, c_{0,1} > 0$ and $t \in [0,1]$ $$\begin{aligned}
    f(c_{0,0}, c_{1,1}, c_{0,1}, t) &= [-(1-t)c_{0,0} + t c_{1,1} + (1-2t) c_{0,1}  - 2t] \\
    & \qquad / [(1-t)^2 c_{0,0} + t^2 c_{1,1} + 2t(1-t) c_{0,1} + 2t(1-t)] .
\end{aligned}$$ We define $F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, t) = 2 \int_0^t f(c_{0,0}, c_{1,1}, c_{0,1}, s) \rmd s + 2 \vareps t$. Finally, we define $$f_{\mathrm{cov}}(c_{0,0}, c_{1,1}, c_{0,1}, \vareps) = \exp[\tfrac{1}{2} F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, 1)] ,$$ as well as $$f_{\mathrm{var}}(c_{0,0}, c_{1,1}, c_{0,1}, \vareps) = \exp[\tfrac{1}{2} F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, 1)] (1 + 2 \int_0^1 \exp[-F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, s)] \rmd s) .$$

<div class="proof">

*Proof.* Let $\Pbb = (\Pbb_{0,1}) \Qbb_{|0,1}$ where $\Pbb_{0,1}$ is a Gaussian random variable with zero mean and covariance matrix $\Sigma \in \rset^{2d \times 2d}$ such that $$\Sigma = \left( \begin{matrix} \Id & c_{0,1} \Id \\
    c_{0,1} \Id & c_{1,1} \Id \end{matrix} \right ) ,$$ where $\Id$ is the $d$-dimensional identity matrix and we assume that $c_{0,1}, c_{1,1} > 0$. We denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. We have that $\Pbb_{1|t}$ is a Gaussian random variable with zero mean. We now compute its covariance matrix. First, we have that $$\mathbb{E}[\bfX_t \bfX_1^\top] = (1-t) \mathbb{E}[\bfX_0 \bfX_1^\top] + t \mathbb{E}[\bfX_1 \bfX_1^\top] = [(1-t) c_{0,1}  + t c_{1,1}] \Id .$$ We also have that $$\begin{aligned}
    \mathbb{E}[\bfX_t \bfX_t^\top] &= (1-t)^2 \mathbb{E}[\bfX_0 \bfX_0^\top] + t(1-t) (\mathbb{E}[\bfX_1 \bfX_0^\top] + \mathbb{E}[\bfX_1 \bfX_0^\top]) + t^2 \mathbb{E}[\bfX_1 \bfX_1^\top] + 2t(1-t) \Id \\
    &= [(1-t)^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1} + 2t(1-t)] \Id \\
    &= [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] \Id . 
\end{aligned}$$ Therefore, we get that for any $t \in [0,1]$ and $x_t  \in \rset^d$ $$\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] = ([(1-t) c_{0,1}  + t c_{1,1}] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] ) x_t .$$ Hence, we have that for any $t \in [0,1]$ and $x_t  \in \rset^d$ $$\begin{aligned}
    & \mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t = ([(1-t) c_{0,1}  + t c_{1,1}] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] - 1) x_t \\
    & \quad = ([(1-t) c_{0,1}  + t c_{1,1} - 1 + t^2 - t^2 c_{1,1} - 2t(1-t) c_{0,1}] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t \\
    & \quad = ([(1-t)(1-2t) c_{0,1}  + t(1-t) c_{1,1} - 1 + t^2] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t \\
    & \quad = (1-t) ([(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t .
\end{aligned}$$ So it follows that $$\begin{aligned}
    & (\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t)/(1-t) \\
    & \qquad \qquad = ([(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t . \label{eq:conditional_expectation_appendix}
\end{aligned}$$ Note that if we set $c_{0,1} = c^2$ and $c_{1,1} = 1$, we recover with $\sigma = 2$. Denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. Combining and we get that $\Pbb^\star$ is associated with $(\bfX_t)_{t \in [0,1]}$ such that for any $t \in [0,1]$ we have $$\rmd \bfX_t = \{([(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) + \vareps \} \bfX_t \rmd t + \sqrt{2} \rmd \bfB_t .$$ Hence, we get that $$\bfX_t = \exp[\tfrac{1}{2} G(t, c_{0,1}, c_{1,1}, \vareps)] \bfX_0 + \Bigl(2 \int_0^t \exp[- G(s, c_{0,1}, c_{1,1}, \vareps)] \rmd s \exp[G(t, c_{0,1}, c_{1,1}, \vareps)]\Bigr)^{1/2} \bfZ ,$$ where $\bfZ \sim \gN(0, \Id)$ is independent from $\bfX_0$ and for any $t \in [0,1]$, $c_{0,1}, c_{1,1}, \vareps > 0$ we have $$G(t, c_{0,1}, c_{1,1}, \vareps) = 2 \int_0^t [(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] \rmd t + 2\vareps t .$$ In addition, we define $$\begin{aligned}
    &g_{\mathrm{cov}}(c_{0,1}, c_{1,1}, \vareps) = \exp[G(1, c_{0,1}, c_{1,1}, \vareps)] , \\
    & g_{\mathrm{var}}(c_{0,1}, c_{1,1}, \vareps) = \exp[G(1, c_{0,1}, c_{1,1}, \vareps)] \Bigl(1 + 2 \int_0^1 \exp[-G(t, c_{0,1}, c_{1,1}, \vareps)] \rmd t \Bigr) .
\end{aligned}$$ Hence, we have that $$\mathbb{E}[\bfX_0 \bfX_1^\top] = g_{\mathrm{cov}}(c_{0,1}, c_{1,1}, \vareps) \Id , \qquad \mathbb{E}[\bfX_1 \bfX_1^\top] = g_{\mathrm{var}}(c_{0,1}, c_{1,1}, \vareps) \Id .$$ Therefore, since for any $n \in \nset$, we have that $\Pbb^{2n+1} = \projM^{\vareps, \vsra}(\Pbb^{2n})$ and $\Pbb^{2n+2} = \Pbb^{2n+1}_{0,1} \Qbb_{|0,1}$, we define $(c_{0,1}^n, c_{1,1}^n)_{n \in \nset}$ such that for any $n \in \nset$ $$\mathbb{E}_{\Pbb^{2n}}[\bfX_0 \bfX_1^\top] = c_{0,1}^n \Id , \qquad \mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_1^\top] = c_{1,1}^n \Id .$$ Note that for any $n \in \nset$, we have that $$\mathbb{E}_{\Pbb^{2n+1}}[\bfX_0 \bfX_1^\top] = c_{0,1}^{n+1} \Id , \qquad \mathbb{E}_{\Pbb^{2n+1}}[\bfX_1 \bfX_1^\top] = c_{1,1}^{n+1} \Id .$$ Since $\Pbb^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$ we get that $c_{0,1}^0 = 0$ and $c_{1,1} = 1$. We have that for any $n \in \nset$ $$c_{0,1}^{n+1} = g_{\mathrm{cov}}(c_{0,1}^n, c_{1,1}^n, \vareps) , \qquad c_{1,1}^{n+1} = g_{\mathrm{var}}(c_{1,1}^n, c_{1,1}^n, \vareps) ,$$ which concludes the proof. ◻

</div>

#### Forward-backward.

Next, we consider the following sequences of path measures $(\Pbb^{n, \vsra})_{n \in \nset}$ and $(\Pbb^{n, \vsla})_{n \in \nset}$. We set $\Pbb^{0, \vsra} = \Pbb^{0, \vsla} = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. For any $n \in \nset$, we define $\Pbb^{2n+2, \vsra} = \Pbb^{2n+1, \vsla}_{0,1} \Qbb_{|0,1}$ and $\Pbb^{2n+2, \vsla} = \Pbb^{2n+1, \vsra}_{0,1} \Qbb_{|0,1}$, i.e. $\Pbb^{2n+2, \vsra} = \projsimpleR(\Pbb^{2n+1, \vsla})$ and $\Pbb^{2n+2, \vsla} = \projsimpleR(\Pbb^{2n+1, \vsra})$. In addition, we define $\Pbb^{2n+1, \vsra} = \projM^{\vareps, \vsra}(\Pbb^{2n, \vsla})$ such that for any $t \in [0,1]$, $\Pbb^{2n+1, \vsra}$ is associated with $(\bfX_t)_{t \in [0,1]}$ where $$\rmd \bfX_t = \{ (\mathbb{E}_{\Pbb^{2n, \vsra}_{1|t}}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) + \vareps \bfX_t \} \rmd t + \sqrt{2} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 ,$$ with $\vareps \in \rset$. Similarly, we define $\Pbb^{2n+1, \vsla} = \projM^{\vareps, \vsla}(\Pbb^{2n, \vsra})$ such that for any $t \in [0,1]$, $\Pbb^{2n+1, \vsla}$ is associated with $(\bfY_{1-t})_{t \in [0,1]}$ where $$\rmd \bfY_t = \{ (\mathbb{E}_{\Pbb^{2n, \vsla}_{0|t}}[\bfX_0 \ | \ \bfY_t] - \bfY_t) / (1 - t) + \vareps \bfY_t \} \rmd t + \sqrt{2} \rmd \bfB_t , \qquad \bfY_0 \sim \pi_1 .$$

The proof is similar to the one of .

<div class="proof">

*Proof.* Let $\Pbb = (\Pbb_{0,1}) \Qbb_{|0,1}$ where $\Pbb_{0,1}$ is a Gaussian random variable with zero mean and covariance matrix $\Sigma \in \rset^{2d \times 2d}$ such that $$\Sigma = \left( \begin{matrix} c_{0,0} \Id & c_{0,1} \Id \\
    c_{0,1} \Id &  \Id \end{matrix} \right ) ,$$ where $\Id$ is the $d$-dimensional identity matrix and $c_{0,1}, c_{0,0} > 0$. We denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. We have that $\Pbb_{1|t}$ is a Gaussian random variable with zero mean. We now compute its covariance matrix. First, we have that $$\mathbb{E}[\bfX_t \bfX_1^\top] = (1-t) \mathbb{E}[\bfX_0 \bfX_1^\top] + t \mathbb{E}[\bfX_1 \bfX_1^\top] = [(1-t) c_{0,1}  + t ] \Id .$$ We also have that $$\begin{aligned}
    \mathbb{E}[\bfX_t \bfX_t^\top] &= (1-t)^2 \mathbb{E}[\bfX_0 \bfX_0^\top] + t(1-t) (\mathbb{E}[\bfX_1 \bfX_0^\top] + \mathbb{E}[\bfX_1 \bfX_0^\top]) + t^2 \mathbb{E}[\bfX_1 \bfX_1^\top] + 2t(1-t) \Id \\
    &= [(1-t)^2 c_{0,0} + t^2  + 2t(1-t) c_{0,1} + 2t(1-t)] \Id \\
    &= [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] \Id . 
\end{aligned}$$ Therefore, we get that for any $t \in [0,1]$ and $x_t  \in \rset^d$ $$\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] = ([(1-t) c_{0,1}  + t ] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] ) x_t .$$ Hence, we have that for any $t \in [0,1]$ and $x_t  \in \rset^d$ $$\begin{aligned}
    & \mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t = ([(1-t) c_{0,1}  + t ] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] - 1) x_t \\
    & \qquad = ([(1-t) c_{0,1}  + t - 2t + t^2 - (1-t)^2 c_{0,0} - 2t(1-t) c_{0,1}] \\
    & \qquad \qquad / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t \\
    & \qquad = ([(1-t)(1-2t) c_{0,1} - (1-t)^2 c_{0,0} - t(1-t)] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t \\
    & \qquad = (1-t)([(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t .
\end{aligned}$$ Finally, we have that for any $t \in [0,1]$ and $x_t  \in \rset^d$ $$\begin{aligned}
    & (\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t)/(1-t) \\
    & \qquad \qquad = ([(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t . \label{eq:conditional_expectation_duo_appendix}
\end{aligned}$$ Note that if we set $c_{0,1} = c^2$ and $c_{0,0} = 1$, we recover with $\sigma = 2$. Denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. Combining and we get that $\Pbb^\star$ is associated with $(\bfX_t)_{t \in [0,1]}$ such that for any $t \in [0,1]$ we have $$\rmd \bfX_t = \{([(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}])  + \vareps \} \bfX_t \rmd t + \sqrt{2} \rmd \bfB_t .$$ Hence, we get that $$\bfX_t = \exp[\tfrac{1}{2} H(t, c_{0,1}, c_{0,0}, \vareps)] \bfX_0 + (2 \int_0^t \exp[- H(s, c_{0,1}, c_{0,0}, \vareps)] \rmd s \exp[H(t, c_{0,1}, c_{0,0}, \vareps)])^{1/2} \bfZ ,$$ where $\bfZ \sim \gN(0, \Id)$ is independent from $\bfX_0$ and for any $t \in [0,1]$, $c_{0,1}, c_{1,1}, \vareps > 0$ we have $$H(t, c_{0,1}, c_{0,0}, \vareps) = 2 \int_0^t [(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] \rmd t + 2 \vareps t .$$ In addition, we define $$\begin{aligned}
    &g_{\mathrm{cov}}(c_{0,1}, c_{0,0}, \vareps) = \exp[\tfrac{1}{2}H(1, c_{0,1}, c_{0,0}, \vareps)] , \\
    & g_{\mathrm{var}}(c_{0,1}, c_{0,0}, \vareps) = \exp[H(1, c_{0,1}, c_{0,0}, \vareps)] \Bigl(1 + 2 \int_0^1 \exp[-H(t, c_{0,1}, c_{0,0}, \vareps)] \rmd t \Bigr) .
\end{aligned}$$ Hence, we have that $$\mathbb{E}[\bfX_0 \bfX_1^\top] = g_{\mathrm{cov}}(c_{0,1}, c_{0,0}, \vareps) \Id , \qquad \mathbb{E}[\bfX_1 \bfX_1^\top] = g_{\mathrm{var}}(c_{0,1}, c_{0,0}, \vareps) \Id . \label{eq:update_equation_appendix}$$ Remember that $\Pbb^{0, \vsra} = \Pbb^{0, \vsla} = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. In addition, for any $n \in \nset$, we have $\Pbb^{2n+2, \vsra} = \Pbb^{2n+1, \vsla}_{0,1} \Qbb_{|0,1}$ and $\Pbb^{2n+2, \vsla} = \Pbb^{2n+1, \vsra}_{0,1} \Qbb_{|0,1}$, i.e. $\Pbb^{2n+2, \vsra} = \projsimpleR(\Pbb^{2n+1, \vsla})$ and $\Pbb^{2n+2, \vsla} = \projsimpleR(\Pbb^{2n+1, \vsra})$. In addition, we also have $\Pbb^{2n+1, \vsra} = \projM^{\vareps, \vsra}(\Pbb^{2n, \vsla})$ and $\Pbb^{2n+1, \vsla} = \projM^{\vareps, \vsla}(\Pbb^{2n, \vsra})$. We also define $(c_{0,1}^{n, \vsra}, c_{1,1}^{n, \vsra})_{n \in \nset}$ such that for any $n \in \nset$ $$\mathbb{E}_{\Pbb^{2n, \vsra}}[\bfX_0 \bfX_1^\top] = c_{0,1}^{n, \vsra} \Id , \qquad \mathbb{E}_{\Pbb^{2n, \vsra}}[\bfX_1 \bfX_1^\top] = c_{1,1}^{n, \vsra} \Id .$$ Finally, we define $(c_{0,1}^{n, \vsla}, c_{1,1}^{n, \vsla})_{n \in \nset}$ such that for any $n \in \nset$ $$\mathbb{E}_{\Pbb^{2n, \vsla}}[\bfX_0 \bfX_1^\top] = c_{0,1}^{n, \vsla} \Id , \qquad \mathbb{E}_{\Pbb^{2n, \vsla}}[\bfX_0 \bfX_0^\top] = c_{0,0}^{n, \vsla} \Id .$$ Using this definition and we get that for any $n \in \nset$ $$\begin{aligned}
    &c_{0,1}^{n+1, \vsra} = g_{\mathrm{cov}}(c_{0,1}^{n, \vsla}, c_{0,0}^{n, \vsla}, \vareps) , \qquad c_{1,1}^{n+1, \vsra} = g_{\mathrm{var}}(c_{0,1}^{n, \vsla}, c_{0,0}^{n, \vsla}, \vareps) , \\
    &c_{0,1}^{n+1, \vsla} = g_{\mathrm{cov}}(c_{0,1}^{n, \vsra}, c_{1,1}^{n, \vsra}, \vareps) , \qquad c_{0,0}^{n+1, \vsla} = g_{\mathrm{var}}(c_{0,1}^{n, \vsra}, c_{1,1}^{n ,\vsra}, \vareps) .
\end{aligned}$$ In addition, we have that $c_{0,1}^{n, \vsla} = c_{0,1}^{n, \vsra} = 0$ and $c_{1,1}^{0, \vsra} = c_{0,0}^{0, \vsla} = 1$. This concludes the proof. ◻

</div>

#### Error accumulation.

In and , we derive the sequences corresponding to the evolution of the variance and the covariance throughout the DSBM iterations in forward-forward mode or forward-backward mode. In what follows, we showcase the behavior of these sequences for different values of $\vareps > 0$. We recall that $\vareps$ corresponds to the error made in the Markov projection, i.e. $\projM$ is replaced by $\projM^{\vareps, \vsra}$ in the forward-forward mode and $\projM$ is replaced by $\projM^{\vareps, \vsra}$ and $\projM^{\vareps, \vsla}$ in the forward-backward mode. First, if we consider the perfect scenario, i.e. $\vareps = 0$, then we observe that both the forward-forward mode and the forward-backward mode satisfy that $\mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_1^\top] = \Id$, see and . Additionally, we can show that in the perfect scenario, i.e. $\vareps = 0$, then both the forward-forward mode and the forward-backward mode satisfy that $\lim_{n \to +\infty} \mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_0^\top] = (\sqrt{2} - 1) \Id$, see and . However, as $\vareps$ increases the behavior between the forward-forward sequence and the forward-backward sequence significantly differs. More precisely, the error explodes as $\vareps$ increases along the DSBM iteration for the forward-forward mode. On the contrary, in the forward-backward mode, the error remains bounded along the DSBM iterations, see and .

<figure id="fig:error_accumulation_variance">
<p><span class="image placeholder" data-original-image-src="img/error_accumulation_11_ff.png" data-original-image-title="" width=".45\linewidth">image</span>  <span class="image placeholder" data-original-image-src="img/error_accumulation_11_fb.png" data-original-image-title="" width=".45\linewidth">image</span></p>
<figcaption>Evolution of <span class="math inline">\((\|\mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_1^\top] - \Id\|)_{n \in \nset}\)</span> in log-space along DSBM iterations (x-axis). Different curves correspond to different values of <span class="math inline">\(\vareps\)</span>, i.e. the larger <span class="math inline">\(\vareps\)</span> the larger the error in the Markovian projection. Left: evolution in the forward-forward mode. Right: evolution in the forward-backward mode.</figcaption>
</figure>

<figure id="fig:error_accumulation_covariance">
<p><span class="image placeholder" data-original-image-src="img/error_accumulation_01_ff.png" data-original-image-title="" width=".45\linewidth">image</span>  <span class="image placeholder" data-original-image-src="img/error_accumulation_01_fb.png" data-original-image-title="" width=".45\linewidth">image</span></p>
<figcaption>Evolution of <span class="math inline">\((\|\mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_0^\top] - \Id\|)_{n \in \nset}\)</span> in log-space along DSBM iterations (x-axis). Different curves correspond to different values of <span class="math inline">\(\vareps\)</span>, i.e. the larger <span class="math inline">\(\vareps\)</span> the larger the error in the Markovian projection. Left: evolution in the forward-forward mode. Right: evolution in the forward-backward mode.</figcaption>
</figure>

# Preconditioning of the loss function

In this section, we provide details on the scaling of the loss function we implement when training our online version of DSBM. We adapt the method of to the case of bridge matching. We only present our derivations in the case of the forward training of the online version of DSBM, i.e. . The preconditioning of the loss described in this setting can be readily extended to the forward-backward loss we consider in practice, i.e. the parametric version of .

We consider the following objective function for any $t  \in [0,1]$ $$\label{eq:rescaled_loss_function}
    \ell_t = \lambda_t \mathbb{E}_{\Pbb}[\| c_t^o \mathrm{nn}_t^\theta(c_t^i \bfX_t) + c_t^s \bfX_t - \tfrac{\bfX_1 - \bfX_t}{1-t} \|^2] .$$ We also define for any $t \in [0,1]$ and $x_t \in \rset^d$, $v_t^\theta(x_t) = c_t^o \mathrm{nn}_t^\theta(c_t^i x_t) + c_t^s x_t$. Hence, $c_t^i$ is an input scaling function, $c_t^o$ is an output scaling function and $c_t^s$ is a skip-connection function. During the training of the online version of DSBM, $\Pbb$ will be given by $\Pbb^n$, where $\Pbb^n = \Pbb_{v^{\theta_n}}$, where the sequence $(\theta_n)_{n \in \nset}$ is given by . Here, we apply the principles of to the case where $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$, i.e. at initialisation of the sequence. In what follows, we assume that $\mathbb{E}_{\pi_0}[\| \bfX_0 \|^2] = \mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] = d$. Note that our considerations can be generalised to $\mathbb{E}_{\pi_0}[\| \bfX_0 \|^2] = \sigma_0^2 d$ and $\mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] = \sigma_1^2 d$. We also have that $$\label{eq:interpolation_preconditioning_appendix}
    \bfX_t = (1-t) \bfX_0 + t \bfX_1 + \sqrt{\vareps t (1-t)} \bfZ , \qquad \bfZ \sim \gN(0, \Id) .$$ Using , we have that for any $t \in [0,1]$ $$\begin{aligned}
    \mathbb{E}_{\Pbb_t}[\| \bfX_t \|^2] &= (1-t)^2 \mathbb{E}_{\pi_0}[\| \bfX_0 \|^2] + t^2 \mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] + \vareps t (1-t) d \\
    &= [(1-t)^2 + t^2 + \vareps t (1-t)] d . 
\end{aligned}$$ We set $c_t^i$ so that $\mathbb{E}[\| c_t^i \bfX_t \|^2] = d$ for every $t \in [0,1]$. Hence, we have that for any $t \in [0,1]$ $$c_t^i = 1 / \sqrt{(1-t)^2 + t^2 + \vareps t (1-t)} .$$ Next, we rewrite . For any $t \in [0,1]$ we have that $$\begin{aligned}
    \ell_t &= \lambda_t \mathbb{E}_{\Pbb}[\| c_t^o \mathrm{nn}_t^\theta(c_t^i \bfX_t) + c_t^s \bfX_t - \tfrac{\bfX_1 - \bfX_t}{1-t} \|^2] \\
    &= (c_t^o)^2 \lambda_t \mathbb{E}_{\Pbb}[\| \mathrm{nn}_t^\theta(c_t^i \bfX_t) - [-c_t^s \bfX_t + \tfrac{\bfX_1 - \bfX_t}{1-t}]/c_t^o \|^2] \\
    &= (c_t^o)^2 \lambda_t \mathbb{E}_{\Pbb}[\| \mathrm{nn}_t^\theta(c_t^i \bfX_t) - [\tfrac{\bfX_1 - (1 + c_t^s(1-t))\bfX_t}{1-t}]/c_t^o \|^2]
\end{aligned}$$ Hence, we get that for any $t \in [0,1]$, $\bfT_t = [\tfrac{\bfX_1 - (1 + c_t^s(1-t))\bfX_t}{1-t}]/c_t^o$ is the target of the network in the regression loss. We are going to fix $c_t^o$ and $c_t^s$ such that i) $\mathbb{E}[\| \bfT_t \|^2] = d$, ii) $c_t^o$ is as small as possible in order not to minimise the error propagation made by the neural network. Using , we have that for any $t \in [0,1]$ $$\begin{aligned}
    \mathbb{E}_{\Pbb_{1,t}}[\| \bfX_1 - (1 + c_t^s(1-t)) \bfX_t \|^2] &= (1 + c_t^s(1-t))^2 \mathbb{E}_{\Pbb_t}[\| \bfX_t \|^2]   + \mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] \\
    & \qquad - 2(1+c_t^s(1-t))\mathbb{E}_{\Pbb_{1,t}}[\langle \bfX_t, \bfX_1 \rangle] \\
    &= (1 + c_t^s(1-t))^2 \mathbb{E}_{\Pbb_t}[\| \bfX_t \|^2] + d - 2(1+c_t^s(1-t)) t d 
\end{aligned}$$ Hence, we get that for any $t \in [0,1]$ $$(c_t^o)^2 = ((1 + c_t^s(1-t))^2 \mathbb{E}_{\Pbb_t}[ \| \bfX_t \|^2]/d + 1 - 2(1+c_t^s(1-t)) t) / (1-t)^2 .$$ We now minimise $(c_t^o)^2$ with respect to $(1+c_t^s(1-t))$. We get that $$1 + c_t^s(1-t) = t / (\mathbb{E}_{\Pbb_t}[ \| \bfX_t \|^2] /d) .$$ Hence, we get that $c_t^s = t/[(1-t)((1-t)^2 + t^2 + \vareps t (1-t))]-1/(1-t)$. With that choice, we get that for any $t \in [0,1]$ $$\begin{aligned}
    (c_t^o)^2 &= (1 - t^2/((1-t)^2 + t^2 + \vareps t (1-t)))/(1-t)^2 . 
\end{aligned}$$ In , the weighting function $\lambda_t$ is set so that the weight in front of the regression loss is equal to one for all times $t \in [0,1]$. Hence, suggests to set $\lambda_t = 1 / (c_t^o)^2$. However, in practice we observe better results by letting $\lambda_t = 1$. This means that the effective weight is given by $1/(c_t^o)^2$. Therefore, for any $t \in [0,1]$ we have $$\begin{aligned}
        &(c_t^i)^2 = (1 + (\vareps -2)t(1-t))^{-1} , \\
        &c_t^s = ((\vareps - 2)t - 1) / (1 + (\vareps - 2)t(1-t)) , \\
        &(c_t^o)^2 = (1 + t + (\vareps - 2)t (1-t)) / (1 - t) . 
    
\end{aligned}$$

<figure id="fig:my_label">
<p><span class="image placeholder" data-original-image-src="img/ci_bridge.png" data-original-image-title="" width=".31\linewidth">image</span>  <span class="image placeholder" data-original-image-src="img/cs_bridge.png" data-original-image-title="" width=".31\linewidth">image</span>  <span class="image placeholder" data-original-image-src="img/co_sq_bridge.png" data-original-image-title="" width=".31\linewidth">image</span></p>
<figcaption>From left to right <span class="math inline">\(((c_t^i)^2)_{t \in [0,1]}\)</span>, <span class="math inline">\((c_t^s)_{t \in [0,1]}\)</span> and <span class="math inline">\(((c_t^o)^2)_{t \in [0,1]}\)</span> for different values of <span class="math inline">\(\vareps \in [0, 10]\)</span>.</figcaption>
</figure>

# Experimental details

In this section, we delve deeper into the specifics of each experiment, implementation details, and share additional results.

We consider two ways of parameterising the vector fields: as in DSBM, we can use two separate neural networks to approximate the forward and backward vector fields, or we can use a single neural network that is conditioned on the direction. In the latter case, we do the conditioning in a similar fashion to how DDM’s neural networks, U-Nets or MLPs, are conditioned on time embeddings. After all, if we work with continuous time variables $t \in [0, 1]$, then the direction signal $s \in \{0, 1\}$ can be thought of as a target time. Thus, we perform the same initial transformations on $t$ and $s$, i.e. computing sinusoidal embeddings followed by a 2-layer MLP, and use the concatenated outputs in adaptive group normalisation layers .

To optimise our networks, we use Adam  with $\beta=(0.9, 0.999)$, and we modify the gradients to keep their global norm below 1.0. We re-initialise the optimiser’s state when the finetuning phase starts.

All image samples in the paper are generated using EMA parameters as it has been known to increase the visual quality of resulting images . Sampling is also the integral part of DSBM’s finetuning stage, both iterative and online. Here, we have two options: sample with EMA or non-EMA parameters. The non-EMA sampling might be easier to implement, while EMA sampling results in a more stable training and slightly better quality, e.g. see AFHQ samples in and for comparison.

For every model used in the paper, we provide hyperparameters in .

## 2D Experiments

In addition to the experiments presented in the main text, we test our models in the simplest 2D data settings used in and . Note, that low-dimensional datasets might not be the ideal showcase for $\alpha$-DSBM given that one can successfully employ less computationally demanding techniques based on minibatch-OT methods .

The results of our bidirectional model finetuned with online updates are given in . During finetuning, we generate samples using 100 Euler–Maruyama steps to solve the forward and backward SDEs. At test time, we solve the forward probability flow ODE (PF-ODE) given by: $$\label{eq:pf-ode}
    \rmd\bfX_t = \frac{1}{2} \big[ v_\theta(1, t, \bfX_t) - v_\theta(0, 1 - t, \bfX_t) \big] \rmd t, \qquad \bfX_0 \sim \pi_0.$$ To evaluate model fit, we compute 2-Wasserstein distance between the true and generated samples (generated with 20 Euler steps). Additionally, we estimate path energy as a measure of trajectory simplicity: $\mathbb{E}_{\bfX_0\sim \pi_0}[\int_0^1 \| v_\theta(t, \bfX_t)\|^2 \rmd t]$ where $v_\theta(t, \bfX_t)$ is the drift of PF-ODE in , and the integral is approximated using 100 steps. We have made a deliberate effort to closely replicate the experimental setup of to ensure the comparability of our results. However, as illustrated in , 2-Wasserstein distance can be very noisy even with 10K samples in the test set. To mitigate this variance, we averaged the 2-Wasserstein distance across five random sets of 10K samples per run, and then averaged these results across multiple runs. Despite these measures, we recommend a future redesign of these 2D experiments to facilitate more robust comparisons between methods.

<figure id="fig:w2">
<p><span><span class="image placeholder" data-original-image-src="img/w2_hist.png" data-original-image-title="" width="40%">image</span></span></p>
<figcaption>A histogram of 2-Wasserstein distances for the ‘moons<span class="math inline">\(\rightarrow\)</span> 8gaussians’ task. These distances are calculated between 10K samples from a finetuned <span class="math inline">\(\alpha\)</span>-DSBM model and 8gaussians distribution, with both sets generated using 100 different random seeds. The wide spread of scores indicates that 2-Wasserstein distance, even computed on 10K samples, may not be an ideal metric for evaluating model fit in this context.</figcaption>
</figure>

## Gaussian data

To parameterise the forward and backward drifts, we use a 2-layer MLP network with 256 hidden units. To process time variables, we compute sinusoidal time embeddings, followed by a 2-layer MLP with 256 hidden units and 50 output units. The resulting time embeddings are then concatenated with $\bfX_t$, so the drift networks receive 100-dimensional input vectors.

For iterative DSBM finetuning, we perform 40K steps with varying number of outer iterations, i.e. when we switch between training the forward and the backward networks. Alternating every 5K steps, corresponds to 8 outer DSBM iteration. Similarly, changing the direction every 1K steps, leads to 40 outer iterations.

We do not have a cache dataloader like in the original DSBM implementation[^2], thus we generate training samples on the fly by sampling either from the forward or the backward network. For this simple task, we also do not use EMA.

During training and evaluation, we use Euler–Maruyama method with 100 equidistant time steps between 0 and 1. The covariance is evaluated using 10K samples.

## MNIST $\leftrightarrow$ EMNIST transfer

We closely follow the setup of and , and train the models to transfer between 10 EMNIST letters, A-E and a-e, and 10 MNIST digits (CC BY-ND 4.0 license). We use the same U-Net architecture with hyperparameters given in .

For DSBM finetuning, we perform 30 outer iterations, i.e. alternating between training the forward and the backward networks, while at each outer iteration a network is trained for 5000 steps. We do not have a cache dataloader and generate training samples on the fly by sampling either from the forward or the backward network with EMA parameters.

During training and evaluation, we use Euler–Maruyama method with 30 equidistant time steps between 0 and 1. For evaluation, we compute FID based on the whole MNIST training set of 60000 examples and a set of 4000 samples that were initialised from each test image in the EMNIST dataset. MSD is computed between 4000 initial EMNIST test examples and their corresponding MNIST samples.

In Figures –, we provide forward and backward samples, i.e. EMNIST $\rightarrow$ MNIST and MNIST $\rightarrow$ EMNIST, from models that differ in parameterisation, finetuning methods, and sampling strategy. For all the models above, we used $\vareps=1$. illustrated the behaviour of the samples when we sweep over the $\vareps$ hyperparameter.

Pretraining a bidirectional model on 4 v3 TPUs takes 1 hour, while the online finetuning stage requires 4 hours on 16 v3 TPUs. The number of pretraining and finetuning steps is chosen to match the experimental setup of .

<figure id="fig:mnist_2net">

<figcaption>EMNIST to MNIST transfer with a 2-networks model. </figcaption>
</figure>

<figure id="fig:mnist_bidirectional">

<figcaption>EMNIST to MNIST transfer with a bidirectional model.</figcaption>
</figure>

<figure id="fig:emnist_2net">

<figcaption>MNIST to EMNIST transfer with a 2-networks model. </figcaption>
</figure>

<figure id="fig:emnist_bidirectional">

<figcaption>MNIST to EMNIST transfer with a bidirectional model.</figcaption>
</figure>

## AFHQ: Cat $\leftrightarrow$ Wild

We consider the problem of image translation between Cat and Wild domains of AFHQ  as introduced by . Each domain has approximately 5000 samples in the training set, and around 500 samples in the test set. We resize the original 512 $\times$ 512 images to 64$\times$<!-- -->64 or 256$\times$<!-- -->256 resolutions.

Our U-Net  implementation is based on with a few improvements suggested in such as rescaling of skip connections by $\nicefrac{1}{\sqrt{2}}$, using residual blocks from BigGAN , and convolution-based up- and downsampling. Hyperparameters are given in . Compared to the straightforward parameterisation of the vector fields, we obtained slightly better results using EDM preconditioning , which we derive in for the case of bridge matching. During training, we use horizontal flips as a way to augment the data.

During training and evaluation, we use Euler–Maruyama method with 100 equidistant time steps between 0 and 1. When evaluating the quality of Cat $\rightarrow$ Wild transfer, we compute FID based on the whole training set of 4576 examples in the Wild domain and a set of 480 samples that were initialised from test images in the Cat domain. LPIPS and MSD are computed between 480 initial Cat images and Wild samples from the model. The same procedure is followed when evaluating in the reverse direction from Wild to Cat. Given that train, and especially the test sets are small, the quantitative results for AFHQ are likely unreliable . In we provide samples from the models finetuned either with an iterative or an online method. While their FID scores are different, the samples look similar between the two models.

As we discussed in the main text, hyperparameter $\vareps$ trades off the visual quality and alignment of the samples in the resulting transfer models. In , we provide AFHQ 64 $\times$ 64 samples for pretrained and finetuned models with different values of $\vareps$. In addition to its relation to EOT, from a DDM perspective, $\vareps$ can be seen as the controlling factor of the noise schedule. As observed by , noise schedules should be adjusted for different image sizes by shifting the noise schedule of some reference resolution where it is proven to be successful. In our case, if we find a good value of $\vareps$ for 64 $\times$ 64 images, then a shifted $\vareps$ for the 256 $\times$ 256 resolution can be computed as $\vareps_{256} = \vareps_{64} \left(\frac{256}{64} \right)^2$. Thus, if we choose $\sqrt\vareps=0.75$ for AFHQ-64, then for AFHQ-256, we can expect $\sqrt{\vareps} = 3.0$ to also work well. Samples from an AFHQ-256 model trained with $\sqrt{\vareps} = 3.0$ are given in .

On 16 v3 TPUs, the bidirectional base and finetuned AFHQ-64 models take 4 and 14 hours to train, respectively. For AFHQ-256, the base model trains for 15 hours, and finetuning takes an additional 37 hours. While we did not experiment with varying pretraining and fine-tuning iterations, these training times suggest that a longer pretraining stage followed by fewer fine-tuning steps may be desirable.

<figure id="fig:afhq_sigma_sweep">

<figcaption>AFHQ 64 <span class="math inline">\(\times\)</span> 64 Wild <span class="math inline">\(\rightarrow\)</span> Cat transfer results for different values of <span class="math inline">\(\sqrt{\vareps}\)</span> in a bidirectional model before and after online finetuning. Low values of <span class="math inline">\(\vareps\)</span> lead to poor sample quality in both base and finetuned models. Excessively high <span class="math inline">\(\vareps\)</span> values impede information passing from the inputs to the outputs, resulting in poor alignment. High values of <span class="math inline">\(\vareps\)</span> also increase blurriness due to noisier SDE trajectories, thus requiring more denoising steps during sampling.</figcaption>
</figure>

<figure id="fig:afhq_nfe_sweep">

<figcaption>AFHQ 64 <span class="math inline">\(\times\)</span> 64 Wild <span class="math inline">\(\rightarrow\)</span> Cat transfer results for varying number of function evaluations (equivalent to time discretisation steps in the Euler-Maruyama method) in a bidirectional model with <span class="math inline">\(\sqrt\vareps=0.75\)</span>, both before and after online finetuning. Post-finetuning, clearer images are achievable with fewer steps. This observation aligns with findings from Rectified Flows <span class="citation" data-cites="liu_flow_2023"></span>.</figcaption>
</figure>

<figure id="fig:afhq64_dsbm_vs_online">

<figcaption>Samples and metrics from a 2-networks model architecture finetuned with DSBM’s iterative procedure vs online finetuning. Within each two rows, initial and transferred samples are on the top and bottom respectively. </figcaption>
</figure>

<figure id="fig:afhq64_bidirectional_online_non_ema">

<figcaption>Uncurated samples for AFHQ 64 <span class="math inline">\(\times\)</span> 64 transfer in a bidirectional model with online finetuning with non-EMA sampling and <span class="math inline">\(\sqrt{\vareps}=0.75\)</span>. Within each two rows, initial and transferred samples are on the top and bottom respectively.</figcaption>
</figure>

<figure id="fig:afhq64_bidirectional_online_ema">

<figcaption>Uncurated samples for AFHQ 64 <span class="math inline">\(\times\)</span> 64 transfer in a bidirectional model with online finetuning and <span class="math inline">\(\sqrt{\vareps}=0.75\)</span>. Within each two rows, initial and transferred samples are on the top and bottom respectively.</figcaption>
</figure>

<figure id="fig:afhq64_reverse">

<figcaption>Samples for AFHQ 64 <span class="math inline">\(\times\)</span> 64 transfer in bidirectional models with online finetuning and different values of <span class="math inline">\(\vareps\)</span>. The models are only trained on Cat and Wild domains, <span class="math inline">\(\pi_0\)</span> and <span class="math inline">\(\pi_1\)</span>, respectively. Thus, in the forward direction the models expect Cat samples as inputs at <span class="math inline">\(t=0\)</span>, and transfer them to the Wild domain at <span class="math inline">\(t=1\)</span>. The reverse transfer holds in the backward direction. Here, we test the models’ behaviour when inputs do not come from the same distribution as during training: we feed Wild samples in the forward direction, and Cat samples in the backward, which is the opposite from what the models expect. Ideally, the model should leave these inputs unchanged, which it does to varying degrees depending on <span class="math inline">\(\vareps\)</span>, variance of the Gaussian noise. As we increase <span class="math inline">\(\vareps\)</span>, less information can pass from the input to the output, thus making them less alike. </figcaption>
</figure>

<figure id="fig:afhq64_dogs">

<figcaption>Samples for AFHQ 64 <span class="math inline">\(\times\)</span> 64 transfer in a bidirectional model with online finetuning and <span class="math inline">\(\sqrt \vareps=2.0\)</span>. The model is only trained on Cat and Wild domains, <span class="math inline">\(\pi_0\)</span> and <span class="math inline">\(\pi_1\)</span>, respectively. Thus, in the forward direction the model expects Cat samples as inputs at <span class="math inline">\(t=0\)</span>, and transfers them to the Wild domain at <span class="math inline">\(t=1\)</span>. The reverse holds in the backward direction. Notably, the model generalises well to the unseen AFHQ Dog domain, often producing high-quality translations. These results come from a model with <span class="math inline">\(\sqrt{\vareps} = 2.0\)</span>, which is higher than our chosen default value of <span class="math inline">\(\sqrt{\vareps} = 0.75\)</span>. Higher noise allows the model to better deal with out-of-distribution inputs. </figcaption>
</figure>

<figure id="fig:afhq256_bidirectional_online_ema">

<figcaption>Uncurated samples for AFHQ 256 <span class="math inline">\(\times\)</span> 256 transfer in a bidirectional model with online finetuning and <span class="math inline">\(\sqrt{\vareps}=3\)</span>. Within each two rows, initial and transferred samples are on the top and bottom respectively.</figcaption>
</figure>

# NeurIPS Paper Checklist

1.  **Claims**

2.  Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?

3.  Answer:

4.  Justification: our main theoretical and experimental contributions are claimed in the abstract and demonstrated in the paper. We summarize our main contributions hereafter. Theoretically, we identify a new family of sequence of path measures related to the IMF algorithm, called $\alpha$-IMF. We show that these sequences correspond to non-parametric updates. We then introduce a parametric update that corresponds to an online version of the DSBM algorithm. We show that our procedure retains the favorable properties of DSBM while not requiring the expensive repeated inner minimisation procedure of DSBM.

5.  **Limitations**

6.  Question: Does the paper discuss the limitations of the work performed by the authors?

7.  Answer:

8.  Justification: The limitations are addressed in the discussion section. The main limitation of our algorithm is that it is not a sampling free methodology. In future work, we would like to see how to mitigate the fact that our algorithm depends on some self-play.

9.  **Theory Assumptions and Proofs**

10. Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

11. Answer:

12. Justification: All theoretical results are proven in the supplementary material, see .

13. **Experimental Result Reproducibility**

14. Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

15. Answer:

16. Justification: Full experimental details are provided in .

17. **Open access to data and code**

18. Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

19. Answer:

20. Justification: Due to IP restrictions, we cannot share the codebase used for this paper. However, we plan to release some notebooks in order to reproduce experiments in a small scale setting.

21. **Experimental Setting/Details**

22. Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimiser, etc.) necessary to understand the results?

23. Answer:

24. Justification: Full experimental details are provided in .

25. **Experiment Statistical Significance**

26. Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?

27. Answer:

28. Justification: All metrics are computed using multiple random seeds and error bars are provided.

29. **Experiments Compute Resources**

30. Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?

31. Answer:

32. Justification: Full details on the compute requirements are given in .

33. **Code Of Ethics**

34. Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <https://neurips.cc/public/EthicsGuidelines>?

35. Answer:

36. Justification: After careful review of the NeurIPS Code of Ethics, we can ensure that the research presented in this paper conforms with the Code of Ethics in every respect. Indeed, we see no immediate safety, security, discrimination, surveillance, deception, harassment, environment, human rights or bias and fairness concerns to our work. In addition, we release details and documentation regarding the datasets and models used. We disclose essential details for reproducibility and have ensured that our work is legally compliant.

37. **Broader Impacts**

38. Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?

39. Answer:

40. Justification: This paper addresses the problem of unpaired dataset translation and proposes an improvement to the DSBM methodology. As the current paper is mostly theoretical and methodological we do not see immediate societal impact of this work and therefore do not discuss these issues. However, we acknowledge that large scale implementation of our algorithm might suffer from the same societal biases as generative models. We hope to address the limitations of such models when turning to more experimental work.

41. **Safeguards**

42. Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

43. Answer:

44. Justification: The paper poses no such risks.

45. **Licenses for existing assets**

46. Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

47. Answer:

48. Justification: We have referenced the license of the datasets we use and cite the original papers that produced the code packages and datasets that we use in that paper.

49. **New Assets**

50. Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

51. Answer:

52. Justification: The paper does not release new assets.

53. **Crowdsourcing and Research with Human Subjects**

54. Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

55. Answer:

56. Justification: The paper does not involve crowdsourcing nor research with human subjects.

57. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**

58. Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

59. Answer:

60. Justification: The paper does not involve crowdsourcing nor research with human subjects.

[^1]: Equal contribution.

[^2]: <https://github.com/yuyang-shi/dsbm-pytorch>
