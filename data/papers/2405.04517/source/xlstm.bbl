\begin{thebibliography}{127}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, et~al.]{Achiam:23gpt4short}
J.~Achiam, S.~Adler, S.~Agarwal, et~al.
\newblock {GPT}-4 technical report.
\newblock \emph{ArXiv}, 2303.08774, 2023.

\bibitem[Anderson et~al.(1977)Anderson, Silverstein, Ritz, and Jones]{Anderson:77}
J.~Anderson, J.~Silverstein, S.~Ritz, and R.~Jones.
\newblock Distinctive features, categorical perception, and probability learning: {S}ome applications of a neural model.
\newblock \emph{Psychological Review}, 84:\penalty0 413--451, 1977.
\newblock \doi{10.1037/0033-295X.84.5.413}.

\bibitem[Anderson(1972)]{Anderson:72}
J.~A. Anderson.
\newblock A simple neural network generating an interactive memory.
\newblock \emph{Mathematical Biosciences}, 14, 1972.
\newblock \doi{10.1016/0025-5564(72)90075-2}.

\bibitem[Arora et~al.(2023)Arora, Eyuboglu, Timalsina, Johnson, Poli, Zou, Rudra, and R\'{e}]{Arora:23arxiv}
S.~Arora, S.~Eyuboglu, A.~Timalsina, I.~Johnson, M.~Poli, J.~Zou, A.~Rudra, and C.~R\'{e}.
\newblock Zoology: {M}easuring and improving recall in efficient language models.
\newblock \emph{ArXiv}, 2312.04927, 2023.

\bibitem[Ba et~al.(2016{\natexlab{a}})Ba, Hinton, Mnih, Leibo, and Ionescu]{Ba:16}
J.~Ba, G.~E. Hinton, V.~Mnih, J.~Z. Leibo, and C.~Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\  4331--4339. Curran Associates, Inc., 2016{\natexlab{a}}.

\bibitem[Ba et~al.(2016{\natexlab{b}})Ba, Kiros, and Hinton]{Ba:16b}
J.~Ba, J.~R. Kiros, and G.~Hinton.
\newblock Layer normalization.
\newblock \emph{ArXiv}, 1607.06450, 2016{\natexlab{b}}.

\bibitem[Bau et~al.(2019)Bau, Belinkov, Sajjad, Durrani, Dalvi, and Glass]{Bau:19}
A.~Bau, Y.~Belinkov, H.~Sajjad, N.~Durrani, F.~Dalvi, and J.~Glass.
\newblock Identifying and controlling important neurons in neural machine translation.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1z-PsR5KX}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, LeBras, Gao, and Choi]{Bisk:20}
Y.~Bisk, R.~Zellers, R.~LeBras, J.~Gao, and Y.~Choi.
\newblock {Piqa}: {R}easoning about physical commonsense in natural language.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~34, pp.\  7432--7439, 2020.

\bibitem[Blodgett et~al.(2016)Blodgett, Green, and O{'}Connor]{Blodgett:16}
S.~L. Blodgett, L.~Green, and B.~O{'}Connor.
\newblock Demographic dialectal variation in social media: {A} case study of {A}frican-{A}merican {E}nglish.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, pp.\  1119--1130, 2016.
\newblock \doi{10.18653/v1/D16-1120}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, et~al.]{Brown:20short}
T.~Brown, B.~Mann, N.~Ryder, et~al.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{Choromanski:21}
K.~M. Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarl{\'{o}}s, P.~Hawkins, J.~Q. Davis, A.~Mohiuddin, L.~Kaiser, D.~B. Belanger, L.~J. Colwell, and A.~Weller.
\newblock Rethinking attention with performers.
\newblock In \emph{9th International Conference on Learning Representations (ICLR)}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=Ua6zuk0WRH}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, et~al.]{Chowdhery:22short}
A.~Chowdhery, S.~Narang, J.~Devlin, et~al.
\newblock {PaLM:} scaling language modeling with pathways.
\newblock \emph{ArXiv}, 2204.02311, 2022.

\bibitem[Chronopoulou et~al.(2022)Chronopoulou, Peters, and Dodge]{Chronopoulou:22}
A.~Chronopoulou, M.~Peters, and J.~Dodge.
\newblock Efficient hierarchical domain adaptation for pretrained language models.
\newblock In \emph{Conference of the North American Chapter of the Association for Computational Linguistics}, pp.\  1336--1351, 2022.
\newblock \doi{10.18653/v1/2022.naacl-main.96}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{Clark:18arxiv}
P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and O.~Tafjord.
\newblock Think you have solved question answering? {T}ry {ARC}, the {AI2} reasoning challenge.
\newblock \emph{ArXiv}, 1803.05457, 2018.

\bibitem[Cover(1965)]{Cover:65}
T.~M. Cover.
\newblock Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition.
\newblock \emph{Electronic Computers, IEEE Transactions on}, EC-14\penalty0 (3):\penalty0 326--334, 1965.

\bibitem[Dao(2024)]{Dao:23}
T.~Dao.
\newblock Flashattention-2: {F}aster attention with better parallelism and work partitioning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, volume~12, 2024.
\newblock URL \url{https://openreview.net/forum?id=mZn2Xyh9Ec}.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R\'{e}]{Dao:22}
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~R\'{e}.
\newblock Flashattention: {F}ast and memory-efficient exact attention with {IO}-awareness.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho (eds.), \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.
\newblock URL \url{https://openreview.net/forum?id=H4DqfPSibmx}.

\bibitem[Dayan \& Willshaw(1991)Dayan and Willshaw]{Dayan:91}
P.~Dayan and D.~J. Willshaw.
\newblock Optimising synaptic learning rules in linear associative memories.
\newblock \emph{Biological Cybernetics}, 65, 1991.
\newblock \doi{10.1007/bf00206223}.

\bibitem[De et~al.(2024)De, Smith, Fernando, Botev, Cristian-Muraru, Gu, Haroun, Berrada, Chen, Srinivasan, Desjardins, Doucet, Budden, Teh, Pascanu, DeFreitas, and Gulcehre]{De:24arxiv}
S.~De, S.~L. Smith, A.~Fernando, A.~Botev, G.~Cristian-Muraru, A.~Gu, R.~Haroun, L.~Berrada, Y.~Chen, S.~Srinivasan, G.~Desjardins, A.~Doucet, D.~Budden, Y.~W. Teh, R.~Pascanu, N.~DeFreitas, and C.~Gulcehre.
\newblock Griffin: {M}ixing gated linear recurrences with local attention for efficient language models.
\newblock \emph{ArXiv}, 2402.19427, 2024.

\bibitem[Degrave et~al.(2022)Degrave, Felici, Buchli, et~al.]{Degrave:22short}
J.~Degrave, F.~Felici, J.~Buchli, et~al.
\newblock Magnetic control of tokamak plasmas through deep reinforcement learning.
\newblock \emph{Nature}, 602:\penalty0 414â€“419, 2022.
\newblock \doi{10.1038/s41586-021-04301-9}.

\bibitem[Del\'{e}tang et~al.(2023)Del\'{e}tang, Ruoss, Grau-Moya, Genewein, Wenliang, Catt, Cundy, Hutter, Legg, Veness, and Ortega]{Deletang:23}
G.~Del\'{e}tang, A.~Ruoss, J.~Grau-Moya, T.~Genewein, L.~K. Wenliang, E.~Catt, C.~Cundy, M.~Hutter, S.~Legg, J.~Veness, and P.~A. Ortega.
\newblock Neural networks and the {Chomsky} hierarchy.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, volume~11, 2023.
\newblock URL \url{https://openreview.net/forum?id=WbxHAzkeQcn}.

\bibitem[Du et~al.(2021)Du, Huang, Dai, et~al.]{Du:21glamshort}
N.~Du, Y.~Huang, A.~M. Dai, et~al.
\newblock {GLaM:} efficient scaling of language models with mixture-of-experts.
\newblock \emph{ArXiv}, 2112.06905, 2021.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Re]{Fu:23}
D.~Y. Fu, T.~Dao, K.~K. Saab, A.~W. Thomas, A.~Rudra, and C.~Re.
\newblock Hungry hungry hippos: {T}owards language modeling with state space models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=COZDy0WYGg}.

\bibitem[Gao et~al.(2021)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{Gao:21arxiv}
L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang, H.~He, A.~Thite, N.~Nabeshima, S.~Presser, and C.~Leahy.
\newblock {The Pile}: {A}n 800gb dataset of diverse text for language modeling.
\newblock \emph{ArXiv}, 2101.00027, 2021.

\bibitem[Gers et~al.(2000)Gers, Schmidhuber, and Cummins]{Gers:00}
F.~A. Gers, J.~Schmidhuber, and F.~Cummins.
\newblock Learning to forget: Continual prediction with {LSTM}.
\newblock \emph{Neural Compututation}, 12\penalty0 (10):\penalty0 2451--2471, 2000.

\bibitem[Google(2023)]{GeminiTeam:23arxiv}
Gemini~Team Google.
\newblock Gemini: {A} family of highly capable multimodal models.
\newblock \emph{ArXiv}, 2312.11805, 2023.

\bibitem[Graves(2013)]{Graves:13}
A.~Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock \emph{ArXiv}, 1308.0850, 2013.

\bibitem[Greenbaum \& Nelson(1996)Greenbaum and Nelson]{Greenbaum:96}
S.~Greenbaum and G.~Nelson.
\newblock The international corpus of {English} ({ICE}) project.
\newblock \emph{World Englishes}, 15\penalty0 (1):\penalty0 3--15, 1996.

\bibitem[Greff et~al.(2015)Greff, Srivastava, Koutn\'{i}k, Steunebrink, and Schmidhuber]{Greff:15}
K.~Greff, R.~K. Srivastava, J.~Koutn\'{i}k, B.~R. Steunebrink, and J.~Schmidhuber.
\newblock {LSTM}: A search space odyssey.
\newblock \emph{ArXiv}, 1503.04069, 2015.

\bibitem[Gu \& Dao(2023)Gu and Dao]{Gu:24arxiv}
A.~Gu and T.~Dao.
\newblock Mamba: {L}inear-time sequence modeling with selective state spaces.
\newblock \emph{ArXiv}, 2312.00752, 2023.

\bibitem[Gu et~al.(2021)Gu, Goel, and R\'{e}]{Gu:21}
A.~Gu, K.~Goel, and C.~R\'{e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{ArXiv}, 2111.00396, 2021.

\bibitem[Gupta et~al.(2022)Gupta, Gu, and Berant]{Gupta:22}
A.~Gupta, A.~Gu, and J.~Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock \emph{ArXiv}, 2203.14343, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He:16}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  770--778, 2016.

\bibitem[Hochreiter(1991)]{Hochreiter:91}
S.~Hochreiter.
\newblock {Untersuchungen zu dynamischen neuronalen Netzen}.
\newblock Master's thesis, Technische Universit\"{a}t M\"{u}nchen, 1991.

\bibitem[Hochreiter \& Schmidhuber(1997{\natexlab{a}})Hochreiter and Schmidhuber]{Hochreiter:97}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997{\natexlab{a}}.

\bibitem[Hochreiter \& Schmidhuber(1997{\natexlab{b}})Hochreiter and Schmidhuber]{Hochreiter:97nips}
S.~Hochreiter and J.~Schmidhuber.
\newblock {LSTM} can solve hard long time lag problems.
\newblock In M.~C. Mozer, M.~I. Jordan, and T.~Petsche (eds.), \emph{Advances in Neural Information Processing Systems (NeurIPS)}, volume~9, pp.\  473--479. MIT Press, Cambridge MA, 1997{\natexlab{b}}.

\bibitem[Hochreiter et~al.(2000)Hochreiter, Bengio, Frasconi, and Schmidhuber]{Hochreiter:00book}
S.~Hochreiter, Y.~Bengio, P.~Frasconi, and J.~Schmidhuber.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.
\newblock In J.~Kolen and S.~Kremer (eds.), \emph{A Field Guide to Dynamical Recurrent Networks}. IEEE, 2000.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and Conwell]{Hochreiter:01}
S.~Hochreiter, A.~Steven Younger, and Peter~R. Conwell.
\newblock Learning to learn using gradient descent.
\newblock In G.~Dorffner, H.~Bischof, and K.~Hornik (eds.), \emph{Proc. Int. Conf. on Artificial Neural Networks (ICANN 2001)}, pp.\  87--94. Springer, 2001.

\bibitem[Hochreiter et~al.(2007)Hochreiter, Heusel, and Obermayer]{Hochreiter:07}
S.~Hochreiter, M.~Heusel, and K.~Obermayer.
\newblock Fast model-based protein homology detection without alignment.
\newblock \emph{Bioinformatics}, 23\penalty0 (14):\penalty0 1728--1736, 2007.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, et~al.]{Hoffmann:22short}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{ArXiv}, 2203.15556, 2022.

\bibitem[Hossain et~al.(2019)Hossain, Sohel, Shiratuddin, and Laga]{Hossain:19}
M.~D. Hossain, F.~Sohel, M.~F. Shiratuddin, and H.~Laga.
\newblock A comprehensive survey of deep learning for image captioning.
\newblock \emph{ACM Computing Surveys (CSUR)}, 51\penalty0 (6):\penalty0 118, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{Kaplan:20}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child, S.~Gray, A.~Radford, J.~Wu, and D.~Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{ArXiv}, 2001.08361, 2020.

\bibitem[Karpathy(2015)]{Karpathy:15blog}
A.~Karpathy.
\newblock The unreasonable effectiveness of recurrent neural networks.
\newblock http://karpathy.github.io/2015/05/21/rnn-effectiveness/, 2015.

\bibitem[Karpathy(2019)]{OpenAI:19}
A.~Karpathy.
\newblock {OpenAI Five} defeats {Dota 2} world champions.
\newblock https://openai.com/research/openai-five-defeats-dota-2-world-champions, 2019.

\bibitem[Karpathy \& Fei-Fei(2015)Karpathy and Fei-Fei]{Karpathy:15}
A.~Karpathy and L.~Fei-Fei.
\newblock Deep visual-semantic alignments for generating image descriptions.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  3128--3137, 2015.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{Katharopoulos:20}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret.
\newblock Transformers are {RNN}s: {F}ast autoregressive transformers with linear attention.
\newblock In E.~H.~Daum\'{e} III and A.~Singh (eds.), \emph{International Conference on Machine Learning (ICML)}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  5156--5165. PMLR, 2020.

\bibitem[Katsch(2023)]{Katsch:23}
T.~Katsch.
\newblock {GateLoop}: {F}ully data-controlled linear recurrence for sequence modeling.
\newblock \emph{ArXiv}, 2311.01927, 2023.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, BenAllal, Li, Mou, {n}ozFerrandis, Jernite, Mitchell, Hughes, Wolf, Bahdanau, vonWerra, and deVries]{Kocetkov:22arxiv}
D.~Kocetkov, R.~Li, L.~BenAllal, J.~Li, C.~Mou, C.~Mu\ {n}ozFerrandis, Y.~Jernite, M.~Mitchell, S.~Hughes, T.~Wolf, D.~Bahdanau, L.~vonWerra, and H.~deVries.
\newblock {The Stack}: {3 TB} of permissively licensed source code.
\newblock \emph{ArXiv}, 2211.15533, 2022.

\bibitem[Kohonen(1972)]{Kohonen:72}
T.~Kohonen.
\newblock Correlation matrix memories.
\newblock \emph{IEEE Transactions on Computers}, C-21\penalty0 (4), 1972.
\newblock \doi{10.1109/tc.1972.5008975}.

\bibitem[Kratzert et~al.(2018)Kratzert, Klotz, Brenner, Schulz, and Herrnegger]{Kratzert:18}
F.~Kratzert, D.~Klotz, C.~Brenner, K.~Schulz, and M.~Herrnegger.
\newblock Rainfall-runoff modelling using long short-term memory {(LSTM)} networks.
\newblock \emph{Hydrology and Earth System Sciences}, 22\penalty0 (11):\penalty0 6005--6022, 2018.

\bibitem[Kratzert et~al.(2019)Kratzert, Klotz, Shalev, Klambauer, Hochreiter, and Nearing]{Kratzert:19}
F.~Kratzert, D.~Klotz, G.~Shalev, G.~Klambauer, S.~Hochreiter, and G.~Nearing.
\newblock Benchmarking a catchment-aware long short-term memory network {(LSTM)} for large-scale hydrological modeling.
\newblock \emph{ArXiv}, 1907.08456, 2019.

\bibitem[Krizhevsky(2009)]{Krizhevsky:09}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, Deptartment of Computer Science, University of Toronto, 2009.

\bibitem[Krotov \& Hopfield(2016)Krotov and Hopfield]{Krotov:16}
D.~Krotov and J.~J. Hopfield.
\newblock Dense associative memory for pattern recognition.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}, pp.\  1172--1180. Curran Associates, Inc., 2016.

\bibitem[Krotov \& Hopfield(2017)Krotov and Hopfield]{Krotov:17}
D.~Krotov and J.~J. Hopfield.
\newblock Dense associative memory is robust to adversarial inputs.
\newblock \emph{ArXiv}, 1701.00939, 2017.

\bibitem[Lakretz et~al.(2019)Lakretz, Kruszewski, Desbordes, Hupkes, Dehaene, and Baroni]{Lakretz:19}
Y.~Lakretz, G.~Kruszewski, T.~Desbordes, D.~Hupkes, S.~Dehaene, and M.~Baroni.
\newblock The emergence of number and syntax units in {LSTM} language models.
\newblock In J.~Burstein, C.~Doran, and T.~Solorio (eds.), \emph{Conference of the North American Chapter of the Association for Computational Linguistics}, pp.\  11--20. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/N19-1002}.

\bibitem[Li et~al.(2022)Li, Cai, Zhang, Chen, and Dey]{Li:22}
Y.~Li, T.~Cai, Y.~Zhang, D.~Chen, and D.~Dey.
\newblock What makes convolutional models great on long sequence modeling?
\newblock \emph{ArXiv}, 2210.09298, 2022.

\bibitem[Liang et~al.(2023)Liang, Bommasani, Lee, et~al.]{Liang:23short}
P.~Liang, R.~Bommasani, T.~Lee, et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{Annals of the New York Academy of Sciences}, 1525:\penalty0 140--146, 2023.

\bibitem[Lin et~al.(2021)Lin, Men, Yang, Zhou, Ding, Zhang, Wang, Wang, Jiang, Jia, Zhang, Zhang, Zou, Li, Deng, Liu, Xue, Zhou, Ma, j.~Yu, Li, Lin, Zhou, Tang, and Yang]{Lin:21}
J.~Lin, R.~Men, A.~Yang, C.~Zhou, M.~Ding, Y.~Zhang, P.~Wang, A.~Wang, L.~Jiang, X.~Jia, J.~Zhang, J.~Zhang, X.~Zou, Z.~Li, X.~Deng, J.~Liu, J.~Xue, H.~Zhou, J.~Ma, j.~Yu, Y.~Li, W.~Lin, J.~Zhou, J.~Tang, and H.~Yang.
\newblock {M6:} {A} {Chinese} multimodal pretrainer.
\newblock \emph{ArXiv}, 2103.00823, 2021.

\bibitem[Linsley et~al.(2018)Linsley, Kim, Veerabadran, Windolf, and Serre]{Linsley:18}
D.~Linsley, J.~Kim, V.~Veerabadran, C.~Windolf, and T.~Serre.
\newblock Learning long-range spatial dependencies with horizontal gated recurrent units.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 31, 2018.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{Loshchilov:19}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Ma et~al.(2022)Ma, Zhou, Kong, He, Gui, Neubig, May, and Zettlemoyer]{Ma:22}
X.~Ma, C.~Zhou, X.~Kong, J.~He, L.~Gui, G.~Neubig, J.~May, and L.~Zettlemoyer.
\newblock Mega: Moving average equipped gated attention.
\newblock \emph{ArXiv}, 2209.10655, 2022.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{Maas:11}
A.~L. Maas, R.~E. Daly, P.~T. Pham, D.~Huang, A.~Y. Ng, and C.~Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, volume~49, pp.\  142--150, 2011.

\bibitem[Magnusson et~al.(2023)Magnusson, Bhagia, Hofmann, et~al.]{Magnusson:23arxivshort}
I.~Magnusson, A.~Bhagia, V.~Hofmann, et~al.
\newblock Paloma: {A} benchmark for evaluating language model fit.
\newblock \emph{ArXiv}, 2312.10523, 2023.

\bibitem[Mehta et~al.(2022)Mehta, Gupta, Cutkosky, and Neyshabur]{Mehta:22}
H.~Mehta, A.~Gupta, A.~Cutkosky, and B.~Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock \emph{ArXiv}, 2206.13947, 2022.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{Merity:17}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations (ICRL)}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Byj72udxe}.

\bibitem[Merrill \& Sabharwal(2023)Merrill and Sabharwal]{Merrill:23}
W.~Merrill and A.~Sabharwal.
\newblock The parallelism tradeoff: {L}imitations of log-precision transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 11:\penalty0 531--545, 2023.
\newblock \doi{10.1162/tacl_a_00562}.

\bibitem[Merrill et~al.(2024)Merrill, Petty, and Sabharwal]{Merrill:24}
W.~Merrill, J.~Petty, and A.~Sabharwal.
\newblock The illusion of state in state-space models.
\newblock \emph{ArXiv}, 2404.08819, 2024.

\bibitem[Milakov \& Gimelshein(2018)Milakov and Gimelshein]{Milakov:18arxiv}
M.~Milakov and N.~Gimelshein.
\newblock Online normalizer calculation for softmax.
\newblock \emph{ArXiv}, 1805.02867, 2018.

\bibitem[Nakano(1972)]{Nakano:72}
K.~Nakano.
\newblock Associatron -- a model of associative memory.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics}, SMC-2\penalty0 (3):\penalty0 380--388, 1972.
\newblock \doi{10.1109/TSMC.1972.4309133}.

\bibitem[Nearing et~al.(2024)Nearing, Cohen, Dube, Gauch, Gilon, Harrigan, Hassidim, Klotz, Kratzert, Metzger, Nevo, Pappenberger, Prudhomme, Shalev, Shenzis, Tekalign, Weitzner, and Kosko]{Nearing:24}
G.~Nearing, D.~Cohen, V.~Dube, M.~Gauch, O.~Gilon, S.~Harrigan, A.~Hassidim, D.~Klotz, F.~Kratzert, A.~Metzger, S.~Nevo, F.~Pappenberger, C.~Prudhomme, G.~Shalev, S.~Shenzis, T.~Y. Tekalign, D.~Weitzner, and Y.~M.~B. Kosko.
\newblock Global prediction of extreme floods in ungauged watersheds.
\newblock \emph{Nature}, 627:\penalty0 559--563, 2024.
\newblock \doi{10.1038/s41586-024-07145-1}.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, et~al.]{Olsson:22short}
C.~Olsson, N.~Elhage, N.~Nanda, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{ArXiv}, 2209.11895, 2022.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{Orvieto:23}
A.~Orvieto, S.~L. Smith, A.~Gu, A.~Fernando, C.~Gulcehre, R.~Pascanu, and S.~De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning (ICML)}. JMLR.org, 2023.
\newblock \doi{10.5555/3618408.3619518}.

\bibitem[Papasavva et~al.(2020)Papasavva, Zannettou, DeCristofaro, Stringhini, and Blackburn]{Papasavva:20}
A.~Papasavva, S.~Zannettou, E.~DeCristofaro, G.~Stringhini, and J.~Blackburn.
\newblock Raiders of the lost {KeK}: 3.5 years of augmented {4chan} posts from the politically incorrect board.
\newblock In \emph{International AAAI Conference on Web and Social Media (ICWSM)}, volume~14, pp.\  885--894, 2020.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, G.~Boleda, and Fern\'{a}ndez]{Paperno:16}
D.~Paperno, G.~Kruszewski, A.~Lazaridou, N.-Q. Pham, R.~Bernardi, S.~Pezzelle, M.~Baroni, Gemma G.~Boleda, and R.~Fern\'{a}ndez.
\newblock The {LAMBADA} dataset: {W}ord prediction requiring a broad discourse context.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, volume~1, pp.\  1525--1534, 2016.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{Penedo:23arxiv}
G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, A.~Cappelli, H.~Alobeidli, B.~Pannier, E.~Almazrouei, and J.~Launay.
\newblock The {RefinedWeb} dataset for {Falcon} {LLM}: {O}utperforming curated corpora with web data, and web data only.
\newblock \emph{ArXiv}, 2306.01116, 2023.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, et~al.]{Peng:23arxivshort}
B.~Peng, E.~Alcaide, Q.~Anthony, et~al.
\newblock {RWKV}: {R}einventing {RNNs} for the transformer era.
\newblock \emph{ArXiv}, 2305.13048, 2023.

\bibitem[Peng et~al.(2024)Peng, Goldstein, Anthony, et~al.]{Peng:24arxivshort}
B.~Peng, D.~Goldstein, Q.~Anthony, et~al.
\newblock {Eagle} and {Finch}: {RWKV} with matrix-valued states and dynamic recurrence.
\newblock \emph{ArXiv}, 2404.05892, 2024.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and R\'{e}]{Poli:23}
M.~Poli, S.~Massaroli, E.~Nguyen, D.~Y. Fu, T.~Dao, S.~Baccus, Y.~Bengio, S.~Ermon, and C.~R\'{e}.
\newblock Hyena hierarchy: {T}owards larger convolutional language models.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning (ICML)}. JMLR.org, 2023.
\newblock \doi{10.5555/3618408.3619572}.

\bibitem[Poli et~al.(2024)Poli, Thomas, Nguyen, Ponnusamy, Deiseroth, Kersting, Suzuki, Hie, Ermon, R\'{e}, Zhang, and Massaroli]{Poli:24arxiv}
M.~Poli, A.~W. Thomas, E.~Nguyen, P.~Ponnusamy, B.~Deiseroth, K.~Kersting, T.~Suzuki, B.~Hie, S.~Ermon, C.~R\'{e}, C.~Zhang, and S.~Massaroli.
\newblock Mechanistic design and scaling of hybrid architectures.
\newblock \emph{ArXiv}, 2403.17844, 2024.

\bibitem[Qin et~al.(2023)Qin, Yang, and Zhong]{Qin:23}
Z.~Qin, S.~Yang, and Y.~Zhong.
\newblock Hierarchically gated recurrent neural network for sequence modeling.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, volume~37, 2023.
\newblock URL \url{https://openreview.net/forum?id=P1TCHxJwLB}.

\bibitem[Qin et~al.(2024)Qin, Yang, Sun, Shen, Li, Sun, and Zhong]{Qin:24arxiv}
Z.~Qin, S.~Yang, W.~Sun, X.~Shen, D.~Li, W.~Sun, and Y.~Zhong.
\newblock {HGRN2}: {G}ated linear {RNNs} with state expansion.
\newblock \emph{ArXiv}, 2404.07904, 2024.

\bibitem[Radev et~al.(2009)Radev, Muthukrishnan, and Qazvinian]{Radev:09}
D.~R. Radev, P.~Muthukrishnan, and V.~Qazvinian.
\newblock The {ACL} anthology network corpus.
\newblock In \emph{Workshop on Text and Citation Analysis for Scholarly Digital Libraries ({NLPIR}4{DL})}, pp.\  54--61. Association for Computational Linguistics, 2009.

\bibitem[Radford et~al.(2017)Radford, Jozefowicz, and Sutskever]{Radford:17arxiv}
A.~Radford, R.~Jozefowicz, and I.~Sutskever.
\newblock Learning to generate reviews and discovering sentiment.
\newblock \emph{ArXiv}, 1704.01444, 2017.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{Radford:19}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \url{https://openai.com/index/better-language-models}, 2019.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, et~al.]{Rae:21short}
J.~W. Rae, S.~Borgeaud, T.~Cai, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training {Gopher}.
\newblock \emph{ArXiv}, 2112.11446, 2021.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{Raffel:19arxiv}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{ArXiv}, 1910.10683, 2019.

\bibitem[Ramsauer et~al.(2021)Ramsauer, Sch\"{a}fl, Lehner, Seidl, Widrich, Gruber, Holzleitner, Pavlovi{\'c}, Sandve, Greiff, Kreil, Kopp, Klambauer, Brandstetter, and Hochreiter]{Ramsauer:21}
H.~Ramsauer, B.~Sch\"{a}fl, J.~Lehner, P.~Seidl, M.~Widrich, L.~Gruber, M.~Holzleitner, M.~Pavlovi{\'c}, G.~K. Sandve, V.~Greiff, D.~Kreil, M.~Kopp, G.~Klambauer, J.~Brandstetter, and S.~Hochreiter.
\newblock {Hopfield} networks is all you need.
\newblock In \emph{International Conference on Learning Representations (ICLR)}. OpenReview, 2021.

\bibitem[Reid et~al.(2022)Reid, Zhong, Gururangan, and Zettlemoyer]{Reid:22}
M.~Reid, V.~Zhong, S.~Gururangan, and L.~Zettlemoyer.
\newblock {M2D2}: {A} massively multi-domain language modeling dataset.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, pp.\  964--975, 2022.

\bibitem[Reid et~al.(2024)Reid, Savinov, Teplyashin, et~al.]{Reid:24arxiv}
M.~Reid, N.~Savinov, D.~Teplyashin, et~al.
\newblock Gemini 1.5: {U}nlocking multimodal understanding across millions of tokens of context.
\newblock \emph{ArXiv}, 2403.05530, 2024.

\bibitem[Ribeiro et~al.(2021)Ribeiro, Blackburn, Bradlyn, DeCristofaro, Stringhini, Long, Greenberg, and Zannettou]{Ribeiro:21}
M.~H. Ribeiro, J.~Blackburn, B.~Bradlyn, E.~DeCristofaro, G.~Stringhini, S.~Long, S.~Greenberg, and S.~Zannettou.
\newblock The evolution of the manosphere across the web.
\newblock In \emph{Proceedings of the international AAAI conference on web and social media}, volume~15, pp.\  196--207, 2021.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{Sakaguchi:21}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock {Winogrande}: {A}n adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, et~al.]{Scao:22arxivshort}
T.~L. Scao, A.~Fan, C.~Akiki, et~al.
\newblock {BLOOM}: {A} {176B}-parameter open-access multilingual language model.
\newblock \emph{ArXiv}, 2211.05100, 2022.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{Schlag:21}
I.~Schlag, K.~Irie, and J.~Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In M.~Meila and T.~Zhang (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning (ICML)}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  9355--9366. PMLR, 2021.

\bibitem[Schmidhuber(1992)]{Schmidhuber:92ncfastweights}
J.~Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to recurrent nets.
\newblock \emph{Neural Computation}, 4\penalty0 (1):\penalty0 131--139, 1992.

\bibitem[Schmidhuber(2015)]{Schmidhuber:15}
J.~Schmidhuber.
\newblock Deep learning in neural networks: {A}n overview.
\newblock \emph{Neural Networks}, 61:\penalty0 85--117, 2015.
\newblock \doi{10.1016/j.neunet.2014.09.003}.

\bibitem[Schulman et~al.(2022)Schulman, Zoph, Kim, Hilton, et~al.]{Schulman:22short}
J.~Schulman, B.~Zoph, C.~Kim, J.~Hilton, et~al.
\newblock {ChatGPT}: Optimizing language models for dialogue.
\newblock https://openai.com/blog/chatgpt/, 2022.
\newblock OpenAI Research.

\bibitem[Sejnowski(1977)]{Sejnowski:77}
T.~J. Sejnowski.
\newblock Storing covariance with nonlinearly interacting neurons.
\newblock \emph{Journal of Mathematical Biology}, 4, 1977.
\newblock \doi{10.1007/BF00275079}.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{Shoeybi:19}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro.
\newblock {Megatron-LM}: {T}raining multi-billion parameter language models using model parallelism.
\newblock \emph{ArXiv}, 1909.08053, 2019.

\bibitem[Smith et~al.(2022)Smith, Warrington, and Linderman]{Smith:22}
J.~T.~H. Smith, A.~Warrington, and S.~W. Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock \emph{ArXiv}, 2208.04933, 2022.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{Soboleva:23}
D.~Soboleva, F.~Al-Khateeb, R.~Myers, J.~R. Steeves, J.~Hestness, and N.~Dey.
\newblock {SlimPajama}: {A} {627B} token cleaned and deduplicated version of {RedPajama}.
\newblock \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, 2023.
\newblock URL \url{https://huggingface.co/datasets/cerebras/SlimPajama-627B}.

\bibitem[Soldaini et~al.(2023)Soldaini, Kinney, Bhagia, et~al.]{Soldaini:24arxivshort}
L.~Soldaini, R.~Kinney, A.~Bhagia, et~al.
\newblock {Dolma}: an open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{ArXiv}, 2306.01116, 2023.

\bibitem[Soltan et~al.(2022)Soltan, Ananthakrishnan, FitzGerald, Gupta, Hamza, Khan, Peris, Rawls, Rosenbaum, Rumshisky, Prakash, Sridhar, Triefenbach, Verma, Tur, and Natarajan]{Soltan:22}
S.~Soltan, S.~Ananthakrishnan, J.~FitzGerald, R.~Gupta, W.~Hamza, H.~Khan, C.~Peris, S.~Rawls, A.~Rosenbaum, A.~Rumshisky, C.~S. Prakash, M.~Sridhar, F.~Triefenbach, A.~Verma, G.~Tur, and P.~Natarajan.
\newblock {AlexaTM 20B:} {F}ew-shot learning using a large-scale multilingual {Seq2Seq} model.
\newblock \emph{ArXiv}, 2208.01448, 2022.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and Schmidhuber]{Srivastava:15}
R.~K. Srivastava, K.~Greff, and J.~Schmidhuber.
\newblock Training very deep networks.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems (NeurIPS)}, volume~28. Curran Associates, Inc., 2015.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{Sun:23arxiv}
Y.~Sun, L.~Dong, S.~Huang, S.~Ma, Y.~Xia, J.~Xue, J.~Wang, and F.~Wei.
\newblock Retentive network: {A} successor to transformer for large language models.
\newblock \emph{ArXiv}, 2307.08621, 2023.

\bibitem[Sutawika et~al.(2023)Sutawika, Gao, Schoelkopf, et~al.]{Sutawika:23short}
L.~Sutawika, L.~Gao, H.~Schoelkopf, et~al.
\newblock {EleutherAI/lm-evaluation-harness: Major refactor}, 2023.

\bibitem[Sutawika et~al.(2024)Sutawika, Schoelkopf, Gao, Abbasi, Biderman, Tow, fattori, Lovering, farzanehnakhaee70, Phang, Thite, Fazz, Wang, Muennighoff, Aflah, sdtblck, nopperl, gakada, tttyuntian, researcher2, Chris, Etxaniz, Lee, Kasner, Khalid, Hsu, Kanekar, Ammanamanchi, Boykis, and AndyZwei]{Sutawika:24}
L.~Sutawika, H.~Schoelkopf, L.~Gao, B.~Abbasi, S.~Biderman, J.~Tow, B.~fattori, C.~Lovering, farzanehnakhaee70, J.~Phang, A.~Thite, Fazz, T.~Wang, N.~Muennighoff, Aflah, sdtblck, nopperl, gakada, tttyuntian, researcher2, Chris, J.~Etxaniz, H.~A. Lee, Z.~Kasner, Khalid, J.~Hsu, A.~Kanekar, P.~S. Ammanamanchi, V.~Boykis, and AndyZwei.
\newblock {EleutherAI/lm-evaluation-harness}, 2024.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and Le]{Sutskever:14nips}
I.~Sutskever, O.~Vinyals, and Q.~V.~V. Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q. Weinberger (eds.), \emph{Advances in Neural Information Processing Systems 27 (NIPS'13)}, pp.\  3104--3112. Curran Associates, Inc., 2014.

\bibitem[Tay et~al.(2020)Tay, Bahri, Metzler, Juan, Zhao, and Zheng]{Tay:20arxiv}
Y.~Tay, D.~Bahri, D.~Metzler, D.-C. Juan, Z.~Zhao, and C.~Zheng.
\newblock Synthesizer: {R}ethinking self-attention in transformer models.
\newblock \emph{ArXiv}, 2005.00743, 2020.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{Tay:21}
Y.~Tay, M.~Dehghani, S.~Abnar, Y.~Shen, D.~Bahri, P.~Pham, J.~Rao, L.~Yang, S.~Ruder, and D.~Metzler.
\newblock Long range arena: {A} benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations (ICRL)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qVyeW-grC2k}.

\bibitem[Thoppilan et~al.(2022)Thoppilan, deFreitas, Hall, et~al.]{Thoppilan:22short}
R.~Thoppilan, D.~deFreitas, J.~Hall, et~al.
\newblock {LaMDA}: {L}anguage models for dialog applications.
\newblock \emph{ArXiv}, 2201.08239, 2022.

\bibitem[TogetherComputer(2023)]{Together:23}
TogetherComputer.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi\`{e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{Touvron:23arxiv}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi\`{e}re, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin, E.~Grave, and G.~Lample.
\newblock Llama: {O}pen and efficient foundation language models.
\newblock \emph{ArXiv}, 2302.1397, 2023.

\bibitem[Vadas \& Curran(2011)Vadas and Curran]{Vadas:11}
D.~Vadas and J.~R. Curran.
\newblock Parsing noun phrases in the {Penn Treebank}.
\newblock \emph{Computational Linguistics}, 37\penalty0 (4):\penalty0 753--809, 2011.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Vaswani:17}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, volume~30, pp.\  5998--6008. Curran Associates, Inc., 2017.

\bibitem[Vinyals et~al.(2017)Vinyals, Ewalds, Bartunov, et~al.]{Vinyals:17short}
O.~Vinyals, T.~Ewalds, S.~Bartunov, et~al.
\newblock Starcraft {II:} {A} new challenge for reinforcement learning.
\newblock \emph{ArXiv}, 1708.04782, 2017.

\bibitem[Wang et~al.(2022)Wang, Yan, Gu, and Rush]{Wang:22}
J.~Wang, J.~N. Yan, A.~Gu, and A.~M. Rush.
\newblock Pretraining without attention.
\newblock \emph{ArXiv}, 2212.10544, 2022.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{Wang:20arxiv}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: {S}elf-attention with linear complexity.
\newblock \emph{ArXiv}, 2006.04768, 2020.

\bibitem[Wang et~al.(2021)Wang, Sun, Xiang, et~al.]{Wang:21arxivshort}
S.~Wang, Y.~Sun, Y.~Xiang, et~al.
\newblock {ERNIE} {3.0} {Titan}: {E}xploring larger-scale knowledge enhanced pre-training for language understanding and generation.
\newblock \emph{ArXiv}, 2112.12731, 2021.

\bibitem[Wu \& He(2018)Wu and He]{Wu:2018}
Y.~Wu and K.~He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  3--19, 2018.

\bibitem[Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant, Barua, and Raffel]{Xue:21}
L.~Xue, N.~Constant, A.~Roberts, M.~Kale, R.~Al-Rfou, A.~Siddhant, A.~Barua, and C.~Raffel.
\newblock m{T}5: {A} massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{Conference of the North American Chapter of the Association for Computational Linguistics}, pp.\  483--498, 2021.
\newblock \doi{10.18653/v1/2021.naacl-main.41}.

\bibitem[Yang \& Zhang(2024)Yang and Zhang]{Yang:24}
S.~Yang and Y.~Zhang.
\newblock {FLA}: {A} {Triton}-based library for hardware-efficient implementations of linear attention mechanism, 2024.
\newblock URL \url{https://github.com/sustcsonglin/flash-linear-attention}.

\bibitem[Yang et~al.(2023)Yang, Wang, Shen, Panda, and Kim]{Yang:23arxiv}
S.~Yang, B.~Wang, Y.~Shen, R.~Panda, and Y.~Kim.
\newblock Gated linear attention transformers with hardware-efficient training.
\newblock \emph{ArXiv}, 2312.06635, 2023.

\bibitem[Zannettou et~al.(2018)Zannettou, Bradlyn, DeCristofaro, Kwak, Sirivianos, Stringini, and Blackburn]{Zannettou:18}
S.~Zannettou, B.~Bradlyn, E.~DeCristofaro, H.~Kwak, M.~Sirivianos, G.~Stringini, and J.~Blackburn.
\newblock What is {Gab}: {A} bastion of free speech or an alt-right echo chamber.
\newblock In \emph{The Web Conference}, pp.\  1007â€“1014, 2018.
\newblock \doi{10.1145/3184558.3191531}.

\bibitem[Zaremba \& Sutskever(2014)Zaremba and Sutskever]{Zaremba:14arxiv}
W.~Zaremba and I.~Sutskever.
\newblock Learning to execute.
\newblock \emph{ArXiv}, 1410.4615, 2014.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{Zellers:19}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock {HellaSwag}: {C}an a machine really finish your sentence?
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, pp.\  4791--4800, 2019.

\bibitem[Zeng et~al.(2022)Zeng, Liu, Du, et~al.]{Zeng:22arxivshort}
A.~Zeng, X.~Liu, Z.~Du, et~al.
\newblock {GLM-130B}: {A}n open bilingual pre-trained model.
\newblock \emph{ArXiv}, 2210.02414, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{Zhang:22opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, T.~Mihaylov, M.~Ott, S.~Shleifer, K.~Shuster, D.~Simig, P.~S. Koura, A.~Sridhar, T.~Wang, and L.~Zettlemoyer.
\newblock {OPT:} {O}pen pre-trained transformer language models.
\newblock \emph{ArXiv}, 2205.01068, 2022.

\end{thebibliography}
