\begin{table}
    \begin{adjustbox}{width=1.0\textwidth}
    \begin{tabular}[htp]{m{1.5cm}m{2cm}m{1.6cm}m{1.85cm}p{1cm}m{1.4cm}m{1.7cm}m{1cm}}
    \toprule
         \multirow{2}{1cm}{\textbf{Task}} & \multirow{2}{2cm}{\textbf{Model}} & \multirow{2}{1.6cm}{\textbf{Number of blocks}} & \multirow{2}{1.8cm}{\textbf{Embedding dimension}} & \multirow{2}{1cm}{\textbf{Batch size}} &  \multirow{2}{1.4cm}{\textbf{Number of steps}} &  \multirow{2}{1.8cm}{\textbf{LR scheduler}} &  \multirow{2}{1.6cm}{\textbf{LR}} \\
         &  &  &  &  &  &  & \\
    \midrule
        \multirow{9}{*}{Retrieval} & Transformer & \multirow{9}{*}{6} & \multirow{9}{*}{128} & \multirow{9}{*}{64} & \multirow{9}{*}{100K} & WCAR & 1e-3 \\
         & Llama &  &  &  &  & WCA & 1e-4\\ 
         & Mamba &  &  &  &  & WCA & 1e-3\\ 
         & LSTM &  &  &  &  & X & X\\ 
         & RWKV-4 &  &  &  &  & WCAR & 1e-3\\
         & LSTM-SM &  &  &  &  & WCA & 1e-3\\ 
         & xLSTM[1:0] &  &  &  &  & WCAR & 1e-3 \\ 
    \midrule
        \multirow{9}{*}{ListOps} & Transformer & \multirow{9}{*}{8} & \multirow{9}{*}{128} & \multirow{9}{*}{32} & \multirow{9}{*}{80K} & WCA & 1e-4 \\ 
         & Llama &  &  &  &  & WCA & 1e-4\\ 
         & Mamba &  &  &  &  & WCA & 1e-3\\ 
         & LSTM &  &  &  &  & WCA & 1e-3\\ 
         & RWKV-4 &  &  &  &  & WCA & 6e-4\\ 
         & LSTM-SM &  &  &  &  & WCA & 1e-3\\ 
         & xLSTM[1:0] &  &  &  &  & WCA & 1e-3 \\ 
    \midrule
        \multirow{9}{*}{Pathfinder} & Transformer & \multirow{9}{*}{6} & \multirow{9}{*}{192} & \multirow{9}{*}{64} & \multirow{9}{*}{500K} & WCAR & 1e-4 \\ 
         & Llama &  &  &  &  & WCAR & 1e-4\\ 
         & Mamba &  &  &  &  & WCA & 1e-3\\ 
         & LSTM &  &  &  &  & X & X\\ 
         & RWKV-4 &  &  &  &  & WCA & 1e-4\\ 
         & LSTM-SM &  &  &  &  & X & X\\ 
         & xLSTM[1:0] &  &  &  &  & WCA & 1e-4 \\ 
    \midrule
        \multirow{9}{1.3cm}{G-Image} & Transformer & \multirow{9}{*}{6} & \multirow{9}{*}{512} & \multirow{9}{*}{64} & \multirow{9}{*}{180K} & WCAR & 6e-4 \\ 
         & Llama &  &  &  &  & WCAR & 6e-4\\ 
         & Mamba &  &  &  &  & WCA & 6e-4\\ 
         & LSTM &  &  &  &  & WCAR & 1e-3\\ 
         & LSTM &  &  &  &  & WCAR & 1e-3\\ 
         & RWKV-4 &  &  &  &  & WCA & 1e-3\\ 
         & LSTM-SM &  &  &  &  & WCA & 6e-4\\ 
         & xLSTM[0:1] &  &  &  &  & WCAR & 1e-3\\ 
    \midrule
        \multirow{9}{1.3cm}{RGB-Image} & Transformer & \multirow{9}{*}{6} & \multirow{9}{*}{512} & \multirow{9}{*}{64} & \multirow{9}{*}{180K} & WCA & 6e-4 \\ 
         & Llama &  &  &  &  & WCAR & 1e-4\\ 
         & Mamba &  &  &  &  & WCAR & 1e-3\\ 
         & LSTM &  &  &  &  & WCAR & 1e-3\\ 
         & RWKV-4 &  &  &  &  & WCAR & 6e-4\\ 
         & LSTM-SM &  &  &  &  & WCA & 6e-4\\ 
         & xLSTM[0:1] &  &  &  &  & WCAR & 1e-3\\        
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{List detailing all the hyperparameters used in each of the Long Range Arena benchmark. "X" denotes models which fail to outperform the random baseline. \JB{Not sure if we need this table at all} 
    %\GK{remove vertical lines to fit style of other tables}
    }
    \label{tab:lra_hyperparameters}
\end{table}