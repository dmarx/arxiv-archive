---
author:
- |
  Kaiser Sun Mark Dredze  
  Johns Hopkins University  
  Baltimore, MD USA  
  `{hsun74,mdredze}@cs.jhu.edu`  
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  Amuro & Char: Analyzing the Relationship between  
  Pre-Training and Fine-Tuning of Large Language Models
---





# Introduction

# Background: Model Training

# Experimental Setup

# Discussion

# Conclusion

# Limitations

We discuss the weaknesses and limitations in the following section.

#### Computing Resource

Due to computational constraints, we can only conduct experiments on a 1B model and a limited amount of datasets. The amount of GPU hours spent for each experiment in this study is listed in Table .

#### Availbility of Pre-training Checkpoints

This study would benefit significantly from including a broader spectrum of models, but the public pre-training checkpoint releases are limited. Open-source LLMs with intermediate checkpoint release include OLMo , TinyLLAMA, RedPajama-Incite, OpenLM, and Pythia. After a series of preliminary experiments, we select these models’ best-performing and robust families.

#### Scaling Law

Recent research shows that the model may resemble emergent capability when scaled to a certain size. Comparatively, find that smaller model is capable of outperforming its larger variant when the computing resources is controlled. To avoid potential confounding factors caused by quantization, our experiments are only conducted on the one-billion model, which may, therefore, conceal the emergent capability brought by larger models while at least giving insights about the potential of small models.

#### Analysis Protocol

show that the evaluation result may be affected by samples that have been memorized by the model during training instead of revealing the reasoning capability. The only analysis protocol used is the downstream performance of a trained model. More investigation should be done into model internals during pre-training dynamics and how they relate to the effects of fine-tuning.

#### Training Paradigm

Although multiple tuning strategies exist, to create a fair comparison environment where checkpoints received the same amount of training, models are fine-tuned with a fixed amount of epochs in this work. On different pre-training stages, the model may converge at a different speed. Further study can be done to study the effect of pre-training on different fine-tuning methods or fine-tuning dynamics in different pre-training stages. We only explored the scenario of full-parameter fine-tuning. Whether parameter-efficient fine-tuning or human preference tuning will lead to a different conclusion also remains an open question.

#### Randomness

In this study, we only assess uncertainty with Bootstrap during evaluation. However, uncertainty may emerge during training, which poses optimizer initialization and data ordering. Due to the computational constraints, we cannot reduce the randomness factor on this angle.

# Acknowledgments

The authors thank Saleh Soltan, Niyati Bafna, Fan Bai, Miriam Wanner, Xinbo Wu, Carlos Aguirre for their helpful feedback.
