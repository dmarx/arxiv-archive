We begin with a brief survey of the core components of LLM training: pre-training, fine-tuning, and instruction fine-tuning. 
We also discuss the related topic of in-context learning as well as different efficient fine-tuning strategies.

We use ``model alignment'' as a general term for techniques that align a model with a desired behavior, which can be accomplished by fine-tuning models after pretraining. The term is also associated with other definitions \cite{shen2024bidirectional}. 
We also note several related studies that explore training dynamics to understand model behavior \cite{tirumala2022memorization, chen2023sudden, tian2023scan}.
With this in mind, we conduct an empirical study on how the amount of pre-training affects the effectiveness of fine-tuning.

\paragraph{Pre-training}
The first step of training a LLM is pre-training on a massive text corpus
 \cite{achiam2023gpt, touvron2023llama, groeneveld2024olmo}.
For decoder-only models in the GPT family, the subject of our paper, work since the introduction of GPT-2 \cite{radford2019language} has focused on scaling up model training. 
Initial work increased model size to hundreds of billions of parameters \cite{brown2020language, rae2021scaling, chowdhery2023palm}, along with explorations in model size, training corpus size, and training data characteristics \cite{hoffmann2022training, gururangan-etal-2020-dont}. 
Since the push towards large models, work has shifted to increasing the amount of pre-training data \cite{together2023redpajama, soldaini2024dolma}, with new models now reaching 15 trillion tokens \cite{llama3modelcard}. 
Studies of model performance on various tasks at different model sizes introduced the idea of emergent model abilities \cite{wei2022emergent}, with new model abilities being revealed as model training grows.

We also recognize a particularly important trend for this paper: model openness. 
Early LLMs were proprietary models accessible only through an API. 
The first large open model, Bloom \cite{bloom-strom-etal-2023-preparing}, allowed widespread LLM evaluation. 
Subsequent open models, such as OPT \cite{zhang2022opt}, LLaMA \cite{touvron2023llama,keles-bayrakli-2024-llama-2} and others \cite{biderman2023pythia, openlm, almazrouei2023falcon}, have become the norm. 
In this paper, we study OLMo \cite{groeneveld2024olmo}, one of the only models to release individual pre-training checkpoints.

\paragraph{Fine-Tuning}
Early work on instruction fine-tuning using reinforcement learning with human feedback (RLHF) \cite{ziegler2019fine, stiennon2020learning, ouyang2022training} demonstrates the dramatic effect that model alignment could have on a pre-training model. 
When a specific task of interest has been identified, supervised fine-tuning can improve a pre-trained model. 
Task-agnostic tuning became popularized with the advent of T5 models \citep{raffel2020exploring}, where a pre-trained LLM is tuned using a general text-to-text solution. 
When multiple tasks are given to the model, the model is commonly given a task-specific prefix or an instruction along with the task input, leading to the development of various methods of prefix tuning \cite{li-liang-2021-prefix} and instruction tuning \cite{wei2021finetuned, mishra-etal-2022-cross, victor2022multitask}.

\paragraph{Instruction Fine-Tuning}
Instruction fine-tuning is preferred when more general model behaviors are desired. 
Popularized through reinforcement-learning with human feedback (RLHF) \cite{christiano2017deep, ziegler2019fine, stiennon2020learning, ouyang2022training} and reinforcement-learning with AI feedback (RLAIF) \cite{lee2023rlaif}, these methods utilize a reward model to simulate human feedback.
Others explore human preference tuning without a reward model \cite{rafailov2024direct, song2024preference, xu2024contrastive}, or study the effects of these tuning methods \citep{shen2024bidirectional,perez-etal-2023-discovering}.
\citet{sharma2024critical} show that supervised fine-tuning can lead to similar performance as RLAIF.

\paragraph{In-Context Learning}
While not the subject of this paper since it does not make changes to model parameters, in-context learning utilizes a small amount of supervised data to improve model performance. 
ICL, also called few-shot learning, 
is also used as an evaluation strategy where the model is given a prompt composed of examples of tasks expected to be solved. The underlying model is evaluated based on its response to the input. 
ICL can benefit from a larger context window that adds more examples, which can spur work on the development of model quantization techniques \cite{dettmers2022gpt3} and the alleviation of hardware constraints \cite{brown2020language, xie2021explanation, min-etal-2022-rethinking}.

\paragraph{Fine-Tuning Techniques}
While model pre-training can be done by a few groups with large resources interested in developing new models, fine-tuning depends on the task and is of broad interest. Therefore, many techniques facilitate time-, memory-, and data-efficient model training through parameter-efficient fine-tuning (PEFT) \cite{hu2021lora, liu2021p, liu2023gpt}, quantization \cite{jacob2018quantization, dettmers2022gpt3, dettmers2024qlora}, and specialized data filtering \cite{xia2024less, zhou2024lima, attendu-corbeil-2023-nlu}.
This paper focuses specifically on full-parameter fine-tuning, while our findings suggest the potential for data-efficient and budget-friendly training by understanding the critical turning point of model training.
Our findings are closely related to the recent study on \textit{phase transition} of model training \cite{olsson2022context, wei2022emergent, chen2023sudden}.






