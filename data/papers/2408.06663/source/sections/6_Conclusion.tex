Our experiments explore the relationship between fine-tuning and pre-training LLMs.
Our findings span from the latent benefits of pretraining to model learning and forgetting during fine-tuning.
Our results show that the model can rapidly pick up the datasets that it could not solve during fine-tuning with only a small amount of supervision.
In the meantime, we identify the aspects that LLM learns and forgets during supervised fine-tuning: task format, task solution, and domain knowledge.
Overall, our results demonstrate the value of analyzing language model training dynamics, and we would like to call for the release of pre-training checkpoints to aid future studies.