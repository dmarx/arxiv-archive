Our study uses fine-tuning of pre-training model checkpoints to understand the dynamics of pre-training and fine-tuning on model performance. 
While our insights suggest directions for future work, we note important limitations inherent in our experiments. 
This study considered a single, relatively small LLM on less than a dozen datasets, and still consumed thousands of hours of GPU training time at significant expense. 
Future work needs to confront these issues on larger models and more datasets. We believe our experiments can focus future work on specific experiments with larger models.


{\em Some datasets can be learned without fine-tuning.}
We discover a dichotomy between datasets. 
Some are learned during model pre-training, while others show no improvements during pre-training. 
Furthermore, the datasets learned during pre-training do not benefit from fine-tuning. 
This observation, combined with our study about what is learned during fine-tuning (Section \ref{sec:finding:what}) suggests that some tasks are presented in a manner that aligns with what the model sees during pre-training, and thus fine-tuning provides no additional information. 
While we could identify what about the tasks placed them in the learned or not learnable during pre-training group, it may be possible to format tasks in a manner that better aligns with pre-training and makes them learnable.

{\em Pre-training models can improve in undetectable ways without fine-tuning.}
Some datasets are not learnable during pre-training but benefit significantly from fine-tuning (\sect{sec:finding:base-eval}). 
However, these datasets still benefited from additional pre-training, even though those benefits were not revealed without fine-tuning (\sect{sec:finding:PTFT}).
Clearly, the model is learning important information about the task, even though it cannot express that information.
The identification of a measure available during pre-training that correlated with post-fine-tuning task performance could be used to guide pre-training and produce models that did better post-fine-tuning. 
Perhaps there is a way in which information about these tasks can be included in pre-training, allowing the model to better utilize the massive amount of pre-training data.
For example, early stopping during pre-training could lead to better utilization of limited training resources if we knew when to stop.


{\em Fine-tuning teaches task format but leads to forgetting unused abilities.}
Our results show that fine-tuning guides the model to understand the format and complete a given task. As this information diminishes, the model's overall ability improves. 
However, fine-tuning comes at the expense of other model abilities, such as the capability of performing on tasks or domains that are unrelated to the fine-tuning task. 
This insight can be helpful in our understanding of the multitask abilities of LLMs, where certain tasks can introduce conflicts during multi-task training \cite{mueller-etal-2022-text}.








