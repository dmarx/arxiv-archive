
The development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks.
In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the model resembles high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated by more pre-training.
\footnote{Code, results, and data to reproduce the experiments are available at \href{https://anonymous.4open.science/r/AmuroCharRelease-DEC5}{https://anonymous.4open.science\\/r/AmuroCharRelease-DEC5}. All the model checkpoints resulting from this work are available at \href{https://huggingface.co/KaiserWhoLearns/PTvsSFT_OLMo1b}{https://huggingface.co/KaiserWhoLearns/PTvsSFT\_OLMo1b}}