\section{Hyperparameter Tuning}
\label{sec:app:hyperparameter}
For both supervised fine-tuning and instruction tuning, we pre-set the effective batch size to 8, and tune the learning rate within \{$2\times 10^{-5}$, $2\times 10^{-6}$, $2\times 10^{-7}$\}.
Each model is fine-tuned for 3 epochs on the supervised fine-tuning tasks and 5 epochs on Tulu for instruction tuning.
In both settings, we adopt an AdamW optimizer with a linear learning rate scheduler.
The optimizer is warmed up for the first $3\%$ of the training time.

\section{Prediction Generation Method}
\label{app:pred_gen}
\input{tables/app_pred_gene_method}
For classification tasks, we examine three different prediction generation methods: Free Generation (\texttt{Free}), Constrained Generation (\texttt{Constrained}), and Token Probability (\texttt{TokenProb}), the results are shown in Table~\ref{app:tab:pred_gen_method}.
In \texttt{Constrained}, we force the output to include at least one label in the acceptable label set.
In \texttt{TokenProb}, we compare the logits of acceptable labels, and select the label with the highest score as final output.
This ablation study is conducted only on the BASE and fine-tuned versions of final checkpoint of the pre-trained model.
We find that, although prediction generation methods bring less effect to the evaluation result of a fine-tuned model, BASE variants suffer much more from not knowing the desired output.
Therefore, we proceed all the classification experiments with \texttt{TokenProb}.

\subsection{Label and Tokenizations}
Depending on the tokenizer variant, the label text may be tokenized differently, leading to evaluation unreliability.
For example, in paraphrase detection, the model could assign probability on both ``\texttt{yes}" and ``\texttt{ yes}" (the same label with a prefix space).
This behavior is reported and explored in various related work \cite{sun-etal-2023-tokenization, batsuren2024evaluating, singh2024tokenization}.
In this study, we leniently regard all individual tokens that contains the whole label or part of the label along with some special charcters that do not affect the semantic as an acceptable target label.


\section{Task Format}
We adopt the task format from \cite{yang2024unveiling}, with an additional task format of input-output.
How each dataset is formated can be found in Table~\ref{tab:app:promptformat}.

\section{GPU Hours per-Experiment}
\label{sec:app:gpuhours}
We show a table of GPU hours spent for each experiment in Table~\ref{app:tab:GPU-hours}.
The total number of GPU hours spent in this project is approximately 1067 A100 hours. We lose track of the GPU hours spent on preliminary experiments, so a lower-bound estimation is reported.
\input{tables/app_gpu_hours}

\section{Per-dataset Figures}
\label{sec:app:per-ds-ckpt-figures}
\input{figures/app_IT_ckpt_perf}
\input{figures/app_FT_ckpt_perf}
We show the model performance on each dataset after supervised fine-tuning and instruction tuning correspondingly in Figure~\ref{fig:sft-ckpt-perf} and Figure~\ref{fig:it-ckpt-perf}.
The datasets that already show improvement during pre-training do not benefit from fine-tuning, while performance improve drastically on the datasets that the model has never learned during pre-training.

\paragraph{Out-of-domain Generalization}
The out-of-domain performance for each dataset with respect to pre-training steps is shown in Figure~\ref{fig:ood-sft-ckpt-perf}.
Overall, the model generalizes well after fine-tuning on NLI tasks, while its performance deteriorates when evaluated on out-of-domain paraphrase detection tasks.
\input{figures/app_ood_ckpt_perf}

\paragraph{Cross-task Generalization}
The cross-task performance for each dataset with respect to pre-training steps is shown in Figure~\ref{fig:cross-task-ckpt-perf-class} and Figure~\ref{fig:cross-task-ckpt-perf-gen}.
\input{figures/app_crosstask_ckpt_perf}

\paragraph{Task-Format}
The performance of models on evaluation sets formatted with different prompt formatting method is shown in Figure~\ref{fig:app:task_format}.

\input{tables/app_promptformat}
\input{figures/app_task_format}

\section{License of Artifacts}
We include the license of artifacts used in this paper in Table~\ref{app:tab:artifact}
\begin{table*}[t]
\scriptsize
\centering
\begin{tabular}{ccc|cc}
\toprule
\textbf{Name} & \textbf{License} &  & \textbf{Name} & \textbf{License} \\ \midrule
OLMo-1b & Apache 2.0 &  & SocialIQa & CC-BY \\
TULU & ODC-BY &  & \text{  }CNN/DailyMail & Apache 2.0 \\
ARC & CC BY-SA &  & TweetQA & CC BY-SA-4.0 \\
OpenbookQA & Apache 2.0 &  & MNLI & CC-BY-3.0 \\
Hellaswag & MIT &  & GPT3NLI & MIT \\
BoolQ & Apache 2.0 &  & RTE & N/A \\
SciQ & CC-BY-NC-3.0 &  & Paws & Free \\
XSum & MIT &  & QQP & Non-Commercial \\
XLSum & CC-BY-NC-SA 4.0 \text{  } &  & STS-B & Other \\ \bottomrule
\end{tabular}
\caption{License of artifacts used in this paper.}
\label{app:tab:artifact}
\end{table*}

\section{Performance Difference Numbers}
\label{sec:app:performance-numbers}
\input{tables/gain_table_by_steps}
The average performance change before and after fine-tuning for each checkpoint is shown in Table~\ref{tab:gain-tab-by-step}.
The data in this table is used to create Figure~\ref{fig:finding:ptftcompare}.

\section{Full Performance Table}
Due to availability of space and the amount of fine-tuned checkpoints, we omit displaying all exact metric values in the paper.
The performance of each fine-tuned variant on each dataset can be found in the \texttt{csv} file in the code base.


\section{Generalization Taxonomy}
\label{sec:app:generalization-taxo}
Following the generalization taxonomy in \citealp{hupkes2023taxonomy}, the evaluation card is included in Table~\ref{app:tab:gen_eval_card}.
\input{tables/app-genbench-table}