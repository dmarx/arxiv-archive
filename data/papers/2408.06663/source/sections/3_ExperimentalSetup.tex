In this section, we describe the model and datasets used. 
The hyperparameter tuning procedure and setup for each fine-tuning setting can be found in Appendix~\ref{sec:app:hyperparameter}.

\subsection{Model Choice} 
Our paper considers OLMo-1B \cite{groeneveld2024olmo}, a high-performing open-source large language model. 
Ideally, we would evaluate multiple models, but OLMo is the only model to release intermediate pre-training checkpoints, and thus the only model that supports our analysis\footnote{\href{https://github.com/allenai/OLMo/tree/main/checkpoints/official}{https://github.com/allenai/OLMo/tree/main/checkpoints}} \footnote{We also experimented with RedPajama-INCITE (\href{https://www.together.ai/blog/redpajama-models-v1}{https://www.together.ai/blog/redpajama-models-v1}), which is the only other model to release checkpoints. After extensive experiments, we found it performed worse than OLMo, given the training data available, and did not support our analysis. Several other models claim to release training checkpoints but have not done so.}.
Despite being the only open model with training checkpoints, it fortunately has several desirable properties. First, 
the model is fully open, including the training details, pre-training data, and fine-tuning data. 
Second, the smaller model size allows us to train a model efficiently on a single A100 GPU. While evaluating a larger model would be desirable, we limit our study to the 1B model given the much larger computational demand of multi-GPU training. Our detailed analysis required significant GPU resources, which would have been prohibitive with a larger model.
We also note that OLMo-1B compares very favorably to the larger version, and recent work has shown that small models can compete with larger ones \cite{team2024gemma}.

We select model pre-training checkpoints uniformly from the pre-training history along with the first and the final checkpoints.

\subsection{Training Procedure}
We fine-tune each of the selected model checkpoints using two different procedures to create fine-tuned models: supervised fine-tuning and instruction tuning. 
The supervised fine-tuning is conducted separately for each model checkpoint and dataset, while the instructing fine-tuning is done once using the instruction dataset.
The instruction-tuned model is evaluated on a suite of LLM benchmarks.

\paragraph{Supervised Fine-tuning}
We adapt the dataset choice from \citealp{yang2024unveiling} for supervised fine-tuning. 
For each in-domain dataset, one to two cross-domain evaluation datasets are supplied.
Each pre-training checkpoint is fully fine-tuned for 3 epochs with a batch size of 8 and learning rates resulting from minimal hyperparameter tuning. 
Each task is formatted using a default prompt-completion format (Table~\ref{tab:app:promptformat}).

\paragraph{Instruction Fine-Tuning}
We instruction-tune the model on T\"{U}LU \cite{ivison2023camels}, following the decision of \citealp{groeneveld2024olmo}.
Each model checkpoint is fully fine-tuned for 5 epochs with a batch size of 8 and a learning rate of $2\times 10^{-6}$.

\input{tables/tasks}
\subsection{Evaluation}
The evaluation challenge is to select a representative number of datasets for different types of tasks to test model abilities, recognizing that each dataset requires evaluating each model checkpoint and its fine-tuned counterparts. 
We also select datasets based on the availability of in-domain and out-of-domain samples.

\paragraph{Datasets}
The datasets are summarized in Table ~\ref{tab:tasks}.
We evaluate the model with an in-domain test set and one or two out-of-domain test sets for each of the supervised fine-tuning tasks.
We conduct experiments on the tasks of summary generation \cite{narayan-etal-2018-dont, hasan-etal-2021-xl, hermann2015teaching}, question generation \cite{sap-etal-2019-social, xiong-etal-2019-tweetqa, welbl2017crowdsourcing}, natural language inference \cite{williams-etal-2018-broad, wang-etal-2018-glue, dagan2006pascal, bar2006second, giampiccolo2007third, bentivogli2009fifth}, and paraphrase detection \cite{zhang-etal-2019-paws, wang-etal-2018-glue, agirre2007semantic}.
Each training set is sub-sampled to a size of 6,000 for fair comparisons.

In instruction fine-tuning, we base our downstream evaluation settings on \citealp{groeneveld2024olmo}, as OLMo is found to have stable performance on these datasets.
The instruction-tuned models are evaluated on ARC (both \texttt{arc easy} and \texttt{arc challenge}) \cite{clark2018think}, OpenbookQA \cite{mihaylov-etal-2018-suit}, Hellaswag \cite{zellers-etal-2019-hellaswag}, BoolQ \cite{clark-etal-2019-boolq}, and SciQ \cite{welbl2017crowdsourcing}. 


\paragraph{Metrics}
We use accuracy \cite{scikit-learn} for classification tasks and \textsc{Rouge-L} \cite{lin-2004-rouge} for generation tasks.
We set the maximum amount of newly generated tokens to 5 for classification tasks and 60 for generation tasks.
Outputs are generated with greedy decoding.
For classification tasks, we experiment with both constrained decoding and logit-based predictions.
We find the best performance by selecting the label with the highest logit of its first subtoken (Appendix~\ref{app:pred_gen}).


