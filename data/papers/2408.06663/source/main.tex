\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}

\newcommand{\fan}[1]{\textcolor{blue}{\bf\small [#1 --Fan]}}


\definecolor{snsblue}{HTML}{4c72b0}
\definecolor{snsorange}{HTML}{dd8452}
\definecolor{snsgreen}{HTML}{66c2a5}
\definecolor{snslightorange}{HTML}{fc8d62}
\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{inconsolata}
\usepackage{colortbl}
\usepackage{xcolor}          
\usepackage{amssymb} 
\usepackage{subcaption}
\usepackage{subfloat}
\usepackage{graphicx}
 \usepackage{amsmath}
\newcommand\sect[1]{\S\ref{#1}}
\newcommand{\priortodo}[1]{{\color{blue} [TODO: #1]}}
\newcommand{\kaiser}[1]{{\color{cyan} [Kaiser: #1]}}
\newcommand{\saleh}[1]{{\color{green} [Saleh: #1]}}
\newcommand{\niyati}[1]{{\color{teal} [Niyati: #1]}}
\newcommand{\carlos}[1]{{\color{purple} [Carlos: #1]}}

\usepackage{tablefootnote}

\title{Amuro \& Char: Analyzing the Relationship between \\ Pre-Training and Fine-Tuning of Large Language Models}

 \author{Kaiser Sun \quad Mark Dredze \\
 Johns Hopkins University
 \\ Baltimore, MD USA \\
\texttt{\{hsun74,mdredze\}@cs.jhu.edu}\\
}

\begin{document}
\maketitle
\begin{abstract}
\input{sections/0_abstract}
\end{abstract}


\section{Introduction}
\input{sections/1_Introduction}

\section{Background: Model Training}

\input{sections/2_RelatedWork}

\section{Experimental Setup}
\input{sections/3_ExperimentalSetup}

\input{sections/4_Findings}



\section{Discussion}
\input{sections/5_Discussion}


\section{Conclusion}
\input{sections/6_Conclusion}

\section*{Limitations}
We discuss the weaknesses and limitations in the following section.

\paragraph{Computing Resource}
Due to computational constraints, we can only conduct experiments on a 1B model and a limited amount of datasets.
The amount of GPU hours spent for each experiment in this study is listed in Table~\ref{app:tab:GPU-hours}.

\paragraph{Availbility of Pre-training Checkpoints}
This study would benefit significantly from including a broader spectrum of models, but the public pre-training checkpoint releases are limited.
Open-source LLMs with intermediate checkpoint release include OLMo \cite{groeneveld2024olmo}, TinyLLAMA, RedPajama-Incite, OpenLM, and Pythia.
After a series of preliminary experiments, we select these models' best-performing and robust families.

\paragraph{Scaling Law}
Recent research shows that the model may resemble emergent capability \cite{wei2022emergent} when scaled to a certain size.
Comparatively, \citealp{hassid2024larger} find that smaller model is capable of outperforming its larger variant when the computing resources is controlled.
To avoid potential confounding factors caused by quantization, our experiments are only conducted on the one-billion model, which may, therefore, conceal the emergent capability brought by larger models while at least giving insights about the potential of small models.

\paragraph{Analysis Protocol}
\citealp{wu2023reasoning} show that the evaluation result may be affected by samples that have been memorized by the model during training instead of revealing the reasoning capability.
The only analysis protocol used is the downstream performance of a trained model.
More investigation should be done into model internals during pre-training dynamics and how they relate to the effects of fine-tuning.

\paragraph{Training Paradigm}
Although multiple tuning strategies exist, to create a fair comparison environment where checkpoints received the same amount of training, models are fine-tuned with a fixed amount of epochs in this work.
On different pre-training stages, the model may converge at a different speed.
Further study can be done to study the effect of pre-training on different fine-tuning methods or fine-tuning dynamics in different pre-training stages.
We only explored the scenario of full-parameter fine-tuning.
Whether parameter-efficient fine-tuning or human preference tuning will lead to a different conclusion also remains an open question.

\paragraph{Randomness}
In this study, we only assess uncertainty with Bootstrap during evaluation.
However, uncertainty may emerge during training, which poses optimizer initialization and data ordering.
Due to the computational constraints, we cannot reduce the randomness factor on this angle.

\section*{Acknowledgments}
The authors thank Saleh Soltan, Niyati Bafna, Fan Bai, Miriam Wanner, Xinbo Wu, Carlos Aguirre for their helpful feedback.

\begin{thebibliography}{88}
    \providecommand{\natexlab}[1]{#1}
    
    \bibitem[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt}
    Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.
    \newblock Gpt-4 technical report.
    \newblock \emph{arXiv preprint arXiv:2303.08774}.
    
    \bibitem[{Agirre et~al.(2007)Agirre, M`arquez, and Wicentowski}]{agirre2007semantic}
    Eneko Agirre, Llu'{i}s M`arquez, and Richard Wicentowski, editors. 2007.
    \newblock \emph{Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)}.
    \newblock Association for Computational Linguistics, Prague, Czech Republic.
    
    \bibitem[{AI@Meta(2024)}]{llama3modelcard}
    AI@Meta. 2024.
    \newblock \href {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md} {Llama 3 model card}.
    
    \bibitem[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon}
    Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023.
    \newblock The falcon series of open language models.
    \newblock \emph{arXiv preprint arXiv:2311.16867}.
    
    \bibitem[{Attendu and Corbeil(2023)}]{attendu-corbeil-2023-nlu}
    Jean-michel Attendu and Jean-philippe Corbeil. 2023.
    \newblock \href {https://doi.org/10.18653/v1/2023.sustainlp-1.9} {{NLU} on data diets: Dynamic data subset selection for {NLP} classification tasks}.
    \newblock In \emph{Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)}, pages 129--146, Toronto, Canada (Hybrid). Association for Computational Linguistics.
    
    \bibitem[{Bar~Haim et~al.(2006)Bar~Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, and Szpektor}]{bar2006second}
    Roy Bar~Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
    \newblock The second {PASCAL} recognising textual entailment challenge.
    
    \bibitem[{Batsuren et~al.(2024)Batsuren, Vylomova, Dankers, Delgerbaatar, Uzan, Pinter, and Bella}]{batsuren2024evaluating}
    Khuyagbaatar Batsuren, Ekaterina Vylomova, Verna Dankers, Tsetsuukhei Delgerbaatar, Omri Uzan, Yuval Pinter, and G{\'a}bor Bella. 2024.
    \newblock Evaluating subword tokenization: Alien subword composition and oov generalization challenge.
    \newblock \emph{arXiv preprint arXiv:2404.13292}.
    
    \bibitem[{Bentivogli et~al.(2009)Bentivogli, Dagan, Dang, Giampiccolo, and Magnini}]{bentivogli2009fifth}
    Luisa Bentivogli, Ido Dagan, Hoa~Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009.
    \newblock The fifth {PASCAL} recognizing textual entailment challenge.
    
    \bibitem[{Bianchi et~al.(2023)Bianchi, Suzgun, Attanasio, R{\"o}ttger, Jurafsky, Hashimoto, and Zou}]{bianchi2023safety}
    Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R{\"o}ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. 2023.
    \newblock Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.
    \newblock \emph{arXiv preprint arXiv:2309.07875}.
    
    \bibitem[{Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff et~al.}]{biderman2023pythia}
    Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al. 2023.
    \newblock Pythia: A suite for analyzing large language models across training and scaling.
    \newblock In \emph{International Conference on Machine Learning}, pages 2397--2430. PMLR.
    
    \bibitem[{Bloom~Str{\"o}m et~al.(2023)Bloom~Str{\"o}m, Slater, Zahran, Berdicevskis, and Schumacher}]{bloom-strom-etal-2023-preparing}
    Eva-Marie Bloom~Str{\"o}m, Onelisa Slater, Aron Zahran, Aleksandrs Berdicevskis, and Anne Schumacher. 2023.
    \newblock \href {https://aclanthology.org/2023.clasp-1.7} {Preparing a corpus of spoken {X}hosa}.
    \newblock In \emph{Proceedings of the 2023 CLASP Conference on Learning with Small Data (LSD)}, pages 62--67, Gothenburg, Sweden. Association for Computational Linguistics.
    
    \bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
    Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.
    \newblock Language models are few-shot learners.
    \newblock \emph{Advances in neural information processing systems}, 33:1877--1901.
    
    \bibitem[{Chen et~al.(2023)Chen, Schwartz-Ziv, Cho, Leavitt, and Saphra}]{chen2023sudden}
    Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew~L Leavitt, and Naomi Saphra. 2023.
    \newblock Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms.
    \newblock \emph{arXiv preprint arXiv:2309.07311}.
    
    \bibitem[{Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2023palm}
    Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2023.
    \newblock Palm: Scaling language modeling with pathways.
    \newblock \emph{Journal of Machine Learning Research}, 24(240):1--113.
    
    \bibitem[{Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei}]{christiano2017deep}
    Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017.
    \newblock Deep reinforcement learning from human preferences.
    \newblock \emph{Advances in neural information processing systems}, 30.
    
    \bibitem[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark-etal-2019-boolq}
    Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.
    \newblock \href {https://doi.org/10.18653/v1/N19-1300} {{B}ool{Q}: Exploring the surprising difficulty of natural yes/no questions}.
    \newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 2924--2936, Minneapolis, Minnesota. Association for Computational Linguistics.
    
    \bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think}
    Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.
    \newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
    \newblock \emph{arXiv preprint arXiv:1803.05457}.
    
    \bibitem[{Computer(2023)}]{together2023redpajama}
    Together Computer. 2023.
    \newblock \href {https://github.com/togethercomputer/RedPajama-Data} {Redpajama: an open dataset for training large language models}.
    
    \bibitem[{Dagan et~al.(2006)Dagan, Glickman, and Magnini}]{dagan2006pascal}
    Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006.
    \newblock The {PASCAL} recognising textual entailment challenge.
    \newblock In \emph{Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment}, pages 177--190. Springer.
    
    \bibitem[{Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer}]{dettmers2022gpt3}
    Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022.
    \newblock Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.
    \newblock \emph{Advances in Neural Information Processing Systems}, 35:30318--30332.
    
    \bibitem[{Dettmers et~al.(2024)Dettmers, Pagnoni, Holtzman, and Zettlemoyer}]{dettmers2024qlora}
    Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024.
    \newblock Qlora: Efficient finetuning of quantized llms.
    \newblock \emph{Advances in Neural Information Processing Systems}, 36.
    
    \bibitem[{Giampiccolo et~al.(2007)Giampiccolo, Magnini, Dagan, and Dolan}]{giampiccolo2007third}
    Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007.
    \newblock The third {PASCAL} recognizing textual entailment challenge.
    \newblock In \emph{Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing}, pages 1--9. Association for Computational Linguistics.
    
    \bibitem[{Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang et~al.}]{groeneveld2024olmo}
    Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al. 2024.
    \newblock Olmo: Accelerating the science of language models.
    \newblock \emph{arXiv preprint arXiv:2402.00838}.
    
    \bibitem[{Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo, Beltagy, Downey, and Smith}]{gururangan-etal-2020-dont}
    Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug Downey, and Noah~A. Smith. 2020.
    \newblock \href {https://doi.org/10.18653/v1/2020.acl-main.740} {Don{'}t stop pretraining: Adapt language models to domains and tasks}.
    \newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 8342--8360, Online. Association for Computational Linguistics.
    
    \bibitem[{Gururangan et~al.(2023)Gururangan, Wortsman, Gadre, Dave, Kilian, Shi, Mercat, Smyrnis, Ilharco, Jordan, Heckel, Dimakis, Farhadi, Shankar, and Schmidt}]{openlm}
    Suchin Gururangan, Mitchell Wortsman, Samir~Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt. 2023.
    \newblock \href {https://github.com/mlfoundations/open_lm/} {{open\_lm}: a minimal but performative language modeling (lm) repository}.
    \newblock GitHub repository.
    
    \bibitem[{Hasan et~al.(2021)Hasan, Bhattacharjee, Islam, Mubasshir, Li, Kang, Rahman, and Shahriyar}]{hasan-etal-2021-xl}
    Tahmid Hasan, Abhik Bhattacharjee, Md.~Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M.~Sohel Rahman, and Rifat Shahriyar. 2021.
    \newblock \href {https://doi.org/10.18653/v1/2021.findings-acl.413} {{XL}-sum: Large-scale multilingual abstractive summarization for 44 languages}.
    \newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pages 4693--4703, Online. Association for Computational Linguistics.
    
    \bibitem[{Hassid et~al.(2024)Hassid, Remez, Gehring, Schwartz, and Adi}]{hassid2024larger}
    Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, and Yossi Adi. 2024.
    \newblock The larger the better? improved llm code-generation via budget reallocation.
    \newblock \emph{arXiv preprint arXiv:2404.00725}.
    
    \bibitem[{Hermann et~al.(2015)Hermann, Kocisky, Grefenstette, Espeholt, Kay, Suleyman, and Blunsom}]{hermann2015teaching}
    Karl~Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015.
    \newblock Teaching machines to read and comprehend.
    \newblock \emph{Advances in neural information processing systems}, 28.
    
    \bibitem[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training}
    Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.
    \newblock Training compute-optimal large language models.
    \newblock \emph{arXiv preprint arXiv:2203.15556}.
    
    \bibitem[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora}
    Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.
    \newblock Lora: Low-rank adaptation of large language models.
    \newblock \emph{arXiv preprint arXiv:2106.09685}.
    
    \bibitem[{Hupkes et~al.(2023)Hupkes, Giulianelli, Dankers, Artetxe, Elazar, Pimentel, Christodoulopoulos, Lasri, Saphra, Sinclair et~al.}]{hupkes2023taxonomy}
    Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, et~al. 2023.
    \newblock A taxonomy and review of generalization research in nlp.
    \newblock \emph{Nature Machine Intelligence}, 5(10):1161--1174.
    
    \bibitem[{Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy et~al.}]{ivison2023camels}
    Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al. 2023.
    \newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2.
    \newblock \emph{arXiv preprint arXiv:2311.10702}.
    
    \bibitem[{Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and Kalenichenko}]{jacob2018quantization}
    Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018.
    \newblock Quantization and training of neural networks for efficient integer-arithmetic-only inference.
    \newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2704--2713.
    
    \bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral}
    Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.
    \newblock Mistral 7b.
    \newblock \emph{arXiv preprint arXiv:2310.06825}.
    
    \bibitem[{Keles and Bayrakl{\i}(2024)}]{keles-bayrakli-2024-llama-2}
    Onur Keles and Omer~Turan Bayrakl{\i}. 2024.
    \newblock \href {https://aclanthology.org/2024.finnlp-1.21} {{LL}a{MA}-2-econ: Enhancing title generation, abstract classification, and academic {Q}{\&}{A} in economic research}.
    \newblock In \emph{Proceedings of the Joint Workshop of the 7th Financial Technology and Natural Language Processing, the 5th Knowledge Discovery from Unstructured Data in Financial Services, and the 4th Workshop on Economics and Natural Language Processing @ LREC-COLING 2024}, pages 212--218, Torino, Italia. ELRA and ICCL.
    
    \bibitem[{Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi}]{lee2023rlaif}
    Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023.
    \newblock Rlaif: Scaling reinforcement learning from human feedback with ai feedback.
    \newblock \emph{arXiv preprint arXiv:2309.00267}.
    
    \bibitem[{Li and Liang(2021)}]{li-liang-2021-prefix}
    Xiang~Lisa Li and Percy Liang. 2021.
    \newblock \href {https://doi.org/10.18653/v1/2021.acl-long.353} {Prefix-tuning: Optimizing continuous prompts for generation}.
    \newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, Online. Association for Computational Linguistics.
    
    \bibitem[{Lin(2004)}]{lin-2004-rouge}
    Chin-Yew Lin. 2004.
    \newblock \href {https://aclanthology.org/W04-1013} {{ROUGE}: A package for automatic evaluation of summaries}.
    \newblock In \emph{Text Summarization Branches Out}, pages 74--81, Barcelona, Spain. Association for Computational Linguistics.
    
    \bibitem[{Liu et~al.(2021)Liu, Ji, Fu, Tam, Du, Yang, and Tang}]{liu2021p}
    Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021.
    \newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.
    \newblock \emph{arXiv preprint arXiv:2110.07602}.
    
    \bibitem[{Liu et~al.(2023)Liu, Zheng, Du, Ding, Qian, Yang, and Tang}]{liu2023gpt}
    Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023.
    \newblock Gpt understands, too.
    \newblock \emph{AI Open}.
    
    \bibitem[{Mehta et~al.(2023)Mehta, Patil, Chandar, and Strubell}]{mehta2023empirical}
    Sanket~Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. 2023.
    \newblock An empirical investigation of the role of pre-training in lifelong learning.
    \newblock \emph{Journal of Machine Learning Research}, 24(214):1--50.
    
    \bibitem[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov-etal-2018-suit}
    Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.
    \newblock \href {https://doi.org/10.18653/v1/D18-1260} {Can a suit of armor conduct electricity? a new dataset for open book question answering}.
    \newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 2381--2391, Brussels, Belgium. Association for Computational Linguistics.
    
    \bibitem[{Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer}]{min-etal-2022-rethinking}
    Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.
    \newblock \href {https://doi.org/10.18653/v1/2022.emnlp-main.759} {Rethinking the role of demonstrations: What makes in-context learning work?}
    \newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 11048--11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
    
    \bibitem[{Mishra et~al.(2022)Mishra, Khashabi, Baral, and Hajishirzi}]{mishra-etal-2022-cross}
    Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.
    \newblock \href {https://doi.org/10.18653/v1/2022.acl-long.244} {Cross-task generalization via natural language crowdsourcing instructions}.
    \newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 3470--3487, Dublin, Ireland. Association for Computational Linguistics.
    
    \bibitem[{Mueller et~al.(2022)Mueller, Andrews, and Dredze}]{mueller-etal-2022-text}
    David Mueller, Nicholas Andrews, and Mark Dredze. 2022.
    \newblock \href {https://doi.org/10.18653/v1/2022.findings-emnlp.206} {Do text-to-text multi-task learners suffer from task conflict?}
    \newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 2843--2858, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
    
    \bibitem[{Narayan et~al.(2018)Narayan, Cohen, and Lapata}]{narayan-etal-2018-dont}
    Shashi Narayan, Shay~B. Cohen, and Mirella Lapata. 2018.
    \newblock \href {https://doi.org/10.18653/v1/D18-1206} {Don{'}t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization}.
    \newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 1797--1807, Brussels, Belgium. Association for Computational Linguistics.
    
    \bibitem[{Narayanan and Aepli(2024)}]{narayanan-aepli-2024-tulu-resource}
    Manu Narayanan and No{\"e}mi Aepli. 2024.
    \newblock \href {https://aclanthology.org/2024.lrec-main.155} {A {T}ulu resource for machine translation}.
    \newblock In \emph{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 1756--1767, Torino, Italia. ELRA and ICCL.
    
    \bibitem[{Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen et~al.}]{olsson2022context}
    Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al. 2022.
    \newblock In-context learning and induction heads.
    \newblock \emph{arXiv preprint arXiv:2209.11895}.
    
    \bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
    Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
    \newblock Training language models to follow instructions with human feedback.
    \newblock \emph{Advances in neural information processing systems}, 35:27730--27744.
    
    \bibitem[{Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}]{scikit-learn}
    F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay. 2011.
    \newblock Scikit-learn: Machine learning in {P}ython.
    \newblock \emph{Journal of Machine Learning Research}, 12:2825--2830.
    
    \bibitem[{Perez et~al.(2023)Perez, Ringer, Lukosiute, Nguyen, Chen, Heiner, Pettit, Olsson, Kundu, Kadavath, Jones, Chen, Mann, Israel, Seethor, McKinnon, Olah, Yan, Amodei, Amodei, Drain, Li, Tran-Johnson, Khundadze, Kernion, Landis, Kerr, Mueller, Hyun, Landau, Ndousse, Goldberg, Lovitt, Lucas, Sellitto, Zhang, Kingsland, Elhage, Joseph, Mercado, DasSarma, Rausch, Larson, McCandlish, Johnston, Kravec, El~Showk, Lanham, Telleen-Lawton, Brown, Henighan, Hume, Bai, Hatfield-Dodds, Clark, Bowman, Askell, Grosse, Hernandez, Ganguli, Hubinger, Schiefer, and Kaplan}]{perez-etal-2023-discovering}
    Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da~Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El~Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel~R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023.
    \newblock \href {https://doi.org/10.18653/v1/2023.findings-acl.847} {Discovering language model behaviors with model-written evaluations}.
    \newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 13387--13434, Toronto, Canada. Association for Computational Linguistics.
    
    \bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever et~al.}]{radford2019language}
    Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al. 2019.
    \newblock Language models are unsupervised multitask learners.
    \newblock \emph{OpenAI blog}, 1(8):9.
    
    \bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling}
    Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021.
    \newblock Scaling language models: Methods, analysis \& insights from training gopher.
    \newblock \emph{arXiv preprint arXiv:2112.11446}.
    
    \bibitem[{Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn}]{rafailov2024direct}
    Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn. 2024.
    \newblock Direct preference optimization: Your language model is secretly a reward model.
    \newblock \emph{Advances in Neural Information Processing Systems}, 36.
    
    \bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}]{raffel2020exploring}
    Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
    \newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
    \newblock \emph{Journal of machine learning research}, 21(140):1--67.
    
    \bibitem[{Riviere et~al.(2024)Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e} et~al.}]{team2024gemma}
    Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al. 2024.
    \newblock Gemma 2: Improving open language models at a practical size.
    \newblock \emph{arXiv preprint arXiv:2408.00118}.
    
    \bibitem[{Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi}]{sap-etal-2019-social}
    Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le~Bras, and Yejin Choi. 2019.
    \newblock \href {https://doi.org/10.18653/v1/D19-1454} {Social {IQ}a: Commonsense reasoning about social interactions}.
    \newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 4463--4473, Hong Kong, China. Association for Computational Linguistics.
    
    \bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal}
    John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.
    \newblock Proximal policy optimization algorithms.
    \newblock \emph{arXiv preprint arXiv:1707.06347}.
    
    \bibitem[{Sclar et~al.(2023)Sclar, Choi, Tsvetkov, and Suhr}]{sclar2023quantifying}
    Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023.
    \newblock Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.
    \newblock \emph{arXiv preprint arXiv:2310.11324}.
    
    \bibitem[{Sharma et~al.(2024)Sharma, Keh, Mitchell, Finn, Arora, and Kollar}]{sharma2024critical}
    Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, and Thomas Kollar. 2024.
    \newblock A critical evaluation of ai feedback for aligning large language models.
    \newblock \emph{arXiv preprint arXiv:2402.12366}.
    
    \bibitem[{Shen et~al.(2024)Shen, Knearem, Ghosh, Alkiek, Krishna, Liu, Ma, Petridis, Peng, Qiwei, Rakshit, Si, Xie, Bigham, Bentley, Chai, Lipton, Mei, Mihalcea, Terry, Yang, Morris, Resnick, and Jurgens}]{shen2024bidirectional}
    Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li~Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey~P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith~Ringel Morris, Paul Resnick, and David Jurgens. 2024.
    \newblock Towards bidirectional human-ai alignment: A systematic review for clarifications, framework, and future directions.
    \newblock \emph{arXiv preprint arXiv:2406.09264}.
    
    \bibitem[{Singh and Strouse(2024)}]{singh2024tokenization}
    Aaditya~K Singh and DJ~Strouse. 2024.
    \newblock Tokenization counts: the impact of tokenization on arithmetic in frontier llms.
    \newblock \emph{arXiv preprint arXiv:2402.14903}.
    
    \bibitem[{Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar et~al.}]{soldaini2024dolma}
    Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al. 2024.
    \newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
    \newblock \emph{arXiv preprint arXiv:2402.00159}.
    
    \bibitem[{Song et~al.(2024)Song, Yu, Li, Yu, Huang, Li, and Wang}]{song2024preference}
    Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2024.
    \newblock Preference ranking optimization for human alignment.
    \newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 18990--18998.
    
    \bibitem[{Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano}]{stiennon2020learning}
    Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul~F Christiano. 2020.
    \newblock Learning to summarize with human feedback.
    \newblock \emph{Advances in Neural Information Processing Systems}, 33:3008--3021.
    
    \bibitem[{Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu}]{su2024roformer}
    Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.
    \newblock Roformer: Enhanced transformer with rotary position embedding.
    \newblock \emph{Neurocomputing}, 568:127063.
    
    \bibitem[{Sun et~al.(2023)Sun, Qi, Zhang, Liu, Wang, and Huang}]{sun-etal-2023-tokenization}
    Kaiser Sun, Peng Qi, Yuhao Zhang, Lan Liu, William Wang, and Zhiheng Huang. 2023.
    \newblock \href {https://doi.org/10.18653/v1/2023.findings-emnlp.887} {Tokenization consistency matters for generative models on extractive {NLP} tasks}.
    \newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 13300--13310, Singapore. Association for Computational Linguistics.
    
    \bibitem[{Tian et~al.(2023)Tian, Wang, Chen, and Du}]{tian2023scan}
    Yuandong Tian, Yiping Wang, Beidi Chen, and Simon~S Du. 2023.
    \newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer.
    \newblock \emph{Advances in Neural Information Processing Systems}, 36:71911--71947.
    
    \bibitem[{Tirumala et~al.(2022)Tirumala, Markosyan, Zettlemoyer, and Aghajanyan}]{tirumala2022memorization}
    Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022.
    \newblock Memorization without overfitting: Analyzing the training dynamics of large language models.
    \newblock \emph{Advances in Neural Information Processing Systems}, 35:38274--38290.
    
    \bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama}
    Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.
    \newblock Llama 2: Open foundation and fine-tuned chat models.
    \newblock \emph{arXiv preprint arXiv:2307.09288}.
    
    \bibitem[{Victor et~al.(2022)Victor, Albert, Colin, Stephen, Lintang, Zaid, Antoine, Arnaud, Arun, Manan et~al.}]{victor2022multitask}
    Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen, Sutawika Lintang, Alyafeai Zaid, Chaffin Antoine, Stiegler Arnaud, Raja Arun, Dey Manan, et~al. 2022.
    \newblock Multitask prompted training enables zero-shot task generalization.
    \newblock In \emph{International Conference on Learning Representations}.
    
    \bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang-etal-2018-glue}
    Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018.
    \newblock \href {https://doi.org/10.18653/v1/W18-5446} {{GLUE}: A multi-task benchmark and analysis platform for natural language understanding}.
    \newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}}, pages 353--355, Brussels, Belgium. Association for Computational Linguistics.
    
    \bibitem[{Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang, Sreedhar, and Kuchaiev}]{wang2024helpsteer2}
    Zhilin Wang, Yi~Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy~J. Zhang, Makesh~Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024.
    \newblock Helpsteer2: Open-source dataset for training top-performing reward models.
    \newblock \emph{arXiv preprint arXiv:2406.08673}.
    
    \bibitem[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned}
    Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.
    \newblock Finetuned language models are zero-shot learners.
    \newblock \emph{arXiv preprint arXiv:2109.01652}.
    
    \bibitem[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent}
    Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al. 2022.
    \newblock Emergent abilities of large language models.
    \newblock \emph{arXiv preprint arXiv:2206.07682}.
    
    \bibitem[{Welbl et~al.(2017)Welbl, Liu, and Gardner}]{welbl2017crowdsourcing}
    Johannes Welbl, Nelson~F Liu, and Matt Gardner. 2017.
    \newblock Crowdsourcing multiple choice science questions.
    \newblock In \emph{Proceedings of the 3rd Workshop on Noisy User-generated Text}, pages 94--106.
    
    \bibitem[{Williams et~al.(2018)Williams, Nangia, and Bowman}]{williams-etal-2018-broad}
    Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.
    \newblock \href {https://doi.org/10.18653/v1/N18-1101} {A broad-coverage challenge corpus for sentence understanding through inference}.
    \newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages 1112--1122, New Orleans, Louisiana. Association for Computational Linguistics.
    
    \bibitem[{Wu et~al.(2023)Wu, Qiu, Ross, Aky{\"u}rek, Chen, Wang, Kim, Andreas, and Kim}]{wu2023reasoning}
    Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky{\"u}rek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023.
    \newblock Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.
    \newblock \emph{arXiv preprint arXiv:2307.02477}.
    
    \bibitem[{Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen}]{xia2024less}
    Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024.
    \newblock Less: Selecting influential data for targeted instruction tuning.
    \newblock \emph{arXiv preprint arXiv:2402.04333}.
    
    \bibitem[{Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma}]{xie2021explanation}
    Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021.
    \newblock An explanation of in-context learning as implicit bayesian inference.
    \newblock \emph{arXiv preprint arXiv:2111.02080}.
    
    \bibitem[{Xiong et~al.(2019)Xiong, Wu, Wang, Kulkarni, Yu, Chang, Guo, and Wang}]{xiong-etal-2019-tweetqa}
    Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulkarni, Mo~Yu, Shiyu Chang, Xiaoxiao Guo, and William~Yang Wang. 2019.
    \newblock \href {https://doi.org/10.18653/v1/P19-1496} {{TWEETQA}: A social media focused question answering dataset}.
    \newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 5020--5031, Florence, Italy. Association for Computational Linguistics.
    
    \bibitem[{Xu et~al.(2024)Xu, Sharaf, Chen, Tan, Shen, Van~Durme, Murray, and Kim}]{xu2024contrastive}
    Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van~Durme, Kenton Murray, and Young~Jin Kim. 2024.
    \newblock Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation.
    \newblock \emph{arXiv preprint arXiv:2401.08417}.
    
    \bibitem[{Yang et~al.(2024)Yang, Zhang, Xu, Lu, Heng, and Lam}]{yang2024unveiling}
    Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng~Ann Heng, and Wai Lam. 2024.
    \newblock Unveiling the generalization power of fine-tuned large language models.
    \newblock \emph{arXiv preprint arXiv:2403.09162}.
    
    \bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers-etal-2019-hellaswag}
    Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
    \newblock \href {https://doi.org/10.18653/v1/P19-1472} {{H}ella{S}wag: Can a machine really finish your sentence?}
    \newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4791--4800, Florence, Italy. Association for Computational Linguistics.
    
    \bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt}
    Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.
    \newblock Opt: Open pre-trained transformer language models.
    \newblock \emph{arXiv preprint arXiv:2205.01068}.
    
    \bibitem[{Zhang et~al.(2019)Zhang, Baldridge, and He}]{zhang-etal-2019-paws}
    Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
    \newblock \href {https://doi.org/10.18653/v1/N19-1131} {{PAWS}: Paraphrase adversaries from word scrambling}.
    \newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 1298--1308, Minneapolis, Minnesota. Association for Computational Linguistics.
    
    \bibitem[{Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu et~al.}]{zhou2024lima}
    Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al. 2024.
    \newblock Lima: Less is more for alignment.
    \newblock \emph{Advances in Neural Information Processing Systems}, 36.
    
    \bibitem[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2019fine}
    Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.
    \newblock Fine-tuning language models from human preferences.
    \newblock \emph{arXiv preprint arXiv:1909.08593}.
    
    \end{thebibliography}
    

\clearpage
\appendix
\input{sections/appendix}
\label{sec:appendix}

\end{document}
