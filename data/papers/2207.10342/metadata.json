{
  "arxivId": "2207.10342",
  "title": "Language Model Cascades",
  "authors": "David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton",
  "abstract": "Prompted models have demonstrated impressive few-shot learning abilities.\nRepeated interactions at test-time with a single model, or the composition of\nmultiple models together, further expands capabilities. These compositions are\nprobabilistic models, and may be expressed in the language of graphical models\nwith random variables whose values are complex data types such as strings.\nCases with control flow and dynamic structure require techniques from\nprobabilistic programming, which allow implementing disparate model structures\nand inference strategies in a unified language. We formalize several existing\ntechniques from this perspective, including scratchpads / chain of thought,\nverifiers, STaR, selection-inference, and tool use. We refer to the resulting\nprograms as language model cascades.",
  "url": "https://arxiv.org/abs/2207.10342",
  "issue_number": 470,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/470",
  "created_at": "2024-12-28T09:13:00.534873",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 47,
  "last_read": "2024-12-29T10:05:52.495491",
  "last_visited": "2024-12-29T10:05:16.735000+00:00",
  "main_tex_file": null,
  "published_date": "2022-07-21T07:35:18Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI"
  ]
}