'1201.1717':
  abstract: "Hyperbolicity is a property of a graph that may be viewed as being a\
    \ \"soft\"\nversion of a tree, and recent empirical and theoretical work has suggested\
    \ that\nmany graphs arising in Internet and related data applications have hyperbolic\n\
    properties. We consider Gromov's notion of \\delta-hyperbolicity, and establish\n\
    several results for small-world and tree-like random graph models. First, we\n\
    study the hyperbolicity of Kleinberg small-world random graphs and show that\n\
    the hyperbolicity of these random graphs is not significantly improved\ncomparing\
    \ to graph diameter even when it greatly improves decentralized\nnavigation. Next\
    \ we study a class of tree-like graphs called ringed trees that\nhave constant\
    \ hyperbolicity. We show that adding random links among the leaves\nsimilar to\
    \ the small-world graph constructions may easily destroy the\nhyperbolicity of\
    \ the graphs, except for a class of random edges added using an\nexponentially\
    \ decaying probability function based on the ring distance among\nthe leaves.\n\
    \  Our study provides one of the first significant analytical results on the\n\
    hyperbolicity of a rich class of random graphs, which shed light on the\nrelationship\
    \ between hyperbolicity and navigability of random graphs, as well\nas on the\
    \ sensitivity of hyperbolic {\\delta} to noises in random graphs."
  arxivId: '1201.1717'
  arxiv_tags:
  - cs.SI
  - cs.DM
  - physics.soc-ph
  authors: Wei Chen, Wenjie Fang, Guangda Hu, Michael W. Mahoney
  created_at: '2024-12-30T08:28:10.543868'
  issue_number: 445
  issue_url: https://github.com/dmarx/papers-feed/issues/445
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:41:46.909Z'
  main_tex_file: null
  published_date: '2012-01-09T09:30:38Z'
  state: open
  title: On the Hyperbolicity of Small-World and Tree-Like Random Graphs
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1201.1717
'1411.1792':
  abstract: 'Many deep neural networks trained on natural images exhibit a curious

    phenomenon in common: on the first layer they learn features similar to Gabor

    filters and color blobs. Such first-layer features appear not to be specific to

    a particular dataset or task, but general in that they are applicable to many

    datasets and tasks. Features must eventually transition from general to

    specific by the last layer of the network, but this transition has not been

    studied extensively. In this paper we experimentally quantify the generality

    versus specificity of neurons in each layer of a deep convolutional neural

    network and report a few surprising results. Transferability is negatively

    affected by two distinct issues: (1) the specialization of higher layer neurons

    to their original task at the expense of performance on the target task, which

    was expected, and (2) optimization difficulties related to splitting networks

    between co-adapted neurons, which was not expected. In an example network

    trained on ImageNet, we demonstrate that either of these two issues may

    dominate, depending on whether features are transferred from the bottom,

    middle, or top of the network. We also document that the transferability of

    features decreases as the distance between the base task and target task

    increases, but that transferring features even from distant tasks can be better

    than using random features. A final surprising result is that initializing a

    network with transferred features from almost any number of layers can produce

    a boost to generalization that lingers even after fine-tuning to the target

    dataset.'
  arxivId: '1411.1792'
  arxiv_tags:
  - cs.LG
  - cs.NE
  authors: Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson
  created_at: '2024-12-30T14:44:57.206046'
  issue_number: 212
  issue_url: https://github.com/dmarx/papers-feed/issues/212
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:43:09.950Z'
  main_tex_file: null
  published_date: '2014-11-06T23:09:37Z'
  state: open
  title: How transferable are features in deep neural networks?
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1411.1792
'1503.03585':
  abstract: 'A central problem in machine learning involves modeling complex data-sets

    using highly flexible families of probability distributions in which learning,

    sampling, inference, and evaluation are still analytically or computationally

    tractable. Here, we develop an approach that simultaneously achieves both

    flexibility and tractability. The essential idea, inspired by non-equilibrium

    statistical physics, is to systematically and slowly destroy structure in a

    data distribution through an iterative forward diffusion process. We then learn

    a reverse diffusion process that restores structure in data, yielding a highly

    flexible and tractable generative model of the data. This approach allows us to

    rapidly learn, sample from, and evaluate probabilities in deep generative

    models with thousands of layers or time steps, as well as to compute

    conditional and posterior probabilities under the learned model. We

    additionally release an open source reference implementation of the algorithm.'
  arxivId: '1503.03585'
  arxiv_tags:
  - cs.LG
  - cond-mat.dis-nn
  - q-bio.NC
  - stat.ML
  authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
  created_at: '2024-12-30T14:44:36.203377'
  issue_number: 253
  issue_url: https://github.com/dmarx/papers-feed/issues/253
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:36.204142'
  last_visited: '2024-12-24T21:08:52.964Z'
  main_tex_file: null
  published_date: '2015-03-12T04:51:37Z'
  state: open
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  total_reading_time_seconds: 15
  url: https://arxiv.org/abs/1503.03585
'1506.06579':
  abstract: 'Recent years have produced great advances in training large, deep neural

    networks (DNNs), including notable successes in training convolutional neural

    networks (convnets) to recognize natural images. However, our understanding of

    how these models work, especially what computations they perform at

    intermediate layers, has lagged behind. Progress in the field will be further

    accelerated by the development of better tools for visualizing and interpreting

    neural nets. We introduce two such tools here. The first is a tool that

    visualizes the activations produced on each layer of a trained convnet as it

    processes an image or video (e.g. a live webcam stream). We have found that

    looking at live activations that change in response to user input helps build

    valuable intuitions about how convnets work. The second tool enables

    visualizing features at each layer of a DNN via regularized optimization in

    image space. Because previous versions of this idea produced less recognizable

    images, here we introduce several new regularization methods that combine to

    produce qualitatively clearer, more interpretable visualizations. Both tools

    are open source and work on a pre-trained convnet with minimal setup.'
  arxivId: '1506.06579'
  arxiv_tags:
  - cs.CV
  - cs.LG
  - cs.NE
  authors: Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson
  created_at: '2024-12-30T14:44:54.228875'
  issue_number: 214
  issue_url: https://github.com/dmarx/papers-feed/issues/214
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:47:02.227Z'
  main_tex_file: null
  published_date: '2015-06-22T12:57:15Z'
  state: open
  title: Understanding Neural Networks Through Deep Visualization
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1506.06579
'1602.05897':
  abstract: 'We develop a general duality between neural networks and compositional

    kernels, striving towards a better understanding of deep learning. We show that

    initial representations generated by common random initializations are

    sufficiently rich to express all functions in the dual kernel space. Hence,

    though the training objective is hard to optimize in the worst case, the

    initial weights form a good starting point for optimization. Our dual view also

    reveals a pragmatic and aesthetic perspective of neural networks and

    underscores their expressive power.'
  arxivId: '1602.05897'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CC
  - cs.DS
  - stat.ML
  authors: Amit Daniely, Roy Frostig, Yoram Singer
  created_at: '2024-12-30T08:27:37.538781'
  issue_number: 496
  issue_url: https://github.com/dmarx/papers-feed/issues/496
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T11:06:20.722Z'
  main_tex_file: null
  published_date: '2016-02-18T18:14:19Z'
  state: open
  title: "Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization\
    \ and a Dual View on Expressivity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1602.05897
'1705.08039':
  abstract: 'Representation learning has become an invaluable approach for learning
    from

    symbolic data such as text and graphs. However, while complex symbolic datasets

    often exhibit a latent hierarchical structure, state-of-the-art methods

    typically learn embeddings in Euclidean vector spaces, which do not account for

    this property. For this purpose, we introduce a new approach for learning

    hierarchical representations of symbolic data by embedding them into hyperbolic

    space -- or more precisely into an n-dimensional Poincar\''e ball. Due to the

    underlying hyperbolic geometry, this allows us to learn parsimonious

    representations of symbolic data by simultaneously capturing hierarchy and

    similarity. We introduce an efficient algorithm to learn the embeddings based

    on Riemannian optimization and show experimentally that Poincar\''e embeddings

    outperform Euclidean embeddings significantly on data with latent hierarchies,

    both in terms of representation capacity and in terms of generalization

    ability.'
  arxivId: '1705.08039'
  arxiv_tags:
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Maximilian Nickel, Douwe Kiela
  created_at: '2024-12-30T08:27:58.558111'
  issue_number: 447
  issue_url: https://github.com/dmarx/papers-feed/issues/447
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:07.585743'
  last_visited: '2024-12-29T01:43:25.284Z'
  main_tex_file: null
  published_date: '2017-05-22T23:14:36Z'
  state: open
  title: Poincaré Embeddings for Learning Hierarchical Representations
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1705.08039
'1705.10359':
  abstract: 'Neural embeddings have been used with great success in Natural Language

    Processing (NLP). They provide compact representations that encapsulate word

    similarity and attain state-of-the-art performance in a range of linguistic

    tasks. The success of neural embeddings has prompted significant amounts of

    research into applications in domains other than language. One such domain is

    graph-structured data, where embeddings of vertices can be learned that

    encapsulate vertex similarity and improve performance on tasks including edge

    prediction and vertex labelling. For both NLP and graph based tasks, embeddings

    have been learned in high-dimensional Euclidean spaces. However, recent work

    has shown that the appropriate isometric space for embedding complex networks

    is not the flat Euclidean space, but negatively curved, hyperbolic space. We

    present a new concept that exploits these recent insights and propose learning

    neural embeddings of graphs in hyperbolic space. We provide experimental

    evidence that embedding graphs in their natural geometry significantly improves

    performance on downstream tasks for several real-world public datasets.'
  arxivId: '1705.10359'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth
  created_at: '2024-12-30T08:28:01.537841'
  issue_number: 448
  issue_url: https://github.com/dmarx/papers-feed/issues/448
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:07.586810'
  last_visited: '2024-12-29T01:43:26.258Z'
  main_tex_file: null
  published_date: '2017-05-29T18:47:30Z'
  state: open
  title: Neural Embeddings of Graphs in Hyperbolic Space
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/1705.10359
'1706.05806':
  abstract: 'We propose a new technique, Singular Vector Canonical Correlation Analysis

    (SVCCA), a tool for quickly comparing two representations in a way that is both

    invariant to affine transform (allowing comparison between different layers and

    networks) and fast to compute (allowing more comparisons to be calculated than

    with previous methods). We deploy this tool to measure the intrinsic

    dimensionality of layers, showing in some cases needless over-parameterization;

    to probe learning dynamics throughout training, finding that networks converge

    to final representations from the bottom up; to show where class-specific

    information in networks is formed; and to suggest new training regimes that

    simultaneously save computation and overfit less. Code:

    https://github.com/google/svcca/'
  arxivId: '1706.05806'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein
  created_at: '2024-12-30T14:44:48.197487'
  issue_number: 218
  issue_url: https://github.com/dmarx/papers-feed/issues/218
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:48.199297'
  last_visited: '2024-12-24T02:53:44.603Z'
  main_tex_file: null
  published_date: '2017-06-19T07:09:20Z'
  state: open
  title: "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning\n\
    \  Dynamics and Interpretability"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1706.05806
'1802.04956':
  abstract: 'For many machine learning problem settings, particularly with structured

    inputs such as sequences or sets of objects, a distance measure between inputs

    can be specified more naturally than a feature representation. However, most

    standard machine models are designed for inputs with a vector feature

    representation. In this work, we consider the estimation of a function

    $f:\mathcal{X} \rightarrow \R$ based solely on a dissimilarity measure

    $d:\mathcal{X}\times\mathcal{X} \rightarrow \R$ between inputs. In particular,

    we propose a general framework to derive a family of \emph{positive definite

    kernels} from a given dissimilarity measure, which subsumes the widely-used

    \emph{representative-set method} as a special case, and relates to the

    well-known \emph{distance substitution kernel} in a limiting case. We show that

    functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are

    Lipschitz-continuous w.r.t. the given distance metric. We provide a tractable

    algorithm to estimate a function from this RKHS, and show that it enjoys better

    generalizability than Nearest-Neighbor estimates. Our approach draws from the

    literature of Random Features, but instead of deriving feature maps from an

    existing kernel, we construct novel kernels from a random feature map, that we

    specify given the distance measure. We conduct classification experiments with

    such disparate domains as strings, time series, and sets of vectors, where our

    proposed framework compares favorably to existing distance-based learning

    methods such as $k$-nearest-neighbors, distance-substitution kernels,

    pseudo-Euclidean embedding, and the representative-set method.'
  arxivId: '1802.04956'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Lingfei Wu, Ian En-Hsu Yen, Fangli Xu, Pradeep Ravikumar, Michael Witbrock
  created_at: '2024-12-30T08:27:43.532109'
  issue_number: 493
  issue_url: https://github.com/dmarx/papers-feed/issues/493
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T10:54:09.487Z'
  main_tex_file: null
  published_date: '2018-02-14T04:58:13Z'
  state: open
  title: 'D2KE: From Distance to Kernel and Embedding'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1802.04956
'1804.03329':
  abstract: 'Hyperbolic embeddings offer excellent quality with few dimensions when

    embedding hierarchical data structures like synonym or type hierarchies. Given

    a tree, we give a combinatorial construction that embeds the tree in hyperbolic

    space with arbitrarily low distortion without using optimization. On WordNet,

    our combinatorial embedding obtains a mean-average-precision of 0.989 with only

    two dimensions, while Nickel et al.''s recent construction obtains 0.87 using

    200 dimensions. We provide upper and lower bounds that allow us to characterize

    the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To

    embed general metric spaces, we propose a hyperbolic generalization of

    multidimensional scaling (h-MDS). We show how to perform exact recovery of

    hyperbolic points from distances, provide a perturbation analysis, and give a

    recovery result that allows us to reduce dimensionality. The h-MDS approach

    offers consistently low distortion even with few dimensions across several

    datasets. Finally, we extract lessons from the algorithms and theory above to

    design a PyTorch-based implementation that can handle incomplete information

    and is scalable.'
  arxivId: '1804.03329'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala
  created_at: '2024-12-30T08:27:55.545975'
  issue_number: 457
  issue_url: https://github.com/dmarx/papers-feed/issues/457
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T02:34:35.575Z'
  main_tex_file: null
  published_date: '2018-04-10T03:39:16Z'
  state: open
  title: Representation Tradeoffs for Hyperbolic Embeddings
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1804.03329
'1806.00468':
  abstract: 'We show that gradient descent on full-width linear convolutional networks
    of

    depth $L$ converges to a linear predictor related to the $\ell_{2/L}$ bridge

    penalty in the frequency domain. This is in contrast to linearly fully

    connected networks, where gradient descent converges to the hard margin linear

    support vector machine solution, regardless of depth.'
  arxivId: '1806.00468'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Suriya Gunasekar, Jason Lee, Daniel Soudry, Nathan Srebro
  created_at: '2024-12-30T08:27:34.531778'
  issue_number: 495
  issue_url: https://github.com/dmarx/papers-feed/issues/495
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:34.533947'
  last_visited: '2024-12-29T11:05:59.663Z'
  main_tex_file: null
  published_date: '2018-06-01T17:58:58Z'
  state: open
  title: Implicit Bias of Gradient Descent on Linear Convolutional Networks
  total_reading_time_seconds: 23
  url: https://arxiv.org/abs/1806.00468
'1906.01563':
  abstract: 'Even though neural networks enjoy widespread use, they still struggle
    to

    learn the basic laws of physics. How might we endow them with better inductive

    biases? In this paper, we draw inspiration from Hamiltonian mechanics to train

    models that learn and respect exact conservation laws in an unsupervised

    manner. We evaluate our models on problems where conservation of energy is

    important, including the two-body problem and pixel observations of a pendulum.

    Our model trains faster and generalizes better than a regular neural network.

    An interesting side effect is that our model is perfectly reversible in time.'
  arxivId: '1906.01563'
  arxiv_tags:
  - cs.NE
  authors: Sam Greydanus, Misko Dzamba, Jason Yosinski
  created_at: '2024-12-30T14:44:51.195715'
  issue_number: 216
  issue_url: https://github.com/dmarx/papers-feed/issues/216
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:49:04.570Z'
  main_tex_file: null
  published_date: '2019-06-04T16:27:55Z'
  state: open
  title: Hamiltonian Neural Networks
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1906.01563
'1912.02757':
  abstract: 'Deep ensembles have been empirically shown to be a promising approach
    for

    improving accuracy, uncertainty and out-of-distribution robustness of deep

    learning models. While deep ensembles were theoretically motivated by the

    bootstrap, non-bootstrap ensembles trained with just random initialization also

    perform well in practice, which suggests that there could be other explanations

    for why deep ensembles work well. Bayesian neural networks, which learn

    distributions over the parameters of the network, are theoretically

    well-motivated by Bayesian principles, but do not perform as well as deep

    ensembles in practice, particularly under dataset shift. One possible

    explanation for this gap between theory and practice is that popular scalable

    variational Bayesian methods tend to focus on a single mode, whereas deep

    ensembles tend to explore diverse modes in function space. We investigate this

    hypothesis by building on recent work on understanding the loss landscape of

    neural networks and adding our own exploration to measure the similarity of

    functions in the space of predictions. Our results show that random

    initializations explore entirely different modes, while functions along an

    optimization trajectory or sampled from the subspace thereof cluster within a

    single mode predictions-wise, while often deviating significantly in the weight

    space. Developing the concept of the diversity--accuracy plane, we show that

    the decorrelation power of random initializations is unmatched by popular

    subspace sampling methods. Finally, we evaluate the relative effects of

    ensembling, subspace based methods and ensembles of subspace based methods, and

    the experimental results validate our hypothesis.'
  arxivId: '1912.02757'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan
  created_at: '2024-12-30T14:44:42.200508'
  issue_number: 224
  issue_url: https://github.com/dmarx/papers-feed/issues/224
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:42.201667'
  last_visited: '2024-12-24T03:01:38.243Z'
  main_tex_file: null
  published_date: '2019-12-05T17:48:18Z'
  state: open
  title: 'Deep Ensembles: A Loss Landscape Perspective'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/1912.02757
'2001.04063':
  abstract: 'This paper presents a new sequence-to-sequence pre-training model called

    ProphetNet, which introduces a novel self-supervised objective named future

    n-gram prediction and the proposed n-stream self-attention mechanism. Instead

    of optimizing one-step-ahead prediction in the traditional sequence-to-sequence

    model, the ProphetNet is optimized by n-step ahead prediction that predicts the

    next n tokens simultaneously based on previous context tokens at each time

    step. The future n-gram prediction explicitly encourages the model to plan for

    the future tokens and prevent overfitting on strong local correlations. We

    pre-train ProphetNet using a base scale dataset (16GB) and a large-scale

    dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail,

    Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question

    generation tasks. Experimental results show that ProphetNet achieves new

    state-of-the-art results on all these datasets compared to the models using the

    same scale pre-training corpus.'
  arxivId: '2001.04063'
  arxiv_tags:
  - cs.CL
  authors: Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,
    Ruofei Zhang, Ming Zhou
  created_at: '2024-12-30T14:44:21.221288'
  issue_number: 286
  issue_url: https://github.com/dmarx/papers-feed/issues/286
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-26T17:17:59.219Z'
  main_tex_file: null
  published_date: '2020-01-13T05:12:38Z'
  state: open
  title: "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2001.04063
'2006.11120':
  abstract: 'A basic operation in Convolutional Neural Networks (CNNs) is spatial
    resizing

    of feature maps. This is done either by strided convolution (donwscaling) or

    transposed convolution (upscaling). Such operations are limited to a fixed

    filter moving at predetermined integer steps (strides). Spatial sizes of

    consecutive layers are related by integer scale factors, predetermined at

    architectural design, and remain fixed throughout training and inference time.

    We propose a generalization of the common Conv-layer, from a discrete layer to

    a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by

    representing the filter as a learned continuous function over sub-pixel

    coordinates. This allows learnable and principled resizing of feature maps, to

    any size, dynamically and consistently across scales. Once trained, the CC

    layer can be used to output any scale/size chosen at inference time. The scale

    can be non-integer and differ between the axes. CC gives rise to new freedoms

    for architectural design, such as dynamic layer shapes at inference time, or

    gradual architectures where the size changes by a small factor at each layer.

    This gives rise to many desired CNN properties, new architectural design

    capabilities, and useful applications. We further show that current Conv-layers

    suffer from inherent misalignments, which are ameliorated by CC layers.'
  arxivId: '2006.11120'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - stat.ML
  authors: Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani
  created_at: '2024-12-30T14:44:33.194102'
  issue_number: 262
  issue_url: https://github.com/dmarx/papers-feed/issues/262
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:33.195947'
  last_visited: '2024-12-25T05:28:48.326Z'
  main_tex_file: null
  published_date: '2020-06-19T13:16:06Z'
  state: open
  title: From Discrete to Continuous Convolution Layers
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2006.11120
'2006.14769':
  abstract: 'We present the Supermasks in Superposition (SupSup) model, capable of

    sequentially learning thousands of tasks without catastrophic forgetting. Our

    approach uses a randomly initialized, fixed base network and for each task

    finds a subnetwork (supermask) that achieves good performance. If task identity

    is given at test time, the correct subnetwork can be retrieved with minimal

    memory usage. If not provided, SupSup can infer the task using gradient-based

    optimization to find a linear superposition of learned supermasks which

    minimizes the output entropy. In practice we find that a single gradient step

    is often sufficient to identify the correct mask, even among 2500 tasks. We

    also showcase two promising extensions. First, SupSup models can be trained

    entirely without task identity information, as they may detect when they are

    uncertain about new data and allocate an additional supermask for the new

    training distribution. Finally the entire, growing set of supermasks can be

    stored in a constant-sized reservoir by implicitly storing them as attractors

    in a fixed-sized Hopfield network.'
  arxivId: '2006.14769'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - stat.ML
  authors: Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad
    Rastegari, Jason Yosinski, Ali Farhadi
  created_at: '2024-12-30T14:45:00.197018'
  issue_number: 211
  issue_url: https://github.com/dmarx/papers-feed/issues/211
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:37:42.270Z'
  main_tex_file: null
  published_date: '2020-06-26T03:16:44Z'
  state: open
  title: Supermasks in Superposition
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2006.14769
'2012.13255':
  abstract: 'Although pretrained language models can be fine-tuned to produce

    state-of-the-art results for a very wide range of language understanding tasks,

    the dynamics of this process are not well understood, especially in the low

    data regime. Why can we use relatively vanilla gradient descent algorithms

    (e.g., without strong regularization) to tune a model with hundreds of millions

    of parameters on datasets with only hundreds or thousands of labeled examples?

    In this paper, we argue that analyzing fine-tuning through the lens of

    intrinsic dimension provides us with empirical and theoretical intuitions to

    explain this remarkable phenomenon. We empirically show that common pre-trained

    models have a very low intrinsic dimension; in other words, there exists a low

    dimension reparameterization that is as effective for fine-tuning as the full

    parameter space. For example, by optimizing only 200 trainable parameters

    randomly projected back into the full space, we can tune a RoBERTa model to

    achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we

    empirically show that pre-training implicitly minimizes intrinsic dimension

    and, perhaps surprisingly, larger models tend to have lower intrinsic dimension

    after a fixed number of pre-training updates, at least in part explaining their

    extreme effectiveness. Lastly, we connect intrinsic dimensionality with low

    dimensional task representations and compression based generalization bounds to

    provide intrinsic-dimension-based generalization bounds that are independent of

    the full parameter count.'
  arxivId: '2012.13255'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta
  created_at: '2024-12-30T14:44:45.216445'
  issue_number: 222
  issue_url: https://github.com/dmarx/papers-feed/issues/222
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:45.217285'
  last_visited: '2024-12-24T02:58:39.730Z'
  main_tex_file: null
  published_date: '2020-12-22T07:42:30Z'
  state: open
  title: "Intrinsic Dimensionality Explains the Effectiveness of Language Model\n\
    \  Fine-Tuning"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2012.13255
'2105.05720':
  abstract: "Recent trend towards increasing large machine learning models require\
    \ both\ntraining and inference tasks to be distributed. Considering the huge cost\
    \ of\ntraining these models, it is imperative to unlock optimizations in computation\n\
    and communication to obtain best performance. However, current logical\nseparation\
    \ between computation and communication kernels in deep learning\nframeworks misses\
    \ the optimization opportunities across such barrier. Breaking\nthis abstraction\
    \ with a holistic consideration can provide many optimizations\nto provide performance\
    \ improvements in distributed workloads. Manually applying\nthese optimizations\
    \ needs modifications in underlying computation and\ncommunication libraries for\
    \ each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present\
    \ CoCoNeT, with a DSL to express a program with both\ncomputation and communication.\
    \ CoCoNeT contains several machine learning aware\ntransformations to optimize\
    \ a program and a compiler to generate high\nperformance kernels. Providing both\
    \ computation and communication as first\nclass constructs allows users to work\
    \ on a high-level abstraction and apply\npowerful optimizations, such as fusion\
    \ or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize\
    \ data-, model-and pipeline-parallel\nworkloads in large language models with\
    \ only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms\
    \ state-of-the-art distributed machine\nlearning implementations."
  arxivId: '2105.05720'
  arxiv_tags:
  - cs.DC
  - cs.LG
  - cs.PL
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2024-12-30T08:26:40.759202'
  issue_number: 581
  issue_url: https://github.com/dmarx/papers-feed/issues/581
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:40.761102'
  last_visited: '2024-12-30T07:58:09.742Z'
  main_tex_file: null
  published_date: '2021-05-12T15:13:43Z'
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_seconds: 38
  url: https://arxiv.org/abs/2105.05720
'2106.10165':
  abstract: 'This book develops an effective theory approach to understanding deep
    neural

    networks of practical relevance. Beginning from a first-principles

    component-level picture of networks, we explain how to determine an accurate

    description of the output of trained networks by solving layer-to-layer

    iteration equations and nonlinear learning dynamics. A main result is that the

    predictions of networks are described by nearly-Gaussian distributions, with

    the depth-to-width aspect ratio of the network controlling the deviations from

    the infinite-width Gaussian description. We explain how these effectively-deep

    networks learn nontrivial representations from training and more broadly

    analyze the mechanism of representation learning for nonlinear models. From a

    nearly-kernel-methods perspective, we find that the dependence of such models''

    predictions on the underlying learning algorithm can be expressed in a simple

    and universal way. To obtain these results, we develop the notion of

    representation group flow (RG flow) to characterize the propagation of signals

    through the network. By tuning networks to criticality, we give a practical

    solution to the exploding and vanishing gradient problem. We further explain

    how RG flow leads to near-universal behavior and lets us categorize networks

    built from different activation functions into universality classes.

    Altogether, we show that the depth-to-width ratio governs the effective model

    complexity of the ensemble of trained networks. By using information-theoretic

    techniques, we estimate the optimal aspect ratio at which we expect the network

    to be practically most useful and show how residual connections can be used to

    push this scale to arbitrary depths. With these tools, we can learn in detail

    about the inductive bias of architectures, hyperparameters, and optimizers.'
  arxivId: '2106.10165'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - hep-th
  - stat.ML
  authors: Daniel A. Roberts, Sho Yaida, Boris Hanin
  created_at: '2024-12-30T08:27:19.544402'
  issue_number: 523
  issue_url: https://github.com/dmarx/papers-feed/issues/523
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T22:46:13.679Z'
  main_tex_file: null
  published_date: '2021-06-18T15:00:00Z'
  state: open
  title: The Principles of Deep Learning Theory
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2106.10165
'2204.00595':
  abstract: 'Large neural networks excel in many domains, but they are expensive to
    train

    and fine-tune. A popular approach to reduce their compute or memory

    requirements is to replace dense weight matrices with structured ones (e.g.,

    sparse, low-rank, Fourier transform). These methods have not seen widespread

    adoption (1) in end-to-end training due to unfavorable efficiency--quality

    tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable

    algorithms to approximate a given dense weight matrix. To address these issues,

    we propose a class of matrices (Monarch) that is hardware-efficient (they are

    parameterized as products of two block-diagonal matrices for better hardware

    utilization) and expressive (they can represent many commonly used transforms).

    Surprisingly, the problem of approximating a dense weight matrix with a Monarch

    matrix, though nonconvex, has an analytical optimal solution. These properties

    of Monarch matrices unlock new ways to train and fine-tune sparse and dense

    models. We empirically validate that Monarch can achieve favorable

    accuracy-efficiency tradeoffs in several end-to-end sparse training

    applications: speeding up ViT and GPT-2 training on ImageNet classification and

    Wikitext-103 language modeling by 2x with comparable model quality, and

    reducing the error on PDE solving and MRI reconstruction tasks by 40%. In

    sparse-to-dense training, with a simple technique called "reverse

    sparsification," Monarch matrices serve as a useful intermediate representation

    to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The

    same technique brings 23% faster BERT pretraining than even the very optimized

    implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse

    fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds

    up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.'
  arxivId: '2204.00595'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
    Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré
  created_at: '2024-12-30T14:44:03.221400'
  issue_number: 335
  issue_url: https://github.com/dmarx/papers-feed/issues/335
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:03.223396'
  last_visited: '2024-12-28T06:07:58.885Z'
  main_tex_file: null
  published_date: '2022-04-01T17:37:29Z'
  state: open
  title: "Monarch: Expressive Structured Matrices for Efficient and Accurate\n  Training"
  total_reading_time_seconds: 35
  url: https://arxiv.org/abs/2204.00595
'2207.10342':
  abstract: 'Prompted models have demonstrated impressive few-shot learning abilities.

    Repeated interactions at test-time with a single model, or the composition of

    multiple models together, further expands capabilities. These compositions are

    probabilistic models, and may be expressed in the language of graphical models

    with random variables whose values are complex data types such as strings.

    Cases with control flow and dynamic structure require techniques from

    probabilistic programming, which allow implementing disparate model structures

    and inference strategies in a unified language. We formalize several existing

    techniques from this perspective, including scratchpads / chain of thought,

    verifiers, STaR, selection-inference, and tool use. We refer to the resulting

    programs as language model cascades.'
  arxivId: '2207.10342'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael
    Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein,
    Kevin Murphy, Charles Sutton
  created_at: '2024-12-30T08:27:49.537065'
  issue_number: 461
  issue_url: https://github.com/dmarx/papers-feed/issues/461
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T08:48:42.548Z'
  main_tex_file: null
  published_date: '2022-07-21T07:35:18Z'
  state: open
  title: Language Model Cascades
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2207.10342
'2208.11665':
  abstract: 'The Manifold Hypothesis is a widely accepted tenet of Machine Learning
    which

    asserts that nominally high-dimensional data are in fact concentrated near a

    low-dimensional manifold, embedded in high-dimensional space. This phenomenon

    is observed empirically in many real world situations, has led to development

    of a wide range of statistical methods in the last few decades, and has been

    suggested as a key factor in the success of modern AI technologies. We show

    that rich and sometimes intricate manifold structure in data can emerge from a

    generic and remarkably simple statistical model -- the Latent Metric Model --

    via elementary concepts such as latent variables, correlation and stationarity.

    This establishes a general statistical explanation for why the Manifold

    Hypothesis seems to hold in so many situations. Informed by the Latent Metric

    Model we derive procedures to discover and interpret the geometry of

    high-dimensional data, and explore hypotheses about the data generating

    mechanism. These procedures operate under minimal assumptions and make use of

    well known, scaleable graph-analytic algorithms.'
  arxivId: '2208.11665'
  arxiv_tags:
  - stat.ME
  - cs.LG
  - stat.ML
  - 62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30
  authors: Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy
  created_at: '2024-12-30T08:28:07.584219'
  issue_number: 452
  issue_url: https://github.com/dmarx/papers-feed/issues/452
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-29T02:26:31.276Z'
  main_tex_file: null
  published_date: '2022-08-24T17:00:16Z'
  state: open
  title: Statistical exploration of the Manifold Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2208.11665
'2302.10866':
  abstract: 'Recent advances in deep learning have relied heavily on the use of large

    Transformers due to their ability to learn at scale. However, the core building

    block of Transformers, the attention operator, exhibits quadratic cost in

    sequence length, limiting the amount of context accessible. Existing

    subquadratic methods based on low-rank and sparse approximations need to be

    combined with dense attention layers to match Transformers, indicating a gap in

    capability. In this work, we propose Hyena, a subquadratic drop-in replacement

    for attention constructed by interleaving implicitly parametrized long

    convolutions and data-controlled gating. In recall and reasoning tasks on

    sequences of thousands to hundreds of thousands of tokens, Hyena improves

    accuracy by more than 50 points over operators relying on state-spaces and

    other implicit and explicit methods, matching attention-based models. We set a

    new state-of-the-art for dense-attention-free architectures on language

    modeling in standard datasets (WikiText103 and The Pile), reaching Transformer

    quality with a 20% reduction in training compute required at sequence length

    2K. Hyena operators are twice as fast as highly optimized attention at sequence

    length 8K, and 100x faster at sequence length 64K.'
  arxivId: '2302.10866'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen
    Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré
  created_at: '2024-12-30T08:28:22.546796'
  issue_number: 401
  issue_url: https://github.com/dmarx/papers-feed/issues/401
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:17:27.699Z'
  main_tex_file: null
  published_date: '2023-02-21T18:29:25Z'
  state: open
  title: 'Hyena Hierarchy: Towards Larger Convolutional Language Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2302.10866
'2302.13714':
  abstract: 'In this work, we investigate a challenging problem, which has been considered

    to be an important criterion in designing codewords for DNA computing purposes,

    namely secondary structure avoidance in single-stranded DNA molecules. In

    short, secondary structure refers to the tendency of a single-stranded DNA

    sequence to fold back upon itself, thus becoming inactive in the computation

    process. While some design criteria that reduces the possibility of secondary

    structure formation has been proposed by Milenkovic and Kashyap (2006), the

    main contribution of this work is to provide an explicit construction of DNA

    codes that completely avoid secondary structure of arbitrary stem length.

    Formally, given codeword length n and arbitrary integer m>=2, we provide

    efficient methods to construct DNA codes of length n that avoid secondary

    structure of any stem length more than or equal to m. Particularly, when m = 3,

    our constructions yield a family of DNA codes of rate 1.3031 bits/nt, while the

    highest rate found in the prior art was 1.1609 bits/nt. In addition, for

    m>=3log n + 4, we provide an efficient encoder that incurs only one redundant

    symbol.'
  arxivId: '2302.13714'
  arxiv_tags:
  - cs.IT
  - math.CO
  - math.IT
  authors: Tuan Thanh Nguyen, Kui Cai, Han Mao Kiah, Duc Tu Dao, Kees A. Schouhamer
    Immink
  created_at: '2024-12-30T08:28:25.573770'
  issue_number: 400
  issue_url: https://github.com/dmarx/papers-feed/issues/400
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:25.574539'
  last_visited: '2024-12-28T07:13:40.029Z'
  main_tex_file: null
  published_date: '2023-02-27T12:22:07Z'
  state: open
  title: "On the Design of Codes for DNA Computing: Secondary Structure Avoidance\n\
    \  Codes"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2302.13714
'2303.09489':
  abstract: 'Time series modeling is a well-established problem, which often requires
    that

    methods (1) expressively represent complicated dependencies, (2) forecast long

    horizons, and (3) efficiently train over long sequences. State-space models

    (SSMs) are classical models for time series, and prior works combine SSMs with

    deep learning layers for efficient sequence modeling. However, we find

    fundamental limitations with these prior approaches, proving their SSM

    representations cannot express autoregressive time series processes. We thus

    introduce SpaceTime, a new state-space time series architecture that improves

    all three criteria. For expressivity, we propose a new SSM parameterization

    based on the companion matrix -- a canonical representation for discrete-time

    processes -- which enables SpaceTime''s SSM layers to learn desirable

    autoregressive processes. For long horizon forecasting, we introduce a

    "closed-loop" variation of the companion SSM, which enables SpaceTime to

    predict many future time-steps by generating its own layer-wise inputs. For

    efficient training and inference, we introduce an algorithm that reduces the

    memory and compute of a forward pass with the companion matrix. With sequence

    length $\ell$ and state-space size $d$, we go from $\tilde{O}(d \ell)$

    na\"ively to $\tilde{O}(d + \ell)$. In experiments, our contributions lead to

    state-of-the-art results on extensive and diverse benchmarks, with best or

    second-best AUROC on 6 / 7 ECG and speech time series classification, and best

    MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1)

    fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more

    accurately on longer horizons than prior state-of-the-art, and (3) speeds up

    training on real-world ETTh1 data by 73% and 80% relative wall-clock time over

    Transformers and LSTMs.'
  arxivId: '2303.09489'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Michael Zhang, Khaled K. Saab, Michael Poli, Tri Dao, Karan Goel, Christopher
    Ré
  created_at: '2024-12-30T14:43:27.202573'
  issue_number: 394
  issue_url: https://github.com/dmarx/papers-feed/issues/394
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:09:58.237Z'
  main_tex_file: null
  published_date: '2023-03-16T17:08:21Z'
  state: open
  title: Effectively Modeling Time Series with Simple Discrete State Spaces
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2303.09489
'2305.06161':
  abstract: 'The BigCode community, an open-scientific collaboration working on the

    responsible development of Large Language Models for Code (Code LLMs),

    introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context

    length, infilling capabilities and fast large-batch inference enabled by

    multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced

    from The Stack, a large collection of permissively licensed GitHub repositories

    with inspection tools and an opt-out process. We fine-tuned StarCoderBase on

    35B Python tokens, resulting in the creation of StarCoder. We perform the most

    comprehensive evaluation of Code LLMs to date and show that StarCoderBase

    outperforms every open Code LLM that supports multiple programming languages

    and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,

    StarCoder outperforms every model that is fine-tuned on Python, can be prompted

    to achieve 40\% pass@1 on HumanEval, and still retains its performance on other

    programming languages. We take several important steps towards a safe

    open-access model release, including an improved PII redaction pipeline and a

    novel attribution tracing tool, and make the StarCoder models publicly

    available under a more commercially viable version of the Open Responsible AI

    Model license.'
  arxivId: '2305.06161'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.PL
  - cs.SE
  authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
    Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii
    Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
    Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,
    Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham
    Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry
    Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
    Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor
    Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger,
    Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
    Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel
    Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
    Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
  created_at: '2024-12-30T14:43:30.205761'
  issue_number: 392
  issue_url: https://github.com/dmarx/papers-feed/issues/392
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:43:30.206539'
  last_visited: '2024-12-28T07:09:41.282Z'
  main_tex_file: null
  published_date: '2023-05-09T08:16:42Z'
  state: open
  title: 'StarCoder: may the source be with you!'
  total_reading_time_seconds: 12
  url: https://arxiv.org/abs/2305.06161
'2307.08691':
  abstract: 'Scaling Transformers to longer sequence lengths has been a major problem
    in

    the last several years, promising to improve performance in language modeling

    and high-resolution image understanding, as well as to unlock new applications

    in code, audio, and video generation. The attention layer is the main

    bottleneck in scaling to longer sequences, as its runtime and memory increase

    quadratically in the sequence length. FlashAttention exploits the asymmetric

    GPU memory hierarchy to bring significant memory saving (linear instead of

    quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines),

    with no approximation. However, FlashAttention is still not nearly as fast as

    optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the

    theoretical maximum FLOPs/s. We observe that the inefficiency is due to

    suboptimal work partitioning between different thread blocks and warps on the

    GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We

    propose FlashAttention-2, with better work partitioning to address these

    issues. In particular, we (1) tweak the algorithm to reduce the number of

    non-matmul FLOPs (2) parallelize the attention computation, even for a single

    head, across different thread blocks to increase occupancy, and (3) within each

    thread block, distribute the work between warps to reduce communication through

    shared memory. These yield around 2$\times$ speedup compared to FlashAttention,

    reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close

    to the efficiency of GEMM operations. We empirically validate that when used

    end-to-end to train GPT-style models, FlashAttention-2 reaches training speed

    of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).'
  arxivId: '2307.08691'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao
  created_at: '2024-12-30T14:43:33.227077'
  issue_number: 391
  issue_url: https://github.com/dmarx/papers-feed/issues/391
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:08:37.022Z'
  main_tex_file: null
  published_date: '2023-07-17T17:50:36Z'
  state: open
  title: "FlashAttention-2: Faster Attention with Better Parallelism and Work\n  Partitioning"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2307.08691
'2307.09288':
  abstract: 'In this work, we develop and release Llama 2, a collection of pretrained
    and

    fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70

    billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for

    dialogue use cases. Our models outperform open-source chat models on most

    benchmarks we tested, and based on our human evaluations for helpfulness and

    safety, may be a suitable substitute for closed-source models. We provide a

    detailed description of our approach to fine-tuning and safety improvements of

    Llama 2-Chat in order to enable the community to build on our work and

    contribute to the responsible development of LLMs.'
  arxivId: '2307.09288'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
  created_at: '2024-12-30T08:28:19.536753'
  issue_number: 434
  issue_url: https://github.com/dmarx/papers-feed/issues/434
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:19.537524'
  last_visited: '2024-12-28T20:46:35.699Z'
  main_tex_file: null
  published_date: '2023-07-18T14:31:57Z'
  state: open
  title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models'
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2307.09288
'2308.06259':
  abstract: 'We present a scalable method to build a high quality instruction following

    language model by automatically labelling human-written text with corresponding

    instructions. Our approach, named instruction backtranslation, starts with a

    language model finetuned on a small amount of seed data, and a given web

    corpus. The seed model is used to construct training examples by generating

    instruction prompts for web documents (self-augmentation), and then selecting

    high quality examples from among these candidates (self-curation). This data is

    then used to finetune a stronger model. Finetuning LLaMa on two iterations of

    our approach yields a model that outperforms all other LLaMa-based models on

    the Alpaca leaderboard not relying on distillation data, demonstrating highly

    effective self-alignment.'
  arxivId: '2308.06259'
  arxiv_tags:
  - cs.CL
  authors: Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer,
    Jason Weston, Mike Lewis
  created_at: '2024-12-30T08:26:46.536724'
  issue_number: 575
  issue_url: https://github.com/dmarx/papers-feed/issues/575
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T04:57:10.471Z'
  main_tex_file: null
  published_date: '2023-08-11T17:47:54Z'
  state: open
  title: Self-Alignment with Instruction Backtranslation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2308.06259
'2309.07965':
  abstract: 'In this work, we introduce the concept of relative Lipschitz saturation,

    along with its key categorical and algebraic properties, and demonstrate how

    such a structure always gives rise to a radicial algebra.'
  arxivId: '2309.07965'
  arxiv_tags:
  - math.AC
  - 13B22
  authors: Thiago da Silva, Guilherme Schultz Netto
  created_at: '2024-12-30T08:28:28.531062'
  issue_number: 398
  issue_url: https://github.com/dmarx/papers-feed/issues/398
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:28.532202'
  last_visited: '2024-12-28T07:12:00.313000+00:00'
  main_tex_file: null
  published_date: '2023-09-14T18:02:12Z'
  state: open
  title: "A survey on relative Lipschitz saturation of algebras and its relation\n\
    \  with radicial algebras"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2309.07965
'2309.12032':
  abstract: 'Structure learning is the crux of causal inference. Notably, causal discovery

    (CD) algorithms are brittle when data is scarce, possibly inferring imprecise

    causal relations that contradict expert knowledge -- especially when

    considering latent confounders. To aggravate the issue, most CD methods do not

    provide uncertainty estimates, making it hard for users to interpret results

    and improve the inference process. Surprisingly, while CD is a human-centered

    affair, no works have focused on building methods that both 1) output

    uncertainty estimates that can be verified by experts and 2) interact with

    those experts to iteratively refine CD. To solve these issues, we start by

    proposing to sample (causal) ancestral graphs proportionally to a belief

    distribution based on a score function, such as the Bayesian information

    criterion (BIC), using generative flow networks. Then, we leverage the

    diversity in candidate graphs and introduce an optimal experimental design to

    iteratively probe the expert about the relations among variables, effectively

    reducing the uncertainty of our belief over ancestral graphs. Finally, we

    update our samples to incorporate human feedback via importance sampling.

    Importantly, our method does not require causal sufficiency (i.e., unobserved

    confounders may exist). Experiments with synthetic observational data show that

    our method can accurately sample from distributions over ancestral graphs and

    that we can greatly improve inference quality with human aid.'
  arxivId: '2309.12032'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Tiago da Silva, Eliezer Silva, António Góis, Dominik Heider, Samuel Kaski,
    Diego Mesquita, Adèle Ribeiro
  created_at: '2024-12-30T14:43:36.198525'
  issue_number: 389
  issue_url: https://github.com/dmarx/papers-feed/issues/389
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:43:36.200393'
  last_visited: '2024-12-28T07:08:27.780000+00:00'
  main_tex_file: null
  published_date: '2023-09-21T12:53:45Z'
  state: open
  title: "Human-in-the-Loop Causal Discovery under Latent Confounding using\n  Ancestral\
    \ GFlowNets"
  total_reading_time_seconds: 12
  url: https://arxiv.org/abs/2309.12032
'2309.14556':
  abstract: 'Researchers have argued that large language models (LLMs) exhibit

    high-quality writing capabilities from blogs to stories. However, evaluating

    objectively the creativity of a piece of writing is challenging. Inspired by

    the Torrance Test of Creative Thinking (TTCT), which measures creativity as a

    process, we use the Consensual Assessment Technique [3] and propose the

    Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product.

    TTCW consists of 14 binary tests organized into the original dimensions of

    Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative

    writers and implement a human assessment of 48 stories written either by

    professional authors or LLMs using TTCW. Our analysis shows that LLM-generated

    stories pass 3-10X less TTCW tests than stories written by professionals. In

    addition, we explore the use of LLMs as assessors to automate the TTCW

    evaluation, revealing that none of the LLMs positively correlate with the

    expert assessments.'
  arxivId: '2309.14556'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.HC
  authors: Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan,
    Chien-Sheng Wu
  created_at: '2024-12-30T14:43:24.411531'
  issue_number: 585
  issue_url: https://github.com/dmarx/papers-feed/issues/585
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T14:42:55.576Z'
  main_tex_file: null
  published_date: '2023-09-25T22:02:46Z'
  state: open
  title: "Art or Artifice? Large Language Models and the False Promise of\n  Creativity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2309.14556
'2310.17157':
  abstract: 'Large language models (LLMs) with hundreds of billions of parameters
    have

    sparked a new wave of exciting AI applications. However, they are

    computationally expensive at inference time. Sparsity is a natural approach to

    reduce this cost, but existing methods either require costly retraining, have

    to forgo LLM''s in-context learning ability, or do not yield wall-clock time

    speedup on modern hardware. We hypothesize that contextual sparsity, which are

    small, input-dependent sets of attention heads and MLP parameters that yield

    approximately the same output as the dense model for a given input, can address

    these issues. We show that contextual sparsity exists, that it can be

    accurately predicted, and that we can exploit it to speed up LLM inference in

    wall-clock time without compromising LLM''s quality or in-context learning

    ability. Based on these insights, we propose DejaVu, a system that uses a

    low-cost algorithm to predict contextual sparsity on the fly given inputs to

    each layer, along with an asynchronous and hardware-aware implementation that

    speeds up LLM inference. We validate that DejaVu can reduce the inference

    latency of OPT-175B by over 2X compared to the state-of-the-art

    FasterTransformer, and over 6X compared to the widely used Hugging Face

    implementation, without compromising model quality. The code is available at

    https://github.com/FMInference/DejaVu.'
  arxivId: '2310.17157'
  arxiv_tags:
  - cs.LG
  authors: Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
    Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen
  created_at: '2024-12-30T14:43:39.227471'
  issue_number: 384
  issue_url: https://github.com/dmarx/papers-feed/issues/384
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:43:39.229852'
  last_visited: '2024-12-28T07:07:05.762Z'
  main_tex_file: null
  published_date: '2023-10-26T05:01:09Z'
  state: open
  title: 'Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time'
  total_reading_time_seconds: 36
  url: https://arxiv.org/abs/2310.17157
'2312.00752':
  abstract: 'Foundation models, now powering most of the exciting applications in
    deep

    learning, are almost universally based on the Transformer architecture and its

    core attention module. Many subquadratic-time architectures such as linear

    attention, gated convolution and recurrent models, and structured state space

    models (SSMs) have been developed to address Transformers'' computational

    inefficiency on long sequences, but they have not performed as well as

    attention on important modalities such as language. We identify that a key

    weakness of such models is their inability to perform content-based reasoning,

    and make several improvements. First, simply letting the SSM parameters be

    functions of the input addresses their weakness with discrete modalities,

    allowing the model to selectively propagate or forget information along the

    sequence length dimension depending on the current token. Second, even though

    this change prevents the use of efficient convolutions, we design a

    hardware-aware parallel algorithm in recurrent mode. We integrate these

    selective SSMs into a simplified end-to-end neural network architecture without

    attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$

    higher throughput than Transformers) and linear scaling in sequence length, and

    its performance improves on real data up to million-length sequences. As a

    general sequence model backbone, Mamba achieves state-of-the-art performance

    across several modalities such as language, audio, and genomics. On language

    modeling, our Mamba-3B model outperforms Transformers of the same size and

    matches Transformers twice its size, both in pretraining and downstream

    evaluation.'
  arxivId: '2312.00752'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Albert Gu, Tri Dao
  created_at: '2024-12-30T14:43:42.200559'
  issue_number: 383
  issue_url: https://github.com/dmarx/papers-feed/issues/383
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T06:55:41.314Z'
  main_tex_file: null
  published_date: '2023-12-01T18:01:34Z'
  state: open
  title: 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2312.00752
'2312.17127':
  abstract: "We study semantic models of probabilistic programming languages over\
    \ graphs,\nand establish a connection to graphons from graph theory and combinatorics.\
    \ We\nshow that every well-behaved equational theory for our graph probabilistic\n\
    programming language corresponds to a graphon, and conversely, every graphon\n\
    arises in this way.\n  We provide three constructions for showing that every graphon\
    \ arises from an\nequational theory. The first is an abstract construction, using\
    \ Markov\ncategories and monoidal indeterminates. The second and third are more\
    \ concrete.\nThe second is in terms of traditional measure theoretic probability,\
    \ which\ncovers 'black-and-white' graphons. The third is in terms of probability\
    \ monads\non the nominal sets of Gabbay and Pitts. Specifically, we use a variation\
    \ of\nnominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\\
    'enyi\ngraphons. In this way, we build new models of graph probabilistic programming\n\
    from graphons."
  arxivId: '2312.17127'
  arxiv_tags:
  - cs.PL
  - cs.LO
  - math.PR
  authors: Nathanael L. Ackerman, Cameron E. Freer, Younesse Kaddar, Jacek Karwowski,
    Sean K. Moss, Daniel M. Roy, Sam Staton, Hongseok Yang
  created_at: '2024-12-30T08:27:28.537115'
  issue_number: 509
  issue_url: https://github.com/dmarx/papers-feed/issues/509
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:28.538601'
  last_visited: '2024-12-29T19:52:09.230Z'
  main_tex_file: null
  published_date: '2023-12-28T17:04:50Z'
  state: open
  title: "Probabilistic programming interfaces for random graphs: Markov\n  categories,\
    \ graphons, and nominal sets"
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2312.17127
'2401.10774':
  abstract: "Large Language Models (LLMs) employ auto-regressive decoding that requires\n\
    sequential computation, with each step reliant on the previous one's output.\n\
    This creates a bottleneck as each step necessitates moving the full model\nparameters\
    \ from High-Bandwidth Memory (HBM) to the accelerator's cache. While\nmethods\
    \ such as speculative decoding have been suggested to address this issue,\ntheir\
    \ implementation is impeded by the challenges associated with acquiring and\n\
    maintaining a separate draft model. In this paper, we present Medusa, an\nefficient\
    \ method that augments LLM inference by adding extra decoding heads to\npredict\
    \ multiple subsequent tokens in parallel. Using a tree-based attention\nmechanism,\
    \ Medusa constructs multiple candidate continuations and verifies them\nsimultaneously\
    \ in each decoding step. By leveraging parallel processing, Medusa\nsubstantially\
    \ reduces the number of decoding steps required. We present two\nlevels of fine-tuning\
    \ procedures for Medusa to meet the needs of different use\ncases: Medusa-1: Medusa\
    \ is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference\
    \ acceleration. Medusa-2: Medusa is fine-tuned\ntogether with the backbone LLM,\
    \ enabling better prediction accuracy of Medusa\nheads and higher speedup but\
    \ needing a special training recipe that preserves\nthe backbone model's capabilities.\n\
    \  Moreover, we propose several extensions that improve or expand the utility\
    \ of\nMedusa, including a self-distillation to handle situations where no training\n\
    data is available and a typical acceptance scheme to boost the acceptance rate\n\
    while maintaining generation quality. We evaluate Medusa on models of various\n\
    sizes and training procedures. Our experiments demonstrate that Medusa-1 can\n\
    achieve over 2.2x speedup without compromising generation quality, while\nMedusa-2\
    \ further improves the speedup to 2.3-3.6x."
  arxivId: '2401.10774'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming
    Chen, Tri Dao
  created_at: '2024-12-30T08:28:31.552216'
  issue_number: 395
  issue_url: https://github.com/dmarx/papers-feed/issues/395
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:31.553017'
  last_visited: '2024-12-28T07:11:26.727Z'
  main_tex_file: null
  published_date: '2024-01-19T15:48:40Z'
  state: open
  title: "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding\
    \ Heads"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2401.10774
'2402.10193':
  abstract: 'Large Language Models (LLMs) are typically trained in two phases:

    pre-training on large internet-scale datasets, and fine-tuning for downstream

    tasks. Given the higher computational demand of pre-training, it''s intuitive
    to

    assume that fine-tuning adds less new information to the model, and is thus

    more compressible. We explore this assumption by decomposing the weights of

    fine-tuned models into their pre-trained components and an additional delta. We

    introduce a simple method, BitDelta, which successfully quantizes this delta

    down to 1 bit without compromising performance. This interesting finding not

    only highlights the potential redundancy of information added during

    fine-tuning, but also has significant implications for the multi-tenant serving

    and multi-tenant storage of fine-tuned models. By enabling the use of a single

    high-precision base model accompanied by multiple 1-bit deltas, BitDelta

    dramatically reduces GPU memory requirements by more than 10x, which can also

    be translated to enhanced generation latency in multi-tenant settings. We

    validate BitDelta through experiments across Llama-2 and Mistral model

    families, and on models up to 70B parameters, showcasing minimal performance

    degradation over all tested settings.'
  arxivId: '2402.10193'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle
    Cai
  created_at: '2024-12-30T14:43:45.223449'
  issue_number: 379
  issue_url: https://github.com/dmarx/papers-feed/issues/379
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:43:45.227170'
  last_visited: '2024-12-28T06:52:07.941000+00:00'
  main_tex_file: null
  published_date: '2024-02-15T18:50:06Z'
  state: open
  title: 'BitDelta: Your Fine-Tune May Only Be Worth One Bit'
  total_reading_time_seconds: 73
  url: https://arxiv.org/abs/2402.10193
'2402.16670':
  abstract: 'Over the last 70 years, we, humans, have created an economic market where

    attention is being captured and turned into money thanks to advertising. During

    the last two decades, leveraging research in psychology, sociology,

    neuroscience and other domains, Web platforms have brought the process of

    capturing attention to an unprecedented scale. With the initial commonplace

    goal of making targeted advertising more effective, the generalization of

    attention-capturing techniques and their use of cognitive biases and emotions

    have multiple detrimental side effects such as polarizing opinions, spreading

    false information and threatening public health, economies and democracies.

    This is clearly a case where the Web is not used for the common good and where,

    in fact, all its users become a vulnerable population. This paper brings

    together contributions from a wide range of disciplines to analyze current

    practices and consequences thereof. Through a set of propositions and

    principles that could be used do drive further works, it calls for actions

    against these practices competing to capture our attention on the Web, as it

    would be unsustainable for a civilization to allow attention to be wasted with

    impunity on a world-wide scale.'
  arxivId: '2402.16670'
  arxiv_tags:
  - cs.SI
  authors: Franck Michel, Fabien Gandon
  created_at: '2024-12-30T14:44:09.189567'
  issue_number: 330
  issue_url: https://github.com/dmarx/papers-feed/issues/330
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-27T22:07:23.174Z'
  main_tex_file: null
  published_date: '2024-02-26T15:46:43Z'
  state: open
  title: "Pay Attention: a Call to Regulate the Attention Market and Prevent\n  Algorithmic\
    \ Emotional Governance"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2402.16670
'2402.19173':
  abstract: 'The BigCode project, an open-scientific collaboration focused on the

    responsible development of Large Language Models for Code (Code LLMs),

    introduces StarCoder2. In partnership with Software Heritage (SWH), we build

    The Stack v2 on top of the digital commons of their source code archive.

    Alongside the SWH repositories spanning 619 programming languages, we carefully

    select other high-quality data sources, such as GitHub pull requests, Kaggle

    notebooks, and code documentation. This results in a training set that is 4x

    larger than the first StarCoder dataset. We train StarCoder2 models with 3B,

    7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate

    them on a comprehensive set of Code LLM benchmarks. We find that our small

    model, StarCoder2-3B, outperforms other Code LLMs of similar size on most

    benchmarks, and also outperforms StarCoderBase-15B. Our large model,

    StarCoder2- 15B, significantly outperforms other models of comparable size. In

    addition, it matches or outperforms CodeLlama-34B, a model more than twice its

    size. Although DeepSeekCoder- 33B is the best-performing model at code

    completion for high-resource languages, we find that StarCoder2-15B outperforms

    it on math and code reasoning benchmarks, as well as several low-resource

    languages. We make the model weights available under an OpenRAIL license and

    ensure full transparency regarding the training data by releasing the SoftWare

    Heritage persistent IDentifiers (SWHIDs) of the source code data.'
  arxivId: '2402.19173'
  arxiv_tags:
  - cs.SE
  - cs.AI
  authors: Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,
    Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu,
    Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu,
    Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li,
    Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu,
    Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun
    Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki,
    Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel
    Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten
    Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados,
    Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming
    Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
  created_at: '2024-12-30T14:43:48.213109'
  issue_number: 377
  issue_url: https://github.com/dmarx/papers-feed/issues/377
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:43:48.214522'
  last_visited: '2024-12-28T06:49:56.113000+00:00'
  main_tex_file: null
  published_date: '2024-02-29T13:53:35Z'
  state: open
  title: 'StarCoder 2 and The Stack v2: The Next Generation'
  total_reading_time_seconds: 9
  url: https://arxiv.org/abs/2402.19173
'2403.00231':
  abstract: 'Large vision-language models (LVLMs) excel across diverse tasks involving

    concrete images from natural scenes. However, their ability to interpret

    abstract figures, such as geometry shapes and scientific plots, remains limited

    due to a scarcity of training datasets in scientific domains. To fill this gap,

    we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for

    enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset

    comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers

    spanning various scientific domains. Drawing from ArXivCap, we introduce

    ArXivQA, a question-answering dataset generated by prompting GPT-4V based on

    scientific figures. ArXivQA greatly enhances open-sourced LVLMs'' mathematical

    reasoning capabilities, achieving a 10.4\% absolute accuracy gain on a

    multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap,

    we devise four vision-to-text tasks for benchmarking LVLMs. Evaluation results

    with state-of-the-art LVLMs underscore their struggle with the nuanced

    semantics of academic figures, while domain-specific training yields

    substantial performance gains. Our error analysis uncovers misinterpretations

    of visual context, recognition errors, and the production of overly simplified

    captions by current LVLMs, shedding light on future improvements.'
  arxivId: '2403.00231'
  arxiv_tags:
  - cs.CV
  - cs.CL
  authors: Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong,
    Qi Liu
  created_at: '2024-12-30T14:44:18.197112'
  issue_number: 311
  issue_url: https://github.com/dmarx/papers-feed/issues/311
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:18.199164'
  last_visited: '2024-12-27T05:14:48.788Z'
  main_tex_file: null
  published_date: '2024-03-01T02:21:30Z'
  state: open
  title: "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of\n\
    \  Large Vision-Language Models"
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2403.00231
'2403.03353':
  abstract: 'This paper introduces a hypothesis space for deep learning that employs
    deep

    neural networks (DNNs). By treating a DNN as a function of two variables, the

    physical variable and parameter variable, we consider the primitive set of the

    DNNs for the parameter variable located in a set of the weight matrices and

    biases determined by a prescribed depth and widths of the DNNs. We then

    complete the linear span of the primitive DNN set in a weak* topology to

    construct a Banach space of functions of the physical variable. We prove that

    the Banach space so constructed is a reproducing kernel Banach space (RKBS) and

    construct its reproducing kernel. We investigate two learning models,

    regularized learning and minimum interpolation problem in the resulting RKBS,

    by establishing representer theorems for solutions of the learning models. The

    representer theorems unfold that solutions of these learning models can be

    expressed as linear combination of a finite number of kernel sessions

    determined by given data and the reproducing kernel.'
  arxivId: '2403.03353'
  arxiv_tags:
  - stat.ML
  - cs.LG
  - math.FA
  authors: Rui Wang, Yuesheng Xu, Mingsong Yan
  created_at: '2024-12-30T08:27:46.542218'
  issue_number: 491
  issue_url: https://github.com/dmarx/papers-feed/issues/491
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:46.543045'
  last_visited: '2024-12-29T10:46:53.922Z'
  main_tex_file: null
  published_date: '2024-03-05T22:42:29Z'
  state: open
  title: Hypothesis Spaces for Deep Learning
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2403.03353
'2403.10304':
  abstract: 'We present a Wikidata-based framework, called KIF, for virtually integrating

    heterogeneous knowledge sources. KIF is written in Python and is released as

    open-source. It leverages Wikidata''s data model and vocabulary plus

    user-defined mappings to construct a unified view of the underlying sources

    while keeping track of the context and provenance of their statements. The

    underlying sources can be triplestores, relational databases, CSV files, etc.,

    which may or may not use the vocabulary and RDF encoding of Wikidata. The end

    result is a virtual knowledge base which behaves like an "extended Wikidata"

    and which can be queried using a simple but expressive pattern language,

    defined in terms of Wikidata''s data model. In this paper, we present the design

    and implementation of KIF, discuss how we have used it to solve a real

    integration problem in the domain of chemistry (involving Wikidata, PubChem,

    and IBM CIRCA), and present experimental results on the performance and

    overhead of KIF'
  arxivId: '2403.10304'
  arxiv_tags:
  - cs.AI
  - cs.DB
  authors: Guilherme Lima, João M. B. Rodrigues, Marcelo Machado, Elton Soares, Sandro
    R. Fiorini, Raphael Thiago, Leonardo G. Azevedo, Viviane T. da Silva, Renato Cerqueira
  created_at: '2024-12-30T14:43:51.217528'
  issue_number: 375
  issue_url: https://github.com/dmarx/papers-feed/issues/375
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T06:48:13.707Z'
  main_tex_file: null
  published_date: '2024-03-15T13:46:36Z'
  state: open
  title: "KIF: A Wikidata-Based Framework for Integrating Heterogeneous Knowledge\n\
    \  Sources"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2403.10304
'2403.12187':
  abstract: 'Motivated by the abundance of functional data such as time series and
    images,

    there has been a growing interest in integrating such data into neural networks

    and learning maps from function spaces to R (i.e., functionals). In this paper,

    we study the approximation of functionals on reproducing kernel Hilbert spaces

    (RKHS''s) using neural networks. We establish the universality of the

    approximation of functionals on the RKHS''s. Specifically, we derive explicit

    error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev

    kernels. Moreover, we apply our findings to functional regression, proving that

    neural networks can accurately approximate the regression maps in generalized

    functional linear models. Existing works on functional learning require

    integration-type basis function expansions with a set of pre-specified basis

    functions. By leveraging the interpolating orthogonal projections in RKHS''s,

    our proposed network is much simpler in that we use point evaluations to

    replace basis function expansions.'
  arxivId: '2403.12187'
  arxiv_tags:
  - stat.ML
  - cs.LG
  - math.ST
  - stat.TH
  authors: Tian-Yi Zhou, Namjoon Suh, Guang Cheng, Xiaoming Huo
  created_at: '2024-12-30T08:27:40.564686'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-30T08:27:40.565450'
  last_visited: '2024-12-29T11:02:27.587000+00:00'
  main_tex_file: null
  published_date: '2024-03-18T18:58:23Z'
  state: open
  title: Approximation of RKHS Functionals by Neural Networks
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2403.12187
'2405.04434':
  abstract: 'We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model

    characterized by economical training and efficient inference. It comprises 236B

    total parameters, of which 21B are activated for each token, and supports a

    context length of 128K tokens. DeepSeek-V2 adopts innovative architectures

    including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees

    efficient inference through significantly compressing the Key-Value (KV) cache

    into a latent vector, while DeepSeekMoE enables training strong models at an

    economical cost through sparse computation. Compared with DeepSeek 67B,

    DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves

    42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum

    generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality

    and multi-source corpus consisting of 8.1T tokens, and further perform

    Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock

    its potential. Evaluation results show that, even with only 21B activated

    parameters, DeepSeek-V2 and its chat versions still achieve top-tier

    performance among open-source models.'
  arxivId: '2405.04434'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang
    Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen,
    Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei
    Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo
    Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li,
    Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan,
    Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun
    Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan
    Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R.
    L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan
    Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang
    Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu
    Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao
    Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang,
    Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang
    Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan
    Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang,
    Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang,
    Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan
    Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou,
    Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren,
    Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao,
    Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin
    Li, Ziwei Xie
  created_at: '2024-12-30T14:44:24.208023'
  issue_number: 281
  issue_url: https://github.com/dmarx/papers-feed/issues/281
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:24.211461'
  last_visited: '2024-12-26T13:01:17.331Z'
  main_tex_file: null
  published_date: '2024-05-07T15:56:43Z'
  state: open
  title: "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts\n  Language\
    \ Model"
  total_reading_time_seconds: 46
  url: https://arxiv.org/abs/2405.04434
'2405.07987':
  abstract: 'We argue that representations in AI models, particularly deep networks,
    are

    converging. First, we survey many examples of convergence in the literature:

    over time and across multiple domains, the ways by which different neural

    networks represent data are becoming more aligned. Next, we demonstrate

    convergence across data modalities: as vision models and language models get

    larger, they measure distance between datapoints in a more and more alike way.

    We hypothesize that this convergence is driving toward a shared statistical

    model of reality, akin to Plato''s concept of an ideal reality. We term such a

    representation the platonic representation and discuss several possible

    selective pressures toward it. Finally, we discuss the implications of these

    trends, their limitations, and counterexamples to our analysis.'
  arxivId: '2405.07987'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  - cs.NE
  authors: Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola
  created_at: '2024-12-30T08:28:13.570462'
  issue_number: 439
  issue_url: https://github.com/dmarx/papers-feed/issues/439
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:39:54.598Z'
  main_tex_file: null
  published_date: '2024-05-13T17:58:30Z'
  state: open
  title: The Platonic Representation Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.07987
'2405.21060':
  abstract: 'While Transformers have been the main architecture behind deep learning''s

    success in language modeling, state-space models (SSMs) such as Mamba have

    recently been shown to match or outperform Transformers at small to medium

    scale. We show that these families of models are actually quite closely

    related, and develop a rich framework of theoretical connections between SSMs

    and variants of attention, connected through various decompositions of a

    well-studied class of structured semiseparable matrices. Our state space

    duality (SSD) framework allows us to design a new architecture (Mamba-2) whose

    core layer is an a refinement of Mamba''s selective SSM that is 2-8X faster,

    while continuing to be competitive with Transformers on language modeling.'
  arxivId: '2405.21060'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao, Albert Gu
  created_at: '2024-12-30T14:43:54.221353'
  issue_number: 374
  issue_url: https://github.com/dmarx/papers-feed/issues/374
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:43:54.222879'
  last_visited: '2024-12-28T06:47:09.217000+00:00'
  main_tex_file: null
  published_date: '2024-05-31T17:50:01Z'
  state: open
  title: "Transformers are SSMs: Generalized Models and Efficient Algorithms\n  Through\
    \ Structured State Space Duality"
  total_reading_time_seconds: 7
  url: https://arxiv.org/abs/2405.21060
'2406.01506':
  abstract: 'The linear representation hypothesis is the informal idea that semantic

    concepts are encoded as linear directions in the representation spaces of large

    language models (LLMs). Previous work has shown how to make this notion precise

    for representing binary concepts that have natural contrasts (e.g., {male,

    female}) as directions in representation space. However, many natural concepts

    do not have natural contrasts (e.g., whether the output is about an animal). In

    this work, we show how to extend the formalization of the linear representation

    hypothesis to represent features (e.g., is_animal) as vectors. This allows us

    to immediately formalize the representation of categorical concepts as

    polytopes in the representation space. Further, we use the formalization to

    prove a relationship between the hierarchical structure of concepts and the

    geometry of their representations. We validate these theoretical results on the

    Gemma and LLaMA-3 large language models, estimating representations for 900+

    hierarchically related concepts using data from WordNet.'
  arxivId: '2406.01506'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch
  created_at: '2024-12-30T08:28:16.542738'
  issue_number: 438
  issue_url: https://github.com/dmarx/papers-feed/issues/438
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:39:53.364Z'
  main_tex_file: null
  published_date: '2024-06-03T16:34:01Z'
  state: open
  title: "The Geometry of Categorical and Hierarchical Concepts in Large Language\n\
    \  Models"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.01506
'2407.01392':
  abstract: 'This paper presents Diffusion Forcing, a new training paradigm where
    a

    diffusion model is trained to denoise a set of tokens with independent

    per-token noise levels. We apply Diffusion Forcing to sequence generative

    modeling by training a causal next-token prediction model to generate one or

    several future tokens without fully diffusing past ones. Our approach is shown

    to combine the strengths of next-token prediction models, such as

    variable-length generation, with the strengths of full-sequence diffusion

    models, such as the ability to guide sampling to desirable trajectories. Our

    method offers a range of additional capabilities, such as (1) rolling-out

    sequences of continuous tokens, such as video, with lengths past the training

    horizon, where baselines diverge and (2) new sampling and guiding schemes that

    uniquely profit from Diffusion Forcing''s variable-horizon and causal

    architecture, and which lead to marked performance gains in decision-making and

    planning tasks. In addition to its empirical success, our method is proven to

    optimize a variational lower bound on the likelihoods of all subsequences of

    tokens drawn from the true joint distribution. Project website:

    https://boyuan.space/diffusion-forcing'
  arxivId: '2407.01392'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - cs.RO
  authors: Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake,
    Vincent Sitzmann
  created_at: '2024-12-30T14:44:15.202144'
  issue_number: 316
  issue_url: https://github.com/dmarx/papers-feed/issues/316
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:15.206543'
  last_visited: '2024-12-27T06:40:03.099Z'
  main_tex_file: null
  published_date: '2024-07-01T15:43:25Z'
  state: open
  title: 'Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion'
  total_reading_time_seconds: 34
  url: https://arxiv.org/abs/2407.01392
'2407.05872':
  abstract: 'Robust and effective scaling of models from small to large width typically

    requires the precise adjustment of many algorithmic and architectural details,

    such as parameterization and optimizer choices. In this work, we propose a new

    perspective on parameterization by investigating a key assumption in prior work

    about the alignment between parameters and data and derive new theoretical

    results under weaker assumptions and a broader set of optimizers. Our extensive

    empirical investigation includes tens of thousands of models trained with all

    combinations of three optimizers, four parameterizations, several alignment

    assumptions, more than a dozen learning rates, and fourteen model sizes up to

    26.8B parameters. We find that the best learning rate scaling prescription

    would often have been excluded by the assumptions in prior work. Our results

    show that all parameterizations, not just maximal update parameterization

    (muP), can achieve hyperparameter transfer; moreover, our novel per-layer

    learning rate prescription for standard parameterization outperforms muP.

    Finally, we demonstrate that an overlooked aspect of parameterization, the

    epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow

    and propose Adam-atan2, a new numerically stable, scale-invariant version of

    Adam that eliminates the epsilon hyperparameter entirely.'
  arxivId: '2407.05872'
  arxiv_tags:
  - cs.LG
  authors: Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman
    Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling,
    Jaehoon Lee, Jeffrey Pennington
  created_at: '2024-12-30T14:44:30.235711'
  issue_number: 255
  issue_url: https://github.com/dmarx/papers-feed/issues/255
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:30.236437'
  last_visited: '2024-12-24T21:09:44.860Z'
  main_tex_file: null
  published_date: '2024-07-08T12:32:51Z'
  state: open
  title: Scaling Exponents Across Parameterizations and Optimizers
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2407.05872
'2407.08608':
  abstract: 'Attention, as a core layer of the ubiquitous Transformer architecture,
    is the

    bottleneck for large language models and long-context applications.

    FlashAttention elaborated an approach to speed up attention on GPUs through

    minimizing memory reads/writes. However, it has yet to take advantage of new

    capabilities present in recent hardware, with FlashAttention-2 achieving only

    35% utilization on the H100 GPU. We develop three main techniques to speed up

    attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to

    (1) overlap overall computation and data movement via warp-specialization and

    (2) interleave block-wise matmul and softmax operations, and (3) block

    quantization and incoherent processing that leverages hardware support for FP8

    low-precision. We demonstrate that our method, FlashAttention-3, achieves

    speedup on H100 GPUs by 1.5-2.0$\times$ with FP16 reaching up to 740 TFLOPs/s

    (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate

    that FP8 FlashAttention-3 achieves 2.6$\times$ lower numerical error than a

    baseline FP8 attention.'
  arxivId: '2407.08608'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani,
    Tri Dao
  created_at: '2024-12-30T14:43:57.200083'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-30T14:43:57.200892'
  last_visited: '2024-12-28T06:46:56.164000+00:00'
  main_tex_file: null
  published_date: '2024-07-11T15:44:48Z'
  state: open
  title: "FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2407.08608
'2407.21787':
  abstract: 'Scaling the amount of compute used to train language models has dramatically

    improved their capabilities. However, when it comes to inference, we often

    limit the amount of compute to only one attempt per problem. Here, we explore

    inference compute as another axis for scaling by increasing the number of

    generated samples. Across multiple tasks and models, we observe that coverage
    -

    the fraction of problems solved by any attempt - scales with the number of

    samples over four orders of magnitude. In domains like coding and formal

    proofs, where all answers can be automatically verified, these increases in

    coverage directly translate into improved performance. When we apply repeated

    sampling to SWE-bench Lite, the fraction of issues solved with

    DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250

    samples, outperforming the single-attempt state-of-the-art of 43% which uses

    more capable frontier models. Moreover, using current API pricing, amplifying

    the cheaper DeepSeek model with five samples is more cost-effective and solves

    more issues than paying a premium for one sample from GPT-4o or Claude 3.5

    Sonnet. Interestingly, the relationship between coverage and the number of

    samples is often log-linear and can be modelled with an exponentiated power

    law, suggesting the existence of inference-time scaling laws. Finally, we find

    that identifying correct samples out of many generations remains an important

    direction for future research in domains without automatic verifiers. When

    solving math word problems from GSM8K and MATH, coverage with Llama-3 models

    grows to over 95% with 10,000 samples. However, common methods to pick correct

    solutions from a sample collection, such as majority voting or reward models,

    plateau beyond several hundred samples and fail to fully scale with the sample

    budget.'
  arxivId: '2407.21787'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le,
    Christopher Ré, Azalia Mirhoseini
  created_at: '2024-12-30T08:27:25.557036'
  issue_number: 513
  issue_url: https://github.com/dmarx/papers-feed/issues/513
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:25.560109'
  last_visited: '2024-12-29T20:31:06.783Z'
  main_tex_file: null
  published_date: '2024-07-31T17:57:25Z'
  state: open
  title: 'Large Language Monkeys: Scaling Inference Compute with Repeated Sampling'
  total_reading_time_seconds: 17
  url: https://arxiv.org/abs/2407.21787
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is

    a critical step towards building generally self-improving agents that can

    operate on open-ended natural language. In this paper, we study the scaling of

    inference-time computation in LLMs, with a focus on answering the question: if

    an LLM is allowed to use a fixed but non-trivial amount of inference-time

    compute, how much can it improve its performance on a challenging prompt?

    Answering this question has implications not only on the achievable performance

    of LLMs, but also on the future of LLM pretraining and how one should tradeoff

    inference-time and pre-training compute. Despite its importance, little

    research attempted to understand the scaling behaviors of various test-time

    inference methods. Moreover, current work largely provides negative results for

    a number of these strategies. In this work, we analyze two primary mechanisms

    to scale test-time computation: (1) searching against dense, process-based

    verifier reward models; and (2) updating the model''s distribution over a

    response adaptively, given the prompt at test time. We find that in both cases,

    the effectiveness of different approaches to scaling test-time compute

    critically varies depending on the difficulty of the prompt. This observation

    motivates applying a "compute-optimal" scaling strategy, which acts to most

    effectively allocate test-time compute adaptively per prompt. Using this

    compute-optimal strategy, we can improve the efficiency of test-time compute

    scaling by more than 4x compared to a best-of-N baseline. Additionally, in a

    FLOPs-matched evaluation, we find that on problems where a smaller base model

    attains somewhat non-trivial success rates, test-time compute can be used to

    outperform a 14x larger model.'
  arxivId: '2408.03314'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-30T08:27:22.552255'
  issue_number: 515
  issue_url: https://github.com/dmarx/papers-feed/issues/515
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:25.558565'
  last_visited: '2024-12-29T20:31:12.333Z'
  main_tex_file: null
  published_date: '2024-08-06T17:35:05Z'
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_seconds: 39
  url: https://arxiv.org/abs/2408.03314
'2408.04809':
  abstract: 'In this paper, we overview one promising avenue of progress at the

    mathematical foundation of deep learning: the connection between deep networks

    and function approximation by affine splines (continuous piecewise linear

    functions in multiple dimensions). In particular, we will overview work over

    the past decade on understanding certain geometrical properties of a deep

    network''s affine spline mapping, in particular how it tessellates its input

    space. As we will see, the affine spline connection and geometrical viewpoint

    provide a powerful portal through which to view, analyze, and improve the inner

    workings of a deep network.'
  arxivId: '2408.04809'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Randall Balestriero, Ahmed Imtiaz Humayun, Richard Baraniuk
  created_at: '2024-12-30T08:27:31.802050'
  issue_number: 500
  issue_url: https://github.com/dmarx/papers-feed/issues/500
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:31.804366'
  last_visited: '2024-12-29T11:11:50.707Z'
  main_tex_file: null
  published_date: '2024-08-09T01:40:12Z'
  state: open
  title: On the Geometry of Deep Learning
  total_reading_time_seconds: 29
  url: https://arxiv.org/abs/2408.04809
'2408.14837':
  abstract: 'We present GameNGen, the first game engine powered entirely by a neural
    model

    that enables real-time interaction with a complex environment over long

    trajectories at high quality. GameNGen can interactively simulate the classic

    game DOOM at over 20 frames per second on a single TPU. Next frame prediction

    achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are

    only slightly better than random chance at distinguishing short clips of the

    game from clips of the simulation. GameNGen is trained in two phases: (1) an

    RL-agent learns to play the game and the training sessions are recorded, and

    (2) a diffusion model is trained to produce the next frame, conditioned on the

    sequence of past frames and actions. Conditioning augmentations enable stable

    auto-regressive generation over long trajectories.'
  arxivId: '2408.14837'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter
  created_at: '2024-12-30T14:44:12.239069'
  issue_number: 322
  issue_url: https://github.com/dmarx/papers-feed/issues/322
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-27T07:47:16.058Z'
  main_tex_file: null
  published_date: '2024-08-27T07:46:07Z'
  state: open
  title: Diffusion Models Are Real-Time Game Engines
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2408.14837
'2409.08514':
  abstract: 'Audio restoration has become increasingly significant in modern society,
    not

    only due to the demand for high-quality auditory experiences enabled by

    advanced playback devices, but also because the growing capabilities of

    generative audio models necessitate high-fidelity audio. Typically, audio

    restoration is defined as a task of predicting undistorted audio from damaged

    input, often trained using a GAN framework to balance perception and

    distortion. Since audio degradation is primarily concentrated in mid- and

    high-frequency ranges, especially due to codecs, a key challenge lies in

    designing a generator capable of preserving low-frequency information while

    accurately reconstructing high-quality mid- and high-frequency content.

    Inspired by recent advancements in high-sample-rate music separation, speech

    enhancement, and audio codec models, we propose Apollo, a generative model

    designed for high-sample-rate audio restoration. Apollo employs an explicit

    frequency band split module to model the relationships between different

    frequency bands, allowing for more coherent and higher-quality restored audio.

    Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently

    outperforms existing SR-GAN models across various bit rates and music genres,

    particularly excelling in complex scenarios involving mixtures of multiple

    instruments and vocals. Apollo significantly improves music restoration quality

    while maintaining computational efficiency. The source code for Apollo is

    publicly available at https://github.com/JusperLee/Apollo.'
  arxivId: '2409.08514'
  arxiv_tags:
  - cs.SD
  - cs.AI
  - eess.AS
  authors: Kai Li, Yi Luo
  created_at: '2024-12-30T08:26:43.562979'
  issue_number: 576
  issue_url: https://github.com/dmarx/papers-feed/issues/576
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:43.563812'
  last_visited: '2024-12-30T05:02:38.295Z'
  main_tex_file: null
  published_date: '2024-09-13T03:25:34Z'
  state: open
  title: 'Apollo: Band-sequence Modeling for High-Quality Audio Restoration'
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2409.08514
'2410.01131':
  abstract: 'We propose a novel neural network architecture, the normalized Transformer

    (nGPT) with representation learning on the hypersphere. In nGPT, all vectors

    forming the embeddings, MLP, attention matrices and hidden states are unit norm

    normalized. The input stream of tokens travels on the surface of a hypersphere,

    with each layer contributing a displacement towards the target output

    predictions. These displacements are defined by the MLP and attention blocks,

    whose vector components also reside on the same hypersphere. Experiments show

    that nGPT learns much faster, reducing the number of training steps required to

    achieve the same accuracy by a factor of 4 to 20, depending on the sequence

    length.'
  arxivId: '2410.01131'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg
  created_at: '2024-12-30T14:44:27.214362'
  issue_number: 270
  issue_url: https://github.com/dmarx/papers-feed/issues/270
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:27.216224'
  last_visited: '2024-12-25T20:54:14.596Z'
  main_tex_file: null
  published_date: '2024-10-01T23:50:09Z'
  state: open
  title: "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere"
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2410.01131
'2411.05899':
  abstract: 'Bayes'' rule naturally allows for inference refinement in a streaming
    fashion,

    without the need to recompute posteriors from scratch whenever new data

    arrives. In principle, Bayesian streaming is straightforward: we update our

    prior with the available data and use the resulting posterior as a prior when

    processing the next data chunk. In practice, however, this recipe entails i)

    approximating an intractable posterior at each time step; and ii) encapsulating

    results appropriately to allow for posterior propagation. For continuous state

    spaces, variational inference (VI) is particularly convenient due to its

    scalability and the tractability of variational posteriors. For discrete state

    spaces, however, state-of-the-art VI results in analytically intractable

    approximations that are ill-suited for streaming settings. To enable streaming

    Bayesian inference over discrete parameter spaces, we propose streaming Bayes

    GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed

    GFlowNets -- a powerful class of amortized samplers for discrete compositional

    objects. Notably, SB-GFlowNet approximates the initial posterior using a

    standard GFlowNet and subsequently updates it using a tailored procedure that

    requires only the newly observed data. Our case studies in linear preference

    learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets

    in sampling from an unnormalized posterior in a streaming setting. As expected,

    we also observe that SB-GFlowNets is significantly faster than repeatedly

    training a GFlowNet from scratch to sample from the full posterior.'
  arxivId: '2411.05899'
  arxiv_tags:
  - cs.LG
  authors: Tiago da Silva, Daniel Augusto de Souza, Diego Mesquita
  created_at: '2024-12-30T14:44:00.195040'
  issue_number: 339
  issue_url: https://github.com/dmarx/papers-feed/issues/339
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:00.196230'
  last_visited: '2024-12-28T06:09:10.852Z'
  main_tex_file: null
  published_date: '2024-11-08T15:53:56Z'
  state: open
  title: Streaming Bayes GFlowNets
  total_reading_time_seconds: 9
  url: https://arxiv.org/abs/2411.05899
'2412.01023':
  abstract: 'Most real-world datasets consist of a natural hierarchy between classes
    or an

    inherent label structure that is either already available or can be constructed

    cheaply. However, most existing representation learning methods ignore this

    hierarchy, treating labels as permutation invariant. Recent work [Zeng et al.,

    2022] proposes using this structured information explicitly, but the use of

    Euclidean distance may distort the underlying semantic context [Chen et al.,

    2013]. In this work, motivated by the advantage of hyperbolic spaces in

    modeling hierarchical relationships, we propose a novel approach HypStructure:

    a Hyperbolic Structured regularization approach to accurately embed the label

    hierarchy into the learned representations. HypStructure is a

    simple-yet-effective regularizer that consists of a hyperbolic tree-based

    representation loss along with a centering loss, and can be combined with any

    standard task loss to learn hierarchy-informed features. Extensive experiments

    on several large-scale vision benchmarks demonstrate the efficacy of

    HypStructure in reducing distortion and boosting generalization performance

    especially under low dimensional scenarios. For a better understanding of

    structured representation, we perform eigenvalue analysis that links the

    representation geometry to improved Out-of-Distribution (OOD) detection

    performance seen empirically. The code is available at

    \url{https://github.com/uiuctml/HypStructure}.'
  arxivId: '2412.01023'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao
  created_at: '2024-12-30T08:28:04.539189'
  issue_number: 440
  issue_url: https://github.com/dmarx/papers-feed/issues/440
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:10.548268'
  last_visited: '2024-12-29T01:41:03.660Z'
  main_tex_file: null
  published_date: '2024-12-02T00:56:44Z'
  state: open
  title: Learning Structured Representations with Hyperbolic Embeddings
  total_reading_time_seconds: 81
  url: https://arxiv.org/abs/2412.01023
'2412.11766':
  abstract: 'Modeling human behavior is essential to accurately predict epidemic spread,

    with behaviors like vaccine hesitancy complicating control efforts. While

    epidemic spread is often treated as a simple contagion, vaccine uptake may

    follow complex contagion dynamics, where individuals'' decisions depend on

    multiple social contacts. Recently, the concept of complex contagion has

    received strong theoretical underpinnings thanks to the generalization of

    spreading phenomena from pairwise to higher-order interactions. Although

    several potential applications have been suggested, examples of complex

    contagions motivated by real data remain scarce. Surveys on COVID-19 vaccine

    hesitancy in the US suggest that vaccination attitudes may indeed depend on the

    vaccination status of social peers, aligning with complex contagion principles.

    In this work, we examine the interactions between epidemic spread, vaccination,

    and vaccine uptake attitudes under complex contagion. Using the SIR model with

    a dynamic, threshold-based vaccination campaign, we simulate scenarios on an

    age-structured multilayer network informed by US contact data. Our results

    offer insights into the role of social dynamics in shaping vaccination behavior

    and epidemic outcomes.'
  arxivId: '2412.11766'
  arxiv_tags:
  - physics.soc-ph
  - cs.SI
  authors: Alfonso de Miguel-Arribas, Alberto Aleta, Yamir Moreno
  created_at: '2024-12-30T14:44:06.210489'
  issue_number: 332
  issue_url: https://github.com/dmarx/papers-feed/issues/332
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:06.211749'
  last_visited: '2024-12-28T05:44:44.089Z'
  main_tex_file: null
  published_date: '2024-12-16T13:37:27Z'
  state: open
  title: "Interplay of epidemic spreading and vaccine uptake under complex social\n\
    \  contagion"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.11766
'2412.11768':
  abstract: 'In this work, we question the necessity of adaptive gradient methods
    for

    training deep neural networks. SGD-SaI is a simple yet effective enhancement to

    stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning

    rate Scaling at Initialization (SaI) to distinct parameter groups, guided by

    their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning

    rates without relying on adaptive second-order momentum, SGD-SaI helps prevent

    training imbalances from the very first iteration and cuts the optimizer''s

    memory usage by half compared to AdamW. Despite its simplicity and efficiency,

    SGD-SaI consistently matches or outperforms AdamW in training a variety of

    Transformer-based tasks, effectively overcoming a long-standing challenge of

    using SGD for training Transformers. SGD-SaI excels in ImageNet-1K

    classification with Vision Transformers(ViT) and GPT-2 pretraining for large

    language models (LLMs, transformer decoder-only), demonstrating robustness to

    hyperparameter variations and practicality for diverse applications. We further

    tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion

    models, where it consistently outperforms state-of-the-art optimizers. From a

    memory efficiency perspective, SGD-SaI achieves substantial memory savings for

    optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)

    and 25.15 GB for Llama2-7B compared to AdamW in full-precision training

    settings.'
  arxivId: '2412.11768'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen
  created_at: '2024-12-30T08:27:16.564411'
  issue_number: 433
  issue_url: https://github.com/dmarx/papers-feed/issues/433
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:19.539482'
  last_visited: '2024-12-28T18:49:01.011000+00:00'
  main_tex_file: null
  published_date: '2024-12-16T13:41:37Z'
  state: open
  title: 'No More Adam: Learning Rate Scaling at Initialization is All You Need'
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2412.11768
'2412.17558':
  abstract: '\textit{Query Optimization} (QO) refers to techniques aimed at enhancing
    the

    efficiency and quality of Large Language Models (LLMs) in understanding and

    answering queries, especially complex ones in scenarios like

    Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the

    limitations of LLMs by dynamically retrieving and leveraging up-to-date

    relevant information, which provides a cost-effective solution to the challenge

    of LLMs producing plausible but potentially inaccurate responses. Recently, as

    RAG evolves and incorporates multiple components that influence its

    performance, QO has emerged as a critical element, playing a pivotal role in

    determining the effectiveness of RAG''s retrieval stage in accurately sourcing

    the necessary multiple pieces of evidence to answer queries correctly. In this

    paper, we trace the evolution of QO techniques by summarizing and analyzing

    significant studies. Through an organized framework and categorization, we aim

    to consolidate existing QO techniques in RAG, elucidate their technological

    foundations, and highlight their potential to enhance the versatility and

    applications of LLMs.'
  arxivId: '2412.17558'
  arxiv_tags:
  - cs.CL
  authors: Mingyang Song, Mao Zheng
  created_at: '2024-12-30T08:27:10.552311'
  issue_number: 554
  issue_url: https://github.com/dmarx/papers-feed/issues/554
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:13.538442'
  last_visited: '2024-12-30T04:45:35.389Z'
  main_tex_file: null
  published_date: '2024-12-23T13:26:04Z'
  state: open
  title: A Survey of Query Optimization in Large Language Models
  total_reading_time_seconds: 46
  url: https://arxiv.org/abs/2412.17558
'2412.17758':
  abstract: 'ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily

    due to an evaluation setup that prevents direct comparison of answer choices

    rather than inherent complexity. Although some researchers have quietly shifted

    to a more appropriate scheme over the last year, the implications of this

    change have yet to be widely acknowledged. We highlight this overlooked shift,

    show how similar evaluation practices falsely imply reasoning deficits in other

    benchmarks, and demonstrate that fairer methods dramatically reduce performance

    gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing

    so, we reveal how evaluation shapes perceived difficulty and offer guidelines

    to ensure that multiple-choice evaluations accurately reflect actual model

    capabilities.'
  arxivId: '2412.17758'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Łukasz Borchmann
  created_at: '2024-12-30T08:27:52.551001'
  issue_number: 458
  issue_url: https://github.com/dmarx/papers-feed/issues/458
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:52.551795'
  last_visited: '2024-12-29T08:28:16.461Z'
  main_tex_file: null
  published_date: '2024-12-23T18:14:36Z'
  state: open
  title: 'In Case You Missed It: ARC ''Challenge'' Is Not That Challenging'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.17758
'2412.17759':
  abstract: 'Multimodal learning, a rapidly evolving field in artificial intelligence,

    seeks to construct more versatile and robust systems by integrating and

    analyzing diverse types of data, including text, images, audio, and video.

    Inspired by the human ability to assimilate information through many senses,

    this method enables applications such as text-to-video conversion, visual

    question answering, and image captioning. Recent developments in datasets that

    support multimodal language models (MLLMs) are highlighted in this overview.

    Large-scale multimodal datasets are essential because they allow for thorough

    testing and training of these models. With an emphasis on their contributions

    to the discipline, the study examines a variety of datasets, including those

    for training, domain-specific tasks, and real-world applications. It also

    emphasizes how crucial benchmark datasets are for assessing models'' performance

    in a range of scenarios, scalability, and applicability. Since multimodal

    learning is always changing, overcoming these obstacles will help AI research

    and applications reach new heights.'
  arxivId: '2412.17759'
  arxiv_tags:
  - cs.AI
  - cs.CV
  - cs.LG
  authors: Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal,
    Ishan Banerjee, Srikant Panda, Tejaswini Kumar
  created_at: '2024-12-30T08:27:13.536066'
  issue_number: 556
  issue_url: https://github.com/dmarx/papers-feed/issues/556
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:13.536863'
  last_visited: '2024-12-30T04:45:50.115Z'
  main_tex_file: null
  published_date: '2024-12-23T18:15:19Z'
  state: open
  title: "Survey of Large Multimodal Model Datasets, Application Categories and\n\
    \  Taxonomy"
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2412.17759
'2412.17799':
  abstract: 'With the recent Nobel Prize awarded for radical advances in protein

    discovery, foundation models (FMs) for exploring large combinatorial spaces

    promise to revolutionize many scientific fields. Artificial Life (ALife) has

    not yet integrated FMs, thus presenting a major opportunity for the field to

    alleviate the historical burden of relying chiefly on manual design and

    trial-and-error to discover the configurations of lifelike simulations. This

    paper presents, for the first time, a successful realization of this

    opportunity using vision-language FMs. The proposed approach, called Automated

    Search for Artificial Life (ASAL), (1) finds simulations that produce target

    phenomena, (2) discovers simulations that generate temporally open-ended

    novelty, and (3) illuminates an entire space of interestingly diverse

    simulations. Because of the generality of FMs, ASAL works effectively across a

    diverse range of ALife substrates including Boids, Particle Life, Game of Life,

    Lenia, and Neural Cellular Automata. A major result highlighting the potential

    of this technique is the discovery of previously unseen Lenia and Boids

    lifeforms, as well as cellular automata that are open-ended like Conway''s Game

    of Life. Additionally, the use of FMs allows for the quantification of

    previously qualitative phenomena in a human-aligned way. This new paradigm

    promises to accelerate ALife research beyond what is possible through human

    ingenuity alone.'
  arxivId: '2412.17799'
  arxiv_tags:
  - cs.AI
  - cs.NE
  authors: Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip
    Isola, David Ha
  created_at: '2024-12-30T14:44:39.235524'
  issue_number: 249
  issue_url: https://github.com/dmarx/papers-feed/issues/249
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T14:44:39.236746'
  last_visited: '2024-12-24T04:42:05.951Z'
  main_tex_file: null
  published_date: '2024-12-23T18:57:00Z'
  state: open
  title: Automating the Search for Artificial Life with Foundation Models
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2412.17799
'2412.18082':
  abstract: 'The item cold-start problem is crucial for online recommender systems,
    as the

    success of the cold-start phase determines whether items can transition into

    popular ones. Prompt learning, a powerful technique used in natural language

    processing (NLP) to address zero- or few-shot problems, has been adapted for

    recommender systems to tackle similar challenges. However, existing methods

    typically rely on content-based properties or text descriptions for prompting,

    which we argue may be suboptimal for cold-start recommendations due to 1)

    semantic gaps with recommender tasks, 2) model bias caused by warm-up items

    contribute most of the positive feedback to the model, which is the core of the

    cold-start problem that hinders the recommender quality on cold-start items. We

    propose to leverage high-value positive feedback, termed pinnacle feedback as

    prompt information, to simultaneously resolve the above two problems. We

    experimentally prove that compared to the content description proposed in

    existing works, the positive feedback is more suitable to serve as prompt

    information by bridging the semantic gaps. Besides, we propose item-wise

    personalized prompt networks to encode pinnaclce feedback to relieve the model

    bias by the positive feedback dominance problem. Extensive experiments on four

    real-world datasets demonstrate the superiority of our model over

    state-of-the-art methods. Moreover, PROMO has been successfully deployed on a

    popular short-video sharing platform, a billion-user scale commercial

    short-video application, achieving remarkable performance gains across various

    commercial metrics within cold-start scenarios'
  arxivId: '2412.18082'
  arxiv_tags:
  - cs.IR
  - cs.AI
  authors: Yuezihan Jiang, Gaode Chen, Wenhan Zhang, Jingchi Wang, Yinjie Jiang, Qi
    Zhang, Jingjian Lin, Peng Jiang, Kaigui Bian
  created_at: '2024-12-30T08:26:55.551355'
  issue_number: 570
  issue_url: https://github.com/dmarx/papers-feed/issues/570
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:55.552642'
  last_visited: '2024-12-30T04:50:12.821000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T01:38:19Z'
  state: open
  title: Prompt Tuning for Item Cold-start Recommendation
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2412.18082
'2412.18168':
  abstract: 'Intuitively, an ideal collaborative filtering (CF) model should learn
    from

    users'' full rankings over all items to make optimal top-K recommendations. Due

    to the absence of such full rankings in practice, most CF models rely on

    pairwise loss functions to approximate full rankings, resulting in an immense

    performance gap. In this paper, we provide a novel analysis using the multiple

    ordinal classification concept to reveal the inevitable gap between a pairwise

    approximation and the ideal case. However, bridging the gap in practice

    encounters two formidable challenges: (1) none of the real-world datasets

    contains full ranking information; (2) there does not exist a loss function

    that is capable of consuming ranking information. To overcome these challenges,

    we propose a pseudo-ranking paradigm (PRP) that addresses the lack of ranking

    information by introducing pseudo-rankings supervised by an original noise

    injection mechanism. Additionally, we put forward a new ranking loss function

    designed to handle ranking information effectively. To ensure our method''s

    robustness against potential inaccuracies in pseudo-rankings, we equip the

    ranking loss function with a gradient-based confidence mechanism to detect and

    mitigate abnormal gradients. Extensive experiments on four real-world datasets

    demonstrate that PRP significantly outperforms state-of-the-art methods.'
  arxivId: '2412.18168'
  arxiv_tags:
  - cs.IR
  authors: Yuhan Zhao, Rui Chen, Li Chen, Shuang Zhang, Qilong Han, Hongtao Song
  created_at: '2024-12-30T08:26:58.544281'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-30T08:27:02.068052'
  last_visited: '2024-12-30T04:49:25.255000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T05:01:16Z'
  state: open
  title: "From Pairwise to Ranking: Climbing the Ladder to Ideal Collaborative\n \
    \ Filtering with Pseudo-Ranking"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.18168
'2412.18431':
  abstract: 'Retrieval-augmented generation systems rely on effective document retrieval

    capabilities. By design, conventional sparse or dense retrievers face

    challenges in multi-hop retrieval scenarios. In this paper, we present GeAR,

    which advances RAG performance through two key innovations: (i) graph

    expansion, which enhances any conventional base retriever, such as BM25, and

    (ii) an agent framework that incorporates graph expansion. Our evaluation

    demonstrates GeAR''s superior retrieval performance on three multi-hop question

    answering datasets. Additionally, our system achieves state-of-the-art results

    with improvements exceeding 10% on the challenging MuSiQue dataset, while

    requiring fewer tokens and iterations compared to other multi-step retrieval

    systems.'
  arxivId: '2412.18431'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.IR
  authors: Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam,
    Damien Graux, Dandan Tu, Zeren Jiang, Ruofei Lai, Yang Ren, Jeff Z. Pan
  created_at: '2024-12-30T08:27:04.570417'
  issue_number: 564
  issue_url: https://github.com/dmarx/papers-feed/issues/564
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:04.571191'
  last_visited: '2024-12-30T04:48:52.907Z'
  main_tex_file: null
  published_date: '2024-12-24T13:45:22Z'
  state: open
  title: 'GeAR: Graph-enhanced Agent for Retrieval-augmented Generation'
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2412.18431
'2412.18537':
  abstract: 'Large Language Models (LLMs) demonstrate remarkable capabilities, yet

    struggle with hallucination and outdated knowledge when tasked with complex

    knowledge reasoning, resulting in factually incorrect outputs. Previous studies

    have attempted to mitigate it by retrieving factual knowledge from large-scale

    knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of

    answers. However, this kind of approach often introduces noise and irrelevant

    data, especially in situations with extensive context from multiple knowledge

    aspects. In this way, LLM attention can be potentially mislead from question

    and relevant information. In our study, we introduce an Adaptive Multi-Aspect

    Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge

    including entities, relations, and subgraphs, and converts each piece of

    retrieved text into prompt embeddings. The Amar framework comprises two key

    sub-components: 1) a self-alignment module that aligns commonalities among

    entities, relations, and subgraphs to enhance retrieved text, thereby reducing

    noise interference; 2) a relevance gating module that employs a soft gate to

    learn the relevance score between question and multi-aspect retrieved data, to

    determine which information should be used to enhance LLMs'' output, or even

    filtered altogether. Our method has achieved state-of-the-art performance on

    two common datasets, WebQSP and CWQ, showing a 1.9\% improvement in accuracy

    over its best competitor and a 6.6\% improvement in logical form generation

    over a method that directly uses retrieved text as context prompts. These

    results demonstrate the effectiveness of Amar in improving the reasoning of

    LLMs.'
  arxivId: '2412.18537'
  arxiv_tags:
  - cs.CL
  authors: Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng,
    Xian Wu, Xiangyu Zhao, Tong Xu, Enhong Chen
  created_at: '2024-12-30T08:27:07.547992'
  issue_number: 563
  issue_url: https://github.com/dmarx/papers-feed/issues/563
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:07.549197'
  last_visited: '2024-12-30T04:47:23.772000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T16:38:04Z'
  state: open
  title: "Harnessing Large Language Models for Knowledge Graph Question Answering\n\
    \  via Adaptive Multi-Aspect Retrieval-Augmentation"
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2412.18537
'2412.18860':
  abstract: 'We introduce a bootstrapping approach to train long-context language
    models

    by exploiting their short-context capabilities only. Our method utilizes a

    simple agent workflow to synthesize diverse long-context instruction tuning

    data, thereby eliminating the necessity for manual data collection and

    annotation. The proposed data synthesis workflow requires only a short-context

    language model, a text retriever, and a document collection, all of which are

    readily accessible within the open-source ecosystem. Subsequently, language

    models are fine-tuned using the synthesized data to extend their context

    lengths. In this manner, we effectively transfer the short-context capabilities

    of language models to long-context scenarios through a bootstrapping process.

    We conduct experiments with the open-source Llama-3 family of models and

    demonstrate that our method can successfully extend the context length to up to

    1M tokens, achieving superior performance across various benchmarks.'
  arxivId: '2412.18860'
  arxiv_tags:
  - cs.CL
  - cs.IR
  authors: Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei
  created_at: '2024-12-30T08:26:49.555274'
  issue_number: 574
  issue_url: https://github.com/dmarx/papers-feed/issues/574
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:49.556588'
  last_visited: '2024-12-30T04:54:02.225000+00:00'
  main_tex_file: null
  published_date: '2024-12-25T10:08:54Z'
  state: open
  title: Bootstrap Your Own Context Length
  total_reading_time_seconds: 29
  url: https://arxiv.org/abs/2412.18860
'2412.18956':
  abstract: 'When you have a question, the most effective way to have the question

    answered is to directly connect with experts on the topic and have a

    conversation with them. Prior to the invention of writing, this was the only

    way. Although effective, this solution exhibits scalability challenges. Writing

    allowed knowledge to be materialized, preserved, and replicated, enabling the

    development of different technologies over the centuries to connect information

    seekers with relevant information. This progression ultimately culminated in

    the ten-blue-links web search paradigm we''re familiar with, just before the

    recent emergence of generative AI. However, we often forget that consuming

    static content is an imperfect solution. With the advent of large language

    models, it has become possible to develop a superior experience by allowing

    users to directly engage with experts. These interactions can of course satisfy

    information needs, but expert models can do so much more. This coming future

    requires reimagining search.'
  arxivId: '2412.18956'
  arxiv_tags:
  - cs.IR
  authors: Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne
  created_at: '2024-12-30T08:26:52.831644'
  issue_number: 572
  issue_url: https://github.com/dmarx/papers-feed/issues/572
  labels:
  - paper
  - rating:downvote
  last_read: '2024-12-30T08:26:52.832952'
  last_visited: '2024-12-30T04:52:50.157000+00:00'
  main_tex_file: null
  published_date: '2024-12-25T18:09:34Z'
  state: open
  title: 'Musings About the Future of Search: A Return to the Past?'
  total_reading_time_seconds: 25
  url: https://arxiv.org/abs/2412.18956
'2412.19442':
  abstract: 'Large Language Models (LLMs) have revolutionized a wide range of domains
    such

    as natural language processing, computer vision, and multi-modal tasks due to

    their ability to comprehend context and perform logical reasoning. However, the

    computational and memory demands of LLMs, particularly during inference, pose

    significant challenges when scaling them to real-world, long-context, and

    real-time applications. Key-Value (KV) cache management has emerged as a

    critical optimization technique for accelerating LLM inference by reducing

    redundant computations and improving memory utilization. This survey provides
    a

    comprehensive overview of KV cache management strategies for LLM acceleration,

    categorizing them into token-level, model-level, and system-level

    optimizations. Token-level strategies include KV cache selection, budget

    allocation, merging, quantization, and low-rank decomposition, while

    model-level optimizations focus on architectural innovations and attention

    mechanisms to enhance KV reuse. System-level approaches address memory

    management, scheduling, and hardware-aware designs to improve efficiency across

    diverse computing environments. Additionally, the survey provides an overview

    of both text and multimodal datasets and benchmarks used to evaluate these

    strategies. By presenting detailed taxonomies and comparative analyses, this

    work aims to offer useful insights for researchers and practitioners to support

    the development of efficient and scalable KV cache management techniques,

    contributing to the practical deployment of LLMs in real-world applications.

    The curated paper list for KV cache management is in:

    \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.'
  arxivId: '2412.19442'
  arxiv_tags:
  - cs.AI
  - cs.DC
  authors: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen,
    Nicole Hu, Wei Dong, Qing Li, Lei Chen
  created_at: '2024-12-30T08:27:02.066552'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-30T08:27:02.067361'
  last_visited: '2024-12-30T04:49:33.842000+00:00'
  main_tex_file: null
  published_date: '2024-12-27T04:17:57Z'
  state: open
  title: "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2412.19442
