'1503.02531':
  abstract: 'A very simple way to improve the performance of almost any machine learning

    algorithm is to train many different models on the same data and then to

    average their predictions. Unfortunately, making predictions using a whole

    ensemble of models is cumbersome and may be too computationally expensive to

    allow deployment to a large number of users, especially if the individual

    models are large neural nets. Caruana and his collaborators have shown that it

    is possible to compress the knowledge in an ensemble into a single model which

    is much easier to deploy and we develop this approach further using a different

    compression technique. We achieve some surprising results on MNIST and we show

    that we can significantly improve the acoustic model of a heavily used

    commercial system by distilling the knowledge in an ensemble of models into a

    single model. We also introduce a new type of ensemble composed of one or more

    full models and many specialist models which learn to distinguish fine-grained

    classes that the full models confuse. Unlike a mixture of experts, these

    specialist models can be trained rapidly and in parallel.'
  arxivId: '1503.02531'
  authors: Geoffrey Hinton, Oriol Vinyals, Jeff Dean
  created_at: '2024-12-22T21:38:50.326157'
  issue_number: 93
  issue_url: https://github.com/dmarx/arxiv-archive/issues/93
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Distilling the Knowledge in a Neural Network
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/1503.02531v1
'1503.03585':
  abstract: 'A central problem in machine learning involves modeling complex data-sets

    using highly flexible families of probability distributions in which learning,

    sampling, inference, and evaluation are still analytically or computationally

    tractable. Here, we develop an approach that simultaneously achieves both

    flexibility and tractability. The essential idea, inspired by non-equilibrium

    statistical physics, is to systematically and slowly destroy structure in a

    data distribution through an iterative forward diffusion process. We then learn

    a reverse diffusion process that restores structure in data, yielding a highly

    flexible and tractable generative model of the data. This approach allows us to

    rapidly learn, sample from, and evaluate probabilities in deep generative

    models with thousands of layers or time steps, as well as to compute

    conditional and posterior probabilities under the learned model. We

    additionally release an open source reference implementation of the algorithm.'
  arxivId: '1503.03585'
  authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
  created_at: '2024-12-23T04:50:15.043823'
  issue_number: 184
  issue_url: https://github.com/dmarx/arxiv-archive/issues/184
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-23T04:56:12.508368'
  state: open
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  total_reading_time_seconds: 28
  url: http://arxiv.org/abs/1503.03585v8
'1710.09412':
  abstract: 'Large deep neural networks are powerful, but exhibit undesirable behaviors

    such as memorization and sensitivity to adversarial examples. In this work, we

    propose mixup, a simple learning principle to alleviate these issues. In

    essence, mixup trains a neural network on convex combinations of pairs of

    examples and their labels. By doing so, mixup regularizes the neural network to

    favor simple linear behavior in-between training examples. Our experiments on

    the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show

    that mixup improves the generalization of state-of-the-art neural network

    architectures. We also find that mixup reduces the memorization of corrupt

    labels, increases the robustness to adversarial examples, and stabilizes the

    training of generative adversarial networks.'
  arxivId: '1710.09412'
  authors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz
  created_at: '2024-12-22T21:37:50.359413'
  issue_number: 162
  issue_url: https://github.com/dmarx/arxiv-archive/issues/162
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'mixup: Beyond Empirical Risk Minimization'
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/1710.09412v2
'1711.06077':
  abstract: 'Image restoration algorithms are typically evaluated by some distortion

    measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify

    perceived perceptual quality. In this paper, we prove mathematically that

    distortion and perceptual quality are at odds with each other. Specifically, we

    study the optimal probability for correctly discriminating the outputs of an

    image restoration algorithm from real images. We show that as the mean

    distortion decreases, this probability must increase (indicating worse

    perceptual quality). As opposed to the common belief, this result holds true

    for any distortion measure, and is not only a problem of the PSNR or SSIM

    criteria. We also show that generative-adversarial-nets (GANs) provide a

    principled way to approach the perception-distortion bound. This constitutes

    theoretical support to their observed success in low-level vision tasks. Based

    on our analysis, we propose a new methodology for evaluating image restoration

    methods, and use it to perform an extensive comparison between recent

    super-resolution algorithms.'
  arxivId: '1711.06077'
  authors: Yochai Blau, Tomer Michaeli
  created_at: '2024-12-22T21:38:26.339770'
  issue_number: 142
  issue_url: https://github.com/dmarx/arxiv-archive/issues/142
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: The Perception-Distortion Tradeoff
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/1711.06077v4
'1711.11586':
  abstract: 'Many image-to-image translation problems are ambiguous, as a single input

    image may correspond to multiple possible outputs. In this work, we aim to

    model a \emph{distribution} of possible outputs in a conditional generative

    modeling setting. The ambiguity of the mapping is distilled in a

    low-dimensional latent vector, which can be randomly sampled at test time. A

    generator learns to map the given input, combined with this latent code, to the

    output. We explicitly encourage the connection between output and the latent

    code to be invertible. This helps prevent a many-to-one mapping from the latent

    code to the output during training, also known as the problem of mode collapse,

    and produces more diverse results. We explore several variants of this approach

    by employing different training objectives, network architectures, and methods

    of injecting the latent code. Our proposed method encourages bijective

    consistency between the latent encoding and output modes. We present a

    systematic comparison of our method and other variants on both perceptual

    realism and diversity.'
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-22T21:39:02.346958'
  issue_number: 22
  issue_url: https://github.com/dmarx/arxiv-archive/issues/22
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-22T22:12:06.028899'
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_minutes: 1
  url: http://arxiv.org/abs/1711.11586v4
'1803.00567':
  abstract: 'Optimal transport (OT) theory can be informally described using the words
    of

    the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in

    hand has to move a large pile of sand lying on a construction site. The goal of

    the worker is to erect with all that sand a target pile with a prescribed shape

    (for example, that of a giant sand castle). Naturally, the worker wishes to

    minimize her total effort, quantified for instance as the total distance or

    time spent carrying shovelfuls of sand. Mathematicians interested in OT cast

    that problem as that of comparing two probability distributions, two different

    piles of sand of the same volume. They consider all of the many possible ways

    to morph, transport or reshape the first pile into the second, and associate a

    "global" cost to every such transport, using the "local" consideration of how

    much it costs to move a grain of sand from one place to another. Recent years

    have witnessed the spread of OT in several fields, thanks to the emergence of

    approximate solvers that can scale to sizes and dimensions that are relevant to

    data sciences. Thanks to this newfound scalability, OT is being increasingly

    used to unlock various problems in imaging sciences (such as color or texture

    processing), computer vision and graphics (for shape manipulation) or machine

    learning (for regression, classification and density fitting). This short book

    reviews OT with a bias toward numerical methods and their applications in data

    sciences, and sheds lights on the theoretical properties of OT that make it

    particularly useful for some of these applications.'
  arxivId: '1803.00567'
  authors: Gabriel Peyr√©, Marco Cuturi
  created_at: '2024-12-22T21:38:32.363971'
  issue_number: 139
  issue_url: https://github.com/dmarx/arxiv-archive/issues/139
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Computational Optimal Transport
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/1803.00567v4
'2009.10195':
  abstract: 'Models that perform well on a training domain often fail to generalize
    to

    out-of-domain (OOD) examples. Data augmentation is a common method used to

    prevent overfitting and improve OOD generalization. However, in natural

    language, it is difficult to generate new examples that stay on the underlying

    data manifold. We introduce SSMBA, a data augmentation method for generating

    synthetic training examples by using a pair of corruption and reconstruction

    functions to move randomly on a data manifold. We investigate the use of SSMBA

    in the natural language domain, leveraging the manifold assumption to

    reconstruct corrupted text with masked language models. In experiments on

    robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently

    outperforms existing data augmentation methods and baseline models on both

    in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews,

    1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.'
  arxivId: '2009.10195'
  authors: Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi
  created_at: '2024-12-22T21:38:47.338171'
  issue_number: 97
  issue_url: https://github.com/dmarx/arxiv-archive/issues/97
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving\n\
    \  Out-of-Domain Robustness"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2009.10195v2
'2105.05720':
  abstract: "Recent trend towards increasing large machine learning models require\
    \ both\ntraining and inference tasks to be distributed. Considering the huge cost\
    \ of\ntraining these models, it is imperative to unlock optimizations in computation\n\
    and communication to obtain best performance. However, current logical\nseparation\
    \ between computation and communication kernels in deep learning\nframeworks misses\
    \ the optimization opportunities across such barrier. Breaking\nthis abstraction\
    \ with a holistic consideration can provide many optimizations\nto provide performance\
    \ improvements in distributed workloads. Manually applying\nthese optimizations\
    \ needs modifications in underlying computation and\ncommunication libraries for\
    \ each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present\
    \ CoCoNeT, with a DSL to express a program with both\ncomputation and communication.\
    \ CoCoNeT contains several machine learning aware\ntransformations to optimize\
    \ a program and a compiler to generate high\nperformance kernels. Providing both\
    \ computation and communication as first\nclass constructs allows users to work\
    \ on a high-level abstraction and apply\npowerful optimizations, such as fusion\
    \ or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize\
    \ data-, model-and pipeline-parallel\nworkloads in large language models with\
    \ only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms\
    \ state-of-the-art distributed machine\nlearning implementations."
  arxivId: '2105.05720'
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2024-12-22T21:39:14.347292'
  issue_number: 49
  issue_url: https://github.com/dmarx/arxiv-archive/issues/49
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2105.05720v5
'2209.02740':
  abstract: 'Networks of weakly coupled oscillators had a profound impact on our

    understanding of complex systems. Studies on model reconstruction from data

    have shown prevalent contributions from hypernetworks with triplet and higher

    interactions among oscillators, in spite that such models were originally

    defined as oscillator networks with pairwise interactions. Here, we show that

    hypernetworks can spontaneously emerge even in the presence of pairwise albeit

    nonlinear coupling given certain triplet frequency resonance conditions. The

    results are demonstrated in experiments with electrochemical oscillators and in

    simulations with integrate-and-fire neurons. By developing a comprehensive

    theory, we uncover the mechanism for emergent hypernetworks by identifying

    appearing and forbidden frequency resonant conditions. Furthermore, it is shown

    that microscopic linear (difference) coupling among units results in coupled

    mean fields, which have sufficient nonlinearity to facilitate hypernetworks.

    Our findings shed light on the apparent abundance of hypernetworks and provide

    a constructive way to predict and engineer their emergence.'
  arxivId: '2209.02740'
  authors: Eddie Nijholt, Jorge Luis Ocampo-Espindola, Deniz Eroglu, Istv√°n Z. Kiss,
    Tiago Pereira
  created_at: '2024-12-22T21:38:41.339078'
  issue_number: 103
  issue_url: https://github.com/dmarx/arxiv-archive/issues/103
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Emergent hypernetworks in weakly coupled oscillators
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2209.02740v1
'2210.14891':
  abstract: 'We present a smoothly broken power law functional form (that we refer
    to as a

    Broken Neural Scaling Law (BNSL)) that accurately models & extrapolates the

    scaling behaviors of deep neural networks (i.e. how the evaluation metric of

    interest varies as amount of compute used for training (or inference), number

    of model parameters, training dataset size, model input size, number of

    training steps, or upstream performance varies) for various architectures & for

    each of various tasks within a large & diverse set of upstream & downstream

    tasks, in zero-shot, prompted, & finetuned settings. This set includes

    large-scale vision, language, audio, video, diffusion, generative modeling,

    multimodal learning, contrastive learning, AI alignment, AI capabilities,

    robotics, out-of-distribution (OOD) generalization, continual learning,

    transfer learning, uncertainty estimation / calibration, OOD detection,

    adversarial robustness, distillation, sparsity, retrieval, quantization,

    pruning, fairness, molecules, computer programming/coding, math word problems,

    "emergent phase transitions", arithmetic, supervised learning,

    unsupervised/self-supervised learning, & reinforcement learning (single agent
    &

    multi-agent). When compared to other functional forms for neural scaling, this

    functional form yields extrapolations of scaling behavior that are considerably

    more accurate on this set. Moreover, this functional form accurately models &

    extrapolates scaling behavior that other functional forms are incapable of

    expressing such as the nonmonotonic transitions present in the scaling behavior

    of phenomena such as double descent & the delayed, sharp inflection points

    present in the scaling behavior of tasks such as arithmetic. Lastly, we use

    this functional form to glean insights about the limit of the predictability of

    scaling behavior. Code is available at

    https://github.com/ethancaballero/broken_neural_scaling_laws'
  arxivId: '2210.14891'
  authors: Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger
  created_at: '2024-12-22T21:38:59.355277'
  issue_number: 85
  issue_url: https://github.com/dmarx/arxiv-archive/issues/85
  labels:
  - paper
  - rating:downvote
  last_read: null
  state: open
  title: Broken Neural Scaling Laws
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2210.14891v17
'2212.07677':
  abstract: 'At present, the mechanisms of in-context learning in Transformers are
    not

    well understood and remain mostly an intuition. In this paper, we suggest that

    training Transformers on auto-regressive objectives is closely related to

    gradient-based meta-learning formulations. We start by providing a simple

    weight construction that shows the equivalence of data transformations induced

    by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on
    a

    regression loss. Motivated by that construction, we show empirically that when

    training self-attention-only Transformers on simple regression tasks either the

    models learned by GD and Transformers show great similarity or, remarkably, the

    weights found by optimization match the construction. Thus we show how trained

    Transformers become mesa-optimizers i.e. learn models by gradient descent in

    their forward pass. This allows us, at least in the domain of regression

    problems, to mechanistically understand the inner workings of in-context

    learning in optimized Transformers. Building on this insight, we furthermore

    identify how Transformers surpass the performance of plain gradient descent by

    learning an iterative curvature correction and learn linear models on deep data

    representations to solve non-linear regression tasks. Finally, we discuss

    intriguing parallels to a mechanism identified to be crucial for in-context

    learning termed induction-head (Olsson et al., 2022) and show how it could be

    understood as a specific case of in-context learning by gradient descent

    learning within Transformers. Code to reproduce the experiments can be found at

    https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
    .'
  arxivId: '2212.07677'
  authors: Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo√£o Sacramento,
    Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov
  created_at: '2024-12-22T21:38:53.336463'
  issue_number: 91
  issue_url: https://github.com/dmarx/arxiv-archive/issues/91
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Transformers learn in-context by gradient descent
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2212.07677v2
'2302.04222':
  abstract: 'Recent text-to-image diffusion models such as MidJourney and Stable Diffusion

    threaten to displace many in the professional artist community. In particular,

    models can learn to mimic the artistic style of specific artists after

    "fine-tuning" on samples of their art. In this paper, we describe the design,

    implementation and evaluation of Glaze, a tool that enables artists to apply

    "style cloaks" to their art before sharing online. These cloaks apply barely

    perceptible perturbations to images, and when used as training data, mislead

    generative models that try to mimic a specific artist. In coordination with the

    professional artist community, we deploy user studies to more than 1000

    artists, assessing their views of AI art, as well as the efficacy of our tool,

    its usability and tolerability of perturbations, and robustness across

    different scenarios and against adaptive countermeasures. Both surveyed artists

    and empirical CLIP-based scores show that even at low perturbation levels

    (p=0.05), Glaze is highly successful at disrupting mimicry under normal

    conditions (>92%) and against adaptive countermeasures (>85%).'
  arxivId: '2302.04222'
  authors: Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben
    Y. Zhao
  created_at: '2024-12-22T21:38:20.348773'
  issue_number: 147
  issue_url: https://github.com/dmarx/arxiv-archive/issues/147
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models'
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2302.04222v5
'2303.08500':
  abstract: 'Protecting personal data against exploitation of machine learning models
    is

    crucial. Recently, availability attacks have shown great promise to provide an

    extra layer of protection against the unauthorized use of data to train neural

    networks. These methods aim to add imperceptible noise to clean data so that

    the neural networks cannot extract meaningful patterns from the protected data,

    claiming that they can make personal data "unexploitable." This paper provides

    a strong countermeasure against such approaches, showing that unexploitable

    data might only be an illusion. In particular, we leverage the power of

    diffusion models and show that a carefully designed denoising process can

    counteract the effectiveness of the data-protecting perturbations. We

    rigorously analyze our algorithm, and theoretically prove that the amount of

    required denoising is directly related to the magnitude of the data-protecting

    perturbations. Our approach, called AVATAR, delivers state-of-the-art

    performance against a suite of recent availability attacks in various

    scenarios, outperforming adversarial training even under distribution mismatch

    between the diffusion model and the protected data. Our findings call for more

    research into making personal data unexploitable, showing that this goal is far

    from over. Our implementation is available at this repository:

    https://github.com/hmdolatabadi/AVATAR.'
  arxivId: '2303.08500'
  authors: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
  created_at: '2024-12-22T21:38:05.355489'
  issue_number: 156
  issue_url: https://github.com/dmarx/arxiv-archive/issues/156
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "The Devil's Advocate: Shattering the Illusion of Unexploitable Data\n  using\
    \ Diffusion Models"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2303.08500v2
'2303.11435':
  abstract: 'Inversion by Direct Iteration (InDI) is a new formulation for supervised

    image restoration that avoids the so-called "regression to the mean" effect and

    produces more realistic and detailed images than existing regression-based

    methods. It does this by gradually improving image quality in small steps,

    similar to generative denoising diffusion models. Image restoration is an

    ill-posed problem where multiple high-quality images are plausible

    reconstructions of a given low-quality input. Therefore, the outcome of a

    single step regression model is typically an aggregate of all possible

    explanations, therefore lacking details and realism. The main advantage of InDI

    is that it does not try to predict the clean target image in a single step but

    instead gradually improves the image in small steps, resulting in better

    perceptual quality. While generative denoising diffusion models also work in

    small steps, our formulation is distinct in that it does not require knowledge

    of any analytic form of the degradation process. Instead, we directly learn an

    iterative restoration process from low-quality and high-quality paired

    examples. InDI can be applied to virtually any image degradation, given paired

    training data. In conditional denoising diffusion image restoration the

    denoising network generates the restored image by repeatedly denoising an

    initial image of pure noise, conditioned on the degraded input. Contrary to

    conditional denoising formulations, InDI directly proceeds by iteratively

    restoring the input low-quality image, producing high-quality results on a

    variety of image restoration tasks, including motion and out-of-focus

    deblurring, super-resolution, compression artifact removal, and denoising.'
  arxivId: '2303.11435'
  authors: Mauricio Delbracio, Peyman Milanfar
  created_at: '2024-12-22T21:38:29.501069'
  issue_number: 140
  issue_url: https://github.com/dmarx/arxiv-archive/issues/140
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for\n\
    \  Image Restoration"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2303.11435v5
'2304.02234':
  abstract: 'Recently developed text-to-image diffusion models make it easy to edit
    or

    create high-quality images. Their ease of use has raised concerns about the

    potential for malicious editing or deepfake creation. Imperceptible

    perturbations have been proposed as a means of protecting images from malicious

    editing by preventing diffusion models from generating realistic images.

    However, we find that the aforementioned perturbations are not robust to JPEG

    compression, which poses a major weakness because of the common usage and

    availability of JPEG. We discuss the importance of robustness for additive

    imperceptible perturbations and encourage alternative approaches to protect

    images against editing.'
  arxivId: '2304.02234'
  authors: Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein
  created_at: '2024-12-22T21:38:02.335695'
  issue_number: 157
  issue_url: https://github.com/dmarx/arxiv-archive/issues/157
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: JPEG Compressed Images Can Bypass Protections Against AI Editing
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2304.02234v2
'2304.15004':
  abstract: 'Recent work claims that large language models display emergent abilities,

    abilities not present in smaller-scale models that are present in larger-scale

    models. What makes emergent abilities intriguing is two-fold: their sharpness,

    transitioning seemingly instantaneously from not present to present, and their

    unpredictability, appearing at seemingly unforeseeable model scales. Here, we

    present an alternative explanation for emergent abilities: that for a

    particular task and model family, when analyzing fixed model outputs, emergent

    abilities appear due to the researcher''s choice of metric rather than due to

    fundamental changes in model behavior with scale. Specifically, nonlinear or

    discontinuous metrics produce apparent emergent abilities, whereas linear or

    continuous metrics produce smooth, continuous predictable changes in model

    performance. We present our alternative explanation in a simple mathematical

    model, then test it in three complementary ways: we (1) make, test and confirm

    three predictions on the effect of metric choice using the InstructGPT/GPT-3

    family on tasks with claimed emergent abilities; (2) make, test and confirm two

    predictions about metric choices in a meta-analysis of emergent abilities on

    BIG-Bench; and (3) show to choose metrics to produce never-before-seen

    seemingly emergent abilities in multiple vision tasks across diverse deep

    networks. Via all three analyses, we provide evidence that alleged emergent

    abilities evaporate with different metrics or with better statistics, and may

    not be a fundamental property of scaling AI models.'
  arxivId: '2304.15004'
  authors: Rylan Schaeffer, Brando Miranda, Sanmi Koyejo
  created_at: '2024-12-22T21:38:56.333819'
  issue_number: 89
  issue_url: https://github.com/dmarx/arxiv-archive/issues/89
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Are Emergent Abilities of Large Language Models a Mirage?
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2304.15004v2
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    for

    generating high-dimensional data. In this paper, we show that the dynamics of

    these models exhibit a spontaneous symmetry breaking that divides the

    generative dynamics into two distinct phases: 1) A linear steady-state dynamics

    around a central fixed-point and 2) an attractor dynamics directed towards the

    data manifold. These two "phases" are separated by the change in stability of

    the central fixed-point, with the resulting window of instability being

    responsible for the diversity of the generated samples. Using both theoretical

    and empirical evidence, we show that an accurate simulation of the early

    dynamics does not significantly contribute to the final generation, since early

    fluctuations are reverted to the central fixed point. To leverage this insight,

    we propose a Gaussian late initialization scheme, which significantly improves

    model performance, achieving up to 3x FID improvements on fast samplers, while

    also increasing sample diversity (e.g., racial composition of generated CelebA

    images). Our work offers a new way to understand the generative dynamics of

    diffusion models that has the potential to bring about higher performance and

    less biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-22T21:39:35.348660'
  issue_number: 24
  issue_url: https://github.com/dmarx/arxiv-archive/issues/24
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2305.19693v3
'2308.10718':
  abstract: "Recent years have witnessed success in AIGC (AI Generated Content). People\n\
    can make use of a pre-trained diffusion model to generate images of high\nquality\
    \ or freely modify existing pictures with only prompts in nature\nlanguage. More\
    \ excitingly, the emerging personalization techniques make it\nfeasible to create\
    \ specific-desired images with only a few images as\nreferences. However, this\
    \ induces severe threats if such advanced techniques\nare misused by malicious\
    \ users, such as spreading fake news or defaming\nindividual reputations. Thus,\
    \ it is necessary to regulate personalization\nmodels (i.e., concept censorship)\
    \ for their development and advancement.\n  In this paper, we focus on the personalization\
    \ technique dubbed Textual\nInversion (TI), which is becoming prevailing for its\
    \ lightweight nature and\nexcellent performance. TI crafts the word embedding\
    \ that contains detailed\ninformation about a specific object. Users can easily\
    \ download the word\nembedding from public websites like Civitai and add it to\
    \ their own stable\ndiffusion model without fine-tuning for personalization. To\
    \ achieve the concept\ncensorship of a TI model, we propose leveraging the backdoor\
    \ technique for good\nby injecting backdoors into the Textual Inversion embeddings.\
    \ Briefly, we\nselect some sensitive words as triggers during the training of\
    \ TI, which will\nbe censored for normal use. In the subsequent generation stage,\
    \ if the triggers\nare combined with personalized embeddings as final prompts,\
    \ the model will\noutput a pre-defined target image rather than images including\
    \ the desired\nmalicious concept.\n  To demonstrate the effectiveness of our approach,\
    \ we conduct extensive\nexperiments on Stable Diffusion, a prevailing open-sourced\
    \ text-to-image model.\nOur code, data, and results are available at\nhttps://concept-censorship.github.io."
  arxivId: '2308.10718'
  authors: Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang
  created_at: '2024-12-22T21:37:56.519186'
  issue_number: 160
  issue_url: https://github.com/dmarx/arxiv-archive/issues/160
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Backdooring Textual Inversion for Concept Censorship
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2308.10718v2
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering

    times by orders of magnitude. While these methods demonstrate state-of-the-art

    quality and speed, they are designed for photogrammetry of static scenes and do

    not generalize well to freely moving humans in the environment. In this work,

    we introduce Human Gaussian Splats (HUGS) that represents an animatable human

    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes

    only a monocular video with a small number of (50-100) frames, and it

    automatically learns to disentangle the static scene and a fully animatable

    human avatar within 30 minutes. We utilize the SMPL body model to initialize

    the human Gaussians. To capture details that are not modeled by SMPL (e.g.

    cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.

    Utilizing 3D Gaussians for animated humans brings new challenges, including the

    artifacts created when articulating the Gaussians. We propose to jointly

    optimize the linear blend skinning weights to coordinate the movements of

    individual Gaussians during animation. Our approach enables novel-pose

    synthesis of human and novel view synthesis of both the human and the scene. We

    achieve state-of-the-art rendering quality with a rendering speed of 60 FPS

    while being ~100x faster to train over previous work. Our code will be

    announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-22T21:39:32.335011'
  issue_number: 20
  issue_url: https://github.com/dmarx/arxiv-archive/issues/20
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'HUGS: Human Gaussian Splats'
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2311.17910v1
'2312.00330':
  abstract: 'Text-to-video (T2V) models have shown remarkable capabilities in generating

    diverse videos. However, they struggle to produce user-desired stylized videos

    due to (i) text''s inherent clumsiness in expressing specific styles and (ii)

    the generally degraded style fidelity. To address these challenges, we

    introduce StyleCrafter, a generic method that enhances pre-trained T2V models

    with a style control adapter, enabling video generation in any style by

    providing a reference image. Considering the scarcity of stylized video

    datasets, we propose to first train a style control adapter using style-rich

    image datasets, then transfer the learned stylization ability to video

    generation through a tailor-made finetuning paradigm. To promote content-style

    disentanglement, we remove style descriptions from the text prompt and extract

    style information solely from the reference image using a decoupling learning

    strategy. Additionally, we design a scale-adaptive fusion module to balance the

    influences of text-based content features and image-based style features, which

    helps generalization across various text and style combinations. StyleCrafter

    efficiently generates high-quality stylized videos that align with the content

    of the texts and resemble the style of the reference images. Experiments

    demonstrate that our approach is more flexible and efficient than existing

    competitors.'
  arxivId: '2312.00330'
  authors: Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang,
    Xintao Wang, Yujiu Yang, Ying Shan
  created_at: '2024-12-22T21:39:20.357358'
  issue_number: 38
  issue_url: https://github.com/dmarx/arxiv-archive/issues/38
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style\n \
    \ Adapter"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2312.00330v2
'2312.07731':
  abstract: 'Recent work proposed a new mechanism to remove protective perturbation
    added

    by Glaze in order to again enable mimicry of art styles from images protected

    by Glaze. Despite promising results shown in the original paper, our own tests

    with the authors'' code demonstrated several limitations of the proposed

    purification approach. The main limitations are 1) purification has a limited

    effect when tested on artists that are not well-known historical artists

    already embedded in original training data, 2) problems in evaluation metrics,

    and 3) collateral damage on mimicry result for clean images. We believe these

    limitations should be carefully considered in order to understand real world

    usability of the purification attack.'
  arxivId: '2312.07731'
  authors: Shawn Shan, Stanley Wu, Haitao Zheng, Ben Y. Zhao
  created_at: '2024-12-22T21:37:53.372547'
  issue_number: 161
  issue_url: https://github.com/dmarx/arxiv-archive/issues/161
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: A Response to Glaze Purification via IMPRESS
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2312.07731v1
'2402.03239':
  abstract: 'Bluesky is a new social network built upon the AT Protocol, a decentralized

    foundation for public social media. It was launched in private beta in February

    2023, and has grown to over 10 million registered users by October 2024. In

    this paper we introduce the architecture of Bluesky and the AT Protocol, and

    explain how the technical design of Bluesky is informed by our goals: to enable

    decentralization by having multiple interoperable providers for every part of

    the system; to make it easy for users to switch providers; to give users agency

    over the content they see; and to provide a simple user experience that does

    not burden users with complexity arising from the system''s decentralized

    nature. The system''s openness allows anybody to contribute to content

    moderation and community management, and we invite the research community to

    use Bluesky as a dataset and testing ground for new approaches in social media

    moderation.'
  arxivId: '2402.03239'
  authors: Martin Kleppmann, Paul Frazee, Jake Gold, Jay Graber, Daniel Holmgren,
    Devin Ivy, Jeromy Johnson, Bryan Newbold, Jaz Volpert
  created_at: '2024-12-22T21:38:38.329881'
  issue_number: 105
  issue_url: https://github.com/dmarx/arxiv-archive/issues/105
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Bluesky and the AT Protocol: Usable Decentralized Social Media'
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2402.03239v2
'2402.14903':
  abstract: 'Tokenization, the division of input text into input tokens, is an often

    overlooked aspect of the large language model (LLM) pipeline and could be the

    source of useful or harmful inductive biases. Historically, LLMs have relied on

    byte pair encoding, without care to specific input domains. With the increased

    use of LLMs for reasoning, various number-specific tokenization schemes have

    been adopted, with popular models like LLaMa and PaLM opting for single-digit

    tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and

    3-digit numbers. In this work, we study the effect this choice has on numerical

    reasoning through the use of arithmetic tasks. We consider left-to-right and

    right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left

    tokenization (enforced by comma separating numbers at inference time) leads to

    largely improved performance. Furthermore, we find that model errors when using

    standard left-to-right tokenization follow stereotyped error patterns,

    suggesting that model computations are systematic rather than approximate. We

    show that the model is able to convert between tokenizations easily, thus

    allowing chain-of-thought-inspired approaches to recover performance on

    left-to-right tokenized inputs. We also find the gap between tokenization

    directions decreases when models are scaled, possibly indicating that larger

    models are better able to override this tokenization-dependent inductive bias.

    In summary, our work performs the first study of how number tokenization

    choices lead to differences in model performance on arithmetic tasks,

    accompanied by a thorough analysis of error patterns. We hope this work

    inspires practitioners to more carefully ablate number tokenization-related

    choices when working towards general models of numerical reasoning.'
  arxivId: '2402.14903'
  authors: Aaditya K. Singh, DJ Strouse
  created_at: '2024-12-22T21:39:08.507083'
  issue_number: 78
  issue_url: https://github.com/dmarx/arxiv-archive/issues/78
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier\
    \ LLMs"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2402.14903v1
'2404.13320':
  abstract: 'Adversarial examples for diffusion models are widely used as solutions
    for

    safety concerns. By adding adversarial perturbations to personal images,

    attackers can not edit or imitate them easily. However, it is essential to note

    that all these protections target the latent diffusion model (LDMs), the

    adversarial examples for diffusion models in the pixel space (PDMs) are largely

    overlooked. This may mislead us to think that the diffusion models are

    vulnerable to adversarial attacks like most deep models. In this paper, we show

    novel findings that: even though gradient-based white-box attacks can be used

    to attack the LDMs, they fail to attack PDMs. This finding is supported by

    extensive experiments of almost a wide range of attacking methods on various

    PDMs and LDMs with different model structures, which means diffusion models are

    indeed much more robust against adversarial attacks. We also find that PDMs can

    be used as an off-the-shelf purifier to effectively remove the adversarial

    patterns that were generated on LDMs to protect the images, which means that

    most protection methods nowadays, to some extent, cannot protect our images

    from malicious attacks. We hope that our insights will inspire the community to

    rethink the adversarial samples for diffusion models as protection methods and

    move forward to more effective protection. Codes are available in

    https://github.com/xavihart/PDM-Pure.'
  arxivId: '2404.13320'
  authors: Haotian Xue, Yongxin Chen
  created_at: '2024-12-22T21:38:14.358400'
  issue_number: 151
  issue_url: https://github.com/dmarx/arxiv-archive/issues/151
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than\n\
    \  We Think"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2404.13320v2
'2405.06865':
  abstract: 'Generative AI models are often used to perform mimicry attacks, where
    a

    pretrained model is fine-tuned on a small sample of images to learn to mimic a

    specific artist of interest. While researchers have introduced multiple

    anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence

    points to a growing trend of mimicry models using videos as sources of training

    data. This paper presents our experiences exploring techniques to disrupt style

    mimicry on video imagery. We first validate that mimicry attacks can succeed by

    training on individual frames extracted from videos. We show that while

    anti-mimicry tools can offer protection when applied to individual frames, this

    approach is vulnerable to an adaptive countermeasure that removes protection by

    exploiting randomness in optimization results of consecutive (nearly-identical)

    frames. We develop a new, tool-agnostic framework that segments videos into

    short scenes based on frame-level similarity, and use a per-scene optimization

    baseline to remove inter-frame randomization while reducing computational cost.

    We show via both image level metrics and an end-to-end user study that the

    resulting protection restores protection against mimicry (including the

    countermeasure). Finally, we develop another adaptive countermeasure and find

    that it falls short against our framework.'
  arxivId: '2405.06865'
  authors: Josephine Passananti, Stanley Wu, Shawn Shan, Haitao Zheng, Ben Y. Zhao
  created_at: '2024-12-22T21:38:11.365867'
  issue_number: 153
  issue_url: https://github.com/dmarx/arxiv-archive/issues/153
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Disrupting Style Mimicry Attacks on Video Imagery
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2405.06865v1
'2405.16567':
  abstract: 'Recent AI systems have shown extremely powerful performance, even surpassing

    human performance, on various tasks such as information retrieval, language

    generation, and image generation based on large language models (LLMs). At the

    same time, there are diverse safety risks that can cause the generation of

    malicious contents by circumventing the alignment in LLMs, which are often

    referred to as jailbreaking. However, most of the previous works only focused

    on the text-based jailbreaking in LLMs, and the jailbreaking of the

    text-to-image (T2I) generation system has been relatively overlooked. In this

    paper, we first evaluate the safety of the commercial T2I generation systems,

    such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive

    prompts. From this empirical study, we find that Copilot and Gemini block only

    12% and 17% of the attacks with naive prompts, respectively, while ChatGPT

    blocks 84% of them. Then, we further propose a stronger automated jailbreaking

    pipeline for T2I generation systems, which produces prompts that bypass their

    safety guards. Our automated jailbreaking framework leverages an LLM optimizer

    to generate prompts to maximize degree of violation from the generated images

    without any weight updates or gradient computation. Surprisingly, our simple

    yet effective approach successfully jailbreaks the ChatGPT with 11.0% block

    rate, making it generate copyrighted contents in 76% of the time. Finally, we

    explore various defense strategies, such as post-generation filtering and

    machine unlearning techniques, but found that they were inadequate, which

    suggests the necessity of stronger defense mechanisms.'
  arxivId: '2405.16567'
  authors: Minseon Kim, Hyomin Lee, Boqing Gong, Huishuai Zhang, Sung Ju Hwang
  created_at: '2024-12-22T21:38:17.332931'
  issue_number: 150
  issue_url: https://github.com/dmarx/arxiv-archive/issues/150
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Automatic Jailbreaking of the Text-to-Image Generative AI Systems
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2405.16567v2
'2405.17399':
  abstract: "The poor performance of transformers on arithmetic tasks seems to stem\
    \ in\nlarge part from their inability to keep track of the exact position of each\n\
    digit inside of a large span of digits. We mend this problem by adding an\nembedding\
    \ to each digit that encodes its position relative to the start of the\nnumber.\
    \ In addition to the boost these embeddings provide on their own, we show\nthat\
    \ this fix enables architectural modifications such as input injection and\nrecurrent\
    \ layers to improve performance even further.\n  With positions resolved, we can\
    \ study the logical extrapolation ability of\ntransformers. Can they solve arithmetic\
    \ problems that are larger and more\ncomplex than those in their training data?\
    \ We find that training on only 20\ndigit numbers with a single GPU for one day,\
    \ we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on\
    \ 100 digit addition problems.\nFinally, we show that these gains in numeracy\
    \ also unlock improvements on other\nmulti-step reasoning tasks including sorting\
    \ and multiplication."
  arxivId: '2405.17399'
  authors: Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian
    R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild,
    Tom Goldstein
  created_at: '2024-12-22T21:39:05.343133'
  issue_number: 79
  issue_url: https://github.com/dmarx/arxiv-archive/issues/79
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Transformers Can Do Arithmetic with the Right Embeddings
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2405.17399v1
'2405.17472':
  abstract: 'Text-to-image diffusion models can be fine-tuned in custom domains to
    adapt

    to specific user preferences, but such adaptability has also been utilized for

    illegal purposes, such as forging public figures'' portraits, duplicating

    copyrighted artworks and generating explicit contents. Existing work focused on

    detecting the illegally generated contents, but cannot prevent or mitigate

    illegal adaptations of diffusion models. Other schemes of model unlearning and

    reinitialization, similarly, cannot prevent users from relearning the knowledge

    of illegal model adaptation with custom data. In this paper, we present

    FreezeAsGuard, a new technique that addresses these limitations and enables

    irreversible mitigation of illegal adaptations of diffusion models. Our

    approach is that the model publisher selectively freezes tensors in pre-trained

    diffusion models that are critical to illegal model adaptations, to mitigate

    the fine-tuned model''s representation power in illegal adaptations, but

    minimize the impact on other legal adaptations. Experiment results in multiple

    text-to-image application domains show that FreezeAsGuard provides 37% stronger

    power in mitigating illegal model adaptations compared to competitive

    baselines, while incurring less than 5% impact on legal model adaptations. The

    source code is available at: https://github.com/pittisl/FreezeAsGuard.'
  arxivId: '2405.17472'
  authors: Kai Huang, Haoming Wang, Wei Gao
  created_at: '2024-12-22T21:37:44.583491'
  issue_number: 167
  issue_url: https://github.com/dmarx/arxiv-archive/issues/167
  labels:
  - paper
  - rating:downvote
  last_read: null
  state: open
  title: "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via\n \
    \ Selective Tensor Freezing"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2405.17472v2
'2405.20053':
  abstract: 'Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context

    learning capabilities; however, their behaviors are often difficult to control.

    By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible

    to fine-tune unsupervised LMs to follow instructions and produce outputs that

    reflect human preferences. Despite its benefits, RLHF has been shown to

    potentially harm a language model''s reasoning capabilities and introduce

    artifacts such as hallucinations where the model may fabricate facts. To

    address this issue we introduce Direct Preference Heads (DPH), a fine-tuning

    framework that enables LMs to learn human preference signals through an

    auxiliary reward head without directly affecting the output distribution of the

    language modeling head. We perform a theoretical analysis of our objective

    function and find strong ties to Conservative Direct Preference Optimization

    (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All

    evaluation suite and demonstrate that our method produces models which achieve

    higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct

    Preference Optimization (DPO) alone.'
  arxivId: '2405.20053'
  authors: Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic
  created_at: '2024-12-22T21:39:29.330589'
  issue_number: 31
  issue_url: https://github.com/dmarx/arxiv-archive/issues/31
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Would I Lie To You? Inference Time Alignment of Language Models using\n\
    \  Direct Preference Heads"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2405.20053v1
'2406.12027':
  abstract: 'Artists are increasingly concerned about advancements in image generation

    models that can closely replicate their unique artistic styles. In response,

    several protection tools against style mimicry have been developed that

    incorporate small adversarial perturbations into artworks published online. In

    this work, we evaluate the effectiveness of popular protections -- with

    millions of downloads -- and show they only provide a false sense of security.

    We find that low-effort and "off-the-shelf" techniques, such as image

    upscaling, are sufficient to create robust mimicry methods that significantly

    degrade existing protections. Through a user study, we demonstrate that all

    existing protections can be easily bypassed, leaving artists vulnerable to

    style mimicry. We caution that tools based on adversarial perturbations cannot

    reliably protect artists from the misuse of generative AI, and urge the

    development of alternative non-technological solutions.'
  arxivId: '2406.12027'
  authors: Robert H√∂nig, Javier Rando, Nicholas Carlini, Florian Tram√®r
  created_at: '2024-12-22T21:38:08.546766'
  issue_number: 154
  issue_url: https://github.com/dmarx/arxiv-archive/issues/154
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Adversarial Perturbations Cannot Reliably Protect Artists From\n  Generative\
    \ AI"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2406.12027v1
'2407.17465':
  abstract: 'The Maximal Update Parametrization ($\mu$P) aims to make the optimal

    hyperparameters (HPs) of a model independent of its size, allowing them to be

    swept using a cheap proxy model rather than the full-size target model. We

    present a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with

    Unit Scaling, a method for designing models that makes them easy to train in

    low-precision. The two techniques have a natural affinity: $\mu$P ensures that

    the scale of activations is independent of model size, and Unit Scaling ensures

    that activations, weights and gradients begin training with a scale of one.

    This synthesis opens the door to a simpler scheme, whose default values are

    near-optimal. This in turn facilitates a more efficient sweeping strategy, with

    u-$\mu$P models reaching a loss that is equal to or lower than comparable

    $\mu$P models and working out-of-the-box in FP8.'
  arxivId: '2407.17465'
  authors: Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y.
    Prince, Bj√∂rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach,
    Douglas Orr
  created_at: '2024-12-22T21:39:11.447420'
  issue_number: 77
  issue_url: https://github.com/dmarx/arxiv-archive/issues/77
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'u-$Œº$P: The Unit-Scaled Maximal Update Parametrization'
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2407.17465v2
'2408.11810':
  abstract: 'Diffusion Models have emerged as powerful generative models for high-quality

    image synthesis, with many subsequent image editing techniques based on them.

    However, the ease of text-based image editing introduces significant risks,

    such as malicious editing for scams or intellectual property infringement.

    Previous works have attempted to safeguard images from diffusion-based editing

    by adding imperceptible perturbations. These methods are costly and

    specifically target prevalent Latent Diffusion Models (LDMs), while

    Pixel-domain Diffusion Models (PDMs) remain largely unexplored and robust

    against such attacks. Our work addresses this gap by proposing a novel

    attacking framework with a feature representation attack loss that exploits

    vulnerabilities in denoising UNets and a latent optimization strategy to

    enhance the naturalness of protected images. Extensive experiments demonstrate

    the effectiveness of our approach in attacking dominant PDM-based editing

    methods (e.g., SDEdit) while maintaining reasonable protection fidelity and

    robustness against common defense methods. Additionally, our framework is

    extensible to LDMs, achieving comparable performance to existing approaches.'
  arxivId: '2408.11810'
  authors: Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao, Ernie Chu, Cheng-Fu Chou, Jun-Cheng
    Chen
  created_at: '2024-12-22T21:37:59.334320'
  issue_number: 158
  issue_url: https://github.com/dmarx/arxiv-archive/issues/158
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain\n \
    \ Diffusion Models"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2408.11810v1
'2410.01131':
  abstract: 'We propose a novel neural network architecture, the normalized Transformer

    (nGPT) with representation learning on the hypersphere. In nGPT, all vectors

    forming the embeddings, MLP, attention matrices and hidden states are unit norm

    normalized. The input stream of tokens travels on the surface of a hypersphere,

    with each layer contributing a displacement towards the target output

    predictions. These displacements are defined by the MLP and attention blocks,

    whose vector components also reside on the same hypersphere. Experiments show

    that nGPT learns much faster, reducing the number of training steps required to

    achieve the same accuracy by a factor of 4 to 20, depending on the sequence

    length.'
  arxivId: '2410.01131'
  authors: Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg
  created_at: '2024-12-22T21:39:26.328348'
  issue_number: 33
  issue_url: https://github.com/dmarx/arxiv-archive/issues/33
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-22T22:12:06.030783'
  state: open
  title: "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere"
  total_reading_time_minutes: 1
  url: http://arxiv.org/abs/2410.01131v1
'2410.15468':
  abstract: 'We consider emergence from the perspective of dynamics: states of a system

    evolving with time. We focus on the role of a decomposition of wholes into

    parts, and attempt to characterize relationships between levels without

    reference to whether higher-level properties are "novel" or "unexpected." We

    offer a classification of different varieties of emergence, with and without

    new ontological elements at higher levels.'
  arxivId: '2410.15468'
  authors: Sean M. Carroll, Achyuth Parola
  created_at: '2024-12-22T21:38:35.356620'
  issue_number: 107
  issue_url: https://github.com/dmarx/arxiv-archive/issues/107
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: What Emergence Can Possibly Mean
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2410.15468v1
'2411.04282':
  abstract: 'Large language models (LLMs) have shown impressive capabilities, but
    still

    struggle with complex reasoning tasks requiring multiple steps. While

    prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at

    inference time, optimizing reasoning capabilities during training remains

    challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled

    framework that formulates reasoning as sampling from a latent distribution and

    optimizes it via variational approaches. LaTRO enables LLMs to concurrently

    improve both their reasoning process and ability to evaluate reasoning quality,

    without requiring external feedback or reward models. We validate LaTRO through

    experiments on GSM8K and ARC-Challenge datasets using multiple model

    architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of

    12.5% over base models and 9.6% over supervised fine-tuning across

    Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that

    pre-trained LLMs possess latent reasoning capabilities that can be unlocked and

    enhanced through our proposed optimization approach in a self-improvement

    manner. The code of LaTRO is available at

    \url{https://github.com/SalesforceAIResearch/LaTRO}.'
  arxivId: '2411.04282'
  authors: Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby
    Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang
  created_at: '2024-12-22T21:37:41.681855'
  issue_number: 169
  issue_url: https://github.com/dmarx/arxiv-archive/issues/169
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities\
    \ via Self-Rewarding"
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2411.04282v2
'2411.18933':
  abstract: 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video

    object segmentation and tracking anything. Key components of SAM 2 that drive

    the impressive video object segmentation performance include a large multistage

    image encoder for frame feature extraction and a memory mechanism that stores

    memory contexts from past frames to help current frame segmentation. The high

    computation complexity of multistage image encoder and memory module has

    limited its applications in real-world tasks, e.g., video object segmentation

    on mobile devices. To address this limitation, we propose EfficientTAMs,

    lightweight track anything models that produce high-quality results with low

    latency and model size. Our idea is based on revisiting the plain,

    nonhierarchical Vision Transformer (ViT) as an image encoder for video object

    segmentation, and introducing an efficient memory module, which reduces the

    complexity for both frame feature extraction and memory computation for current

    frame segmentation. We take vanilla lightweight ViTs and efficient memory

    module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets

    for video object segmentation and track anything tasks. We evaluate on multiple

    video segmentation benchmarks including semi-supervised VOS and promptable

    video segmentation, and find that our proposed EfficientTAM with vanilla ViT

    perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and

    ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs

    also perform favorably over original SAM with ~20x speedup on A100 and ~20x

    parameter reduction. On mobile devices such as iPhone 15 Pro Max, our

    EfficientTAMs can run at ~10 FPS for performing video object segmentation with

    reasonable quality, highlighting the capability of small models for on-device

    video object segmentation applications.'
  arxivId: '2411.18933'
  authors: Yunyang Xiong, Chong Zhou, Xiaoyu Xiang, Lemeng Wu, Chenchen Zhu, Zechun
    Liu, Saksham Suri, Balakrishnan Varadarajan, Ramya Akula, Forrest Iandola, Raghuraman
    Krishnamoorthi, Bilge Soran, Vikas Chandra
  created_at: '2024-12-22T21:39:23.333775'
  issue_number: 37
  issue_url: https://github.com/dmarx/arxiv-archive/issues/37
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Efficient Track Anything
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2411.18933v1
'2411.19722':
  abstract: 'Removing modeling constraints and unifying architectures across domains
    has

    been a key driver of the recent progress in training large multimodal models.

    However, most of these models still rely on many separately trained components

    such as modality-specific encoders and decoders. In this work, we further

    streamline joint generative modeling of images and text. We propose an

    autoregressive decoder-only transformer - JetFormer - which is trained to

    directly maximize the likelihood of raw data, without relying on any separately

    pretrained components, and can understand and generate both text and images.

    Specifically, we leverage a normalizing flow model to obtain a soft-token image

    representation that is jointly trained with an autoregressive multimodal

    transformer. The normalizing flow model serves as both an image encoder for

    perception tasks and an image decoder for image generation tasks during

    inference. JetFormer achieves text-to-image generation quality competitive with

    recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained

    image autoencoders, which are trained with a complex mixture of losses,

    including perceptual ones. At the same time, JetFormer demonstrates robust

    image understanding capabilities. To the best of our knowledge, JetFormer is

    the first model that is capable of generating high-fidelity images and

    producing strong log-likelihood bounds.'
  arxivId: '2411.19722'
  authors: Michael Tschannen, Andr√© Susano Pinto, Alexander Kolesnikov
  created_at: '2024-12-22T21:39:17.352649'
  issue_number: 40
  issue_url: https://github.com/dmarx/arxiv-archive/issues/40
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-22T22:12:06.030253'
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_minutes: 5
  url: http://arxiv.org/abs/2411.19722v1
'2412.04619':
  abstract: 'Language models (LMs), like other neural networks, often favor shortcut

    heuristics based on surface-level patterns. Although LMs behave like n-gram

    models early in training, they must eventually learn hierarchical syntactic

    representations to correctly apply grammatical rules out-of-distribution (OOD).

    In this work, we use case studies of English grammar to explore how complex,

    diverse training data drives models to generalize OOD. We construct a framework

    that unifies our understanding of random variation with training dynamics, rule

    selection with memorization, and data diversity with complexity. We show that

    these factors are nuanced, and that intermediate levels of diversity and

    complexity lead to inconsistent behavior across random seeds and to unstable

    training dynamics. Our findings emphasize the critical role of training data in

    shaping generalization patterns and illuminate how competing model strategies

    lead to inconsistent generalization outcomes across random seeds. Code is

    available at https://github.com/sunnytqin/concept_comp.git.'
  arxivId: '2412.04619'
  authors: Tian Qin, Naomi Saphra, David Alvarez-Melis
  created_at: '2024-12-22T21:37:47.359110'
  issue_number: 164
  issue_url: https://github.com/dmarx/arxiv-archive/issues/164
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-22T22:12:06.028083'
  state: open
  title: 'Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization'
  total_reading_time_minutes: 1
  url: http://arxiv.org/abs/2412.04619v3
'2412.11965':
  abstract: 'Attention heads are one of the building blocks of large language models

    (LLMs). Prior work on investigating their operation mostly focused on analyzing

    their behavior during inference for specific circuits or tasks. In this work,

    we seek a comprehensive mapping of the operations they implement in a model. We

    propose MAPS (Mapping Attention head ParameterS), an efficient framework that

    infers the functionality of attention heads from their parameters, without any

    model training or inference. We showcase the utility of MAPS for answering two

    types of questions: (a) given a predefined operation, mapping how strongly

    heads across the model implement it, and (b) given an attention head, inferring

    its salient functionality. Evaluating MAPS on 20 operations across 6 popular

    LLMs shows its estimations correlate with the head''s outputs during inference

    and are causally linked to the model''s predictions. Moreover, its mappings

    reveal attention heads of certain operations that were overlooked in previous

    studies, and valuable insights on function universality and architecture biases

    in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS

    to characterize the salient operations of a given head. Our pipeline produces

    plausible operation descriptions for most heads, as assessed by human judgment,

    while revealing diverse operations.'
  arxivId: '2412.11965'
  authors: Amit Elhelo, Mor Geva
  created_at: '2024-12-22T21:38:23.347636'
  issue_number: 144
  issue_url: https://github.com/dmarx/arxiv-archive/issues/144
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-22T21:38:23.348486'
  state: open
  title: Inferring Functionality of Attention Heads from their Parameters
  total_reading_time_minutes: 1
  url: http://arxiv.org/abs/2412.11965v1
'2412.13145':
  abstract: 'Could an AI have conscious experiences? Any answer to this question should

    conform to Evidentialism - that is, it should be based not on intuition, dogma

    or speculation but on solid scientific evidence. I argue that such evidence is

    hard to come by and that the only justifiable stance on the prospects of

    artificial consciousness is agnosticism. In the current debate, the main

    division is between biological views that are sceptical of artificial

    consciousness and functional views that are sympathetic to it. I argue that

    both camps make the same mistake of over-estimating what the evidence tells us.

    Scientific insights into consciousness have been achieved through the study of

    conscious organisms. Although this has enabled cautious assessments of

    consciousness in various creatures, extending this to AI faces serious

    obstacles. AI thus presents consciousness researchers with a dilemma: either

    reach a verdict on artificial consciousness but violate Evidentialism; or

    respect Evidentialism but offer no verdict on the prospects of artificial

    consciousness. The dominant trend in the literature has been to take the first

    option while purporting to follow the scientific evidence. I argue that if we

    truly follow the evidence, we must take the second option and adopt

    agnosticism.'
  arxivId: '2412.13145'
  authors: Tom McClelland
  created_at: '2024-12-22T21:38:44.340063'
  issue_number: 102
  issue_url: https://github.com/dmarx/arxiv-archive/issues/102
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Agnosticism About Artificial Consciousness
  total_reading_time_minutes: 0
  url: http://arxiv.org/abs/2412.13145v1
