'1503.03585':
  abstract: 'A central problem in machine learning involves modeling complex data-sets

    using highly flexible families of probability distributions in which learning,

    sampling, inference, and evaluation are still analytically or computationally

    tractable. Here, we develop an approach that simultaneously achieves both

    flexibility and tractability. The essential idea, inspired by non-equilibrium

    statistical physics, is to systematically and slowly destroy structure in a

    data distribution through an iterative forward diffusion process. We then learn

    a reverse diffusion process that restores structure in data, yielding a highly

    flexible and tractable generative model of the data. This approach allows us to

    rapidly learn, sample from, and evaluate probabilities in deep generative

    models with thousands of layers or time steps, as well as to compute

    conditional and posterior probabilities under the learned model. We

    additionally release an open source reference implementation of the algorithm.'
  arxivId: '1503.03585'
  authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
  created_at: '2024-12-25T08:51:50.651097'
  issue_number: 118
  issue_url: https://github.com/dmarx/arxiv-archive/issues/118
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:52:20.584864'
  last_visited: '2024-12-22T07:09:20.505Z'
  state: open
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  total_reading_time_seconds: 241
  url: https://arxiv.org/abs/1503.03585
'1705.08926':
  abstract: 'Cooperative multi-agent systems can be naturally used to model many real

    world problems, such as network packet routing and the coordination of

    autonomous vehicles. There is a great need for new reinforcement learning

    methods that can efficiently learn decentralised policies for such systems. To

    this end, we propose a new multi-agent actor-critic method called

    counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised

    critic to estimate the Q-function and decentralised actors to optimise the

    agents'' policies. In addition, to address the challenges of multi-agent credit

    assignment, it uses a counterfactual baseline that marginalises out a single

    agent''s action, while keeping the other agents'' actions fixed. COMA also uses
    a

    critic representation that allows the counterfactual baseline to be computed

    efficiently in a single forward pass. We evaluate COMA in the testbed of

    StarCraft unit micromanagement, using a decentralised variant with significant

    partial observability. COMA significantly improves average performance over

    other multi-agent actor-critic methods in this setting, and the best performing

    agents are competitive with state-of-the-art centralised controllers that get

    access to the full state.'
  arxivId: '1705.08926'
  authors: Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli,
    Shimon Whiteson
  created_at: '2024-12-25T08:52:53.646297'
  issue_number: 127
  issue_url: https://github.com/dmarx/arxiv-archive/issues/127
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:25:41.427Z'
  state: open
  title: Counterfactual Multi-Agent Policy Gradients
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1705.08926
'1711.06077':
  abstract: 'Image restoration algorithms are typically evaluated by some distortion

    measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify

    perceived perceptual quality. In this paper, we prove mathematically that

    distortion and perceptual quality are at odds with each other. Specifically, we

    study the optimal probability for correctly discriminating the outputs of an

    image restoration algorithm from real images. We show that as the mean

    distortion decreases, this probability must increase (indicating worse

    perceptual quality). As opposed to the common belief, this result holds true

    for any distortion measure, and is not only a problem of the PSNR or SSIM

    criteria. We also show that generative-adversarial-nets (GANs) provide a

    principled way to approach the perception-distortion bound. This constitutes

    theoretical support to their observed success in low-level vision tasks. Based

    on our analysis, we propose a new methodology for evaluating image restoration

    methods, and use it to perform an extensive comparison between recent

    super-resolution algorithms.'
  arxivId: '1711.06077'
  authors: Yochai Blau, Tomer Michaeli
  created_at: '2024-12-25T08:52:29.580023'
  issue_number: 142
  issue_url: https://github.com/dmarx/arxiv-archive/issues/142
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T08:19:51.755Z'
  state: open
  title: The Perception-Distortion Tradeoff
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1711.06077
'1711.11586':
  abstract: 'Many image-to-image translation problems are ambiguous, as a single input

    image may correspond to multiple possible outputs. In this work, we aim to

    model a \emph{distribution} of possible outputs in a conditional generative

    modeling setting. The ambiguity of the mapping is distilled in a

    low-dimensional latent vector, which can be randomly sampled at test time. A

    generator learns to map the given input, combined with this latent code, to the

    output. We explicitly encourage the connection between output and the latent

    code to be invertible. This helps prevent a many-to-one mapping from the latent

    code to the output during training, also known as the problem of mode collapse,

    and produces more diverse results. We explore several variants of this approach

    by employing different training objectives, network architectures, and methods

    of injecting the latent code. Our proposed method encourages bijective

    consistency between the latent encoding and output modes. We present a

    systematic comparison of our method and other variants on both perceptual

    realism and diversity.'
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-25T08:53:11.630795'
  issue_number: 14
  issue_url: https://github.com/dmarx/arxiv-archive/issues/14
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T10:10:33.576785'
  last_visited: '2024-12-20T03:22:35.613000+00:00'
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/1711.11586
'1803.00567':
  abstract: 'Optimal transport (OT) theory can be informally described using the words
    of

    the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in

    hand has to move a large pile of sand lying on a construction site. The goal of

    the worker is to erect with all that sand a target pile with a prescribed shape

    (for example, that of a giant sand castle). Naturally, the worker wishes to

    minimize her total effort, quantified for instance as the total distance or

    time spent carrying shovelfuls of sand. Mathematicians interested in OT cast

    that problem as that of comparing two probability distributions, two different

    piles of sand of the same volume. They consider all of the many possible ways

    to morph, transport or reshape the first pile into the second, and associate a

    "global" cost to every such transport, using the "local" consideration of how

    much it costs to move a grain of sand from one place to another. Recent years

    have witnessed the spread of OT in several fields, thanks to the emergence of

    approximate solvers that can scale to sizes and dimensions that are relevant to

    data sciences. Thanks to this newfound scalability, OT is being increasingly

    used to unlock various problems in imaging sciences (such as color or texture

    processing), computer vision and graphics (for shape manipulation) or machine

    learning (for regression, classification and density fitting). This short book

    reviews OT with a bias toward numerical methods and their applications in data

    sciences, and sheds lights on the theoretical properties of OT that make it

    particularly useful for some of these applications.'
  arxivId: '1803.00567'
  authors: Gabriel Peyr√©, Marco Cuturi
  created_at: '2024-12-25T08:52:35.571245'
  issue_number: 138
  issue_url: https://github.com/dmarx/arxiv-archive/issues/138
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T08:08:46.564Z'
  state: open
  title: Computational Optimal Transport
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1803.00567
'1804.08838':
  abstract: 'Many recently trained neural networks employ large numbers of parameters
    to

    achieve good performance. One may intuitively use the number of parameters

    required as a rough gauge of the difficulty of a problem. But how accurate are

    such notions? How many parameters are really needed? In this paper we attempt

    to answer this question by training networks not in their native parameter

    space, but instead in a smaller, randomly oriented subspace. We slowly increase

    the dimension of this subspace, note at which dimension solutions first appear,

    and define this to be the intrinsic dimension of the objective landscape. The

    approach is simple to implement, computationally tractable, and produces

    several suggestive conclusions. Many problems have smaller intrinsic dimensions

    than one might suspect, and the intrinsic dimension for a given dataset varies

    little across a family of models with vastly different sizes. This latter

    result has the profound implication that once a parameter space is large enough

    to solve a problem, extra parameters serve directly to increase the

    dimensionality of the solution manifold. Intrinsic dimension allows some

    quantitative comparison of problem difficulty across supervised, reinforcement,

    and other types of learning where we conclude, for example, that solving the

    inverted pendulum problem is 100 times easier than classifying digits from

    MNIST, and playing Atari Pong from pixels is about as hard as classifying

    CIFAR-10. In addition to providing new cartography of the objective landscapes

    wandered by parameterized models, the method is a simple technique for

    constructively obtaining an upper bound on the minimum description length of a

    solution. A byproduct of this construction is a simple approach for compressing

    networks, in some cases by more than 100 times.'
  arxivId: '1804.08838'
  authors: Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski
  created_at: '2024-12-25T08:52:14.571974'
  issue_number: 198
  issue_url: https://github.com/dmarx/arxiv-archive/issues/198
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:52:14.574062'
  last_visited: '2024-12-24T02:11:53.179Z'
  state: open
  title: Measuring the Intrinsic Dimension of Objective Landscapes
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/1804.08838
'1810.01588':
  abstract: 'Interpreting the prediction mechanism of complex models is currently
    one of

    the most important tasks in the machine learning field, especially with layered

    neural networks, which have achieved high predictive performance with various

    practical data sets. To reveal the global structure of a trained neural network

    in an interpretable way, a series of clustering methods have been proposed,

    which decompose the units into clusters according to the similarity of their

    inference roles. The main problems in these studies were that (1) we have no

    prior knowledge about the optimal resolution for the decomposition, or the

    appropriate number of clusters, and (2) there was no method with which to

    acquire knowledge about whether the outputs of each cluster have a positive or

    negative correlation with the input and output dimension values. In this paper,

    to solve these problems, we propose a method for obtaining a hierarchical

    modular representation of a layered neural network. The application of a

    hierarchical clustering method to a trained network reveals a tree-structured

    relationship among hidden layer units, based on their feature vectors defined

    by their correlation with the input and output dimension values.'
  arxivId: '1810.01588'
  authors: Chihiro Watanabe
  created_at: '2024-12-25T08:52:08.666289'
  issue_number: 233
  issue_url: https://github.com/dmarx/arxiv-archive/issues/233
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:52:08.667993'
  last_visited: '2024-12-24T03:07:45.421000+00:00'
  state: open
  title: "Interpreting Layered Neural Networks via Hierarchical Modular\n  Representation"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/1810.01588
'1901.10159':
  abstract: 'To understand the dynamics of optimization in deep neural networks, we

    develop a tool to study the evolution of the entire Hessian spectrum throughout

    the optimization process. Using this, we study a number of hypotheses

    concerning smoothness, curvature, and sharpness in the deep learning

    literature. We then thoroughly analyze a crucial structural feature of the

    spectra: in non-batch normalized networks, we observe the rapid appearance of

    large isolated eigenvalues in the spectrum, along with a surprising

    concentration of the gradient in the corresponding eigenspaces. In batch

    normalized networks, these two effects are almost absent. We characterize these

    effects, and explain how they affect optimization speed through both theory and

    experiments. As part of this work, we adapt advanced tools from numerical

    linear algebra that allow scalable and accurate estimation of the entire

    Hessian spectrum of ImageNet-scale neural networks; this technique may be of

    independent interest in other applications.'
  arxivId: '1901.10159'
  authors: Behrooz Ghorbani, Shankar Krishnan, Ying Xiao
  created_at: '2024-12-25T08:51:59.632060'
  issue_number: 239
  issue_url: https://github.com/dmarx/arxiv-archive/issues/239
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:51:59.636049'
  last_visited: '2024-12-24T03:33:51.377Z'
  state: open
  title: "An Investigation into Neural Net Optimization via Hessian Eigenvalue\n \
    \ Density"
  total_reading_time_seconds: 102
  url: https://arxiv.org/abs/1901.10159
'1904.08779':
  abstract: 'We present SpecAugment, a simple data augmentation method for speech

    recognition. SpecAugment is applied directly to the feature inputs of a neural

    network (i.e., filter bank coefficients). The augmentation policy consists of

    warping the features, masking blocks of frequency channels, and masking blocks

    of time steps. We apply SpecAugment on Listen, Attend and Spell networks for

    end-to-end speech recognition tasks. We achieve state-of-the-art performance on

    the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.

    On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language

    model, and 5.8% WER with shallow fusion with a language model. This compares to

    the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we

    achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5''00 test set

    without the use of a language model, and 6.8%/14.1% with shallow fusion, which

    compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.'
  arxivId: '1904.08779'
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2024-12-25T09:14:38.288901'
  issue_number: 60
  issue_url: https://github.com/dmarx/arxiv-archive/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-17T14:41:05.563Z'
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1904.08779
'1906.04358':
  abstract: 'Not all neural network architectures are created equal, some perform
    much

    better than others for certain tasks. But how important are the weight

    parameters of a neural network compared to its architecture? In this work, we

    question to what extent neural network architectures alone, without learning

    any weight parameters, can encode solutions for a given task. We propose a

    search method for neural network architectures that can already perform a task

    without any explicit weight training. To evaluate these networks, we populate

    the connections with a single shared weight parameter sampled from a uniform

    random distribution, and measure the expected performance. We demonstrate that

    our method can find minimal neural network architectures that can perform

    several reinforcement learning tasks without weight training. On a supervised

    learning domain, we find network architectures that achieve much higher than

    chance accuracy on MNIST using random weights. Interactive version of this

    paper at https://weightagnostic.github.io/'
  arxivId: '1906.04358'
  authors: Adam Gaier, David Ha
  created_at: '2024-12-25T08:52:02.584766'
  issue_number: 235
  issue_url: https://github.com/dmarx/arxiv-archive/issues/235
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:52:02.587068'
  last_visited: '2024-12-24T03:27:56.957Z'
  state: open
  title: Weight Agnostic Neural Networks
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1906.04358
'2006.11120':
  abstract: 'A basic operation in Convolutional Neural Networks (CNNs) is spatial
    resizing

    of feature maps. This is done either by strided convolution (donwscaling) or

    transposed convolution (upscaling). Such operations are limited to a fixed

    filter moving at predetermined integer steps (strides). Spatial sizes of

    consecutive layers are related by integer scale factors, predetermined at

    architectural design, and remain fixed throughout training and inference time.

    We propose a generalization of the common Conv-layer, from a discrete layer to

    a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by

    representing the filter as a learned continuous function over sub-pixel

    coordinates. This allows learnable and principled resizing of feature maps, to

    any size, dynamically and consistently across scales. Once trained, the CC

    layer can be used to output any scale/size chosen at inference time. The scale

    can be non-integer and differ between the axes. CC gives rise to new freedoms

    for architectural design, such as dynamic layer shapes at inference time, or

    gradual architectures where the size changes by a small factor at each layer.

    This gives rise to many desired CNN properties, new architectural design

    capabilities, and useful applications. We further show that current Conv-layers

    suffer from inherent misalignments, which are ameliorated by CC layers.'
  arxivId: '2006.11120'
  authors: Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani
  created_at: '2024-12-25T08:51:44.913555'
  issue_number: 262
  issue_url: https://github.com/dmarx/arxiv-archive/issues/262
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:51:44.915391'
  last_visited: '2024-12-25T05:28:48.326Z'
  state: open
  title: From Discrete to Continuous Convolution Layers
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2006.11120
'2010.15110':
  abstract: 'In suitably initialized wide networks, small learning rates transform
    deep

    neural networks (DNNs) into neural tangent kernel (NTK) machines, whose

    training dynamics is well-approximated by a linear weight expansion of the

    network at initialization. Standard training, however, diverges from its

    linearization in ways that are poorly understood. We study the relationship

    between the training dynamics of nonlinear deep networks, the geometry of the

    loss landscape, and the time evolution of a data-dependent NTK. We do so

    through a large-scale phenomenological analysis of training, synthesizing

    diverse measures characterizing loss landscape geometry and NTK dynamics. In

    multiple neural architectures and datasets, we find these diverse measures

    evolve in a highly correlated manner, revealing a universal picture of the deep

    learning process. In this picture, deep network training exhibits a highly

    chaotic rapid initial transient that within 2 to 3 epochs determines the final

    linearly connected basin of low loss containing the end point of training.

    During this chaotic transient, the NTK changes rapidly, learning useful

    features from the training data that enables it to outperform the standard

    initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid

    chaotic transient, the NTK changes at constant velocity, and its performance

    matches that of full network training in 15% to 45% of training time. Overall,

    our analysis reveals a striking correlation between a diverse set of metrics

    over training time, governed by a rapid chaotic to stable transition in the

    first few epochs, that together poses challenges and opportunities for the

    development of more accurate theories of deep learning.'
  arxivId: '2010.15110'
  authors: Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
    Daniel M. Roy, Surya Ganguli
  created_at: '2024-12-25T08:51:56.557402'
  issue_number: 245
  issue_url: https://github.com/dmarx/arxiv-archive/issues/245
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T04:03:01.478Z'
  state: open
  title: "Deep learning versus kernel learning: an empirical study of loss\n  landscape\
    \ geometry and the time evolution of the Neural Tangent Kernel"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2010.15110
'2106.04647':
  abstract: 'Adapting large-scale pretrained language models to downstream tasks via

    fine-tuning is the standard method for achieving state-of-the-art performance

    on NLP benchmarks. However, fine-tuning all weights of models with millions or

    billions of parameters is sample-inefficient, unstable in low-resource

    settings, and wasteful as it requires storing a separate copy of the model for

    each task. Recent work has developed parameter-efficient fine-tuning methods,

    but these approaches either still require a relatively large number of

    parameters or underperform standard fine-tuning. In this work, we propose

    Compacter, a method for fine-tuning large-scale language models with a better

    trade-off between task performance and the number of trainable parameters than

    prior work. Compacter accomplishes this by building on top of ideas from

    adapters, low-rank optimization, and parameterized hypercomplex multiplication

    layers. Specifically, Compacter inserts task-specific weight matrices into a

    pretrained model''s weights, which are computed efficiently as a sum of

    Kronecker products between shared "slow" weights and "fast" rank-one matrices

    defined per Compacter layer. By only training 0.047% of a pretrained model''s

    parameters, Compacter performs on par with standard fine-tuning on GLUE and

    outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our

    code is publicly available at~\url{https://github.com/rabeehk/compacter}.'
  arxivId: '2106.04647'
  authors: Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder
  created_at: '2024-12-25T08:52:11.637571'
  issue_number: 202
  issue_url: https://github.com/dmarx/arxiv-archive/issues/202
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:52:11.639175'
  last_visited: '2024-12-24T02:31:09.658Z'
  state: open
  title: 'Compacter: Efficient Low-Rank Hypercomplex Adapter Layers'
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2106.04647
'2203.15556':
  abstract: 'We investigate the optimal model size and number of tokens for training
    a

    transformer language model under a given compute budget. We find that current

    large language models are significantly undertrained, a consequence of the

    recent focus on scaling language models whilst keeping the amount of training

    data constant. By training over 400 language models ranging from 70 million to

    over 16 billion parameters on 5 to 500 billion tokens, we find that for

    compute-optimal training, the model size and the number of training tokens

    should be scaled equally: for every doubling of model size the number of

    training tokens should also be doubled. We test this hypothesis by training a

    predicted compute-optimal model, Chinchilla, that uses the same compute budget

    as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla

    uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1

    (178B), and Megatron-Turing NLG (530B) on a large range of downstream

    evaluation tasks. This also means that Chinchilla uses substantially less

    compute for fine-tuning and inference, greatly facilitating downstream usage.

    As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%

    on the MMLU benchmark, greater than a 7% improvement over Gopher.'
  arxivId: '2203.15556'
  authors: Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den
    Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen,
    Jack W. Rae, Oriol Vinyals, Laurent Sifre
  created_at: '2024-12-25T10:10:34.211204'
  issue_number: 67
  issue_url: https://github.com/dmarx/arxiv-archive/issues/67
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-18T22:03:26.021Z'
  state: open
  title: Training Compute-Optimal Large Language Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2203.15556
'2210.14891':
  abstract: 'We present a smoothly broken power law functional form (that we refer
    to as a

    Broken Neural Scaling Law (BNSL)) that accurately models & extrapolates the

    scaling behaviors of deep neural networks (i.e. how the evaluation metric of

    interest varies as amount of compute used for training (or inference), number

    of model parameters, training dataset size, model input size, number of

    training steps, or upstream performance varies) for various architectures & for

    each of various tasks within a large & diverse set of upstream & downstream

    tasks, in zero-shot, prompted, & finetuned settings. This set includes

    large-scale vision, language, audio, video, diffusion, generative modeling,

    multimodal learning, contrastive learning, AI alignment, AI capabilities,

    robotics, out-of-distribution (OOD) generalization, continual learning,

    transfer learning, uncertainty estimation / calibration, OOD detection,

    adversarial robustness, distillation, sparsity, retrieval, quantization,

    pruning, fairness, molecules, computer programming/coding, math word problems,

    "emergent phase transitions", arithmetic, supervised learning,

    unsupervised/self-supervised learning, & reinforcement learning (single agent
    &

    multi-agent). When compared to other functional forms for neural scaling, this

    functional form yields extrapolations of scaling behavior that are considerably

    more accurate on this set. Moreover, this functional form accurately models &

    extrapolates scaling behavior that other functional forms are incapable of

    expressing such as the nonmonotonic transitions present in the scaling behavior

    of phenomena such as double descent & the delayed, sharp inflection points

    present in the scaling behavior of tasks such as arithmetic. Lastly, we use

    this functional form to glean insights about the limit of the predictability of

    scaling behavior. Code is available at

    https://github.com/ethancaballero/broken_neural_scaling_laws'
  arxivId: '2210.14891'
  authors: Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger
  created_at: '2024-12-25T08:53:08.561080'
  issue_number: 85
  issue_url: https://github.com/dmarx/arxiv-archive/issues/85
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-21T05:51:08.748Z'
  state: open
  title: Broken Neural Scaling Laws
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2210.14891
'2302.04222':
  abstract: 'Recent text-to-image diffusion models such as MidJourney and Stable Diffusion

    threaten to displace many in the professional artist community. In particular,

    models can learn to mimic the artistic style of specific artists after

    "fine-tuning" on samples of their art. In this paper, we describe the design,

    implementation and evaluation of Glaze, a tool that enables artists to apply

    "style cloaks" to their art before sharing online. These cloaks apply barely

    perceptible perturbations to images, and when used as training data, mislead

    generative models that try to mimic a specific artist. In coordination with the

    professional artist community, we deploy user studies to more than 1000

    artists, assessing their views of AI art, as well as the efficacy of our tool,

    its usability and tolerability of perturbations, and robustness across

    different scenarios and against adaptive countermeasures. Both surveyed artists

    and empirical CLIP-based scores show that even at low perturbation levels

    (p=0.05), Glaze is highly successful at disrupting mimicry under normal

    conditions (>92%) and against adaptive countermeasures (>85%).'
  arxivId: '2302.04222'
  authors: Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben
    Y. Zhao
  created_at: '2024-12-25T08:52:23.548718'
  issue_number: 147
  issue_url: https://github.com/dmarx/arxiv-archive/issues/147
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-22T16:18:52.405Z'
  state: open
  title: 'Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2302.04222
'2302.11529':
  abstract: 'Transfer learning has recently become the dominant paradigm of machine

    learning. Pre-trained models fine-tuned for downstream tasks achieve better

    performance with fewer labelled examples. Nonetheless, it remains unclear how

    to develop models that specialise towards multiple tasks without incurring

    negative interference and that generalise systematically to non-identically

    distributed tasks. Modular deep learning has emerged as a promising solution to

    these challenges. In this framework, units of computation are often implemented

    as autonomous parameter-efficient modules. Information is conditionally routed

    to a subset of modules and subsequently aggregated. These properties enable

    positive transfer and systematic generalisation by separating computation from

    routing and updating modules locally. We offer a survey of modular

    architectures, providing a unified view over several threads of research that

    evolved independently in the scientific literature. Moreover, we explore

    various additional purposes of modularity, including scaling language models,

    causal inference, programme induction, and planning in reinforcement learning.

    Finally, we report various concrete applications where modularity has been

    successfully deployed such as cross-lingual and cross-modal knowledge transfer.

    Related talks and projects to this survey, are available at

    https://www.modulardeeplearning.com/.'
  arxivId: '2302.11529'
  authors: Jonas Pfeiffer, Sebastian Ruder, Ivan Vuliƒá, Edoardo Maria Ponti
  created_at: '2024-12-25T08:52:05.560932'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-25T08:52:05.561980'
  last_visited: '2024-12-24T03:25:53.647000+00:00'
  state: open
  title: Modular Deep Learning
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2302.11529
'2303.11435':
  abstract: 'Inversion by Direct Iteration (InDI) is a new formulation for supervised

    image restoration that avoids the so-called "regression to the mean" effect and

    produces more realistic and detailed images than existing regression-based

    methods. It does this by gradually improving image quality in small steps,

    similar to generative denoising diffusion models. Image restoration is an

    ill-posed problem where multiple high-quality images are plausible

    reconstructions of a given low-quality input. Therefore, the outcome of a

    single step regression model is typically an aggregate of all possible

    explanations, therefore lacking details and realism. The main advantage of InDI

    is that it does not try to predict the clean target image in a single step but

    instead gradually improves the image in small steps, resulting in better

    perceptual quality. While generative denoising diffusion models also work in

    small steps, our formulation is distinct in that it does not require knowledge

    of any analytic form of the degradation process. Instead, we directly learn an

    iterative restoration process from low-quality and high-quality paired

    examples. InDI can be applied to virtually any image degradation, given paired

    training data. In conditional denoising diffusion image restoration the

    denoising network generates the restored image by repeatedly denoising an

    initial image of pure noise, conditioned on the degraded input. Contrary to

    conditional denoising formulations, InDI directly proceeds by iteratively

    restoring the input low-quality image, producing high-quality results on a

    variety of image restoration tasks, including motion and out-of-focus

    deblurring, super-resolution, compression artifact removal, and denoising.'
  arxivId: '2303.11435'
  authors: Mauricio Delbracio, Peyman Milanfar
  created_at: '2024-12-25T08:52:32.573339'
  issue_number: 140
  issue_url: https://github.com/dmarx/arxiv-archive/issues/140
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T08:12:43.437Z'
  state: open
  title: "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for\n\
    \  Image Restoration"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2303.11435
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    for

    generating high-dimensional data. In this paper, we show that the dynamics of

    these models exhibit a spontaneous symmetry breaking that divides the

    generative dynamics into two distinct phases: 1) A linear steady-state dynamics

    around a central fixed-point and 2) an attractor dynamics directed towards the

    data manifold. These two "phases" are separated by the change in stability of

    the central fixed-point, with the resulting window of instability being

    responsible for the diversity of the generated samples. Using both theoretical

    and empirical evidence, we show that an accurate simulation of the early

    dynamics does not significantly contribute to the final generation, since early

    fluctuations are reverted to the central fixed point. To leverage this insight,

    we propose a Gaussian late initialization scheme, which significantly improves

    model performance, achieving up to 3x FID improvements on fast samplers, while

    also increasing sample diversity (e.g., racial composition of generated CelebA

    images). Our work offers a new way to understand the generative dynamics of

    diffusion models that has the potential to bring about higher performance and

    less biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-25T09:14:51.398188'
  issue_number: 24
  issue_url: https://github.com/dmarx/arxiv-archive/issues/24
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T09:58:39.593Z'
  state: open
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2305.19693
'2309.03060':
  abstract: 'Many areas of machine learning and science involve large linear algebra

    problems, such as eigendecompositions, solving linear systems, computing matrix

    exponentials, and trace estimation. The matrices involved often have Kronecker,

    convolutional, block diagonal, sum, or product structure. In this paper, we

    propose a simple but general framework for large-scale linear algebra problems

    in machine learning, named CoLA (Compositional Linear Algebra). By combining a

    linear operator abstraction with compositional dispatch rules, CoLA

    automatically constructs memory and runtime efficient numerical algorithms.

    Moreover, CoLA provides memory efficient automatic differentiation, low

    precision computation, and GPU acceleration in both JAX and PyTorch, while also

    accommodating new objects, operations, and rules in downstream packages via

    multiple dispatch. CoLA can accelerate many algebraic operations, while making

    it easy to prototype matrix structures and algorithms, providing an appealing

    drop-in tool for virtually any computational effort that requires linear

    algebra. We showcase its efficacy across a broad range of applications,

    including partial differential equations, Gaussian processes, equivariant model

    construction, and unsupervised learning.'
  arxivId: '2309.03060'
  authors: Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson
  created_at: '2024-12-25T08:52:41.705306'
  issue_number: 132
  issue_url: https://github.com/dmarx/arxiv-archive/issues/132
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:34:16.085Z'
  state: open
  title: "CoLA: Exploiting Compositional Structure for Automatic and Efficient\n \
    \ Numerical Linear Algebra"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2309.03060
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering

    times by orders of magnitude. While these methods demonstrate state-of-the-art

    quality and speed, they are designed for photogrammetry of static scenes and do

    not generalize well to freely moving humans in the environment. In this work,

    we introduce Human Gaussian Splats (HUGS) that represents an animatable human

    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes

    only a monocular video with a small number of (50-100) frames, and it

    automatically learns to disentangle the static scene and a fully animatable

    human avatar within 30 minutes. We utilize the SMPL body model to initialize

    the human Gaussians. To capture details that are not modeled by SMPL (e.g.

    cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.

    Utilizing 3D Gaussians for animated humans brings new challenges, including the

    artifacts created when articulating the Gaussians. We propose to jointly

    optimize the linear blend skinning weights to coordinate the movements of

    individual Gaussians during animation. Our approach enables novel-pose

    synthesis of human and novel view synthesis of both the human and the scene. We

    achieve state-of-the-art rendering quality with a rendering speed of 60 FPS

    while being ~100x faster to train over previous work. Our code will be

    announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-25T09:14:48.166602'
  issue_number: 18
  issue_url: https://github.com/dmarx/arxiv-archive/issues/18
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T08:20:12.812Z'
  state: open
  title: 'HUGS: Human Gaussian Splats'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2311.17910
'2401.17671':
  abstract: 'Recent advancements in artificial intelligence have sparked interest
    in the

    parallels between large language models (LLMs) and human neural processing,

    particularly in language comprehension. While prior research has established

    similarities in the representation of LLMs and the brain, the underlying

    computational principles that cause this convergence, especially in the context

    of evolving LLMs, remain elusive. Here, we examined a diverse selection of

    high-performance LLMs with similar parameter sizes to investigate the factors

    contributing to their alignment with the brain''s language processing

    mechanisms. We find that as LLMs achieve higher performance on benchmark tasks,

    they not only become more brain-like as measured by higher performance when

    predicting neural responses from LLM embeddings, but also their hierarchical

    feature extraction pathways map more closely onto the brain''s while using fewer

    layers to do the same encoding. We also compare the feature extraction pathways

    of the LLMs to each other and identify new ways in which high-performing models

    have converged toward similar hierarchical processing mechanisms. Finally, we

    show the importance of contextual information in improving model performance

    and brain similarity. Our findings reveal the converging aspects of language

    processing in the brain and LLMs and offer new directions for developing models

    that align more closely with human cognitive processing.'
  arxivId: '2401.17671'
  authors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima
    Mesgarani
  created_at: '2024-12-25T09:14:25.524870'
  issue_number: 70
  issue_url: https://github.com/dmarx/arxiv-archive/issues/70
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T11:12:49.772Z'
  state: open
  title: "Contextual Feature Extraction Hierarchies Converge in Large Language\n \
    \ Models and the Brain"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2401.17671
'2402.14903':
  abstract: 'Tokenization, the division of input text into input tokens, is an often

    overlooked aspect of the large language model (LLM) pipeline and could be the

    source of useful or harmful inductive biases. Historically, LLMs have relied on

    byte pair encoding, without care to specific input domains. With the increased

    use of LLMs for reasoning, various number-specific tokenization schemes have

    been adopted, with popular models like LLaMa and PaLM opting for single-digit

    tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and

    3-digit numbers. In this work, we study the effect this choice has on numerical

    reasoning through the use of arithmetic tasks. We consider left-to-right and

    right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left

    tokenization (enforced by comma separating numbers at inference time) leads to

    largely improved performance. Furthermore, we find that model errors when using

    standard left-to-right tokenization follow stereotyped error patterns,

    suggesting that model computations are systematic rather than approximate. We

    show that the model is able to convert between tokenizations easily, thus

    allowing chain-of-thought-inspired approaches to recover performance on

    left-to-right tokenized inputs. We also find the gap between tokenization

    directions decreases when models are scaled, possibly indicating that larger

    models are better able to override this tokenization-dependent inductive bias.

    In summary, our work performs the first study of how number tokenization

    choices lead to differences in model performance on arithmetic tasks,

    accompanied by a thorough analysis of error patterns. We hope this work

    inspires practitioners to more carefully ablate number tokenization-related

    choices when working towards general models of numerical reasoning.'
  arxivId: '2402.14903'
  authors: Aaditya K. Singh, DJ Strouse
  created_at: '2024-12-25T08:53:17.551409'
  issue_number: 78
  issue_url: https://github.com/dmarx/arxiv-archive/issues/78
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T22:43:04.367Z'
  state: open
  title: "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier\
    \ LLMs"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2402.14903
'2403.14781':
  abstract: 'In this study, we introduce a methodology for human image animation by

    leveraging a 3D human parametric model within a latent diffusion framework to

    enhance shape alignment and motion guidance in curernt human generative

    techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear)

    model as the 3D human parametric model to establish a unified representation of

    body shape and pose. This facilitates the accurate capture of intricate human

    geometry and motion characteristics from source videos. Specifically, we

    incorporate rendered depth images, normal maps, and semantic maps obtained from

    SMPL sequences, alongside skeleton-based motion guidance, to enrich the

    conditions to the latent diffusion model with comprehensive 3D shape and

    detailed pose attributes. A multi-layer motion fusion module, integrating

    self-attention mechanisms, is employed to fuse the shape and motion latent

    representations in the spatial domain. By representing the 3D human parametric

    model as the motion guidance, we can perform parametric shape alignment of the

    human body between the reference image and the source video motion.

    Experimental evaluations conducted on benchmark datasets demonstrate the

    methodology''s superior ability to generate high-quality human animations that

    accurately capture both pose and shape variations. Furthermore, our approach

    also exhibits superior generalization capabilities on the proposed in-the-wild

    dataset. Project page: https://fudan-generative-vision.github.io/champ.'
  arxivId: '2403.14781'
  authors: Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Qingkun Su, Yinghui Xu, Xun
    Cao, Yao Yao, Hao Zhu, Siyu Zhu
  created_at: '2024-12-25T08:52:17.670574'
  issue_number: 197
  issue_url: https://github.com/dmarx/arxiv-archive/issues/197
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-23T21:24:01.786Z'
  state: open
  title: "Champ: Controllable and Consistent Human Image Animation with 3D\n  Parametric\
    \ Guidance"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2403.14781
'2405.17399':
  abstract: "The poor performance of transformers on arithmetic tasks seems to stem\
    \ in\nlarge part from their inability to keep track of the exact position of each\n\
    digit inside of a large span of digits. We mend this problem by adding an\nembedding\
    \ to each digit that encodes its position relative to the start of the\nnumber.\
    \ In addition to the boost these embeddings provide on their own, we show\nthat\
    \ this fix enables architectural modifications such as input injection and\nrecurrent\
    \ layers to improve performance even further.\n  With positions resolved, we can\
    \ study the logical extrapolation ability of\ntransformers. Can they solve arithmetic\
    \ problems that are larger and more\ncomplex than those in their training data?\
    \ We find that training on only 20\ndigit numbers with a single GPU for one day,\
    \ we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on\
    \ 100 digit addition problems.\nFinally, we show that these gains in numeracy\
    \ also unlock improvements on other\nmulti-step reasoning tasks including sorting\
    \ and multiplication."
  arxivId: '2405.17399'
  authors: Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian
    R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild,
    Tom Goldstein
  created_at: '2024-12-25T08:53:14.588756'
  issue_number: 79
  issue_url: https://github.com/dmarx/arxiv-archive/issues/79
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T22:43:44.883Z'
  state: open
  title: Transformers Can Do Arithmetic with the Right Embeddings
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.17399
'2405.20053':
  abstract: 'Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context

    learning capabilities; however, their behaviors are often difficult to control.

    By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible

    to fine-tune unsupervised LMs to follow instructions and produce outputs that

    reflect human preferences. Despite its benefits, RLHF has been shown to

    potentially harm a language model''s reasoning capabilities and introduce

    artifacts such as hallucinations where the model may fabricate facts. To

    address this issue we introduce Direct Preference Heads (DPH), a fine-tuning

    framework that enables LMs to learn human preference signals through an

    auxiliary reward head without directly affecting the output distribution of the

    language modeling head. We perform a theoretical analysis of our objective

    function and find strong ties to Conservative Direct Preference Optimization

    (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All

    evaluation suite and demonstrate that our method produces models which achieve

    higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct

    Preference Optimization (DPO) alone.'
  arxivId: '2405.20053'
  authors: Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic
  created_at: '2024-12-25T09:14:45.554978'
  issue_number: 31
  issue_url: https://github.com/dmarx/arxiv-archive/issues/31
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T17:25:17.781Z'
  state: open
  title: "Would I Lie To You? Inference Time Alignment of Language Models using\n\
    \  Direct Preference Heads"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.20053
'2406.06158':
  abstract: 'While the impressive performance of modern neural networks is often

    attributed to their capacity to efficiently extract task-relevant features from

    data, the mechanisms underlying this rich feature learning regime remain

    elusive, with much of our theoretical understanding stemming from the opposing

    lazy regime. In this work, we derive exact solutions to a minimal model that

    transitions between lazy and rich learning, precisely elucidating how

    unbalanced layer-specific initialization variances and learning rates determine

    the degree of feature learning. Our analysis reveals that they conspire to

    influence the learning regime through a set of conserved quantities that

    constrain and modify the geometry of learning trajectories in parameter and

    function space. We extend our analysis to more complex linear models with

    multiple neurons, outputs, and layers and to shallow nonlinear networks with

    piecewise linear activation functions. In linear networks, rapid feature

    learning only occurs from balanced initializations, where all layers learn at

    similar speeds. While in nonlinear networks, unbalanced initializations that

    promote faster learning in earlier layers can accelerate rich learning. Through

    a series of experiments, we provide evidence that this unbalanced rich regime

    drives feature learning in deep finite-width networks, promotes

    interpretability of early layers in CNNs, reduces the sample complexity of

    learning hierarchical data, and decreases the time to grokking in modular

    arithmetic. Our theory motivates further exploration of unbalanced

    initializations to enhance efficient feature learning.'
  arxivId: '2406.06158'
  authors: Daniel Kunin, Allan Ravent√≥s, Cl√©mentine Domin√©, Feng Chen, David Klindt,
    Andrew Saxe, Surya Ganguli
  created_at: '2024-12-25T08:53:05.600070'
  issue_number: 120
  issue_url: https://github.com/dmarx/arxiv-archive/issues/120
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:10:41.441Z'
  state: open
  title: "Get rich quick: exact solutions reveal how unbalanced initializations\n\
    \  promote rapid feature learning"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.06158
'2406.06248':
  abstract: 'Dense linear layers are the dominant computational bottleneck in foundation

    models. Identifying more efficient alternatives to dense matrices has enormous

    potential for building more compute-efficient models, as exemplified by the

    success of convolutional networks in the image domain. In this work, we

    systematically explore structured matrices as replacements for dense matrices.

    We show that different structures often require drastically different

    initialization scales and learning rates, which are crucial to performance,

    especially as models scale. Using insights from the Maximal Update

    Parameterization, we determine the optimal scaling for initialization and

    learning rates of these unconventional layers. Finally, we measure the scaling

    laws of different structures to compare how quickly their performance improves

    with compute. We propose a novel matrix family containing Monarch matrices, the

    Block Tensor-Train (BTT), which we show performs better than dense matrices for

    the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT

    achieves exponentially lower training loss than dense when training MLPs and

    ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less

    compute and is more efficient than dense for training small GPT-2 language

    models.'
  arxivId: '2406.06248'
  authors: Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon
    Wilson
  created_at: '2024-12-25T08:52:47.591714'
  issue_number: 130
  issue_url: https://github.com/dmarx/arxiv-archive/issues/130
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:34:10.041Z'
  state: open
  title: 'Compute Better Spent: Replacing Dense Layers with Structured Matrices'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.06248
'2406.19354':
  abstract: 'The model editing problem concerns how language models should learn new
    facts

    about the world over time. While empirical research on model editing has drawn

    widespread attention, the conceptual foundations of model editing remain shaky

    -- perhaps unsurprisingly, since model editing is essentially belief revision,

    a storied problem in philosophy that has eluded succinct solutions for decades.

    Model editing nonetheless demands a solution, since we need to be able to

    control the knowledge within language models. With this goal in mind, this

    paper critiques the standard formulation of the model editing problem and

    proposes a formal testbed for model editing research. We first describe 12 open

    problems with model editing, based on challenges with (1) defining the problem,

    (2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the

    first place. Many of these challenges are extremely difficult to address, e.g.

    determining far-reaching consequences of edits, labeling probabilistic

    entailments between facts, and updating beliefs of agent simulators. Next, we

    introduce a semi-synthetic dataset for model editing based on Wikidata, where

    we can evaluate edits against labels given by an idealized Bayesian agent. This

    enables us to say exactly how belief revision in language models falls short of

    a desirable epistemic standard. We encourage further research exploring

    settings where such a gold standard can be compared against. Our code is

    publicly available at: https://github.com/peterbhase/LLM-belief-revision'
  arxivId: '2406.19354'
  authors: Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal
  created_at: '2024-12-25T08:52:50.560111'
  issue_number: 129
  issue_url: https://github.com/dmarx/arxiv-archive/issues/129
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:28:19.231Z'
  state: open
  title: "Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision\
    \ Work in LLMs?"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.19354
'2407.05872':
  abstract: 'Robust and effective scaling of models from small to large width typically

    requires the precise adjustment of many algorithmic and architectural details,

    such as parameterization and optimizer choices. In this work, we propose a new

    perspective on parameterization by investigating a key assumption in prior work

    about the alignment between parameters and data and derive new theoretical

    results under weaker assumptions and a broader set of optimizers. Our extensive

    empirical investigation includes tens of thousands of models trained with all

    combinations of three optimizers, four parameterizations, several alignment

    assumptions, more than a dozen learning rates, and fourteen model sizes up to

    26.8B parameters. We find that the best learning rate scaling prescription

    would often have been excluded by the assumptions in prior work. Our results

    show that all parameterizations, not just maximal update parameterization

    (muP), can achieve hyperparameter transfer; moreover, our novel per-layer

    learning rate prescription for standard parameterization outperforms muP.

    Finally, we demonstrate that an overlooked aspect of parameterization, the

    epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow

    and propose Adam-atan2, a new numerically stable, scale-invariant version of

    Adam that eliminates the epsilon hyperparameter entirely.'
  arxivId: '2407.05872'
  authors: Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman
    Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling,
    Jaehoon Lee, Jeffrey Pennington
  created_at: '2024-12-25T08:51:47.647943'
  issue_number: 255
  issue_url: https://github.com/dmarx/arxiv-archive/issues/255
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T11:11:15.489905'
  last_visited: '2024-12-25T10:23:47.480000+00:00'
  state: open
  title: Scaling Exponents Across Parameterizations and Optimizers
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2407.05872
'2407.17465':
  abstract: 'The Maximal Update Parametrization ($\mu$P) aims to make the optimal

    hyperparameters (HPs) of a model independent of its size, allowing them to be

    swept using a cheap proxy model rather than the full-size target model. We

    present a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with

    Unit Scaling, a method for designing models that makes them easy to train in

    low-precision. The two techniques have a natural affinity: $\mu$P ensures that

    the scale of activations is independent of model size, and Unit Scaling ensures

    that activations, weights and gradients begin training with a scale of one.

    This synthesis opens the door to a simpler scheme, whose default values are

    near-optimal. This in turn facilitates a more efficient sweeping strategy, with

    u-$\mu$P models reaching a loss that is equal to or lower than comparable

    $\mu$P models and working out-of-the-box in FP8.'
  arxivId: '2407.17465'
  authors: Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y.
    Prince, Bj√∂rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach,
    Douglas Orr
  created_at: '2024-12-25T08:53:20.576075'
  issue_number: 72
  issue_url: https://github.com/dmarx/arxiv-archive/issues/72
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T18:35:47.488Z'
  state: open
  title: 'u-$Œº$P: The Unit-Scaled Maximal Update Parametrization'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2407.17465
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is

    a critical step towards building generally self-improving agents that can

    operate on open-ended natural language. In this paper, we study the scaling of

    inference-time computation in LLMs, with a focus on answering the question: if

    an LLM is allowed to use a fixed but non-trivial amount of inference-time

    compute, how much can it improve its performance on a challenging prompt?

    Answering this question has implications not only on the achievable performance

    of LLMs, but also on the future of LLM pretraining and how one should tradeoff

    inference-time and pre-training compute. Despite its importance, little

    research attempted to understand the scaling behaviors of various test-time

    inference methods. Moreover, current work largely provides negative results for

    a number of these strategies. In this work, we analyze two primary mechanisms

    to scale test-time computation: (1) searching against dense, process-based

    verifier reward models; and (2) updating the model''s distribution over a

    response adaptively, given the prompt at test time. We find that in both cases,

    the effectiveness of different approaches to scaling test-time compute

    critically varies depending on the difficulty of the prompt. This observation

    motivates applying a "compute-optimal" scaling strategy, which acts to most

    effectively allocate test-time compute adaptively per prompt. Using this

    compute-optimal strategy, we can improve the efficiency of test-time compute

    scaling by more than 4x compared to a best-of-N baseline. Additionally, in a

    FLOPs-matched evaluation, we find that on problems where a smaller base model

    attains somewhat non-trivial success rates, test-time compute can be used to

    outperform a 14x larger model.'
  arxivId: '2408.03314'
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-25T09:14:41.873414'
  issue_number: 57
  issue_url: https://github.com/dmarx/arxiv-archive/issues/57
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-17T13:48:02.489Z'
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2408.03314
'2409.09347':
  abstract: 'Mass transport problems arise in many areas of machine learning whereby
    one

    wants to compute a map transporting one distribution to another. Generative

    modeling techniques like Generative Adversarial Networks (GANs) and Denoising

    Diffusion Models (DDMs) have been successfully adapted to solve such transport

    problems, resulting in CycleGAN and Bridge Matching respectively. However,

    these methods do not approximate Optimal Transport (OT) maps, which are known

    to have desirable properties. Existing techniques approximating OT maps for

    high-dimensional data-rich problems, such as DDM-based Rectified Flow and

    Schr\"odinger Bridge procedures, require fully training a DDM-type model at

    each iteration, or use mini-batch techniques which can introduce significant

    errors. We propose a novel algorithm to compute the Schr\"odinger Bridge, a

    dynamic entropy-regularised version of OT, that eliminates the need to train

    multiple DDM-like models. This algorithm corresponds to a discretisation of a

    flow of path measures, which we call the Schr\"odinger Bridge Flow, whose only

    stationary point is the Schr\"odinger Bridge. We demonstrate the performance of

    our algorithm on a variety of unpaired data translation tasks.'
  arxivId: '2409.09347'
  authors: Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, Arnaud Doucet
  created_at: '2024-12-25T08:52:38.557977'
  issue_number: 136
  issue_url: https://github.com/dmarx/arxiv-archive/issues/136
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T08:05:51.003Z'
  state: open
  title: Schr√∂dinger Bridge Flow for Unpaired Data Translation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.09347
'2410.01131':
  abstract: 'We propose a novel neural network architecture, the normalized Transformer

    (nGPT) with representation learning on the hypersphere. In nGPT, all vectors

    forming the embeddings, MLP, attention matrices and hidden states are unit norm

    normalized. The input stream of tokens travels on the surface of a hypersphere,

    with each layer contributing a displacement towards the target output

    predictions. These displacements are defined by the MLP and attention blocks,

    whose vector components also reside on the same hypersphere. Experiments show

    that nGPT learns much faster, reducing the number of training steps required to

    achieve the same accuracy by a factor of 4 to 20, depending on the sequence

    length.'
  arxivId: '2410.01131'
  authors: Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg
  created_at: '2024-12-25T09:14:44.412538'
  issue_number: 270
  issue_url: https://github.com/dmarx/arxiv-archive/issues/270
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T22:12:38.241640'
  last_visited: '2024-12-25T21:21:58.886000+00:00'
  state: open
  title: "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere"
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2410.01131
'2410.02117':
  abstract: 'Dense linear layers are the dominant computational bottleneck in large
    neural

    networks, presenting a critical need for more efficient alternatives. Previous

    efforts focused on a small number of hand-crafted structured matrices and

    neglected to investigate whether these structures can surpass dense layers in

    terms of compute-optimal scaling laws when both the model size and training

    examples are optimally allocated. In this work, we present a unifying framework

    that enables searching among all linear operators expressible via an Einstein

    summation. This framework encompasses many previously proposed structures, such

    as low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,

    along with many novel structures. To analyze the framework, we develop a

    taxonomy of all such operators based on their computational and algebraic

    properties and show that differences in the compute-optimal scaling laws are

    mostly governed by a small number of variables that we introduce. Namely, a

    small $\omega$ (which measures parameter sharing) and large $\psi$ (which

    measures the rank) reliably led to better scaling laws. Guided by the insight

    that full-rank structures that maximize parameters per unit of compute perform

    the best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture

    obtained by sparsifying computation in the BTT structure. In contrast to the

    standard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE

    in every single linear layer of the model, including the projection matrices in

    the attention blocks. We find BTT-MoE provides a substantial compute-efficiency

    gain over dense layers and standard MoE.'
  arxivId: '2410.02117'
  authors: Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen,
    Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson
  created_at: '2024-12-25T08:52:44.638064'
  issue_number: 131
  issue_url: https://github.com/dmarx/arxiv-archive/issues/131
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:34:12.263Z'
  state: open
  title: "Searching for Efficient Linear Layers over a Continuous Space of\n  Structured\
    \ Matrices"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.02117
'2410.11900':
  abstract: 'Modern Question Answering (QA) and Reasoning approaches based on Large

    Language Models (LLMs) commonly use prompting techniques, such as

    Chain-of-Thought (CoT), assuming the resulting generation will have a more

    granular exploration and reasoning over the question space and scope. However,

    such methods struggle with generating outputs that are faithful to the

    intermediate chain of reasoning produced by the model. On the other end of the

    spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to

    combine LLMs with external symbolic solvers. While such approaches boast a high

    degree of faithfulness, they usually require a model trained for code

    generation and struggle with tasks that are ambiguous or hard to formalise

    strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided

    $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel

    interpretable approach for traversing the problem space using task

    decompositions. We use the LLM to plan a solution, soft-formalise the query

    into facts and predicates using a logic programming code and simulate that code

    execution using an exhaustive multi-hop search over the defined space. Our

    method allows us to compute the faithfulness of the reasoning process w.r.t.

    the generated code and analyse the steps of the multi-hop search without

    relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$

    out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model

    faithfulness positively correlates with overall performance and further

    demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors

    sufficient for and leading to the correct answer with optimal reasoning during

    the multi-hop search.'
  arxivId: '2410.11900'
  authors: Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle
    Augenstein
  created_at: '2024-12-25T08:53:02.582125'
  issue_number: 121
  issue_url: https://github.com/dmarx/arxiv-archive/issues/121
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:17:19.744Z'
  state: open
  title: 'FLARE: Faithful Logic-Aided Reasoning and Exploration'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.11900
'2410.15815':
  abstract: 'We present a method for computing free-energy differences using thermodynamic

    integration with a neural network potential that interpolates between two

    target Hamiltonians. The interpolation is defined at the sample distribution

    level, and the neural network potential is optimized to match the corresponding

    equilibrium potential at every intermediate time-step. Once the interpolating

    potentials and samples are well-aligned, the free-energy difference can be

    estimated using (neural) thermodynamic integration. To target molecular

    systems, we simultaneously couple Lennard-Jones and electrostatic interactions

    and model the rigid-body rotation of molecules. We report accurate results for

    several benchmark systems: a Lennard-Jones particle in a Lennard-Jones fluid,

    as well as the insertion of both water and methane solutes in a water solvent

    at atomistic resolution using a simple three-body neural-network potential.'
  arxivId: '2410.15815'
  authors: B√°lint M√°t√©, Fran√ßois Fleuret, Tristan Bereau
  created_at: '2024-12-25T08:52:59.568043'
  issue_number: 123
  issue_url: https://github.com/dmarx/arxiv-archive/issues/123
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:19:55.200Z'
  state: open
  title: Solvation Free Energies from Neural Thermodynamic Integration
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.15815
'2412.05265':
  abstract: 'This manuscript gives a big-picture, up-to-date overview of the field
    of

    (deep) reinforcement learning and sequential decision making, covering

    value-based RL, policy-gradient methods, model-based methods, and various other

    topics (including a very brief discussion of RL+LLMs).'
  arxivId: '2412.05265'
  authors: Kevin Murphy
  created_at: '2024-12-25T08:52:56.565714'
  issue_number: 125
  issue_url: https://github.com/dmarx/arxiv-archive/issues/125
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:23:45.214Z'
  state: open
  title: 'Reinforcement Learning: An Overview'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.05265
'2412.09349':
  abstract: 'Controllable human image animation aims to generate videos from reference

    images using driving videos. Due to the limited control signals provided by

    sparse guidance (e.g., skeleton pose), recent works have attempted to introduce

    additional dense conditions (e.g., depth map) to ensure motion alignment.

    However, such strict dense guidance impairs the quality of the generated video

    when the body shape of the reference character differs significantly from that

    of the driving video. In this paper, we present DisPose to mine more

    generalizable and effective control signals without additional dense input,

    which disentangles the sparse skeleton pose in human image animation into

    motion field guidance and keypoint correspondence. Specifically, we generate a

    dense motion field from a sparse motion field and the reference image, which

    provides region-level dense guidance while maintaining the generalization of

    the sparse pose control. We also extract diffusion features corresponding to

    pose keypoints from the reference image, and then these point features are

    transferred to the target pose to provide distinct identity information. To

    seamlessly integrate into existing models, we propose a plug-and-play hybrid

    ControlNet that improves the quality and consistency of generated videos while

    freezing the existing model parameters. Extensive qualitative and quantitative

    experiments demonstrate the superiority of DisPose compared to current methods.

    Code:

    \href{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.'
  arxivId: '2412.09349'
  authors: Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng,
    Long Chen
  created_at: '2024-12-25T08:52:20.571004'
  issue_number: 191
  issue_url: https://github.com/dmarx/arxiv-archive/issues/191
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:52:20.575599'
  last_visited: '2024-12-23T21:16:01.436Z'
  state: open
  title: "DisPose: Disentangling Pose Guidance for Controllable Human Image\n  Animation"
  total_reading_time_seconds: 350
  url: https://arxiv.org/abs/2412.09349
'2412.11965':
  abstract: 'Attention heads are one of the building blocks of large language models

    (LLMs). Prior work on investigating their operation mostly focused on analyzing

    their behavior during inference for specific circuits or tasks. In this work,

    we seek a comprehensive mapping of the operations they implement in a model. We

    propose MAPS (Mapping Attention head ParameterS), an efficient framework that

    infers the functionality of attention heads from their parameters, without any

    model training or inference. We showcase the utility of MAPS for answering two

    types of questions: (a) given a predefined operation, mapping how strongly

    heads across the model implement it, and (b) given an attention head, inferring

    its salient functionality. Evaluating MAPS on 20 operations across 6 popular

    LLMs shows its estimations correlate with the head''s outputs during inference

    and are causally linked to the model''s predictions. Moreover, its mappings

    reveal attention heads of certain operations that were overlooked in previous

    studies, and valuable insights on function universality and architecture biases

    in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS

    to characterize the salient operations of a given head. Our pipeline produces

    plausible operation descriptions for most heads, as assessed by human judgment,

    while revealing diverse operations.'
  arxivId: '2412.11965'
  authors: Amit Elhelo, Mor Geva
  created_at: '2024-12-25T08:52:26.559501'
  issue_number: 144
  issue_url: https://github.com/dmarx/arxiv-archive/issues/144
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T10:10:33.575357'
  last_visited: '2024-12-22T08:37:29.114000+00:00'
  state: open
  title: Inferring Functionality of Attention Heads from their Parameters
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/2412.11965
'2412.17799':
  abstract: 'With the recent Nobel Prize awarded for radical advances in protein

    discovery, foundation models (FMs) for exploring large combinatorial spaces

    promise to revolutionize many scientific fields. Artificial Life (ALife) has

    not yet integrated FMs, thus presenting a major opportunity for the field to

    alleviate the historical burden of relying chiefly on manual design and

    trial-and-error to discover the configurations of lifelike simulations. This

    paper presents, for the first time, a successful realization of this

    opportunity using vision-language FMs. The proposed approach, called Automated

    Search for Artificial Life (ASAL), (1) finds simulations that produce target

    phenomena, (2) discovers simulations that generate temporally open-ended

    novelty, and (3) illuminates an entire space of interestingly diverse

    simulations. Because of the generality of FMs, ASAL works effectively across a

    diverse range of ALife substrates including Boids, Particle Life, Game of Life,

    Lenia, and Neural Cellular Automata. A major result highlighting the potential

    of this technique is the discovery of previously unseen Lenia and Boids

    lifeforms, as well as cellular automata that are open-ended like Conway''s Game

    of Life. Additionally, the use of FMs allows for the quantification of

    previously qualitative phenomena in a human-aligned way. This new paradigm

    promises to accelerate ALife research beyond what is possible through human

    ingenuity alone.'
  arxivId: '2412.17799'
  authors: Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip
    Isola, David Ha
  created_at: '2024-12-25T08:51:53.638425'
  issue_number: 247
  issue_url: https://github.com/dmarx/arxiv-archive/issues/247
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-25T08:51:53.641477'
  last_visited: '2024-12-24T04:41:56.160Z'
  state: open
  title: Automating the Search for Artificial Life with Foundation Models
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2412.17799
