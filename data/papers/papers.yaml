'2105.05720':
  abstract: "Recent trend towards increasing large machine learning models require\
    \ both\ntraining and inference tasks to be distributed. Considering the huge cost\
    \ of\ntraining these models, it is imperative to unlock optimizations in computation\n\
    and communication to obtain best performance. However, current logical\nseparation\
    \ between computation and communication kernels in deep learning\nframeworks misses\
    \ the optimization opportunities across such barrier. Breaking\nthis abstraction\
    \ with a holistic consideration can provide many optimizations\nto provide performance\
    \ improvements in distributed workloads. Manually applying\nthese optimizations\
    \ needs modifications in underlying computation and\ncommunication libraries for\
    \ each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present\
    \ CoCoNeT, with a DSL to express a program with both\ncomputation and communication.\
    \ CoCoNeT contains several machine learning aware\ntransformations to optimize\
    \ a program and a compiler to generate high\nperformance kernels. Providing both\
    \ computation and communication as first\nclass constructs allows users to work\
    \ on a high-level abstraction and apply\npowerful optimizations, such as fusion\
    \ or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize\
    \ data-, model-and pipeline-parallel\nworkloads in large language models with\
    \ only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms\
    \ state-of-the-art distributed machine\nlearning implementations."
  arxivId: '2105.05720'
  arxiv_tags:
  - cs.DC
  - cs.LG
  - cs.PL
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2024-12-30T07:58:32.819784'
  issue_number: 581
  issue_url: https://github.com/dmarx/papers-feed/issues/581
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T07:59:22.536350'
  last_visited: '2024-12-30T07:59:15.568000+00:00'
  main_tex_file: null
  published_date: '2021-05-12T15:13:43Z'
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_seconds: 38
  url: https://arxiv.org/abs/2105.05720
