'1503.03585':
  abstract: 'A central problem in machine learning involves modeling complex data-sets

    using highly flexible families of probability distributions in which learning,

    sampling, inference, and evaluation are still analytically or computationally

    tractable. Here, we develop an approach that simultaneously achieves both

    flexibility and tractability. The essential idea, inspired by non-equilibrium

    statistical physics, is to systematically and slowly destroy structure in a

    data distribution through an iterative forward diffusion process. We then learn

    a reverse diffusion process that restores structure in data, yielding a highly

    flexible and tractable generative model of the data. This approach allows us to

    rapidly learn, sample from, and evaluate probabilities in deep generative

    models with thousands of layers or time steps, as well as to compute

    conditional and posterior probabilities under the learned model. We

    additionally release an open source reference implementation of the algorithm.'
  arxivId: '1503.03585'
  authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
  created_at: '2024-12-24T19:26:21.748418'
  issue_number: 118
  issue_url: https://github.com/dmarx/arxiv-archive/issues/118
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-24T19:26:21.751933'
  state: open
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  total_reading_time_seconds: 180
  url: https://arxiv.org/abs/1503.03585
'1705.08926':
  abstract: 'Cooperative multi-agent systems can be naturally used to model many real

    world problems, such as network packet routing and the coordination of

    autonomous vehicles. There is a great need for new reinforcement learning

    methods that can efficiently learn decentralised policies for such systems. To

    this end, we propose a new multi-agent actor-critic method called

    counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised

    critic to estimate the Q-function and decentralised actors to optimise the

    agents'' policies. In addition, to address the challenges of multi-agent credit

    assignment, it uses a counterfactual baseline that marginalises out a single

    agent''s action, while keeping the other agents'' actions fixed. COMA also uses
    a

    critic representation that allows the counterfactual baseline to be computed

    efficiently in a single forward pass. We evaluate COMA in the testbed of

    StarCraft unit micromanagement, using a decentralised variant with significant

    partial observability. COMA significantly improves average performance over

    other multi-agent actor-critic methods in this setting, and the best performing

    agents are competitive with state-of-the-art centralised controllers that get

    access to the full state.'
  arxivId: '1705.08926'
  authors: Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli,
    Shimon Whiteson
  created_at: '2024-12-24T19:26:51.733189'
  issue_number: 127
  issue_url: https://github.com/dmarx/arxiv-archive/issues/127
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Counterfactual Multi-Agent Policy Gradients
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1705.08926
'1710.09412':
  abstract: 'Large deep neural networks are powerful, but exhibit undesirable behaviors

    such as memorization and sensitivity to adversarial examples. In this work, we

    propose mixup, a simple learning principle to alleviate these issues. In

    essence, mixup trains a neural network on convex combinations of pairs of

    examples and their labels. By doing so, mixup regularizes the neural network to

    favor simple linear behavior in-between training examples. Our experiments on

    the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show

    that mixup improves the generalization of state-of-the-art neural network

    architectures. We also find that mixup reduces the memorization of corrupt

    labels, increases the robustness to adversarial examples, and stabilizes the

    training of generative adversarial networks.'
  arxivId: '1710.09412'
  authors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz
  created_at: '2024-12-24T19:26:33.743045'
  issue_number: 163
  issue_url: https://github.com/dmarx/arxiv-archive/issues/163
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'mixup: Beyond Empirical Risk Minimization'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1710.09412
'1711.11586':
  abstract: 'Many image-to-image translation problems are ambiguous, as a single input

    image may correspond to multiple possible outputs. In this work, we aim to

    model a \emph{distribution} of possible outputs in a conditional generative

    modeling setting. The ambiguity of the mapping is distilled in a

    low-dimensional latent vector, which can be randomly sampled at test time. A

    generator learns to map the given input, combined with this latent code, to the

    output. We explicitly encourage the connection between output and the latent

    code to be invertible. This helps prevent a many-to-one mapping from the latent

    code to the output during training, also known as the problem of mode collapse,

    and produces more diverse results. We explore several variants of this approach

    by employing different training objectives, network architectures, and methods

    of injecting the latent code. Our proposed method encourages bijective

    consistency between the latent encoding and output modes. We present a

    systematic comparison of our method and other variants on both perceptual

    realism and diversity.'
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-24T19:27:39.765702'
  issue_number: 56
  issue_url: https://github.com/dmarx/arxiv-archive/issues/56
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1711.11586
'1810.01588':
  abstract: 'Interpreting the prediction mechanism of complex models is currently
    one of

    the most important tasks in the machine learning field, especially with layered

    neural networks, which have achieved high predictive performance with various

    practical data sets. To reveal the global structure of a trained neural network

    in an interpretable way, a series of clustering methods have been proposed,

    which decompose the units into clusters according to the similarity of their

    inference roles. The main problems in these studies were that (1) we have no

    prior knowledge about the optimal resolution for the decomposition, or the

    appropriate number of clusters, and (2) there was no method with which to

    acquire knowledge about whether the outputs of each cluster have a positive or

    negative correlation with the input and output dimension values. In this paper,

    to solve these problems, we propose a method for obtaining a hierarchical

    modular representation of a layered neural network. The application of a

    hierarchical clustering method to a trained network reveals a tree-structured

    relationship among hidden layer units, based on their feature vectors defined

    by their correlation with the input and output dimension values.'
  arxivId: '1810.01588'
  authors: Chihiro Watanabe
  created_at: '2024-12-24T19:26:09.743541'
  issue_number: 230
  issue_url: https://github.com/dmarx/arxiv-archive/issues/230
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-24T19:26:09.745189'
  state: open
  title: "Interpreting Layered Neural Networks via Hierarchical Modular\n  Representation"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/1810.01588
'1901.10159':
  abstract: 'To understand the dynamics of optimization in deep neural networks, we

    develop a tool to study the evolution of the entire Hessian spectrum throughout

    the optimization process. Using this, we study a number of hypotheses

    concerning smoothness, curvature, and sharpness in the deep learning

    literature. We then thoroughly analyze a crucial structural feature of the

    spectra: in non-batch normalized networks, we observe the rapid appearance of

    large isolated eigenvalues in the spectrum, along with a surprising

    concentration of the gradient in the corresponding eigenspaces. In batch

    normalized networks, these two effects are almost absent. We characterize these

    effects, and explain how they affect optimization speed through both theory and

    experiments. As part of this work, we adapt advanced tools from numerical

    linear algebra that allow scalable and accurate estimation of the entire

    Hessian spectrum of ImageNet-scale neural networks; this technique may be of

    independent interest in other applications.'
  arxivId: '1901.10159'
  authors: Behrooz Ghorbani, Shankar Krishnan, Ying Xiao
  created_at: '2024-12-24T19:26:03.829269'
  issue_number: 239
  issue_url: https://github.com/dmarx/arxiv-archive/issues/239
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "An Investigation into Neural Net Optimization via Hessian Eigenvalue\n \
    \ Density"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1901.10159
'1904.08779':
  abstract: 'We present SpecAugment, a simple data augmentation method for speech

    recognition. SpecAugment is applied directly to the feature inputs of a neural

    network (i.e., filter bank coefficients). The augmentation policy consists of

    warping the features, masking blocks of frequency channels, and masking blocks

    of time steps. We apply SpecAugment on Listen, Attend and Spell networks for

    end-to-end speech recognition tasks. We achieve state-of-the-art performance on

    the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.

    On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language

    model, and 5.8% WER with shallow fusion with a language model. This compares to

    the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we

    achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5''00 test set

    without the use of a language model, and 6.8%/14.1% with shallow fusion, which

    compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.'
  arxivId: '1904.08779'
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2024-12-24T19:27:33.740589'
  issue_number: 60
  issue_url: https://github.com/dmarx/arxiv-archive/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1904.08779
'1906.04358':
  abstract: 'Not all neural network architectures are created equal, some perform
    much

    better than others for certain tasks. But how important are the weight

    parameters of a neural network compared to its architecture? In this work, we

    question to what extent neural network architectures alone, without learning

    any weight parameters, can encode solutions for a given task. We propose a

    search method for neural network architectures that can already perform a task

    without any explicit weight training. To evaluate these networks, we populate

    the connections with a single shared weight parameter sampled from a uniform

    random distribution, and measure the expected performance. We demonstrate that

    our method can find minimal neural network architectures that can perform

    several reinforcement learning tasks without weight training. On a supervised

    learning domain, we find network architectures that achieve much higher than

    chance accuracy on MNIST using random weights. Interactive version of this

    paper at https://weightagnostic.github.io/'
  arxivId: '1906.04358'
  authors: Adam Gaier, David Ha
  created_at: '2024-12-24T19:26:06.835408'
  issue_number: 235
  issue_url: https://github.com/dmarx/arxiv-archive/issues/235
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-24T19:26:06.837230'
  state: open
  title: Weight Agnostic Neural Networks
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1906.04358
'1906.05433':
  abstract: 'Climate change is one of the greatest challenges facing humanity, and
    we, as

    machine learning experts, may wonder how we can help. Here we describe how

    machine learning can be a powerful tool in reducing greenhouse gas emissions

    and helping society adapt to a changing climate. From smart grids to disaster

    management, we identify high impact problems where existing gaps can be filled

    by machine learning, in collaboration with other fields. Our recommendations

    encompass exciting research questions as well as promising business

    opportunities. We call on the machine learning community to join the global

    effort against climate change.'
  arxivId: '1906.05433'
  authors: David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre
    Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques,
    Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik
    Mukkavilli, Konrad P. Kording, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John
    C. Platt, Felix Creutzig, Jennifer Chayes, Yoshua Bengio
  created_at: '2024-12-24T19:27:15.806507'
  issue_number: 111
  issue_url: https://github.com/dmarx/arxiv-archive/issues/111
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Tackling Climate Change with Machine Learning
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1906.05433
'1912.02757':
  abstract: 'Deep ensembles have been empirically shown to be a promising approach
    for

    improving accuracy, uncertainty and out-of-distribution robustness of deep

    learning models. While deep ensembles were theoretically motivated by the

    bootstrap, non-bootstrap ensembles trained with just random initialization also

    perform well in practice, which suggests that there could be other explanations

    for why deep ensembles work well. Bayesian neural networks, which learn

    distributions over the parameters of the network, are theoretically

    well-motivated by Bayesian principles, but do not perform as well as deep

    ensembles in practice, particularly under dataset shift. One possible

    explanation for this gap between theory and practice is that popular scalable

    variational Bayesian methods tend to focus on a single mode, whereas deep

    ensembles tend to explore diverse modes in function space. We investigate this

    hypothesis by building on recent work on understanding the loss landscape of

    neural networks and adding our own exploration to measure the similarity of

    functions in the space of predictions. Our results show that random

    initializations explore entirely different modes, while functions along an

    optimization trajectory or sampled from the subspace thereof cluster within a

    single mode predictions-wise, while often deviating significantly in the weight

    space. Developing the concept of the diversity--accuracy plane, we show that

    the decorrelation power of random initializations is unmatched by popular

    subspace sampling methods. Finally, we evaluate the relative effects of

    ensembling, subspace based methods and ensembles of subspace based methods, and

    the experimental results validate our hypothesis.'
  arxivId: '1912.02757'
  authors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan
  created_at: '2024-12-24T19:26:15.744003'
  issue_number: 224
  issue_url: https://github.com/dmarx/arxiv-archive/issues/224
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-24T19:26:15.745214'
  state: open
  title: 'Deep Ensembles: A Loss Landscape Perspective'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/1912.02757
'2010.15110':
  abstract: 'In suitably initialized wide networks, small learning rates transform
    deep

    neural networks (DNNs) into neural tangent kernel (NTK) machines, whose

    training dynamics is well-approximated by a linear weight expansion of the

    network at initialization. Standard training, however, diverges from its

    linearization in ways that are poorly understood. We study the relationship

    between the training dynamics of nonlinear deep networks, the geometry of the

    loss landscape, and the time evolution of a data-dependent NTK. We do so

    through a large-scale phenomenological analysis of training, synthesizing

    diverse measures characterizing loss landscape geometry and NTK dynamics. In

    multiple neural architectures and datasets, we find these diverse measures

    evolve in a highly correlated manner, revealing a universal picture of the deep

    learning process. In this picture, deep network training exhibits a highly

    chaotic rapid initial transient that within 2 to 3 epochs determines the final

    linearly connected basin of low loss containing the end point of training.

    During this chaotic transient, the NTK changes rapidly, learning useful

    features from the training data that enables it to outperform the standard

    initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid

    chaotic transient, the NTK changes at constant velocity, and its performance

    matches that of full network training in 15% to 45% of training time. Overall,

    our analysis reveals a striking correlation between a diverse set of metrics

    over training time, governed by a rapid chaotic to stable transition in the

    first few epochs, that together poses challenges and opportunities for the

    development of more accurate theories of deep learning.'
  arxivId: '2010.15110'
  authors: Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
    Daniel M. Roy, Surya Ganguli
  created_at: '2024-12-24T19:26:00.807486'
  issue_number: 245
  issue_url: https://github.com/dmarx/arxiv-archive/issues/245
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Deep learning versus kernel learning: an empirical study of loss\n  landscape\
    \ geometry and the time evolution of the Neural Tangent Kernel"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2010.15110
'2203.15556':
  abstract: 'We investigate the optimal model size and number of tokens for training
    a

    transformer language model under a given compute budget. We find that current

    large language models are significantly undertrained, a consequence of the

    recent focus on scaling language models whilst keeping the amount of training

    data constant. By training over 400 language models ranging from 70 million to

    over 16 billion parameters on 5 to 500 billion tokens, we find that for

    compute-optimal training, the model size and the number of training tokens

    should be scaled equally: for every doubling of model size the number of

    training tokens should also be doubled. We test this hypothesis by training a

    predicted compute-optimal model, Chinchilla, that uses the same compute budget

    as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla

    uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1

    (178B), and Megatron-Turing NLG (530B) on a large range of downstream

    evaluation tasks. This also means that Chinchilla uses substantially less

    compute for fine-tuning and inference, greatly facilitating downstream usage.

    As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%

    on the MMLU benchmark, greater than a 7% improvement over Gopher.'
  arxivId: '2203.15556'
  authors: Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den
    Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen,
    Jack W. Rae, Oriol Vinyals, Laurent Sifre
  created_at: '2024-12-24T19:27:30.826587'
  issue_number: 67
  issue_url: https://github.com/dmarx/arxiv-archive/issues/67
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Training Compute-Optimal Large Language Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2203.15556
'2302.11529':
  abstract: 'Transfer learning has recently become the dominant paradigm of machine

    learning. Pre-trained models fine-tuned for downstream tasks achieve better

    performance with fewer labelled examples. Nonetheless, it remains unclear how

    to develop models that specialise towards multiple tasks without incurring

    negative interference and that generalise systematically to non-identically

    distributed tasks. Modular deep learning has emerged as a promising solution to

    these challenges. In this framework, units of computation are often implemented

    as autonomous parameter-efficient modules. Information is conditionally routed

    to a subset of modules and subsequently aggregated. These properties enable

    positive transfer and systematic generalisation by separating computation from

    routing and updating modules locally. We offer a survey of modular

    architectures, providing a unified view over several threads of research that

    evolved independently in the scientific literature. Moreover, we explore

    various additional purposes of modularity, including scaling language models,

    causal inference, programme induction, and planning in reinforcement learning.

    Finally, we report various concrete applications where modularity has been

    successfully deployed such as cross-lingual and cross-modal knowledge transfer.

    Related talks and projects to this survey, are available at

    https://www.modulardeeplearning.com/.'
  arxivId: '2302.11529'
  authors: Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, Edoardo Maria Ponti
  created_at: '2024-12-24T19:26:12.831797'
  issue_number: 227
  issue_url: https://github.com/dmarx/arxiv-archive/issues/227
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-24T19:26:12.832870'
  state: open
  title: Modular Deep Learning
  total_reading_time_seconds: 22
  url: https://arxiv.org/abs/2302.11529
'2309.03060':
  abstract: 'Many areas of machine learning and science involve large linear algebra

    problems, such as eigendecompositions, solving linear systems, computing matrix

    exponentials, and trace estimation. The matrices involved often have Kronecker,

    convolutional, block diagonal, sum, or product structure. In this paper, we

    propose a simple but general framework for large-scale linear algebra problems

    in machine learning, named CoLA (Compositional Linear Algebra). By combining a

    linear operator abstraction with compositional dispatch rules, CoLA

    automatically constructs memory and runtime efficient numerical algorithms.

    Moreover, CoLA provides memory efficient automatic differentiation, low

    precision computation, and GPU acceleration in both JAX and PyTorch, while also

    accommodating new objects, operations, and rules in downstream packages via

    multiple dispatch. CoLA can accelerate many algebraic operations, while making

    it easy to prototype matrix structures and algorithms, providing an appealing

    drop-in tool for virtually any computational effort that requires linear

    algebra. We showcase its efficacy across a broad range of applications,

    including partial differential equations, Gaussian processes, equivariant model

    construction, and unsupervised learning.'
  arxivId: '2309.03060'
  authors: Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson
  created_at: '2024-12-24T19:26:39.759422'
  issue_number: 132
  issue_url: https://github.com/dmarx/arxiv-archive/issues/132
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "CoLA: Exploiting Compositional Structure for Automatic and Efficient\n \
    \ Numerical Linear Algebra"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2309.03060
'2401.17671':
  abstract: 'Recent advancements in artificial intelligence have sparked interest
    in the

    parallels between large language models (LLMs) and human neural processing,

    particularly in language comprehension. While prior research has established

    similarities in the representation of LLMs and the brain, the underlying

    computational principles that cause this convergence, especially in the context

    of evolving LLMs, remain elusive. Here, we examined a diverse selection of

    high-performance LLMs with similar parameter sizes to investigate the factors

    contributing to their alignment with the brain''s language processing

    mechanisms. We find that as LLMs achieve higher performance on benchmark tasks,

    they not only become more brain-like as measured by higher performance when

    predicting neural responses from LLM embeddings, but also their hierarchical

    feature extraction pathways map more closely onto the brain''s while using fewer

    layers to do the same encoding. We also compare the feature extraction pathways

    of the LLMs to each other and identify new ways in which high-performing models

    have converged toward similar hierarchical processing mechanisms. Finally, we

    show the importance of contextual information in improving model performance

    and brain similarity. Our findings reveal the converging aspects of language

    processing in the brain and LLMs and offer new directions for developing models

    that align more closely with human cognitive processing.'
  arxivId: '2401.17671'
  authors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima
    Mesgarani
  created_at: '2024-12-24T19:27:27.764523'
  issue_number: 70
  issue_url: https://github.com/dmarx/arxiv-archive/issues/70
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Contextual Feature Extraction Hierarchies Converge in Large Language\n \
    \ Models and the Brain"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2401.17671
'2405.17472':
  abstract: 'Text-to-image diffusion models can be fine-tuned in custom domains to
    adapt

    to specific user preferences, but such adaptability has also been utilized for

    illegal purposes, such as forging public figures'' portraits, duplicating

    copyrighted artworks and generating explicit contents. Existing work focused on

    detecting the illegally generated contents, but cannot prevent or mitigate

    illegal adaptations of diffusion models. Other schemes of model unlearning and

    reinitialization, similarly, cannot prevent users from relearning the knowledge

    of illegal model adaptation with custom data. In this paper, we present

    FreezeAsGuard, a new technique that addresses these limitations and enables

    irreversible mitigation of illegal adaptations of diffusion models. Our

    approach is that the model publisher selectively freezes tensors in pre-trained

    diffusion models that are critical to illegal model adaptations, to mitigate

    the fine-tuned model''s representation power in illegal adaptations, but

    minimize the impact on other legal adaptations. Experiment results in multiple

    text-to-image application domains show that FreezeAsGuard provides 37% stronger

    power in mitigating illegal model adaptations compared to competitive

    baselines, while incurring less than 5% impact on legal model adaptations. The

    source code is available at: https://github.com/pittisl/FreezeAsGuard.'
  arxivId: '2405.17472'
  authors: Kai Huang, Haoming Wang, Wei Gao
  created_at: '2024-12-24T19:26:27.743237'
  issue_number: 167
  issue_url: https://github.com/dmarx/arxiv-archive/issues/167
  labels:
  - paper
  - rating:downvote
  last_read: null
  state: open
  title: "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via\n \
    \ Selective Tensor Freezing"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.17472
'2406.06158':
  abstract: 'While the impressive performance of modern neural networks is often

    attributed to their capacity to efficiently extract task-relevant features from

    data, the mechanisms underlying this rich feature learning regime remain

    elusive, with much of our theoretical understanding stemming from the opposing

    lazy regime. In this work, we derive exact solutions to a minimal model that

    transitions between lazy and rich learning, precisely elucidating how

    unbalanced layer-specific initialization variances and learning rates determine

    the degree of feature learning. Our analysis reveals that they conspire to

    influence the learning regime through a set of conserved quantities that

    constrain and modify the geometry of learning trajectories in parameter and

    function space. We extend our analysis to more complex linear models with

    multiple neurons, outputs, and layers and to shallow nonlinear networks with

    piecewise linear activation functions. In linear networks, rapid feature

    learning only occurs from balanced initializations, where all layers learn at

    similar speeds. While in nonlinear networks, unbalanced initializations that

    promote faster learning in earlier layers can accelerate rich learning. Through

    a series of experiments, we provide evidence that this unbalanced rich regime

    drives feature learning in deep finite-width networks, promotes

    interpretability of early layers in CNNs, reduces the sample complexity of

    learning hierarchical data, and decreases the time to grokking in modular

    arithmetic. Our theory motivates further exploration of unbalanced

    initializations to enhance efficient feature learning.'
  arxivId: '2406.06158'
  authors: Daniel Kunin, Allan Raventós, Clémentine Dominé, Feng Chen, David Klindt,
    Andrew Saxe, Surya Ganguli
  created_at: '2024-12-24T19:27:03.818540'
  issue_number: 120
  issue_url: https://github.com/dmarx/arxiv-archive/issues/120
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Get rich quick: exact solutions reveal how unbalanced initializations\n\
    \  promote rapid feature learning"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.06158
'2406.06248':
  abstract: 'Dense linear layers are the dominant computational bottleneck in foundation

    models. Identifying more efficient alternatives to dense matrices has enormous

    potential for building more compute-efficient models, as exemplified by the

    success of convolutional networks in the image domain. In this work, we

    systematically explore structured matrices as replacements for dense matrices.

    We show that different structures often require drastically different

    initialization scales and learning rates, which are crucial to performance,

    especially as models scale. Using insights from the Maximal Update

    Parameterization, we determine the optimal scaling for initialization and

    learning rates of these unconventional layers. Finally, we measure the scaling

    laws of different structures to compare how quickly their performance improves

    with compute. We propose a novel matrix family containing Monarch matrices, the

    Block Tensor-Train (BTT), which we show performs better than dense matrices for

    the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT

    achieves exponentially lower training loss than dense when training MLPs and

    ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less

    compute and is more efficient than dense for training small GPT-2 language

    models.'
  arxivId: '2406.06248'
  authors: Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon
    Wilson
  created_at: '2024-12-24T19:26:45.734120'
  issue_number: 130
  issue_url: https://github.com/dmarx/arxiv-archive/issues/130
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Compute Better Spent: Replacing Dense Layers with Structured Matrices'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.06248
'2406.19354':
  abstract: 'The model editing problem concerns how language models should learn new
    facts

    about the world over time. While empirical research on model editing has drawn

    widespread attention, the conceptual foundations of model editing remain shaky

    -- perhaps unsurprisingly, since model editing is essentially belief revision,

    a storied problem in philosophy that has eluded succinct solutions for decades.

    Model editing nonetheless demands a solution, since we need to be able to

    control the knowledge within language models. With this goal in mind, this

    paper critiques the standard formulation of the model editing problem and

    proposes a formal testbed for model editing research. We first describe 12 open

    problems with model editing, based on challenges with (1) defining the problem,

    (2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the

    first place. Many of these challenges are extremely difficult to address, e.g.

    determining far-reaching consequences of edits, labeling probabilistic

    entailments between facts, and updating beliefs of agent simulators. Next, we

    introduce a semi-synthetic dataset for model editing based on Wikidata, where

    we can evaluate edits against labels given by an idealized Bayesian agent. This

    enables us to say exactly how belief revision in language models falls short of

    a desirable epistemic standard. We encourage further research exploring

    settings where such a gold standard can be compared against. Our code is

    publicly available at: https://github.com/peterbhase/LLM-belief-revision'
  arxivId: '2406.19354'
  authors: Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal
  created_at: '2024-12-24T19:26:48.736089'
  issue_number: 129
  issue_url: https://github.com/dmarx/arxiv-archive/issues/129
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision\
    \ Work in LLMs?"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.19354
'2407.17465':
  abstract: 'The Maximal Update Parametrization ($\mu$P) aims to make the optimal

    hyperparameters (HPs) of a model independent of its size, allowing them to be

    swept using a cheap proxy model rather than the full-size target model. We

    present a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with

    Unit Scaling, a method for designing models that makes them easy to train in

    low-precision. The two techniques have a natural affinity: $\mu$P ensures that

    the scale of activations is independent of model size, and Unit Scaling ensures

    that activations, weights and gradients begin training with a scale of one.

    This synthesis opens the door to a simpler scheme, whose default values are

    near-optimal. This in turn facilitates a more efficient sweeping strategy, with

    u-$\mu$P models reaching a loss that is equal to or lower than comparable

    $\mu$P models and working out-of-the-box in FP8.'
  arxivId: '2407.17465'
  authors: Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y.
    Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach,
    Douglas Orr
  created_at: '2024-12-24T19:27:24.757693'
  issue_number: 72
  issue_url: https://github.com/dmarx/arxiv-archive/issues/72
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'u-$μ$P: The Unit-Scaled Maximal Update Parametrization'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2407.17465
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is

    a critical step towards building generally self-improving agents that can

    operate on open-ended natural language. In this paper, we study the scaling of

    inference-time computation in LLMs, with a focus on answering the question: if

    an LLM is allowed to use a fixed but non-trivial amount of inference-time

    compute, how much can it improve its performance on a challenging prompt?

    Answering this question has implications not only on the achievable performance

    of LLMs, but also on the future of LLM pretraining and how one should tradeoff

    inference-time and pre-training compute. Despite its importance, little

    research attempted to understand the scaling behaviors of various test-time

    inference methods. Moreover, current work largely provides negative results for

    a number of these strategies. In this work, we analyze two primary mechanisms

    to scale test-time computation: (1) searching against dense, process-based

    verifier reward models; and (2) updating the model''s distribution over a

    response adaptively, given the prompt at test time. We find that in both cases,

    the effectiveness of different approaches to scaling test-time compute

    critically varies depending on the difficulty of the prompt. This observation

    motivates applying a "compute-optimal" scaling strategy, which acts to most

    effectively allocate test-time compute adaptively per prompt. Using this

    compute-optimal strategy, we can improve the efficiency of test-time compute

    scaling by more than 4x compared to a best-of-N baseline. Additionally, in a

    FLOPs-matched evaluation, we find that on problems where a smaller base model

    attains somewhat non-trivial success rates, test-time compute can be used to

    outperform a 14x larger model.'
  arxivId: '2408.03314'
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-24T19:27:36.739853'
  issue_number: 57
  issue_url: https://github.com/dmarx/arxiv-archive/issues/57
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2408.03314
'2409.09347':
  abstract: 'Mass transport problems arise in many areas of machine learning whereby
    one

    wants to compute a map transporting one distribution to another. Generative

    modeling techniques like Generative Adversarial Networks (GANs) and Denoising

    Diffusion Models (DDMs) have been successfully adapted to solve such transport

    problems, resulting in CycleGAN and Bridge Matching respectively. However,

    these methods do not approximate Optimal Transport (OT) maps, which are known

    to have desirable properties. Existing techniques approximating OT maps for

    high-dimensional data-rich problems, such as DDM-based Rectified Flow and

    Schr\"odinger Bridge procedures, require fully training a DDM-type model at

    each iteration, or use mini-batch techniques which can introduce significant

    errors. We propose a novel algorithm to compute the Schr\"odinger Bridge, a

    dynamic entropy-regularised version of OT, that eliminates the need to train

    multiple DDM-like models. This algorithm corresponds to a discretisation of a

    flow of path measures, which we call the Schr\"odinger Bridge Flow, whose only

    stationary point is the Schr\"odinger Bridge. We demonstrate the performance of

    our algorithm on a variety of unpaired data translation tasks.'
  arxivId: '2409.09347'
  authors: Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, Arnaud Doucet
  created_at: '2024-12-24T19:26:36.737440'
  issue_number: 136
  issue_url: https://github.com/dmarx/arxiv-archive/issues/136
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Schrödinger Bridge Flow for Unpaired Data Translation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.09347
'2410.02117':
  abstract: 'Dense linear layers are the dominant computational bottleneck in large
    neural

    networks, presenting a critical need for more efficient alternatives. Previous

    efforts focused on a small number of hand-crafted structured matrices and

    neglected to investigate whether these structures can surpass dense layers in

    terms of compute-optimal scaling laws when both the model size and training

    examples are optimally allocated. In this work, we present a unifying framework

    that enables searching among all linear operators expressible via an Einstein

    summation. This framework encompasses many previously proposed structures, such

    as low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,

    along with many novel structures. To analyze the framework, we develop a

    taxonomy of all such operators based on their computational and algebraic

    properties and show that differences in the compute-optimal scaling laws are

    mostly governed by a small number of variables that we introduce. Namely, a

    small $\omega$ (which measures parameter sharing) and large $\psi$ (which

    measures the rank) reliably led to better scaling laws. Guided by the insight

    that full-rank structures that maximize parameters per unit of compute perform

    the best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture

    obtained by sparsifying computation in the BTT structure. In contrast to the

    standard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE

    in every single linear layer of the model, including the projection matrices in

    the attention blocks. We find BTT-MoE provides a substantial compute-efficiency

    gain over dense layers and standard MoE.'
  arxivId: '2410.02117'
  authors: Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen,
    Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson
  created_at: '2024-12-24T19:26:42.741915'
  issue_number: 131
  issue_url: https://github.com/dmarx/arxiv-archive/issues/131
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Searching for Efficient Linear Layers over a Continuous Space of\n  Structured\
    \ Matrices"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.02117
'2410.02423':
  abstract: 'In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm

    for solving imaging inverse problems. PnP methods leverage the strength of

    pre-trained denoisers, often deep neural networks, by integrating them in

    optimization schemes. While they achieve state-of-the-art performance on

    various inverse problems in imaging, PnP approaches face inherent limitations

    on more generative tasks like inpainting. On the other hand, generative models

    such as Flow Matching pushed the boundary in image sampling yet lack a clear

    method for efficient use in image restoration. We propose to combine the PnP

    framework with Flow Matching (FM) by defining a time-dependent denoiser using
    a

    pre-trained FM model. Our algorithm alternates between gradient descent steps

    on the data-fidelity term, reprojections onto the learned FM path, and

    denoising. Notably, our method is computationally efficient and

    memory-friendly, as it avoids backpropagation through ODEs and trace

    computations. We evaluate its performance on denoising, super-resolution,

    deblurring, and inpainting tasks, demonstrating superior results compared to

    existing PnP algorithms and Flow Matching based state-of-the-art methods.'
  arxivId: '2410.02423'
  authors: Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl
  created_at: '2024-12-24T19:27:09.736538'
  issue_number: 115
  issue_url: https://github.com/dmarx/arxiv-archive/issues/115
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'PnP-Flow: Plug-and-Play Image Restoration with Flow Matching'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.02423
'2410.11900':
  abstract: 'Modern Question Answering (QA) and Reasoning approaches based on Large

    Language Models (LLMs) commonly use prompting techniques, such as

    Chain-of-Thought (CoT), assuming the resulting generation will have a more

    granular exploration and reasoning over the question space and scope. However,

    such methods struggle with generating outputs that are faithful to the

    intermediate chain of reasoning produced by the model. On the other end of the

    spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to

    combine LLMs with external symbolic solvers. While such approaches boast a high

    degree of faithfulness, they usually require a model trained for code

    generation and struggle with tasks that are ambiguous or hard to formalise

    strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided

    $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel

    interpretable approach for traversing the problem space using task

    decompositions. We use the LLM to plan a solution, soft-formalise the query

    into facts and predicates using a logic programming code and simulate that code

    execution using an exhaustive multi-hop search over the defined space. Our

    method allows us to compute the faithfulness of the reasoning process w.r.t.

    the generated code and analyse the steps of the multi-hop search without

    relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$

    out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model

    faithfulness positively correlates with overall performance and further

    demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors

    sufficient for and leading to the correct answer with optimal reasoning during

    the multi-hop search.'
  arxivId: '2410.11900'
  authors: Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle
    Augenstein
  created_at: '2024-12-24T19:27:00.844949'
  issue_number: 121
  issue_url: https://github.com/dmarx/arxiv-archive/issues/121
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'FLARE: Faithful Logic-Aided Reasoning and Exploration'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.11900
'2410.15468':
  abstract: 'We consider emergence from the perspective of dynamics: states of a system

    evolving with time. We focus on the role of a decomposition of wholes into

    parts, and attempt to characterize relationships between levels without

    reference to whether higher-level properties are "novel" or "unexpected." We

    offer a classification of different varieties of emergence, with and without

    new ontological elements at higher levels.'
  arxivId: '2410.15468'
  authors: Sean M. Carroll, Achyuth Parola
  created_at: '2024-12-24T19:27:21.822104'
  issue_number: 107
  issue_url: https://github.com/dmarx/arxiv-archive/issues/107
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: What Emergence Can Possibly Mean
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.15468
'2410.15815':
  abstract: 'We present a method for computing free-energy differences using thermodynamic

    integration with a neural network potential that interpolates between two

    target Hamiltonians. The interpolation is defined at the sample distribution

    level, and the neural network potential is optimized to match the corresponding

    equilibrium potential at every intermediate time-step. Once the interpolating

    potentials and samples are well-aligned, the free-energy difference can be

    estimated using (neural) thermodynamic integration. To target molecular

    systems, we simultaneously couple Lennard-Jones and electrostatic interactions

    and model the rigid-body rotation of molecules. We report accurate results for

    several benchmark systems: a Lennard-Jones particle in a Lennard-Jones fluid,

    as well as the insertion of both water and methane solutes in a water solvent

    at atomistic resolution using a simple three-body neural-network potential.'
  arxivId: '2410.15815'
  authors: Bálint Máté, François Fleuret, Tristan Bereau
  created_at: '2024-12-24T19:26:57.743877'
  issue_number: 123
  issue_url: https://github.com/dmarx/arxiv-archive/issues/123
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Solvation Free Energies from Neural Thermodynamic Integration
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.15815
'2410.24054':
  abstract: 'We develop EigenVI, an eigenvalue-based approach for black-box variational

    inference (BBVI). EigenVI constructs its variational approximations from

    orthogonal function expansions. For distributions over $\mathbb{R}^D$, the

    lowest order term in these expansions provides a Gaussian variational

    approximation, while higher-order terms provide a systematic way to model

    non-Gaussianity. These approximations are flexible enough to model complex

    distributions (multimodal, asymmetric), but they are simple enough that one can

    calculate their low-order moments and draw samples from them. EigenVI can also

    model other types of random variables (e.g., nonnegative, bounded) by

    constructing variational approximations from different families of orthogonal

    functions. Within these families, EigenVI computes the variational

    approximation that best matches the score function of the target distribution

    by minimizing a stochastic estimate of the Fisher divergence. Notably, this

    optimization reduces to solving a minimum eigenvalue problem, so that EigenVI

    effectively sidesteps the iterative gradient-based optimizations that are

    required for many other BBVI algorithms. (Gradient-based methods can be

    sensitive to learning rates, termination criteria, and other tunable

    hyperparameters.) We use EigenVI to approximate a variety of target

    distributions, including a benchmark suite of Bayesian models from posteriordb.

    On these distributions, we find that EigenVI is more accurate than existing

    methods for Gaussian BBVI.'
  arxivId: '2410.24054'
  authors: Diana Cai, Chirag Modi, Charles C. Margossian, Robert M. Gower, David M.
    Blei, Lawrence K. Saul
  created_at: '2024-12-24T19:27:18.827532'
  issue_number: 109
  issue_url: https://github.com/dmarx/arxiv-archive/issues/109
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "EigenVI: score-based variational inference with orthogonal function\n  expansions"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.24054
'2411.04282':
  abstract: 'Large language models (LLMs) have shown impressive capabilities, but
    still

    struggle with complex reasoning tasks requiring multiple steps. While

    prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at

    inference time, optimizing reasoning capabilities during training remains

    challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled

    framework that formulates reasoning as sampling from a latent distribution and

    optimizes it via variational approaches. LaTRO enables LLMs to concurrently

    improve both their reasoning process and ability to evaluate reasoning quality,

    without requiring external feedback or reward models. We validate LaTRO through

    experiments on GSM8K and ARC-Challenge datasets using multiple model

    architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of

    12.5% over base models and 9.6% over supervised fine-tuning across

    Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that

    pre-trained LLMs possess latent reasoning capabilities that can be unlocked and

    enhanced through our proposed optimization approach in a self-improvement

    manner. The code of LaTRO is available at

    \url{https://github.com/SalesforceAIResearch/LaTRO}.'
  arxivId: '2411.04282'
  authors: Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby
    Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang
  created_at: '2024-12-24T19:26:24.737243'
  issue_number: 169
  issue_url: https://github.com/dmarx/arxiv-archive/issues/169
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities\
    \ via Self-Rewarding"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.04282
'2412.04619':
  abstract: 'Language models (LMs), like other neural networks, often favor shortcut

    heuristics based on surface-level patterns. Although LMs behave like n-gram

    models early in training, they must eventually learn hierarchical syntactic

    representations to correctly apply grammatical rules out-of-distribution (OOD).

    In this work, we use case studies of English grammar to explore how complex,

    diverse training data drives models to generalize OOD. We construct a framework

    that unifies our understanding of random variation with training dynamics, rule

    selection with memorization, and data diversity with complexity. We show that

    these factors are nuanced, and that intermediate levels of diversity and

    complexity lead to inconsistent behavior across random seeds and to unstable

    training dynamics. Our findings emphasize the critical role of training data in

    shaping generalization patterns and illuminate how competing model strategies

    lead to inconsistent generalization outcomes across random seeds. Code is

    available at https://github.com/sunnytqin/concept_comp.git.'
  arxivId: '2412.04619'
  authors: Tian Qin, Naomi Saphra, David Alvarez-Melis
  created_at: '2024-12-24T19:26:30.741122'
  issue_number: 164
  issue_url: https://github.com/dmarx/arxiv-archive/issues/164
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.04619
'2412.05265':
  abstract: 'This manuscript gives a big-picture, up-to-date overview of the field
    of

    (deep) reinforcement learning and sequential decision making, covering

    value-based RL, policy-gradient methods, model-based methods, and various other

    topics (including a very brief discussion of RL+LLMs).'
  arxivId: '2412.05265'
  authors: Kevin Murphy
  created_at: '2024-12-24T19:26:54.820444'
  issue_number: 125
  issue_url: https://github.com/dmarx/arxiv-archive/issues/125
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Reinforcement Learning: An Overview'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.05265
'2412.06264':
  abstract: 'Flow Matching (FM) is a recent framework for generative modeling that
    has

    achieved state-of-the-art performance across various domains, including image,

    video, audio, speech, and biological structures. This guide offers a

    comprehensive and self-contained review of FM, covering its mathematical

    foundations, design choices, and extensions. By also providing a PyTorch

    package featuring relevant examples (e.g., image and text generation), this

    work aims to serve as a resource for both novice and experienced researchers

    interested in understanding, applying and further developing FM.'
  arxivId: '2412.06264'
  authors: Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian
    Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat
  created_at: '2024-12-24T19:27:06.740851'
  issue_number: 117
  issue_url: https://github.com/dmarx/arxiv-archive/issues/117
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Flow Matching Guide and Code
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.06264
'2412.09349':
  abstract: 'Controllable human image animation aims to generate videos from reference

    images using driving videos. Due to the limited control signals provided by

    sparse guidance (e.g., skeleton pose), recent works have attempted to introduce

    additional dense conditions (e.g., depth map) to ensure motion alignment.

    However, such strict dense guidance impairs the quality of the generated video

    when the body shape of the reference character differs significantly from that

    of the driving video. In this paper, we present DisPose to mine more

    generalizable and effective control signals without additional dense input,

    which disentangles the sparse skeleton pose in human image animation into

    motion field guidance and keypoint correspondence. Specifically, we generate a

    dense motion field from a sparse motion field and the reference image, which

    provides region-level dense guidance while maintaining the generalization of

    the sparse pose control. We also extract diffusion features corresponding to

    pose keypoints from the reference image, and then these point features are

    transferred to the target pose to provide distinct identity information. To

    seamlessly integrate into existing models, we propose a plug-and-play hybrid

    ControlNet that improves the quality and consistency of generated videos while

    freezing the existing model parameters. Extensive qualitative and quantitative

    experiments demonstrate the superiority of DisPose compared to current methods.

    Code:

    \href{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.'
  arxivId: '2412.09349'
  authors: Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng,
    Long Chen
  created_at: '2024-12-24T19:26:18.826512'
  issue_number: 191
  issue_url: https://github.com/dmarx/arxiv-archive/issues/191
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "DisPose: Disentangling Pose Guidance for Controllable Human Image\n  Animation"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.09349
'2412.13663':
  abstract: 'Encoder-only transformer models such as BERT offer a great performance-size

    tradeoff for retrieval and classification tasks with respect to larger

    decoder-only models. Despite being the workhorse of numerous production

    pipelines, there have been limited Pareto improvements to BERT since its

    release. In this paper, we introduce ModernBERT, bringing modern model

    optimizations to encoder-only models and representing a major Pareto

    improvement over older encoders. Trained on 2 trillion tokens with a native

    8192 sequence length, ModernBERT models exhibit state-of-the-art results on a

    large pool of evaluations encompassing diverse classification tasks and both

    single and multi-vector retrieval on different domains (including code). In

    addition to strong downstream performance, ModernBERT is also the most speed

    and memory efficient encoder and is designed for inference on common GPUs.'
  arxivId: '2412.13663'
  authors: Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar
    Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom
    Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli
  created_at: '2024-12-24T19:27:12.741544'
  issue_number: 113
  issue_url: https://github.com/dmarx/arxiv-archive/issues/113
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast,\
    \ Memory Efficient, and Long Context Finetuning and Inference"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.13663
'2412.17799':
  abstract: 'With the recent Nobel Prize awarded for radical advances in protein

    discovery, foundation models (FMs) for exploring large combinatorial spaces

    promise to revolutionize many scientific fields. Artificial Life (ALife) has

    not yet integrated FMs, thus presenting a major opportunity for the field to

    alleviate the historical burden of relying chiefly on manual design and

    trial-and-error to discover the configurations of lifelike simulations. This

    paper presents, for the first time, a successful realization of this

    opportunity using vision-language FMs. The proposed approach, called Automated

    Search for Artificial Life (ASAL), (1) finds simulations that produce target

    phenomena, (2) discovers simulations that generate temporally open-ended

    novelty, and (3) illuminates an entire space of interestingly diverse

    simulations. Because of the generality of FMs, ASAL works effectively across a

    diverse range of ALife substrates including Boids, Particle Life, Game of Life,

    Lenia, and Neural Cellular Automata. A major result highlighting the potential

    of this technique is the discovery of previously unseen Lenia and Boids

    lifeforms, as well as cellular automata that are open-ended like Conway''s Game

    of Life. Additionally, the use of FMs allows for the quantification of

    previously qualitative phenomena in a human-aligned way. This new paradigm

    promises to accelerate ALife research beyond what is possible through human

    ingenuity alone.'
  arxivId: '2412.17799'
  authors: Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip
    Isola, David Ha
  created_at: '2024-12-24T19:25:57.999064'
  issue_number: 247
  issue_url: https://github.com/dmarx/arxiv-archive/issues/247
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-24T19:25:58.000547'
  state: open
  title: Automating the Search for Artificial Life with Foundation Models
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2412.17799
