\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2017)]{arpit2017closer}
Devansh Arpit et~al.
\newblock {A Closer Look at Memorization in Deep Networks}.
\newblock In \emph{the 34th International Conference on Machine Learning}, 2017.

\bibitem[Chen et~al.(2023)Chen, Zhu, and Li]{chen2023sample}
Wenkai Chen, Chuang Zhu, and Mengting Li.
\newblock {Sample Prior Guided Robust Model Learning to Suppress Noisy Labels}.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, pages 3--19. Springer, 2023.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, et~al.]{dean2012large}
Jeffrey Dean, Greg~S Corrado, Rajat Monga, et~al.
\newblock {Large Scale Distributed Deep Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{5206848}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock {ImageNet: A large-scale hierarchical image database}.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255, 2009.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, et~al.]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, et~al.
\newblock {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock {Sharpness-Aware Minimization for Efficiently Improving Generalization}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He]{goyal2018accuratelargeminibatchsgd}
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Han et~al.(2018)Han, Yao, Liu, et~al.]{han2018coteaching}
Bo Han, Quanming Yao, Xingrui Liu, et~al.
\newblock {Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 8527--8537, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pages 770--778, 2016.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock {Training Compute-Optimal Large Language Models}.
\newblock In \emph{International Conference on Neural Information Processing Systems}, pages 30016--30030, 2022.

\bibitem[Huang et~al.(2017)]{baidu2017ringallreduce}
Yanping Huang et~al.
\newblock {Large-Scale Distributed Deep Learning: Lessons Learned from 3,000,000 GPU Hours on TitanX}.
\newblock In \emph{the 25th ACM Symposium on Operating Systems Principles}, pages 19--33, 2017.

\bibitem[Keskar et~al.(2016)]{keskar2016large}
Nitish~Shirish Keskar et~al.
\newblock {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock Master's thesis, University of Toronto, 2009.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Socher, and Hoi]{li2020dividemix}
Junnan Li, Richard Socher, and Steven~C.H. Hoi.
\newblock {DivideMix: Learning with Noisy Labels as Semi-supervised Learning}.
\newblock In \emph{International Conference on Learning Representations}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Zhao, Varma, Salpekar, Noordhuis, Li, Paszke, Smith, Vaughan, Damania, and Chintala]{li2020pytorch}
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.
\newblock {PyTorch Distributed: Experiences on Accelerating Data Parallel Training}.
\newblock \emph{Proceedings of the VLDB Endowment}, 13\penalty0 (12):\penalty0 3005--3018, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2017)Li, Yang, Song, Cao, Luo, and Li]{li2020learning}
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li.
\newblock {Learning from Noisy Labels with Distillation}.
\newblock In \emph{2017 IEEE International Conference on Computer Vision (ICCV)}, 2017.

\bibitem[Liu et~al.(2022)Liu, Zhu, Qu, and You]{liu2022self}
Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You.
\newblock {Robust Training under Label Noise by Over-parameterization}.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock {Decoupled Weight Decay Regularization}.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, et~al.]{mccandlish2018empirical}
Sam McCandlish, Jared Kaplan, Dario Amodei, et~al.
\newblock {An Empirical Model of Large-Batch Training}.
\newblock \emph{arXiv preprint arXiv:1812.06162}, 2018.

\bibitem[Neelakantan et~al.(2015)Neelakantan, Vilnis, Le, Sutskever, Kaiser, Kurach, and Martens]{neelakantan2015addinggradientnoiseimproves}
Arvind Neelakantan, Luke Vilnis, Quoc~V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens.
\newblock {Adding Gradient Noise Improves Learning for Very Deep Networks}, 2015.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{pascanu2013difficultytrainingrecurrentneural}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock {On the Difficulty of Training Recurrent Neural Networks}.
\newblock \emph{International Conference on Machine Learning}, 2013.

\bibitem[Piao et~al.(2023)Piao, Synn, Park, and Kim]{Piao_2023}
Xinyu Piao, Doangjoo Synn, Jooyoung Park, and Jong-Kook Kim.
\newblock {Enabling Large Batch Size Training for DNN Models Beyond the Memory Limit While Maintaining Performance}.
\newblock \emph{IEEE Access}, 11:\penalty0 102981–102990, 2023.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock {Some Methods of Speeding Up the Convergence of Iteration Methods}.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Prechelt(1998)]{prechelt1998early}
Lutz Prechelt.
\newblock \emph{{Early Stopping - But When?}}, pages 55--69.
\newblock Springer, 1998.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock {Learning to Reweight Examples for Robust Deep Learning}.
\newblock In \emph{International Conference on Machine Learning}, pages 4334--4343, 2018.

\bibitem[Robbins and Monro(1951)]{robbins1951sgd}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0 (3):\penalty0 400--407, 1951.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, et~al.]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raghavendra Puri, et~al.
\newblock {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Srivastava et~al.(2014)]{srivastava2014dropout}
Nitish Srivastava et~al.
\newblock {Dropout: A Simple Way to Prevent Neural Networks from Overfitting}.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1929--1958, 2014.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock {Lecture 6.5-RMSProp: Divide the Gradient by a Running Average of its Recent Magnitude}.
\newblock \emph{Coursera: Neural Networks for Machine Learning}, 4\penalty0 (2), 2012.

\bibitem[Tishby and Zaslavsky(2015)]{tishby2015deep}
Naftali Tishby and Noga Zaslavsky.
\newblock {Deep Learning and the Information Bottleneck Principle}.
\newblock In \emph{IEEE Information Theory Workshop}, 2015.

\bibitem[Wei et~al.(2022)Wei, Zhu, Cheng, Liu, Niu, and Liu]{wei2022learning}
Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu.
\newblock {Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Dong, Wang, Feng, Wu, Chen, and Zhao]{xiao2023promix}
Ruixuan Xiao, Yiwen Dong, Haobo Wang, Lei Feng, Runze Wu, Gang Chen, and Junbo Zhao.
\newblock {ProMix: Combating Label Noise via Maximizing Clean Sample Utility}.
\newblock In \emph{Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence}, pages 4442--4450, 2023.

\bibitem[Yao et~al.(2024)Yao, Liu, Cen, Huang, Zhang, Yu, and Zhao]{yao2023gradientshapingmulticonstraintsafe}
Yihang Yao, Zuxin Liu, Zhepeng Cen, Peide Huang, Tingnan Zhang, Wenhao Yu, and Ding Zhao.
\newblock {Gradient Shaping for Multi-Constraint Safe Reinforcement Learning}.
\newblock In \emph{6th Annual Learning for Dynamics \& Control Conference}, 2024.

\bibitem[You et~al.(2020)]{you2020large}
Yang You et~al.
\newblock {Large Batch Optimization for Deep Learning: Training BERT in 76 Minutes}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and Finn]{yu2020gradientsurgerymultitasklearning}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
\newblock {Gradient Surgery for Multi-Task Learning}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, et~al.]{zhuang2020adabelief}
Juntang Zhuang, Tommy Tang, Yifan Ding, et~al.
\newblock {AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\end{thebibliography}
