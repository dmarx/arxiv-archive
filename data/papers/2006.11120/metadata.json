{
  "arxivId": "2006.11120",
  "title": "From Discrete to Continuous Convolution Layers",
  "authors": "Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani",
  "abstract": "A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing\nof feature maps. This is done either by strided convolution (donwscaling) or\ntransposed convolution (upscaling). Such operations are limited to a fixed\nfilter moving at predetermined integer steps (strides). Spatial sizes of\nconsecutive layers are related by integer scale factors, predetermined at\narchitectural design, and remain fixed throughout training and inference time.\nWe propose a generalization of the common Conv-layer, from a discrete layer to\na Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by\nrepresenting the filter as a learned continuous function over sub-pixel\ncoordinates. This allows learnable and principled resizing of feature maps, to\nany size, dynamically and consistently across scales. Once trained, the CC\nlayer can be used to output any scale/size chosen at inference time. The scale\ncan be non-integer and differ between the axes. CC gives rise to new freedoms\nfor architectural design, such as dynamic layer shapes at inference time, or\ngradual architectures where the size changes by a small factor at each layer.\nThis gives rise to many desired CNN properties, new architectural design\ncapabilities, and useful applications. We further show that current Conv-layers\nsuffer from inherent misalignments, which are ameliorated by CC layers.",
  "url": "https://arxiv.org/abs/2006.11120",
  "issue_number": 262,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/262",
  "created_at": "2024-12-25T08:51:44.913555",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 24,
  "last_read": "2024-12-25T08:51:44.915391",
  "last_visited": "2024-12-25T05:28:48.326Z"
}