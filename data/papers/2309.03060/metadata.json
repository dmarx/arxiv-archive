{
  "arxivId": "2309.03060",
  "title": "CoLA: Exploiting Compositional Structure for Automatic and Efficient\n  Numerical Linear Algebra",
  "authors": "Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson",
  "abstract": "Many areas of machine learning and science involve large linear algebra\nproblems, such as eigendecompositions, solving linear systems, computing matrix\nexponentials, and trace estimation. The matrices involved often have Kronecker,\nconvolutional, block diagonal, sum, or product structure. In this paper, we\npropose a simple but general framework for large-scale linear algebra problems\nin machine learning, named CoLA (Compositional Linear Algebra). By combining a\nlinear operator abstraction with compositional dispatch rules, CoLA\nautomatically constructs memory and runtime efficient numerical algorithms.\nMoreover, CoLA provides memory efficient automatic differentiation, low\nprecision computation, and GPU acceleration in both JAX and PyTorch, while also\naccommodating new objects, operations, and rules in downstream packages via\nmultiple dispatch. CoLA can accelerate many algebraic operations, while making\nit easy to prototype matrix structures and algorithms, providing an appealing\ndrop-in tool for virtually any computational effort that requires linear\nalgebra. We showcase its efficacy across a broad range of applications,\nincluding partial differential equations, Gaussian processes, equivariant model\nconstruction, and unsupervised learning.",
  "url": "https://arxiv.org/abs/2309.03060",
  "issue_number": 132,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/132",
  "created_at": "2024-12-24T19:26:39.759422",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null
}