\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Mart\'{i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dandelion Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris
  Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
  Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas,
  Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
  Xiaoqiang Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Anil et~al.(2020)Anil, Gupta, Koren, Regan, and
  Singer]{anil2020shampoo}
Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer.
\newblock {Scalable Second Order Optimization for Deep Learning}.
\newblock \emph{Preprint arXiv 2002.09018v2}, 2020.

\bibitem[Arnoldi(1951)]{arnoldi1951principle}
Walter~Edwin Arnoldi.
\newblock The principle of minimized iterations in the solution of the matrix
  eigenvalue problem.
\newblock \emph{Quarterly of applied mathematics}, 9\penalty0 (1):\penalty0
  17--29, 1951.

\bibitem[Becker and Lecun(1989)]{becker1989improving}
S~Becker and Yann Lecun.
\newblock Improving the convergence of back-propagation learning with
  second-order methods.
\newblock In \emph{Proceedings of the 1988 Connectionist Models Summer School,
  San Mateo}, pages 29--37. Morgan Kaufmann, 1989.

\bibitem[Bell et~al.(2023)Bell, Olson, Schroder, and Southworth]{pyamg2023}
Nathan Bell, Luke~N. Olson, Jacob Schroder, and Ben Southworth.
\newblock {PyAMG: Algebraic Multigrid Solvers in Python}.
\newblock \emph{Journal of Open Source Software}, 2023.

\bibitem[Bezanson et~al.(2014)Bezanson, Edelman, Karpinski, and
  Shah]{bezanson2014julia}
Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral~B. Shah.
\newblock {Julia: A Fresh Approach to Numerical Computing}.
\newblock \emph{arXiv preprint arXiv:1411.1607}, 2014.

\bibitem[Bonilla et~al.(2007)Bonilla, Chai, and Williams]{bonilla2007multitask}
Edwin~V. Bonilla, Kian Ming~A. Chai, and Christopher K.~I. Williams.
\newblock {Multi-task Gaussian Process Prediction}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2007.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs.
\newblock \emph{SoftwareX}, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brezina et~al.(2005)Brezina, Falgout, MacLachlan, Manteuffel,
  McCormick, and Ruge]{brezina2006asa}
M.~Brezina, R.~Falgout, S.~MacLachlan, T.~Manteuffel, S.~McCormick, and
  J.~Ruge.
\newblock {Adaptive Smoothed Aggregation ($\alpha$SA) Multigrid}.
\newblock \emph{SIAM Review}, 2005.

\bibitem[Charlier et~al.(2021)Charlier, Feydy, Glaunes, Collin, and
  Durif]{charlier2021kernel}
Benjamin Charlier, Jean Feydy, Joan~Alexis Glaunes, Fran{\c{c}}ois-David
  Collin, and Ghislain Durif.
\newblock Kernel operations on the {GPU}, with autodiff, without memory
  overflows.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 3457--3462, 2021.

\bibitem[Cunningham et~al.(2008)Cunningham, Shenoy, and
  Sahani]{cunningham2008fast}
John~P Cunningham, Krishna~V Shenoy, and Maneesh Sahani.
\newblock Fast gaussian process methods for point process intensity estimation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  192--199, 2008.

\bibitem[Cuturi(2013)]{cuturi2013sinkhorn}
Marco Cuturi.
\newblock {Sinkhorn Distances: Lightspeed Computation of Optimal Transport}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2013.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and
  R\'{e}]{dao2019butterfly}
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\'{e}.
\newblock {Learning Fast Algorithms for Linear Transforms Using Butterfly
  Factorizations}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Davis(2006)]{davis2006direct}
Timothy~A Davis.
\newblock \emph{Direct methods for sparse linear systems}.
\newblock SIAM, 2006.

\bibitem[Du and Zaki(2021)]{du2021evolutional}
Yifan Du and Tamer~A Zaki.
\newblock {Evolutional Deep Neural Network}.
\newblock \emph{Physical Review E}, 104\penalty0 (4):\penalty0 045303, 2021.

\bibitem[Finzi et~al.(2021)Finzi, Welling, and Wilson]{finzi2021practical}
Marc Finzi, Max Welling, and Andrew~Gordon Wilson.
\newblock {A Practical Method for Constructing Equivariant Multilayer
  Perceptrons for Arbitrary Matrix Groups}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Finzi et~al.(2023)Finzi, Potapczynski, Choptuik, and
  Wilson]{finzi2023nivp}
Marc Finzi, Andres Potapczynski, Matthew Choptuik, and Andrew~Gordon Wilson.
\newblock {A Stable and Scalable Method for Solving Initial Value PDEs with
  Neural Networks}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2023.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and R\'{e}]{fu2023hippos}
Daniel~Y. Fu, Tri Dao, Khaled~K. Saab, Armin~W. Thomas, Atri Rudra, and
  Christopher R\'{e}.
\newblock {Hungry Hungry Hippos: Towards Language Modeling with State Space
  Models}.
\newblock \emph{Preprint arXiv 2212.14052v3}, 2023.

\bibitem[Gardner et~al.(2018{\natexlab{a}})Gardner, Pleiss, Wu, Weinberger, and
  Wilson]{gardner2018product}
Jacob Gardner, Geoff Pleiss, Ruihan Wu, Kilian Weinberger, and Andrew Wilson.
\newblock Product kernel interpolation for scalable gaussian processes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1407--1416. PMLR, 2018{\natexlab{a}}.

\bibitem[Gardner et~al.(2018{\natexlab{b}})Gardner, Pleiss, Bindel, Weinberger,
  and Wilson]{gardner2018gpytorch}
Jacob~R. Gardner, Geoff Pleiss, David Bindel, Kilian~Q. Weinberger, and
  Andrew~Gordon Wilson.
\newblock {GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with
  {GPU} Acceleration}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2018{\natexlab{b}}.

\bibitem[Golub and Loan(2018)]{golub2018matrix}
Gene~H Golub and Charles F~Van Loan.
\newblock \emph{{Matrix Computations}}.
\newblock The Johns Hopkins University Press, 2018.
\newblock Fourth Edition.

\bibitem[Hutchinson(1989)]{hutchinson1989stochastic}
Michael~F Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation},
  18\penalty0 (3):\penalty0 1059--1076, 1989.

\bibitem[Johnshon and Zhang(2013)]{johnson2013accelerating}
Rie Johnshon and Tong Zhang.
\newblock {Accelerating Stochastic Gradient Descent using Predictive Variance
  Reduction}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2013.

\bibitem[Kapoor et~al.(2021)Kapoor, Finzi, Wang, and Wilson]{kapoor2021skiing}
Sanyam Kapoor, Marc Finzi, Ke~Alexander Wang, and Andrew Gordon~Gordon Wilson.
\newblock {SKIing on Simplices: Kernel Interpolation on the Permutohedral
  Lattice for Scalable Gaussian Processes}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Katzfuss and Guinness(2021)]{katzfuss2021general}
Matthias Katzfuss and Joseph Guinness.
\newblock A general framework for vecchia approximations of gaussian processes.
\newblock \emph{Statistical science}, 36\penalty0 (1):\penalty0 124--141, 2021.

\bibitem[Knyazev(2000)]{knyazev2000lobpcg}
Andrew Knyazev.
\newblock {Toward The Optimal Preconditioned Eigensolver: Locally Optimal Block
  Preconditioned Conjugate Gradient Method}.
\newblock \emph{SIAM Journal on Scientific Computing}, 2000.

\bibitem[Kovachki et~al.(2021)Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya,
  Stuart, and Anandkumar]{kovachi2021neuraloperator}
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik
  Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock {Neural Operator: Learning Maps Between Function Spaces}.
\newblock \emph{Preprint arXiv 2108.08481v3}, 2021.

\bibitem[Li et~al.(2018)Li, Farkhoor, Liu, and Yosinski]{li2018intrinsic}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock {Measuring the Intrinsic Dimension of Objective Landscapes}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Loper et~al.(2021)Loper, Blei, Cunningham, and
  Paninski]{loper2021general}
Jackson Loper, David Blei, John~P Cunningham, and Liam Paninski.
\newblock A general linear-time inference method for gaussian processes on one
  dimension.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 10580--10615, 2021.

\bibitem[Maclaurin(2016)]{maclaurin2016modeling}
Dougal Maclaurin.
\newblock \emph{Modeling, inference and optimization with composable
  differentiable procedures}.
\newblock PhD thesis, School of Engineering and Applied Sciences, Harvard
  University, 2016.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Dougal Maclaurin, David Duvenaud, and Ryan Adams.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International conference on machine learning}, pages
  2113--2122. PMLR, 2015.

\bibitem[Maddox et~al.(2022)Maddox, Potapczynski, and
  Wilson]{maddox2022lowpres}
Wesley~J. Maddox, Andres Potapczynski, and Andrew~Gordon Wilson.
\newblock {Low-Precision Arithmetic for Fast Gaussian Processes}.
\newblock \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  2022.

\bibitem[Martens(2010)]{martens2010deep}
James Martens.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, pages 735--742, 2010.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pages
  2408--2417. PMLR, 2015.

\bibitem[Martinsson and Tropp(2020)]{martinsson2020rnla}
Per-Gunnar Martinsson and Joel Tropp.
\newblock \emph{{Randomized Numerical Linear Algebra: Foundations \&
  Algorithms}}.
\newblock arXiv 2002.01387v3, 2020.

\bibitem[Ng et~al.(2001)Ng, Jordan, and Weiss]{ng2001spectral}
Andrew~Y. Ng, Michael~I. Jordan, and Yair Weiss.
\newblock {On Spectral Clustering: Analysis and an algorithm}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2001.

\bibitem[Nguyen et~al.(2022)Nguyen, Goel, Gu, Downs, Shah, Dao, Baccus, and
  R\'{e}]{nguyen2022s4nd}
Eric Nguyen, Karan Goel, Albert Gu, Gordon~W. Downs, Preey Shah, Tri Dao,
  Stephen~A. Baccus, and Christopher R\'{e}.
\newblock {S4ND: Modeling Images and Videos as Multidimensional Signals Using
  State Spaces}.
\newblock \emph{Preprint arXiv 2210.06583v2}, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Perez et~al.(2018)Perez, Strub, de~Vries, Dumoulin, and
  Courville]{perez2018film}
Ethan Perez, Florian Strub, Harm de~Vries, Vincent Dumoulin, and Aaron~C.
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Proceedings of the Thirty-Second {AAAI} Conference on
  Artificial Intelligence, (AAAI-18), the 30th innovative Applications of
  Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on
  Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,
  Louisiana, USA, February 2-7, 2018}, pages 3942--3951. {AAAI} Press, 2018.

\bibitem[Rahimi and Recht(2007)]{rahimi2007randomfeatures}
Ali Rahimi and Ben Recht.
\newblock {Random Features for Large-Scale Kernel Machines}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2007.

\bibitem[Ravasi and Vasconcelos(2020)]{pylops}
Matteo Ravasi and Ivan Vasconcelos.
\newblock {PyLops—A linear-operator Python library for scalable algebra and
  optimization}.
\newblock \emph{SoftwareX}, 11:\penalty0 100361, 2020.
\newblock ISSN 2352-7110.
\newblock \doi{https://doi.org/10.1016/j.softx.2019.100361}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S2352711019301086}.

\bibitem[Roux et~al.(2007)Roux, Manzagol, and Bengio]{roux2007topmoumoute}
Nicolas Roux, Pierre-Antoine Manzagol, and Yoshua Bengio.
\newblock Topmoumoute online natural gradient algorithm.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Saad and Schultz(1986)]{saad1986gmres}
Youcef Saad and Martin~H Schultz.
\newblock Gmres: A generalized minimal residual algorithm for solving
  nonsymmetric linear systems.
\newblock \emph{SIAM Journal on scientific and statistical computing},
  7\penalty0 (3):\penalty0 856--869, 1986.

\bibitem[Saad(2003)]{saad2003iterative}
Yousef Saad.
\newblock \emph{Iterative methods for sparse linear systems}.
\newblock SIAM, 2003.

\bibitem[Shamir(2015)]{shamir2015stochastic}
Ohad Shamir.
\newblock {A Stochastic PCA and SVD Algorithm with an Exponential Convergence
  Rate}.
\newblock \emph{arXiv preprint arXiv:1409.2848v5}, 2015.

\bibitem[Snelson and Ghahramani(2005)]{snelson2005sparse}
Edward Snelson and Zoubin Ghahramani.
\newblock Sparse gaussian processes using pseudo-inputs.
\newblock \emph{Advances in neural information processing systems}, 18, 2005.

\bibitem[Tokui et~al.(2015)Tokui, Oono, Hido, and Clayton]{tokui2015chainer}
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton.
\newblock Chainer: a next-generation open source framework for deep learning.
\newblock In \emph{NeurIPS Workshop on Machine Learning Systems (LearningSys)},
  volume~5, pages 1--6, 2015.

\bibitem[Trefethen and Bau(1997)]{trefethen1997NLA}
Lloyd~N. Trefethen and David Bau.
\newblock \emph{{Numerical Linear Algebra}}.
\newblock SIAM, 1997.

\bibitem[van~den Berg and Friedlander(2013)]{spot}
Ewout van~den Berg and Michael~P. Friedlander.
\newblock {Spot – A Linear-Operator Toolbox}.
\newblock \emph{SoftwareX}, 2013.
\newblock URL \url{http://www.cs.ubc.ca/labs/scl/spot/}.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}]{scipy}
Pauli Virtanen, Ralf Gommers, Travis~E. Oliphant, Matt Haberland, Tyler Reddy,
  David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
  Bright, St{\'e}fan~J. {van der Walt}, Matthew Brett, Joshua Wilson, K.~Jarrod
  Millman, Nikolay Mayorov, Andrew R.~J. Nelson, Eric Jones, Robert Kern, Eric
  Larson, C~J Carey, {\.I}lhan Polat, Yu~Feng, Eric~W. Moore, Jake
  {VanderPlas}, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen,
  E.~A. Quintero, Charles~R. Harris, Anne~M. Archibald, Ant{\^o}nio~H. Ribeiro,
  Fabian Pedregosa, Paul {van Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.
\newblock \doi{10.1038/s41592-019-0686-2}.

\bibitem[Wang et~al.(2019)Wang, Pleiss, Gardner, Tyree, Weinberger, and
  Wilson]{wang2019exact}
Ke~Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian~Q Weinberger, and
  Andrew~Gordon Wilson.
\newblock Exact gaussian processes on a million data points.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Wilson and Nickisch(2015)]{wilson2015kernel}
Andrew Wilson and Hannes Nickisch.
\newblock Kernel interpolation for scalable structured gaussian processes
  (kiss-gp).
\newblock In \emph{International conference on machine learning}, pages
  1775--1784. PMLR, 2015.

\bibitem[Wilson et~al.(2014)Wilson, Gilboa, Nehorai, and
  Cunningham]{wilson2014fast}
Andrew~G Wilson, Elad Gilboa, Arye Nehorai, and John~P Cunningham.
\newblock Fast kernel learning for multidimensional pattern extrapolation.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Woodbury(1950)]{woodbury1950inverting}
Max~A Woodbury.
\newblock \emph{Inverting modified matrices}.
\newblock Department of Statistics, Princeton University, 1950.

\bibitem[Xu et~al.(2018)Xu, He, De~Sa, Mitliagkas, and Re]{xu2018accelerated}
Peng Xu, Bryan He, Christopher De~Sa, Ioannis Mitliagkas, and Chris Re.
\newblock Accelerated stochastic power iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 58--67. PMLR, 2018.

\end{thebibliography}
