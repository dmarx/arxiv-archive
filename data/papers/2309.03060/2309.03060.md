---
abstract: |
  Many areas of machine learning and science involve large linear algebra problems, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation. The matrices involved often have Kronecker, convolutional, block diagonal, sum, or product structure. In this paper, we propose a simple but general framework for large-scale linear algebra problems in machine learning, named *CoLA* (Compositional Linear Algebra). By combining a linear operator abstraction with compositional dispatch rules, CoLA automatically constructs memory and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient automatic differentiation, low precision computation, and GPU acceleration in both JAX and PyTorch, while also accommodating new objects, operations, and rules in downstream packages via multiple dispatch. CoLA can accelerate many algebraic operations, while making it easy to prototype matrix structures and algorithms, providing an appealing drop-in tool for virtually any computational effort that requires linear algebra. We showcase its efficacy across a broad range of applications, including partial differential equations, Gaussian processes, equivariant model construction, and unsupervised learning.
author:
- |
    
  **Andres Potapczynski[^1]$\text{\,\,\,}^1$ Marc Finzi$^*$$^2$ Geoff Pleiss$^{3, 4}$ Andrew Gordon Wilson$^1$**  
  $^1$New York University, $^2$Carnegie Mellon University, $^3$University of British Columbia,  
  $^4$Vector Institute  
bibliography:
- refs.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra"
---





# Introduction

The framework of automatic differentiation has revolutionized machine learning. Although the rules that govern derivatives have long been known, automatically computing derivatives was a nontrivial process that required (1) efficient implementations of base-case primitive derivatives, (2) software abstractions (autograd and computation graphs) to compose these primitives into complex computations, and (3) a mechanism for users to modify or extend compositional rules to new functions. Once libraries such as PyTorch, Chainer, Tensorflow, JAX, and others figured out the correct abstractions, the impact was enormous. Efforts that previously went into deriving and implementing gradients could be repurposed into developing new models.

In this paper, we automate another notorious bottleneck for ML methods: performing large-scale linear algebra (e.g. matrix solves, eigenvalue problems, nullspace computations). These ubiquitous operations are at the heart of principal component analysis, Gaussian processes, normalizing flows, equivariant neural networks, and many other applications . Modeling assumptions frequently manifest themselves as algebraic structure—such as diagonal dominance, sparsity, or a low-rank factorization. Given a structure (e.g., the sum of low-rank plus diagonal matrices) and a linear algebraic operation (e.g., linear solves), there is often a computational routine (e.g. the linear-time Woodbury inversion formula) with lower computational complexity than a general-purpose routine (e.g., the cubic-time Cholesky decomposition). However, exploiting structure for faster computation is often an intensive implementation process. Rather than having an object ${\boldsymbol{\mathbf{A}}}$ in code that represents a low-rank-plus-diagonal matrix and simply calling ${\tt solve}({\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{b}}})$, a practitioner must instead store the low-rank factor ${\boldsymbol{\mathbf{F}}}$ as a matrix, the diagonal ${\boldsymbol{\mathbf{d}}}$ as a vector, and implement the Woodbury formula from scratch. Implementing structure-aware routines in machine learning models is often seen as a major research undertaking. For example, a nontrivial portion of the Gaussian process literature is devoted to deriving specialty inference algorithms for structured kernel matrices .

<div id="tab:matrix_operations">

<table>
<caption> <strong>Many structures have explicit composition rules to exploit.</strong> Here we show the existence of a dispatch rule (<span style="background-color: cyan!30">(4,4)</span>)  that can be used to accelerate a linear algebraic operation for some matrix structure over what is possible with the dense and iterative base cases. Many combinations (shown with <span style="background-color: purple!30">(4,4)</span>) are automatically accelerated as a consequence of other rules, since for example <code>Eigs</code> and <code>Diag</code> are used in other routines. In absence of a rule, the operation will fall back to the iterative and dense base case for each operation (shown in <span style="background-color: green!30">(4,4)</span>). Columns are basic linear operator types such as D: Diagonal, T: Triangular, P: Permutation, C: Convolution, S:Sparse, Pr: Projection and composition operators such as sum, product, Kronecker product, block diagonal and concatenation. All compositional rules can be mixed and matched and are implemented through multiple dispatch. </caption>
<thead>
<tr class="header">
<th colspan="2" style="text-align: center;"></th>
<th colspan="6" style="text-align: center;"></th>
<th colspan="5" style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span>3-13</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Case  </td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">  </td>
<td style="text-align: center;"><p></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span><span class="math inline">\(\begin{bmatrix} {\boldsymbol{\mathbf{A}}} &amp; {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} &amp; {\boldsymbol{\mathbf{B}}} \end{bmatrix}\)</span> </span></td>
<td style="text-align: center;"><span><span class="math inline">\(\begin{bmatrix} {\boldsymbol{\mathbf{A}}} &amp; {\boldsymbol{\mathbf{B}}} \\ {\boldsymbol{\mathbf{C}}} &amp; {\boldsymbol{\mathbf{D}}} \end{bmatrix}\)</span> </span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\({\boldsymbol{\mathbf{A}}}^{-1}\)</span></td>
<td style="text-align: center;"><span style="background-color: green!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><p><span style="background-color: cyan!30">(10,10)</span></p></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Eigs<span class="math inline">\(({\boldsymbol{\mathbf{A}}})\)</span></td>
<td style="text-align: center;"><span style="background-color: green!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span> </td>
<td style="text-align: center;"><p><span style="background-color: cyan!30">(10,10)</span></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Diag<span class="math inline">\(({\boldsymbol{\mathbf{A}}})\)</span></td>
<td style="text-align: center;"><span style="background-color: green!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"> </td>
<td style="text-align: center;"><p><span style="background-color: cyan!30">(10,10)</span></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathrm{Tr}({\boldsymbol{\mathbf{A}}}\)</span>)</td>
<td style="text-align: center;"><span style="background-color: green!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span> </td>
<td style="text-align: center;"><p><span style="background-color: purple!30">(10,10)</span></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\exp({\boldsymbol{\mathbf{A}}}\)</span>)</td>
<td style="text-align: center;"><span style="background-color: green!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span> </td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathrm{det}({\boldsymbol{\mathbf{A}}})\)</span></td>
<td style="text-align: center;"><span style="background-color: green!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span> </td>
<td style="text-align: center;"><p><span style="background-color: purple!30">(10,10)</span></p></td>
<td style="text-align: center;"><span style="background-color: cyan!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"><span style="background-color: purple!30">(10,10)</span></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>

**Many structures have explicit composition rules to exploit.** Here we show the existence of a dispatch rule (<span style="background-color: cyan!30">(4,4)</span>)  that can be used to accelerate a linear algebraic operation for some matrix structure over what is possible with the dense and iterative base cases. Many combinations (shown with <span style="background-color: purple!30">(4,4)</span>) are automatically accelerated as a consequence of other rules, since for example `Eigs` and `Diag` are used in other routines. In absence of a rule, the operation will fall back to the iterative and dense base case for each operation (shown in <span style="background-color: green!30">(4,4)</span>). Columns are basic linear operator types such as D: Diagonal, T: Triangular, P: Permutation, C: Convolution, S:Sparse, Pr: Projection and composition operators such as sum, product, Kronecker product, block diagonal and concatenation. All compositional rules can be mixed and matched and are implemented through multiple dispatch.

</div>

As with automatic differentiation, structure-aware linear algebra is ripe for automation. We introduce a general numerical framework that dramatically simplifies implementations efforts while achieving a high degree of computational efficiency. In code, we represent structure matrices as `LinearOperator` objects which adhere to the same API as standard dense matrices. For example, a user can call ${\boldsymbol{\mathbf{A}}}^{-1} {\boldsymbol{\mathbf{b}}}$ or $\texttt{eig}({\boldsymbol{\mathbf{A}}})$ on any `LinearOperator` ${\boldsymbol{\mathbf{A}}}$, and under-the-hood our framework derives a computationally efficient algorithm built from our set of compositional *dispatch rules* (see ). If little is known about ${\boldsymbol{\mathbf{A}}}$, the derived algorithm reverts to a general-purpose base case (e.g. Gaussian elimination or GMRES for linear solves). Conversely, if ${\boldsymbol{\mathbf{A}}}$ is known to be the Kronecker product of a lower triangular matrix and a positive definite Toeplitz matrix, for example, the derived algorithm uses specialty algorithms for Kronecker, triangular, and positive definite matrices. Through this compositional pattern matching, our framework can match or outperform special-purpose implementations across numerous applications despite relying on only a small number of base `LinearOperator` types.

Furthermore, our framework offers additional novel functionality that is necessary for ML applications (see ). In particular, we automatically compute gradients, diagonals, transposes and adjoints of linear operators, and we modify classic iterative algorithms to ensure numerical stability in low precision. We also support specialty algorithms, such as SVRG and a novel variation of Hutchinson’s diagonal estimator , which exploit *implicit structure* common to matrices in machine learning applications (namely, the ability to express matrices as large-scale sums amenable to stochastic approximations). Moreover, our framework is easily extensible in *both directions*: a user can implement a new linear operator (i.e. one column in ), or a new linear algebraic operation (i.e. one row in ). Finally, our routines benefit from GPU and TPU acceleration and apply to symmetric and non-symmetric operators for both real and complex numbers.

We term our framework *CoLA* (**Co**mpositional **L**inear **A**lgebra), which we package in a library that supports both PyTorch and JAX. We showcase the extraordinary versatility of CoLA with a broad range of applications in and , including: PCA, spectral clustering, multi-task Gaussian processes, equivariant models, neural PDEs, random Fourier features, and PDEs like minimal surface or the Schrödinger equation. Not only does CoLA provide competitive performance to specialized packages but it provides significant speedups especially in applications with compositional structure (Kronecker, block diagonal, product, etc). Our package is available at <https://github.com/wilson-labs/cola>.

# Background and Related Work

**Structured matrices** Structure appears throughout machine learning applications, either occurring naturally through properties of the data, or artificially as a constraint to simplify complexity. A nonexhausitve list of examples includes: (1) low-rank matrices, which admit efficient solves and determinants ; (2) sparse matrices, which admit fast methods for linear solves and eigenvalue problems ; (3) Kronecker-factorizable matrices, which admit efficient spectral decompositions; (4) Toeplitz or circulant matrices, which admit fast matrix-vector products. See and for applications that use these structures. Beyond these explicit types, we also consider *implicit structures*, such as matrices with clustered eigenvalues or matrices with simple unbiased estimates. Though these implicit structures do not always fall into straightforward categorizations, it is possible to design algorithms that exploit their inherent properties (see ).

**Iterative matrix-free algorithms** Unlike direct methods, which typically require dense instantiations of matrices, matrix-free algorithms only access matrices through routines that perform matrix-vector multiples (MVMs) . The most common matrix-free algorithms—such as conjugate gradients, GMRES, Lanczos and Arnoldi iteration—fall under the category of Krylov subspace methods, which iteratively apply MVMs to refine a solution until a desired error tolerance is achieved. Though the rate of convergence depends on the conditioning or spectrum of the matrix, the number of iterations required is often much less than the size of the matrix. These algorithms often provide significant computational speedups for structured matrices that admit sub-quadratic MVMs (e.g. sparse, circulant, Toeplitz, etc.) or when using accelerated hardware (GPUs or TPUs) designed for efficient parallel MVMs .

**Multiple dispatch** Popularized by `Julia` , multiple dispatch is a functional programming paradigm for defining type-specific behaviors. Under this paradigm, a given function (e.g. `solve`) can have multiple definitions, each of which are specific to a particular set of input types. A base-case definition `solve[LinearOperator]` would use a generic matrix-vector solve algorithm (e.g. Gaussian elimination or GMRES), while a type-specific definition (e.g. `solve[Sum]`, for sums of matrices) would use a special purpose algorithm that makes use of the subclass’ structure (e.g. SVRG, see ). When a user calls $\texttt{solve}({\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{b}}})$ at runtime, the *dispatcher* determines which definition of `solve` to use based on the types of ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{b}}}$. Crucially, dispatch rules can be written for compositional patterns of types. For example, a `solve[Sum[LowRank, Diagonal]]` function will apply the Woodbury formula to a `Sum` operator that composes `LowRank` and `Diagonal` matrices. (In contrast, under an inheritance paradigm, one would need to define a specific `SumOfLowRankAndDiagonal` sub-class that uses the Woodbury formula, rather than relying on the composition of general purpose types.)

**Existing frameworks for exploiting structure** Achieving fast computations with structured matrices is often a manual effort. Consider for example the problems of second order/natural gradient optimization, which require matrix solves with (potentially large) Hessian/Fisher matrices. Researchers have proposed tackling these solves with matrix-free methods , diagonal approximations , low-rank approximations , or Kronecker-factorizable approximations . Despite their commonality—relying on structure for fast solves—all methods currently require different implementations, reducing interoperability and adding overhead to experimenting with new structured approximations. As an alternative, there are existing libraries like SciPy Sparse , Spot , PyLops , or GPyTorch , which offer a unified interface for using matrix-free algorithms with any type of structured matrices. A user provides an efficient MVM function for a given matrix and then chooses the appropriate iterative method (e.g. conjugate gradients or GMRES) to perform the desired operation (e.g. linear solve). With these libraries, a user can adapt to different structures simply by changing the MVM routine. However, this increased interoperability comes at the cost of efficiency, as the iterative routines are not optimal for every type of structure. (For example, Kronecker products admit efficient inverses that are asymptotically faster than conjugate gradients; see .) Moreover, these libraries often lack modern features (e.g. GPU acceleration or automatic differentiation) or are specific to certain types of matrices (see ).

# CoLA: Compositional Linear Algebra

We now discuss all the components that make CoLA. In we first describe the core MVM based `LinearOperator` abstraction, and in we discuss our core compositional framework for identifying and automatically exploiting structure for fast computations. In , we highlight how CoLA exploits structure frequently encountered in ML applications beyond well-known analytic formulae (e.g. the Woodbury identity). Finally, in we present CoLA’s machine learning-specific features, like automatic differentiation, support for low-precision, and hardware acceleration.

## Deriving Linear Algebraic Operations Through Fast MVMs

Borrowing from existing frameworks like `Scipy Sparse`, the central object of our framework is the `LinearOperator`: a linear function on a finite dimensional vector space, defined by how it acts on vectors via a matrix-vector multiply $\texttt{MVM}_A: {\boldsymbol{\mathbf{v}}} \mapsto {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{v}}}$. While this function has a matrix representation for a given basis, we do not need to store or compute this matrix to perform a MVM. Avoiding the dense representation of the operator saves memory and often compute.

Some basic examples of `LinearOperators` are: unstructured `Dense` matrices, which are represented by a 2-dimensional array and use the standard `MVM` routine $\left[{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{v}}}\right]_{i} = \sum_{j=1} A_{ij} v_{j}$; `Sparse` matrices, which can be represented by key/value arrays of the nonzero entries with the standard CSR-sparse `MVM` routine; `Diagonal` matrices, which are represented by a 1-dimensional array of the diagonal entries and where the `MVM` is given by $\left[{\boldsymbol{\mathbf{\texttt{Diag}}}}({\boldsymbol{\mathbf{d}}}) {\boldsymbol{\mathbf{v}}}\right]_{i} = d_i v_{i}$; `Convolution` operators, which are represented by a convolutional filter array and where the `MVM` is given by $\texttt{Conv}({\boldsymbol{\mathbf{a}}}) {\boldsymbol{\mathbf{v}}} = {\boldsymbol{\mathbf{a}}} * {\boldsymbol{\mathbf{v}}}$ ; or `JVP` operators—the Jacobian represented implicitly through an autograd Jacobian Vector Product—represented by a function and an input ${\boldsymbol{\mathbf{x}}}$ and where the `MVM` is given by $\texttt{Jacobian}(f,{\boldsymbol{\mathbf{x}}}){\boldsymbol{\mathbf{v}}} = \mathrm{JVP}(f,{\boldsymbol{\mathbf{x}}},{\boldsymbol{\mathbf{v}}})$. In CoLA, each of these examples are sub-classes of the `LinearOperator` superclass.

Through the `LinearOperator`’s `MVM`, it is possible to derive other linear algebraic operations. As a simple example, we obtain the dense representation of the `LinearOperator` by calling `MVM`$({\boldsymbol{\mathbf{e}}}_1)$, $\ldots$, `MVM`$({\boldsymbol{\mathbf{e}}}_N)$, on each unit vector ${\boldsymbol{\mathbf{e}}}_i$. We now describe several key operations supported by our framework, some well-established, and others novel to CoLA.

**Solves, eigenvalue problems, determinants, and functions of matrices** As a base case for larger matrices, CoLA uses *Krylov subspace methods* (, ) for many matrix operations. Specifically, we use GMRES for matrix solves and Arnoldi for finding eigenvalues, determinants, and functions of matrices. Both of these algorithms can be applied to any non-symmetric and/or complex linear operator. When `LinearOperator`s are annotated with additional structure (e.g. self-adjoint, positive semi-definite) we use more efficient Krylov algorithms like MINRES, conjugate gradients, and Lanczos (see ). As stated in , these algorithms are matrix free (and thus memory efficient), amenable to GPU acceleration, and asymptotically faster than dense methods. See for a full list of Krylov methods used by CoLA.

**Transposes and complex conjugations** In alternative frameworks like `Scipy Sparse` a user must manually define a transposed `MVM` ${\boldsymbol{\mathbf{v}}} \mapsto {\boldsymbol{\mathbf{A}}}^\intercal {\boldsymbol{\mathbf{v}}}$ for linear operator objects. In contrast, CoLA uses a novel autograd trick to derive the transpose from the core `MVM` routine. We note that ${\boldsymbol{\mathbf{A}}}^\intercal {\boldsymbol{\mathbf{v}}}$ is the vector-Jacobian product (VJP) of the vector ${\boldsymbol{\mathbf{v}}}$ and the Jacobian $\partial {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{w}}} / \partial {\boldsymbol{\mathbf{w}}}$. Thus, the function $\texttt{transpose}({\boldsymbol{\mathbf{A}}})$ returns a `LinearOperator` object that uses $\mathrm{VJP}(\texttt{MVM}_{{\boldsymbol{\mathbf{A}}}}, {\boldsymbol{\mathbf{0}}}, {\boldsymbol{\mathbf{v}}})$ as its `MVM`. We extend this idea to Hermitian conjugates, using the fact that ${\boldsymbol{\mathbf{A}}}^* {\boldsymbol{\mathbf{v}}} = (\overline{{\boldsymbol{\mathbf{A}}}})^\intercal {\boldsymbol{\mathbf{v}}} = \overline{({\boldsymbol{\mathbf{A}}}^\intercal \overline{{\boldsymbol{\mathbf{v}}}})}$.

**Other operations** In we outline how to stochastically compute diagonals and traces of operators with `MVM`s, and in we discuss a novel approach for computing memory-efficient derivatives of iterative methods through `MVM`s.

**Implementation** CoLA implements all operations (`solve`, `eig`, `logdet`, `transpose`, `conjugate`, etc.) following a functional programming paradigm rather than as methods of the `LinearOperator` object. This is not a minor implementation detail: as we demonstrate in the next section, it is crucial for the efficiency and compositional power of our framework.

## Beyond Fast MVMs: Exploiting Explicit Structure Using Composition Rules

<div class="center">

<div id="tab:ops_runtimes">

|              |     $\Pi_i^M {\boldsymbol{\mathbf{A}}}_i$     |  $\sum_i^M {\boldsymbol{\mathbf{A}}}_i$   | $\text{BlockDiag}({\boldsymbol{\mathbf{A}}},{\boldsymbol{\mathbf{B}}})$ | $\text{Kron}({\boldsymbol{\mathbf{A}}},{\boldsymbol{\mathbf{B}}})$ |
|:-------------|:---------------------------------------------:|:-----------------------------------------:|:-----------------------------------------------------------------------:|:------------------------------------------------------------------:|
| MVM ($\tau$) |                $\sum_i\tau_i$                 |              $\sum_i \tau_i$              |                             $\tau_A+\tau_B$                             |                       $\tau_AN_B+N_A\tau_B$                        |
| Solve ($s$)  | $\sum_i\kappa_i\tau_i\log\tfrac{M}{\epsilon}$ | $(1+\kappa/M)\tau\log\tfrac{1}{\epsilon}$ |                                $s_A+s_B$                                |                          $s_AN_B+N_As_B$                           |
| Eigs $(E)$   |  $\tau\log\tfrac{M}{\epsilon}\Pi_i \kappa_i$  | $(1+\kappa/M)\tau\log\tfrac{1}{\epsilon}$ |                                $E_A+E_B$                                |                             $E_A+E_B$                              |

**CoLA selects the best rates for each operation or structure combination.** Asymptotic runtimes resulting from dispatch rules on compositional linear operators in our framework. Listed operations are matrix vector multiplies, linear solves, and eigendecomposition. Here $\epsilon$ denotes error tolerance. For a given operator of size $N \times N$, we denote $\tau$ as its MVM cost, $s$ its linear solve cost, $E$ its eigendecomposition cost and $\kappa$ its condition number. A lower script indicates to which matrix the operation belongs to.

</div>

</div>

While the GMRES algorithm can compute solves more efficiently than corresponding dense methods such as the Cholesky decomposition, especially with GPU parallelization and preconditioning, it is not the most efficient algorithm for many `LinearOperators`. For example, if ${\boldsymbol{\mathbf{A}}} = \texttt{Diag}({\boldsymbol{\mathbf{a}}})$, then we know that ${\boldsymbol{\mathbf{A}}}^{-1} = \texttt{Diag}({\boldsymbol{\mathbf{a}}}^{-1})$ without needing to solve a linear system. Similarly, solves with triangular matrices can be inverted efficiently through back substitution, and solves with circulant matrices can be computed efficiently in the Fourier domain $\texttt{Conv}({\boldsymbol{\mathbf{a}}}) = \mathcal{F}^{-1}\texttt{Diag}(\mathcal{F} {\boldsymbol{\mathbf{a}}})\mathcal{F}$ (where $\mathcal{F}$ is the Fourier transform linear operator). We offer more examples in (left).

As described in , we use multiple dispatch to implement these special case methods. For example, we implement the `solve[Diagonal]`, `solve[Triangular]`, and `solve[Circulant]` dispatch rules using the efficient routines described above. If a specific `LinearOperator` subclass does not have a specific `solve` dispatch rule then we default to the base-case `solve` rule using GMRES. This behaviour also applies to other operations, such as `logdet`, `eig`, `diagonal`, etc.

The dispatch framework makes it easy to implement one-off rules for the basic `LinearOperator` sub-classes described in . However, its true power lies in the use of compositional rules, which we describe below.

**Compositional Linear Operators** In addition to the base `LinearOperator` sub-classes (e.g. `Sparse`, `Diagonal`, `Convolution`), our framework provides mechanisms to compose multiple `LinearOperator`s together. Some frequently used compositional structures are `Sum` ($\sum_i {\boldsymbol{\mathbf{A}}}_i$), `Product` ($\Pi_i {\boldsymbol{\mathbf{A}}}_i$), `Kronecker` (${\boldsymbol{\mathbf{A}}}\otimes {\boldsymbol{\mathbf{B}}}$), `KroneckerSum` (${\boldsymbol{\mathbf{A}}}\oplus {\boldsymbol{\mathbf{B}}}$), `BlockDiag` $[
{\boldsymbol{\mathbf{A}}}, 0; \:\:
0, {\boldsymbol{\mathbf{B}}}]$ and `Concatenation` $[{\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{B}}}]$. Each of these compositional `LinearOperator`s are defined by (1) the base `LinearOperator` objects to be composed, and (2) a corresponding `MVM` routine, which is typically written in terms of the `MVM`s of the composed `LinearOperators`. For example, $\texttt{MVM}_\texttt{Sum} = {\boldsymbol{\mathbf{v}}} \mapsto \sum_i \texttt{MVM}_i({\boldsymbol{\mathbf{v}}})$, where $\texttt{MVM}_i$ are the $\texttt{MVM}$ routines for the component `LinearOperator`s.

Dispatch rules for compositional operators are especially powerful. For example, consider `Kronecker` products where we have the rule $({\boldsymbol{\mathbf{A}}} \otimes {\boldsymbol{\mathbf{B}}})^{-1} =   {\boldsymbol{\mathbf{A}}}^{-1} \otimes {\boldsymbol{\mathbf{B}}}^{-1}$. Though simple, this rule yields highly efficient routines for numerous structures. For example, suppose we want to solve $({\boldsymbol{\mathbf{A}}} \otimes {\boldsymbol{\mathbf{B}}} \otimes {\boldsymbol{\mathbf{C}}}) {\boldsymbol{\mathbf{x}}} = {\boldsymbol{\mathbf{b}}}$ where ${\boldsymbol{\mathbf{A}}}$ is dense, ${\boldsymbol{\mathbf{B}}}$ is diagonal, and ${\boldsymbol{\mathbf{C}}}$ is triangular. From the rules, the solve would be split over the product, using GMRES for ${\boldsymbol{\mathbf{A}}}$, diagonal inversion for ${\boldsymbol{\mathbf{B}}}$, and forward substitution for ${\boldsymbol{\mathbf{C}}}$. This breakdown is much more efficient than the base case (GMRES with $\texttt{MVM}_\texttt{Kron}$).

When exploited to their full potential, these composition rules provide both asymptotic speedups (shown in ) as well as runtime improvements on real problems across practical sizes (shown in ). Splitting up the problem with composition rules yields speedups in surprising ways even in the fully iterative case. To illustrate, consider one large CG solve with the matrix power ${\boldsymbol{\mathbf{B}}}={\boldsymbol{\mathbf{A}}}^n$; in general, the runtime is upper-bounded by $O(n\tau \sqrt{\kappa^{n}} \log \tfrac{1}{\epsilon})$, where $\tau$ is the time for a MVM with ${\boldsymbol{\mathbf{A}}}$, $\kappa$ is the condition number of ${\boldsymbol{\mathbf{A}}}$, and $\epsilon$ is the desired error tolerance. However, splitting the product via a composition rule into a sequence of solves has a much smaller upper-bound of $O(n\tau\sqrt{\kappa}\log \tfrac{n}{\epsilon})$. We observe this speedup in the solving the Bi-Poisson PDE shown in (b).

**Additional flexibly and efficiency via parametric typing** A crucial advantage of multiple dispatch is the ability to write simple special rules for compositions of specific operators. While a general purpose `solve`\[Sum\] method (SVRG; see next section) yields efficiency over the GMRES base case, it is not the most efficient algorithm when the `Sum` operator is combining a `LowRank` and a `Diagonal` operator. In this case, the Woodbury formula would be far more efficient. To account for this, CoLA allows for dispatch rules on *parametric types*; that is, the user defines a $\texttt{solve[Sum[LowRank, Diagonal]]}$ dispatch rule that is used if the `Sum` operator is specifically combining a `LowRank` and a `Diagonal` linear operator. Coding these rules without multiple dispatch would require specialty defining sub-classes like `LowRankPlusDiagonal` over the `LinearOperator` object, increasing complexity and hampering extendibility.

**Decoration/annotation operators** Finally, we include several *decorator* types that annotate existing `LinearOperator`s with additional structure. For example, we define `SelfAdjoint` (Hermetian/symmetric), `Unitary` (orthonormal), and `PSD` (positive semi-definite) operators, each of which wraps an existing `LinearOperator` object. None of these decorators define a specialty `MVM`; however, these decorators can be used to define dispatch rules for increased efficiency. For example `solve[PSD]` can use conjugate gradients rather than GMRES, and `solve[PSD[Tridiagonal]]` can use the linear time tridiagonal Cholesky decomposition .

**Taken together** Our framework defines 16 base linear operators, 5 compositional linear operators, 6 decoration linear operators, and roughly 70 specialty dispatch rules for `solve`, `eig`, and other operations. (See for a short summary and for a complete list of rules.) We note that these numbers are relatively small compared with existing solutions yet—as we demonstrate in — these operators and dispatch rules are sufficient to match or exceed performance of specialty implementations in numerous applications. Finally, we note that CoLA is extensible by users in *both directions*. A user can write their own custom dispatch rules, either to (1) define a new `LinearOperator` and special dispatch rules for it, or (2) to define a new algebraic operation for all `LinearOperators`, and crucially this requires no changes to the original implementation.

## Exploiting Implicit Structure in Machine Learning Applications

So far we have discussed *explicit* matrix structures and composition rules for which there are simple analytic formulas easily found in well-known references . However, current large systems—especially those found in machine learning— often have *implicit structure* and special properties that yield additional efficiencies. In particular, many ML problems give rise to linear operators composed of large summations which are amenable to stochastic algorithms. Below we outline two impactful general purpose algorithms used in CoLA to exploit this implicit structure.

**Accelerating iterative algorithms on large sums with SVRG** Stochastic gradient descent (SGD) is widely used for optimizating problems with very large or infinite sums to avoid having to traverse the full dataset per iteration. Like Monte Carlo estimation, SGD is very quick to converge to a few decimal places but very slow to converge to higher accuracies. When an exact solution is required on a problem with a finite sum, the stochastic variance reduced gradient (SVRG) algorithm is much more compelling, converging on strongly convex problems (and many others) at an exponential rate, with runtime $O((1+\kappa/M)\log \tfrac{1}{\epsilon})$ where $\kappa$ is the condition number and $\epsilon$ is the desired accuracy. When the condition number and the number of elements in the sum is large, SVRG becomes a desirable alternative even to classical deterministic iterative algorithms such as CG or Lanczos whose runtimes are bounded by $O(\sqrt{\kappa} \log \tfrac{1}{\epsilon})$. shows the impact of using SVRG to exploit the structure of different linear operators that are composed of large sums.

**Stochastic diagonal and trace estimation with reduced variance** Another case where we exploit implicit structure is when estimating the trace or the diagonal of a linear operator. While collecting the diagonal for a dense matrix is a trivial task, it is a costly algorithm for an arbitrary `LinearOperator` defined only through its `MVM`—it requires computing $\texttt{Diag}({\boldsymbol{\mathbf{A}}}) = \sum_{i=1}^N e_i \odot {\boldsymbol{\mathbf{A}}} e_i$ where $\odot$ is the Hadamard (elementwise) product. If we need merely an approximation or unbiased estimate of the diagonal (or the sum of the diagonal), we can instead perform stochastic diagonal estimation $\overline{\texttt{Diag}}({\boldsymbol{\mathbf{A}}}) = \frac{1}{n}\sum_{j=1}^n z_j\odot {\boldsymbol{\mathbf{A}}} z_j$ where the $z_j \in \mathbb{R}^N$ are any randomly sampled probe vectors with covariance $I$. We extend this randomized estimator to use randomization both in the probes, and random draws from a sum when ${\boldsymbol{\mathbf{A}}} = \sum_{i=1}^M{\boldsymbol{\mathbf{A}}}_i$: $$\overline{\texttt{Diag}}\left(\textstyle{\sum}_{i=1}^M {\boldsymbol{\mathbf{A}}}_i\right) := \textstyle{\sum}_{ij}z_{ij}\odot {\boldsymbol{\mathbf{A}}}_i z_{ij}.$$ In we derive the variance of this estimator and we show that it converges faster than the base Hutchinson estimator when applied `Sum` structures. We validate empirically this analysis in .

## Automatic Differentiation and Machine Learning Readiness

**Memory efficient auto-differentiation** In ML applications, we want to backpropagate through operations like ${\boldsymbol{\mathbf{A}}}^{-1}$, $\texttt{Eigs}({\boldsymbol{\mathbf{A}}})$, $\texttt{Tr}({\boldsymbol{\mathbf{A}}})$, $\exp({\boldsymbol{\mathbf{A}}})$, $\log \mathrm{det}({\boldsymbol{\mathbf{A}}})$. To achieve this, in CoLA we define a novel concept of the gradient of a `LinearOperator` which we detail in . For routines like GMRES, SVRG, and Arnoldi, we utilize a custom backward pass that does not require backproagating through the iterations of these algorithms. This custom backward pass results in substantial memory savings (the computation graph does not have to store the intermediate iterations of these algorithms), which we demonstrate in ().

**Low precision linear algebra** By default, all routines in CoLA support the standard `float32` and `float64` precisions. Moreover, many CoLA routines also support `float16` and `bfloat16` half precision using algorithmic modifications for increased stability. In particular, we use variants of the GMRES, Arnoldi, and Lanczos iterations that are less susceptible to instabilities that arise through orthogonalization and we use the half precision variant of conjugate gradients introduced by . See for further details.

**Multi framework support and GPU/TPU acceleration** CoLA is compatible with both PyTorch and JAX. This compatibility not only makes our framework *plug-and-play* with existing implemented models, but it also adds GPU/TPU support, differentiating it from existing solutions (see ). CoLA’s iterative algorithms are the class of linear algebra algorithms that benefit most from hardware accelerators as the main bottleneck of these algorithms are the MVMs executed at each iteration, which can easily be parallelized on hardware such as GPUs. empirically shows the additional impact of hardware accelerators across different datasets and linear algebra operations.

# Applications

We now apply CoLA to an extensive list of applications showing the impact, value and broad applicability of our numerical linear algebra framework, as illustrated in . This list of applications encompasses PCA, linear regression, Gaussian processes, spectral clustering, and partial differential equations like the Schrödinger equation or minimal surface problems. In contrast to ( & ), the applications presented here have a basic structure (sparse, vector-product, etc) but not a compositional structure (Kronecker, product, block diagonal, etc). We choose these applications due to their popularity and heterogeneity (the linear operators have different properties: self-adjoint, positive definite, symmetric and non-symmetric), and to show that CoLA performs in any application. We compare against several well-known libraries, sometimes providing runtime improvements but other times performing equally. This is remarkable as our numerical framework does not specialize in any of those applications (like `GPyTorch`) nor does it rely on Fortran implementations of high-level algorithms (like `sklearn` or `SciPy`). Below we describe each of the applications found in .

**Principal Component Analysis** PCA is a classical ML technique that finds the directions in the data that capture the most variance. PCA can be performed by computing the right singular vectors of ${\boldsymbol{\mathbf{X}}} \in \mathbb{R}^{N\times D}$. When the number of data points $N$ is very large, stochastic methods like SVRG in VR-PCA can accelerate finding the eigenvectors over SVD or Lanczos, as shown in (a).

**Spectral Clustering** Spectral clustering finds clusters of individual nodes in a graph by analyzing the graph Laplacian ${\boldsymbol{\mathbf{L}}} = {\boldsymbol{\mathbf{D}}} - {\boldsymbol{\mathbf{W}}}$ where ${\boldsymbol{\mathbf{D}}}$ denotes a diagonal matrix containing the degree of the nodes and ${\boldsymbol{\mathbf{W}}}$ the weights on the edges between nodes. This problem requires finding the smallest $k$ eigenvectors of ${\boldsymbol{\mathbf{L}}}$. We run this experiment on the high energy physics arXiv paper citation graph (cit-HepPh).

**Gaussian processes** GPs are flexible nonparametric probabilistic models where inductive biases are expressed through a covariance (kernel) function. At its core, training a GP involves computing and taking gradients of the log determinant of a kernel $\log \left|{\boldsymbol{\mathbf{K}}}\right|$ and of a quadratic term ${\boldsymbol{\mathbf{y}}}^{T} {\boldsymbol{\mathbf{K}}}^{-1} {\boldsymbol{\mathbf{y}}}$ (where ${\boldsymbol{\mathbf{y}}}$ is the vector of observations).

**Schrödinger Equation** In this problem we characterize the spectrum of an atom or molecule by finding the eigenspectrum of a PDE operator in a Schrodinger equation ${\boldsymbol{\mathbf{H}}}\psi = E\psi$. After discretizing $\psi$ to a grid, we compute the smallest eigenvalues and eigenvectors of the operator ${\boldsymbol{\mathbf{H}}}$ which for this experiment is non-symmetric as we perform a compactfying transform.

**Minimal Surface** Here we solve a set of nonlinear PDEs with the objective of finding the surface that locally minimizes its area under given boundary constraints. When applied to the graph of a function, the PDE can be expressed as $f(z)=(1+z_x^2)z_{yy}-2z_xz_yz_{xy}+(1+z_y^2)z_{xx}=0$ and solved by root finding on a discrete grid. Applying Newton-Raphson, we iteratively solve the non-symmetric linear system $z \gets  z-{\boldsymbol{\mathbf{J}}}^{-1}f(z)$ where ${\boldsymbol{\mathbf{J}}}$ is the Jacobian of the PDE operator.

**Bi-Poisson Equation** The Bi-Poisson equation $\Delta^2 u = \rho$ is a linear boundary value PDE relevant in continuum mechanics, where $\Delta$ is the Laplacian. When discretized using a grid, the result is a large symmetric system to be solved. We show speedups from the product structure in (b).

**Neural PDEs** Neural networks show promise for solving high dimensional PDEs. One approach for initial value problems requires advancing an ODE on the neural network parameters $\theta$, where $\dot{\theta} = {\boldsymbol{\mathbf{M}}}(\theta)^{-1}F(\theta)$ where ${\boldsymbol{\mathbf{M}}}$ is an operator defined from Jacobian of the neural network which decomposes as the sum over data points ${\boldsymbol{\mathbf{M}}} = \tfrac{1}{N}\sum_i {\boldsymbol{\mathbf{M}}}_i$ and where $F$ is determined by the governing dynamics of the PDE . By leveraging the sum structure with SVRG, we provide further speedups over as shown in (c).

**Equivariant Neural Network Construction** As shown in , constructing the equivariant layers of a neural network for a given data type and symmetry group is equivalent to finding the nullspace of a large linear equivariance constraint ${\boldsymbol{\mathbf{C}}} {\boldsymbol{\mathbf{v}}}={\boldsymbol{\mathbf{0}}}$, where the constraint matrix ${\boldsymbol{\mathbf{C}}}$ is highly structured, being a block diagonal matrix of concatenated Kronecker products and Kronecker sums of sparse matrices. In (c) we show the empirical benefits of exploiting this structure.

# Discussion

We have presented the CoLA framework for structure-aware linear algebraic operations in machine learning applications and beyond. Building on top of dense and iterative algorithms, we leverage explicit composition rules via multiple dispatch to achieve algorithmic speedups across a wide variety of practical applications. Algorithms like SVRG and a novel variation of Hutchinson’s diagonal estimator exploit implicit structure common to large-scale machine learning problems. Finally, CoLA supports many features necessary for machine learning research and development, including memory efficient automatic differentiation, multi-framework support of both JAX and PyTorch, hardware acceleration, and lower precision.

While structure exploiting methods are used across different application domains, domain knowledge often does not cross between communities. We hope that our framework brings these disparate communities and ideas together, enabling rapid development and reducing the burden of deploying fast methods for linear algebra at scale. Much like how automatic differentiation simplified and accelerated the training of machine learning models—with custom autograd functions as the exception rather than the rule—CoLA has the potential to streamline scalable linear algebra.

# Acknowledgements

This work is supported by NSF Award 1922658, NSF CAREER IIS-2145492, BigHat Biosciences, Capital One, and an Amazon Research Award.

# Appendix Outline

This Appendix is organized as follows:

- In we describe various dispatch rules including the base rules, the composition rules and rules derived from other rules.

- In we provide an extended discussion of several noteworthy features of CoLA, such as doubly stochastic estimators and memory-efficient autograd implementation.

- In we include pseudo-code on various of the iterative methods incorporated in CoLA and discuss modifications to improve lower precision performance.

- In we expand on the details of the experiments in the main text.

# Dispatch Rules

We now present the linear algebra identities that we use to exploit structure in CoLA.

## Core Functions

### Inverses

We incorporate several identities for the compositional operators: product, Kronecker product, block diagonal and sum. For product we have $({\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}})^{-1} = ({\boldsymbol{\mathbf{B}}}^{-1}{\boldsymbol{\mathbf{A}}}^{-1})$ and for Kronecker product we have $({\boldsymbol{\mathbf{A}}}\otimes {\boldsymbol{\mathbf{B}}})^{-1} = {\boldsymbol{\mathbf{A}}}^{-1}\otimes {\boldsymbol{\mathbf{B}}}^{-1}$. In terms of block compositions we have the following identities:

$$\begin{split}
      {\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix} }^{-1}
        =
        \begin{bmatrix} {\boldsymbol{\mathbf{A}}}^{-1} & {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}}^{-1} \end{bmatrix}
          \quad \text{and} \quad
      {\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{B}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix}}^{-1} = {\begin{bmatrix} {\boldsymbol{\mathbf{A}}}^{-1} & -{\boldsymbol{\mathbf{A}}}^{-1}{\boldsymbol{\mathbf{B}}}{\boldsymbol{\mathbf{D}}}^{-1} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}}^{-1}\end{bmatrix}}
    \end{split}$$ $$\begin{split}
{\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{B}}} \\ {\boldsymbol{\mathbf{C}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix}}^{-1}
  =
  {\begin{bmatrix} {\boldsymbol{\mathbf{I}}} & -{\boldsymbol{\mathbf{A}}}^{-1}{\boldsymbol{\mathbf{B}}}\\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{I}}}\end{bmatrix}}
  {\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{0}}}\\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}}-{\boldsymbol{\mathbf{C}}}{\boldsymbol{\mathbf{A}}}^{-1}{\boldsymbol{\mathbf{B}}} \end{bmatrix}}^{-1}
  {\begin{bmatrix} {\boldsymbol{\mathbf{I}}}& {\boldsymbol{\mathbf{0}}}\\ -{\boldsymbol{\mathbf{C}}}{\boldsymbol{\mathbf{A}}}^{-1} & {\boldsymbol{\mathbf{I}}} \end{bmatrix}}
    \end{split}$$

Finally, for sum we have the Woodbury identity and its variants. Namely, for Woodbury we have $$\begin{split}
      \left({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{U}}} {\boldsymbol{\mathbf{B}}} {\boldsymbol{\mathbf{V}}}\right)^{-1}
      =
      {\boldsymbol{\mathbf{A}}}^{-1} -
      {\boldsymbol{\mathbf{A}}}^{-1} {\boldsymbol{\mathbf{U}}} \left({\boldsymbol{\mathbf{B}}}^{-1} + {\boldsymbol{\mathbf{V}}} {\boldsymbol{\mathbf{A}}}^{-1} {\boldsymbol{\mathbf{U}}}\right)^{-1} {\boldsymbol{\mathbf{V}}} {\boldsymbol{\mathbf{A}}}^{-1}
      ,
    \end{split}$$ the Kailath variant where $$\begin{split}
      \left({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{B}}} {\boldsymbol{\mathbf{C}}}\right)^{-1}
      =
      {\boldsymbol{\mathbf{A}}}^{-1}
      -
      {\boldsymbol{\mathbf{A}}}^{-1}{\boldsymbol{\mathbf{B}}}\left({\boldsymbol{\mathbf{I}}} + {\boldsymbol{\mathbf{C}}} {\boldsymbol{\mathbf{A}}}^{-1} {\boldsymbol{\mathbf{B}}}\right){\boldsymbol{\mathbf{C}}} {\boldsymbol{\mathbf{A}}}^{-1}
    \end{split}$$ and the rank one update via the Sherman-Morrison formula $$\begin{split}
      \left({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{b}}} {\boldsymbol{\mathbf{c}}}^{\intercal}\right)^{-1}
      =
      {\boldsymbol{\mathbf{A}}}^{-1} -
      \frac{1}{1 + {\boldsymbol{\mathbf{c}}}^{\intercal} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{b}}}}
      {\boldsymbol{\mathbf{A}}}^{-1}{\boldsymbol{\mathbf{b}}} {\boldsymbol{\mathbf{c}}}^{\intercal} {\boldsymbol{\mathbf{A}}}^{-1}
      .
    \end{split}$$

Besides the compositional operators, we have some rules for some special operators. For example, for ${\boldsymbol{\mathbf{A}}} = \texttt{Diag}\left({\boldsymbol{\mathbf{a}}}\right)$ we have ${\boldsymbol{\mathbf{A}}}^{-1} = \texttt{Diag}\left({\boldsymbol{\mathbf{a}}}^{-1}\right)$. Also, if ${\boldsymbol{\mathbf{Q}}}$ is unitary then ${\boldsymbol{\mathbf{Q}}}^{-1} = {\boldsymbol{\mathbf{Q}}}^{*}$ or if ${\boldsymbol{\mathbf{Q}}}$ is orthonormal then ${\boldsymbol{\mathbf{Q}}}^{-1} = {\boldsymbol{\mathbf{Q}}}^{\intercal}$.

### Eigendecomposition

We now assume that the matrices in this section are diagonalizable. That is, $\texttt{Eigs}\left({\boldsymbol{\mathbf{A}}}\right)= {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}}, {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}}$, where ${\boldsymbol{\mathbf{A}}} = {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}} {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}} {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}}^{-1}$. In terms of the compositional operators, there is not a general rule for product or sum. However, for the Kronecker product we have $\texttt{Eigs}({\boldsymbol{\mathbf{A}}}\otimes {\boldsymbol{\mathbf{B}}}) = {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}} \otimes {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{B}}}},\ {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}} \otimes {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{B}}}}$ and for the Kronecker sum we have $\texttt{Eigs}({\boldsymbol{\mathbf{A}}}\oplus {\boldsymbol{\mathbf{B}}}) = {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}} \oplus {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{B}}}},\ {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}} \otimes {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{B}}}}$. Finally, for block diagonal we have $$\begin{split}
    \texttt{Eigs}\bigg({\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix} }\bigg)
    =
      \begin{bmatrix} {\boldsymbol{\mathbf{\Lambda_A}}} & {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{\Lambda_D}}} \end{bmatrix}, \ \begin{bmatrix} {\boldsymbol{\mathbf{V_A}}} & {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{V_D}}} \end{bmatrix}
  .
    \end{split}$$

### Diagonal

As a base case, if we need to compute $\texttt{Diag}\left({\boldsymbol{\mathbf{A}}}\right)$ for a general matrix ${\boldsymbol{\mathbf{A}}}$ we may compute each diagonal element by ${\boldsymbol{\mathbf{e}}}_{i}^{\intercal}{\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{e}}}_{i}$. Additionally, if ${\boldsymbol{\mathbf{A}}}$ is large enough we switch to randomized estimation $\texttt{Diag}({\boldsymbol{\mathbf{A}}}) \approx( {\boldsymbol{\mathbf{Z}}} \odot {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{Z}}})\mathbf{1}/N$ with ${\boldsymbol{\mathbf{Z}}} \sim \mathcal{N}(0,1)^{d\times N}$ where $N$ is the number of samples used to approximate the diagonal. In terms of compositional operators, we have that for sum $\texttt{Diag}\left({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{B}}}\right) = \texttt{Diag}\left({\boldsymbol{\mathbf{A}}}\right) + \texttt{Diag}\left({\boldsymbol{\mathbf{B}}}\right)$. For Kronecker product we have $\texttt{Diag}({\boldsymbol{\mathbf{A}}} \otimes {\boldsymbol{\mathbf{B}}}) = \texttt{vec}\big(\texttt{Diag}({\boldsymbol{\mathbf{A}}})\texttt{Diag}({\boldsymbol{\mathbf{B}}})^{\intercal}\big)$ and for Kronecker sum $\texttt{Diag}({\boldsymbol{\mathbf{A}}} \oplus {\boldsymbol{\mathbf{B}}}) = \texttt{vec}\big(\texttt{Diag}\left({\boldsymbol{\mathbf{A}}}\right)\mathbf{1}^\intercal + \mathbf{1}\texttt{Diag}\left({\boldsymbol{\mathbf{B}}}\right)^\intercal\big)$. Finally, for block composition we have $$\begin{split}
      \texttt{Diag}\bigg({\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{B}}} \\ {\boldsymbol{\mathbf{C}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix}}\bigg)
        =
        [\texttt{Diag}({\boldsymbol{\mathbf{A}}}),\texttt{Diag}({\boldsymbol{\mathbf{D}}})]
        .
    \end{split}$$

### Transpose / Adjoint

As explained in , as a base case we have an automatic procedure to compute the transpose or adjoint of any operator ${\boldsymbol{\mathbf{A}}}$ via autodiff. However, we also incorporate the following rules. For sum we have $\left({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{B}}}\right)^{*} = {\boldsymbol{\mathbf{A}}}^{*} + {\boldsymbol{\mathbf{B}}}^{*}$ and $\left({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{B}}}\right)^{\intercal} = {\boldsymbol{\mathbf{A}}}^{\intercal} + {\boldsymbol{\mathbf{B}}}^{\intercal}$. For product we have $\left({\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}\right)^{*} = {\boldsymbol{\mathbf{B}}}^{*}{\boldsymbol{\mathbf{A}}}^{*}$ and $\left({\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}\right)^{\intercal} = {\boldsymbol{\mathbf{B}}}^{\intercal} {\boldsymbol{\mathbf{A}}}^{\intercal}$. For Kronecker product we have $\left({\boldsymbol{\mathbf{A}}} \otimes {\boldsymbol{\mathbf{B}}}\right)^{*} = {\boldsymbol{\mathbf{A}}}^{*} \otimes {\boldsymbol{\mathbf{B}}}^{*}$ and $\left({\boldsymbol{\mathbf{A}}} \otimes {\boldsymbol{\mathbf{B}}}\right)^{\intercal} = {\boldsymbol{\mathbf{A}}}^{\intercal} \otimes {\boldsymbol{\mathbf{B}}}^{\intercal}$. For the Kronecker sum we have $\left({\boldsymbol{\mathbf{A}}} \oplus {\boldsymbol{\mathbf{B}}}\right)^{*} = {\boldsymbol{\mathbf{A}}}^{*} \oplus {\boldsymbol{\mathbf{B}}}^{*}$ and $\left({\boldsymbol{\mathbf{A}}} \oplus {\boldsymbol{\mathbf{B}}}\right)^{\intercal} = {\boldsymbol{\mathbf{A}}}^{\intercal} \oplus {\boldsymbol{\mathbf{B}}}^{\intercal}$. In terms of block composition we have $$\begin{split}
      \bigg({\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{B}}} \\ {\boldsymbol{\mathbf{C}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix} }\bigg)^{*}
        =
      {\begin{bmatrix} {\boldsymbol{\mathbf{A}}}^{*} & {\boldsymbol{\mathbf{C}}}^{*} \\ {\boldsymbol{\mathbf{B}}}^{*} & {\boldsymbol{\mathbf{D}}}^{*} \end{bmatrix} }
        \quad \text{and} \quad
      \bigg({\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{B}}} \\ {\boldsymbol{\mathbf{C}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix} }\bigg)^{\intercal}
        =
      {\begin{bmatrix} {\boldsymbol{\mathbf{A}}}^{\intercal} & {\boldsymbol{\mathbf{C}}}^{\intercal} \\ {\boldsymbol{\mathbf{B}}}^{\intercal} & {\boldsymbol{\mathbf{D}}}^{\intercal} \end{bmatrix} }
        .
    \end{split}$$ Finally for the annotated operators we have the following rules. ${\boldsymbol{\mathbf{A}}}^{*} = {\boldsymbol{\mathbf{A}}}$ if ${\boldsymbol{\mathbf{A}}}$ is self-adjoint and ${\boldsymbol{\mathbf{A}}}^{\intercal} = {\boldsymbol{\mathbf{A}}}$ if ${\boldsymbol{\mathbf{A}}}$ is symmetric.

### Pseudo-inverse

As a base case, if we need to compute ${\boldsymbol{\mathbf{A}}}^{+}$, we may use $\texttt{SVD}\left({\boldsymbol{\mathbf{A}}}\right)= {\boldsymbol{\mathbf{U}}}, {\boldsymbol{\mathbf{\Sigma}}}, {\boldsymbol{\mathbf{V}}}$ and therefore set ${\boldsymbol{\mathbf{A}}}^{+} = {\boldsymbol{\mathbf{U}}}{\boldsymbol{\mathbf{\Sigma}}}^{+} {\boldsymbol{\mathbf{V}}}^{*}$, where ${\boldsymbol{\mathbf{\Sigma}}}^{+}$ inverts the nonzero diagonal scalars. If the size of ${\boldsymbol{\mathbf{A}}}$ is too large, then we may use randomized SVD. Yet, it is uncommon to simply want ${\boldsymbol{\mathbf{A}}}^{+}$, usually we want to solve a least-squares problem and therefore we can use solvers that are not as expensive to run as SVD. For the compositional operators we have the following identities. For product $\left({\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}\right)^{+} = \left({\boldsymbol{\mathbf{A}}}^{+} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}\right)^{+} \left({\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{B}}} {\boldsymbol{\mathbf{B}}}^{+}\right)^{+}$ and for Kronecker product we have $\left({\boldsymbol{\mathbf{A}}} \otimes {\boldsymbol{\mathbf{B}}}\right)^{+} = {\boldsymbol{\mathbf{A}}}^{+} \otimes {\boldsymbol{\mathbf{B}}}^{+}$. For block diagonal we have $$\begin{split}
      \bigg({\begin{bmatrix} {\boldsymbol{\mathbf{A}}} & {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}} \end{bmatrix} }\bigg)^{+}
        =
      \begin{bmatrix} {\boldsymbol{\mathbf{A}}}^{+} & {\boldsymbol{\mathbf{0}}} \\ {\boldsymbol{\mathbf{0}}} & {\boldsymbol{\mathbf{D}}}^{+} \end{bmatrix}
  .
    \end{split}$$ Finally, we have some identities that are mathematically trivial but that are necessary when recursively exploiting structure as that would save computation. For example, if ${\boldsymbol{\mathbf{Q}}}$ is unitary we know that ${\boldsymbol{\mathbf{Q}}}^{+} = {\boldsymbol{\mathbf{Q}}}$ and similarly when ${\boldsymbol{\mathbf{Q}}}$ is orthonormal. If ${\boldsymbol{\mathbf{A}}}$ is self-adjoint, then ${\boldsymbol{\mathbf{A}}}^{+} = {\boldsymbol{\mathbf{A}}}^{-1}$ and also if it is symmetric and PSD.

## Derived Functions

Interestingly, the previous core functions allow us to derive multiple rules from the previous ones. To illustrate, we have that $\texttt{Tr}\left({\boldsymbol{\mathbf{A}}}\right) = \sum_{i}^{} \texttt{Diag}\left({\boldsymbol{\mathbf{A}}}\right)_{i}$. Additionally, if ${\boldsymbol{\mathbf{A}}}$ is PSD we have that $f\left({\boldsymbol{\mathbf{A}}}\right) = {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}} f\left({\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}}\right) {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}}^{-1}$ and if ${\boldsymbol{\mathbf{A}}}$ is both symmetric and PSD then $f\left({\boldsymbol{\mathbf{A}}}\right) = {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}} f\left({\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}}\right) {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}}^{\intercal}$. where in both cases we used $\texttt{Eigs}\left({\boldsymbol{\mathbf{A}}}\right) = {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}}, {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}}$. Some example functions for PSD matrices are $\texttt{Sqrt}\left({\boldsymbol{\mathbf{A}}}\right) = {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}} {\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}}^{1/2} {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}}^{-1}$ or $\texttt{Log}\left({\boldsymbol{\mathbf{A}}}\right) = {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}} \log{\boldsymbol{\mathbf{\Lambda}}}_{{\boldsymbol{\mathbf{A}}}} {\boldsymbol{\mathbf{V}}}_{{\boldsymbol{\mathbf{A}}}}^{-1}$. Which also this rules allow us to define $\texttt{LogDet}\left({\boldsymbol{\mathbf{A}}}\right) = \texttt{Tr}\left(\texttt{Log}\left({\boldsymbol{\mathbf{A}}}\right)\right)$.

## Other matrix identities

We emphasize that there are a myriad more matrix identities that we do not intentionally include such as $\texttt{Tr}({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{B}}}) = \texttt{Tr}({\boldsymbol{\mathbf{A}}}) + \texttt{Tr}({\boldsymbol{\mathbf{B}}})$ or $\texttt{Tr}({\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{B}}}) = \texttt{Tr}({\boldsymbol{\mathbf{B}}} {\boldsymbol{\mathbf{A}}})$ when ${\boldsymbol{\mathbf{A}}}$ and ${\boldsymbol{\mathbf{B}}}$ are squared. These additional cases are not part of our dispatch rules as either they are automatically computed from other rules (as in the first example) or they do not yield any computational savings (as in the second example).

# Features in CoLA

## Doubly stochastic diagonal and trace estimation

**Singly Stochastic Trace Estimator** Consider the traditional stochastic trace estimator: $$\overline{\texttt{Tr}}[\texttt{Base}]({\boldsymbol{\mathbf{A}}}) = \tfrac{1}{n}\sum_{j=1}^n {\boldsymbol{\mathbf{z}}}^{\intercal}_j {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{z}}}_j$$ with each ${\boldsymbol{\mathbf{z}}}_{j} \sim \mathcal{N}({\boldsymbol{\mathbf{0}}},{\boldsymbol{\mathbf{I}}}_{D})$ where ${\boldsymbol{\mathbf{A}}}$ is a $D\times D$ matrix. When ${\boldsymbol{\mathbf{A}}}$ is itself a sum ${\boldsymbol{\mathbf{A}}} = \tfrac{1}{m}\sum_{i=1}^m {\boldsymbol{\mathbf{A}}}_i$, we can expand the trace as $\overline{\texttt{Tr}}[\texttt{Base}]({\boldsymbol{\mathbf{A}}}) = \tfrac{1}{mn}\sum_{j=1}^n\sum_{i=1}^m {\boldsymbol{\mathbf{z}}}^{\intercal}_j {\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{z}}}_j$, with probe variables shared across elements of the sum.

Consider the quadratic form $Q:={\boldsymbol{\mathbf{z}}}^{\intercal} {\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{z}}}$, which for Gaussian random variables has a cumulant generating function of $K_Q(t) = \log \mathbb{E}[e^{tQ}] = -\tfrac{1}{2} \log \mathrm{det}({\boldsymbol{\mathbf{I}}}-2t{\boldsymbol{\mathbf{A}}})$. From the generating function we can derive the mean and variance of this estimator: $\mathbb{E}[Q] = K_Q'(0) = \mathrm{Tr}({\boldsymbol{\mathbf{A}}})$ and $\mathrm{Var}[Q] = K_Q''(0) = 2 \mathrm{Tr}({\boldsymbol{\mathbf{A}}}^2)$. Since $\overline{\texttt{Tr}}[\texttt{Base}]({\boldsymbol{\mathbf{A}}})$ is a sum of independent random draws of $Q$, we see: $$\mathbb{E}\big[\overline{\texttt{Tr}}[\texttt{Base}]({\boldsymbol{\mathbf{A}}})\big] = \mathrm{Tr}({\boldsymbol{\mathbf{A}}}) \quad \mathrm{and}\quad \mathrm{Var}\big[\overline{\texttt{Tr}}[\texttt{Base}]({\boldsymbol{\mathbf{A}}})\big] = \frac{2}{n} \mathrm{Tr}({\boldsymbol{\mathbf{A}}}^2).$$

**Doubly Stochastic Trace Estimator** For the doubly stochastic estimator, we choose probe variables which are sampled independently for each element of the sum: $$\overline{\texttt{Tr}}[\texttt{Sum}]({\boldsymbol{\mathbf{A}}}) = \tfrac{1}{nm}\sum_{j=1}^n \sum_{i=1}^m {\boldsymbol{\mathbf{z}}}^{\intercal}_{ij} {\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{z}}}_{ij}.$$ Separating out the elements of the sum, we can write the estimator as $\overline{\texttt{Tr}}[\texttt{Sum}]({\boldsymbol{\mathbf{A}}}) = \tfrac{1}{n}\sum_{j=1}^n R_j$ where $R_j$ are independent random samples of the value $R = \tfrac{1}{m} \sum_{i=1}^m {\boldsymbol{\mathbf{z}}}_i^{\intercal} {\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{z}}}_i$. The cumulant generating function is merely $K_R(t) = \sum_{i=1}^m K_{Q_i}(t/m)$ where $Q_i = {\boldsymbol{\mathbf{z}}}^{\intercal} {\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{z}}}$. Taking derivatives we find that, $$\mathbb{E}[R] = K_R'(0) = \tfrac{1}{m}\sum_{i=1}^m \mathrm{Tr}({\boldsymbol{\mathbf{A}}}_i) =  \mathrm{Tr}({\boldsymbol{\mathbf{A}}}),$$ $$\textrm{Var}[R] = K_R''(0) = \tfrac{1}{m^2}\sum_{i=1}^m 2\mathrm{Tr}({\boldsymbol{\mathbf{A}}}_i^2) =\tfrac{2}{m}\mathrm{Tr}(\tfrac{1}{m}\sum_{i=1}^m {\boldsymbol{\mathbf{A}}}_i^2)$$ Assuming bounded moments on ${\boldsymbol{\mathbf{A}}}_i$, then both ${\boldsymbol{\mathbf{A}}} = \tfrac{1}{m}\sum_i {\boldsymbol{\mathbf{A}}}_i$ and $S({\boldsymbol{\mathbf{A}}}) = \tfrac{1}{m}\sum_i {\boldsymbol{\mathbf{A}}}_i^2$ will converge to fixed values as $m \rightarrow \infty$. Given that $\overline{\texttt{Tr}}[\texttt{Sum}]({\boldsymbol{\mathbf{A}}}) = \tfrac{1}{n}\sum_{j=1}^n R_j$, we can now write the mean and variance of the doubly stochastic estimator: $$\mathbb{E}\big[\overline{\texttt{Tr}}[\texttt{Sum}]({\boldsymbol{\mathbf{A}}})\big] = \mathrm{Tr}({\boldsymbol{\mathbf{A}}}) \quad \mathrm{and}\quad \mathrm{Var}\big[\overline{\texttt{Tr}}[\texttt{Sum}]({\boldsymbol{\mathbf{A}}})\big] = \frac{2}{mn} \mathrm{Tr}(S({\boldsymbol{\mathbf{A}}})).$$

As the error of the estimator can be bounded by the square root of the variance, showing that while the error for $\overline{\texttt{Tr}}[\texttt{Base}]$ is $O(1/\sqrt{n})$ (even when applied to sum structures), whereas the error for $\overline{\texttt{Tr}}[\texttt{Sum}]$ is $O(1/\sqrt{nm})$, a significant asymptotic variance reduction.

The related stochastic diagonal estimator $$\overline{\texttt{Diag}}[\texttt{Sum}]({\boldsymbol{\mathbf{A}}}) = \tfrac{1}{nm}\sum_{j=1}^n \sum_{i=1}^m {\boldsymbol{\mathbf{z}}}_{ij}\odot {\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{z}}}_{ij}.$$ achieves the same $O(1/\sqrt{nm})$ convergence rate, though we omit this derivation for brevity as it is follows the same steps.

In we empirically how our doubly stochastic diagonal estimator outperforms the standard Hutchinson estimator.

## Autograd rules for iterative algorithms

For machine learning applications, we want to seamlessly interweave linear algebra operations with automatic differentiation. The most basic strategy is to simply let the autograd engine trace through the operations and backpropagate accordingly. However, when using iterative methods like conjugate gradients or Lanczos, this naive approach is extremely memory inefficient and, for problems with many iterations, the cost can be prohibitive (as seen in ). However, the linear algebra operations corresponding to inverse, eigendecomposition and trace estimation have simple closed form derivatives which we can implement to avoid the prohibitive memory consumption and reduce runtime.

Simply put, for an operation like $f = \texttt{CGSolve}$, $\texttt{CGSolve}({\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{b}}}) = {\boldsymbol{\mathbf{A}}}^{-1} {\boldsymbol{\mathbf{b}}}$ we must define a Vector Jacobian Product: $\texttt{VJP}(f, ({\boldsymbol{\mathbf{A}}}, {\boldsymbol{\mathbf{b}}}), {\boldsymbol{\mathbf{v}}}) = \big({\boldsymbol{\mathbf{v}}}^{\intercal} \frac{\partial f}{\partial {\boldsymbol{\mathbf{A}}}}, {\boldsymbol{\mathbf{v}}}^{\intercal} \frac{\partial f}{\partial {\boldsymbol{\mathbf{b}}}}\big)$. However, for matrix-free linear operators, we cannot afford to store the dense matrix ${\boldsymbol{\mathbf{A}}}$, and thus neither can we store the gradients with respect to each of its elements! Instead we must (recursively) consider how the linear operator was constructed in terms of its differentiable arguments. In other words, we must flatten the tree structure of possibly nested differentiable arguments into a vector: $\theta = \texttt{flatten}[{\boldsymbol{\mathbf{A}}}]$. For example for ${\boldsymbol{\mathbf{A}}} = \texttt{Kron}\big(\texttt{Diag}(\theta_1), \texttt{Conv}(\theta_2)\big)$, $\texttt{flatten}[{\boldsymbol{\mathbf{A}}}] = [\theta_1,\theta_2]$. From this perspective, we consider ${\boldsymbol{\mathbf{A}}}$ as a container or tree of its arguments $\theta$, and define $v^{\intercal} \frac{\partial f}{\partial {\boldsymbol{\mathbf{A}}}}:= \texttt{unflatten}[v^{\intercal} \frac{\partial f}{\partial {\boldsymbol{\mathbf{\theta}}}}]$ which coincides with the usual definition for dense matrices. Applying to inverses, we can now write a simple $\texttt{VJP}$: $${\boldsymbol{\mathbf{v}}}^\intercal\tfrac{\partial f}{\partial {\boldsymbol{\mathbf{A}}}}
    =
    \texttt{unflatten}[\texttt{VJP}\big(\theta \mapsto \texttt{unflatten}(\theta) {\boldsymbol{\mathbf{A}}}^{-1} {\boldsymbol{\mathbf{b}}}, \theta, {\boldsymbol{\mathbf{A}}}^{-1} {\boldsymbol{\mathbf{v}}}\big)]$$ for ${\boldsymbol{\mathbf{v}}}^{\intercal}\frac{\partial f}{\partial {\boldsymbol{\mathbf{\theta}}}} =
{\boldsymbol{\mathbf{v}}}^{\intercal} ({\boldsymbol{\mathbf{A}}}^{-1})^{\intercal}(\partial_\theta {\boldsymbol{\mathbf{A}}}_\theta)  {\boldsymbol{\mathbf{A}}}^{-1}{\boldsymbol{\mathbf{b}}}$, and we will adopt this notation below for brevity. Doing so gives a memory cost which is constant in the number of solver iterations, and proportional to the memory used in the forward pass. Below we list the autograd rules for some of the iterative routines that we implement in CoLA with their `VJP` definitions.

1.  ${\boldsymbol{\mathbf{y}}}=\texttt{Solve}({\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{}}}},{\boldsymbol{\mathbf{b}}}): \quad
          {\boldsymbol{\mathbf{w}}}^\intercal\tfrac{\partial {\boldsymbol{\mathbf{y}}}}{\partial {\boldsymbol{\mathbf{\theta}}}}
          =
          -({\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{}}}}^{-1} {\boldsymbol{\mathbf{w}}})^\intercal(\partial_{{\boldsymbol{\mathbf{\theta}}}} {\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{\theta}}}})  ({\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{}}}}^{-1} {\boldsymbol{\mathbf{b}}})$

2.  ${\boldsymbol{\mathbf{\lambda}}}, {\boldsymbol{\mathbf{V}}} = \texttt{Eigs}({\boldsymbol{\mathbf{A}}}): \quad
          {\boldsymbol{\mathbf{w}}}^\intercal\tfrac{\partial \lambda}{\partial {\boldsymbol{\mathbf{\theta}}}}
          =
          {\boldsymbol{\mathbf{w}}}^\intercal\texttt{Diag}\big({\boldsymbol{\mathbf{V}}}^{-1} (\partial_{{\boldsymbol{\mathbf{\theta}}}} {\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{\theta}}}}) {\boldsymbol{\mathbf{V}}} \big)$

3.  ${\boldsymbol{\mathbf{\lambda}}}, {\boldsymbol{\mathbf{V}}} = \texttt{Eigs}({\boldsymbol{\mathbf{A}}}): \quad
          {\boldsymbol{\mathbf{w}}}^\intercal\tfrac{\partial {\boldsymbol{\mathbf{v}}}_{i}}{\partial {\boldsymbol{\mathbf{\theta}}}} =
          {\boldsymbol{\mathbf{w}}}^{\intercal}(\lambda_{i} {\boldsymbol{\mathbf{I}}} - {\boldsymbol{\mathbf{A}}})^{+} \partial_{{\boldsymbol{\mathbf{\theta}}}} {\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{\theta}}}} {\boldsymbol{\mathbf{v}}}_{i}$

4.  $y = \log |{\boldsymbol{\mathbf{A}}}|: \quad
          \frac{\partial y}{\partial {\boldsymbol{\mathbf{\theta}}}}
          =
          \texttt{Tr} \big({\boldsymbol{\mathbf{A}}}^{-1} \partial_{{\boldsymbol{\mathbf{\theta}}}} {\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{\theta}}}}\big)$

5.  ${\boldsymbol{\mathbf{y}}} = \texttt{Diag}({\boldsymbol{\mathbf{A}}}): \quad
          {\boldsymbol{\mathbf{w}}}^{\intercal} \frac{\partial {\boldsymbol{\mathbf{y}}}}{\partial{{\boldsymbol{\mathbf{\theta}}}}}
          =
          {\boldsymbol{\mathbf{w}}}^{\intercal} \texttt{Diag}\left(\partial_{{\boldsymbol{\mathbf{\theta}}}} {\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{\theta}}}}\right)$

In we show the practical benefits of our autograd rules. We take gradients of different linear solves ${\boldsymbol{\mathbf{A}}}_{{\boldsymbol{\mathbf{\theta}}}}^{-1} {\boldsymbol{\mathbf{b}}}$ that were derived using conjugate gradients (CG), where each solve required an increasing number of CG iterations.

# Algorithmic Details

In this section we expand upon three different points introduced in the main paper. For the first point we argue why SVRG leads to gradients with reduced variants. For the second points we display all the iterative methods that we use as base algorithms in CoLA. Finally, for the third point we expand upon CoLA’s strategy for dealing with the different numerical precisions that we support.

## SVRG

In simplest form, SVRG performs gradient descent with the varianced reduced gradient $$\label{eq:svrg_eq}
    {\boldsymbol{\mathbf{w}}} \gets {\boldsymbol{\mathbf{w}}} -  \eta(g_i({\boldsymbol{\mathbf{w}}})-g_i({\boldsymbol{\mathbf{w}}}_0)+g({\boldsymbol{\mathbf{w}}}_0))$$ where $g_i$ represents the stochastic gradient evaluated at only a single element or minibatch of the sum, and $g({\boldsymbol{\mathbf{w}}}_0)$ is the full batch gradient evaluated at the anchor point ${\boldsymbol{\mathbf{w}}}_0$ which is recomputed at the end of each epoch with an updated anchor.

With different loss functions, we can use this update rule to solve symmetric or non-symmetric linear systems, to compute the top eigenvectors or even find the nullspace of a matrix. Despite the fact that the corresponding objectives are not strongly convex in the last two cases, it has been shown that gradient descent and thus SVRG will converge at this exponential rate . Below we list the gradients that enable us to solve different linear algebra problems:

<div id="your_label">

|                                  | Symmetric Solve ${\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{w}}} = {\boldsymbol{\mathbf{b}}}$ |       Top-$k$ Eigenvectors ${\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{W}}} = {\boldsymbol{\mathbf{W}}}{\boldsymbol{\mathbf{\Lambda}}}$        | Nullspace ${\boldsymbol{\mathbf{A}}}{\boldsymbol{\mathbf{W}}}=0$ |
|:--------------------------------:|:-------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------:|
| $g_i({\boldsymbol{\mathbf{w}}})$ |        ${\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{w}}} - {\boldsymbol{\mathbf{b}}}$        | $-{\boldsymbol{\mathbf{A}}}_i{\boldsymbol{\mathbf{W}}}+{\boldsymbol{\mathbf{W}}}{\boldsymbol{\mathbf{W}}}^{\intercal} {\boldsymbol{\mathbf{W}}}$  |      ${\boldsymbol{\mathbf{A}}}_i{\boldsymbol{\mathbf{W}}}$      |

SVRG gradients for solving different linear algebra problems.

</div>

In each of the three cases listed above, we can recognize that if the average of all the gradients $g(w)$ is $0$, then the corresponding linear algebra solution has been recovered.

While it may seem that we need to take three complete passes through $\{{\boldsymbol{\mathbf{A}}}_i\}$ per SVRG epoch (due to the three terms in ), we can reduce this cost to two complete passes exploiting the fact that the gradients are linear in the matrix object, replacing ${\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{W}}} - {\boldsymbol{\mathbf{A}}}_i {\boldsymbol{\mathbf{W}}}_0$ with ${\boldsymbol{\mathbf{A}}}_i({\boldsymbol{\mathbf{W}}}-{\boldsymbol{\mathbf{W}}}_0)$ where appropriate. In all of the `Sum` structure experiments where we leverage SVRG, the x-axis measures the total number of passes through $\{{\boldsymbol{\mathbf{A}}}_i\}_{i=1}^m$, two for each epoch for SVRG.

## Iterative methods

In we list the different iterative methods (base cases) that we use for different linear algebraic operations as well as for different types of linear operators. As seen in , there are many alternatives to our base cases, however we opted for algorithms that are known to be performant, that are well-studied and that are popular amongst practitioners. A comprehensive explanation of our bases cases and their alternatives can be found in and .

<div id="tab:iterative">

|                                                **Linear Algebra Op**                                                 | **Base Case** |   **Alternatives**    |
|:--------------------------------------------------------------------------------------------------------------------:|:-------------:|:---------------------:|
|          ${\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{x}}} = {\boldsymbol{\mathbf{b}}}$ (non-symmetric)           |     GMRES     | BiCGSTAB, LGMRES, QMR |
|           ${\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{x}}} = {\boldsymbol{\mathbf{b}}}$ (self-adjoint)           |    MINRES     |         GMRES         |
|               ${\boldsymbol{\mathbf{A}}} {\boldsymbol{\mathbf{x}}} = {\boldsymbol{\mathbf{b}}}$ (PSD)                |      CG       |         GMRES         |
|                              $\texttt{Eigs}({\boldsymbol{\mathbf{A}}})$ (non-symmetric)                              |    Arnoldi    |   IRAM, Bi-Lanczos    |
|                              $\texttt{Eigs}({\boldsymbol{\mathbf{A}}})$ (self-adjoint)                               |    Lanczos    |        LOBPCG         |
|                                           ${\boldsymbol{\mathbf{A}}}^{+}$                                            |      CG       |      LSQR, LSMR       |
| ${\boldsymbol{\mathbf{A}}} = {\boldsymbol{\mathbf{U}}} {\boldsymbol{\mathbf{\Sigma}}} {\boldsymbol{\mathbf{V}}}^{*}$ | Lanczos, rSVD |    Jacobi-Davidson    |
|                                    $f({\boldsymbol{\mathbf{A}}})$ (self-adjoint)                                     |      SLQ      |        Arnoldi        |

**CoLA’s base case iterative algorithm and some alternatives.** We now expand on the acronyms. GMRES: Generalized Minimum RESidual, BiCGSTAB: BiConjugate Gradient STABilized, QMR: Quasi-Minimal Residual, MINRES: MINimum RESidual, CG: Conjugate Gradients, IRAM: Implicitly Restarted Arnoldi Method, LOBPCG: Locally Optimal Block Preconditioned Conjugate Gradients, Bi-Lanczos: Bidiagonal Lanczos, CGS: Conjugate Gradient Squared, LSQR: Least squares QR, LSMR: Least squares Minimal Residual iteration, LGMRES: Least squares Generalized Minimum RESidual, rSVD: randomized Singular Value Decomposition, and SLQ: Stochastic Lanczos Quadrature.

</div>

## Lower precision linear algebra

The accumulation of round-off error is usually the breaking point of several numerical linear algebra (NLA) routines. As such, it is common to use precisions like `float64` or higher, especially when running these routines on a CPU. In contrast, in machine learning, lower precisions like `float32` or `float16` are ubiquitously used because more parameters and data can be fitted into the GPU memory (whose memory is usually much lower than CPUs) and because the MVMs can be done faster (the CUDA kernels are optimized for operations on these precisions). Additionally, the round-off error incurred on MVMs is not as detrimental when training machine learning models (as we are already running noisy optimization algorithms) as when solving linear algebra problems (where round-off error can lead us to poor solutions). Thus, it is an active area of research in NLA to derive routines which utilize lower precisions than `float64` or that mix precisions in order to achieve better runtimes without a complete degradation of the quality of the solution.

In CoLA we take a two prong approach to deal with lower precisions in our NLA routines. First, we incorporate additional variants of well-known algorithms that propagate less round-off error at the expense of requiring more computation, as seen in . Second, we integrate novel variants of algorithms that are designed to be used on lower precisions such as the CG modification found in . We now discuss the first approach.

As discussed in , there are two algorithms that are key for eigendecompositions. The first is Arnoldi (applicable to any operator), and the second is Lanczos (for symmetric operators) — where actually Lanczos can be viewed as a simplified version of Arnoldi. Central to these algorithms is the use of an orthogonalization step which is well-known to be a source of numerical instability. One approach to aggressively ameliorate the propagation of round-off error during orthogonalization is to use Householder projectors, which is the strategy that we use in CoLA. Given a unitary vector ${\boldsymbol{\mathbf{u}}}$, a Householder projector (or Householder reflector) is defined as the following operator ${\boldsymbol{\mathbf{R}}} = {\boldsymbol{\mathbf{I}}} - 2 {\boldsymbol{\mathbf{u}}} {\boldsymbol{\mathbf{u}}}^{*}$. When applied to a vector ${\boldsymbol{\mathbf{x}}}$ the result ${\boldsymbol{\mathbf{R}}}{\boldsymbol{\mathbf{x}}}$ is basically a reflection of ${\boldsymbol{\mathbf{x}}}$ over the ${\boldsymbol{\mathbf{u}}}^{\intercal}$ space. To easily visualize this, suppose that ${\boldsymbol{\mathbf{x}}} \in \mathbb{R}^{2}$ and ${\boldsymbol{\mathbf{u}}} = {\boldsymbol{\mathbf{e}}}_{1}$. Hence, $$\begin{split}
      {\boldsymbol{\mathbf{R}}} {\boldsymbol{\mathbf{x}}}
      =
      \begin{pmatrix}
        x_{1} \\ x_{2}
      \end{pmatrix}
      -
      2
      \begin{pmatrix}
        x_{1} \\ 0
      \end{pmatrix}
      =
      \begin{pmatrix}
        -x_{1} \\ x_{2}
      \end{pmatrix}
    \end{split}$$ which is exactly the reflection of the vector across the axis generated by ${\boldsymbol{\mathbf{e}}}_{2}$. Most notably, ${\boldsymbol{\mathbf{R}}}$ is unitary ${\boldsymbol{\mathbf{R}}} {\boldsymbol{\mathbf{R}}}^{*} = {\boldsymbol{\mathbf{I}}}$ which can be easily verified from the definition. Being unitary is crucial as under the usual round-off error model, applying ${\boldsymbol{\mathbf{R}}}$ to another matrix ${\boldsymbol{\mathbf{A}}}$ does not worsen the already accumulated error ${\boldsymbol{\mathbf{E}}}$. Mathematically, $\left\lVert{\boldsymbol{\mathbf{R}}}\left({\boldsymbol{\mathbf{A}}} + {\boldsymbol{\mathbf{E}}}\right) - {\boldsymbol{\mathbf{R}}} {\boldsymbol{\mathbf{A}}}\right\rVert = \left\lVert{\boldsymbol{\mathbf{R}}} {\boldsymbol{\mathbf{E}}}\right\rVert = \left\lVert{\boldsymbol{\mathbf{E}}}\right\rVert$, where the last equality results from basic properties of unitary matrices. We are going to use Arnoldi as an example of how Householder projectors are used during orthogonalization. In we have an example of two different variants of Arnoldi present in CoLA. The implementations are notably different and also it is easy to see how Algorithm is more expensive than Algorithm . First, note that for Algorithm we have two for loops (line 6 and line 8) whereas for Algorithm we only have one (line 4-6). Worse, the two for loops in Algorithm require more flops than the only for loop in Algorithm . Note that we do not always favor the more expensive but robust implementation of an algorithm as in some cases, like when running GMRES, the round-off error is not as impactful to the quality of the solution, and shorter runtimes are actually more desirable.

<figure id="fig:arnoldi-versions">

<figcaption>Different versions of the same algorithm, but the Householder variant being more numerically robust.</figcaption>
</figure>

# Experimental Details

In this section we expand upon the details of all the experiments ran in the paper. Such details include the datasets that were used, the hyperparameters of different algorithms and the specific choices of algorithms used both for CoLA but also for the alternatives. We run each of the experiments 3 times and compute the mean dropping the first observation (as usually the first run contains some compiling time much is not too large). We do not display the standard deviation as those numbers are imperceptible for each experiment. In terms of hardware, the CPU experiments were run on an Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz and the GPU experiments were run on a NVIDIA GeForce RTX 2080 Ti.

## Datasets

Below we enumerate the datasets that we used in the various applications. Most of the datasets are sourced from the University of California at Irvine’s (UCI) Machine Learning Respository that can be found here: <https://archive.ics.uci.edu/ml/datasets.php>. Also, a community repo hosting these UCI benchmarks can be found here: <https://github.com/treforevans/uci_datasets> (we have no affiliation).

1.  *Elevators*. This dataset is a modified version of the *Ailerons* dataset, where the goal is to to predict the control action on the ailerons of the aircraft. This UCI dataset consists of $N=14$K observations and has $D=18$ dimensions.

2.  *Kin40K*. The full name of this UCI dataset is *Statlog (Shuttle) Data Set*. This dataset contains information about NASA shuttle flights and we used a subset that consists of $N=40$K observations and has $D=8$ dimensions.

3.  *Buzz*. The full name of this UCI dataset is *Buzz in social media*. This dataset consists of examples of buzz events from Twitter and Tom’s Hardware. We used a subset consisting of $N=430$K observations and has $D=77$ dimensions.

4.  *Song*. The full name of this UCI dataset is *YearPredictionMSD*. This dataset consists of $N=386.5$K observations and it has $D=90$ audio features such as 12 timbre average features and 78 timbre covariance features.

5.  *cit-HepPh*. This dataset is based on arXiv’s HEP-PH (high energy physics phenomenology) citation graph and can be found here: <https://snap.stanford.edu/data/cit-HepPh.html>. The dataset covers all the citations from January 1993 to April 2003 of $|V|=34,549$ papers, ultimately containing $|E|=421,578$ directed edges. The notion of relationship that we used in our spectral clustering experiment creates a connection between two papers when at least one cites another (undirected symmetric graph). Therefore the dataset that we used has the same number of nodes but instead $|E|=841,798$ undirected edges.

## Compositional experiments

This section pertains to the experiments of displayed in . We now elaborate on each of ’s panels.

1.  The multi-task GP problem exploits the structure of the following Kronecker operator ${\boldsymbol{\mathbf{K}}}_T \otimes {\boldsymbol{\mathbf{K}}}_X$, where ${\boldsymbol{\mathbf{K}}}_{T}$ is a kernel matrix containing the correlation between the tasks and ${\boldsymbol{\mathbf{K}}}_{X}$ is a RBF kernel on the data. For this experiment, we used a synthetic Gaussian dataset where the train data ${\boldsymbol{\mathbf{x}}}_i \sim \mathcal{N}({\boldsymbol{\mathbf{0}}}, {\boldsymbol{\mathbf{I}}}_D)$ which has dimension $D=33$, $N=1$K and we used $T=11$ tasks (where the tasks basically set the size of ${\boldsymbol{\mathbf{K}}}_T$). We used conjugate gradients (CG) as the iterative method, where we set the hyperparameters to a tolerance of $10^{-6}$ and to a maximum number of iterations to $1$K. We used the exact same hyperparameters for CoLA.

2.  For the bi-poisson problem we set up the maximum grid to be $N=1000^2$. Since this PDE problem involves solving a symmetric linear system, we used CG as the iterative method with a tolerance of $10^{-11}$ and a maximum number of iterations of $10$K. The previous parameters also apply for CoLA. We note that PDE problems are usually solved to higher tolerances as the numerical error compounds as we advance the PDE.

3.  For the EMLP experiment we consider solving the equivariance constraints to find the equivariant linear layers of a graph neural network with $5$ nodes. To solve this problem, we need to find the nullspace of a large structured constraint matrix. We use the uniformly channel heuristic from which distributes the $N$ channels across tensors of different orders. We consider our approach which exploits the block diagonal structure, separating the nullspaces into blocks, as opposed to the direct iterative approach exploiting only the fast MVMs of the constraint matrix. We use a tolerance of $10^{-5}$.

## Sum structure experiments

This section pertains to the experiments of contained in . We now elaborate on each of ’s panels.

1.  In this experiment we computed the first principal component of the *Buzz* dataset. For the iterative method we used power iteration with a maximum number of iterations of $300$ and a stop tolerance of $10^{-7}$. CoLA used SVRG also with the same stop tolerance and maximum number of iterations. Additionally, we set SVRG’s batch size to $10$K and the learning rate to $0.0008$. We note that a single power iteration roughly contains $43 / 2 = 21.5$ times more MVMs than a single iteration of SVRG. In this particular case, the length of the sum is given by the number of observations and therefore SVRG uses $430 / 10 = 43$ times less elements per iteration, where $10$ comes from the $10$K batch size. Finally, the $2$ is explained by noting that SVRG incurs in a full sum update on every epoch.

2.  In this experiment we trained a GP by estimating the covariance RBF kernel with $J=1$K random Fourier features (RFFs). The hyperparameters for the RBF kernel are the following: length scale ($\ell=0.1$), output scale ($a=1$) and likelihood noise ($\sigma^2=0.1$). Moreover, we used CG as the iterative solver with a tolerance of $10^{-8}$ and $100$ as the maximum number of iterations (the convergence took much less iterations than the max). For SVRG we used the same tolerance but set the maximum number of iterations to $10$K, a batch size of $100$ and learning rate of $0.004$. We note that a single CG iteration roughly contains $10 / 2 = 5$ times more MVMs than a single iteration of SVRG. In this particular case, the length of the sum is given by the number of RFFs and therefore SVRG uses $1000 / 100 = 10$ times less elements per iteration, where $100$ comes from the batch size.

3.  In this experiment we implemented the Neural-IVP method from . We consider the time evolution of a wave equation in two spatial dimensions. At each integrator step, a linear system ${\boldsymbol{\mathbf{M}}}(\theta) \dot{\theta} = F(\theta)$ must be solved to find $\dot{\theta}$, for a $d=12\text{K}\times12$K dimensional matrix. While use conjugate gradients to solve the linear system, we demonstrate the advantages of using SVRG, as ${\boldsymbol{\mathbf{M}}}(\theta) = \tfrac{1}{m}\sum_{i=1}^m M_i(\theta)$ is a sum over the evaluation at $m=50$K distinct sample locations within the domain. In this experiment we use a batch size of $500$ for SVRG, and employ rank $250$ randomized Nyström preconditioning for both SVRG and the iterative CG baseline.

## Hardware speed-up comparisons

This section pertains to the experiments of . For all these experiments we computed the runtime reduction as a fraction between the time that it takes CoLA to run some linear algebra operation and PyTorch using the same hardware. As an example, assume that PyTorch takes 200 seconds to compute a solve using a CPU and 100 seconds to compute the same solve but now using a GPU. Moreover, assume that CoLA’s iterative algorithm takes 100 seconds to compute the same solve on a CPU and 40 seconds on a GPU. Thus, the runtime reduction would be $100 / 200 = 0.5\%$ for the CPU column whereas $40 / 100 = 0.4$ for the GPU column.

1.  **Solves**. In this experiment we calculated the % runtime reduction when running `torch.linalg.solve` on the Trefethen $N=20K$ matrix market sparse operator. In this experiment, CG was run with a tolerance of $10^{-11}$ and a maximum number of iterations equal to the operator size.

2.  **Eigenvalue estimation**. In this experiment we calculated the % runtime reduction when running `torch.linalg.eigh` on the mhd4800b $N=4.8K$ matrix market sparse operator. In this experiment, Lanczos was run with a tolerance of $10^{-9}$ and a maximum number of iterations equal to $100$.

3.  **Log determinant computation**. In this experiment we calculated the % runtime reduction when running `torch.linalg.logdet` on the bcsstk18 $N=11.9K$ matrix market sparse operator. In this experiment, the stochastic Lanczos quadrature was run using $30$ Lanczos probe estimates and $25$ samples.

## Applications

This section pertains to the experiments of displayed in . We now elaborate on each of ’s panels.

1.  In this experiment we compute 5, 10 and 20 PCA components for the *Buzz* dataset. We compared against `sklearn` which uses the Lanczos algorithm through the fast Fortran-based `ARPACK` numerical library. In this case, CoLA uses randomized SVD with a rank $3000$ approximation.

2.  In this experiment we fit a Ridge regression on the *Song* dataset with a regularization coefficient set to $0.1$. We compared against `sklearn` using their fastest least-square solver `lsqr` with a tolerance of $10^{-4}$. In this case, CoLA uses CG with the same tolerance and with a maximum number of iterations set to $1$K. Additionally, we ran CoLA using CPU and GPU whereas we used only CPU for `sklearn` as it has no GPU support. We observe how in the arguably most popular ML method, CoLA is able to beat a leading package such as `sklearn`.

3.  In this experiment we fit a GP with a RBF kernel on two datasets: *Elevators* and *Kin40K*. We only used up to 20K observations from *Kin40K* as that was the maximum number of observations that would fit the GPU memory without needing to partition the MVMs. We compare against `GPyTorch` which uses CG and stochastic Lanczos quadrature (SLQ) to compute and optimize the negative log-marginal likelihood (loss function). Both experiments were run on a GPU for $100$ iterations using Adam as an optimizer with learning rate of $0.1$ with the default values of $\beta_1 = 0.9$ and $\beta_2=0.999$. Additionally, for both GPyTorch and CoLA, the CG tolerance was set to $10^{-4}$ with a maximum number of CG iterations of $250$ and $20$ probes were used for SLQ. Note that both CoLA and GPyTorch have similar throughputs, for example GPyTorch runs a 100 iterations on *Elevators* on 43 seconds whereas CoLA runs a 100 iterations on 49 seconds. When training a GP, we solve a block of 11 linear systems (1 based on ${\boldsymbol{\mathbf{y}}}$ and 10 based on random probes) where one key difference is that the CG solver for GPyTorch has a stopping criteria based on the convergence of the mean solves whereas CoLA has a stopping criteria based on the convergence of all the solves.

4.  In this experiment we run spectral clustering on the *cit-HepPh* dataset using an embedding size of 8 and also 8 clusters for k-means (with only 1 run of k-means after estimating the embeddings). We compare against `sklearn` using two different solvers, one based on Lanczos iterations using `ARPACK` and another using an Algebraic Multi-Grid solver `AMG`. In this case, CoLA also uses Lanczos iterations with a default tolerance of $10^{-6}$. We see how `sklearn`’s `AMG` solver runs faster than CoLA’s but this is mostly the algorithmic constants as they have similar asymptotical behavior (similar slopes).

5.  In this experiment we solve the Schrödinger equation to find the energy levels of the hydrogen atom on a $3$-dimensional finite difference grid with up to $N=5$K points. In order to handle the infinite spatial extent, we compactify the domain by applying the arctan function. Under this change of coordinates, the Laplacian has a different form, and hence the matrix forming the discretized Hamiltonian is no longer symmetric. We compare against `SciPy`’s Arnoldi implementation with $20$ iterations where CoLA also uses Arnoldi with the same number of iterations. Surprisingly, CoLA’s JAX jitted code has a competitive runtime when compare to `SciPy`’s runtime using `ARPACK`.

6.  In this experiment we solve a minimal surface problem on a grid of maximum size of $N=100^2$ points. To solve this problem we have to run Netwon-Rhapson where each inner step involves a linear solve of an non-symmetric operator. We compare against `SciPy`’s GMRES implementation as well as `JAX`’s integrated version of `SciPy`. The main difference between the two is that `SciPy` calls the fast and highly-optimized `ARPACK` library whereas `SciPy (JAX)` has its only `Python` implementation of GMRES which only uses `JAX`’s primitives (equally as it is done in CoLA). The tolerance for this experiment was 5e-3. We see how CoLA’s GMRES implementation is competitive with `SciPy (JAX)` but it still does not beat `ARPACK` mostly due to the faster runtime of using a lower level GMRES implementation.

[^1]: Equal contribution.
