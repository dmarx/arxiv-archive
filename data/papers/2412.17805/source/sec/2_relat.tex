\section{Related Work}
\label{sec:relat}

\paragraph{Video Variational Autoencoder} Video Variational Autoencoders (VAEs)~\cite{kingma2014auto} can be broadly categorized into discrete and continuous types. Discrete video VAEs compress videos into discrete tokens by learning a codebook for quantization and have achieved state-of-the-art performance in video reconstruction, as demonstrated by models like MAGVIT-v2~\cite{yu2023language}. However, these VAEs are not suitable for Latent Video Diffusion Models (LVDMs)~\cite{he-lvdm} due to the lack of necessary gradients for backpropagation, which hinders smooth optimization.

In contrast, continuous Video VAEs compress videos into continuous latent representations that are widely adopted in LVDMs. 
In earlier video generation studies, including Stable Video Diffusion~\cite{blattmann2023stable}, the Video VAE was directly adapted from the image VAE used in Stable Diffusion~\cite{rombach2022high}, achieving a compression ratio of $1 \times 8 \times 8$  by processing each frame independently. 
To further reduce the temporal redundancy, more recent studies~\cite{zhao2024cv,pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109,opensora,xu2024easyanimatehighperformancelongvideo,yang2024cogvideox} have trained their VAEs to achieve a more efficient compression ratio of $4 \times 8 \times 8$. 

Despite these advancements, all of the aforementioned video VAEs struggle with accurately reconstructing videos with large motions due primarily to their limited ability to handle the temporal dimension effectively. A high-quality Video VAE that can robustly reconstruct videos with significant motion is critical in the LVDM pipeline, as it ensures efficient latent space compression, maintains temporal coherence and reduces computational overhead~\cite{metamoviegen}. Without a robust VAE, large motions in videos can lead to poor latent representations, negatively impacting the quality and overall performance of the LVDMs.

\paragraph{Latent Video Diffusion Models} 
Latent Video Diffusion Models (LVDMs) are widely used in foundational video generation models including Sora 
~\cite{videoworldsimulators2024}, OpenSora~\cite{opensora}, Open Sora Plan~\cite{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109}, VideoCrafter1~\cite{he-videocrafter1}, VideoCrafter2~\cite{chen2024videocrafter2overcomingdatalimitations}, Latte\cite{ma2024latte}, CogVideoX~\cite{yang2024cogvideox}, DynamiCrafter~\cite{xing2023dynamicrafter}, Vidu~\cite{bao2024vidu}, Hunyuan Video~\cite{kong2024hunyuanvideo}, controllable video generation~\cite{he-animate-a-story, follow-your-pose, follow-your-emoji}, and multimodal video generation models~\cite{he-seeing-and-hearing, he-llm-survey}.
% have recently achieved significant success in video generation tasks. 
%
The general pipeline for these LVDMs consists of two primary steps. First, the raw video is compressed into a latent space via a video Variational Autoencoder (VAE), significantly reducing computational complexity. In the second step, a diffusion model operates within this latent space, learning the desired transformations. The performance of LVDMs is critically dependent on video VAEs, as the quality of the generated video is heavily influenced by the latent space representation and the encoding-decoding capabilities of the VAE.


In image generation tasks, Stable Diffusion series~\cite{rombach2022high, podell2023sdxl, sd35} has excelled, largely due to its efficient VAE that reconstructs diverse image types with high fidelity. However, no existing VAE in video generation achieves comparable quality, particularly due to challenges in compressing the temporal dimension. This limitation hinders the performance of LVDMs, especially in high-motion scenarios.

