\section{Related Works}
In this section, we mainly introduce the related works using LLMs for KG construction. 


\input{tables/comparison}


\subsection{LM-as-KB Paradigm}
The first study of in-depth analysis of the relational knowledge present in the pretrained language model is \cite{llmaskb}. Authors show that language models could answer the queries structured as ``fill-in-the-blank" cloze statements, such as ``Dante was birn in [MASK]" to predict what is the mask token. The question ``Language models as knowledge based?" (LM-as-KB) is firstly proposed, which is also the title of the work \cite{llmaskb}. The language models have many advantages over the symbolic and structured knowledge base, which are schema-free requiring no schema engineering and are able to handle open class of relations.  The LAMA (LAnguage Model Analysis) probe benchmark is introduced and widely used to test the factual and commonsense knowledge in the language model. Since then, a stirred discussion as to what extend language model could be an alternative to, or at least be a proxy for explicit knowledge bases is conducted. \cite{llmaskborforkb} further discuss the difference between the LM-as-KB paradigm and the structured KB paradigm from the perspective of construction, schema, maintenance, knows what it knows, etc, as shown in Table \ref{tab:difference}\footnote{We do not include the  perspective of entity disambiguation in Table \ref{tab:difference} which is included in the original table in \cite{llmaskborforkb}, since the large language models in now-days such as GPT 3.5/4 are also commonly good at entity disambiguation. }. In Table \ref{tab:difference}, we also add LLMKG into comparison. As we can see, the LM-as-KB could be constructed in a self/Unsupervised way and is schema-free. And the conventional structured KBs are easy to maintain, has a clear knowledge border that knows what it knows, and has the ability to trace the origin of the knowledge by keeping the reference sources. While the LLMKG, we are interested in this paper, keeps the advantages of the LM-as-KB paradigm and the conventional structured KB. It is a schema-free structured KGs constructed based on language models. 

Though empirical experiments shows the potential of LM-as-KB paradigm, some works question this paradigm and point out the shortcoming of this paradigm. 
\cite{llmaskb-guess} shows that pre-trained masked language models are making educated guessing when filling the mask and the performance of prompt-based paradigm is prompt-biased. The improvements by incorporating illustrative cases and external contexts is mainly due to entity type guidance and golden answer leakage. 
\cite{head-to-tail} explore how knowledgeable are LLMs by constructing a benchmark Head-to-Tail consists of 18k question-answer pairs of head, torso, and tail facts in terms of popularity, and it shows that existing LLMs including GPT4 are still far from being perfect in terms of remembering of factual knowledge, especially for facts of torso-to-tail entities. The accuracy of question answering over open and specific domains of the tail entities is only $27.3\%$ and $10.6\%$ respectively, which is quite low, and is apparently not enough to support regarding LLMs as knowledgeable for tail entities. 
\cite{lmastkg} points that it is still challenging for LMs to memorize conflicting information which also hinders the memorization of other unrelated one-to-one relationships.

Apart from directly regarding the LM trained on vast text corpus as knowledge base, the LM-as-KB paradigm could also be achieved by post training to enable language models memorizing and reasoning among large scale knowledge base. 
Starting from the pre-trained checkpoints of open-source language models including T5 and LLaMA-2, \cite{lmmemorizekb} continue train these language models on a filtered Wikidata containing 46M triples. Experiment results show that LMs are capable of memorizing triples in knowledge base at large scale, and the larger the model is, the quicker it learns. But the infrequent knowledge is more challenging to memorize, irrespective of the language model size. It also shows language model trained on knowledge based are better at retrieving and organizing long-tail knowledge when answering questions related to long-tail knowledge.  Further more, experiments on missing facts prediction shows that the fine-tuned language models are capable of inferring missing entities of triples from existing knowledge to some extent and struggle with inverse reasoning.  

These works proves the language models could serve as the knowledge base to some extend. However, their knowledge probing performance will be affected by the form of prompts. They are struggle to memorize triples about long-tail entities and relations, conflict information such as triples about 1-N, N-N relations, and has limited reasoning capabilities over triples, this shortcomings still exists after post-training the language model with corpus from existing knowledge bases. Thus language models cannot replace symbolic knowledge base as a trustful knowledge resources. Thus more and more works investigate how the enable large language models and knowledge graphs benefit each other.         

\subsection{LLM for KG Construction}
A lot of knowledge graphs have been constructed and are assets for recommender system, search engine and other applications. However, they are far from complete. Thus 
constructing accurate and complete structured knowledge graphs is challenging.
\cite{llm4kbc} explored the potential of GPT models for knowledge base completion. It shows involvement of language model for completion significantly reduce the construction cost and with proper thresholding, GPT-3 successfully extend Wikidata by 27M facts at $90\%$ precision, showing the effectiveness of large language model for knowledge graph completion. However, it also reveals that language model do not produce statements of a high enough accuracy, which achieve a quite low recall, ranging from $0.01$ to $0.70$, when constrain the precision to be high such as above $90\%$ or $95\%$. 
Missing triples could also be inferred based on the graph structures, 
\cite{kopa} propose a method KoPA to incorporate the structural information of knowledge graphs into large language models. KoPA includes a knowledge prefix adapter to which projects structural embeddings into textual space and obtains virtual knowledge tokens positioned as a prefix of the input prompt. KoPA shows better link prediction performance over embedding-based, PLM-based, training-free LLM-based, and fine-tuning LLM-based baselines. 
\cite{DBLP:conf/coling/WangHHLYL24} focus on the problem that in the process of using LLM for KGC reordering, problems such as mismatches, misordering, and omissions may be encountered. To this end, KC-GenRe, a knowledge constraint generative KGC reordering method based on LLM, is introduced. To overcome the mismatch problem, the KGC reordering task is formulated as a candidate identifier sorting generation problem implemented by generative LLM. To address the misordering problem, a knowledge-guided interactive training method is developed, which improves the identification and ranking of candidates. To address the omission problem, a knowledge-enhanced constraint inference method is designed to enable context prompting and control generation for effective ranking. KG-GenRe achieved gains of up to $6.7\%$ and $7.7\%$ on the MRR and Hits@1 indicators on the four datasets compared to the previous method, and up to $9.0\%$ and $11.1\%$ compared to the non-reordered method. 
Description-based KGC, which leverages pre-trained language models (PLMs) to learn entity and relation representations with their names or descriptions, is limited by the quality of text and the incomplete structure \cite{DBLP:conf/coling/XuZLWZ000C24}. To address this issue, \cite{DBLP:conf/coling/XuZLWZ000C24} propose MPIKGC, a general framework to compensate for the deficiency of contextualized knowledge and improve KGC by querying large language models (LLMs) from various perspectives, which involves leveraging the reasoning, explanation, and summarization capabilities of LLMs to expand entity descriptions, understand relations, and extract structures, respectively. 
To overcome the challenge that the static and noisy nature of existing corpora generally limits the potential of PLM-based KGC models, \cite{DBLP:conf/eacl/LiTCL24} introduces the Contextualization Distillation strategy, a versatile plug-and-play approach that is compatible with both discriminative and generative KGC frameworks. The approach first instructs LLMs to transform compact structural triples into context-rich fragments, and then introduces two customized auxiliary tasks - reconstruction and contextualization - that allow smaller KGC models to absorb insights from these rich triplets. 
To alleviate the low efficiency of text-based KGC and the long-tail entities problems of triple-based KGC, \cite{DBLP:conf/emnlp/WeiH0K23} proposes KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. It alleviates the long-tail problem without incurring additional training overhead.
In addition, \cite{DBLP:journals/corr/abs-2310-08279} analyzed to verify the hypothesis that LLMs, without fine-tuning, can refine entity descriptions, serving as an auxiliary knowledge source. It found that without fine-tuning, LLMs can further improve the quality of entity text descriptions.  LLMs exhibit text-generation hallucination issues and selectively output words with multiple meanings, which can be mitigated by contextualizing prompts to constrain LLM outputs. Larger model sizes do not necessarily guarantee better performance, even the 7B model can achieve optimized results in this comparative task. These findings underscore the untapped potential of large models in text-based KGC, which is a promising direction for further research in KGC.


These works demonstrate that LLMs could be utilized to complete current knowledge graphs, either by extracting knowledge from parameters of the LLMs or enhancing the reasoning capabilities of predicting missing triples. 






