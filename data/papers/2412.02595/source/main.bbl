\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abadji et~al.(2022)Abadji, Ortiz~Suarez, Romary, and Sagot}]{abadji-etal-2022-towards}
Julien Abadji, Pedro Ortiz~Suarez, Laurent Romary, and Beno{\^\i}t Sagot. 2022.
\newblock \href {https://aclanthology.org/2022.lrec-1.463} {Towards a cleaner document-oriented multilingual crawled corpus}.
\newblock In \emph{Proceedings of the Thirteenth Language Resources and Evaluation Conference}, pages 4344--4355, Marseille, France. European Language Resources Association.

\bibitem[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{arXiv preprint arXiv:2404.14219}.

\bibitem[{Adler et~al.(2024)Adler, Agarwal, Aithal, Anh, Bhattacharya, Brundyn, Casper, Catanzaro, Clay, Cohen et~al.}]{adler2024nemotron}
Bo~Adler, Niket Agarwal, Ashwath Aithal, Dong~H Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et~al. 2024.
\newblock Nemotron-4 340b technical report.
\newblock \emph{arXiv preprint arXiv:2406.11704}.

\bibitem[{Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebron, and Sanghai}]{ainslie2023gqa}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023.
\newblock Gqa: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4895--4901.

\bibitem[{Barbaresi(2021)}]{barbaresi-2021-trafilatura}
Adrien Barbaresi. 2021.
\newblock \href {https://aclanthology.org/2021.acl-demo.15} {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}}.
\newblock In \emph{Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations}, pages 122--131. Association for Computational Linguistics.

\bibitem[{{Ben Allal}(2024)}]{fineweb-edu-discussion}
Loubna {Ben Allal}. 2024.
\newblock Most of the data is duplicated?
\newblock \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/discussions/7}.
\newblock Accessed: October 24, 2024.

\bibitem[{Bisk et~al.(2020)Bisk, Zellers, Gao, Choi et~al.}]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al. 2020.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439.

\bibitem[{Brack et~al.(2024)Brack, Ostendorff, Ortiz~Suarez, Saiz, Castilla, Palomar-Giner, Shvets, Schramowski, Rehm, Villegas, and Kersting}]{brack-etal-2024-community}
Manuel Brack, Malte Ostendorff, Pedro Ortiz~Suarez, Jos{\'e}~Javier Saiz, I{\~n}aki~Lacunza Castilla, Jorge Palomar-Giner, Alexander Shvets, Patrick Schramowski, Georg Rehm, Marta Villegas, and Kristian Kersting. 2024.
\newblock \href {https://aclanthology.org/2024.mrl-1.19} {Community {OSCAR}: A community effort for multilingual web data}.
\newblock In \emph{Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)}, pages 232--235, Miami, Florida, USA. Association for Computational Linguistics.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}.

\bibitem[{Eldan and Li(2023)}]{eldan2023tinystories}
Ronen Eldan and Yuanzhi Li. 2023.
\newblock Tinystories: How small can language models be and still speak coherent english?
\newblock \emph{arXiv preprint arXiv:2305.07759}.

\bibitem[{Fan et~al.(2019)Fan, Jernite, Perez, Grangier, Weston, and Auli}]{fan2019eli5}
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019.
\newblock Eli5: Long form question answering.
\newblock \emph{arXiv preprint arXiv:1907.09190}.

\bibitem[{Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou}]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023.
\newblock \href {https://doi.org/10.5281/zenodo.10256836} {A framework for few-shot language model evaluation}.

\bibitem[{Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi et~al.}]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, et~al. 2023.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}.

\bibitem[{Heafield(2011)}]{heafield2011kenlm}
Kenneth Heafield. 2011.
\newblock Kenlm: Faster and smaller language model queries.
\newblock In \emph{Proceedings of the sixth workshop on statistical machine translation}, pages 187--197.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
\newblock \href {https://openreview.net/forum?id=d7KBjmI3GmQ} {Measuring massive multitask language understanding}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Joulin et~al.(2016{\natexlab{a}})Joulin, Grave, Bojanowski, Douze, J{\'e}gou, and Mikolov}]{joulin2016fasttext}
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H{\'e}rve J{\'e}gou, and Tomas Mikolov. 2016{\natexlab{a}}.
\newblock Fasttext.zip: Compressing text classification models.
\newblock \emph{arXiv preprint arXiv:1612.03651}.

\bibitem[{Joulin et~al.(2016{\natexlab{b}})Joulin, Grave, Bojanowski, and Mikolov}]{joulin2016bag}
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016{\natexlab{b}}.
\newblock Bag of tricks for efficient text classification.
\newblock \emph{arXiv preprint arXiv:1607.01759}.

\bibitem[{Kudugunta et~al.(2023)Kudugunta, Caswell, Zhang, Garcia, Choquette-Choo, Lee, Xin, Kusupati, Stella, Bapna, and Firat}]{kudugunta2023madlad400multilingualdocumentlevellarge}
Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher~A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023.
\newblock \href {https://arxiv.org/abs/2309.04662} {Madlad-400: A multilingual and document-level large audited dataset}.
\newblock \emph{Preprint}, arXiv:2309.04662.

\bibitem[{Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy}]{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pages 785--794.

\bibitem[{Lee et~al.(2022)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch, and Carlini}]{lee2022deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022.
\newblock Deduplicating training data makes language models better.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8424--8445.

\bibitem[{Li et~al.(2024)Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora et~al.}]{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al. 2024.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock \emph{arXiv preprint arXiv:2406.11794}.

\bibitem[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock \emph{arXiv preprint arXiv:2309.05463}.

\bibitem[{Maini et~al.(2024)Maini, Seto, Bai, Grangier, Zhang, and Jaitly}]{maini2024rephrasing}
Pratyush Maini, Skyler Seto, He~Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. 2024.
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock In \emph{ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models}.

\bibitem[{Merrick et~al.(2024)Merrick, Xu, Nuti, and Campos}]{merrick2024arctic}
Luke Merrick, Danmei Xu, Gaurav Nuti, and Daniel Campos. 2024.
\newblock Arctic-embed: Scalable, efficient, and accurate text embedding models.
\newblock \emph{arXiv preprint arXiv:2405.05374}.

\bibitem[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 2381--2391.

\bibitem[{Muennighoff et~al.(2024)Muennighoff, Rush, Barak, Le~Scao, Tazi, Piktus, Pyysalo, Wolf, and Raffel}]{muennighoff2024scaling}
Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le~Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin~A Raffel. 2024.
\newblock Scaling data-constrained language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Parmar et~al.(2024)Parmar, Prabhumoye, Jennings, Liu, Jhunjhunwala, Wang, Patwary, Shoeybi, and Catanzaro}]{parmar2024data}
Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo~Liu, Aastha Jhunjhunwala, Zhilin Wang, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2024.
\newblock Data, data everywhere: A guide for pretraining dataset construction.
\newblock \emph{arXiv preprint arXiv:2407.06380}.

\bibitem[{Penedo et~al.(2024)Penedo, Kydl{\'\i}{\v{c}}ek, Lozhkov, Mitchell, Raffel, Von~Werra, Wolf et~al.}]{penedo2024fineweb}
Guilherme Penedo, Hynek Kydl{\'\i}{\v{c}}ek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von~Werra, Thomas Wolf, et~al. 2024.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.
\newblock \emph{arXiv preprint arXiv:2406.17557}.

\bibitem[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Alobeidli, Cappelli, Pannier, Almazrouei, and Launay}]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.
\newblock The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:79155--79172.

\bibitem[{Pomik{\'a}lek(2011)}]{pomikalek2011removing}
Jan Pomik{\'a}lek. 2011.
\newblock Removing boilerplate and duplicate content from web corpora.
\newblock \emph{Disertacn{\i} pr{\'a}ce, Masarykova univerzita, Fakulta informatiky}.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21(140):1--67.

\bibitem[{Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64(9):99--106.

\bibitem[{Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi}]{sap2019social}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le~Bras, and Yejin Choi. 2019.
\newblock Social iqa: Commonsense reasoning about social interactions.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 4463--4473.

\bibitem[{Shazeer(2020)}]{shazeer2020glu}
Noam Shazeer. 2020.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}.

\bibitem[{Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}.

\bibitem[{Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar et~al.}]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al. 2024.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}.

\bibitem[{Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant}]{talmor2019commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4149--4158.

\bibitem[{Tang et~al.(2024)Tang, Ranjan, Pangarkar, Liang, Wang, An, Rao, Jin, Wang, Cheng, Sun, Mu, Miller, Ma, Peng, Liu, and Xing}]{txt360data2024}
Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li~An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma, Yue Peng, Zhengzhong Liu, and Eric~P. Xing. 2024.
\newblock Txt360: A top-quality llm pre-training dataset requires the perfect blend.
\newblock \url{https://huggingface.co/spaces/LLM360/TxT360}.
\newblock Accessed: October 24, 2024.

\bibitem[{Team et~al.(2024)Team, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e} et~al.}]{team2024gemma}
Gemma Team, Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al. 2024.
\newblock Gemma 2: Improving open language models at a practical size.
\newblock \emph{arXiv preprint arXiv:2408.00118}.

\bibitem[{Teknium(2023)}]{OpenHermes}
Teknium. 2023.
\newblock Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants.
\newblock \url{https://huggingface.co/datasets/teknium/OpenHermes-2.5}.
\newblock Accessed: October 24, 2024.

\bibitem[{Tokpanov et~al.(2024)Tokpanov, Glorioso, Anthony, and Millidge}]{tokpanov2024zyda25trilliontoken}
Yury Tokpanov, Paolo Glorioso, Quentin Anthony, and Beren Millidge. 2024.
\newblock \href {https://arxiv.org/abs/2411.06068} {Zyda-2: a 5 trillion token high-quality dataset}.
\newblock \emph{Preprint}, arXiv:2411.06068.

\bibitem[{Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi}]{wang2023self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 13484--13508.

\bibitem[{Wenzek et~al.(2020)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave}]{wenzek-etal-2020-ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave. 2020.
\newblock \href {https://aclanthology.org/2020.lrec-1.494} {{CCN}et: Extracting high quality monolingual datasets from web crawl data}.
\newblock In \emph{Proceedings of the Twelfth Language Resources and Evaluation Conference}, pages 4003--4012, Marseille, France. European Language Resources Association.

\bibitem[{Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant, Barua, and Raffel}]{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.41} {m{T}5: A massively multilingual pre-trained text-to-text transformer}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 483--498, Online. Association for Computational Linguistics.

\bibitem[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang et~al.}]{yang2024qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et~al. 2024.
\newblock Qwen2 technical report.
\newblock \emph{arXiv preprint arXiv:2407.10671}.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4791--4800.

\end{thebibliography}
