---
abstract: |
  Recent English Common Crawl datasets like FineWeb-Edu and <span class="smallcaps">DCLM</span> achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing $90\%$ of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over <span class="smallcaps">DCLM</span>, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches <span class="smallcaps">DCLM</span> on MMLU, but contains four times more unique real tokens than <span class="smallcaps">DCLM</span>. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at <https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html>.
author:
- |
  **Dan Su<sup>\*</sup>**, **Kezhi Kong<sup>\*</sup>**, **Ying Lin<sup>\*</sup>**, **Joseph Jennings**, **Brandon Norick**,  
  **Markus Kliegl<sup></sup>**, **Mostofa Patwary**, **Mohammad Shoeybi**, **Bryan Catanzaro**  
    
  NVIDIA  
  <sup>\*</sup>Equal contribution. <sup></sup>Correspondence to <mkliegl@nvidia.com>.
bibliography:
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset"
---





# Introduction

<figure id="fig:main">
<span class="image placeholder" data-original-image-src="figures/figure1.pdf" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption>MMLU scores for 8B parameter models trained for 1T tokens. Compared to <span class="smallcaps">DCLM</span>, our methods enable us to either create a 4<span class="math inline">\(\times\)</span> larger dataset of similar quality or increase the MMLU using a high quality subset of the tokens. Having a larger dataset, in the sense of unique real tokens, is crucial when training over long horizons such as 15T tokens.</figcaption>
</figure>

Internet crawl is the largest source of unique tokens for training LLMs and can be seen as serving two main purposes: high-quality content and diversity. Recent English datasets derived from Common Crawl[^1] such as FineWeb-Edu  and <span class="smallcaps">DCLM</span>  have emphasized high-quality content that boosts benchmark accuracies over data quantity. They have demonstrated significant strides in achieving benchmark results competitive with some of the best closed models at a small scale (e.g., DCLM’s 7B model trained over 2.6T tokens), primarily thanks to the use of model-based filters to extract high-quality educational and instructional content. However, this comes at the cost of data quantity: they remove around $90\%$ of the data. Such aggressive pruning may not be the most effective strategy when training larger models over longer token horizons (e.g., Llama 3.1 includes 8B–405B parameter models, trained for 15T tokens  and Gemma 2 27B was trained for 13T tokens ). Both <span class="smallcaps">DCLM</span> and FineWeb-Edu contain around $80\%$ near-duplicates (1T and 0.2T unique tokens, respectively)  and to train on these datasets for many trillions of tokens implies seeing essentially the same samples many times during training. This could lead to inferior models, as find there are diminishing returns after four epochs compared to training on more unique tokens.

In this paper, we show how to achieve a better trade-off between benchmark accuracy and data quantity with a combination of classifier ensembling, synthetic data generation, and reduced reliance on heuristic filters. Our main contributions are:

1.  We propose a method for transforming English Common Crawl into a 6.3T token long-horizon pretraining dataset, consisting of 4.4T globally deduplicated original tokens and 1.9T synthetically generated tokens. We release the dataset[^2] and plan to release an implementation as part of the open-source NeMo Curator library.[^3]

2.  We prove the effectiveness of this method by comparing to the state-of-the-art open English Common Crawl datasets <span class="smallcaps">DCLM</span> and FineWeb-Edu (Figure ).

    1.  A 1.1T-token high-quality subset of our data achieves a 5.6 MMLU improvement over <span class="smallcaps">DCLM</span>, showing the superiority of our method over a relatively short token horizon.

    2.  Our full dataset performs on par with <span class="smallcaps">DCLM</span> while having 4$\times$ as many unique real tokens.

    3.  This larger size enables state-of-the-art results over long token horizons: An 8B parameter model trained for 15T tokens using a weighted version of our dataset achieves higher overall accuracy than Llama 3.1 8B, and in particular MMLU 70.3 vs. Llama’s 65.3. Note that Llama 3.1 8B was also trained on 15T tokens .

3.  We conduct ablation studies and find:

    1.  Ensembling different model-based classifiers can help select a larger and more diverse set of high quality tokens.

    2.  Rephrasing can effectively reduce noise and errors in low-quality data and produce diverse variants with fresh unique tokens from high-quality data, leading to better results in downstream tasks.

    3.  Disabling traditional non-learned heuristic filters for high-quality data can further boost high quality token yield without hurting accuracy.

Finally, we remark that our overall guiding principle is to shift from a static, non-learned, heuristic pipeline towards a more learned flywheel whose performance will naturally get better over time. As our data improves, so will the LLMs we train, and these improved LLMs will in turn improve our data as we use them to generate better synthetic data and quality classifications.

# Methods

In this section we explain our efforts to build the best English Common Crawl pretraining dataset for LLMs. Our efforts can be splitted into three folds. First, we talk about our efforts in boosting token yield by utilizing text extractor and heuristic filters more properly in Section . Second, we introduce the model-based quality labeling pipeline methods in Section . Third, we introduce our synthetic data generation method to further improve the data quality in Section .

## HTML-to-text Extractor & Filter

Extracted texts from HTMLs are the foundation and major source of LLM pretraining dataset, so it is of great significance to analyze and understand the extraction tools for optimal data quality and token yield. Moreover, heuristic filters are often utilized to remove low-quality tokens with human-designed heuristics , which may also put good tokens at the risk of being removed. We carefully examine both aspects with the assist of the FineWeb-Edu classifier , a model-based quality classifier that had shown effectiveness in identifying high-quality tokens that are significant in boosting the strength of LLMs.

<div id="table:extract_filter_stats">

|                      | **\#Tokens** | **\#HQ tokens** | **\#HQ +%** |
|:---------------------|:------------:|:---------------:|:-----------:|
| Trafilatura-filtered |     994      |       80        |     \-      |
| Justext-filtered     |    1,380     |       104       |    28.6%    |
| Justext              |    1,804     |       127       |    57.4%    |

Extraction and filteration token count statistics (billion). Tokens counted after deduplication.

</div>

#### HTML-to-text Extraction

We test two HTML-to-text extractors, Justext  and Trafilatura . Qualitatively, we view both extractors at the same level of quality. While quantitatively, we calculate token yields of both extractors on 13 selected snapshots of Common Crawl (see Appendix ). The statistics were reported in Table . We see that Justext can yield more tokens, notably more high-quality tokens (+28.6%) by the standard of Fineweb-Edu classifier (score 3, 4, and 5). We highlight that, boosting unique token amount is of great importance when building long-horizon pretraining dataset, e.g., 15T tokens for Llama3.1. We keep only English text, as determined by pycld2[^4] and the FastText lid176 language classifier[^5] with threshold 0.3 . After extraction, we apply global fuzzy deduplication as well as exact substring deduplication , using the NeMo Curator library[^6] and the deduplicate-text-datasets library,[^7] respectively.

#### Filtering

Conventionally, heuristic filters are leveraged to remove low-quality tokens from the pretraining dataset as a post-processing step . We revisit the filtering pipeline as in . Such pipeline sequentially consists of a set of heuristic filters proposed by and a perplexity filter based on a KenLM model  trained on Wikipedia and books data . To quantitatively better understand the effectiveness of the filtering pipeline, we calculate the token yield and report the numbers in Table . We find the filtering pipeline removes a non-trivial portion of high-quality tokens (-18.1%) classified by FineWeb-Edu classifier from the dataset.

Given the impact that the heuristic filters have on the high-quality token yield, we propose to NOT apply such filters to the high-quality tokens distinguished by model-based quality classifers (described in the next section), but only use those on the low-quality splits. In the experiment section we empirically verify the impact of both the extractor and filter on pretraining data quality through downstream benchmarks. We refer readers to Section  for detailed results.

## Model-based Quality Labeling

Recent work  use model-based classifiers to extract high-quality pretraining documents from English Common Crawl. However, both of the two quality classifiers have a limited recall (less than 10%) of high-quality tokens, and this will become a bottleneck to train an LLM over a long horizon. Also, the quality labels assigned by the quality classifier are not necessarily aligned with LLM’s downstream task performance. Therefore, we propose our ensemble-based quality labeling pipeline method. Specifically, we first build three quality classifiers, each of which has different high-quality preferences. Then, we ensemble the three classifiers to score all the documents, and split the crawl corpus into different quality buckets based on the quality score. Finally, we regroup the fine-grained document buckets into 5 different quality levels based on their corresponding performance on downstream task.

#### Quality Classifier Training

Preparing pretraining documents with quality annotations is the first key step in building a quality classifier . Similar to the work  [^8], we constructed two versions of quality annotation data. We prompt Mistral 8x22B-instruct[^9] and Nemotron-340B-instruct , to score web documents from FineWeb based on their educational value on a scale from 0 to 5. We then fine-tune a linear regression model on top of the Snowflake-arctic-embed-m embedding model  using the two different version of training sets. The two models have been trained for 20 epochs with a learning rate of 3e-4, with the embedding and encoder layers frozen, and we selected the checkpoint with the highest F1 score on the held-out validation set.

We also employ the DCLM classifier which is a fastText-based classifier released by . The DCLM classifier is trained on a combination of instruction-formatted data  and high-scoring posts data from ELI5 subreddit , and has shown stronger performance in identifying high-quality pretraining tokens, compared to the FineWeb-Edu classifier  . The DCLM classifier will offer a new perspective in labeling high-quality pretraining documents, and will help increase the recall of high-quality tokens.

#### Quality Scoring and Bucketing

First, we use each of the three classifiers to predict the quality scores for all the documents. Then based on the ranked quality score from each classifier, we rounded the model’s output score to integers from 0 to 19. So that each score bucket will have around 5% of the documents, and bucket 19 will have the top 5% highest quality documents. We then assign the final quality score for each document by ensembling the three classifiers’ integer score by a maximum operation. The number of documents distribution in each buckets will be skewed by the ensemble operation.

#### Quality Labeling

In order to assign a quality label that is more aligned with their real performance on downstream tasks, we further group the fine-grained quality score predicted by three classifiers into 5 downstream quality categories. We used annealing to access each data bucket’s downstream task’s quality. Specifically, we measure the quality of each bucket by continuous pretraining with 50B tokens on a 70% trained 8B models. We assign 66% of weight to the default data mix and 34% to the dataset that we are evaluating. By comparing the average performance of each bucket over 9 tasks, we group the 20 buckets into 5 big categories, with the final distribution shown in Table .

<div id="table:quality_label_stats">

| **Quality Label** | **Buckets** | **Token ($\%$)** |
|:------------------|:-----------:|:----------------:|
| High              |     19      |      12.63       |
| Medium-High       |     18      |      11.52       |
| Medium            |    12-17    |      46.24       |
| Medium-Low        |    7-11     |      20.43       |
| Low               |     0-6     |       9.18       |

Common Crawl quality labels statistics.

</div>

## Synthetic Data Generation

Upon reviewing samples across the quality tiers, we observe that documents with lower scores tend to contain more noise and errors, while those scoring higher generally exhibit good writing and formatting. Therefore, we employ different strategies when generating data from low- and high-quality documents.

For low-quality data, our goal is to improve the quality by reducing noise and errors while preserving useful information, thereby decreasing training compute expenses. As shown by , rephrasing web data using a medium-sized language model yields an enhanced parallel corpus of synthetic data, thereby reducing model perplexity and boosting its accuracy on downstream tasks. Unlike existing methods that create new content such as textbooks and short stories , our rephrasing-based approach does not utilize the language model as a knowledge bank but focuses on transforming provided texts into another style, allowing it to operate with a lighter-weight model. We adopt the <span class="roman">Wikipedia</span> style prompt from  to rewrite low-quality documents (Prompt 5 in Appendix ), which effectively reduces errors and redundancies and improves formatting.

For high-quality data, we aim to obtain more unique tokens and condense essential knowledge. According to , adding repeated tokens yields a diminishing return, especially after 4 epochs. For high-quality documents, we generate synthetic data using four additional prompts: (1) <span class="roman">Diverse Question-Answer (QA) pairs</span>: ask questions in various forms (e.g., yes/no question, open-ended question, multi-choice question) about factual information in the text and provide the correct answers; (2) <span class="roman">Distill</span>: rewrite the text into a concise and clear passage; (3) <span class="roman">Extract knowledge</span>: rewrite knowledge from the text and disregard uninformative content; (4) <span class="roman">Knowledge list</span>: extract key information from the text as an organized list. We require the model to provide clear and concise responses while preserving factual information and concrete details such as numbers. The full prompts are shown in Appendix .

As we increase the length of provided text, the model shows a tendency to produce over-simplified outputs with reduced detail. Therefore, we chunk each document into segments, each of which contains one or more complete lines and is shorter than a specific token limit.[^10] Over-length lines exceeding the token limit are discarded.

Our post-processing steps include removing incomplete results, eliminating specific Markdown formatting (e.g., double asterisks), stripping away prefixes of certain patterns (e.g., “*Here is a paraphrased version:*” and “*Paraphrased Text:*”), removing quotation marks enclosing the entire response, and filtering out under-length outputs (i.e., shorter than 50 tokens). For <span class="roman">Wikipedia</span> results, we concatenate passages generated from segments belonging to the same original document. For <span class="roman">Diverse QA Pairs</span> results, we shuffle the generated question and answer pairs, retain up to a number based on the length of the segment, and append the pairs to the end of the segment.

Using the instruct version of Mistral NeMo 12B[^11] with FP8 inference, we synthesize over 1.8T tokens as Table  shows, including 336.3B tokens from low-quality documents and 1.5T tokens from high-quality documents. We do not use medium-quality documents for synthetic data generation. We employ TensorRT-LLM[^12] and NeMo-Skills[^13] to enable large-scale data synthesis.

<div id="table:synthetic_data_stats">

| **Source**                      | **\#Raw** | **Prompt**                                   | **\#Synthetic** |
|:--------------------------------|:---------:|:---------------------------------------------|:---------------:|
| <span class="roman">Low</span>  |   403.0   | <span class="roman">Wikipedia</span>         |      336.3      |
| <span class="roman">High</span> |   451.3   | <span class="roman">Wikipedia</span>         |      372.9      |
|                                 |           | <span class="roman">Diverse QA Pairs</span>  |      499.5      |
|                                 |           | <span class="roman">Distill</span>           |      157.6      |
|                                 |           | <span class="roman">Extract Knowledge</span> |      303.6      |
|                                 |           | <span class="roman">Knowledge List</span>    |      203.2      |

Synthetic data token count statistics (billion).

</div>

## Putting It All Together

<div id="table:dataset_sizes">

| **Dataset**                         | **Total** | **Unique** | **Synthetic** |
|:------------------------------------|:---------:|:----------:|:-------------:|
| FineWebEdu-2                        |    5.4    |    1.1     |      \-       |
| FineWebEdu                          |    1.3    |    0.2     |      \-       |
| <span class="smallcaps">DCLM</span> |    3.8    |    1.0     |      \-       |
| Nemotron-CC                         |    6.3    |    4.4     |      1.9      |
| Nemotron-CC-HQ                      |    1.1    |    0.6     |      0.5      |

Dataset sizes in trillions of tokens. "Unique" shows the estimated number of tokens after global fuzzy deduplication of the real tokens.

</div>

Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens. This dataset has roughly 4$\times$ more unique tokens than FineWebEdu-2 and <span class="smallcaps">DCLM</span>, since both of those datasets only underwent a sharded form of approximate deduplication and contain roughly $80\%$ fuzzy duplicates . To enable a fairer comparison over relatively short token horizons, we thus also consider a 1.1T token high quality subset of our data (Nemotron-CC-HQ), consisting of just the highest-scoring real and diverse QA pairs synthetic data. The size breakdown of the datasets is shown in Table .

# Experiments

## Experiment Setup

#### Training Setup

We use the open source Megatron-LM library[^14]  to train standard 8B parameter transformer LLMs. The hyperparameter details are shown in Appendix .

#### Data Blend

Unless otherwise noted, we train for 1T tokens on a blend of $73\%$ English Common Crawl data and $27\%$ a fixed mix of specialized math, code, papers, books, patents, and Wikipedia datasets . When comparing datasets, we vary only the $73\%$ English Common Crawl portion.

#### Evaluation Setup

We use the open source LM Evaluation Harness library[^15]  to evaluate on the following ten common sense and reasoning tasks (reported metric in parentheses): ARC-Easy and ARC-Challenge (normalized accuracy) , Hellaswag (normalized accuracy) , Winogrande (accuracy) , RACE (accuracy) , PIQA (normalized accuracy) , Social IQA (accuracy) , Commonsense QA (accuracy) , Openbook QA (normalized accuracy) , and MMLU (accuracy) .

## Main Results

#### Short Token Horizon (1T)

To validate the quality of our datasets, we first train standard 8B parameter transformer LLMs over a relatively short 1T token horizon. The results are shown in Table . Our high quality dataset (Nemotron-CC-HQ) shows accuracy gains over <span class="smallcaps">DCLM</span> and FineWeb-Edu on all tasks except RACE. In particular, there is a 5.6 MMLU and 3.1 average gain over <span class="smallcaps">DCLM</span>. This shows the effectiveness of our classifier ensembling and synthetic data even in the non-data-constrained setting. Our complete 6.3T token dataset (Nemotron-CC) gives MMLU and average accuracies roughly on par with <span class="smallcaps">DCLM</span>. But since this dataset contains 4$\times$ more unique real tokens, we expect it to be superior in data-constrained settings like 15T token training runs.

#### Long Token Horizon (15T)

Our dataset contributed 7.2T of the tokens used to train an 8B model for 15T tokens. As shown in Table , our model achieves a higher average accuracy than Llama 3.1 8B, which was also trained for 15T tokens, including an MMLU score of 70.3 vs. Llama’s 65.3. This shows that our dataset is indeed suitable for state-of-the-art training over long token horizons.

## Ablation Study

To further investigate the contribution and effect of each module in our method, we conducted thorough ablation studies.

#### Extractor & Filter Comparison

As we have discussed in Section , by deploying Justext instead of Trafilatura and removing filter from the post-processing step, we can attain significantly 57.4% more high-quality tokens. We also conduct ablation studies to better understand the impact of the extractor selection and the removal of filter through downstream benchmarks. We carry out four 8B-1T experiments. We report the benchmark scores in Table . Beyond the token-yield benefit by leveraging Justext instead of Trafilatura and not using heuristic filters, we see that combining these two does not impact the downstream task accuracies with only marginal differences (comparing Trafilatura filtered vs. Justext unfiltered). Moreover, when we ONLY remove filter from high-quality tokens, the results get further improved (comparing Justext unfiltered vs. Justext HQ unfiltered). In particular, MMLU gets boosted by +2%. Note that, the motivation behind removing filter is to boost token yield, especially on high-quality tokens due to the notable scarcity of such. Given the experimental results and considering the overall growth in token yield, we opt to only remove filter from high-quality tokens.

<div id="table:extactor_filter_ablation">

| **Exp name**          | **MMLU** | **Avg (non-MMLU)** |
|:----------------------|:--------:|:------------------:|
| Trafilatura filtered  |   55.4   |        60.6        |
| Justext filtered      |   54.1   |      **60.9**      |
| Justext unfiltered    |   55.5   |        60.3        |
| Justext HQ unfiltered | **57.5** |        60.6        |

Ablation studies on extractor and filter. HQ means high-quality data judged by FineWeb-Edu classifier (score 3,4,5). See Appendix for more details.

</div>

#### Classifiers Comparison

Assembling different classifiers to label the document quality is one of the key steps in constructing our datasets, so we did thorough analysis and comparison of the component.

We did a detailed comparison of two types of classifiers that we employ in our method: the FineWeb-Edu classifier which score document quality based on their educational-level, and the DCLM-based classifier which value the informativeness of the document. We compare the high-quality documents predicted by the two classifiers on a randomly selected Common Crawl Snapshot (CC-MAIN-2021-21). Table  shows the document statistics comparison. We can see that only 10% of the documents are predicted as high quality by both classifiers, while 35.4% documents are predicted as high quality by FineWeb-Edu classifier only, and 54.4% of documents are predicted as high-quality by DCLM classifier. Therefore, ensembling different classifiers can increase the recall of high-quality documents from Common Crawl.[^16]

We further compare each of the classifiers with the ensembled method[^17] by their downstream tasks’ performances. We pretrain 8B parameters LLMs with 1T tokens, using the high-quality documents labeled by different classifiers on randomly selected 13 Common Crawl snapshots (see Appendix ). Table  shows the detailed comparison on different evaluation tasks. We can see that the ensembled method greatly boost the high-quality tokens percentage from 9% to 25%, while still achieving the highest general language understanding performance on average on all the tasks. The ensembled method also outperforms the FineWeb-Edu classifier and the DCLM classifier, in terms of the high-quality token percentage, and is on-par or slightly better on the 9 evaluation tasks. This is very important since more unique high-quality tokens is the key in pretraining larger LLMs on longer tokens horizons.

#### Evaluating Synthetic Data

As Table  shows, this ablation study aim to answer two questions: (1) Does rephrasing low-quality improve accuracies on downstream tasks? (2) Can synthetic data help offset the decreasing value of duplicated data reported in ? To answer these questions, we train four 8B models with the same hyperparameters on different blends of 1T tokens: (1) LQ-Base: original Common Crawl data; (2) LQ-Synthetic: an augmented version of LQ-Base where the low-quality documents are rephrased; (3) HQ-Base: a blend containing eightfold high-quality documents and less low- and medium-quality documents; (4) HQ-Synthetic: a variant of HQ-Base where 4 repetitions of the high-quality documents are swapped out for synthetic datasets.

By comparing the results between LQ-Base and LQ-Synthetic, we can see that rephraing low-quality data leads to 1.50 absolute gains on average score. We also observe noticeable boosts from 1.80% to 4.75% on ARC-Easy, ARC-Challenge, OpenbookQA, CommonsenseQA; however, we also encounter slight accuracy drops on some tasks, which may indicate potential misinformation introduced by data synthesis. Current practices typically utilize data curation approaches to detect and eliminate noisy examples. Due to time and resource constraints, we leave the detailed exploration of this issue for future efforts.

The comparison between HQ-Base and HQ-Synthetic shows that swapping 4 out of 8 epochs of high-quality data with a mix of synthetic datasets improves accuracy on most benchmarks. This improvement could potentially result from two factors: the incorporation of fresh unique tokens and styles that enable the model to learn specific abilities (e.g., question answering) or absorb knowledge more efficiently.

# Related Work

The Phi series of models pioneered training on small amounts of very high quality data, including curated Web and synthetic data . However, their focus is on shorter token horizons and they share limited details. FineWeb-Edu and <span class="smallcaps">DCLM</span> are the main points of comparison for our paper . We build upon their core idea of model-based filtering, but show how to improve the filtering and data quantity through a combination of other techniques. Other English Common Crawl datasets such as C4, DOLMA, Gopher, RefinedWeb, TxT360 largely focus on extraction and non-learned heuristics . Just as for FineWeb-Edu and <span class="smallcaps">DCLM</span>, the core pipeline we started from incorporates many of these ideas, but our paper describes how to modify and go beyond these non-learned techniques to achieve state-of-the-art accuracy and diversity. Concurrent work Zyda-2 shows how to filter, cross-deduplicate, and combine the FineWeb-Edu, <span class="smallcaps">DCLM</span>, Zyda-1, and Dolma-CC datasets into a higher-accuracy and larger whole . In contrast, we focus on techniques for the creation of a new English Common Crawl dataset rather than combinations or modifications of existing datasets. Finally, many works have focused on creating multilingual datasets . We leave extension of our ideas beyond English to the future.

Synthetic datasets have been widely used in language model pre-training and post-training. In , the authors show that smaller or simpler models trained on a synthetic dataset of short stories are capable of generating fluent and consistent stories. Similarly, smaller models trained using high-quality synthetic textbook and exercise datasets can achieve impressive high accuracy on coding benchmarks . These approaches typically require a powerful language model, such as GPT-3.5 and GPT-4 in , to synthesize new contents. Instead, shows that compact models such as Qwen-1.8B and Mistral-7B are adequate to rephrase web data. This approach generates diverse, high-quality synthetic data that effectively lowers model perplexity and boosts performance across benchmarks. We adopt this main idea, but explore more prompts and show how to specialize them for low and high quality data.

# Conclusion

For producing long-horizon pretraining tokens for LLM from English Common Crawl data, we showed how to improve upon the state of the art and achieve better trade-offs between benchmark accuracy and data quantity, as measured by number of unique real tokens. Specifically, we showed the efficacy of ensembling model-based quality filters, rephrasing low and high quality documents, and reducing the reliance on non-learned heuristics.

# Limitations

Some of the key limitations of our work are as follows. For the model-based filter ensembling and quality bucketing, we only had time and resources to try a single strategy. Though it is effective, it is possible this could be improved upon in future work, especially to improve the sensitivity at the higher-quality end of the spectrum. For the rephrased data, we did not verify the factual accuracy or fidelity to the original contents. More work is required to understand the risks of hallucinations or loss of content diversity in this setting and how to mitigate them. We also only looked at rephrasing low and high quality data. It could be interesting to explore how to best rephrase medium quality data as well. We did not do ablations on all parts of the pipeline. There is probably room for improvement with, for example, the language identification. Finally, we tried our methods only on English text. More work is needed to adapt our methods to other languages.

# Acknowledgments

We thank the Common Crawl Foundation for hosting the dataset. We thank Pedro Ortiz Suarez for valuable feedback that improved the paper and Greg Lindahl for help with improving the data formatting and layout.

# Comparison of FineWeb-Edu and DCLM Classifier

Different classifiers have different standards for high-quality documents. Thus, ensemble multiple classifiers will help increase the recall of high-quality documents. We did a detailed comparison of two of the classifiers that we employ in our method: the FineWeb-Edu classifier which score document quality based on their educational-level, and the DCLM based classifier which value the informativeness of the document.

We compare the high-quality documents predicted by the two classifiers on one Common Crawl snapshot (dated 2021-21). Table  show the document statistics comparison. We further show the detailed URL domains comparison between the two classifiers’ predictions in Table . We can see that each classifier has their own high-quality domain preferences. Among the top 1k domains, only 368 domains are in the intersection. Therefore, ensemble of different classifiers can help increase retrieving more high-quality documents from Common Crawl.

# Bucket Comparison

To better understand the quality of data in each of our 20 data buckets, we carry out ablation studies to test their benchmark accuracies. For each study, we take a 900B-token checkpoint and continue the pretraining for 50B more tokens. For 34% of the 50B tokens we used the bucket data being tested, while we fixed the other 66% as the same data distribution of the 900B pretraining process to make sure the distribution did not shift too much. See Figure  for the results. The average accuracy is calculated across 13 downstream tasks. Note that Bucket 19 greatly outperforms all other buckets and the differences within bucket 12-18 are marginal. We used the results here as a reference when designing the quality labels in Table .

<figure id="fig:ablation_buckets">
<span class="image placeholder" data-original-image-src="figures/ablation_buckets.pdf" data-original-image-title="" width="40%"></span>
<figcaption>Ablation study on the buckets.</figcaption>
</figure>

# Training Details

As mentioned in Section , we use the open source Megatron-LM library[^18]  to train 8B parameter transformer LLMs for 1T tokens. The key hyperparameters are as follows: We use 32 transformer layers with hidden dimension 4096, 32 attention heads, and SwiGLU activations . For the attention, we use grouped query attention with 8 query groups . We use the Adam optimizer with $\beta_1 = 0.9, \beta_2 = 0.95, \epsilon=1e{-8}$, weight decay $0.1$, and the cosine learning rate schedule with peak learning rate at 3e-4 and minimum learning rate at 3e-6. A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs.

# Common Crawl Snapshots

For the main datasets, we used the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30.

The thirteen Common Crawl snapshots we use in some of the analysis and 1T token experiments are CC-MAIN-2023-23, CC-MAIN-2023-14, CC-MAIN-2023-06, CC-MAIN-2022-49, CC-MAIN-2022-27, CC-MAIN-2022-05, CC-MAIN-2021-43, CC-MAIN-2021-21, CC-MAIN-2021-04, CC-MAIN-2020-45, CC-MAIN-2020-29, CC-MAIN-2020-05, CC-MAIN-2019-35.

# Extractor & Filter Ablation

The Avg tasks include ARC-Easy, ARC-Challenge, Hellaswag, Winogrande, RACE, PIQA, Commonsense QA, Openbook QA.

Note that we only use FineWeb-Edu classifier for the quality labels of this ablation study and analysis. We do not use it in the final preparation of our dataset. See Section for the details of our classifiers being used eventually to prepare the data.

# Prompt Templates

Prompts 1-5 show the prompt templates we use for synthetic data generation.

[^1]: <https://commoncrawl.org/>

[^2]: <https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html>

[^3]: <https://github.com/NVIDIA/NeMo-Curator>

[^4]: <https://pypi.org/project/pycld2/>

[^5]: <https://fasttext.cc/docs/en/language-identification.html>

[^6]: <https://github.com/NVIDIA/NeMo-Curator>

[^7]: <https://github.com/google-research/deduplicate-text-datasets>

[^8]: We use the same 460K document samples as in the FineWeb-Edu-Annotation dataset.

[^9]: <https://mistral.ai/news/mixtral-8x22b/>

[^10]: The token limit is set to 512 for <span class="roman">Wikiepdia</span>, 2,000 for <span class="roman">Distill</span>, 1,400 for <span class="roman">Extract Knowledge</span> and 1,000 for <span class="roman">Diverse QA Pairs</span> and <span class="roman">Knowledge List</span>, including tokens from the prompt and chat format.

[^11]: <https://mistral.ai/news/mistral-nemo>

[^12]: <https://github.com/NVIDIA/TensorRT-LLM>

[^13]: <https://github.com/NVIDIA/NeMo-Skills>

[^14]: <https://github.com/NVIDIA/Megatron-LM>

[^15]: <https://github.com/EleutherAI/lm-evaluation-harness>

[^16]: Detailed URL domain comparison can be found in Appendix 

[^17]: Note that we did not employ FineWeb-Edu classifier in our ensemble for license issue, since it is trained with annotations from Llama3.

[^18]: <https://github.com/NVIDIA/Megatron-LM>
