{
  "arxivId": "1906.04358",
  "title": "Weight Agnostic Neural Networks",
  "authors": "Adam Gaier, David Ha",
  "abstract": "Not all neural network architectures are created equal, some perform much\nbetter than others for certain tasks. But how important are the weight\nparameters of a neural network compared to its architecture? In this work, we\nquestion to what extent neural network architectures alone, without learning\nany weight parameters, can encode solutions for a given task. We propose a\nsearch method for neural network architectures that can already perform a task\nwithout any explicit weight training. To evaluate these networks, we populate\nthe connections with a single shared weight parameter sampled from a uniform\nrandom distribution, and measure the expected performance. We demonstrate that\nour method can find minimal neural network architectures that can perform\nseveral reinforcement learning tasks without weight training. On a supervised\nlearning domain, we find network architectures that achieve much higher than\nchance accuracy on MNIST using random weights. Interactive version of this\npaper at https://weightagnostic.github.io/",
  "url": "http://arxiv.org/abs/1906.04358v2",
  "issue_number": 235,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/235",
  "created_at": "2024-12-24T04:16:45.841019",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 20,
  "last_read": "2024-12-24T04:16:45.842903"
}