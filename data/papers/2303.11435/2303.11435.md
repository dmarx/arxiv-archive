---
abstract: |
  Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called “regression to the mean” effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.

  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.

  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising.
author:
- |
  Mauricio Delbracio mdelbra@google.com  
  Google Research Peyman Milanfar milanfar@google.com  
  Google Research
bibliography:
- main.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  Inversion by Direct Iteration:  
  An Alternative to Denoising Diffusion for Image Restoration
---





# Introduction

Recovering a high-quality image from a low-quality observation is a fundamental problem in computer vision and computational imaging. Single image restoration is a highly ill-posed inverse problem where multiple plausible sharp and clean images could lead to the very same degraded observation. The typical supervised approach is to formulate image restoration as a problem of inferring the underlying image given a low-quality version of it, by training a model with paired examples of the relevant degradation  . One of the most common approaches is to directly minimize a pixel reconstruction error using the $L_1$ or $L_2$ loss; an approach that correlates well with the popular PSNR (peak signal-to-noise-ratio) metric. However, it has been observed often in recent literature that measures such as PSNR (and in general point-distortion metrics) do not correlate well to human perception . Despite these shortcomings, much of the recent research work has been focused on improving deep architectures and optimizing a variety of point-loss formulations, resulting in general models that give an aggregate improved image in one step of inference.

To see the issues more concretely, let’s assume that we are given image pairs $(\vx,\vy) \sim p(\vx, \vy)$ where $\vx$ represents a target high-quality image, and $\vy$ represents the respective degraded observation. For instance, $\vx$ may be pristine images degraded by a combination of blur/compression and noise to yield $\vy$. A typical regression approach would predict $\vx$ directly from $\vy$ using a trained model $\hat{\vx}(\vy) = F_\theta(\vy) \approx \vx$, by minimizing the expected pixel error in some (e.g. $L_p$) metric as follows: $$\min_\theta \E_{\vx,\vy} \| F_\theta(\vy) - \vx \|_p  \approx \min_\theta \sum_i \| F_\theta(\vy^i) - \vx^i \|_p.$$ In the case $p=2$, the minimum mean-squared error (MMSE) optimal solution is the conditional expectation: $\vx_{\text{MMSE}} (\vy) = \condExpec{\vx}{\vy} = \int \vx p(\vx \mid \vy) d\vx.$

This evidently results in an image that is the (weighted) average of all plausible reconstructions[^1]. This resulting image will not have a natural appearance as the details have been wiped out due to the effect aggregation (i.e. “regression to the mean”) effect. The problem is compounded for more ill-posed problems. That is, the more ill-posed the inverse problem, the larger the set of plausible reconstructions and therefore the more severe the effect of the aggregation implied by the expectation of the posterior. To mitigate this problem, recent works have introduced additional loss terms  that seek a balance in the formulation so the final image has improved perceptual quality (more on this in the next section).

In this work, we explicitly address this problem by avoiding single-step prediction of the clean image, and instead iterating a series of inferences, where at each step we solve an ‘easier’ (i.e., less ill-posed) inverse problem than the original. Specifically, we generate a sequence of intermediate restorations where at each step the goal is to reconstruct only a slightly less corrupted image. The core observation underlying this approach is this: *a small-step restoration largely avoids the regression-to-the-mean effect because the set of plausible ‘slightly-less-bad’ images is relatively small*. The core technical component enabling our approach is still a single deep model, but one that is trained to predict a better image given one with an intermediate level of degradation in the previous step, as summarized by Algorithm 1.

# Background

Recently, much work on imaging inverse problems has been focused on using generative formulations . Generative adversarial formulations train restoration networks with an adversarial loss that forces the restored image to be on the distribution of high-quality signals  . GANs are in general hard to train and also hard to control image hallucinations since the two terms play an antagonic role .

Image priors, including generative ones, can be used to solve inverse problems in an unsupervised fashion where the degradation operator is only know at inference time . DDPMs have been recently adapted for unsupervised model-based image restoration .

**How this approach compares to Denoising Diffusion.** Denoising Diffusion Probabilistic Models (DDPMs)  and Score-based models  have emerged as two powerful classes of generative models that produce high-quality samples by inverting a *known* diffusion (degradation) process. The standard Gaussian denoising formulation has been extended to more general corruption process  . The main common idea is to analytically define a known degradation process that is reversed to generate new samples starting from a fully degraded image (e.g., pure noise). The inference procedure makes use of the known analytical degradation at every step.

By contrast, in our formulation we do not require knowledge of any analytic form of the degradation process, we directly learn an iterative restoration process from low-quality/high-quality paired examples. This implies that we can apply our iterative procedure to virtually any degradation as long as we are given image pairs. Additionally, our formulation is motivated only from the idea of splitting the original inverse problem into multiple smaller ones. We do not require any knowledge of the underlying probability distributions, or the conditional distributions, at any step. Our inference procedure is solely based on the idea of restoring the signal a little bit at each step. This approach, with minimal assumptions, gives a unified formulation for any supervised image restoration problem under the same framework.

The natural extension of DDPMs to image restoration tasks is through the use of a *Conditional* DDPM models (cDDPM) . The goal of a cDDPM is to generate plausible reconstructions given the low-quality input (e.g., by generating samples from the posterior distribution). The idea is to train a supervised *denoising* diffusion model using paired examples that is conditioned on the low-quality input. The denoising network learns to generate a valid restored image (sample) by repeatedly denoising an initial image of pure noise. Our formulation has some similarity to conditional diffusion models, but contrary to the denoising formulation, we directly proceed by iteratively restoring the input image.

Overall, our method is straightforward to implement and train, and produces high-quality results. We evaluate the formulation on four different restoration tasks using different perceptual quality metrics. As shown, our method produces samples of higher quality than the state-of-the-art regression formulations while maintaining high-fidelity with respect to the original sample.

# Related Work

The goal of image restoration is to generate a high-quality image from its degraded low-quality measurement (e.g., low-resolution, compressed, noisy, blurry). Since the seminal super-resolution work of many recent image restoration methods adopt an end-to-end supervised formulation where a deep neural network is trained to directly produce a point estimate  These methods rely on low-quality high-quality image pairs to train a regression model. Most of the work has been focused on developing better and more powerful network architectures  so we can achieve better pixel-level reconstruction. While this formulation leads to state-of-the-art PSNR, the image generated is at best an average of all plausible solutions (regression to the mean). In the limit case where the low-quality image is completely obfuscated the best prediction in terms of PSNR is the average of the distribution.

Generative adversarial networks , and adversarial formulations  have been introduced to push the generated image towards the manifold of natural images. GANs suffer from unstable training , while being prone to introduce significant image hallucinations. This is a direct consequence of a non-reference formulation that directly tries to minimize the distance of the range of the generator to the manifold of natural images .

proved that there is a trade-off between image perceptual quality and distortion. It is not possible to minimize both distortion and perceptual quality simultaneously. In fact, minimizing the average point distortion (e.g., PSNR) can be only done in detriment of the perceptual quality .

A powerful way of avoiding the regression to the mean is to formulate the problem as one of sampling from the posterior distribution . An additional benefit of this formulation is to be able to generate multiple different plausible solutions that can be used for uncertainty quantification  or improving fairness .

Variational auto-encoders , Normalizing flows , and Diffusion probabilistic models (DPMs)  have been successfully applied to different image restoration tasks, where a diverse set of candidates can be generated from the learned posterior .

Denoising Diffusion Probabilistic Models (DDPMs) , Score-based models  and their recent generalizations  generate high-quality samples by inverting a known degradation process. The main strategy is to analytically define a known degradation process that is reversed to generate new samples starting from a fully degraded image (e.g., pure noise).  introduced Cold Diffusion a generative framework to generate images by reverting arbitrary (known) degradations. They show promising results even with non-stochastic degradations such as blur, masking or pixelization. The strategy is to define intermediate analytical degradations (diffusion) and then revert them step by step. presented Soft Diffusion a generalization of diffusion models to linear degradations. The authors argue that noise is a fundamental component that is needed to provably learn the score.

In this work, we adopt a similar strategy and propose to decompose the image restoration problem into a sequence of intermediate steps each of them being a much easier problem to solve (i.e., less ill-posed). This path of intermediate reconstructions takes us from a low-quality input to a high-quality reconstruction through a series of slightly less corrupted signals. Different than in traditional generative diffusion formulations, the degradation is only *implicitly* given through a series of pair images (low-quality and high-quality). In our formulation, we define the intermediate steps as a convex combination of the target/input signals. This induces a simple linear propagation from the high-quality sample to the low-quality one.

An alternative formulation of the supervised image restoration problem is to use a conditional denoising diffusion models to generate samples from the posterior distribution . The overall idea is to train a denoising diffusion model that is conditioned on the low-quality input. The denoising network learns to generate a valid restored image by repeatedly denoising an initial image of pure noise. Our formulation has some similarity to conditional diffusion models, but contrary to the denoising formulation, we directly proceed by iteratively restoring the input image.

The very recent work by  and , and the concurrent work by  introduce related image restoration techniques based on ODE/SDE diffusion formulations. A major difference with our work is that InDI is completely motivated and formulated from elementary principles by splitting the restoration tasks into multiple smaller ones. There is also significant amount of recent related work that analyzes the connection of diffusion image generation with Bridge and Flow matching, Optimal Transport, and Schrodinger bridges . Finally, the concurrent work of  adopts a linear diffusion scheme for image generation similar to, but less general than, the one in InDI.

# InDI: Our Proposed Formulation

Given $(\vx,\vy) \sim p(\vx,\vy)$, we define a continuous forward degradation process by $$\begin{aligned}
\vx_t = (1-t) \vx + t \vy, \quad \text{with} \,\, t \in [0,1].
\label{eq:model}
\end{aligned}$$ The idea of this forward process is that it starts from a clean sharp image at time $t=0$, and then degrades it to the blurry/noisy observation at time $t=1$. Here, $\vx_t$ indexed by $t$, represents an intermediate degraded image between the low-quality input $\vy$ (i.e., $t=1$) and the high-quality sharp target $\vx$ (i.e., $t=0$). Following the common notation in diffusion models, we will refer to the index $t$ as the time-step.

Our recovery method starts with the input degraded image (time $t=1$), and then at a given time-step $t$ generates the best possible reconstruction at time $t-\delta$. This can be done, for example, by the short-time conditional mean $\hat{\vx}_{t-\delta} = \condExpec{\vx_{t-\delta}}{\hat \vx_t}$ to the estimate. As we will show, by repeating this process we can thus invert the full degradation little by little. The following proposition provides the cornerstone of our approach. f

The proof is a direct consequence of change of variables and is given in Appendix .

According to this proposition, the posterior mean (e.g. MMSE estimate) at time $s<t$ can be deduced from the estimate at time $t$ by first estimating the clean image ($\vx = \vx_0$), and then doing a convex combination with the estimate $\hat \vx_s$ at time $s$. We can then apply the following scheme to move from $t$ to $s=t-\delta$, $$\begin{aligned}
\label{eq:iterative_scheme_ideal}
\hat{\vx}_{t-\delta} &= \condExpec{\vx_{t-\delta}}{\hat{\vx}_t} = \frac{\delta}{t} \condExpec{\vx_0}{\hat{\vx}_t} + \left(1-\frac{\delta}{t}\right)\hat{\vx}_t.
\end{aligned}$$ The process starts from $\hat{\vx}_1 = \vy$, and the step $\delta<1$ controls the “speed” of the reverse process (e.g., at constant “speed”, $\delta = \frac{1}{N}$, where $N$ controls the total number of steps).

An intuitive motivation for the requirement in Remark  is that we need to move through a path of plausible samples ${\hat{\vx}_t}$ at every step $t$. One simple way to guarantee this is by adding a small amount of noise to $\vy$. Then $p(\vy)$, and therefore $p(\vx_t)$ will be non-zero everywhere. More discussion about this is presented at the end of this section, but first, we present a toy example to motivate our approach.

#### A Toy Example:

Let us assume we observe noisy samples $\vy = \mH \vx + \vn$, where $\vn~\sim \mathcal{N}(0,\sigma^2Id)$, drawn from a discrete multimodal distribution $p(\vx) = \sum_{i=1}^d w_i \delta_{\vx-\vc_i}$, where $\vc_i \in \mathbb{R}^N$ and $w_i \ge 0$ and $\sum_{i=1}^d w_i=1$.

Let $\vx_t$ be the intermediate degraded samples according to . In this simple example, there is a closed form expression for all posterior distributions and conditional means; namely, $$\begin{aligned}
    p(\vx_t | \vx) = G\left(\frac{\vx_t - \mH_t \vx}{\sigma_t}\right) \quad \text{and} \quad
    p(\vx_t) = \sum_{i=1}^d w_i G\left(\frac{\vx_t - \mH_t \vc_i}{\sigma_t}\right),
\end{aligned}$$ where $\mH_t = (1-t)\mI + t\mH$ and $G(\vx)$ is a Gaussian kernel with identity covariance and $\sigma_t = t \sigma$. Then, $$\E_{\vx \sim p(\vx | \vx_t)}[\vx] = \int \frac{p(\vx_t | \vx) p (\vx) }{p(\vx_t)} \vx d\vx=
\frac{\sum_{i=1}^d \vc_i w_i G\left(\frac{\vx_t - \mH_t\vc_i}{\sigma_t}\right)}{\sum_{i=1}^d w_i G\left(\frac{\vx_t - \mH_t\vc_i}{\sigma_t}\right)}$$ The iteration from  becomes: $$\hat{\vx}_{t-\delta} = \frac{\delta}{t} \frac{\sum_{i=1}^d \vc_i w_i G\left(\frac{\hat \vx_t - \mH_t\vc_i}{\sigma_t}\right)}{\sum_{i=1}^d w_i G\left(\frac{\hat \vx_t - \mH_t\vc_i}{\sigma_t}\right)} + \left(1-\frac{\delta}{t}\right)\hat{\vx}_t.$$ Figure  shows the results of applying the iterative regression scheme given by the above equation in two different examples. The iterative regression converges to one of the four possible modes (shown in orange), while the regression to the mean is always a weighted average of all possible modes (i.e., a blurry reconstruction, shown in red).

**Training and Inference:** InDI ideal iterative scheme given by  requires to compute an estimate of the clean image at every step $t$ (i.e., $\condExpec{\vx_0}{\vx_t}$). To that extent, we train a family of regressors $F_\theta(\cdot; t)$, each of them specialized in reconstructing $\vx_0$ from $\vx_t$ at a given $t$. That is, $$\begin{aligned}
\min_\theta \E_{\vx,\vy \sim p(\vx,\vy)} \E_{t\sim p(t)}\| F_\theta(\vx_t,t) - \vx \|_p,
\end{aligned}$$ where $p(t)$ is a predefined distribution for $t$ (e.g., uniform). The model $F_\theta$ allows us to do incremental reconstruction where from time step $t$ we predict the slightly less corrupted signal at time $t-\delta$ as given in . Thus, the iterative scheme becomes: $$\begin{aligned}
\hat \vx_{t-\delta} &= \frac{\delta}{t} F_\theta(\hat \vx_t, t) +  \left(1-\frac{\delta}{t} \right)\hat \vx_t, 
\label{eq:baseline_sampler}
\end{aligned}$$ where $0 < \delta \le 1$. Although $\delta$ could be a function of time, in practice we use a constant time step, $\delta = \frac{1}{N}$, where $N$ is the number of steps.

#### InDI as a Residual Flow ODE:

In the limit, as $\delta \to 0$, leads to an ordinary differential equation (ODE); namely $$\begin{aligned}
\frac{ d\vx_t}{d t} = \lim_{\delta \to 0} \frac{\vx_t - \vx_{t-\delta}}{\delta} = \frac{\vx_t - F_\theta(\vx_t, t)}{t},
\label{eq:ode_formulation}
\end{aligned}$$ where in the ideal case $F_\theta(\vx_t,t) = \condExpec{\vx_0}{\vx_t}$. The ODE can be interpreted as a “residual flow” because the right-hand side is the (normalized) residual of the inversion process at time $t$. We are interested in the solution of this equation at $t=0$, starting from the initial condition $\vx_1=\vy$ at $t=1$. The residual flow formulation can be used to develop other numerical procedures using standard ODE solvers. Exploring this is left to future work.

Another use of the continuous formulation is to understand the behavior of the proposed iterative procedure in terms of concrete examples. In Appendix  we show how the residual flow can be used to analyze the specific case where the prior is Gaussian and the restoration task is denoising.

#### Connection to Denoising Score-Matching and Probabilistic ODE:

An interesting connection emerges when the degradation is Gaussian noise (standard deviation $\sigma^2$). In this case, InDI’s ODE in  boils down to the score-matching probabilistic ODE of . More specifically, let $\vx_t = \vx + t \vn$, so the noise level in $\vx_t$ is $\sigma^2_t = t^2 \sigma^2$. The probabilistic flow ODE (Eq(13) in ) is given by $$\frac{d\vx_t}{dt} = -\frac{1}{2}\frac{d(\sigma_t^2)}{dt} \nabla_{\vx_t} \log p_t(\vx_t) = -t \sigma^2  \nabla_{\vx_t} \log p_t(\vx_t).$$ According to the denoising score-matching (DSM) approximation (), $$-\nabla_{\vx_t} \log p_t(\vx_t) \approx \frac{\vx_t - F_\theta(\vx_t,t)}{\sigma^2_t} = \frac{\vx_t - F_\theta(\vx_t,t)}{t^2\sigma^2},$$ so we end up recovering precisely the same ODE as in InDI. Furthermore, when $F_\theta(\vx_t,t)= \condExpec{\vx_0}{\vx_t}$ (i.e., MMSE estimator) the DSM approximation is exact and the relation is given by Tweedie’s formula .

**Stochastic Perturbation:** To make sure we have the regularity requirements for the iterative procedure from to be well defined (Remark ), we add a small amount of white noise to the low-quality input. As shown in Section  this leads to a significant improvement in image quality in certain tasks (in particular those that are restorations from deterministic degradations).

Our model with this noise perturbation becomes: $$\begin{aligned}
\vx_t = (1-t) \vx + t \vy' = (1-t) \vx + t \vy + t \epsilon \vn \quad \text{with} \,\, t \in [0,1],
\label{eq:model_noise_constant}
\end{aligned}$$ where $\vy' = \vy + \epsilon \vn$, $\epsilon$ is a small constant (e.g., $\epsilon=0.01$, where image values are in $[-1,1]$), and $\vn \sim \mathcal{N}(0,Id)$.

A slightly more general formulation incorporates the perturbation as a general Brownian motion, where we can explicitly control the level of noise at each step. That is, $$\begin{aligned}
\vx_t = (1-t) \vx + t \vy + \sqrt{t} \epsilon_t \veta_t \quad \text{with} \,\, t \in [0,1],
\label{eq:model_noise_brownian}
\end{aligned}$$ where $\epsilon_t$ is a non-negative function, and $\veta_t$ is the standard Brownian motion having zero mean and covariance $t\mI$ at index $t$.

In this more general setting, the base training objective becomes $$\begin{aligned}
\min_\theta \E_{\vx,\vy \sim p(\vx,\vy)} \E_{t\sim p(t)} \E_{\vn \sim \mathcal{N}(0, Id)}\left\| F_\theta \left((1-t) \vx + t \vy + t\epsilon_t\vn ; t \right) - \vx \right\|_p.
\label{eq:full_loss}
\end{aligned}$$ And as a result, the general inference procedure in  becomes: $$\begin{aligned}
\hat \vx_{t-\delta} &= \frac{\delta}{t} F_\theta(\hat \vx_t, t) + \left(1-\frac{\delta}{t} \right) \hat \vx_t + (t-\delta)\sqrt{\epsilon_{t-\delta}^2 - \epsilon_t^2}\vzeta 
\label{eq:baseline_sampler_noise},
\end{aligned}$$ where the reconstruction process starts from $t=1$, $\hat  \vx_{1} = \vy + \epsilon \vn$ and $\vn \sim \mathcal{N}(0, Id)$. At each step, a new $\vzeta \sim \mathcal{N}(0, Id)$ is sampled and noise is added to the current state. The added Gaussian noise is such that the noise at time $t$ has variance $t^2{\epsilon_t}^2$ as required by . To be well defined $\epsilon_t$ needs to be a non-negative non-increasing function of $t$. In the limit case where $\epsilon_t = \epsilon$ we are in the simplified case given by , while if $\epsilon_t = \frac{\epsilon}{\sqrt{t}}$ the noise perturbation is a pure Brownian motion.

Our full iterative restoration inference scheme is given in Algorithm .

# Experiments

We train and evaluate our framework on four widely popular image restoration tasks: motion deblurring, defocus deblurring, compression artifacts removal and single image super-resolution. Our formulation is generative-based, and we show that can be used for image generation even if this is not the main focus of the present work. To evaluate the quality of the proposed method we compute several distortion based and perceptual metrics: PSNR, LPIPS , FID (Fréchet Inception Distance) , and KID (Kernel Inception Distance) .

**Perception–Distortion tradeoff :** To illustrate the potential of the method, we present results when using different number of steps for the reconstruction. This has a direct impact on the perception–distortion tradeoff. In general, a single step reconstruction with our model, will lead to an estimate that minimizes the average point distortion (e.g., PSNR) but this can be only done to the detriment of the perceptual quality.

**Model Architecture and Training:** We adopt a U-Net-like architecture similar to the ones in diffusion strategies . Following  we removed attention layers and group normalization to have a fully-convolutional architecture. The size of the model varies for each evaluated task (in general we chose a model size proportional to the size of the dataset to avoid significant overfitting). The model is trained on image crops using ADAM optimizer. Learning rates and other hyper-parameters are given in Appendix . For each experiment, we train a single model $F_\theta(\cdot; t)$ that is conditioned on the parameter $t$. The model is trained using the loss function of  with $p=1$. We found that the distribution of $t$, $p(t)$ plays an important role. In Section  we present an empirical analysis of its impact.

## Motion Deblurring

Motion deblurring is a very challenging restoration task. Motion is intrinsically random in the sense that a priori we don’t have a known degradation model. The current best end-to-end deep learning solution is to train regression models using paired data sharp, blurry frames. One of the most adopted training datasets is the GoPro motion deblurring dataset  containing 3214 pairs of clean and blurry $1280 \times 720$ images ($1111$ are reserved for evaluation). The blurry frames are generated by recording high-frame rate video clips and then averaging consecutive frames to simulate blurs caused due to longer exposure. We follow the standard setup and perform training data augmentation with random horizontal/vertical flips and 90/180/270 rotations. We did not introduce additional noise to the blurry inputs ($\epsilon =0$ in ).

Figure  shows a visual comparison of our iterative image restoration and current state-of-the-art deblurring models. The iterative scheme produces images with much more details than regression based solutions (Restormer , MAXIM ). Our results are similar to the ones generated by current conditional diffusion models (DvSR ). Quantitative results on the GoPro dataset are presented in Table . The proposed iterative reconstruction procedure achieves a new state-of-the-art performance across perceptual metrics while maintaining competitive PSNR to existing methods.

**Number of steps.** Figure  shows the impact of the number of inference steps on the Perception–Distortion trade-off . While doing a reconstruction on a single step (e.g., direct regression) produces the best PSNR, the perceptual metrics are significantly improved when the number of steps is larger than one. Both metrics can’t be optimized simultaneously .

## Single-Image Super-resolution

We evaluated the iterative restoration methodology on single-image $4\times$ super-resolution on the `div2k` dataset . This dataset contains 1000 2K-resolution images (800 for training, 100 images for validation, 100 testing). We compare to other state-of-the art models that span from regression models having powerful architectures  and/or generative formulations: GAN based, i.e., LDL , ESRGAN , BSRGAN ; and also based on Normalizing Flows, SRFLOW .

Figure (b) summarizes the quantitative results on $4\times$ SR div2k validation dataset. Figure  shows a selection of results. Our proposed framework leads to upscaled images with more defined structure than regression based formulations producing larger PSNR, e.g., RRDB . The recently introduced adversarial formulation LDL  produces slightly better fine grain details. This could indicate that in the situation where there is limited training data, careful adversarial formulation may be more data efficient.

**The importance of adding noise in deterministic super-resolution.** In our formulation of super-resolution, the degradation is a deterministic linear (blurring plus subsampling) operator. Figure (a) shows the importance of adding a small amount of noise to the input image. Directly applying the original iterative procedure (without adding noise to the input) leads to a blurry reconstruction (high PSNR but low FID score, $\epsilon=0.0$ in Figure  (a)). Adding a small amount of noise ($\epsilon > 0$ in Figure (a)) leads to significant better results in terms of perceptual quality (e.g., FID score).

## Defocus deblurring

Defocus deblurring is the task of reducing the blur due to limited depth-of-field or misfocus. For such purposes we used the Canon dual-pixel (DP) defocus dataset (DDPD) provided by , and train a defocus deblurring model only using single image input (i.e., we don’t use the dual-pixel images given in the dataset). The DDPD dataset contains 1000 pairs of sharp and blurry $6720 \times 4480$ images, of which 30% are reserved for validation and testing. The blurry/sharp frames are generated by capturing two consecutive snapshots by changing the camera parameters (lens aperture). high-frame rate video clips and then averaging consecutive frames to simulate blurs caused due to longer exposure.

Figure  shows a visual comparison of our iterative image restoration when a different number of inference steps is used. Increasing the number of steps has a direct impact on the quality of the result. Quantitative results on the DDPD dataset are summarized in Table  in Appendix. As in the other experiments the best PSNR is obtained with a single step (direct regression), while the best perceptual metrics are obtained when the restoration is done in multiple steps. We did not introduce additional noise to the blurry inputs ($\epsilon =0$ in ).

## Compression artifact removal

JPEG compression introduces blocking artifacts and lack of high-frequency details. We evaluated the proposed method on the task of removing strong JPEG compression artifacts (quality factor 15). To generate the training data we use the 1000 `div2k` high-quality images . We evaluated the model on div2k validation set.

Figure  show some visual results of restored images with the model applying a different number of steps. As more inference steps are used the restored images have more details. More results on JPEG compression removal are discussed in the next section.

# Discussion

## A generative framework

A natural question to ask is whether the proposed approach is also generative in the spirit of diffusion formulations. Namely, if we take our formulation to the limit where the low-quality image is fully degraded, then could we potentially generate new samples from scratch?

To test the idea we trained a restoration model that starts from pure Gaussian noise paired to a $64\times64$ celebA image and then proceed as described above. Figure  shows some generated samples with this formulation. The generated samples have a FID=9.19, which is not state-of-the-art[^2] but illustrate the point. In this specific case, our proposed methodology leads to a similar denoising training loss as the one in DDPM . Despite this similarity, the two methods come from different motivations/formulations and therefore have different inference strategies. We didn’t fine-tune architecture or hyper-parameters to boost the performance since our goal was to present the idea and show that the formulation, at its core, can be generative as well.

## Comparison of Inference Algorithms

In what follows we discuss different alternatives for recovering the clean sample with the trained models.

**Naive Procedure and Relevance to Cold Diffusion:** Given , one may be tempted to directly replace the clean image by the current estimate. This would lead to $\hat{\vx}_{t} = (1-t) F_\theta(\hat \vx_s, t) + t \vy$, and the inference iterative rule would become $$\begin{aligned}
\hat\vx_{t-\delta} &= (1-t+\delta) F_\theta(\hat\vx_t, t) + (t-\delta) \vy.
\label{eq:naive_sampler}
\end{aligned}$$

Cold Diffusion  proposes to generate images by inverting an arbitrary *known* degradation $D(\vx, s)$, where $s$ controls the strength. Our formulation is more general in the sense that we don’t require an explicit knowledge of $D$. To apply Cold Diffusion sampling in our context, we define $D(\vx, s) = (1-s)\vx + s \vy$ (given by ). This leads to Cold Diffusion’s naive sampling (Algorithm 1 in ), $$\begin{aligned}
\hat\vx_{t-\delta} &= D(F_\theta(\hat\vx_t,t),t-\delta)  = (1-t+\delta)F_\theta(\hat\vx_t,t) + (t - \delta)\vy.
\end{aligned}$$ Note that this sampling scheme is the same as the one in Eq. . Cold diffusion improved sampling (Algorithm 2 in ) is given by, $$\begin{aligned}
\hat\vx_{t-\delta} &= \hat\vx_t - D(F_\theta(\hat\vx_t, t),t) + D(F_\theta(\hat\vx_t, t),t-\delta) \\
&= \hat\vx_t - (1-t)F_\theta(\hat\vx_t, t) - t\vy + (1-t + \delta)F_\theta(\hat\vx_t, t) + (t - \delta )\vy \\
&= \hat\vx_t + \delta (F_\theta(\hat\vx_t, t)-\vy).
\label{eq:cold_diffusion_sampler}
\end{aligned}$$ In Figure  we compare our inference algorithm (), the naive inference algorithm (), and our adaptation of the Cold Diffusion sampler to our formulation (). In general, the naive sampler produces good results with very few steps (N=2,3) but then diverges. Our adaptation of Cold Diffusion sampler produces competitive results, while leading to slightly worse FID scores for the same distortion level than our proposed algorithm. In the limit, as the number of steps becomes very large, Cold Diffusion sampler seems to converge to a stable point, while ours after a certain large number of steps, deteriorates.

Figure . shows the FID score of CelebA 64x64 generated images when using the three different variants of the inference algorithm (sampler).

## Impact of distribution p(t)

The impact of the distribution of $t$ used during training has a clear impact on performance. We evaluated several different options that are summarized in Figure  (a). Figure  (b) shows the results when different distributions are adopted. The best results are obtained when the model is trained with a bias towards $t=1$ (more degradation). Intuitively, this could imply that the iterative procedure needs to be more certain of the direction to move at the very early steps of the procedure. Nonetheless, the best distribution can depend on a combination of model capacity and restoration task so we are not drawing general conclusions.

## Impact of adding noise on inverting deterministic degradations

JPEG compression is a non-linear, but deterministic, degradation. We empirically verified that adding a small amount of noise helps to improve the results as shown in Figure . We tested the variant of the inference algorithm that adds noise at each step (so the noise becomes a Brownian motion, e.g., $\epsilon_t=\epsilon / \sqrt{t}$), and adding a constant noise level at the initial step ($\epsilon_t = \epsilon$). We did not observe any practical difference in the two approaches.

## Comparison to a Conditional Denoising Diffusion Model

We compare InDI to a vanilla conditional DDPM . We trained a vanilla conditional DDPM, using the (continuous) noise level as an additional input, similarly as done in . The model architecture is the same as in InDI but the auxiliary noise image (needed in any DDPM) is concatenated with the low-quality input at each step. Figure  shows a comparison between InDI and the conditional DDPM. To generate the DDPM plot we merged several possible noise schedules using different number of steps that span the perception-distortion tradeoff. InDI produces comparable results using significant less number of steps than the vanilla DDPM.

# Conclusions and Limitations

We presented a novel formulation of image restoration that circumvents the regression-to-the mean problem. This allows us to get restored images with superior realism and perceptual quality, while still having a low distortion error. Our method is motivated by the observation that restoration from a small distortion is a better-conditioned problem. We therefore break a restoration task into many small ones – each of them easier (and less ill-posed) than the larger problem we solve overall. This enables our iterative approach to transforming the degraded image into a high-quality image, in spirit similar to current generative diffusion models.

**Limitations.** The present formulation is a supervised one, requiring paired training data. As such, for each type of degradation we need to train a specialized model, in contrast to unsupervised formulations such as RED , PnP , or DDRM . Additionally, given the dependence to paired training data, its performance for out-of-distribution samples is not guaranteed. This question requires more in-depth analysis. Finally, while the proposed iterative inference algorithm produces high-quality restorations, in some tasks performance degrades after a certain number of steps. This is likely due to the accumulation of errors, and will likely require a more robust inference scheme.

For future work, we would like to better characterize the limiting points of the proposed inference procedure. Other possible research avenues are developing robust formulations that can successfully handle out-of-domain input.

### Acknowledgments

The authors would like to thank our colleagues Jon Barron, Tim Salimans, Jascha Sohl-dickstein, Ben Poole, José Lezama, Sergey Ioffe, and Jason Baldridge for helpful discussions.

# Proof of Proposition 

<div class="proof">

*Proof.* We have $\vx_s = (1-s) \vx + s \vy$, and $\vx_t = (1-t) \vx + t \vy$, so by substituting $\vy$ from one to the other we get, $$\begin{aligned}
\vx_s &= (1-s) \vx + s \left( \frac{\vx_t  -(1-t)\vx}{t} \right) \\
      &= \vx -s \vx + \frac{s}{t}\vx_t -\frac{s}{t}\vx  + s\vx  \\
      &= \left(1-\frac{s}{t} \right)\vx  + \frac{s}{t}\vx_t. 
\end{aligned}$$ Then, $$\begin{aligned}
\condExpec{\vx_s}{\vx_t} &= \int \vx_s p_{\vx_s | \vx_t}(\vx_s | \vx_t) d \vx_s \\
&=\int \vx_s p_{\vx | \vx_t} \left(\frac{t \vx_s - s \vx_t}{t-s} \Big | \vx_t\right) \frac{t}{t-s} d \vx_s \\
&=\int \frac{(t-s) \vx + s \vx_t}{t} p_{\vx | \vx_t} ( \vx | \vx_t) d\vx \\
&= \left(1-\frac{s}{t}\right) \int \vx p_{\vx | \vx_t} ( \vx | \vx_t) d\vx + \frac{s}{t} \vx_t \\%\int p_{\vx | \vx_s} ( \vx | \vx_s) d\vx \\
&= \left(1 - \frac{s}{t}\right)  \condExpec{\vx}{\vx_t} + \frac{s}{t} \vx_t.
\end{aligned}$$ where we have applied the fact that $p_{\vx_s | \vx_t}(\vx_s | \vx_t) = p_{\vx | \vx_t}(\vx | \vx_t) \frac{t}{t-s}$ and $\vx = \frac{t \vx_s - s \vx_t}{t-s}$. ◻

</div>

# Denoising with a Gaussian Prior

Let’s analyze InDI’s behavior in the particular case where $p(\textbf{x}) = N(\textbf{c}, \sigma^2_c \textbf{I})$, and the restoration task is denoising $\textbf{y} = \textbf{x} + \textbf{n}$, where $\textbf{n}$ is white Gaussian of fixed standard deviation $\sigma_N$. Then, $p(\textbf{y} | \textbf{x}) = N(\textbf{x}, \sigma^2_N \textbf{I})$ and $\mathbb{E}[\textbf{x}|\textbf{x}_t]= \frac{\sigma^2_c \textbf{x}_t + t^2\sigma^2_N \textbf{c}}{\sigma^2_c + t^2\sigma^2_N}$, where we have used the fact that $\textbf{x}_t = (1-t)\textbf{x} + t \textbf{y} = \textbf{x} + t\textbf{n}$.

InDI’s ideal ODE is given by $\frac{d \textbf{x}_t} {dt} = \frac{\textbf{x}_t-\mathbb{E}[\textbf{x} | \textbf{x}_t]}{t},$ which in this case becomes: $$\frac{ d\textbf{x}_t}{d t} = \frac{t \sigma^2_N (\textbf{x}_t-\textbf{c})}{\sigma^2_N t^2 + \sigma^2_c}.$$ We are interested in solving this equation at $t=0$, with boundary condition $\textbf{x}_1=\textbf{y}$ at $t=1$. This is a separable ODE having general solution: $\textbf{x}_t = \textbf{c} + (\textbf{y} - \textbf{c}) \sqrt{ \frac{t^2 + \alpha^2}{1 + \alpha^2}}$, where $\alpha = \frac{\sigma_c}{\sigma_N}$. The solution at $t=0$, is $$\textbf{x}_\text{InDI} = \textbf{c} + (\textbf{y}-\textbf{c}) \sqrt{\frac{\sigma^2_c}{\sigma^2_c + \sigma^2_N}}.$$

Note that $\mathbb{E}[\textbf{x}_\text{InDI}] = \textbf{c} = \mathbb{E}[\textbf{x}]$, and the covariance $\text{cov}(\textbf{x}_\text{InDI}) = \text{cov}(\textbf{y}) \frac{\sigma^2_c}{\sigma^2_c + \sigma^2_N} = \text{cov}(\textbf{x})$, since $\text{cov}(\textbf{y}) = (\sigma^2_c + \sigma^2_N)\textbf{I}.$ Given $p(\textbf{x})$ and $p(\textbf{x}_\text{InDI})$ are Gaussian distributions with same mean and covariance, we have $p(\textbf{x}_\text{InDI})=p (\textbf{x})$. That is, in the limit, InDI generates samples from the prior distribution $p(\textbf{x})$.

It is worth noting that in this case the MMSE and MAP estimates coincide, $$\textbf{x}_\text{MMSE}  = \textbf{x}_\text{MAP} = \frac{\sigma^2_c \textbf{y} + \sigma^2_N \textbf{c}}{\sigma^2_c + \sigma^2_N},$$ and are in fact different from InDI’s estimate.

# Model and Training Details

In all our restoration experiments we use a U-Net-like architecture  similar to the one in SR3  and DvSR . We followed the same adaptations as the ones introduced in  to make it fully-convolutional (removed self-attention layers and group normalization). Our U-Net has an adaptive number of resolutions each of them having an arbitrary number of channels (given by a multiplication factor from a base set of channels).

Table  summarizes the model definition for each of the tested applications.

All models are trained for 500K steps using 32 TPUv3 cores. We used the Adam optimizer with a fixed learning rate, and EMA decay rate of 0.9999. Models were trained using the respective indicated distribution for $p(t)$. For the super-resolution model, low-resolution crops of size $64\times64$ are upscaled using bilinear interpolation to $256\times256$ before feeding them into the model.

# Additional Results

[^1]: A similar statement is true for other $p\neq 2$ in which case the mean is replaced by another aggregation operator (e.g. median for $L_1$)

[^2]: It is competitive with other methods from a couple years ago
