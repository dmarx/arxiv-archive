\section{Introduction}

% Goal statement
Photorealistic rendering and animation of human bodies is an important area of research with many applications in AR/VR, visual effects, visual try-on, movie production, etc. 
%\st{Successful tools for creating human avatars should enable easy data capture, efficient computation, and create a photo-realistic and animatable representation of the subject human. Unfortunately, existing approaches fall short of meeting these requirements.} 
{Early works~\cite{alexander2010emily, alexander2013digitalira, anguelov2005scape} for creating human avatars relied on capturing high-quality data in a multi-camera capture setup, extensive compute, and lots of manual effort.} 
Recent work addresses these problem by directly generating 3D avatars from videos % can be categorized into two main approaches.
% The first approach~\cite{} builds upon 
using 3D parametric body models like SMPL~\cite{SMPL:2015, pavlakos2019smplx}, which offers advantages such as efficient rasterization and the ability to adapt to unseen deformations. However, the fixed topological structure of parameteric models limit the modeling of clothing, intricate hairstyles and other details of the geometry.
%due to the inherent constraints of template meshes, such as fixed topologies and surface-like geometries.

Recent advancements have explored the use of neural fields for modeling 3D human avatars~\cite{guo2023vid2avatar, jiang2022neuman, weng2022humannerf, peng2021neuralbody, bergman2022gnarf, dong2023ag3d}, often using a parametric body models as a scaffold for modeling deformations. Neural fields excel in capturing details like clothing, accessories, and hair, surpassing the quality that can be achieved by rasterization of parametric models with texture and other properties. However, they come with trade-offs, notably being less efficient to train and render.
%This inefficiency arises from the need to query numerous points along the camera ray to render a single pixel. 
Furthermore, deformation of neural fields in a versatile manner presents challenges, often requiring recourse to an inefficient root-finding loop, which adversely affects both training and rendering durations~\cite{chen2021snarf,jiang2022instantavatar,yu2023monohuman}.

%Therefore, \ar{our goal is to enable photorealistic animatable avatar with minimal capture.. compute .. To this end, we formulate our problem as a neural rendering of ...}.
%\st{we are interested in the} scenario where only one single video is provided, and our goal is to reconstruct the human model and the static scene model, and enable novel pose rendering of the human, without any expensive multi-cameras setups or manual annotations. 

% What is the problem with the existing work
% Recent work on Neural Fields include NeRFs~\cite{mildenhall2020nerf}, MPI~\cite{}, Image-based rendering~\cite{IBR_stuff}, point-based rendering ~\cite{adop, pointersect, pulsar}. However, their ability to capture deformable entities such as humans, bodies etc is either too slow or too bad. 

%


To address these challenges, we introduce a novel avatar representation \emph{\acronym---Human Gaussian Splats}. 
% To reduce rendering time and improve deformability, 
% \acronym employs a set of \gauss to represent a canonical human geometry and builds on 3D Gaussian Splatting (3DGS)~\cite{kerbl3Dgaussians} that offers improved training and rendering speeds as compared to implicit NeRF representations~\cite{jiang2022neuman, liu2021neuralactor}. 
\acronym represents both the human and the scene as 3D Gaussians and utilizing the 3D Gaussian Splatting (3DGS)~\cite{kerbl3Dgaussians} for its improved training and rendering speeds as compared to implicit NeRF representations~\cite{jiang2022neuman, liu2021neuralactor}. 
%
While utilizing the 3D Gaussian representation allows explicit control of human-body deformation, it also creates new problems. Specifically, a realistic animation of human motions requires a coordination of individual Gaussians to retain surface integrity (\ie, without generating holes or pop outs).

%To take advantage of explicit 3D Gaussians, HUGS learns a forward deformation model for animation.
To enable human-body deformation,
we introduce a novel deformation model that represents the human body in a canonical space using the 3D Gaussians.   % and deforms them using a learned Linear Blend Skinning (LBS)~\cite{SMPL:2015}. 
The deformation model predicts the mean-shifts, rotations, and scale of the 3D Gaussians to fit the subject's body shape (in the canonical pose).  
% as well the Spherical Harmonics (SH) coefficients~\cite{ramamoorthi2001efficient} for view-dependent color. 
%
Moreover, the deformation model predicts the Linear Blend Skinning (LBS) weights~\cite{SMPL:2015} that are used to deform the canonical human into the final pose.
%
We initialize HUGS from the parameteric \smpl body shape model~\cite{SMPL:2015} but allow the Gaussians to deviate, increase, and pruned from the \smpl model. 
%
This enables \acronym to model the geometry and appearance details (\eg, hair and clothing) beyond the \smpl model.
%
The learned LBS weights also coordinate the movement of Gaussians during animation. 
%
% This enables \acronym to use the versatile LBS deformation of the parameteric shape models~\cite{SMPL:2015, pavlakos2019smplx} while giving us the flexibility of represent details in the geometry and appearance using the 3D Gaussians.
HUGS is trained on a single monocular video with 50-100 frames and learns a disentangled representation of the human and scene, enabling versatile use of the avatars in different scenes. 

In summary, our main contributions are 
\begin{itemize}[leftmargin=*]
    \item We propose Human Gaussian Splats (HUGS), a neural representation for a human embedded in the scene that enables novel pose synthesis of the human and novel view synthesis of the human and the scene. 
    %
    \item We propose a novel forward deformation module that represents the target human in a canonical space using \gauss and learns to animate them using LBS to unobserved poses.
    %
    \item HUGS enables fast creation and rendering of animatable human avatars from in-the-wild monocular videos with a small number of (50-100) frames, taking 30 minutes to train, improving over baselines ~\cite{jiang2022neuman, guo2023vid2avatar} by ${\sim}100{\times}$, while rendering at {60 frames per second (FPS)} at HD resolution.\footnote{The train/rendering speed is thanks to 3DGS~\cite{kerbl3Dgaussians}, our contribution is enabling it for deformable cases such as humans.}. 
    %
    \item HUGS achieves state-of-the-art reconstruction quality over baselines such as NeuMan~\cite{jiang2022neuman} and Vid2Avatar~\cite{guo2023vid2avatar} on the NeuMan dataset and the ZJU-Mocap dataset.
    %\ot{We should also mention improvement wrt other methods not just NeuMan}
    %
    % \item We perform joint optimization of scene and human gaussians to showcase the mutual benefit of joint optimization.
    % \item We obtain state-of-the-art performance on NeuMan and ZJU-Mocap datasets while achieving realtime rendering speeds.
\end{itemize}
% \jg{Is there a contribution here about clothes?}
%The final piece is to learn avatars which can be transferable. Even though HUGS is trained on a single monocular video, it learns a  disentangled representation of the human and the scene enable transfer of human avatars to different scenes (\ar{see teaser figure. explain teaser}). 

%Specifically, we optimize a set of 3D Gaussians to represent the human geometry in a canonical space. 
%For animation, a forward deformation module transforms these canonical points into a deformed space, \st{utilizing learned pose blendshapes and skinning weights, guided by pose parameters from a pre-trained parametric body model like \smpl.}\ar{No need for details here, pose blendshapes/parameters have still not been introduced.}

% \textcolor{magenta}{mention the benefits of having smpl as the template, shape control, correspondence across different avatars.}

 


%To address these challenges, we introduce a novel avatar representation \emph{\acronym\;-- Human Gaussian Splats}. \acronym employs a set of \gauss~\cite{kerbl3Dgaussians} to represent the canonical geometry. It also learns a forward deformation model for animation. For rendering, \acronym uses 3D Gaussian Splatting (3DGS)~\cite{kerbl3Dgaussians} which enables real-time rendering. \jg{Even though HUGS is trained on a single monocular video, it learns a ...}{}HUGS is trained on a single monocular video and learns a disentangled representation of the human and the scene enable transfer of human avatars to different scenes (\ar{see teaser figure. explain teaser}). 

%Specifically, we optimize a set of 3D Gaussians to represent the human geometry in a canonical space. 
%For animation, a forward deformation module transforms these canonical points into a deformed space, \st{utilizing learned pose blendshapes and skinning weights, guided by pose parameters from a pre-trained parametric body model like \smpl.}\ar{No need for details here, pose blendshapes/parameters have still not been introduced.}

% \textcolor{magenta}{mention the benefits of having smpl as the template, shape control, correspondence across different avatars.}

%HUGS builds on 3DGS~\cite{kerbl3Dgaussians} that offers improved training and rendering speeds as compared to implicit NeRF representations~\cite{jiang2022neuman, liu2021neuralactor}. Moreover, the explicit representation of 3D Gaussians allows better control of human-body deformation. We introduce a novel deformation model that represents the human body in a canonical space using the 3D Gaussian points and deforms them using Linear Blend Skinning (LBS)~\cite{SMPL:2015}. The deformation model predicts the mean-shifts and rotations of the 3D Gaussians as well the Spherical Harmonics (SH) coefficients~\cite{ramamoorthi2001efficient} for view-dependent color. In addition, the deformation model also predicts the LBS weights and pose-corrections that are used for deformation of the canonical human into the final pose. This enables us to use the versatile LBS deformation of the parameteric shape models~\cite{SMPL:2015, pavlakos2019smplx} while giving us the flexibility of represent details in the geometry and appearance using the 3D Gaussians.

%In contrast to implicit representations, our approach based on \gauss allows for efficient rendering through a differentiable rasterizer~\cite{kerbl3Dgaussians}. Additionally, they can be deformed effectively using established techniques \eg linear blend skinning. Compared to meshes, 3D Gaussians exhibit greater flexibility and versatility. Furthermore, in contrast to point clouds, they do not result in holes in the final rendered images.  
%\rick{isn't it inaccurate? maybe say it is a continuous representation so easier to optimize.}
%
%Beyond their capacity to accommodate changes in topology to model accessories and clothing, they are also well-suited for representing intricate volumetric structures, including hair. Additionally, existing joint human and scene~\cite{jiang2022neuman, guo2023vid2avatar} novel view synthesis approaches use \nerf as their representation. They need to deal with ray warping and ray classification to correctly disentangle scene and human points. Our use of \gauss prevents this issue by rasterizing the scene and human \gauss jointly when obeying their respective depth. This results in much more accurate scene and human separation compared to existing works~\cite{jiang2022neuman,guo2023vid2avatar}.

% How do we solve it
%\ar{We demonstrate that ... }
%In summary, HUGS learns a disentangled representation of the human and scene enabling versatile use of the avatars in different scene. HUGS enables fast creation and rendering of animatable human avatars from monocular videos improving the training and rendering time \ar{$\sim$ 100x} over baselines such as NeuMan~\cite{jiang2022neuman} with quality improvements.
%We demonstrate the ability of our method using videos captured in the wild show that HUGS
%, the proposed representation combines the advantages of popular mesh, point-based and implicit representations, and 
%surpasses  both in many challenging scenarios. 


% Works on speeding up neural fields -- instant-ngp, kilonerf. Direct optimization alternative -- plenoxels -- quality is an issue. Recent work gaussian splatting achieves high quality rendering and 


% Contributions


% What do we propose 
% To address these issues, we propose HUGS, a novel animatable avatar representation that uses 3D Gaussians to represent the canonical geometry and learns how to deform 3D gaussians for animation. Specifically, we optimize a set of 3D gaussians to represent the geometry of a subject in a canonical space. For animation, a forward deformation module maps the canonical points to the deformed space with learned pose blendshapes and skinning weights, given pose parameters of a pretrained parametric body model \ie \smpl. Compared to implicit representations, our 3D Gaussians-based representation can be rendered efficiently with a standard differentiable rasterizer. Moreover, they can be deformed effectively using established techniques, \eg, skinning. Compared to meshes, 3D gaussians are considerably more flexible and versatile. And compared to point clouds, they do not suffer from holes in the final rendered images. Besides the ability to conform the topology to model accessories and clothing, they can also represent complex volume-like structures such as hair.