Photorealistic rendering and animation of human bodies is an important area of research with many applications in AR/VR, visual effects, visual try-on, movie production, etc. Successful tools for creating human avatars should enable easy data capture, efficient computation, and create a photo-realistic and animatable representation of the subject human. Unfortunately, existing approaches fall short of meeting these requirements. Previous work~\cite{alexander2010emily, alexander2013digitalira} for creating human avatars relied on capturing high quality data in a multi-camera capture setup, extensive compute, and a lot manual effort.

Therefore, our goal is to enable photorealistic animatable avatar with minimal capture.. compute .. To this end, we formulate our problem as a neural rendering of ....
scenario where only one single video is provided, and our goal is to reconstruct the human model and the static scene model, and enable novel pose rendering of the human, without any expensive multi-cameras setups or manual annotations. 

What is the problem with the existing work?
Recent work on Neural Fields include NeRFs~\cite{mildenhall2020nerf}, MPI~\cite{}, Image-based rendering~\cite{IBR_stuff}, point-based rendering ~\cite{adop, pointersect, pulsar}. However, their ability to capture deformable entities such as humans, bodies etc is either too slow or too bad. 

%
Recent approaches for generating 3D avatars from videos can be categorized into two main approaches. The first approach~\cite{} builds upon 3D parametric body models like SMPL(-X)~\cite{}, which offers advantages such as efficient rasterization and the ability to adapt to unseen deformations. However, it has limitations when it comes to modeling individuals with clothing or intricate hairstyles due to the inherent constraints of template meshes, such as fixed topologies and surface-like geometries.

Alternatively, recent advancements have explored the use of neural field representations for modeling 3D human avatars~\cite{jiang2022neuman}. These neural implicit representations excel in capturing details like clothing, accessories, and hair, surpassing the capabilities of parametric body model-based techniques. However, they come with trade-offs, notably being less efficient to train and render. 
This inefficiency arises from the need to query numerous points along the camera ray to render a single pixel. 
Furthermore, deformation of neural fields in a versatile manner presents challenges, often requiring recourse to an inefficient root-finding loop, which adversely affects both training and rendering durations~\cite{}.

To address these challenges, we introduce a novel avatar representation \emph{\acronym\;-- Human Gaussian Splats}. \acronym employs a set of \gauss to represent the canonical geometry and learns how to deform \gauss for animation. Specifically, we optimize a set of 3D Gaussians to represent the human geometry in a canonical space. For animation, a forward deformation module transforms these canonical points into a deformed space, \st{utilizing learned pose blendshapes and skinning weights, guided by pose parameters from a pre-trained parametric body model like \smpl.}\ar{No need for details here, pose blendshapes/parameters have still not been introduced.}



In contrast to implicit representations, our approach based on \gauss allows for efficient rendering through a differentiable rasterizer~\cite{kerbl3Dgaussians}. Additionally, they can be deformed effectively using established techniques \eg linear blend skinning. Compared to meshes, 3D Gaussians exhibit greater flexibility and versatility. Furthermore, in contrast to point clouds, they do not result in holes in the final rendered images.  
\rick{isn't it inaccurate? maybe say it is a continuous representation so easier to optimize.}
%
Beyond their capacity to accommodate changes in topology to model accessories and clothing, they are also well-suited for representing intricate volumetric structures, including hair. Additionally, existing joint human and scene~\cite{jiang2022neuman, guo2023vid2avatar} novel view synthesis approaches use \nerf as their representation. They need to deal with ray warping and ray classification to correctly disentangle scene and human points. Our use of \gauss prevents this issue by rasterizing the scene and human \gauss jointly when obeying their respective depth. This results in much more accurate scene and human separation compared to existing works~\cite{jiang2022neuman,guo2023vid2avatar}.

% How do we solve it
We demonstrate that ... 
As demonstrated using various videos captured in the wild, the proposed representation combines the advantages of popular mesh, point-based and implicit representations, and surpasses both in many challenging scenarios. In summary, our contributions are:


   1. We propose a forward deformation module with 3D gaussians to learn a canonical human gaussian model.
   2. We perform joint optimization of scene and human gaussians to showcase the mutual benefit of joint optimization.
   3. We obtain state-of-the-art performance on NeuMan and ZJU-Mocap datasets while achieving realtime rendering speeds.


To address these issues, we propose HUGS, a novel animatable avatar representation that uses 3D Gaussians to represent the canonical geometry and learns how to deform 3D gaussians for animation. Specifically, we optimize a set of 3D gaussians to represent the geometry of a subject in a canonical space. For animation, a forward deformation module maps the canonical points to the deformed space with learned pose blendshapes and skinning weights, given pose parameters of a pretrained parametric body model \ie \smpl. Compared to implicit representations, our 3D Gaussians-based representation can be rendered efficiently with a standard differentiable rasterizer. Moreover, they can be deformed effectively using established techniques, \eg, skinning. Compared to meshes, 3D gaussians are considerably more flexible and versatile. And compared to point clouds, they do not suffer from holes in the final rendered images. Besides the ability to conform the topology to model accessories and clothing, they can also represent complex volume-like structures such as hair.