\input{figures/qual_sota}
\input{figures/qual_composition}

\section{Experiments}
% \subsection{Implementation details}
\subsection{Datasets}
% \paragraph{NeuMan} We use NeuMan dataset, a collection of 6 videos about 10 to 20 seconds long each, with a single person captured using a mobile phone. Moreover, the camera reasonably pans through the scene to enable multi-view reconstruction. The sequences are named â€“ Seattle, Citron, Parking, Bike, Jogging and Lab. Following the ~\cite{jiang2022neuman} we split frames into 80\% training frames, 10\% validation frames, and 10\% test frames.

% \paragraph{ZJU-MoCap} We evaluate our method on the ZJU-MoCap dataset~\cite{zjumocap} For ZJU-MoCap, following \cite{weng2022humannerf} we select 6 subjects (377, 386, 387, 392, 393, 394) with diverse motions and use images captured by "camera 1" as input and the other 22 cameras for evaluation. We directly apply camera matrices, body pose, and segmentation provided by the dataset.
\paragraph{NeuMan Dataset~\cite{jiang2022neuman}} consists of six videos, each lasting between 10 to 20 seconds, featuring a single individual captured using a mobile phone. The camera pans through the scenes, facilitating multi-view reconstruction. The sequences are denoted as Seattle, Citron, Parking, Bike, Jogging, and Lab. Following the approach outlined in~\cite{jiang2022neuman}, we split frames into 80\% training frames, 10\% validation frames, and 10\% test frames.

\paragraph{ZJU-MoCap Dataset~\cite{peng2021neuralbody}} consists of videos of a human captured in a lab using multi-view capture setup. To align with the methodology in~\cite{weng2022humannerf,yu2023monohuman}, we select six subjects (377, 386, 387, 392, 393, 394) showcasing diverse motions. We employed images captured by "camera 1" as input and utilized the other 22 cameras for evaluation. The camera matrices, body pose, and segmentation provided by the dataset were directly applied in our evaluation process.

\subsection{Qualitative Results}
\label{sec:qualitative}
\paragraph{State-of-the-art Comparison.} We show the qualitative results of our method in ~\cref{fig:qualitative_sota} and compare it with Vid2Avatar~\cite{guo2023vid2avatar} and NeuMan~\cite{jiang2022neuman}. The results are shown from the test samples of the NeuMan dataset~\cite{jiang2022neuman} that are not seen during training.
In the scene background regions, HUGS shows better reconstruction quality than both Vid2Avatar and NeuMan. Vid2Avatar shows blurry scene reconstruction with several artifacts. In contrast, NeuMan shows better scene reconstruction quality but misses fine details such as the house numbers (zoomed-in) in the first row, the wooden plank (zoomed-in) in the second row and the cupboard (zoomed-in) in the third row. In comparison, HUGS shows better reconstruction quality and preserves these fine details as shown in the zoomed-in regions.

In the human regions, Vid2Avatar shows artifacts in the hand region (row 1) and blurry reconstruction in the feet (row 2) and arm region (row 3). In contrast, NeuMan gets better details of the feet regions in some cases (row 2) and introduces artifacts in hands (row 2) and feet (row 3) regions in other cases. In comparison, our method preserves the details around hand and feet and shows better reconstruction quality. Furthermore, our method also preserves the structure around clothing (row 1) where the wrinkles are reconstructed well while preserving the structure of the zipper (zoomed-in) around it compared to previous work.

In summary, we note that HUGS shows better reconstruction quality of both the scene and the human as compared to previous methods while being orders of magnitude faster to train and render (see \S \ref{sec:timing} for speed comparison). We will provide additional qualitative results with videos in the Supp. Mat.

\input{figures/canonical}
\paragraph{Canonical Human Shapes.} In ~\cref{fig:canonical}, we show the reconstruction of the human in the canonical space. We note that our method captures fine details around the feet and hands of the human which look noisy in the case of NeuMan~\cite{jiang2022neuman}. Furthermore, we note that our method preserves rich details on the face. This enables us to achieve high reconstruction quality during the animation phase. 

\input{tables/neuman_human_scene}
\input{tables/neuman_human}
\input{tables/zju_mocap}

\paragraph{Disentanglement of the Human and the Scene.} HUGS allows for a disentangled represenation of the human and the scene by storing their Gaussian features separately. This allows us to move the human to different scenes. In \cref{fig:composition}, we show the composition of human captured in one scene into a different scene. We show additional video results in the supplemental material.


\subsection{Quantitative Results}
\label{sec:quantitative}
We compare the performance of our method with baselines such as NeRF-T~\cite{li2021neural}, HyperNeRF~\cite{park2021hypernerf} and the existing state-of-the-art methods -- NeuMan~\cite{jiang2022neuman} and Vid2Avatar~\cite{guo2023vid2avatar}.

In \cref{tab:neuman_human_scene}, we evaluate the reconstruction quality on the NeuMan dataset~\cite{jiang2022neuman} on three different metrics -- Peak Signal-to-Noise Ration (PSNR), SSIM~\cite{ssim} and LPIPS~\cite{zhang2018lpips}. NeRF-T and HyperNeRF are general dynamic scene reconstruction methods and do not specialize for humans. Therefore, they show poor reconstruction quality. On the other hand, NeuMan and Vid2Avatar employ specialized models for the human and the scene. NeuMan employs a NeRF-based~\cite{mildenhall2020nerf} approach for both scene and human modeling. Vid2Avatar utilizes an implicit SDF model and volume rendering for scene and human representation. Therefore, both NeuMan and Vid2Avatar show improved reconstruction quality. In comparison, our method achieves state-of-the-art performance across all the scenes and metrics except PSNR on the \textit{Bike} sequence where we show competitive performance.

In \cref{tab:neuman_human}, we further evaluate the reconstruction error but only on the regions containing the human. We first take a tight crop around the human region in the ground truth image. This crop is used over all the predictions, and the reconstruction error is evaluated over the cropped samples. It should be noted that we take rectangular crops of the region and do not use any segmentation mask since reconstruction metrics are highly sensitive to masks. Under this evaluation, we show state-of-the-art performance across all scenes and metrics except PSNR on the \textit{Jogging} sequence where we show competitive performance. 

In addition, we evaluate our method using the ZJU Mocap dataset~\cite{peng2021neuralbody} in ~\cref{tab:zju}. We compare with recent previous work that report their evaluation on this dataset which include NeuralBody~\cite{peng2021neuralbody}, HumanNerf~\cite{weng2022humannerf}, and MonoHuman~\cite{yu2023monohuman}. 

%As these models are all NeRF-based, they share the drawback of being slow in both training and rendering. Notably, our method stands out as both faster and more accurate than the listed state-of-the-art models, providing a compelling combination of efficiency and precision.



%In \cref{tab:neuman_human_scene} \cref{tab:neuman_human}, we present a comparative analysis of our method against existing joint human and scene novel view synthesis approaches. Given the novelty of this task, research in this domain is limited. Notable existing works include NeuMan~\cite{jiang2022neuman} and Vid2Avatar~\cite{guo2023vid2avatar}.

\label{sec:timing}
\input{figures/timings}
\input{tables/neuman_ablation_human}

\paragraph{Speed.} In ~\cref{fig:timing}, we compare the training and rendering time of our method with previous work. The use of 3DGS~\cite{kerbl3Dgaussians} speeds up our training and rendering times by a significant margin. We note that HUGS is {$96 \times$} faster than Vid2Avatar and {$336 \times $} faster than NeuMan  training within 30 minutes.
At rendering time, we do not rely on MLPs and only use the LBS weights, enabling higher frame rate.
%
Our method achieves {60 FPS} outperforming NeuMan by {${\sim}7600 \times$} and Vid2Avatar by {${\sim}3800 \times$}. We benchmark all the methods on a single GeForce 3090Ti GPU. \\
%Furthermore, our method enables training within 30 minutes \ar{z times} faster than NeuMan.

%However, NeRF models are characterized by lengthy training and rendering times. Vid2Avatar, on the other hand, utilizes an implicit SDF model and volume rendering for scene and human representation, but this method proves to be prohibitively slow, as indicated by the training and inference time comparison in the tables. In contrast, 

In summary, our model demonstrates efficiency in both training and rendering, delivering superior results compared to existing methods. 
Our model not only outperforms established NeRF and implicit-SDF based models but does so at orders of magnitude faster speeds.


\subsection{Ablation Experiments}
% \input{tables/neuman_ablation}
\input{figures/ablation}

We show the effect of ablating over our method in ~\cref{fig:ablation}. We note that removing LBS from our full model results in floating artifacts that are mainly introduced in the corner region or the body. We also experiment by keeping the number of Human Gaussians to be fixed by disabling densification. This results in floaters around the edges (row 1, on the side of the shirt) since noisy Gaussians are not culled and large Gaussians are not split. 

Furthermore, we examine the effect of removing the loss on the human pixels $\mathcal{L}_h$. This results in loss of fine details in the human region as evident from the reconstruction of the shoes (row 2). In addition, removing the triplane+MLP and directly optimizing the 3DGS parameters results in noisy estimates. Please refer to supplemental material for a detailed ablation and analysis of our contributions.

In ~\cref{tab:neuman_ablation_human_only}, we show quantitative results on the NeuMan dataset by evaluating over only the human-regions by cropping it using a tight bounding box. We evaluate rendering quality using PSNR, SSIM and LPIPS metrics.

%In Table~\cref{tab:neuman_ablation}, we conduct ablation experiments to assess the impact of individual components in our method. When the GCN models are removed, and optimization is performed directly on the 3D Gaussian parameters, the resulting surfaces exhibit increased noise and incompleteness.

% \ar{left $\downarrow$ for MK's reference}

% Ablation experiments will include these discussions:
% \begin{itemize}
%     \item Discuss the effect of replacing GCN with MLP.
%     \item Discuss removing LBS and Posedirs and using SMPL LBS + Posedirs
%     \item Discuss the effect of normal and scale regularizers
%     \item Discuss the effect of joint scene\&human optimization vs. separate optimization
% \end{itemize}

% \subsection{Demos}

% \paragraph{Benefits of using SMPL topology} Leveraging SMPL as the template provides several advantages. It enables the manipulation of body shape using SMPL shape parameters. Also using the consistent SMPL topology allows us to transfer properties from one subject to another \eg clothing. Most significantly, the deformation parameters of SMPL serve as effective regularization for the estimated LBS and pose correctives.

% Since our representation is explicit, we can even simulate the human body in a physical simulation

% \textcolor{red}{MK: mention that triplane results in much lower number of gaussians compared to wo Triplane version}

% \input{figures/qual_canon_nva}
% \input{figures/qual_demo}

