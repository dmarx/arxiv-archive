{
  "arxivId": "2412.19048",
  "title": "Jasper and Stella: distillation of SOTA embedding models",
  "authors": "Dun Zhang, FulongWang",
  "abstract": "A crucial component of many deep learning applications (such as FAQ and RAG)\nis dense retrieval, in which embedding models are used to convert raw text to\nnumerical vectors and then get the most similar text by MIPS (Maximum Inner\nProduct Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and\nAIR-Bench) have been established to evaluate embedding models accurately.\nThanks to these benchmarks, we can use SOTA models; however, the deployment and\napplication of these models in industry were hampered by their large vector\ndimensions and numerous parameters. To alleviate this problem, 1) we present a\ndistillation technique that can enable a smaller student model to achieve good\nperformance. 2) Inspired by MRL we present a training approach of reducing the\nvector dimensions based on its own vectors or its teacher vectors. 3) We do\nsimple yet effective alignment training between images and text to make our\nmodel a multimodal encoder. We trained Stella and Jasper models using the\ntechnologies above and achieved high scores on the MTEB leaderboard. We release\nthe model and data at Hugging Face Hub\n(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training\nlogs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",
  "url": "https://arxiv.org/abs/2412.19048",
  "issue_number": 543,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/543",
  "created_at": "2024-12-30T04:38:33.847265",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-30T04:38:22.948Z",
  "main_tex_file": null,
  "published_date": "2024-12-26T04:05:28Z",
  "arxiv_tags": [
    "cs.IR"
  ]
}