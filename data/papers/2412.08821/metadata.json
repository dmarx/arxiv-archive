{
  "arxivId": "2412.08821",
  "title": "Large Concept Models: Language Modeling in a Sentence Representation\n  Space",
  "authors": "LCM team, Lo\u00efc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-juss\u00e0, David Dale, Hady Elsahar, Kevin Heffernan, Jo\u00e3o Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo S\u00e1nchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk",
  "abstract": "LLMs have revolutionized the field of artificial intelligence and have\nemerged as the de-facto tool for many tasks. The current established technology\nof LLMs is to process input and generate output at the token level. This is in\nsharp contrast to humans who operate at multiple levels of abstraction, well\nbeyond single words, to analyze information and to generate creative content.\nIn this paper, we present an attempt at an architecture which operates on an\nexplicit higher-level semantic representation, which we name a concept.\nConcepts are language- and modality-agnostic and represent a higher level idea\nor action in a flow. Hence, we build a \"Large Concept Model\". In this study, as\nproof of feasibility, we assume that a concept corresponds to a sentence, and\nuse an existing sentence embedding space, SONAR, which supports up to 200\nlanguages in both text and speech modalities.\n  The Large Concept Model is trained to perform autoregressive sentence\nprediction in an embedding space. We explore multiple approaches, namely MSE\nregression, variants of diffusion-based generation, and models operating in a\nquantized SONAR space. These explorations are performed using 1.6B parameter\nmodels and training data in the order of 1.3T tokens. We then scale one\narchitecture to a model size of 7B parameters and training data of about 2.7T\ntokens. We perform an experimental evaluation on several generative tasks,\nnamely summarization and a new task of summary expansion. Finally, we show that\nour model exhibits impressive zero-shot generalization performance to many\nlanguages, outperforming existing LLMs of the same size. The training code of\nour models is freely available.",
  "url": "https://arxiv.org/abs/2412.08821",
  "issue_number": 419,
  "issue_url": "https://github.com/dmarx/arxiv-archive/issues/419",
  "created_at": "2024-12-28T16:15:12.320019",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 37,
  "last_read": "2024-12-28T16:15:12.321429",
  "last_visited": "2024-12-28T15:26:36.997Z",
  "main_tex_file": null,
  "published_date": "2024-12-11T23:36:20Z",
  "arxiv_tags": [
    "cs.CL"
  ]
}