\section{Conclusion}
\label{sec:conclusion}
Our work introduces Multimodal ArXiv, comprising ArXivCap and ArXivQA, aims at advancing the scientific comprehension of LVLMs. 
Experiments show that fine-tuning on ArXivQA notably enhances LVLMs' mathematical reasoning capabilities. 
Moreover, our comprehensive evaluations across four vision-to-text tasks on ArXivCap underscore the challenges in understanding scientific figures for LVLMs, while highlighting the substantial improvements achieved by in-domain training. Our error analysis offers valuable insights for the ongoing development of LVLMs.
% , paving the way for future advancements in multimodal understanding within scientific domains.

% In this paper, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA for benchmarking and improving the scientific understanding of LVLMs. 
% We find that fine-tuning on ArXivQA significantly boosts the mathematical reasoning ability of LVLMs.
% Extensive evaluation results of four vision-to-text tasks on ArXivCap reveal the limited comprehension of LVLMs for scientific figures, and that in-domain training yields substantial improvements.
% Our error analysis provides insights for future development of LVLMs.

% Additionally, we leverage GPT-4V to generate a question-answering dataset using caption-figure pairs, which has proven effective in enhancing the mathematical reasoning abilities of LVLMs.
% By providing this dataset, we anticipate fostering a rich avenue for further studies, ultimately advancing LVLMs.



\section*{Limitations}
Our study has several limitations worth noting. Firstly, our exploration may not encompass the full spectrum of LVLMs due to the rapid evolution of architectures and training methodologies such as parameter-efficient tuning~\citep{hu2021lora,ma2024paramtuning}.
Nevertheless, we believe our dataset could still be effective for other LVLMs and the findings are generalizable.
We show that our ArXivQA dataset could also boost LLaVA-series models across scientific understanding benchmarks in Appendix~\ref{apx:llava}.
Secondly, our Multimodal ArXiv dataset sources from ArXiv papers due to their accessibility and open-source licenses. This approach may overlook the diversity of disciplines and data modalities present in the broader scientific literature. 
Future research could incorporate a broader range of datasets and domains to enrich the coverage of scientific knowledge, and explore dynamic data selection methods to improve performance and sample efficiency~\citep{li-etal-2021-dynamic,chen2024vlm_select}.

% Future research could incorporate a broader range of datasets and domains to enrich the coverage of scientific knowledge, and explore dynamic data selection methods for better performance and sample efficiency~\citep{li-etal-2021-dynamic,chen2024vlm_select}.

