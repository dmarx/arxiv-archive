\section{Multimodal ArXiv}
This section presents a detailed construction process of our Multimodal ArXiv dataset, consisting of two sets: ArXivCap~(\S\ref{subsec:ArXivcap}) and ArXivQA~(\S\ref{subsec:arxiv_qa}).
% and then key statistics of the curated dataset.

% Figure~\ref{fig:chunk_example} illustrates a case sampled from our dataset.
% Step by Step?

\input{figs/process-overview}

\subsection{ArXivCap}
\label{subsec:ArXivcap}

% \subsubsection{Construction Process of ArXivCap}
\paragraph{Construction Process} We outline the creation process of ArXivCap below and Figure~\ref{fig:dataset-curation-process} gives an overview.

\noindent\emph{Paper Filtering with Publication Type:}
\DatasetName is extracted from ArXiv~\cite{clement2019use}, which is under CC-0 licence for modification and distribution. 
% Therefore, we have the permission to create and distribute \DatasetName. 
% The files in the ArXiv dataset are in tar format, which includes LaTeX files and their corresponding image files. 
The raw files of papers posted on ArXiv tar files before June 2023 are downloaded. 
To ensure the quality of our dataset, we employ a rigorous selection process to filter potentially low-quality papers that might influence the figure-caption pair quality. 
Firstly, we retrieve meta-information for papers from Semantic Scholar~\cite{kinney2023semantic}, which contains the publication record for each paper. 
Papers with publication types \texttt{JournalArticle},  \texttt{Conference}, or \texttt{Review} are kept as we assume the peer-review process could ensure the overall figure-caption quality is satisfactory.
We further exclude papers with titles exceeding 100 words or abstracts longer than 300 words, in alignment with common submission requirements.
% We further filter out papers with titles longer than 100 words, or with an abstract longer than 300 words, according to common submission requirements.
% for examples. 
% There are 2,285,111 papers and 13,945,502 images in total till this step.


% \input{figs/chunk-example}

% \paragraph{Unify Image Format} 
% We utilize ImageMagisk~\cite{imagemagick} to convert images of various formats, including .ps, .epsi, .PS, .pdf, .PDF, .EPS, .eps, etc., into the JPEG format, while preserving the original jpg/jpeg and png images. ImageMagisk is a robust tool for image editing and transformation. Additionally, we standardize the output format to jpg/jpeg in RGB mode and remove the white border around images.

% according to our prior.
% This ensures that we include papers published in reputable journals, conference proceedings, or recognized review articles.
% We further keep papers satisfying the following requirements:
\noindent\emph{Figure-Caption Pair Extraction:}
Images and captions are extracted from the original LaTeX files by matching the syntax. 
We further use a robust tool ImageMagisk~\cite{imagemagick} to convert images into JPEG format for easy processing.
The extracted images and captions are stored in a designed chunk structure,
which consists of either a single figure-caption pair or multiple figures with their respective sub-captions and a main caption for the overall description.
This format is more consistent with the layout of academic papers, and
Figure~\ref{fig:chunk_example} illustrates the chunk structure.

% \begin{enumerate}% [leftmargin=2em]
%     \item The number of words in the abstract is in  $[10, 300]$.
%     \item The number of words in the title is in $[1, 100]$.
% \end{enumerate}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/chunk-v2.pdf}
    \caption{Illustration of two types of figure-caption pairs. (Left) Single-Figure pair. (Right) Multiple-Figure caption pair has multiple sub-figures with corresponding sub-captions and an overall main caption. }
    \label{fig:chunk_example}
\end{figure}
% \paragraph{Filter Images}
% After resizing images using Lanczos resampling, we ensure that the maximum dimension of each image does not exceed 2016 pixels. Following a manual inspection of the images, we apply the following filters to refine the selection process further, retaining only the images that meet the specified requirements:

% \begin{enumerate}
%     \item The ratio between the width and height of the image is less than 100.
%     \item The width or height of the image is larger than 224 pixels.
%     \item The number of pixels in the image is less than 89,478,485.
% \end{enumerate}


\noindent\emph{Caption Cleaning and Image Filtering:}
After a manual inspection of the initially collected dataset, we design several transformations to clean the captions and filter the images.

\noindent\emph{Caption Cleaning}: (i) Chunks with captions shorter than 5 words are removed; (ii) For captions with LaTeX expressions such as math formulas and references, we apply the \texttt{pylatexenc}\footnote{https://github.com/phfaist/pylatexenc} to transform the LaTeX to text with math formulas retained, citations to a special symbol \texttt{<cit.>}, references to \texttt{<ref>}. An illustration of caption cleaning can be found in Appendix~\ref{apx:caption_clean}.
%\textgreater"


\noindent\emph{Image Filtering}: We remove images that are deemed to be problematic according to the following rules:
(i) Images with an aspect ratio larger than 100; (ii) Images with the shortest edge shorter than 224 pixels; and (iii) Images with pixel numbers larger than the decompression bombs threshold.
% The ratio between the width and height of the image is less than 100.
%     \item The width or height of the image is larger than 224 pixels.
%     \item The number of pixels in the image is less than 89,478,485.
% Replace incorrect newline characters with empty spaces.
    % \item Replace consecutive empty spaces with a single space.
    % Remove chunks that have captions with fewer than 5 words or no words at all.
   % Utilize pylatexenc to convert LaTeX to text, ensuring that math formulas are retained. Additionally, normalize citations and tables to "\textless ref \textgreater" and "\textless cit. \textgreater" format. (\cref{tab:pylatexenc_clean})
% \end{enumerate}

After these processes, 100 pairs are sampled to perform an additional manual inspection, where we found all of these pairs contained clear images and correct caption descriptions. We provide visualized figure-caption pairs in Appendix~\ref{apx:case_illustrations}.
\paragraph{Statistics of ArXivCap}
% In this section, we present a general analysis of \DatasetName. Detailed analysis can be found in the Appendix. 

\input{tables/detail_statistic_title_abstract_image_caption}

\input{tables/compare}

Table~\ref{tab:detail_statistic_title_abstract_image_caption} lists the dataset statistics. 
ArXivCap consists of 572K papers, containing 6.4M high-quality images in total with 193M words. 
A word cloud illustration of captions can be found in the Appendix~\ref{apx:caption_word_cloud}.
Figure~\ref{fig:domain-distribution} demonstrates the paper domain distribution extracted from ArXiv, where we find that our ArXivCap covers 32 domains, such as computer science, mathematics, physics, and economics.
As shown in Table~\ref{tab:dataset_comparison_1}, compared with previous scientific figure datasets, our ArXivCap is the largest figure-caption dataset collected from real papers and covers a wide range of scientific domains, serving as a valuable resource for improving and benchmarking LVLMs.

%, and our ArXivQA is the only open-domain QA datasets with diverse figure types.

% , providing a comprehensive general knowledge for LVLMs.

% It is the largest image-caption dataset for academic figures, encompassing a diverse array of domains and image categories. \cref{tab:dataset_comparison_1} presents comparison between \DatasetName and other datasets.
 % We get the domain category for each paper from \cite{ArXiv.org_submitters_2023}. 



% \input{tables/statistics}

\input{figs/domain-distribution}

\subsection{ArXivQA}
\label{subsec:arxiv_qa}
As our ArXivCap contains diverse images from scientific domains, we assume that learning to answer questions about these figures could boost scientific reasoning ability. Following the successful practice of LLaVA~\citep{liu2023llava}, we adopt GPT-4V to generate instruction-tuning datasets for generating the QA pairs based on the figures extracted from scientific papers.
Specifically, we design a prompting template to query GPT-4V for generating QA pairs based on 35K images randomly sampled from our ArXivCap.
Table~\ref{tab:prompt_for_ArXivqa} in Appendix~\ref{apx:prompt_template} provides the template we used for the prompt.
The generated pairs are parsed according to the format requirement and we discard the samples without options and rationales.
There are 100K QA pairs after filtering the invalid samples. 
The dataset comprises questions with an average word count of 16.98 for the question text.
% and options for the question with an average word count of 31.86. 
On average, there are 4.20 options per question and the average length of the text for a single option is 7.59 words.
Appendix~\ref{apx:case_illustrations} provides samples from the ArXivQA dataset.

As a preliminary study, we sample 1,000 samples from ArXivQA and prompt open-sourced LVLMs to predict answers given the questions and options. 
A simple prompt is designed to employ GPT-4 for extracting the answer label from the model generations.
For human performance, we ask four authors to perform predictions on a 100-sample subset (where 17 samples are from the CS domain). Each of them is asked to answer 50 samples and the accuracy scores are obtained by averaging two annotators.
As shown in Table~\ref{tab:arxiv_qa_acc}, most models struggle to perform satisfactorily on the ArXivQA dataset, falling far behind human performance. This verifies our premise that current open-sourced LVLMs fail to understand scientific figures. We also notice that simply increasing the model scale from 7B (LLaVa-1.5-7B) to 13B (LLaVa-1.5-13B) does not yield a significant boost, which indicates that the ability for multimodal mathematical reasoning cannot be simply acquired from the LLM-side only. 



\input{tables/arxivqa_acc}
% Domain and Option Number, Option length, verb visualization?


