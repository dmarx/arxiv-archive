\input{tables/ArXivqa}

\section{Experiments}

We conduct experiments to (i) validate the effectiveness of ArXivQA for boosting multimodal scientific reasoning for open-source LVLMs~(\S\ref{subsec:math_reasoning}) and (ii) benchmark LVLMs capability to comprehend scientific figures with ArXivCap~(\S\ref{subsec:exp_arxivcap}).

\input{math_reasoning}


\input{03_method}
\subsubsection{Experimental Settings}
\label{subsubsec:arxiv_cap_setting}




\paragraph{Dataset}
We divide ArXivCap into training and test sets with a 9:1 ratio for evaluation. The test set includes:
161.3K samples for single-figure captioning,
12.8K samples for multiple-figure captioning,
57.2K samples for contextualized captioning, and
57.2K samples for title generation.

\paragraph{Evaluated Models}
We select various LVLMs covering different architectures. 
(1) LVLMs designed for dealing with a single image, BLIP2-OPT-6.7B~\citep{li2023blip2}
, InstructBLIP-Vicuna7B~\citep{dai2023instructblip}, 
% MiniGPT4~\citep{zhu2023minigpt4}, 
LLaVA-1.5-7B/13B~\citep{liu2023llava15}. Due to the ability limitation, we only benchmark these models on the single image captioning task;
(2) LVLMs capable of handling interleaved text-image inputs, such as OpenFlamingo-9B~\citep{Alayrac2022FlamingoAV,awadalla2023openflamingo}, IDEFICS-Instruct-9B~\citep{laurencon2023obelics}, Qwen-VL-Chat-7B~\citep{Qwen-VL}. These models are evaluated on all the tasks we proposed;
(3) Proprietary models such as Gemini 1.0 Pro Vision and GPT-4V.
Due to the large scale of our test set, we randomly sample a subset consisting of 500 instances for evaluating these two models to reduce costs, with corresponding scores colored in \textcolor{gray}{grey}. Details of evaluated models and the task prompts used are provided in Appendix~\ref{apx:evaluation_details}.

\paragraph{Training Settings} 
To investigate whether in-domain training can enhance the model's capabilities, we train the Qwen-VL-Chat-7B on ArXivCap using the same setting as in \S\ref{subsubsec:exp_setting}. To fit the input length limit, we set the maximum number of figures per sample to four. The training process takes 70 hours with 8 NVIDIA A100s.
% To investigate whether in-domain training can enhance the model's ability, we train the Qwen-VL-Chat-7B for three epochs and adopt the AdamW optimizer~\citep{adam} with a learning rate of 1e-5 and weight decay set to 0.05, following the original setup of Qwen-VL-Chat.
% The dataset is split in a 9:1 ratio into training and test sets.
% We set the maximum figure number in a sample to $4$ to fit the input length limit.
% The training costs 70 hours with 8 NVIDIA A100 GPUs.

\paragraph{Metrics}BLEU-2~\citep{bleu}, ROUGE-L~\citep{lin-2004-rouge} and BERT-Score~\citep{bertscore} are adopted as the automatic evaluation metrics.
% We adopted BLEU-2 \citep{bleu}, ROUGE-L \citep{lin-2004-rouge}, and BERT-Score \citep{bertscore} as our automatic evaluation metrics. 
We also explore using GPT-4 to assist in caption evaluation. Our findings in Appendix \ref{apx:gpt4_eval} indicate that ROUGE-L and BLEU-2 scores are highly correlated with GPT-4's annotations. We primarily use these three metrics due to their convenience. A manual error analysis is conducted to supplement the automatic metrics~(\S\ref{subsubsec:manual_eval}).


% We also explore using GPT-4 to aid caption evaluation. Our results in Appendix~\ref{apx:gpt4_eval} show that ROUGE-L and BLEU-2 are highly correlated with the score annotated by GPT-4. We mainly use previous three metrics due to their convinence.
% Besides, a manual error analysis is performed to supplement the automatic evaluation (\S\ref{subsubsec:manual_eval}).
% As the automatic metrics cannot faithfully reflect the generation quality, we randomly sample pairs and perform human evaluation. We also incorporate GPT-4V for evaluation.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/domain_performance.pdf}
    \caption{Relative accuracy changes brought by the training on different domain ArXivQA samples.}
    \label{fig:domain_analysis}
\end{figure}
\input{tables/single_cap}
\input{tables/cap_with_title_abstract}
\input{tables/main_ret}
\subsubsection{Results}
\paragraph{Results of Single-Figure Captioning}
The evaluation results for the single-figure captioning task are presented in Table \ref{tab:single_cap_ret}. Despite achieving near-perfect performance on conventional image captioning tasks like MSCOCO \citep{lin2014mscoco}, open-source LVLMs, such as LLaVA models, face challenges when applied to academic figures. 
% Among the LVLMs, Qwen-VL-Chat emerges as the top-performing open-source candidate, surpassing its counterparts. 
For closed models, GPT-4V performs comparably with Gemini 1.0 Pro Vision.
Furthermore, continuous training on our dataset yields a significant performance boost for this task. For instance, fine-tuning results in a notable increase in the BLEU-2 score from 4.4 to 8.9, indicating a promising avenue for enhancing academic figure comprehension through domain-specific training.
We also investigate whether providing additional context information, such as the paper title and abstract, could help models generate better figure captions. As shown in Table \ref{tab:cap_with_meta_ret}, adding the title is beneficial evidenced by the boosted scores, while providing abstracts brings negligible gains.

% The evaluation results for the single figure captioning task are presented in Table~\ref{tab:single_cap_ret}. 
% Despite achieving almost perfect performance on conventional image captioning tasks such as MSCOCO~\citep{lin2014mscoco}, LVLMs encounter challenges when applied to academic figures. Among LVLMs, Qwen-VL-Chat emerges as the top-performing open-source candidate, surpassing its counterparts. 
% GPT-4V performs comparably with the Gemini 1.0 Pro. 
% Moreover, continuous training on our dataset yields a significant performance boost for this task. For instance, fine-tuning results in a notable increase in the BLEU-2 score from 4.4 to 8.9, indicating a promising avenue for enhancing academic figure comprehension through domain-specific training.
% We further investigate whether providing additional context information such as paper title and abstract could help models caption figures better.
% As shown in Table~\ref{tab:cap_with_meta_ret}, adding the title is beneficial for generating better captions while providing abstracts seems to bring negligible gains.

\paragraph{Results of Multiple-Figure Captioning}
As shown in the first block of Table \ref{tab:ret_three_tasks}, similar to single-figure captioning, multiple-image captioning poses a challenge for current open-source LVLMs. For instance, Qwen-VL-Chat achieves only a 3.0 BLEU-2 and a 7.2 ROUGE-L score on this task, considerably lower than its performance in single-figure captioning. In contrast, GPT-4V consistently demonstrates proficiency in both tasks, suggesting a balanced ability to capture semantics across multiple images. Notably, training on our ArXivCap dataset yields more pronounced improvements for this task, culminating in Qwen-VL-Chat even surpassing the performance of the GPT-4V model. This enhancement underscores the pivotal role of our dataset in facilitating LVLMs to enhance reasoning capabilities over multiple images, leading to more effective summarization of scientific figures.

\paragraph{Results of Contextualized Captioning}
In the middle block of Table~\ref{tab:ret_three_tasks}, we find that IDEFICS-Instruct-9B achieves the best performance on this task.
This achievement is largely attributed to its remarkable proficiency in leveraging contextual cues, stemming from its extensive pre-training involving interleaved image-text pairs~\citep{laurencon2023obelics}. 
Interestingly, fine-tuning on ArXivCap results in marginal performance declines across all metrics, with GPT-4V achieving the lowest scores as well. 
This phenomenon can be attributed to the tendency of sequential captions to exhibit similar patterns, thereby favoring models that effectively leverage contextual cues.
% such as copying previous captions.
% Interestingly, fine-tuning on ArXivCap leads to marginal performance declines across all metrics and GPT-4V achieves the lowest scores as well.
% We attribute this phenomenon to the fact that sequential captions usually have similar patterns, therefore models that pay more attention to contextual cues can obtain higher scores.
% We hypothesize that the fine-tuned model would focus more on the image content instead of referring to the previous captions when generating the new caption. 
We perform two more challenging evaluations by (i) providing context pairs from another paper and (ii) randomly shuffling the order of figure-caption pairs in the context.
As shown in Table~\ref{tab:order_analysis}, 
the performance with random contexts degrades significantly, validating our previous hypothesis.
Instead, the fine-tuned model demonstrates more robust captioning results under these settings, evidenced by the slight 8\% drop on ROUGE-L compared to the 31\% of the original model with shuffled context orders.


\begin{table}[t!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc}
    \toprule
       Model  &   BLEU-2 ($\Delta\downarrow$) & ROUGE-L ($\Delta\downarrow$)  \\
    \midrule
        Qwen-VL-Chat-7B & 17.0 & 22.1\\ 
       \quad + random contexts & 	5.7 (66.5\%)&  13.0 (38.1\%)\\ 
       \quad  + shuffle order& 12.0 (29.4\%) &15.1 (31.7\%) \\ 
       
    \midrule 
      Qwen-VL-Chat-7B$_\text{ArXivCap}$ & 16.1 & 21.2\\ 
       \quad + random contexts	& 7.5 (53.4\%)	& 14.3 (32.5\%)\\ 
        \quad + shuffle order & 14.1 (12.4\%) & 19.5 (8.0\%) \\
    
    \bottomrule
    \end{tabular}}
    \caption{Contextualized captioning performance is influenced by the order. After tuning on the ArXivCap, the model is more robust to the order of the history captions.}
    \label{tab:order_analysis}
\end{table}
\paragraph{Results of Title Generation}
The results are presented in the last block of Table~\ref{tab:ret_three_tasks}.
Notably, the title generation task poses a formidable challenge, evident in the significantly lower overall BLEU-2 score compared to the captioning tasks. This suggests the inherent difficulty in generating precise predictions for paper titles.
% We observe that for the title generation task, the overall BLEU-2 score is much lower than the captioning tasks, indicating that it is challenging to generate the exact prediction for the paper. 
A contrasting picture emerges when considering the ROUGE-L and BERT-Score metrics, which either closely align or surpass the performance on captioning tasks. This underscores the model's proficiency in producing semantic-related results given the presented figures. 
Consistent with the previous two tasks,
fine-tuning the model on our dataset yields substantial enhancements for the title generation task. The BLEU-2 score jumps impressively from 2.6 to 6.7, while the ROUGE-L score sees a commendable increase from 15.8 to 23.5. 
These findings highlight the challenge of title generation for current LVLMs and the effectiveness of our dataset in improving the model's capability to generate accurate titles.

% Consistent with the previous two tasks, fine-tuning the model on our dataset also brings a clear improvement for the title generation, increasing the BLEU-2 score from 2.6 to 6.7 and ROUGE-L score from 15.8 to 23.5.


% \subsection{Analysis}
% In this section, we further perform additional analysis to understand the bottleneck of existing models.


% \paragraph{Pure Text2Text Model}
% OCR analysis 
% add results on the OCR t

% \paragraph{Prompt Sensitivity}
% \paragraph{Order Sensitivity of Contextualized Captioning}


% \paragraph{Transfer to Science QA dataset}

\subsection{Analysis}

\begin{figure}
    \centering
\includegraphics[width=0.8\linewidth]{figs/pie_chart.pdf}
    \caption{Manual analysis of the generated captions. }
    \label{fig:error_pie_chart}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=0.62\linewidth]{figs/mathvista_case_study_v2.pdf}
    \caption{ArXivQA enables the model not only to answer questions related to scientific figures in papers (left) but also to improve mathematical understanding ability (right). The model not only selects correct options but also gives reasonable rationale.}
    \label{fig:math_case_study}
\end{figure*}

\paragraph{Manual Evaluation of Generated Captions} 
\label{subsubsec:manual_eval}
We conduct a manual inspection for single-figure captioning results. To ensure a more informed evaluation, we focus on a paper from the CS domain, leveraging our domain knowledge to assess caption quality better. 
The quality of generated captions is assessed by scrutinizing the figure, the ground-truth caption, the paper title, and the abstract.
We categorize captions into the following quality types according to our preliminary inspection: (1) \emph{Acceptable}, where captions accurately encapsulate the scientific figure's essence, aligning with the intended information of the ground-truth; (2) \emph{Over Simplification}, instances where the model oversimplifies content, offering a broad overview while neglecting specific details and nuances present in the ground truth; (3) \emph{Recognition Error}, where the model inaccurately recognizes and describes key visual and textual elements in the scientific figure, such as colors, numerical values, or textual context; and (4) \emph{Contextual Misinterpretation}, where the model misinterprets the specific context of the scientific figure, resulting in captions relevant in a generic sense but inaccurate for the given figure. Visualized generated captions of different types are shown in Figure~\ref{fig:caption_type} of Appendix~\ref{apx:caption_type}.
The results of 100 manually examined captions are depicted in Figure~\ref{fig:error_pie_chart}, revealing that only 16\% of captions are deemed acceptable when compared to human-written ones. 
Among unsatisfactory captions, contextual misinterpretation emerges as the dominant issue, suggesting a need for incorporating more contextual information as suggested in Table~\ref{tab:cap_with_meta_ret}. Oversimplification is another concern, with generic captions identified. Additionally, 23\% and 19\% of examined captions suffer from the oversimplification issue and recognition errors in reported numbers/texts in the caption, respectively. The former is attributed to the highly frequent simple caption in the training dataset and the latter issue could be addressed through potential integration with OCR results.
% In summary,  underscores opportunities to enhance captioning quality. 
Our manual evaluation suggests future efforts may benefit from incorporating additional context clues, such as paper metadata, improving the model's fundamental perception abilities, and utilizing external information.


% The error distribution comes from Figure~\ref{fig:error_pie_chart} shows that 16\% of the captions are perceived as acceptable compared with the human-written ones, indicating sufficient room for further improvements.
% Among the unsatisfactory captions, the most dominant type is contextual misinterpretation, which could be alleviated by incorporating more contextual information.
% Another issue is oversimplification, where the captions are generic to the present figure, such as (a) in Figure X.
% Also, 19\% of examined captions suffer from the recognition ability of the base model, i.e., the numbers are wrongly reported, which we think can be combined with OCR results.
% In summary, our manual evaluation suggests that future endeavors can be conducted to improve the captioning quality by incorporating more context clues, such as meta data of the paper, and improve the fundamental perception ability of the model.
% Acceptable 0.16
% Over Simplification 0.23
% Contextual Misinterpretation 0.42
% Recognition Error 0.19


\paragraph{Case Study of MathVista}
We conduct case studies to illuminate the tuning effects facilitated by our ArXivQA dataset. In the left segment of Figure~\ref{fig:math_case_study}, ArXivQA helps the model accurately answer a question related to the presented bar plot. 
% Notably, our ArXivQA, rooted in figures extracted from scientific papers, extends its impact beyond visual data. 
The right part in Figure~\ref{fig:math_case_study} demonstrates that ArXivQA can enhance algebraic reasoning abilities. Here, a question involving the derivative of a function is correctly answered, accompanied by a lucid reasoning rationale.
Figure~\ref{fig:geometry_fail} in Appendix~\ref{apx:failure_mathvista} highlights a challenging geometry problem where both models generate hallucinated outputs. 
These illustrative cases collectively affirm the efficacy of our dataset.
% reinforcing the observed quantitative accuracy improvements. 
% These nuanced examinations not only provide a qualitative understanding of the tuning effects but also underscore the dataset's broader impact on diverse mathematical reasoning tasks.


% We further present case studies to understand the improvement of the tuning effect of our ArXivQA dataset. As shown in the left part of Figure~\ref{fig:math_case_study}, the model after fine-tuning produces a correct answer for the question related to the presented bar plot. While our ArXivQA is constructed based on the figures extracted from scientific papers, the right case in Figure~\ref{fig:math_case_study} suggests that fine-tuning may also improve the algebraic reasoning ability, where a question of function derivative is correctly answered with reasoning rationale.
% We also present a failed geometry problem in Figure~\ref{fig:geometry_fail} in the Appendix where we found both models produce hallucinated outputs for a challenging geometry problem. Overall, these intuitive cases confirm the effectiveness of our dataset, echoing previous quantitative accuracy boost.