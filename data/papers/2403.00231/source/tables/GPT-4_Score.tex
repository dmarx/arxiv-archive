\begin{table*}[t!]
    \centering
\tiny 
\begin{tcolorbox}
Annotation Instruction:\\
As an annotator, your role is to serve as an unbiased and objective judge in evaluating the accuracy of captions produced by a Large Vision-Language Model (LVLM) for scientific figures. These figures are extracted from academic papers, and to aid your assessment, we will provide you with the paper's title and abstract for necessary context.

You will be presented with the original caption—referred to as the 'ground truth'—and the LVLM generated caption, termed the 'prediction'. You could take into account the context given by the paper's title and abstract for background knowledge, comparing it critically with both captions.

In your assessment, please pay attention to the factual alignment, including but not limiting to the following aspects:\\
- Numerical data and statistics: Verify their accuracy and correspondence to the data presented in the figure.\\
- Symbols: Check for correct representation and usage in the context of the scientific subject matter.\\
- Factual content: Ensure all facts are consistent with those stated in the ground truth caption and the paper's content.\\

<title>{title}</title>\\
<abstract>{abstract}</abstract>\\
<ground truth>{gt}</ground truth>\\
<prediction>{pred}</prediction>\\

Compare the prediction to the ground truth, provide a brief analysis, and assign a score using one of the following quality labels: <Perfect>, <Good>, <Fair>, <Poor>, <Incorrect>.

Below we describe the detail criteria for score:
<Perfect>: The prediction is almost identical to the ground truth, with only minor, inconsequential differences that do not change the meaning. All numerical data, symbols, and factual content are accurate and consistent.\\
<Good>: The prediction is largely similar to the ground truth but has some noticeable differences that may slightly change the meaning. However, the core information is still correct, and the numerical data, symbols, and factual content are mostly accurate and consistent with the figure content.\\
<Fair>: The prediction captures the basic idea of the ground truth but has significant differences that change the meaning in a way that cannot be ignored. There may be some inaccuracies or inconsistencies in the numerical data, symbols, or factual content when compared to the figure content.\\
<Poor>: The prediction is related to the ground truth but has serious errors or omissions that significantly change the meaning. The numerical data, symbols, or factual content may be largely inaccurate or inconsistent with the figure content.\\
<Incorrect>: The prediction is completely different or irrelevant to the ground truth, with no similarities between the two. The numerical data, symbols, and factual content are entirely inaccurate or inconsistent with the figure content.

Give a brief analysis within 100 words and then output a quality label wrapped with "<>".
    \end{tcolorbox}
    \caption{Prompt template designed for GPT-4 to evaluate generated captions based on the paper title, abstract, and ground truth.}
    \label{tab:prompt_for_gpt4_score_caption}
\end{table*}

\begin{table}[t!]
    \centering
    \small  
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lcccc@{}}
         \toprule
         Model & BLEU-2 & ROUGE-L & BERT-S & GPT-4 Score \\
         \midrule
BLIP-2-OPT-6.7B & 1.5 & 6.6 & 81.3 & 1.18 \\
InstructBLIP-Vicuna7B & 3.5 & 10.3 & 83.6 & 1.48 \\
LLaVA-1.5-7B & 2.3 & 10.4 & 83.3 & 1.80 \\
LLaVA-1.5-13B & 2.7 & 11.0 & 83.6 & 1.69 \\
OpenFlamingo-9B & 5.8 & 10.3 & 82.7 & 1.52 \\
IDEFICS-Instruct-9B & 2.1 & 9.3 & 83.8 & 1.55 \\
Qwen-VL-Chat & 4.7 & 11.1 & 82.0 & 1.81 \\
Qwen-VL-Chat tuned w/ ArXivCap & 8.6 & 15.3 & 83.2 & 2.03 \\
         \bottomrule
    \end{tabular}}
    \caption{Results of 500 single-figure captions generated by various models.}
    \label{tab:gpt4_score_single_caption}
\end{table}

