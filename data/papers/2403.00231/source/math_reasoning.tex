
\subsection{Boosting LVLMs with ArXivQA}
\label{subsec:math_reasoning}

\subsubsection{Experimental Settings}
\label{subsubsec:exp_setting}
We adopt Qwen-VL-Chat-7B~\citep{Qwen-VL}
as the backbone due to its support for interleaved image-text input formats and high-resolution images.
We fine-tune it on our ArXivCap (Qwen-VL-Chat-7B$_\text{ArXivCap}$), ArXivQA (Qwen-VL-Chat-7B$_\text{ArXivQA}$) and combination of these two datasets (Qwen-VL-Chat-7B$_\text{ArXivCap + ArXivQA}$) for three epochs with a learning rate of 1e-5 following the original paper.
We combine the answer and the rationale in ArXivQA to form the target output during training. 
Models are evaluated on MathVista~\citep{mathvista}, a benchmark that requires fine-grained, deep visual understanding and compositional reasoning. MathVista contains 6,141 examples, consisting of five multimodal tasks Figure QA, Geometry Problem Solving, Math word problem, Text Book QA, and Visual QA. 
We select 478 multiple-choice questions in the \texttt{testmini} split to avoid the inconsistency of answer parsing. We compute the accuracy scores and adopt the provided prediction files for calculating the baseline performance.
% is adopted as the metric.

\subsubsection{Results}

As shown in Table~\ref{tab:mathvista_ret},
fine-tuning on our Multimodal ArXiv, especially on the ArXivQA dataset, consistently boosts the performance, helping the open-sourced Qwen-VL-Chat achieve a comparable overall MathVista reasoning performance.
Due to the wide coverage of the scientific figures, the performance gain mainly comes from significantly improved Geometry Problem Solving, Textbook QA, and Visual QA tasks. For example, after fine-tuning on the ArXivQA dataset, the accuracy is increased from 19.1\% to 34.0\% and from 46.7\% to 70.0\% on Geometry Problem Solving and Textbook QA tasks, respectively.
The improvement on Math Word Problem is marginal, where we think the domain-specific data augmentation can be further explored with a curated filtering dataset on our dataset~\citep{gao2023gllava}.
On the contrary,
the accuracy of Figure QA deteriorates slightly compared with the original backbone model, which we attribute to the fact that most of the plots in the Figure QA evaluation are sampled from synthesized datasets such as DVQA~\citep{kafle2018dvqa}, exhibiting great gaps between real-world paper figures.

\subsubsection{Analysis}
We investigate how different subject domains affect mathematical reasoning ability using pairs of questions and answers (QA). We focus on six domains with more than 5K samples each. From each domain, we randomly choose a subset of 5K samples to ensure fairness in comparison. We then fine-tune the Qwen-VL-Chat base model using QA pairs from each domain and observe how it affects the model's accuracy compared to its original state.
Figure~\ref{fig:domain_analysis} demonstrates the relative accuracy changes (i.e., $\frac{\text{Accuracy after Fine-tuning}}{\text{Original Accuracy}} - 1 $) after training the model on QA pairs from each domain. Our findings reveal several key points: (i) QA pairs from the Computer Science (CS) domain are highly effective for improving mathematical reasoning ability, achieving a notable 27.09\% relative improvement. We attribute this to the compositional nature of the CS area. (ii) The most beneficial domain varies depending on the specific task. For instance, QA pairs from astrophysics domains enhance geometry problem-solving, while those from Condensed Matter improve performance in math word problems. (iii) Most domains hurt the Figure QA task. This suggests that synthetic Figure QA might not be the best benchmark for assessing realistic reasoning ability.
These findings underscore the efficacy of generated QA pairs and offer valuable insights for future research, such as adjusting task-specific weights in the dataset accordingly.
