\subsection{Benchmarking LVLMs on ArXivCap}
\label{subsec:exp_arxivcap}
\subsubsection{Evaluated Tasks}
\label{subsubsec:evaluated_task}
Four vision-to-text tasks to benchmark LVLMs' ability to comprehend scientific figures.
\paragraph{Single-Figure Captioning} 
Similar to the traditional image captioning setup~\citep{lin2014mscoco}, single-figure captioning requires the model to generate a caption for the given single figure. 
The captions generated by the model are expected to encapsulate the nuanced details within these figures, including numbers and mathematical formulas, presenting a unique challenge for models to identify and articulate these elements accurately.
Formally, given an image-caption pair $(I, C)$, the LVLM $\mathcal{M}$ is asked to generate the caption given an instruction prompt $P_s$ to hint the goal of scientific captioning:
\begin{equation*}
    \hat{C} = \mathcal{M} (I, P_s),
\end{equation*}
where $\hat{C}$ would be evaluated according to the ground-truth $C$.
% formalize it 

\paragraph{Multiple-Figure Captioning}
We introduce a more intricate challenge involving applying reasoning across multiple images. This task, termed Multiple-Figure Captioning, necessitates the model to craft a comprehensive summary caption for subfigures. 
As exemplified in Figure~\ref{fig:chunk_example}, the model is tasked with generating an overarching caption for two or more subfigures, leveraging visual clues to draw comparisons and formulate summary captions.
% Specifically, Multiple Image Captioning requires the model to generate a summary caption for a series of related subfigures. For example, as illustrated in \cref{fig:chunk_example}, given two subfigures, the model is asked to generate the overall caption for these two figures by making comparisons and drawing conclusions from the visual clues.
Formally, given a list of figures $L = \left( I_1, \ldots, I_n\right)$, the model is asked to generate the ground-truth main caption $C$ by considering all the semantics in the figures with a task prompt $P_m$:
\begin{equation*}
    \hat{C} = \mathcal{M} ( L, P_m ) = \mathcal{M} ( I_1, \ldots, I_n, P_m).
\end{equation*}


\paragraph{Contextualized Captioning}
Inspired by the evolving in-context learning capabilities of LLMs~\citep{brown2020language,icl_survey}, we introduce a contextualized captioning task to examine the in-context learning ability of LVLMs. 
In this task, the model is presented with a set of figure-caption pairs, and its goal is to generate a caption for a given image based on the provided demonstrations.
Given a sequential image-captions pairs $S = \{ ( I_i, C_i)  \}_{i=1}^n$ consisting of $n$ pairs of image $I_i$ and the corresponding $C_i$, the contextualized image captioning task can be formalized as follows:
\begin{equation*}
    \hat{C_n} = \mathcal{M} ( I_1, C_1, \ldots, I_{n-1}, C_{n-1}, I_n, P_c).
\end{equation*}
The model is supposed to leverage the context history to enhance the accuracy and coherence of the generated caption.

\paragraph{Title Generation}
This task requires a nuanced understanding of figures and captions to distill essential observations into a high-level summary of the presented results for LVLMs.
Specifically, instead of producing the captions for the figures, this task requires the model to connect different figures and corresponding captions to infer the paper title.
Let $S = \{ (I_i, C_i) \}_{i=1}^m$ be a sequence of $m$ figure-caption pairs in the extracted paper. 
Note that $I_i$ could be a single figure or a multiple-figure, and we reuse $I_i$ for simplicity here.
The title generation asks $\mathcal{M}$ to generate the title for the paper given a task prompt $P_t$:
\begin{equation*}
   \hat{T} = \mathcal{M} ( I_1, C_1, \ldots, I_{m}, C_m, P_t ) .
\end{equation*}
The prediction $\hat{T}$ is evaluated by comparing it to the original title $T$.




