\section{Introduction}
\label{sec:intro}

% The evolution of Large Language Models (LLMs)~\citep{gpt3,touvron2023llama,chatgpt} and vision foundation models~\citep{radford2021clip}
% has catalyzed advancements in constructing multi-modal intelligent agents capable of exploring the multi-modal realm, which are achieved by integrating LLMs and pre-trained vision encoders, followed by fine-tuning on image-text pairs~\citep{madureira-2021-flamingos,awadalla2023openflamingo} or specifically curated vision instruction-tuning datasets~\citep{liu2023llava,li2023m3it,Qwen-VL}.
% The evolution of Large Language Models (LLMs)~\citep{gpt3,touvron2023llama,chatgpt} and vision foundation models~\citep{radford2021clip} has catalyzed advancements in constructing intelligent agents capable of processing multi-modal information. 
Large vision-language models (LVLMs), which integrate large language models (LLMs)~\citep{gpt3,touvron2023llama} with pre-trained vision encoders through cross-modal alignment training~\citep{madureira-2021-flamingos,liu2023llava,li2023m3it}, 
have demonstrated remarkable perceptual and cognitive capabilities in processing concrete images from everyday scenes~\citep{gpt4v,fu2023mme,yang2023dawn,reka2024core}.
However, recent studies have shown that open-source LVLMs struggle to understand abstract figures, such as geometric shapes in multimodal mathematical reasoning~\citep{mathvista,zhang2024mathverse} and scientific plots~\citep{mmmu}. 
The inadequacy of training datasets in scientific domains that involve complex reasoning with abstract figures is the main underlying cause.
% This limitation stems from the scarcity of training datasets in scientific domains that incorporate complex reasoning involving abstract figures.
% , as most multi-modal instruction datasets focus on general images
% However, it remains unclear \lpk{if we know it's basically not satisfactory, then we can say it's not good rather than remains unclear} whether LVLMs can effectively reason over scientific figures and generate concise, human-like summary descriptions for paper figures. 
% This inquiry holds significance on two fronts: from a scientific perspective, it offers an ideal testbed for examining whether LVLMs can handle complex semantics involving reasoning processes that go beyond what is required by traditional vision question-answering~(QA) tasks. On a practical level, the implementation of a machine-assisted captioning tool could significantly enhance research efficiency, rendering scientific figures more accessible for individuals with visual impairments. \lpk{left here.}
% traditional vision-language tasks have 

To address this, we construct Multimodal ArXiv by utilizing the rich resources in preprints hosted on arXiv to improve the ability to understand scientific literature in LVLMs.
We first curate ArXivCap, a
diverse scientific figure-caption dataset.
In contrast to previous scientific figure datasets, which consist of synthesized figures~\citep{chen2020figcap}
or are restricted to simple captioning scenarios in the computer science domain~\citep{hsu-etal-2021-scicap-generating}, our dataset is composed of figures extracted from academic papers across a range of domains.
ArXivCap has 6.4M images and 3.9M captions from 572K papers. 
We also keep the subfigure structure, and titles of original papers, thereby supporting diverse evaluation tasks.
We further instruct GPT-4V to generate 100K multiple-choice question-answering~(QA) pairs for the figures in ArXivCap.
The resulting ArXivQA dataset could naturally serve as a pivotal resource for improving the scientific reasoning abilities of LVLMs.

We validate the effectiveness of our Multimodal ArXiv dataset from two dimensions: reasoning ability measured by QA accuracy and generation performance through novel vision-to-text tasks. 
Our experiments demonstrate that ArXivQA brings a significant 10.4\% absolute accuracy boost for Qwen-VL-Chat~\citep{Qwen-VL}, on the MathVista~\citep{mathvista}, a challenging benchmark for multimodal mathematical reasoning. 
Additionally, detailed analysis uncovers the relationship between paper domains and fine-grained task performance.
Moreover, using ArXivCap, we define four generation tasks of varying complexity to benchmark the ability of LVLMs to comprehend scientific plots:
(1) captioning a single academic figure, (2) generating overall summaries for multiple sub-figures, 
(3) in-context figure captioning given previous figure-caption pairs, and (4) generating paper titles from figure-caption pairs. 
We examine various LVLMs, including open-source models as well as proprietary models including GPT-4V~\citep{gpt4v} and Gemini 1.0 Pro Vision~\citep{team2023gemini}.
Evaluation results reveal that despite that current LVLMs still face challenges generating faithful captions for scientific figures, in-domain training on our dataset yields substantial performance improvements across all four tasks.
Manual error analysis underscores that LVLMs still suffer from misinterpretation of the visual context, recognition errors, and overly simplified captions, paving the way for future studies.
% \lpk{i hate say `something bad, while something good', i prefer say `despite something bad, something good' -- look at the brighter side :p}
% Evaluation results reveal that current LVLMs still face challenges generating faithful captions for scientific figures, while in-domain training on our dataset yields substantial performance improvements across all four tasks.





% In summary, we posit that our curated dataset and comprehensive experimental evaluation serve as a valuable resource for a thorough analysis and improvement of LVLMs' understanding of scientific figures, providing insights into the ongoing development of LVLMs.

% Specifically, .
% r by further pre-training with image-text pairs (Alayrac et al.;
% Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu
% et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models



% OCR supplement training