\section{Related Work}
\label{sec:related}
Recent advancements in LVLMs have seen notable progress in model architecture, training paradigms, and dataset creation~\citep{zhang2024mm}.
% The landscape of LVLMs has been evolving rapidly recently, and the endeavors can be roughly divided into model architecture improvement, training paradigm exploration, and dataset creation. 
% \paragraph{Model Architecture Exploration} 

\paragraph{Model Architecture} LVLMs typically comprise three core modules: (i) a vision encoder for image feature extraction, (ii) a modality alignment module to integrate visual features into the language model embedding space, and (iii) an LLM backbone for decoding multimodal context. 
CLIP~\citep{radford2021clip} is widely used for image encoding, while LLaMA~\citep{touvron2023llama} and Vicuna~\citep{vicuna2023} serve as popular choices for LLMs. 
The alignment module varies from simple linear projections~\citep{liu2023llava,zhu2023minigpt4} to more complex architectures like gated cross-attention layers~ substantiated by Flamingo and IDEFICS~\citep{Alayrac2022FlamingoAV,awadalla2023openflamingo}. Innovations such as the Q-Former module in BLIP2~\citep{li2023blip2} and instruction integration in InstructBLIP~\citep{dai2023instructblip} further enhance alignment capabilities. 
Additionally, Fuyu-8B~\citep{fuyu-8b} introduces a novel framework mapping raw image pixels directly to the LLM embedding space.


% LVLMs consist of three key modules, (i) a vision encoder for encoding images into visual features, (ii) a modality alignment module for mapping the visual features into the embedding space of LLMs, and (iii) an LLM backbone responsible for decoding the response given the multimodal context.
% CLIP~\citep{radford2021clip} is adopted for encoding images and LLaMA~\citep{touvron2023llama,touvron2023llama2} and Vicuna~\citep{vicuna2023} are common choices for LLMs due to their promising performance.
% Regarding the alignment module, 
% (Open)Flamingo~\citep{Alayrac2022FlamingoAV,awadalla2023openflamingo} designs a gated cross-attention layer, where the keys and values are transformations of the vision features while the queries are derived from the language inputs.
% % This paradigm allows LLMs to digest the interleaved context and produce visual-grounded text.
% Another commonly adopted practice is treating the visual features as a prefix, and learning the alignment module by reconstructing captions given images. 
% The alignment module can be a simple linear projection layer~\citep{liu2023llava,zhu2023minigpt4} or a two-layer MLP for better capacity~\citep{liu2023llava15}.
% The Q-Former module introduced by BLIP-2~\citep{li2023blip2} utilizes a suite of learnable query tokens to attend to the visual features for alignment.
% InstructBLIP~\citep{dai2023instructblip} and MM-ICL~\citep{zhao2023mmicl} further incorporate the instruction information during the alignment process for better in-context learning ability~\citep{icl_survey}.
% Fuyu-8B~\citep{fuyu-8b} presents a new architecture that directly projects the raw pixels of images into the embedding space of the LLM to perform the following training and decoding.
\paragraph{Training Paradigms} Regarding the training recipes, PaLI-X~\citep{Chen2023PaLIX} investigates the scaling effects of both vision encoders and language models, highlighting the advantages of scaling both components. Qwen-VL~\citep{Qwen-VL} increases input image resolution and explores different module unfreezing strategies. Alignment methodologies such as RLHF training~\citep{ouyang2022instructgpt}, e.g., LLaVA-RLHF~\citep{2023llavarlhf}, and preference optimization through AI feedback~\citep{2023vlfeedback}
demonstrate effectiveness in aligning LVLMs with human preferences.

% \paragraph{Training Recipe Exploration} 
% PaLIX~\citep{Chen2023PaLIX} delves deeper into the scaling effects of the vision encoder and the language model, showcasing the great potential advantages of scaling both modules.
% Qwen-VL~\citep{Qwen-VL} increases the resolution of images and the number of training samples, demonstrating the significance of dataset quality.
% Beyond the instruction fine-tuning,
% LLaVA-RLHF~\citep{2023llavarlhf} explores RLHF training~\citep{ouyang2022instructgpt} and Silkie~\citep{2023vlfeedback} investigates direct preference optimization~\citep{dpo}, showcasing a better-aligned behavior with human preferences.
\paragraph{Dataset Curation} Dataset quality significantly impacts LVLM performance. Modality alignment training often utilizes web-scale image-caption pairs such as Laion-400M~\citep{laion400m}, with recent studies favoring cleaned captions~\citep{sharegpt4v,capsfusion}. Instruction fine-tuning~(IFT) helps LVLMs respond according to user queries, triggering the exploration of high-quality IFT datasets. 
Efforts include multimodal instruction collections such as MultiInstruct~\citep{multiinstruct} and M$^3$IT~\citep{li2023m3it}, dialog-style datasets such as LLaVA~\citep{liu2023llava} and domain-specific datasets for medical~\citep{med-llava} and text-rich images~\citep{llavar}.
In the scientific domain, FigCAP~\citep{chen2019figcap} and FigureQA~\citep{kahou2017figureqa} are created based on synthetic figures.
DVQA~\citep{kafle2018dvqa} creates heuristic-based questions for bar charts only.
SciCap~\citep{hsu-etal-2021-scicap-generating}, SciCap+~\citep{Yang2023SciCap+}, and M-Paper~\citep{hu2023mplugpaperowl} collect figure-caption pairs from specific domains such as computer science.
Compared with these datasets, our ArXivCap is sourced from diverse scientific domains with a much larger scale, enabling more comprehensive improvements and evaluations. Besides, we employ GPT-4V for creating ArXivQA with challenging questions, showcasing its effectiveness in boosting the mathematical reasoning ability of LVLMs.
% and benchmark LVLMs with four vision-to-text tasks on ArXivCap, demonstrating their limited scientific understanding ability.


% SciCap~\citep{hsu-etal-2021-scicap-generating} introduces a figure-caption dataset based on computer
% science arXiv papers. 
% However, the subfigures are removed and image-related information, such as the formula in the captions are normalized. This pre-processing
% SciCap+~\citep{Yang2023SciCap+} extends SciCap by incorporating extracted OCR tokens from the figures and the paragraphs mentioning the target figure into the caption generalization.
% Our work follows this line while proposing a more diverse coverage of the paper domain and task format, e.g., generating summary sentences for multiple related subfigures. Besides, we benchmark the ability of recently proposed LVLMs and show that they struggle to handle complex scientific figures.

