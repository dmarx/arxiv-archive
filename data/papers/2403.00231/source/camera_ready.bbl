\begin{thebibliography}{62}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei, Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski, Barreira, Vinyals, Zisserman, and Simonyan}]{Alayrac2022FlamingoAV}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{ArXiv preprint}, abs/2204.14198.

\bibitem[{Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu, Marathe, Bitton, Gadre, Sagawa, Jitsev, Kornblith, Koh, Ilharco, Wortsman, and Schmidt}]{awadalla2023openflamingo}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang~Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. 2023.
\newblock Openflamingo: An open-source framework for training large autoregressive vision-language models.
\newblock \emph{ArXiv preprint}, abs/2308.01390.

\bibitem[{Bai et~al.(2023{\natexlab{a}})Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023{\natexlab{a}}.
\newblock Qwen technical report.
\newblock \emph{ArXiv preprint}, abs/2309.16609.

\bibitem[{Bai et~al.(2023{\natexlab{b}})Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou}]{Qwen-VL}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023{\natexlab{b}}.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock \emph{ArXiv preprint}, abs/2308.12966.

\bibitem[{Bavishi et~al.(2023)Bavishi, Elsen, Hawthorne, Nye, Odena, Somani, and Ta\c{s}\i{}rlar}]{fuyu-8b}
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa\u{g}nak Ta\c{s}\i{}rlar. 2023.
\newblock Introducing our multimodal models.

\bibitem[{Brown et~al.(2020{\natexlab{a}})Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020{\natexlab{a}}.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}.

\bibitem[{Brown et~al.(2020{\natexlab{b}})Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020{\natexlab{b}}.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}.

\bibitem[{Chen et~al.(2020)Chen, Zhang, Koh, Kim, Cohen, and Rossi}]{chen2020figcap}
Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, and Ryan Rossi. 2020.
\newblock Figure captioning with relation maps for reasoning.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 1537--1545.

\bibitem[{Chen et~al.(2019)Chen, Zhang, Koh, Kim, Cohen, Yu, Rossi, and Bunescu}]{chen2019figcap}
Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong Yu, Ryan Rossi, and Razvan Bunescu. 2019.
\newblock Figure captioning with reasoning and sequence-level training.
\newblock \emph{ArXiv preprint}, abs/1906.02850.

\bibitem[{Chen et~al.(2023{\natexlab{a}})Chen, Li, Dong, Zhang, He, Wang, Zhao, and Lin}]{sharegpt4v}
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023{\natexlab{a}}.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock \emph{ArXiv preprint}, abs/2311.12793.

\bibitem[{Chen et~al.(2024)Chen, Wu, Chen, Liu, He, Xiong, Liu, Guo, and Huang}]{chen2024vlm_select}
Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi~He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, and Heng Huang. 2024.
\newblock Your vision-language model itself is a strong filter: Towards high-quality instruction tuning with data selection.
\newblock \emph{ArXiv preprint}, abs/2402.12501.

\bibitem[{Chen et~al.(2023{\natexlab{b}})Chen, Djolonga, Padlewski, Mustafa, Changpinyo, Wu, Ruiz, Goodman, Wang, Tay, Shakeri, Dehghani, Salz, Lucic, Tschannen, Nagrani, Hu, Joshi, Pang, Montgomery, Pietrzyk, Ritter, Piergiovanni, Minderer, Pavetic, Waters, Li, Alabdulmohsin, Beyer, Amelot, Lee, Steiner, Li, Keysers, Arnab, Xu, Rong, Kolesnikov, Seyedhosseini, Angelova, Zhai, Houlsby, and Soricut}]{Chen2023PaLIX}
Xi~Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos~Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi~Tay, Siamak Shakeri, Mostafa Dehghani, Daniel~M. Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo~Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, A.~J. Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim~M. Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. 2023{\natexlab{b}}.
\newblock Pali-x: On scaling up a multilingual vision and language model.
\newblock \emph{ArXiv preprint}, abs/2305.18565.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing. 2023.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality.

\bibitem[{Clement et~al.(2019)Clement, Bierbaum, O'Keeffe, and Alemi}]{clement2019use}
Colin~B Clement, Matthew Bierbaum, Kevin~P O'Keeffe, and Alexander~A Alemi. 2019.
\newblock On the use of arxiv as a dataset.
\newblock \emph{ArXiv preprint}, abs/1905.00075.

\bibitem[{Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi}]{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2305.06500.

\bibitem[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, Li, and Sui}]{icl_survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2022.
\newblock \href {http://arxiv.org/abs/2301.00234} {A survey for in-context learning}.

\bibitem[{Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Qiu, Lin, Yang, Zheng et~al.}]{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al. 2023.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock \emph{ArXiv preprint}, abs/2306.13394.

\bibitem[{Gao et~al.(2023)Gao, Pi, Zhang, Ye, Zhong, Wang, Hong, Han, Xu, Li, and Kong}]{gao2023gllava}
Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. 2023.
\newblock \href {http://arxiv.org/abs/2312.11370} {G-llava: Solving geometric problem with multi-modal large language model}.

\bibitem[{{Gemini Team}(2023)}]{team2023gemini}
{Gemini Team}. 2023.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{ArXiv preprint}, abs/2312.11805.

\bibitem[{Google(2023)}]{bard}
Google. 2023.
\newblock Bard.

\bibitem[{Hsu et~al.(2021)Hsu, Giles, and Huang}]{hsu-etal-2021-scicap-generating}
Ting-Yao Hsu, C~Lee Giles, and Ting-Hao Huang. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.findings-emnlp.277} {{S}ci{C}ap: Generating captions for scientific figures}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 3258--3264.

\bibitem[{Hu et~al.(2023)Hu, Shi, Xu, Ye, Ye, Yan, Li, Qian, Zhang, and Huang}]{hu2023mplugpaperowl}
Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi~Qian, Ji~Zhang, and Fei Huang. 2023.
\newblock \href {http://arxiv.org/abs/2311.18248} {mplug-paperowl: Scientific diagram analysis with the multimodal large language model}.

\bibitem[{Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen}]{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2022.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.

\bibitem[{{ImageMagick Studio LLC}()}]{imagemagick}
{ImageMagick Studio LLC}.
\newblock Imagemagick.

\bibitem[{Kafle et~al.(2018)Kafle, Price, Cohen, and Kanan}]{kafle2018dvqa}
Kushal Kafle, Brian~L. Price, Scott Cohen, and Christopher Kanan. 2018.
\newblock \href {https://doi.org/10.1109/CVPR.2018.00592} {{DVQA:} understanding data visualizations via question answering}.
\newblock In \emph{2018 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018}, pages 5648--5656.

\bibitem[{Kahou et~al.(2017)Kahou, Michalski, Atkinson, K{\'a}d{\'a}r, Trischler, and Bengio}]{kahou2017figureqa}
Samira~Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, {\'A}kos K{\'a}d{\'a}r, Adam Trischler, and Yoshua Bengio. 2017.
\newblock Figureqa: An annotated figure dataset for visual reasoning.
\newblock \emph{ArXiv preprint}, abs/1710.07300.

\bibitem[{Kinney et~al.(2023)Kinney, Anastasiades, Authur, Beltagy, Bragg, Buraczynski, Cachola, Candra, Chandrasekhar, Cohan, Crawford, Downey, Dunkelberger, Etzioni, Evans, Feldman, Gorney, Graham, Hu, Huff, King, Kohlmeier, Kuehl, Langan, Lin, Liu, Lo, Lochner, MacMillan, Murray, Newell, Rao, Rohatgi, Sayre, Shen, Singh, Soldaini, Subramanian, Tanaka, Wade, Wagner, Wang, Wilhelm, Wu, Yang, Zamarron, Zuylen, and Weld}]{kinney2023semantic}
Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz~Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex~D. Wade, Linda Wagner, Lucy~Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine~Van Zuylen, and Daniel~S. Weld. 2023.
\newblock \href {http://arxiv.org/abs/2301.10140} {The semantic scholar open data platform}.

\bibitem[{Laurençon et~al.(2023)Laurençon, Saulnier, Tronchon, Bekman, Singh, Lozhkov, Wang, Karamcheti, Rush, Kiela, Cord, and Sanh}]{laurencon2023obelics}
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander~M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023.
\newblock \href {http://arxiv.org/abs/2306.16527} {Obelics: An open web-scale filtered dataset of interleaved image-text documents}.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Wong, Zhang, Usuyama, Liu, Yang, Naumann, Poon, and Gao}]{med-llava}
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023{\natexlab{a}}.
\newblock Llava-med: Training a large language-and-vision assistant for biomedicine in one day.
\newblock \emph{ArXiv preprint}, abs/2306.00890.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Li, Savarese, and Hoi}]{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023{\natexlab{b}}.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock \emph{ArXiv preprint}, abs/2301.12597.

\bibitem[{Li et~al.(2021)Li, Lin, Ren, Li, Zhou, and Sun}]{li-etal-2021-dynamic}
Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, and Xu~Sun. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.31} {Dynamic knowledge distillation for pre-trained language models}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 379--389.

\bibitem[{Li et~al.(2023{\natexlab{c}})Li, Xie, Li, Chen, Wang, Chen, Yang, Wang, and Kong}]{2023vlfeedback}
Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. 2023{\natexlab{c}}.
\newblock Silkie: Preference distillation for large visual language models.
\newblock \emph{ArXiv preprint}, abs/2312.10665.

\bibitem[{Li et~al.(2023{\natexlab{d}})Li, Yin, Li, Chen, Wang, Ren, Li, Yang, Xu, Sun, Kong, and Liu}]{li2023m3it}
Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu~Sun, Lingpeng Kong, and Qi~Liu. 2023{\natexlab{d}}.
\newblock {M$^3$IT}: A large-scale dataset towards multi-modal multilingual instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2306.04387.

\bibitem[{Lin(2004)}]{lin-2004-rouge}
Chin-Yew Lin. 2004.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}, pages 74--81.

\bibitem[{Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick}]{lin2014mscoco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick. 2014.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee}]{liu2023llava15}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee. 2023{\natexlab{a}}.
\newblock Improved baselines with visual instruction tuning.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee}]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. 2023{\natexlab{b}}.
\newblock Visual instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2304.08485.

\bibitem[{Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao}]{mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023.
\newblock Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models.
\newblock \emph{ArXiv preprint}, abs/2310.02255.

\bibitem[{Lu et~al.(2022)Lu, Mishra, Xia, Qiu, Chang, Zhu, Tafjord, Clark, and Kalyan}]{lu2022scienceqa}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock In \emph{The 36th Conference on Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Ma et~al.(2024)Ma, Chu, Yang, Lin, Gao, and Zhao}]{ma2024paramtuning}
Xinyu Ma, Xu~Chu, Zhibang Yang, Yang Lin, Xin Gao, and Junfeng Zhao. 2024.
\newblock Parameter efficient quasi-orthogonal fine-tuning via givens rotation.
\newblock \emph{ArXiv preprint}, abs/2404.04316.

\bibitem[{Madureira(2021)}]{madureira-2021-flamingos}
Brielen Madureira. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.teachingnlp-1.14} {Flamingos and hedgehogs in the croquet-ground: Teaching evaluation of {NLP} systems for undergraduate students}.
\newblock In \emph{Proceedings of the Fifth Workshop on Teaching NLP}, pages 87--91.

\bibitem[{OpenAI(2023)}]{gpt4v}
OpenAI. 2023.
\newblock Gpt-4v(ision) system card.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022instructgpt}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:27730--27744.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu}]{bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock \href {https://doi.org/10.3115/1073083.1073135} {{B}leu: a method for automatic evaluation of machine translation}.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics}, pages 311--318.

\bibitem[{Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever}]{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning Research}, pages 8748--8763.

\bibitem[{Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser et~al.}]{reid2024gemini}
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al. 2024.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock \emph{ArXiv preprint}, abs/2403.05530.

\bibitem[{Reka(2024)}]{reka2024core}
Team Reka. 2024.
\newblock Reka {C}ore, {F}lash, and {E}dge: A series of powerful multimodal language models.

\bibitem[{Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis, Katta, Coombes, Jitsev, and Komatsuzaki}]{laion400m}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
\newblock \emph{ArXiv preprint}, abs/2111.02114.

\bibitem[{Sun et~al.(2023)Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang, Keutzer, and Darrell}]{2023llavarlhf}
Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. 2023.
\newblock Aligning large multimodal models with factually augmented rlhf.
\newblock \emph{ArXiv preprint}, abs/2309.14525.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv preprint}, abs/2302.13971.

\bibitem[{Wang et~al.(2023)Wang, Li, Chen, Zhu, Lin, Cao, Liu, Liu, and Sui}]{wang2023large}
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu, Tianyu Liu, and Zhifang Sui. 2023.
\newblock Large language models are not fair evaluators.
\newblock \emph{ArXiv preprint}, abs/2305.17926.

\bibitem[{Xu et~al.(2023)Xu, Shen, and Huang}]{multiinstruct}
Zhiyang Xu, Ying Shen, and Lifu Huang. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.acl-long.641} {Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pages 11445--11465.

\bibitem[{Yang et~al.(2023{\natexlab{a}})Yang, Li, Lin, Wang, Lin, Liu, and Wang}]{yang2023dawn}
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023{\natexlab{a}}.
\newblock The dawn of lmms: Preliminary explorations with gpt-4v (ision).
\newblock \emph{ArXiv preprint}, abs/2309.17421.

\bibitem[{Yang et~al.(2023{\natexlab{b}})Yang, Dabre, Tanaka, and Okazaki}]{Yang2023SciCap+}
Zhishen Yang, Raj Dabre, Hideki Tanaka, and Naoaki Okazaki. 2023{\natexlab{b}}.
\newblock Scicap+: A knowledge augmented dataset to study the challenges of scientific figure captioning.
\newblock \emph{ArXiv preprint}, abs/2306.03491.

\bibitem[{Yu et~al.(2023)Yu, Sun, Zhang, Cui, Zhang, Wang, and Liu}]{capsfusion}
Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu. 2023.
\newblock Capsfusion: Rethinking image-text data at scale.
\newblock \emph{ArXiv preprint}, abs/2310.20550.

\bibitem[{Yu et~al.(2024)Yu, Yang, Li, Wang, Lin, Liu, Wang, and Wang}]{yu2024mmvet}
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2024.
\newblock Mm-vet: Evaluating large multimodal models for integrated capabilities.
\newblock In \emph{International conference on machine learning}. PMLR.

\bibitem[{Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, Wei, Yu, Yuan, Sun, Yin, Zheng, Yang, Liu, Huang, Sun, Su, and Chen}]{mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen. 2023.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock \emph{ArXiv preprint}, abs/2311.16502.

\bibitem[{Zhang et~al.(2024{\natexlab{a}})Zhang, Yu, Li, Dong, Su, Chu, and Yu}]{zhang2024mm}
Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 2024{\natexlab{a}}.
\newblock Mm-llms: Recent advances in multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2401.13601}.

\bibitem[{Zhang et~al.(2024{\natexlab{b}})Zhang, Jiang, Zhang, Lin, Guo, Qiu, Zhou, Lu, Chang, Gao et~al.}]{zhang2024mathverse}
Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et~al. 2024{\natexlab{b}}.
\newblock Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?
\newblock \emph{ArXiv preprint}, abs/2403.14624.

\bibitem[{Zhang et~al.(2020)Zhang, Kishore, Wu, Weinberger, and Artzi}]{bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q. Weinberger, and Yoav Artzi. 2020.
\newblock Bertscore: Evaluating text generation with {BERT}.
\newblock In \emph{8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}.

\bibitem[{Zhang et~al.(2023)Zhang, Zhang, Gu, Zhou, Lipka, Yang, and Sun}]{llavar}
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023.
\newblock \href {http://arxiv.org/abs/2306.17107} {Llavar: Enhanced visual instruction tuning for text-rich image understanding}.

\bibitem[{Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny}]{zhu2023minigpt4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock \emph{ArXiv preprint}, abs/2304.10592.

\end{thebibliography}
