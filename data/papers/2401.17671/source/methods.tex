\subsection{Human Intracranial Recordings}

Eight subjects undergoing clinical evaluation for drug-resistant epilepsy participated in the study. Electrodes were implanted intracranially (iEEG) with the clinical goal of identifying epileptogenic foci for surgical removal. Any electrodes showing signs of epileptiform discharges, as identified by an epileptologist, were not analyzed in this study. Prior to electrode implantation, all subjects provided written informed consent for research participation. The research protocol was approved by the institutional review board at North Shore University Hospital.

Subjects listened to naturalistic recordings of voice actors reading passages from stories and conversations. To ensure the subjects were paying attention to the stimuli, one of the voices in the recording occasionally directed a question at the listener directly, or the stories were paused and the subject was asked a question, to check their understanding. The subjects were able to effectively answer each question. These pauses separated the stimulus into separate passages.

The envelope of the high-gamma band (70-150 Hz) of the raw neural recordings was computed using the Hilbert transform \cite{edwards2009comparison} and downsampled to 100 Hz. This signal was used as the neural response due to its correlation with neuronal firing rates \cite{ray2011different, steinschneider2008spectrotemporal} and its common use in auditory neuroscience research \cite{mesgarani2014phonetic, bouchard2013functional}. We restricted our analysis to speech-responsive electrodes, which we estimated using a t-test between each electrode's response to the first second of the stimulus compared to last second of silence preceding it (FDR corrected, $p<0.05$ \cite{holm1979simple}), which left $707$ electrodes for analysis. We extracted average word responses from each electrode by taking the average high-gamma signal value in a 100ms window around the midpoint of each word.


\subsection{Large Language Models}

We analyzed $12$ LLMs of approximately $7$ billion parameters downloaded from Hugging Face and implemented with its \texttt{Transformers} library \cite{wolf2019huggingface}, including the most recent and most popular open-source LLMs. We selected these models by searching the Hugging Face Hub for 7 billion parameter models, then using as many of the trending or most-downloaded models that we were able to run without issue. 

We computed two similar evaluation metrics to those used by LLaMA 2 \cite{touvron2023llama2}: Reading Comprehension and Commonsense Reasoning. As measures of English language understanding, these are both highly related to the listening comprehension task which was performed by the human subjects in the study. As in \cite{touvron2023llama2}, these metrics were created by averaging the model's performance on a certain set of related tasks. All individual benchmarks were computed for each model using the Language Model Evaluation Harness \cite{evalharness} on Github.

\begin{itemize}
  \item Reading Comprehension - This metric was the average 0-shot performance of a model on SQuAD 2.0 \cite{rajpurkar2018know} and BoolQ \cite{clark2019boolq}.
  \item Commonsense Reasoning - This metric consists of the average 0-shot performance on OpenBookQA \cite{mihaylov2018can}, PIQA \cite{bisk2020piqa}, HellaSwag \cite{zellers2019hellaswag}, and WinoGrande \cite{sakaguchi2021winogrande}.
\end{itemize}

Overall LLM Performance was computed as the average Reading Comprehension and Commonsense Reasoning scores.

The models used, and their benchmark performance and overall LLM performance scores, are shown in Table \ref{table:models}.

% \begin{center}
% \begin{tabular}{||c | c||} 
%  \hline
%  Models Used & Parameters \\ [0.5ex] 
%  \hline\hline
%  CerebrasGPT-6.7B \cite{dey2023cerebras} & 6658404352 \\
%  \hline
%  FairseqDense-6.7B \cite{artetxe2021efficient} & 6650032128 \\
%  \hline
%  Falcon-7B \cite{almazrouei2023falcon} & 6921720704 \\
%  \hline
%  Galactica-6.7B \cite{taylor2022galactica} & 6657359872 \\
%  \hline
%  LLaMA-7B \cite{touvron2023llama} & 6738415616 \\
%  \hline
%  LLaMA2-7B \cite{touvron2023llama2} & 6738415616 \\
%  \hline
%  LeoLM-7B \cite{leo2023leohessianai} & 6738415616 \\
%  \hline
%  MPT-7B \cite{mosaic2023introducing} & 6649286656 \\
%  \hline
%  Mistral-7B \cite{jiang2023mistral} & 7241732096 \\
%  \hline
%  OPT-6.7B \cite{zhang2022opt} & 6658473984 \\
%  \hline
%  Pythia-6.9B \cite{biderman2023pythia} & 6857302016 \\
%  \hline
%  XwinLM-7B \cite{xwin2023xwin} & 6738415616 \\
%  \hline
% \end{tabular}
% \label{table:models}
% \end{center}

\begin{table}[!h]
\begin{center}
\begin{tabular}{||c | c | c | c||} 
 \hline
 Models Used &  \multicolumn{1}{|p{2.5cm}|}{\centering Reading \\ Comprehension} & \multicolumn{1}{|p{2.5cm}|}{\centering Commonsense \\ Reasoning}  & LLM Performance \\ [0.5ex] 
 \hline\hline

 Galactica-6.7B \cite{taylor2022galactica} & 0.486 & 0.535 & 0.511 \\
 \hline
 CerebrasGPT-6.7B \cite{dey2023cerebras} & 0.565 & 0.575 & 0.570 \\
 \hline
 Pythia-6.9B \cite{biderman2023pythia} & 0.568 & 0.597 & 0.582 \\
 \hline
 OPT-6.7B \cite{zhang2022opt} & 0.581 & 0.616 & 0.598 \\
 \hline
 FairseqDense-6.7B \cite{artetxe2021efficient} & 0.575 & 0.628 & 0.602 \\
 \hline
 LeoLM-7B \cite{leo2023leohessianai} & 0.634 & 0.646 & 0.640 \\
 \hline
 MPT-7B \cite{mosaic2023introducing} & 0.620 & 0.665 & 0.643 \\
 \hline
 Falcon-7B \cite{almazrouei2023falcon} & 0.619 & 0.669 & 0.644 \\
 \hline
 LLaMA-7B \cite{touvron2023llama} & 0.626 & 0.674 & 0.650 \\
 \hline
 LLaMA2-7B \cite{touvron2023llama2} & 0.639 & 0.671 & 0.655 \\
 \hline
 XwinLM-7B \cite{xwin2023xwin} & 0.648 & 0.673 & 0.660 \\
 \hline
 Mistral-7B \cite{jiang2023mistral} & 0.669 & 0.703 & 0.686 \\
 \hline
\end{tabular}
\caption{All models used in the study, along with their computed benchmark performances.}
\label{table:models}
\end{center}
\end{table}

% \begin{center}
% \begin{tabular}{||c ||} 
%  \hline
%  Models Used \\ [0.5ex] 
%  \hline\hline
%  CerebrasGPT-6.7B \cite{dey2023cerebras} \\
%  \hline
%  FairseqDense-6.7B \cite{artetxe2021efficient} \\
%  \hline
%  Falcon-7B \cite{almazrouei2023falcon} \\
%  \hline
%  Galactica-6.7B \cite{taylor2022galactica} \\
%  \hline
%  LLaMA-7B \cite{touvron2023llama} \\
%  \hline
%  LLaMA2-7B \cite{touvron2023llama2} \\
%  \hline
%  LeoLM-7B \cite{leo2023leohessianai} \\
%  \hline
%  MPT-7B \cite{mosaic2023introducing} \\
%  \hline
%  Mistral-7B \cite{jiang2023mistral} \\
%  \hline
%  OPT-6.7B \cite{zhang2022opt} \\
%  \hline
%  Pythia-6.9B \cite{biderman2023pythia} \\
%  \hline
%  XwinLM-7B \cite{xwin2023xwin} \\
%  \hline
% \end{tabular}
% \label{table:models}
% \end{center}

In order to extract LLM embeddings for each stimulus passage (approximately 30-60 seconds when spoken), we fed the text to the model and extracted the embeddings of each layer when given a causal attention mask. When limiting the contextual window of the model, the attention mask was truncated to only include the most recent $N$ tokens. For multi-token words, we used the embedding of the last token in the word. Thus, for each passage, we extracted a tensor of embeddings of shape $(L_{layers}, N_{words}, D_{dimensions})$ from each model.

% \subsection{LLM Benchmark Evaluations}

% We computed two similar evaluation metrics to those used by LLaMA 2 \cite{touvron2023llama2}: Reading Comprehension and Commonsense Reasoning. As measures of English language understanding, these are both highly related to the listening comprehension task which was performed by the human subjects in the study. As in \cite{touvron2023llama2}, these metrics were created by averaging the model's performance on a certain set of related tasks. All individual benchmarks were computed for each model using the Language Model Evaluation Harness \cite{evalharness} on Github.

% \begin{itemize}
%   \item Reading Comprehension - This metric was the average 0-shot performance of a model on SQuAD 2.0 \cite{rajpurkar2018know} and BoolQ \cite{clark2019boolq}.
%   \item Commonsense Reasoning - This metric consists of the average 0-shot performance on OpenBookQA \cite{mihaylov2018can}, PIQA \cite{bisk2020piqa}, HellaSwag \cite{zellers2019hellaswag}, and WinoGrande \cite{sakaguchi2021winogrande}.
% \end{itemize}

% Overall LLM Performance was computed as the average Reading Comprehension and Commonsense Reasoning scores.



\subsection{Ridge Regression Mapping from Embeddings to Neural Responses}

We performed PCA to reduce the dimensionality of each model's embeddings to 500 components. For a given model, PCA was performed for each layer separately. Then, we fit $10$-fold cross-validated ridge regression models to predict the average word responses from each layer's embeddings, sweeping over a range of regularization parameters for each training fold, using \texttt{scikit-learn}'s \texttt{RidgeCV} model \cite{pedregosa2011scikit}.



\subsection{Electrode Localization and Brain Plotting}

Each subject's electrode positions were mapped to the subject's brain using \texttt{iELVis} \cite{groppe2017ielvis} to perform co-registration between pre- and post-implant MRI scans. Then, the subject-specific electrode locations were mapped to the FreeSurfer average brain \cite{fischl2004automatically}. Euclidean distance from posteromedial HG (TE1.1) \cite{morosan2001human} was computed in this average brain, since TE1.1 is a landmark of primary auditory cortex \cite{baumann2013unified, norman2022multiscale, norman2018neural, mischler2023deep}. When visualizing electrodes on the average brain, all subdural electrodes were snapped to the nearest surface point.

\subsection{Comparing LLMs with Centered Kernel Alignment}

To estimate the similarity between high-dimensional embeddings of different models, we used CKA \cite{kornblith2019similarity}, a similarity metric which is related to CCA but has been shown to perform well in high-dimensional scenarios between neural network features. We used the RBF kernel to allow for nonlinear similarity measurement. For a given pair of models, we computed the CKA similarity between the embeddings of one layer of the first model with another layer of the second model. Iterating over all pairs of layers for those two models produced a single similarity matrix. These similarity matrices were then grouped by whether they described a comparison between two models in the top-5 of all LLMs for benchmark performance, one model in the top-5 and the other in the bottom-5, or two models in the bottom-5, and then averaged.

\subsection{Data and Code Availability}

Although the iEEG recordings used in this study cannot be made publicly available, they can be requested from the author [N.M.]. Code for preprocessing neural recordings, including extracting the high-gamma envelope and identifying responsive electrodes is available in the \texttt{naplib-python} package \cite{mischler2023naplib}.


