The intersection of artificial intelligence and neuroscience has emerged as a frontier of great interest, particularly in understanding how large language models (LLMs) and the human brain process language. Prior research has laid foundational work in this area, uncovering intriguing parallels in feature extraction and representational similarities between LLMs and neural responses during language processing. Studies \cite{toneva2019interpreting, abnar2019blackbox, schrimpf2021neural, caucheteux2020language, hosseini2022artificial, anderson2021deep, caucheteux2021disentangling, caucheteux2022brains, sun2020neural} have demonstrated that the representations learned by LLMs can be linearly mapped to neural responses, suggesting that both LLMs and the brain utilize comparable features in language processing. However, these findings offer limited insight into the fundamental characteristics of LLMs that enable this brain-like processing.

Further investigations have delved into different aspects of LLMs to elucidate their resemblance to brain processes. Some studies \cite{goldstein2022shared, caucheteux2023evidence} support the predictive coding hypothesis in human language processing by finding stronger similarities with autoregressive LLMs. Others \cite{caucheteux2022brains, hosseini2022artificial, antonello2023scaling, antonello2023predictive} have explored various factors, such as the LLM language modeling performance, model size and capacity, and the generalizability of linguistic representations as indicators of brain-like processing. These studies imply that the quality of an LLM significantly contributes to its brain-like representations, yet the underlying reason for this similarity remain an open question. Is it merely a matter of scaling up the models \cite{antonello2023scaling}, or do these models share fundamental computational principles that increasingly align well with the spoken language processing pathway in the human brain? This question is significant as it may suggest a potential shift in the paradigm of model optimization. Although both brains \cite{hickok2007cortical, hasson2008hierarchy, lerner2011topographic, ding2017characterizing} and LLMs \cite{ethayarajh2019contextual, tenney2019bert} process speech and language in hierarchical pathways, most studies have analyzed the similarity of their representations without detailed comparisons of the hierarchical processes through which they are created. Thus, it is still unclear whether brains and models arrive at similar representations through the same or different pathways.

We aim to answer these questions by examining the interplay between LLM performance, neural predictability, anatomical alignment, and contextual encoding, potentially paving the way toward models that perform with high accuracy and process language in a manner similar to the human brain. We examined 12 open-source, pre-trained LLMs, all uniform in size but diverse in their linguistic capabilities, particularly in language-understanding tasks such as reading comprehension. We recorded neural responses with intracranial electroencephalography (iEEG) in the auditory cortex and speech processing regions of neurosurgical patients as they listened to speech. We then predicted these neural responses from the embeddings extracted from each layer of the LLMs as they processed the same linguistic input. This approach allowed us to pinpoint which layers and aspects of the LLMs were most predictive of brain activity and explore how variations in model performance align with differences in neural prediction and their anatomical and functional correspondence. Our findings offer a fresh perspective on the evolving landscape of LLMs, providing insights that reveal more intricate parallels in language comprehension between artificial and biological systems, uncover new potential reasons for LLM performance differences, and point to a convergence in LLMs towards a more optimal, brain-like language processing system.


