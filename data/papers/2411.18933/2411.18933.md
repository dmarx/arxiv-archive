# Introduction {#sec:intro}

![Comparative analysis. (Left) Speed comparison between EfficientTAM and SAM 2 on a single NVIDIA A100 GPU. While SAM 2 is challenging for on-device deployment, our EfficientTAM can run 261 ms per frame on iPhone 15 Pro Max. (Right) FPS/Parameter/Performance comparison of EfficientTAM, SAM 2, and other efficient models for zero-shot video object segmentation on SA-V test. We benchmark FPS (frames per second) of all models with 1024 × 1024 input resolution on a single NVIDIA A100.](figures/intro.png){#fig:throughput width="\\linewidth"}

Segment Anything Model 2 (SAM 2) [@ravi2024sam] is a foundational model for unified image and video object segmentation, achieving state-of-the-art performance in various segmentation tasks such as zero-shot image segmentation [@kirillov2023segment; @chen2023semantic; @deng2023segment; @chen2023sam], semi-supervised video object segmentation [@pont20172017; @xu2018youtube; @oh2019video; @bhat2020learning; @robinson2020learning; @li2022recurrent; @yang2022decoupling; @cheng2022xmem; @zhang2023joint; @wang2023look; @wu2023scalable; @cheng2024putting; @yang2024scalable], interactive video segmentation [@caelles20182018; @heo2020interactive; @cheng2021modular; @homayounfar2021videoclick; @yang2023track; @cheng2023segment; @rajivc2023segment; @cheng2024putting; @delatolas2024learning], and other real-world applications [@zhang2024evf; @xiong2024sam2; @shen2024performance; @zhang2024sam2; @ding2024sam2long; @qiu2024ded; @tang2024segment; @zhou2024sam2]. SAM 2 uses a multistage image encoder to extract hierarchical frame features and introduces a memory module to cross-attend to both current frame features and stored memories from observed frames for consistent object segmentation across frames and interactive tracking in videos.

Despite these advantages, SAM 2 is not efficient for mobile deployment, particularly because the large image encoder (e.g., HieraB+) and memory module are expensive. The default image encoder of SAM 2, HieraB+ [@ryali2023hiera], is parameter inefficient, e.g., $\sim$80M parameters. While SAM 2 provides a tiny version, it has a running time of 43.8 FPS comparable to 47.2 FPS of the default SAM 2 model, due to the hierarchical image encoder. Additionally, that the memory tokens (e.g., a concatenation of spatial memory tokens and object pointer tokens) are long, e.g., $\sim$30K, which hurts the efficiency of the memory module with cross-attention.

In this paper, we revisit plain, nonhierarchical image encoder for video object segmentation and tracking anything. We propose using a lightweight vanilla ViT image encoder (e.g., ViT-Tiny/-Small[@touvron2021training]) as EfficientSAMs[@xiong2024efficientsam] did to reduce the complexity of SAM 2 while maintaining decent performance. Further, we propose an efficient cross-attention method for accelerating the memory module. This is achieved by leveraging the underlying structure of memory spatial tokens. We observed that the memory spatial tokens have strong locality and a coarser representation of memory spatial tokens can be a good proxy for performing cross-attention. We show that this yields a good alternative to the original memory module.

To evaluate our method, we conduct extensive experiments across video and image segmentation benchmarks, including MOSE, DAVIS, LVOS, and SA-V for video segmentation, and SA-23 for image segmentation. Our EfficientTAM outperforms strong semi-supervised video object segmentation methods such as Cutie-base, XMem, and DEVA while being more efficient. Compared with SAM 2, our EfficientTAM is comparable, e.g., 74.5% vs 74.7% on SA-V test dataset, with $\sim$ 2x reduced FPS. On image segmentation benchmark, SA-23, our EfficientTAM achieves 60.7% accuracy for zero-shot image segmentation compared to 59.1% accuracy for SAM and and 61.9% for SAM 2. We also benchmarked our EfficientTAM model on iPhone 15 Pro Max, which can run $\sim$ 10 frames per second with reasonable video segmentation performance.

Our main contributions can be summarized as follows:

-   We revisit using plain, non-hierarchical image encoder, ViT-Tiny/-Small for video object segmentation and show that vanilla ViT can achieve competing performance comparing to SAM 2 with hierarchical image encoder.

-   We propose an efficient memory cross-attention by exploiting the underlying memory spatial token structure and demonstrate the favorable performance.

-   We deliver EfficientTAMs, lightweight video object segmentation and track anything models with state-of-the-art quality-efficiency tradeoffs ([1](#fig:throughput){reference-type="ref" reference="fig:throughput"}), which is complementary to SAM 2 for practical deployment.

# Related Work {#sec:formatting}

**Video Object Segmentation (VOS)** is a fundamental task in computer vision, segments objects of interest from the background and tracks target objects in a video. In the unsupervised setting [@grundmann2010efficient; @brox2010object; @lee2011key; @xu2012evaluation; @fragkiadaki2012video; @perazzi2012saliency; @zhang2013video; @li2013video; @papazoglou2013fast; @faktor2014video; @wang2015saliency; @taylor2015causal; @perazzi2016benchmark], VOS models segment salient objects without a reference mask. In the semi-supervised setting [@pont20172017; @xu2018youtube; @oh2019video; @bhat2020learning; @robinson2020learning; @li2022recurrent; @yang2022decoupling; @cheng2022xmem; @zhang2023joint; @wang2023look; @wu2023scalable; @cheng2024putting; @yang2024scalable], VOS requires tracking and segmenting objects based on a first-frame mask of target objects. For interactive video object segmentation (iVOS) [@caelles20182018; @heo2020interactive; @cheng2021modular; @homayounfar2021videoclick; @yang2023track; @cheng2023segment; @rajivc2023segment; @cheng2024putting; @delatolas2024learning], iVOS models perform object segmentation in videos (masklets) with user guidance, e.g., clicks, bounding boxes, scribbles. In SAM 2 [@ravi2024sam]. Semi-supervised VOS and iVOS have been extended to promptable visual segmentation (PVS), where the model can be interactively prompted with different types of inputs such as clicks, boxes, and masks on any frame in a video for segmenting and tracking a valid object.

have achieved huge success on various vision tasks including image classification [@dosovitskiy2020image], object detection [@li2022exploring], image segmentation [@cheng2022masked; @kirillov2023segment], video classification [@fan2021multiscale], and video object segmentation [@duke2021sstvos; @yang2023track]. The original ViT family scales from the efficient ViT-Tiny up to ViT-Huge, with a plain, non-hierarchical architecture. There are also hierarchical vision transformers that combine transformers with hierarchical stage structure, such as Swin [@liu2021swin], MViT [@fan2021multiscale; @li2022mvitv2], PViT [@wang2021pyramid], and Hiera [@ryali2023hiera]. While being successful, hierarchical models are usually slower than their plain ViT counterparts for practical deployment [@ryali2023hiera]. Combining ViT with convolutions [@lecun1989backpropagation] has been explored for fast hybrid models such as MobileViT [@mehta2021mobilevit], LeViT [@graham2021levit], EfficientFormer[@li2022efficientformer], Next-ViT[@li2022next], Tiny-ViT[@wu2022tinyvit], Castling-ViT[@you2023castling], EfficientViT [@liu2023efficientvit], and MobileNetv4 [@qin2024mobilenetv4]. This line of progression towards building efficient ViTs is orthogonal to our EfficientTAM work towards building efficient video object segmentation. Following SAM [@kirillov2023segment] and EfficientSAMs [@xiong2024efficientsam], we are pursuing plain ViT backbones for efficient video object segmentation and track anything tasks.

The field has developed methods to reduce the quadratic cost of standard self-attention with respect to input sequence length [@attention_is_all_you_need]. Local windowed attention has been applied in [@beltagy2020longformer; @zaheer2020bigbird] for reducing the complexity of self-attention. In [@shen2018efficient; @katharopoulos-et-al-2020], a linear dot product approximation is proposed to linearize the softmax matrix in self-attention by heuristically separating keys and queries. In [@choromanski2020rethinking], the Performer model uses random features to approximate self-attention, achieving linear time and memory cost. Nyströmformer in [@xiong2021nystromformer] makes use of the Nyström method to approximate self-attention with a linear cost. Linformer [@wang2020linformer] shows that self-attention is low-rank, which can be approximated by learning linear projection matrices for the keys and values. The approach of [@liu2023efficientvit; @you2023castling] leverages the associative property of matrix multiplication for efficient attentions in vision transformers. This direction has shown success and has achieved decent performance on vision tasks. However, in preliminary experiments we found that these methods underperformed in a memory cross-attention module when adapted for efficiency improvement.

SAM [@kirillov2023segment] is a vision foundation model that can segment any object in an image using interactive prompts such as points and bounding boxes. SAM has demonstrated remarkable zero-shot transfer performance and high versatility for many vision tasks including a broad range of segmentation applications [@chen2023semantic; @cen2023sad; @deng2023segment; @chen2023sam], in-painting [@yu2023inpaint], image restoration [@jiang2023restore], image editing [@gao2023editanything], image shadow removal [@zhang2023deshadow], medical image segmentation [@ma2023segment], camouflaged object detection [@tang2023can], transparent object detection [@han2023segment], concept-based explanation [@sun2023explain], semantic communication [@tariq2023segment], and object tracking [@cheng2023segment; @yang2023track]. The strong ability on image segmentation with flexible prompts motivates the extension of SAM for video object segmentation and track anything. Track Anything Model (TAM) [@yang2023track] combines SAM and XMem [@cheng2022xmem] for interactive video object tracking and segmentation with SAM for frame segmentation and XMem for tracking. SAM-Track [@cheng2023segment] perform object tracking and segmentation in videos by combining SAM [@kirillov2023segment], DeAOT [@yang2022decoupling], and Grounding-Dino [@liu2023grounding]. The latest SAM 2 [@ravi2024sam] extended SAM for video segmentation through a hierarchical image encoder for frame embeddings and a memory module that conditions current frame embeddings on past frames. Motivated by mobile app use-cases and computationally-constrained applications, recent works have reduced the computational cost of SAM, such as MobileSAM [@zhang2023faster], FastSAM [@zhao2023fast], and EfficientSAM [@xiong2024efficientsam]. The present paper focuses on improving the efficiency challenges of SAM 2 for practical deployment of video object segmentation and track anything.

# Approach

## Preliminaries

**Segment Anything.** SAM [@kirillov2023segment] contains a ViT image encoder and a prompt-guided mask decoder. The encoder takes an image and outputs image embeddings. Then the decoder takes the image embeddings and a prompt, which allows cutting out any object from the background in an image. SAM is trained on an image dataset of over 1B masks.

**Segment Anything 2.** The architecture of segment anything 2 (SAM 2) [@ravi2024sam] largely follows SAM, which consists of a hierarchical image encoder, a prompt-guided lightweight mask decoder, and a new memory mechanism. SAM 2 uses a hierarchical image encoder, Hiera [@ryali2023hiera], to produce image embeddings for each frame. The stride 16 and 32 features from Stage 3 and 4 are used for the memory module. The stride 4 and 8 features from Stage 1 and Stage 2 are not used in the memory module but are fed to upsampling layers in the mask decoder for generating segmentation masks. For stable object tracking, SAM 2 employs a memory mechanism consisting of a lightweight memory encoder, a lightweight memory bank, and a memory attention module. It stores information from past frames and uses the memory attention module to perform cross-attention between the stored memory in the memory bank and current frame features, thereby understanding temporal dependencies in video.

The memory attention module consists of a stack of transformer blocks. Each block contains self-attention, cross-attention, and MLP. The first transformer block takes the image embedding from the current frame as an input. The core component of each transformer block, cross-attention, integrates the current frame embedding and the memory stored in memory bank to produce an embedding with temporal correspondence information. For memory tokens, it includes two parts, the spatial embedding tokens from memory encoder and the object-level pointer tokens from mask decoder. Let us assume the number of spatial tokens is $n$, the number of object-level pointer tokens is $P$, and $d_m$ is the channel dimension, memory tokens can be formulated as $M_{b} = \begin{bmatrix} 
    M_s\in \mathbb{R}^{n\times d_m}\\
    M_p\in \mathbb{R}^{P\times d_m}
    \end{bmatrix}.$

Let $L$ be the number of tokens and $d_q$ be the dimension of each token for input frame features after self-attention, $X \in \mathbb{R}^{L \times d_q}$. The input sequence $X \in \mathbb{R}^{L \times d_q}$ is linearly projected to input queries $Q \in \mathbb{R}^{L\times d}$, and the memory tokens, $M_{b} \in \mathbb{R}^{(n+P)\times d_m}$ are linearly projected to keys $K \in \mathbb{R}^{(n+P) \times d}$, and values $V \in \mathbb{R}^{(n+P) \times d}$ respectively, where $d$ is the embedding dimension of queries, keys, and values. The scaled dot-product cross attention mechanism applied on the queries $Q$, keys $K$, values $V$ can be formally written as, $$\label{eq:crossattn}
     \textsf{C}(Q, K, V) =  \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V,$$ where the $\text{softmax}$ operation is applied row-wise. A single head cross attention is used in the memory module. In later discussion, we also consider keys and values as memory tokens for simplification.

**Efficiency Bottleneck.** Despite the advantages of the hierarchical image encoder for multiscale frame feature extraction and cross-attention for integrating current frame features with stored memory, it poses the challenges for practical deployment of SAM 2. The inefficient SAM 2 (tiny) even shows comparable FPS to the base SAM 2, 47.2 FPS vs 43.8 FPS due to the hierarchical design of the image encoder and the use of hierarchical features, which also makes SAM 2 challenging to deploy on mobile devices. Moreover, the number of tokens in keys and values for performing cross-attention in the memory module are super long, e.g., $30K$. It leads to a large computation and memory cost when performing cross-attention, which becomes the efficiency bottleneck of the memory module for real-world deployment.

![image](figures/arch.pdf){width="90%"}

## Efficient Video Object Segmentation and Track Anything

We now address the efficiency issue of SAM 2 for building efficient video object segmentation and track anything model, EfficientTAM. Motivated by the high quality segmentation performance of SAM and EfficientSAM, we revisit using plain, non-hierarchical lightweight image encoders such as ViT-Small/ViT-Tiny, for frame feature extraction. We found that the use of vanilla ViT for frame feature extraction makes EfficientTAM highly efficient and deployable on mobile devices. Further, we introduce an efficient memory module to reduce the computation and memory cost by proposing an efficient cross-attention operation. Based on these two designs, we build efficient video object segmentation and track anything model by largely following SAM2. [\[fig:efficienttams\]](#fig:efficienttams){reference-type="ref" reference="fig:efficienttams"} illusrates an overview of our proposed EfficientTAM.

**Efficient Image Encoder.** The image encoder's role is to produce feature embeddings for each high-resolution frame. We use a SAMI [@xiong2024efficientsam] pretrained vanilla ViT image encoder [@dosovitskiy2020image; @touvron2021training] to extract frame features. Differing from the image encoder of SAM 2, our image encoder provides a single-scale feature map and no other features in the mask decoder are added to the upsampling layers during decoding for segmentation mask generation. We adopt the lightweight image encoders ViT-Small and ViT-Tiny with a $16\times 16$ patch size. Following [@li2022exploring], we use $14\times 14$ non-overlapping windowed attention and 4 equally-spaced global attention blocks to efficiently extract features from high-resolution frames. Our image encoder outputs a single-scale feature embedding with a $16$x reduced resolution, which takes high-resolution (e.g., $1024\times 1024$) frames as input and transforms it into a dense embedding of downscaled size $64\times 64$.

**Efficient Memory Module.** The memory module leverages information from previous frames to facilitate consistent object tracking. Cross-attention is a major efficiency bottleneck of the memory module in SAM 2 [@ravi2024sam] due to its long memory token sequence. We now discuss how exploiting the underlying structure of memory tokens --- local smoothness (strong locality) within spatial memory tokens --- can yield a more efficient cross-attention.

Consider two consecutive memory spatial tokens, $k_i$ and $k_{i+1}$, local smoothness implies that $||k_i - k_{i+1}||^2_2 \leq \frac{c_K}{n^2}$, for $i = 1, \dots, n - 1$, where $c_K$ is a positive constant. This suggests that given a sufficient small local window, $l_w \times l_h$, using a single token to represent other tokens in the homogeneous window may provide a coarser representation of the full set of memory spatial tokens $K_s$ as $\Tilde{K}_s$. We can construct a good surrogate of $K_s$ with the same size, $\Bar{K}_s$, from $\Tilde{K}_s$ by repeating the single token in each window $l_w\times l_h$ times. Under the smoothness assumption, $\Bar{K}_s$ will not be far from $K_s$. Empirically, we observed that a coarser representation of spatial memory tokens is good surrogate of the full spatial memory tokens. [\[fig:cross\_attn\]](#fig:cross_attn){reference-type="ref" reference="fig:cross_attn"} confirms the coarser representation of input keys and values are close to the original keys and values of cross-attention in the memory module.

Utilizing highly correlated neighboring tokens in cross-attention, we perform average pooling to efficiently compute a coarser representation for keys $K$ and values $V$ in our model. For input spatial tokens $K_s = [k_{11}, \dots, k_{1h}; \dots; k_{w1}, \dots, k_{wh}]$ where $w \times h$ is the resolution size, we divide the $n = w\times h$ tokens into $k = \Tilde{w}\times \Tilde{h}$ rectangular pooling regions and compute the average token of each region. For simplicity, we assume $w$ is divisible by $\Tilde{w}$ and $h$ is divisible by $\Tilde{h}$. Denote $l_w = \frac{w}{\Tilde{w}}, l_h = \frac{h}{\Tilde{h}}$. $\Tilde{K}_s$ and $\Tilde{V}_s$ can be computed by averaging each region as, $$\begin{aligned}
\label{eq:keys-values}
\small
\Tilde{k}_{ij} & = \sum_{p = i \times l_w + 1}^{(i + 1) \times l_w}\sum_{q = j \times l_h + 1}^{(j + 1) \times l_h} \frac{k_{pq}}{l_w \times l_h}, \nonumber \\
\Tilde{v}_{ij} & = \sum_{p = i \times l_w + 1}^{(i + 1) \times l_w}\sum_{q = j \times l_h + 1}^{(j + 1) \times l_h} \frac{v_{pq}}{l_w \times l_h}, \end{aligned}$$ where $i = 1, \cdots, \Tilde{w}, j=1, \cdots, \Tilde{h}$. This token-pooling scheme requires a single scan of the tokens leading to an efficient coarse token generation. We find that using averaging pooling with window size, $2 \times 2$, is sufficient to ensure a good approximation for spatial memory tokens.

Assume $\Tilde{K}_s$ is a coarser representation of memory spatial keys, $K_s$, we can construct a good surrogate of $K_s \in \mathbb{R}^{n \times d}$ with the same size, $\Bar{K}_s \in \mathbb{R}^{n \times d}$ from $\Tilde{K}_s \in \mathbb{R}^{\Tilde{w}\Tilde{h} \times d}$ by stacking each $\Tilde{k}_i, i = 1, \dots, \Tilde{w}\Tilde{h}$, $l_w\times l_h$ times, which can be written as, $$\begin{aligned}
    \Bar{K}_s = [\underbrace{\Tilde{k}_1; \dots; \Tilde{k}_1}_{l_w\times l_h}; \underbrace{\Tilde{k}_2; \dots; \Tilde{k}_2}_{l_w\times l_h}; \dots; \underbrace{\Tilde{k}_{\Tilde{w}\Tilde{h}}; \dots; \Tilde{k}_{\Tilde{w}\Tilde{h}}}_{l_w\times l_h}]\end{aligned}$$ Similarly, we stack each $\Tilde{v}_i, i =1, \dots, \Tilde{w}\Tilde{h}$, $l_w\times l_h$ times to construct $\Bar{V}_s  \in \mathbb{R}^{n \times d}$ as a good surrogate of values, $V_s \in \mathbb{R}^{n\times d}$, which can be written as, $$\begin{aligned}
    \small 
    \Bar{V}_s= [\underbrace{\Tilde{v}_1; \dots; \Tilde{v}_1}_{l_w\times l_h}; \underbrace{\Tilde{v}_2; \dots; \Tilde{v}_2}_{l_w \times l_h}; \dots; \underbrace{\Tilde{v}_{\Tilde{w}\Tilde{h}}; \dots; \Tilde{v}_{\Tilde{w}\Tilde{h}}}_{l_w \times l_h}]\end{aligned}$$ Then we concatenate this coarse spatial tokens with object pointer tokens, $\Bar{K} = [\Bar{K}_s; K_p]\in \mathbb{R}^{(n+P)\times d}$ and $\Bar{V} = [\Bar{V}_s; K_p]\in \mathbb{R}^{(n+P)\times d}$, for a good surrogate of original memory tokens, $K$ and $V$. For the coarse memory tokens, $\bar{K}$ and $\bar{V}$, we have, $$\begin{aligned}
\label{eq:replace}
    \text{softmax}\left(\frac{Q\Bar{K}^{T}}{\sqrt{d}}\right)\Bar{V} = \text{softmax}\left(A \right)\Tilde{V},\end{aligned}$$ where $A = [\frac{Q\Tilde{K}_s^{T}}{\sqrt{d}} + \ln{(l_w\times l_h)}, \frac{QK_p^{T}}{\sqrt{d}}] \in \mathbb{R}^{L\times (\Tilde{w}\Tilde{h} + P)}$, $\Tilde{V} = [\Tilde{V}_s; V_p] \in \mathbb{R}^{(\Tilde{w}\Tilde{h}+P)\times d}$. We provide a proof of [\[eq:replace\]](#eq:replace){reference-type="ref" reference="eq:replace"} in the appendix. Since $\Bar{K}$ and $\Bar{V}$ are good surrogate of $K$ and $V$ respectively, we obtain a good surrogate of the original cross-attention, $\text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V$ in [\[eq:crossattn\]](#eq:crossattn){reference-type="ref" reference="eq:crossattn"}, $$\label{eq:scrossattn}
     \Bar{\textsf{C}}(Q, K, V) =  \text{softmax}\left(\frac{Q\bar{K}^T}{\sqrt{d}}\right)\bar{V}.$$ With [\[eq:replace\]](#eq:replace){reference-type="ref" reference="eq:replace"}, we have an efficient version of cross-attention, $$\label{eq:ecrossattn}
     \Bar{\textsf{C}}(Q, K, V) = \text{softmax}(A)\Tilde{V}.$$ **Link to efficient cross-attention variants.** Interestingly, we can find some cross-attention variants based on our proposed efficient cross-attention in [\[eq:ecrossattn\]](#eq:ecrossattn){reference-type="ref" reference="eq:ecrossattn"}. We notice there is a constant for balancing the attention score between coarse spatial tokens and object pointer tokens, avoiding reducing the attention to spatial tokens after pooling. If we remove this constant, it can lead to a linformer variant using averaging pooling to replace the learnable projection. Instead of removing the constant, we add it to keys for regularizing the attention between coarse spatial tokens and object pointer tokens in [\[eq:acrossattn\]](#eq:acrossattn){reference-type="ref" reference="eq:acrossattn"}, for obtaining another variant.

$$\label{eq:acrossattn}
     \Tilde{\textsf{C}}(Q, K, V) = \text{softmax}\left(\frac{Q\Tilde{K}^{T}}{\sqrt{d}} \right)\Tilde{V},$$ where $\Tilde{K} = [\Tilde{K}_s + \ln{(l_w\times l_h)}, K_p]\in\mathbb{R}^{(\Tilde{w}\Tilde{h}+P)\times d}$.

It is feasible to achieve a good surrogate of the original cross-attention because spatial memory embeddings have strong locality. Our efficient cross-attention is close to the original cross-attention, visualized in [\[fig:cross\_attn\]](#fig:cross_attn){reference-type="ref" reference="fig:cross_attn"}.

# Experiments

## Experimental Setting

**Pretraining.** The SA-1B dataset consists of 11M diverse, high resolution images with 1.1B high-quality segmentation masks. Similar to [@ravi2024sam], we pretrain our EfficientTAM without memory components on SA-1B dataset [@kirillov2023segment] for 90k steps. Our ViT image encoder is initialized from pre-trained ViTs [@xiong2024efficientsam] . We use the AdamW optimizer  [@loshchilov2017decoupled] with a momentum, ($\beta_1 = 0.9$, $\beta_2 = 0.999$), a global batch size of 256, and a initial learning rate of $4e-4$. The learning rate is decayed by a reciprocal square root learning rate schedule [@zhai2022scaling] with 1k iterations linear warmup and 5k iterations linear cooldown. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay [@clark2020electra] is set to 0.8. We apply horizontal flip augmentation and resize the input image resolution to $1024\times 1024$. We restrict our training to $64$ masks per image. Our models are pre-trained on 256 A100 GPUs with 80GB GPU memory with a linear combination of focal and dice loss for mask prediction (e.g., a ratio of 20:1). Bfloat16 is used during the training.

**Full Training Datasets.** Following [@ravi2024sam], we train our EfficientTAM including memory components on SA-V dataset [@ravi2024sam] and a 10% subset of SA-1B [@kirillov2023segment]. SA-V is a large-scale and diverse video segmentation dataset, including 51K videos captured across 47 countries and 600K mask annotations covering whole objects and parts. SA-V video resolution ranges from 240p to 4K and duration ranges from 4 seconds to 138 seconds. Unlike SAM 2, we do not use other open-source datasets or internal datasets during our training for a fair comparison with baselines.

**Full Training Implementation Details.** Similar to  [@ravi2024sam], we train our EfficientTAM for 300k steps after pretraining. We use the AdamW optimizer  [@loshchilov2017decoupled] with a momentum, ($\beta_1 = 0.9$, $\beta_2 = 0.999$), a batch size of 256, and a initial learning rate of $6e-5$ for image encoder and $3e-4$ for other components of the model. The learning rate is decayed by a cosine schedule with 15k iterations linear warmup. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay [@clark2020electra] is set to 0.8. We apply horizontal flip image augmentation and resize the input image resolution to $1024\times 1024$. For video, we apply horizontal flip augmentation, affine transformation with degree $25$ and shear $20$, color jittering with brightness $0.1$, contrast $0.03$, saturation $0.03$, gray scale augmentation with a probability of $0.05$, We restrict our training to $64$ masks per image and $3$ masks per frame for video. Our models are trained on 256 A100-80G GPUs with a linear combination of focal and dice losses for mask prediction, mean-absolution-error loss for IoU prediction, and cross-entropy loss for object prediction. The ratio for the linear combination loss is 20:1:1:1. Bfloat16 is used for training.

**Downstream Tasks/Datasets/Models.** [*Tasks and Datasets.*]{.underline} We consider zero-shot video tasks including promptable video segmentation and semi-supervised video object segmentation, and zero-shot image tasks to demonstrate the competing capabilities of EfficientTAM on image and video segmentation. For zero-shot image tasks, we evaluate EfficientTAM on 37 datasets including 23 datasets of SA-23 [@kirillov2023segment] and 14 video datasets introduced in [@ravi2024sam]. For zero-shot video tasks, we evaluate our EfficientTAM on 9 densely annotated datasets for promptable video segmentation. We use 17 video datasets to evaluate zero-shot accuracy under interactive semi-supervised VOS setting using different prompts. For the standard semi-supervised VOS setting where a ground-truth mask on the first frame is provided, MOSE [@ding2023mose], DAVIS2017 [@pont20172017], LVOS [@hong2024lvos], SA-V [@ravi2024sam], and YTVOS [@xu2018youtube] are used to measure the VOS accuracy. We refer readers to [@kirillov2023segment; @ravi2024sam] for the details of these datasets. [*Models.*]{.underline} We use our EfficientTAM for zero-shot image and video tasks.

**Baselines and Evaluation Metrics.** [*Baselines.*]{.underline} For the standard semi-supervised VOS task, where the first-frame mask is provided, we compare the performance of our EfficientTAM with SAM 2[@ravi2024sam], Cutie-base[@cheng2024putting], DEVA [@cheng2023tracking], XMem [@cheng2022xmem], etc. For the zero-shot promptable video segmentation task and the interactive semi-supervised video object segmentation task using different prompts, we compare our method with SAM2 [@ravi2024sam], SAM+XMem++ [@ravi2024sam], and SAM+Cutie [@ravi2024sam]. For zero-shot image segmentation task, we compare with SAM [@kirillov2023segment] and SAM2 [@ravi2024sam]. Note that we use the opensource version of SAM 2 (without training on MOSE/LVOS/YTVOS) for comparison. We also acknowledge the very recent release of SAM 2.1 trained with long memory contexts. [*Evaluation Metrics.*]{.underline} We evaluate our method and all baselines using the accuracy metrics of the combined $\mathcal{J}$(region similarity)&$\mathcal{F}$(contour accuracy), for zero-shot video segmentation tasks; mIoU (mean intersection over union) for zero-shot image segmentation tasks. For efficiency metrics, we compare the number of model parameters or inference throughput on GPU (e.g, A100) and latency on mobile devices (e.g., iPhone 15 Pro Max). We follow SAM 2 [@ravi2024sam] to report metrics. When providing main results on MOSE, LVOS and YTVOS, we submit to their benchmarking servers to evaluate on, *MOSE val*, *LVOS val*, and *YTVOS2019 val*, for final performance. For ablation studies, we evaluate on a MOSE development set, *MOSE dev* with 200 randomly-sampled videos from the MOSE training split [@ravi2024sam].

## Main Results

**Standard Semi-Supervised Video Object Segmentation.** Semi-supervised video object segmentation is the process of object segmentation and tracking in a video based on a ground-truth mask on the first frame. We follow SAM 2 [@ravi2024sam] and report accuracy of our methods on this standard semi-supervised video object segmentation task. We also report latency on a single A100 GPU with a batch size of 1. We evaluate EfficientTAMs with different image encoders, ViT-Tiny and ViT-Small, and memory modules, original memory block and efficient memory block with a $2\times2$ window pooling for a trade-off between efficiency and accuracy. EfficientTAM-S denotes EfficientTAM using a ViT-Small image encoder and the original memory block, and EfficientTAM-S/2 denotes EfficientTAM with a ViT-Small image encoder and efficiency memory block with a $2\times 2$ window pooling. [\[tab:vos\]](#tab:vos){reference-type="ref" reference="tab:vos"} compares our EfficientTAM with VOS baselines including SAM 2 [@ravi2024sam], Cutie-base [@cheng2024putting], and XMem [@cheng2022xmem]. On SA-V test, our EfficientTAM-S achieves 74.5 $\mathcal{J}$&$\mathcal{F}$, outperforming Cutie-base, Cutie-base+, and XMem by 12.2, 12.9, and 14.4, respectively. On long-term video object segmentation benchmark, LVOS, we can also see that Our EfficientTAM-S outperform Cutie-base and XMem by a large margin. Notice that our EfficientTAM-S only underperforms SAM 2 by $<2$ $\mathcal{J}$&$\mathcal{F}$ or $\mathcal{G}$ across 5 video benchmarks with $\sim$2x speedup and $\sim$2.4x fewer parameters. Further, EfficientTAM with efficient memory attention performs slightly worse than the one with original memory attention, but with much speedup, especially on mobile devices, $>$2x reduced latency on iPhone 15. For example, EfficientSAM-S achieves 74.5 $\mathcal{J}$&$\mathcal{F}$ on SA-V test with 1010.8 ms running time per frame on iPhone 15. EfficientSAM-S/2 with efficient cross-memory attention obtain 74.0 $\mathcal{J}$&$\mathcal{F}$ with only 450 ms. These results show the extraordinary benefits of EfficientTAMs for semi-supervised video object segmentation and validate the advantages of our methods for practical deployment.

**Promptable Video Segmentation.** Similar to SAM 2 [@ravi2024sam], we evaluate promptable video segmentation using two settings, offline evaluation and online evaluation. For offline evaluation, we make multiple passes through a video to annotate frames w.r.t. the largest model error. For online evaluation, we make a single pass through the video to annotate frames. 3 clicks per frame are used for the evaluations on 9 densely annotated video datasets including EndoVis, ESD, LVOSv2, LV-VIS, UVO, VOST, PUMaVOS, Virtual KITTI 2, and VIPSeg. Average $\mathcal{J}$&$\mathcal{F}$ accuracy over $1, \dots, 8$ interacted frames is reported. [\[fig:pvs\]](#fig:pvs){reference-type="ref" reference="fig:pvs"} shows the comparison between our method and strong baselines including SAM 2, SAM + XMem++, and SAM + Cutie. EfficientTAM outperforms SAM + XMem++ and SAM + Cutie for both evaluation settings. EfficientTAM also reduces the gap between SAM 2 for offline and online settings. Specifically, with 8 annotated frames with 3-click, EfficientTAM-S and EfficientTAM-S/2 achieve $\sim$ 82 $\mathcal{J}$&$\mathcal{F}$ in average for offline evaluation setting and $\sim$ 81 $\mathcal{J}$&$\mathcal{F}$ in average for online evaluation, outperforming SAM + XMem++, and SAM + Cutie by $>$3 $\mathcal{J}$&$\mathcal{F}$ and reducing the gap of SAM 2. This set of experiments further validate the effectiveness of our EfficientTAM on promptable video segmentation.

**Interactive Semi-Supervised Video Object Segmentation.** We also evaluate our method on the interactive semi-supervised video object segmentation task with click, box, or mask prompts provided only on the first frame by following SAM 2. In [\[tab:interactive\]](#tab:interactive){reference-type="ref" reference="tab:interactive"}, we report the average $\mathcal{J}$&$\mathcal{F}$ accuracy over 17 video datasets for each type of prompt. We observe that EfficientTAM outperforms SAM + XMem++, and SAM + Cutie with different input prompts. We also notice the reduced gap between EfficientTAM and SAM 2. With 1 click, our EfficientTAM-S obtain 63 $\mathcal{J}$&$\mathcal{F}$ accuracy, with a 6 $\mathcal{J}$&$\mathcal{F}$ gain over SAM + XMem++ and SAM + Cutie and a slight loss, 1.3 $\mathcal{J}$&$\mathcal{F}$ comparing to SAM 2. In summary, EfficientTAM performs favorably on the interactive semi-supervised VOS task using different prompts.

**Segment Anything on Images.** We now evaluate our model for the segment anything task on images. In Table [\[tab:sa23\]](#tab:sa23){reference-type="ref" reference="tab:sa23"}, we report 1-click and 5-click mIoU accuracy on both SA-23 benchmark, plus the new benchmark introduced in SAM 2 [@ravi2024sam] with 14 video datasets from video domain. We compare our EfficientTAMs with SAM (ViT-H) and HQ-SAM (ViT-H). Our EfficientTAM-S obtains a 2.6 mIoU improvement over SAM (ViT-H) and 1.6 mIoU improvement over HQ-SAM (ViT-H) on 1-click accuracy. For 5-click, we observe consistent improvement over SAM (ViT-H) and HQ-SAM (ViT-H). We also notice a significant improvement on the video benchmarks of SA-23 and the one with 14 new videos. This indicates our EfficientTAMs are strong for both image and video segmentation.

**Qualitative Evaluation.** [\[fig:visual\_vost\]](#fig:visual_vost){reference-type="ref" reference="fig:visual_vost"} shows two video examples. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target object as SAM 2. More video examples are in the appendix. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient.

## Ablation Studies

**Impact of the object pointer tokens.** We study the effect of the object pointer tokens when performing cross-attention in the memory module. We ablate the cross-attention with or without the object pointer tokens. We find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 $\mathcal{J}$&$\mathcal{F}$, consistent with SAM 2 [@ravi2024sam]. This demonstrates that object pointer tokens need to be cross-attended with spatial tokens from the memory bank.

**Structure of memory tokens.** We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. We observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance drop, 2.3 $\mathcal{J}$&$\mathcal{F}$ on SA-V test.

**Impact of window size.** We perform an averaging pooling for a good surrogate in [\[eq:ecrossattn\]](#eq:ecrossattn){reference-type="ref" reference="eq:ecrossattn"}. We experiment with window sizes $2\times 2$ and $4 \times 4$. We find increasing the window from $2\times 2$ to $4\times 4$ for efficient cross-attention will lead to $\sim$ 1 $\mathcal{J}$&$\mathcal{F}$ accuracy drop with marginal speed improvement. Therefore, we use window size $2\times 2$ to achieve a trade-off between accuracy and efficiency.

**Linear cross-attention.** We explore adapting one representative efficient attention method such as linear attention [@choromanski2020rethinking; @cai2023efficientvit; @you2023castling] by leveraging the associative property of matrix multiplication. We find that linear attention using associative property of matrix multiplication leads to significant performance drop, $>10$ $\mathcal{J}$&$\mathcal{F}$ accuracy on SA-V test, comparing to our proposed efficient cross-attention. Therefore, leveraging the underlying token structure for efficient cross-attention is more effective.

**Efficient cross-attention variants.** We compare efficient cross-attention variants. We find that the Linformer variant underperforms the efficient cross-attention in [\[eq:ecrossattn\]](#eq:ecrossattn){reference-type="ref" reference="eq:ecrossattn"}, 73.4 vs 74 $\mathcal{J}$&$\mathcal{F}$ on SA-V test. However, we find that [\[eq:acrossattn\]](#eq:acrossattn){reference-type="ref" reference="eq:acrossattn"}, can achieve comparable performance, shown in [\[tab:cross\]](#tab:cross){reference-type="ref" reference="tab:cross"}.

**Impact of input resolution.** We ablate the impact of input resolution for video object segmentation. By default, we used $1024\times 1024$. We experiment with different input resolution, e.g., $512\times 512$. [\[tab:res\]](#tab:res){reference-type="ref" reference="tab:res"} shows that decreasing the input resolution leads to some performance drop. But it improves the efficiency, especially on mobile device, 12.5x speedup on iPhone 15. This gives flexibility for practical deployments with different latency and quality needs.

# Conclusions

We revisited using a plain, non-hierachical image encoder for building efficient video object segmentation and track anything model, EfficientTAM. With a vanilla lightweight ViT image encoder, EfficientTAM demonstrated competing image and video segmentation capabilities as hierarchical image encoder while being more efficient and deployable on mobile devices. We also proposed an efficient memory module with faster cross-attention, leveraging the locality of spatial memory embeddings. The efficient memory module further improves EfficientTAM's accuracy-efficiency tradeoff on video segmentation and tracking anything. Extensive experiments on semi-supervised video object segmentation, promptable video segmentation, and the segment anything tasks consistently validate the advantages of our EfficientTAM. Our preliminary work suggests that EfficientTAM has many potential applications for on-device tracking anything.

# Acknowledgments

We thank Chaitanya Ryali for valuable discussions and data access support. We thank Ronghang Hu for suggestions. Thanks to Nikhila Ravi for supporting 1 node of A100 for benchmarking.

# Efficient Cross-Attention

Assume $\Tilde{K}_s$ is a coarser representation of memory spatial keys, $K_s$, a good surrogate of $K_s \in \mathbb{R}^{n \times d}$ with the same size, $\Bar{K}_s \in \mathbb{R}^{n \times d}$ from $\Tilde{K}_s \in \mathbb{R}^{\Tilde{w}\Tilde{h} \times d}$ is constructed by stacking each $\Tilde{k}_i, i = 1, \dots, \Tilde{w}\Tilde{h}$, $l_w\times l_h$ times, $$\begin{aligned}
    \Bar{K}_s = [\underbrace{\Tilde{k}_1; \dots; \Tilde{k}_1}_{l_w\times l_h}; \underbrace{\Tilde{k}_2; \dots; \Tilde{k}_2}_{l_w\times l_h}; \dots; \underbrace{\Tilde{k}_{\Tilde{w}\Tilde{h}}; \dots; \Tilde{k}_{\Tilde{w}\Tilde{h}}}_{l_w\times l_h}]\end{aligned}$$ Each $\Tilde{v}_i, i =1, \dots, \Tilde{w}\Tilde{h}$, is stacked $l_w\times l_h$ times to make $\Bar{V}_s  \in \mathbb{R}^{n \times d}$ as a good surrogate of values, $V_s \in \mathbb{R}^{n\times d}$, $$\begin{aligned}
    \small 
    \Bar{V}_s= [\underbrace{\Tilde{v}_1; \dots; \Tilde{v}_1}_{l_w\times l_h}; \underbrace{\Tilde{v}_2; \dots; \Tilde{v}_2}_{l_w \times l_h}; \dots; \underbrace{\Tilde{v}_{\Tilde{w}\Tilde{h}}; \dots; \Tilde{v}_{\Tilde{w}\Tilde{h}}}_{l_w \times l_h}]\end{aligned}$$ The concatenation of coarse spatial tokens with object pointer tokens is, $\Bar{K} = [\Bar{K}_s; K_p]\in \mathbb{R}^{(n+P)\times d}$ and $\Bar{V} = [\Bar{V}_s; V_p]\in \mathbb{R}^{(n+P)\times d}$.

[\[lem:equiv\]]{#lem:equiv label="lem:equiv"} For the coarse memory tokens, $\bar{K}$ and $\bar{V}$, queries $Q\in\mathbb{R}^{L\times d}$, we have, $$\begin{aligned}
\label{eq:replace}
    \text{softmax}\left(\frac{Q\Bar{K}^{T}}{\sqrt{d}}\right)\Bar{V} = \text{softmax}\left(A \right)\Tilde{V},\end{aligned}$$ where $A = [\frac{Q\Tilde{K}_s^{T}}{\sqrt{d}} + \ln{(l_w\times l_h)}, \frac{QK_p^{T}}{\sqrt{d}}] \in \mathbb{R}^{L\times (\Tilde{w}\Tilde{h} + P)}$, $\Tilde{V} = [\Tilde{V}_s; V_p] \in \mathbb{R}^{(\Tilde{w}\Tilde{h}+P)\times d}$.

Denote $Q = [q_1; \dots; q_L]$, where $q_i \in \mathbb{R}^{1 \times d}$. The cross-attention matrix, $\Bar{C} = \text{softmax}\left(\frac{Q\Bar{K}^{T}}{\sqrt{d}}\right)\Bar{V} \in \mathbb{R}^{L\times d}$. The softmax matrix $\bar{S} = \text{softmax}\left(\frac{Q\Bar{K}^{T}}{\sqrt{d}}\right)\in \mathbb{R}^{L\times (n+P)}$ can be formulated as, $$\resizebox{0.7\linewidth}{!}{$
\bar{S} = D_{\mathcal{S}}\begin{bmatrix} 
    e(\frac{q_1}{\sqrt{d}}\Tilde{k}_1^T) & \dots & e(\frac{q_1}{\sqrt{d}}\Tilde{k}_1^T) &\dots & e(\frac{q_1}{\sqrt{d}}\Tilde{k}_{\Tilde{w}\Tilde{h}}^T) & \dots & e(\frac{q_1}{\sqrt{d}}K_p^T)\\
    \vdots & \dots & \vdots & \dots & \vdots &\dots & \dots \\
    e(\frac{q_L}{\sqrt{d}}\Tilde{k}_1^T) & \dots  & e(\frac{q_L}{\sqrt{d}}\Tilde{k}_1^T) &\dots & e(\frac{q_L}{\sqrt{d}}\Tilde{k}_{\Tilde{w}\Tilde{h}}^T) & \dots & e(\frac{q_L}{\sqrt{d}}K_p^T)
    \end{bmatrix}$}$$ where $D_{\mathcal{S}}$ is a $L\times L$ diagonal matrix, which normalizes each row of the $\bar{S}$ matrix such that the row entries sum up to 1, and $e(\cdot)$ denotes $\exp(\cdot)$. For each row of the cross-attention matrix, we have, $$\begin{aligned}
\label{eq:entry}
    \Bar{C}_{ij} & = D_{{\mathcal{S}}_{ii}}(\underbrace{e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_1 + \dots e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_1}_{l_w\times l_h} 
     + \dots + \underbrace{e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_{\Tilde{w}\Tilde{h}}  + \dots e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_{\Tilde{w}\Tilde{h}}}_{l_w\times l_h}  + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    &= D_{{\mathcal{S}}_{ii}}(l_w\times l_h\times (e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_1 + \dots +  e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_{\Tilde{w}\Tilde{h}}) + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    & = D_{{\mathcal{S}}_{ii}}(l_w\times l_h \times e(\frac{q_i}{\sqrt{d}}\Tilde{K}_s^T)\Tilde{V}_s^T + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    & = D_{{\mathcal{S}}_{ii}}(e(\ln(l_w\times l_h) + \frac{q_i}{\sqrt{d}}\Tilde{K}_s^T)\Tilde{V}_s + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    & = \text{softmax}[\frac{q_i\Tilde{K}_s^{T}}{\sqrt{d}} + \ln{(l_w\times l_h)}, \frac{q_i\Tilde{K}_p^{T}}{\sqrt{d}}][\Tilde{V}_s; V_p] \end{aligned}$$ where $D_{{\mathcal{S}}_{ii}}$ is the $i^{\text{th}}$ diagonal element of the matrix $D_{\mathcal{S}}$. Note that the right side of [\[eq:entry\]](#eq:entry){reference-type="ref" reference="eq:entry"} is the $i^{\text{th}}$ row of $\text{softmax}\left(A \right)\Tilde{V}$. It concludes the proof.

# Ablation Studies

**Impact of the object pointer tokens.** We study the effect of the object pointer tokens when performing cross-attention in the memory module. We ablate the cross-attention with or without the object pointer tokens. When performing cross-attention, we find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 $\mathcal{J}$&$\mathcal{F}$, shown in [\[tab:pointer\]](#tab:pointer){reference-type="ref" reference="tab:pointer"}. The observations are consistent with SAM 2 [@ravi2024sam]. This demonstrates that object pointer tokens need to be cross-attended with spatial tokens.

**Structure of memory tokens.** We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. In [\[tab:pooling\]](#tab:pooling){reference-type="ref" reference="tab:pooling"}, we observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance drop, 2.3 $\mathcal{J}$&$\mathcal{F}$ on SA-V test.

**Local windowed cross-attention.** We adapt local windowed attention for efficient cross-attention by partitioning input tokens into 4 non-overlapping segments (windows), within which we conduct cross-attention. In [\[tab:windowed\]](#tab:windowed){reference-type="ref" reference="tab:windowed"}, we find that local windowed cross-attention underperforms our proposed efficient cross-attention using averaging pooling, 72.4 vs 74.0 $\mathcal{J}$&$\mathcal{F}$ on SA-V test dataset. These results demonstrate the effectiveness of our efficient cross-attention by leveraging the strong locality of spatial memory tokens.

**Efficient cross-attention variant.** We observe that [\[eq:acrossattn\]](#eq:acrossattn){reference-type="ref" reference="eq:acrossattn"} in the main paper is close to original cross-attention, visualized in [\[fig:across\_attn\]](#fig:across_attn){reference-type="ref" reference="fig:across_attn"}. This suggests that [\[eq:acrossattn\]](#eq:acrossattn){reference-type="ref" reference="eq:acrossattn"} can also serve as a surrogate of the original cross-attention.

# Qualitative Evaluation

We provide more qualitative results of EfficientTAMs for video and image instance segmentation. [\[fig:visual\_vost3\]](#fig:visual_vost3){reference-type="ref" reference="fig:visual_vost3"} shows two challenging video examples with occluded objects. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target occluded object as SAM 2. For image segmentation, we also observe that our EfficientTAM can generate quality image segmentation results as SAM and SAM 2, shown in [\[fig:visual\_seg\]](#fig:visual_seg){reference-type="ref" reference="fig:visual_seg"}. We report the predicted masks with two types of prompts, point and box, and also segment everything results. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient.
